<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-25T01:41:52Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Josh-XT/Agent-LLM</title>
    <updated>2023-04-25T01:41:52Z</updated>
    <id>tag:github.com,2023-04-25:/Josh-XT/Agent-LLM</id>
    <link href="https://github.com/Josh-XT/Agent-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Artificial Intelligence Automation Platform. AI Instruction management from various providers, has an adaptive memory, and a versatile plugin system with many commands including web browsing. Supports many AI providers and models and growing support every day.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent-LLM&lt;/h1&gt; &#xA;&lt;p&gt;Agent-LLM is an Artificial Intelligence Automation Platform designed for efficient AI instruction management across multiple providers. Equipped with adaptive memory, this versatile solution offers a powerful plugin system that supports a wide range of commands, including web browsing. With growing support for numerous AI providers and models, Agent-LLM is constantly evolving to cater to diverse applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/vfXjyuKZ&#34;&gt;&lt;img src=&#34;https://assets-global.website-files.com/6257adef93867e50d84d30e2/636e0a6a49cf127bf92de1e2_icon_clyde_blurple_RGB.png&#34; height=&#34;70&#34; style=&#34;margin: 0 10px&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://twitter.com/Josh_XT&#34;&gt;&lt;img src=&#34;https://img.freepik.com/free-icon/twitter_318-674515.jpg&#34; height=&#34;70&#34; style=&#34;margin: 0 10px&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/Josh-XT/Agent-LLM&#34;&gt;&lt;img src=&#34;https://qph.cf2.quoracdn.net/main-qimg-729a22aba98d1235fdce4883accaf81e&#34; height=&#34;70&#34; style=&#34;margin: 0 10px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;⚠️ &lt;strong&gt;Please note that using some AI providers, such as OpenAI&#39;s API, can be expensive. Monitor your usage carefully to avoid incurring unexpected costs. We&#39;re NOT responsible for your usage under any circumstance.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;⚠️ &lt;strong&gt;This project is under active development and may still have issues.&lt;/strong&gt; We appreciate your understanding and patience. If you encounter any problems, please first check the open issues. If your issue is not listed, kindly create a new issue detailing the error or problem you experienced. Thank you for your support!&lt;/p&gt; &#xA;&lt;h2&gt;Screenshot and Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/102809327/233758245-94535c01-d4e8-4f9c-9b1c-244873361c85.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;From &lt;a href=&#34;https://www.youtube.com/@intheworldofai&#34;&gt;World of AI&lt;/a&gt; on YouTube: &lt;a href=&#34;https://www.youtube.com/watch?v=g0_36Mf2-To&#34;&gt;Agent LLM: AI Automation Bot for Managing and Implementing AI Through Applications&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;⚠️ Run this in Docker or a Virtual Machine!&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;re welcome to disregard this message, but if you do and the AI decides that the best course of action for its task is to build a command to format your entire computer, that is on you. Understand that this is given full unrestricted terminal access by design and that we have no intentions of building any safeguards. This project intends to stay light weight and versatile for the best possible research outcomes.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#agent-llm&#34;&gt;Agent-LLM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#screenshot-and-video&#34;&gt;Screenshot and Video&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#%EF%B8%8F-run-this-in-docker-or-a-virtual-machine&#34;&gt;⚠️ Run this in Docker or a Virtual Machine!&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#key-features&#34;&gt;Key Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#web-application-features&#34;&gt;Web Application Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#running-a-mac&#34;&gt;Running a Mac?&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#not-using-openai-no-problem&#34;&gt;Not using OpenAI? No problem!&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#configuration&#34;&gt;Configuration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#api-endpoints&#34;&gt;API Endpoints&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#extending-functionality&#34;&gt;Extending Functionality&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#commands&#34;&gt;Commands&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#ai-providers&#34;&gt;AI Providers&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#donations-and-sponsorships&#34;&gt;Donations and Sponsorships&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Adaptive long-term and short-term memory management&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Versatile plugin system with extensible commands for various AI models&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Wide compatibility with multiple AI providers, including:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;OpenAI GPT-3.5, GPT-4&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Oobabooga Text Generation Web UI&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Kobold&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;llama.cpp&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;FastChat&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Google Bard&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;And More!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Web browsing and command execution capabilities&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Code evaluation support&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Seamless Docker deployment&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Integration with Hugging Face for audio-to-text conversion&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Interoperability with platforms like Twitter, GitHub, Google, DALL-E, and more&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Text-to-speech options featuring Brian TTS, Mac OS TTS, and ElevenLabs&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Continuously expanding support for new AI providers and services&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Web Application Features&lt;/h2&gt; &#xA;&lt;p&gt;The frontend web application of Agent-LLM provides an intuitive and interactive user interface for users to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manage agents: View the list of available agents, add new agents, delete agents, and switch between agents.&lt;/li&gt; &#xA; &lt;li&gt;Set objectives: Input objectives for the selected agent to accomplish.&lt;/li&gt; &#xA; &lt;li&gt;Start tasks: Initiate the task manager to execute tasks based on the set objective.&lt;/li&gt; &#xA; &lt;li&gt;Instruct agents: Interact with agents by sending instructions and receiving responses in a chat-like interface.&lt;/li&gt; &#xA; &lt;li&gt;Available commands: View the list of available commands and click on a command to insert it into the objective or instruction input boxes.&lt;/li&gt; &#xA; &lt;li&gt;Dark mode: Toggle between light and dark themes for the frontend.&lt;/li&gt; &#xA; &lt;li&gt;Built using NextJS and Material-UI&lt;/li&gt; &#xA; &lt;li&gt;Communicates with the backend through API endpoints&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Obtain an OpenAI API key from &lt;a href=&#34;https://platform.openai.com&#34;&gt;OpenAI&lt;/a&gt; and add it to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in your &lt;code&gt;.env&lt;/code&gt; file using the provided &lt;a href=&#34;https://github.com/Josh-XT/Agent-LLM/raw/main/.env.example&#34;&gt;.env.example&lt;/a&gt; as a template.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/docker-compose.yml&#xA;wget https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/.env.example&#xA;mv .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the following Docker command in the folder with your &lt;code&gt;.env&lt;/code&gt; file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Access the web interface at &lt;a href=&#34;http://localhost&#34;&gt;http://localhost&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Running a Mac?&lt;/h3&gt; &#xA;&lt;p&gt;You&#39;ll need to run &lt;code&gt;docker-compose&lt;/code&gt; to build if the command above does not work.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose -f docker-compose-mac.yml up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Not using OpenAI? No problem!&lt;/h3&gt; &#xA;&lt;p&gt;We are constantly trying to expand our AI provider support. Take a look at our Jupyter Notebooks for Quick starts for these:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reminder:&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Josh-XT/Agent-LLM/main/#%EF%B8%8F-run-this-in-docker-or-a-virtual-machine&#34;&gt;⚠️ Run this in Docker or a Virtual Machine!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Josh-XT/Agent-LLM/raw/main/notebooks/openai.ipynb&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Josh-XT/Agent-LLM/raw/main/notebooks/llamacpp.ipynb&#34;&gt;llamacpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Josh-XT/Agent-LLM/raw/main/notebooks/oobabooga.ipynb&#34;&gt;Oobabooga Text Generation Web UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Josh-XT/Agent-LLM/raw/main/notebooks/chatgpt.ipynb&#34;&gt;ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Josh-XT/Agent-LLM/raw/main/notebooks/bard.ipynb&#34;&gt;Google Bard&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more detailed setup and configuration instructions, refer to the sections below.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Agent-LLM utilizes a &lt;code&gt;.env&lt;/code&gt; configuration file to store AI language model settings, API keys, and other options. Use the supplied &lt;code&gt;.env.example&lt;/code&gt; as a template to create your personalized &lt;code&gt;.env&lt;/code&gt; file. Configuration settings include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;INSTANCE CONFIG&lt;/strong&gt;: Set the agent name, objective, and initial task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI_PROVIDER&lt;/strong&gt;: Choose between OpenAI, llama.cpp, or Oobabooga for your AI provider.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI_PROVIDER_URI&lt;/strong&gt;: Set the URI for custom AI providers such as Oobabooga Text Generation Web UI (default is &lt;a href=&#34;http://127.0.0.1:7860&#34;&gt;http://127.0.0.1:7860&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MODEL_PATH&lt;/strong&gt;: Set the path to the AI model if using llama.cpp or other custom providers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;COMMANDS_ENABLED&lt;/strong&gt;: Enable or disable command extensions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MEMORY SETTINGS&lt;/strong&gt;: Configure short-term and long-term memory settings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI_MODEL&lt;/strong&gt;: Specify the AI model to be used (e.g., gpt-3.5-turbo, gpt-4, text-davinci-003, Vicuna, etc.).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI_TEMPERATURE&lt;/strong&gt;: Set the AI temperature (leave default if unsure).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MAX_TOKENS&lt;/strong&gt;: Set the maximum number of tokens for AI responses (default is 2000).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;WORKING_DIRECTORY&lt;/strong&gt;: Set the agent&#39;s working directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;EXTENSIONS_SETTINGS&lt;/strong&gt;: Configure settings for OpenAI, Hugging Face, Selenium, Twitter, and GitHub.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VOICE_OPTIONS&lt;/strong&gt;: Choose between Brian TTS, Mac OS TTS, or ElevenLabs for text-to-speech.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a detailed explanation of each setting, refer to the &lt;code&gt;.env.example&lt;/code&gt; file provided in the repository.&lt;/p&gt; &#xA;&lt;h2&gt;API Endpoints&lt;/h2&gt; &#xA;&lt;p&gt;Agent-LLM provides several API endpoints for managing agents, managing tasks, and managing chains.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about the API endpoints and their usage, visit the API documentation at &lt;a href=&#34;http://localhost:5000/docs&#34;&gt;http://localhost:5000/docs&lt;/a&gt; (swagger) or &lt;a href=&#34;http://localhost:5000/redoc&#34;&gt;http://localhost:5000/redoc&lt;/a&gt; (Redoc).&lt;/p&gt; &#xA;&lt;h2&gt;Extending Functionality&lt;/h2&gt; &#xA;&lt;h3&gt;Commands&lt;/h3&gt; &#xA;&lt;p&gt;To introduce new commands, generate a new Python file in the &lt;code&gt;commands&lt;/code&gt; folder and define a class inheriting from the &lt;code&gt;Commands&lt;/code&gt; class. Implement the desired functionality as methods within the class and incorporate them into the &lt;code&gt;commands&lt;/code&gt; dictionary.&lt;/p&gt; &#xA;&lt;h3&gt;AI Providers&lt;/h3&gt; &#xA;&lt;p&gt;To switch AI providers, adjust the &lt;code&gt;AI_PROVIDER&lt;/code&gt; setting in the &lt;code&gt;.env&lt;/code&gt; file. The application is compatible with OpenAI, Oobabooga Text Generation Web UI, and llama.cpp. To support additional providers, create a new Python file in the &lt;code&gt;provider&lt;/code&gt; folder and implement the required functionality.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This project was inspired by and utilizes code from the following repositories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yoheinakajima/babyagi&#34;&gt;babyagi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Significant-Gravitas/Auto-GPT&#34;&gt;Auto-GPT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please consider exploring and contributing to these projects as well.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to Agent-LLM! If you&#39;re interested in contributing, please check out the open issues, submit pull requests, or suggest new features. To stay updated on the project&#39;s progress, follow &lt;a href=&#34;https://twitter.com/Josh_XT&#34;&gt;@Josh_XT&lt;/a&gt; on Twitter.&lt;/p&gt; &#xA;&lt;h2&gt;Donations and Sponsorships&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate any support for Agent-LLM&#39;s development, including donations, sponsorships, and any other kind of assistance. If you would like to support us, please contact us through our &lt;a href=&#34;https://discord.gg/Na8M7mTayp&#34;&gt;Discord server&lt;/a&gt; or Twitter &lt;a href=&#34;https://twitter.com/Josh_XT&#34;&gt;@Josh_XT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re always looking for ways to improve Agent-LLM and make it more useful for our users. Your support will help us continue to develop and enhance the application. Thank you for considering to support us!&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Run Agent-LLM using Docker (recommended) or by running the &lt;code&gt;app.py&lt;/code&gt; script. The application will load the initial task and objective from the configuration file and begin task execution. As tasks are completed, Agent-LLM will generate new tasks, prioritize them, and continue working through the task list.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>danielgross/LlamaAcademy</title>
    <updated>2023-04-25T01:41:52Z</updated>
    <id>tag:github.com,2023-04-25:/danielgross/LlamaAcademy</id>
    <link href="https://github.com/danielgross/LlamaAcademy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A school for camelids&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LlamaAcademy: Teaching Llamas How to Code&lt;/h1&gt; &#xA;&lt;h3&gt;Teach GPTs to read API documentation using LLaMA, LoRA, and Langchain.&lt;/h3&gt; &#xA;&lt;p&gt;Wouldn&#39;t it be great if GPTs could learn about new APIs? With LlamaAcademy you can teach GPTs to call Stripe, Notion, or even your own product&#39;s API. Instead of hosting API &lt;em&gt;documentation&lt;/em&gt;, you can host an API &lt;em&gt;implementation&lt;/em&gt;! Just point LlamaAcademy at your API docs, run the script, and -- &lt;em&gt;shazam&lt;/em&gt;! -- a new LLaMA model will be created for you. You can host that model on your server, and users can call your bespoke mini-GPT to write their API glue.&lt;/p&gt; &#xA;&lt;h4&gt;Seriously?&lt;/h4&gt; &#xA;&lt;p&gt;Well, sort of. LlamaAcademy is experimental -- we haven&#39;t gotten it to consistently generate great code (yet). We&#39;d love help with that, if you&#39;re into that sort of thing.&lt;/p&gt; &#xA;&lt;h4&gt;Demo: A Llama That Learned Notion&#39;s API&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/51882888/232329429-c7aadc40-8251-41f3-b4bb-9ac41ac2c6f8.mp4&#34;&gt;https://user-images.githubusercontent.com/51882888/232329429-c7aadc40-8251-41f3-b4bb-9ac41ac2c6f8.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;LlamaAcademy is a pipeline that combines the following steps: crawling, data generation using GPT3.5 and GPT4 and fine-tuning Vicuna-13B on synthetic data. &lt;img src=&#34;https://raw.githubusercontent.com/danielgross/LlamaAcademy/main/assets/data_generation.jpg&#34; alt=&#34;LlamaAcademy Data Generation&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You need to install firefox and Elinks, then install all necessary pythonic dependencies. You also need to input an OPENAI_KEY.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install firefox elinks&#xA;conda env create --file=environment.yaml&#xA;conda env config vars set OPENAI_API_KEY=YOUR_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;LlamaAcademy uses a simple interface by abstracting every user hyper-parameters with a configuration file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;GENERATE: True # Turn off if you don&#39;t want to generate the data&#xA;API_DOCS: https://developers.notion.com/reference &#xA;DEPTH_CRAWLING: 1 # 0 if your API website is long and not hierarchical (for example polygon.io). Otherwise, feel free to set, it might take much longer if your webiste has many children.&#xA;SUMMARIZE_DOCS: True&#xA;MICRO_BATCH_SIZE: 3  &#xA;BATCH_SIZE: 12&#xA;EPOCHS: 4  &#xA;LEARNING_RATE: 3e-4  &#xA;WARMUP_STEPS: 5&#xA;CUTOFF_LEN: 2048 &#xA;LORA_R: 8&#xA;LORA_ALPHA: 16&#xA;LORA_DROPOUT: 0.05&#xA;OPENAI_ENGINE: &#34;gpt-4&#34;&#xA;NUM_PROMPT_INSTRUCTIONS: 3&#xA;NUM_TASKS_TO_GENERATE: 200 # Recommended number of examples&#xA;DATA_PATH: &#34;assets/&#34;&#xA;OUTPUT_DIR: &#34;output/lora-vicuna-api-notion&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the fine-tuning process, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python3 main.py --config configs/vicuna_13b.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the training, run export LoRA model to HuggingFace weights by doing this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 export_hf.py --base_model jeffwan/vicuna-13b --model_folder output/lora-vicuna-api-notion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run inference with LangChain:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 inference.py --model_folder output/lora-vicuna-api-notion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Hardware requirements&lt;/h3&gt; &#xA;&lt;p&gt;This code is tested with 1 RTX A6000 instance in vast.ai (approximated 0.6$/1h). The peak VRAM is 27.8 GB, therefore, any GPU with VRAM &amp;gt; 30GB will be safe for fine-tuning. The fine-tuning is done after 20 minutes with 100 examples, the data generation is completed after 1 hour (most of the time spent in GPT-4 instances generation and crawling process due to screen scraping being quite expensive).&lt;/p&gt; &#xA;&lt;h2&gt;Plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement (IA)^3 for few-shot fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement flash_attention.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement scratch-pad based GPT-4 agent to generate multi-turn planning and generating code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code Files&lt;/h2&gt; &#xA;&lt;p&gt;This repository provides the following Folders and Files&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;assets/&lt;/code&gt;: The folder contains seed tasks + training URLs to generate the data (see self-instruct for more information). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;data.json&lt;/code&gt;: generated data will be saved here for training.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;generated_instructions.jsonl&lt;/code&gt;: generated instructions for instruction tuning will be saved here.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;training_urls.json&lt;/code&gt;: common API for crawling and generating the training data (other direction).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seed_tasks.json&lt;/code&gt;: human written seed tasks for self-instruct process (4-10 examples are recommended).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;prompt_summary.txt&lt;/code&gt;: prompt for GPT3.5-turbo extract and summarize the crawled API documents.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;prompt_input_code.txt&lt;/code&gt;: prompt for GPT4 generate code with references queried from the vector score.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;configs/&lt;/code&gt;: The folder for the configuration files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;chain.py&lt;/code&gt;: The file for custom Langchain pipeline and agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;data_gen.py&lt;/code&gt;: The file implementing data generation using GPT3.5, GPT4, Bing with different strategies.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;main.py&lt;/code&gt;: The main inteference file for user to customize their Alpaca to API references (scraping API references website, generating instruction-code pairs and fine-tuning Vicuna).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inference.py&lt;/code&gt;: Allow user to inference with a trained model with a query related to the API (using Langchain + LlamaAcademy).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;environment.yaml&lt;/code&gt;: The file for the dependencies.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utils.py&lt;/code&gt;: The file for the helper functions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;memorizing.py&lt;/code&gt;: (Still under construction) Using &lt;a href=&#34;https://arxiv.org/pdf/2203.08913.pdf&#34;&gt;memory fine-tuning method&lt;/a&gt; to force Vicuna to memorize API references without pre-training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ingest_docs.py&lt;/code&gt;: Implementing API references crawling using Elinks and Selenium.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>RiseInRose/MiniGPT-4-ZH</title>
    <updated>2023-04-25T01:41:52Z</updated>
    <id>tag:github.com,2023-04-25:/RiseInRose/MiniGPT-4-ZH</id>
    <link href="https://github.com/RiseInRose/MiniGPT-4-ZH" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MiniGPT-4 中文部署翻译 完善部署细节&lt;/p&gt;&lt;hr&gt;&lt;p&gt;MiniGPT-4: 使用先进的大型语言模型增强视觉语言理解 作者为朱德尧、陈俊、沈晓倩、李翔和Mohamed Elhoseiny。*表示贡献相等。&lt;/p&gt; &#xA;&lt;p&gt;所属机构为沙特阿拉伯国王科技大学。&lt;/p&gt; &#xA;&lt;h2&gt;在线演示&lt;/h2&gt; &#xA;&lt;p&gt;点击图像与MiniGPT-4聊天，了解有关您的图像的信息。 &lt;a href=&#34;https://minigpt-4.github.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/figs/online_demo.png&#34; alt=&#34;demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;更多的例子可以在&lt;a href=&#34;https://minigpt-4.github.io&#34;&gt;项目页面&lt;/a&gt;中找到。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://minigpt-4.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/MiniGPT_4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Vision-CAIR/minigpt4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Vision-CAIR/MiniGPT-4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1OK4kYsZphwt5DXchKkzMBjYF6jnkqh4R?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=__tftoxpBAw&amp;amp;feature=youtu.be&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;新闻&lt;/h2&gt; &#xA;&lt;p&gt;我们现在提供了一个与 Vicuna-7B 对齐的预训练MiniGPT-4！演示GPU内存消耗现在可以低至12GB。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;翻译同学的话&lt;/h2&gt; &#xA;&lt;p&gt;Vicuna-13B fp16 目前已知需要 35G 显存，加载图片后，会到43G，运行时内存高峰大概14G。&lt;/p&gt; &#xA;&lt;p&gt;显存不够的，可以看裁剪设置，位置在运行demo的时候。&lt;/p&gt; &#xA;&lt;p&gt;转换权重时，大概需要80G内存，可以尝试增大 swap 空间。&lt;br&gt; 我原本以为，翻译这篇文档就能帮助大家部署，我自己实际部署下来，发现中间有超级多的坑，非常不利于小白。后续我准备制作一个一键部署包，方便大家。敬请期待～ 有新消息我会及时发布到群里面。&lt;/p&gt; &#xA;&lt;p&gt;这是我最近发现的一个&lt;a href=&#34;https://colab.research.google.com/github/camenduru/MiniGPT-4-colab/blob/main/minigpt4_colab.ipynb#scrollTo=QdfSfmJD4fAc&#34;&gt;一键安装包&lt;/a&gt;。感谢@camenduru 同学。很奇怪的是，他的模型只有433MB大小。而我转出来的有37G。&lt;br&gt; 问题解决了，项目实际运行时，还是需要去下载模型的。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;目前测试下来，一键安装包，运行时需要17G显存。&lt;/p&gt; &#xA;&lt;p&gt;感谢群友 成浩 同学提供的 windows安装踩坑指南 &lt;a href=&#34;https://xlch.wolai.com/pBtGyPh6hyGx118o4deTk&#34;&gt;https://xlch.wolai.com/pBtGyPh6hyGx118o4deTk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果翻译对您有帮助，请帮忙右上角 点击 star.&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/#%E5%9B%BD%E5%86%85%E4%BA%A4%E6%B5%81%E7%BE%A4&#34;&gt;欢迎加入国内AI商业应用交流群&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;简介&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MiniGPT-4使用一个投影层来将来自BLIP-2的冻结视觉编码器与冻结的LLM Vicuna对齐。&lt;/li&gt; &#xA; &lt;li&gt;我们通过两个阶段来训练MiniGPT-4。第一个传统的预训练阶段在使用4个A100大约10小时内，使用大约500万个图像-文本对进行训练。第一阶段过后，Vicuna能够理解图像。但是其生成能力受到了严重的影响。&lt;/li&gt; &#xA; &lt;li&gt;为了解决这个问题和提高可用性，我们提出了一种通过模型和ChatGPT自身创建高质量图像-文本对的新方法。基于此，我们创建了一个小型（总共3500对）但是高质量的数据集。&lt;/li&gt; &#xA; &lt;li&gt;第二个微调阶段在对话模板上使用该数据集进行训练，以显著提高其生成可靠性和整体可用性。令人惊讶的是，这个阶段具有计算效率，并且只需要使用单个A100大约7分钟的时间。&lt;/li&gt; &#xA; &lt;li&gt;MiniGPT-4能够产生许多类似于GPT-4中展示的新兴视觉语言能力。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/figs/overview.png&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;入门指南：&lt;/p&gt; &#xA;&lt;h3&gt;安装&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1.准备代码和环境&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;请先将我们的代码库克隆到本地，创建一个Python环境，然后通过以下命令激活它&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Vision-CAIR/MiniGPT-4.git&#xA;cd MiniGPT-4&#xA;conda env create -f environment.yml&#xA;conda activate minigpt4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.准备预训练的Vicuna权重&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;不想自己折腾的直接下载我们准备好的权重（公开可能会有版权问题，所以暂时先放到微信群了。），然后跳转到第 3 步&lt;/p&gt; &#xA;&lt;p&gt;当前版本的MiniGPT-4是建立在Vicuna-13B v0版本之上的。请参考我们的说明&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/PrepareVicuna.md&#34;&gt;here&lt;/a&gt;来准备Vicuna权重。&lt;/p&gt; &#xA;&lt;h3&gt;here 的翻译如下：&lt;/h3&gt; &#xA;&lt;p&gt;如何准备Vicuna权重&lt;/p&gt; &#xA;&lt;p&gt;Vicuna是一种基于LLAMA的LLM，性能接近于ChatGPT，并且是开源的。我们当前使用的是Vicuna-13B v1.1版本。&lt;/p&gt; &#xA;&lt;p&gt;为了准备Vicuna的权重，首先从 &lt;a href=&#34;https://huggingface.co/lmsys/vicuna-13b-delta-v1.1&#34;&gt;https://huggingface.co/lmsys/vicuna-13b-delta-v1.1&lt;/a&gt; 下载Vicuna的增量权重。如果你已经安装了git-lfs（&lt;a href=&#34;https://git-lfs.com%EF%BC%89%EF%BC%8C&#34;&gt;https://git-lfs.com），&lt;/a&gt; 可以通过以下方式完成：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git lfs install&#xA;# git clone https://huggingface.co/lmsys/vicuna-13b-delta-v0 &#xA;git clone https://huggingface.co/lmsys/vicuna-13b-delta-v1.1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;请注意，这并不是直接可用的工作权重，而是工作权重与LLAMA-13B原始权重之间的差异（由于LLAMA的规则，我们无法分发LLAMA的权重）。&lt;/p&gt; &#xA;&lt;p&gt;然后，您需要获取原始的LLAMA-13B权重，可以按照HuggingFace提供的说明&lt;a href=&#34;https://huggingface.co/transformers/model_doc/gpt2.html#transformers-gpt2-preprocessing-script&#34;&gt;here&lt;/a&gt;或者从互联网上下载。&lt;/p&gt; &#xA;&lt;h3&gt;原始权重获取如下：&lt;/h3&gt; &#xA;&lt;p&gt;提示：&lt;/p&gt; &#xA;&lt;p&gt;直接使用迅雷下载，&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/CDEE3052D85C697B84F4C1192F43A2276C0DAEA0.torrent&#34;&gt;种子在此&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;下面是备选方案： ---- 备选方案开始。----&lt;/p&gt; &#xA;&lt;p&gt;可以通过填写表格来获取LLaMA模型的权重。你肯定不用填写，因为“热心网友”已经泄漏出来了 网址如下：&lt;a href=&#34;https://github.com/facebookresearch/llama/issues/149&#34;&gt;https://github.com/facebookresearch/llama/issues/149&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;小白请推荐百度网盘 &lt;a href=&#34;https://pan.baidu.com/s/1ujG85wgQFuIyf74N9k2MDQ?pwd=nu4d&#34;&gt;https://pan.baidu.com/s/1ujG85wgQFuIyf74N9k2MDQ?pwd=nu4d&lt;/a&gt; 不充会员的，可以使用 ipfs，或者迅雷。具体方法如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 找到这个部分，你可以直接点击链接进入，使用迅雷下载。&#xA;Full backup: ipfs://Qmb9y5GCkTG7ZzbBWMu2BXwMkzyCKcUjtEKPpgdZ7GEFKm&#xA;&#xA;7B: ipfs://QmbvdJ7KgvZiyaqHw5QtQxRtUd7pCAdkWWbzuvyKusLGTw &#xA;13B: ipfs://QmPCfCEERStStjg4kfj3cmCUu1TP7pVQbxdFMwnhpuJtxk &#xA;30B: ipfs://QmSD8cxm4zvvnD35KKFu8D9VjXAavNoGWemPW1pQ3AF9ZZ &#xA;65B: ipfs://QmdWH379NQu8XoesA8AFw9nKV2MpGR4KohK7WyugadAKTh&#xA;&#xA;也可以使用Kubo CLI中的以下命令：&#xA;&#xA;# 可选：预加载 7B 模型。检索您尚未拥有的内容。如有需要，请替换为其他 CID。&#xA;ipfs refs -r QmbvdJ7KgvZiyaqHw5QtQxRtUd7pCAdkWWbzuvyKusLGTw&#xA;&#xA;# 可选：固定7B模型。GC会删除您不使用的旧内容，这可以防止启用 GC 后模型被清除。&#xA;ipfs pin add QmbvdJ7KgvZiyaqHw5QtQxRtUd7pCAdkWWbzuvyKusLGTw&#xA;&#xA;# 通过CLI从IPFS下载并保存到磁盘：&#xA;ipfs get QmbvdJ7KgvZiyaqHw5QtQxRtUd7pCAdkWWbzuvyKusLGTw --output ./7B&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;下载完成后，去百度网盘下载 tokenizer_checklist.chk tokenizer.model 这2个文件。&lt;/p&gt; &#xA;&lt;p&gt;---- 备选方案结束。----&lt;/p&gt; &#xA;&lt;p&gt;文件夹结构如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── 13B&#xA;│&amp;nbsp;&amp;nbsp; ├── =&#xA;│&amp;nbsp;&amp;nbsp; ├── checklist.chk&#xA;│&amp;nbsp;&amp;nbsp; ├── consolidated.00.pth&#xA;│&amp;nbsp;&amp;nbsp; ├── consolidated.01.pth&#xA;│&amp;nbsp;&amp;nbsp; └── params.json&#xA;├── tokenizer_checklist.chk&#xA;└── tokenizer.model&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;下载完权重之后，需要使用转换脚本将它们转换为Hugging Face Transformers格式。可以使用以下命令（示例）调用脚本： 脚本地址：&lt;a href=&#34;https://github.com/huggingface/transformers/raw/main/src/transformers/models/llama/convert_llama_weights_to_hf.py&#34;&gt;https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 转换之前，建议 安装环境依赖，否则转模型到一半会报错，及其浪费时间。&#xA;&#xA;git clone https://github.com/lm-sys/FastChat&#xA;cd FastChat&#xA;# 查看tag&#xA;git tag&#xA;# 切换到最新的tag分支&#xA;git checkout v0.2.3&#xA;# 安装&#xA;pip install e .&#xA;&#xA;# 安装其他依赖&#xA;pip install transformers[sentencepiece]&#xA;&#xA;# 注意，这里，需要安装transforms 环境。国内 镜像源更新不及时，请使用原版pip源，或者直接从项目安装&#xA;其他依赖：如果出现timeout，可以使用魔法，或者版本不要求最新时，使用国内源&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/transformers/models/llama/convert_llama_weights_to_hf.py \&#xA;    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;报错解决：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ValueError: Couldn&#39;t instantiate the backend tokenizer from one of:&#xA;&#xA;https://stackoverflow.com/questions/65431837/transformers-v4-x-convert-slow-tokenizer-to-fast-tokenizer&#xA;&#xA;pip install transformers[sentencepiece]&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注意替换上面的地址为你的文件系统的真实地址。&lt;/p&gt; &#xA;&lt;p&gt;转换完成后，可以通过以下方式加载模型和分词器：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import LlamaForCausalLM, LlamaTokenizer&#xA;&#xA;tokenizer = LlamaTokenizer.from_pretrained(&#34;/output/path&#34;)&#xA;model = LlamaForCausalLM.from_pretrained(&#34;/output/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;当这两个权重准备好后，我们可以使用Vicuna团队的工具来创建真正的工作权重。首先，安装与v0 Vicuna兼容的库：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/transformers@v0.1.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后，运行以下命令以创建最终的工作权重：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m fastchat.model.apply_delta --base /path/to/llama-13b-hf/  --target /path/to/save/working/vicuna/weight/  --delta /path/to/vicuna-13b-delta-v0/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;报错解决：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ValueError: Tokenizer class LLaMATokenizer does not exist or is not currently imported.&#xA;修改 llama-13b-hf/tokenizer_config.json/ 的  &#34;tokenizer_class&#34;: &#34;LLaMATokenizer&#34; =&amp;gt;&#xA; &#34;tokenizer_class&#34;: &#34;LlamaTokenizer&#34;&#xA;&#xA;killed &#xA;转换13B需要 80G左右内存，通常的家用电脑无法承载。可以考虑开启swap&#xA;参考：https://www.cnblogs.com/erlou96/p/14578820.html#_label3_0&#xA;    https://timberkito.com/?p=98 &#xA;&#xA;&#xA;RuntimeError: The size of tensor a (32000) must match the size of tensor b (32001) at non-singleton dimension 0&#xA;参考：https://github.com/lm-sys/FastChat/issues/486&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;现在，您可以准备好使用Vicuna权重了！&lt;/p&gt; &#xA;&lt;p&gt;最终得到的权重文件应该放在一个文件夹内，具有以下结构：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;vicuna_weights&#xA;├── config.json&#xA;├── generation_config.json&#xA;├── pytorch_model.bin.index.json&#xA;├── pytorch_model-00001-of-00003.bin&#xA;...   &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后，在模型配置文件&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/minigpt4/configs/models/minigpt4.yaml#L16&#34;&gt;here&lt;/a&gt;的第16行设定vicuna权重的路径。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. 准备预训练的MiniGPT-4检查点&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;根据您准备的Vicuna模型下载预训练检查点。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint Aligned with Vicuna 13B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint Aligned with Vicuna 7B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1a4zLvaiDBr-36pasffmgpvH5P7CKmpze/view?usp=share_link&#34;&gt;下载&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R/view?usp=sharing&#34;&gt;下载&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;然后，在评估配置文件&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/eval_configs/minigpt4_eval.yaml#L10&#34;&gt;minigpt4_eval.yaml&lt;/a&gt;的第11行中设置预训练检查点的路径。&lt;/p&gt; &#xA;&lt;h3&gt;在本地启动演示&lt;/h3&gt; &#xA;&lt;p&gt;通过运行以下命令在本地机器上试用我们的演示&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/demo.py&#34;&gt;demo.py&lt;/a&gt;：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py --cfg-path eval_configs/minigpt4_eval.yaml  --gpu-id 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;为了节省GPU内存，Vicuna默认为8位加载，搜索宽度为1。&lt;br&gt; 该配置需要大约23G的GPU内存用于Vicuna 13B和11.5G的GPU内存用于Vicuna 7B。&lt;/p&gt; &#xA;&lt;h3&gt;模型裁剪&lt;/h3&gt; &#xA;&lt;p&gt;模型裁剪，有很多种方法。下面提供最简单的一种。（注意，模型裁剪，可能会影响模型精度。导致效果不好。建议没有经验的同学可以同时也体验一下完整版，否则，可能觉得模型很垃圾。）&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 先不写了。我发现需要我帮忙的那个朋友是3090 24G显存的，而且有4张，没有天理了！！！  他只是输错了 device编号而已。  （骂骂咧咧 。。。。。。 &#xA;# ( 骂完手动狗头保命～ 虽然我错了，但写教程是不可能了，一辈子都不可能&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;对于更强大的GPU，您可以在配置文件&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/eval_configs/minigpt4_eval.yaml&#34;&gt;minigpt4_eval.yaml&lt;/a&gt;中将low_resource设置为False并使用更大的搜索宽度以16位运行模型。&lt;/p&gt; &#xA;&lt;p&gt;windows 部署遇到问题可以参考这个 issue &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4/issues/28&#34;&gt;https://github.com/Vision-CAIR/MiniGPT-4/issues/28&lt;/a&gt; 实际上如果显存高，可以修改 minigpt4/models/mini_gpt4.py 92行 load_in_8bit=False 关闭8bit，这样就可以不安装这个包。&lt;/p&gt; &#xA;&lt;p&gt;感谢@WangRongsheng，您也可以在&lt;a href=&#34;https://colab.research.google.com/drive/1OK4kYsZphwt5DXchKkzMBjYF6jnkqh4R?usp=sharing&#34;&gt;Colab&lt;/a&gt;上运行我们的代码。&lt;/p&gt; &#xA;&lt;h3&gt;训练&lt;/h3&gt; &#xA;&lt;p&gt;MiniGPT-4的训练包含两个对齐阶段。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. 第一阶段预训练&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;在第一个预训练阶段中，使用来自Laion和CC数据集的图像文本对训练模型， 以对齐视觉和语言模型。要下载和准备数据集，请查看我们的&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/dataset/README_1_STAGE.md&#34;&gt;第一阶段数据集准备说明&lt;/a&gt;。 在第一阶段之后，视觉特征被映射并可以被语言模型理解。 要启动第一阶段训练，请运行以下命令。在我们的实验中，我们使用了4个A100。 您可以在配置文件&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/train_configs/minigpt4_stage1_pretrain.yaml&#34;&gt;train_configs/minigpt4_stage1_pretrain.yaml&lt;/a&gt;中更改保存路径。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage1_pretrain.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;只有第一阶段训练的MiniGPT-4检查点可在此处下载 &lt;a href=&#34;https://drive.google.com/file/d/1u9FRRBB3VovP1HxCAlpD9Lw4t4P6-Yq8/view?usp=share_link&#34;&gt;here&lt;/a&gt;。 与第二阶段之后的模型相比，此检查点经常生成不完整和重复的句子。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. 第二阶段微调&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;在第二个阶段中，我们使用自己创建的小型高质量图像文本对数据集，并将其转换为对话格式，以进一步对齐MiniGPT-4。 要下载和准备我们的第二阶段数据集，请查看我们的&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/dataset/README_2_STAGE.md&#34;&gt;第二阶段数据集准备说明&lt;/a&gt;。 要启动第二阶段对齐，请先在&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/train_configs/minigpt4_stage2_finetune.yaml&#34;&gt;train_configs/minigpt4_stage2_finetune.yaml&lt;/a&gt;中指定第1阶段训练的检查点文件的路径。 您也可以在那里指定输出路径。 然后，运行以下命令。在我们的实验中，我们使用1个A100。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage2_finetune.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;经过第二阶段的对齐，MiniGPT-4能够以连贯且易于使用的方式讨论图像。&lt;/p&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/blip-2&#34;&gt;BLIP2&lt;/a&gt; ：MiniGPT-4的模型架构遵循BLIP-2。如果您以前不知道它，请不要忘记检查这个伟大的开源工作！&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;Lavis&lt;/a&gt; ：这个存储库是基于Lavis构建的！&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt; ：只有13B个参数的Vicuna的神奇语言能力真是太棒了。它是开源的！&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;如果您在研究或应用中使用MiniGPT-4，请引用以下BibTeX：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhu2022minigpt4,&#xA;      title={MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models}, &#xA;      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and xiang Li and Mohamed Elhoseiny},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;国内交流群&lt;/h2&gt; &#xA;&lt;p&gt;目前群主部署了minigpt4 的在线版本，星球的朋友可以先体验，不想加星球的，可以等星球的同学体验完了，再白嫖（一般是晚上）～&lt;/p&gt; &#xA;&lt;p&gt;群主会不定期发布 各类亮眼项目体验版本 供大家体验，星球主要沉淀一些商业AI最新讯息，帮助大家节约时间。欢迎各位读者老爷，漂亮姐姐给我的项目点赞！&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;关注公众号加群&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;知识星球&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/img/qrcode.png&#34; width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/img/WechatIMG81.jpeg&#34; width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;许可证.&lt;/h2&gt; &#xA;&lt;p&gt;此存储库采用&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/LICENSE.md&#34;&gt;BSD 3-Clause许可证&lt;/a&gt;。&lt;br&gt; 许多代码基于&lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;Lavis&lt;/a&gt;，这里是BSD 3-Clause许可证&lt;a href=&#34;https://raw.githubusercontent.com/RiseInRose/MiniGPT-4-ZH/main/LICENSE_Lavis.md&#34;&gt;here&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;感谢&lt;/h2&gt; &#xA;&lt;p&gt;本项目 fork 自 &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;https://github.com/Vision-CAIR/MiniGPT-4&lt;/a&gt; 大部分翻译来自 &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;https://github.com/Vision-CAIR/MiniGPT-4&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>