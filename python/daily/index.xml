<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-11T01:37:51Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVlabs/eg3d</title>
    <updated>2022-06-11T01:37:51Z</updated>
    <id>tag:github.com,2022-06-11:/NVlabs/eg3d</id>
    <link href="https://github.com/NVlabs/eg3d" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Efficient Geometry-aware 3D Generative Adversarial Networks (EG3D)&lt;br&gt;&lt;sub&gt;Official PyTorch implementation of the CVPR 2022 paper&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/teaser.jpeg&#34; alt=&#34;Teaser image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Geometry-aware 3D Generative Adversarial Networks&lt;/strong&gt;&lt;br&gt; Eric R. Chan*, Connor Z. Lin*, Matthew A. Chan*, Koki Nagano*, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein&lt;br&gt;&lt;em&gt;* equal contribution&lt;/em&gt;&lt;br&gt; &lt;br&gt;&lt;a href=&#34;https://nvlabs.github.io/eg3d/&#34;&gt;https://nvlabs.github.io/eg3d/&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Abstract: &lt;em&gt;Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;For business inquiries, please visit our website and submit the form: &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA Research Licensing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We recommend Linux for performance and compatibility reasons.&lt;/li&gt; &#xA; &lt;li&gt;1–8 high-end NVIDIA GPUs. We have done all testing and development using V100, RTX3090, and A100 GPUs.&lt;/li&gt; &#xA; &lt;li&gt;64-bit Python 3.8 and PyTorch 1.11.0 (or later). See &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt; for PyTorch install instructions.&lt;/li&gt; &#xA; &lt;li&gt;CUDA toolkit 11.3 or later. (Why is a separate CUDA toolkit installation required? See &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary&#34;&gt;Troubleshooting&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Python libraries: see &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/environment.yml&#34;&gt;environment.yml&lt;/a&gt; for exact library dependencies. You can use the following commands with Miniconda3 to create and activate your Python environment: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;cd eg3d&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;conda env create -f environment.yml&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;conda activate eg3d&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Pre-trained networks are stored as &lt;code&gt;*.pkl&lt;/code&gt; files that can be referenced using local filenames. See &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/models.md&#34;&gt;Models&lt;/a&gt; for download links to pre-trained checkpoints.&lt;/p&gt; &#xA;&lt;h2&gt;Generating media&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate videos using pre-trained model&#xA;&#xA;python gen_videos.py --outdir=out --trunc=0.7 --seeds=0-3 --grid=2x2 \&#xA;    --network=networks/network_snapshot.pkl&#xA;&#xA;# Generate the same 4 seeds in an interpolation sequence&#xA;&#xA;python gen_videos.py --outdir=out --trunc=0.7 --seeds=0-3 --grid=1x1 \&#xA;    --network=networks/network_snapshot.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate images and shapes (as .mrc files) using pre-trained model&#xA;&#xA;python gen_samples.py --outdir=out --trunc=0.7 --shapes=true --seeds=0-3 \&#xA;    --network=networks/network_snapshot.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We visualize our .mrc shape files with &lt;a href=&#34;https://www.cgl.ucsf.edu/chimerax/&#34;&gt;UCSF Chimerax&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To visualize a shape in ChimeraX do the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Import the &lt;code&gt;.mrc&lt;/code&gt; file with &lt;code&gt;File &amp;gt; Open&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find the selected shape in the Volume Viewer tool &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The Volume Viewer tool is located under &lt;code&gt;Tools &amp;gt; Volume Data &amp;gt; Volume Viewer&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Change volume type to &#34;Surface&#34;&lt;/li&gt; &#xA; &lt;li&gt;Change step size to 1&lt;/li&gt; &#xA; &lt;li&gt;Change level set to 10 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Note that the optimal level can vary by each object, but is usually between 2 and 20. Individual adjustment may make certain shapes slightly sharper&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;In the &lt;code&gt;Lighting&lt;/code&gt; menu in the top bar, change lighting to &#34;Full&#34;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Interactive visualization&lt;/h2&gt; &#xA;&lt;p&gt;This release contains an interactive model visualization tool that can be used to explore various characteristics of a trained model. To start it, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;python visualizer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/visualizer_guide.md&#34;&gt;&lt;code&gt;Visualizer Guide&lt;/code&gt;&lt;/a&gt; for a description of important options.&lt;/p&gt; &#xA;&lt;h2&gt;Using networks from Python&lt;/h2&gt; &#xA;&lt;p&gt;You can use pre-trained networks in your own Python code as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.python&#34;&gt;with open(&#39;ffhq.pkl&#39;, &#39;rb&#39;) as f:&#xA;    G = pickle.load(f)[&#39;G_ema&#39;].cuda()  # torch.nn.Module&#xA;z = torch.randn([1, G.z_dim]).cuda()    # latent codes&#xA;c = torch.cat([cam2world_pose.reshape(-1, 16), intrinsics.reshape(-1, 9)], 1) # camera parameters&#xA;img = G(z, c)[&#39;image&#39;]                           # NCHW, float32, dynamic range [-1, +1], no truncation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above code requires &lt;code&gt;torch_utils&lt;/code&gt; and &lt;code&gt;dnnlib&lt;/code&gt; to be accessible via &lt;code&gt;PYTHONPATH&lt;/code&gt;. It does not need source code for the networks themselves — their class definitions are loaded from the pickle via &lt;code&gt;torch_utils.persistence&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The pickle contains three networks. &lt;code&gt;&#39;G&#39;&lt;/code&gt; and &lt;code&gt;&#39;D&#39;&lt;/code&gt; are instantaneous snapshots taken during training, and &lt;code&gt;&#39;G_ema&#39;&lt;/code&gt; represents a moving average of the generator weights over several training steps. The networks are regular instances of &lt;code&gt;torch.nn.Module&lt;/code&gt;, with all of their parameters and buffers placed on the CPU at import and gradient computation disabled by default.&lt;/p&gt; &#xA;&lt;p&gt;The generator consists of two submodules, &lt;code&gt;G.mapping&lt;/code&gt; and &lt;code&gt;G.synthesis&lt;/code&gt;, that can be executed separately. They also support various additional options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.python&#34;&gt;w = G.mapping(z, conditioning_params, truncation_psi=0.5, truncation_cutoff=8)&#xA;img = G.synthesis(w, camera_params)[&#39;image]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/eg3d/gen_samples.py&#34;&gt;&lt;code&gt;gen_samples.py&lt;/code&gt;&lt;/a&gt; for complete code example.&lt;/p&gt; &#xA;&lt;h2&gt;Preparing datasets&lt;/h2&gt; &#xA;&lt;p&gt;Datasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file &lt;code&gt;dataset.json&lt;/code&gt; for labels. Each label is a 25-length list of floating point numbers, which is the concatenation of the flattened 4x4 camera extrinsic matrix and flattened 3x3 camera intrinsic matrix. Custom datasets can be created from a folder containing images; see &lt;code&gt;python dataset_tool.py --help&lt;/code&gt; for more information. Alternatively, the folder can also be used directly as a dataset, without running it through &lt;code&gt;dataset_tool.py&lt;/code&gt; first, but doing so may lead to suboptimal performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FFHQ&lt;/strong&gt;: Download and process the &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;Flickr-Faces-HQ dataset&lt;/a&gt; using the following commands.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure the &lt;a href=&#34;https://github.com/sicxu/Deep3DFaceRecon_pytorch/tree/6ba3d22f84bf508f0dde002da8fff277196fef21&#34;&gt;Deep3DFaceRecon_pytorch&lt;/a&gt; submodule is properly initialized&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the following commands&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;cd dataset_preprocessing/ffhq&#xA;python runme.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;AFHQv2&lt;/strong&gt;: Download and process the &lt;a href=&#34;https://github.com/clovaai/stargan-v2/raw/master/README.md#animal-faces-hq-dataset-afhq&#34;&gt;AFHQv2 dataset&lt;/a&gt; with the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the AFHQv2 images zipfile from the &lt;a href=&#34;https://github.com/clovaai/stargan-v2/&#34;&gt;StarGAN V2 repository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the following commands:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;cd dataset_preprocessing/afhq&#xA;python runme.py &#34;path/to/downloaded/afhq.zip&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;ShapeNet Cars&lt;/strong&gt;: Download and process renderings of the cars category of &lt;a href=&#34;https://shapenet.org/&#34;&gt;ShapeNet&lt;/a&gt; using the following commands. NOTE: the following commands download renderings of the ShapeNet cars from the &lt;a href=&#34;https://www.vincentsitzmann.com/srns/&#34;&gt;Scene Representation Networks repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;cd dataset_preprocessing/shapenet&#xA;python runme.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;You can train new networks using &lt;code&gt;train.py&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Train with FFHQ from scratch with raw neural rendering resolution=64, using 8 GPUs.&#xA;python train.py --outdir=~/training-runs --cfg=ffhq --data=~/datasets/FFHQ_512.zip \&#xA;  --gpus=8 --batch=32 --gamma=1 --gen_pose_cond=True&#xA;&#xA;# Second stage finetuning of FFHQ to 128 neural rendering resolution (optional).&#xA;python train.py --outdir=~/training-runs --cfg=ffhq --data=~/datasets/FFHQ_512.zip \&#xA;  --resume=~/training-runs/ffhq_experiment_dir/network-snapshot-025000.pkl \&#xA;  --gpus=8 --batch=32 --gamma=1 --gen_pose_cond=True --neural_rendering_resolution_final=128&#xA;&#xA;# Train with Shapenet from scratch, using 8 GPUs.&#xA;python train.py --outdir=~/training-runs --cfg=shapenet --data=~/datasets/cars_train.zip \&#xA;  --gpus=8 --batch=32 --gamma=0.3&#xA;&#xA;# Train with AFHQ, finetuning from FFHQ with ADA, using 8 GPUs.&#xA;python train.py --outdir=~/training-runs --cfg=afhq --data=~/datasets/afhq.zip \&#xA;  --gpus=8 --batch=32 --gamma=5 --aug=ada --neural_rendering_resolution_final=128 --gen_pose_cond=True --gpc_reg_prob=0.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/training_guide.md&#34;&gt;Training Guide&lt;/a&gt; for a guide to setting up a training run on your own data.&lt;/p&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/models.md&#34;&gt;Models&lt;/a&gt; for recommended training configurations and download links for pre-trained checkpoints.&lt;/p&gt; &#xA;&lt;p&gt;The results of each training run are saved to a newly created directory, for example &lt;code&gt;~/training-runs/00000-ffhq-ffhq512-gpus8-batch32-gamma1&lt;/code&gt;. The training loop exports network pickles (&lt;code&gt;network-snapshot-&amp;lt;KIMG&amp;gt;.pkl&lt;/code&gt;) and random image grids (&lt;code&gt;fakes&amp;lt;KIMG&amp;gt;.png&lt;/code&gt;) at regular intervals (controlled by &lt;code&gt;--snap&lt;/code&gt;). For each exported pickle, it evaluates FID (controlled by &lt;code&gt;--metrics&lt;/code&gt;) and logs the result in &lt;code&gt;metric-fid50k_full.jsonl&lt;/code&gt;. It also records various statistics in &lt;code&gt;training_stats.jsonl&lt;/code&gt;, as well as &lt;code&gt;*.tfevents&lt;/code&gt; if TensorBoard is installed.&lt;/p&gt; &#xA;&lt;h2&gt;Quality metrics&lt;/h2&gt; &#xA;&lt;p&gt;By default, &lt;code&gt;train.py&lt;/code&gt; automatically computes FID for each network pickle exported during training. We recommend inspecting &lt;code&gt;metric-fid50k_full.jsonl&lt;/code&gt; (or TensorBoard) at regular intervals to monitor the training progress. When desired, the automatic computation can be disabled with &lt;code&gt;--metrics=none&lt;/code&gt; to speed up the training slightly.&lt;/p&gt; &#xA;&lt;p&gt;Additional quality metrics can also be computed after the training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Previous training run: look up options automatically, save result to JSONL file.&#xA;python calc_metrics.py --metrics=fid50k_full \&#xA;    --network=~/training-runs/network-snapshot-000000.pkl&#xA;&#xA;# Pre-trained network pickle: specify dataset explicitly, print result to stdout.&#xA;python calc_metrics.py --metrics=fid50k_full --data=~/datasets/ffhq_512.zip \&#xA;    --network=ffhq-128.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the metrics can be quite expensive to compute (up to 1h), and many of them have an additional one-off cost for each new dataset (up to 30min). Also note that the evaluation is done using a different random seed each time, so the results will vary if the same metric is computed multiple times.&lt;/p&gt; &#xA;&lt;p&gt;References:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.08500&#34;&gt;GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium&lt;/a&gt;, Heusel et al. 2017&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.01401&#34;&gt;Demystifying MMD GANs&lt;/a&gt;, Bińkowski et al. 2018&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;!-- ## License&#xA;&#xA;Copyright &amp;copy; 2021, NVIDIA Corporation &amp; affiliates. All rights reserved.&#xA;&#xA;This work is made available under the [Nvidia Source Code License](https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt). --&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{Chan2022,&#xA;  author = {Eric R. Chan and Connor Z. Lin and Matthew A. Chan and Koki Nagano and Boxiao Pan and Shalini De Mello and Orazio Gallo and Leonidas Guibas and Jonathan Tremblay and Sameh Khamis and Tero Karras and Gordon Wetzstein},&#xA;  title = {Efficient Geometry-aware {3D} Generative Adversarial Networks},&#xA;  booktitle = {CVPR},&#xA;  year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;This is a research reference implementation and is treated as a one-time code drop. As such, we do not accept outside code contributions in the form of pull requests.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank David Luebke, Jan Kautz, Jaewoo Seo, Jonathan Granskog, Simon Yuen, Alex Evans, Stan Birchfield, Alexander Bergman, and Joy Hsu for feedback on drafts, Alex Chan, Giap Nguyen, and Trevor Chan for help with diagrams, and Colette Kress and Bryan Catanzaro for allowing use of their photographs. This project was in part supported by Stanford HAI and a Samsung GRO. Koki Nagano and Eric Chan were partially supported by DARPA’s Semantic Forensics (SemaFor) contract (HR0011-20-3-0005). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. Distribution Statement &#34;A&#34; (Approved for Public Release, Distribution Unlimited).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jina-ai/dalle-flow</title>
    <updated>2022-06-11T01:37:51Z</updated>
    <id>tag:github.com,2022-06-11:/jina-ai/dalle-flow</id>
    <link href="https://github.com/jina-ai/dalle-flow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Human-in-the-Loop workflow for creating HD images from text&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/banner.svg?raw=true&#34; alt=&#34;DALL·E Flow: A Human-in-the-Loop workflow for creating HD images from text&#34; width=&#34;60%&#34;&gt; &lt;br&gt; &lt;b&gt;A Human-in-the-Loop&lt;sup&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Human-in-the-loop&#34;&gt;?&lt;/a&gt;&lt;/sup&gt; workflow for creating HD images from text&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://slack.jina.ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-2.8k-blueviolet?logo=slack&amp;amp;logoColor=white&amp;amp;style=flat-square&#34; alt=&#34;Open in Google Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Open-in%20Colab-orange?logo=google-colab&amp;amp;style=flat-square&#34; alt=&#34;Open in Google Colab&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;DALL·E Flow is an interactive workflow for generating high-definition images from text prompt. First, it leverages &lt;a href=&#34;https://github.com/borisdayma/dalle-mini&#34;&gt;DALL·E-Mega&lt;/a&gt; to generate image candidates, and then calls &lt;a href=&#34;https://github.com/jina-ai/clip-as-service&#34;&gt;CLIP-as-service&lt;/a&gt; to rank the candidates w.r.t. the prompt. The preferred candidate is fed to &lt;a href=&#34;https://github.com/Jack000/glid-3-xl&#34;&gt;GLID-3 XL&lt;/a&gt; for diffusion, which often enriches the texture and background. Finally, the candidate is upscaled to 1024x1024 via &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR&#34;&gt;SwinIR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;DALL·E Flow is built with &lt;a href=&#34;https://github.com/jina-ai/jina&#34;&gt;Jina&lt;/a&gt; in a client-server architecture, which gives it high scalability, non-blocking streaming, and a modern Pythonic interface. Client can interact with the server via gRPC/Websocket/HTTP with TLS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why Human-in-the-Loop?&lt;/strong&gt; Generative art is a creative process. While recent advances of DALL·E unleash people&#39;s creativity, having a single-prompt-single-output UX/UI locks the imagination to a &lt;em&gt;single&lt;/em&gt; possibility, which is bad no matter how fine this single result is. DALL·E Flow is an alternative to the one-liner, by formalizing the generative art as an iterative procedure.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;DALL·E Flow is in client-server architecture.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/dalle-flow/main/#Client&#34;&gt;Client usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/dalle-flow/main/#Server&#34;&gt;Server usage, i.e. deploy your own server&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;⚠️ &lt;strong&gt;2022/5/23&lt;/strong&gt; Fix an upstream bug in CLIP-as-service. This bug makes the 2nd diffusion step irrelevant to the given texts. New Dockerfile proved to be reproducible on a AWS EC2 &lt;code&gt;p2.x8large&lt;/code&gt; instance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022/5/13b&lt;/strong&gt; Removing TLS as Cloudflare gives 100s timeout, making DALLE Flow in usable &lt;a href=&#34;https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb&#34;&gt;Please &lt;em&gt;reopen&lt;/em&gt; the notebook in Google Colab!&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🔐 &lt;strong&gt;2022/5/13&lt;/strong&gt; New Mega checkpoint! All connections are now with TLS, &lt;a href=&#34;https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb&#34;&gt;Please &lt;em&gt;reopen&lt;/em&gt; the notebook in Google Colab!&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🐳 &lt;strong&gt;2022/5/10&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/dalle-flow/main/#run-in-docker&#34;&gt;A Dockerfile is added! Now you can easily deploy your own DALL·E Flow&lt;/a&gt;. New Mega checkpoint! Smaller memory-footprint, the whole Flow can now fit into &lt;strong&gt;one GPU with 21GB memory&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🌟 &lt;strong&gt;2022/5/7&lt;/strong&gt; New Mega checkpoint &amp;amp; multiple optimization on GLID3: less memory-footprint, use &lt;code&gt;ViT-L/14@336px&lt;/code&gt; from CLIP-as-service, &lt;code&gt;steps 100-&amp;gt;200&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🌟 &lt;strong&gt;2022/5/6&lt;/strong&gt; DALL·E Flow just got updated! &lt;a href=&#34;https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb&#34;&gt;Please &lt;em&gt;reopen&lt;/em&gt; the notebook in Google Colab!&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Revised the first step: 16 candidates are generated, 8 from DALL·E Mega, 8 from GLID3-XL; then ranked by CLIP-as-service.&lt;/li&gt; &#xA;   &lt;li&gt;Improved the flow efficiency: the overall speed, including diffusion and upscaling are much faster now!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Gallery&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20realistic%20photo%20of%20a%20muddy%20dog.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a realistic photo of a muddy dog&#34; title=&#34;a realistic photo of a muddy dog&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/A%20scientist%20comparing%20apples%20and%20oranges%2C%20by%20Norman%20Rockwell.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;A scientist comparing apples and oranges, by Norman Rockwell&#34; title=&#34;A scientist comparing apples and oranges, by Norman Rockwell&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/an%20oil%20painting%20portrait%20of%20the%20regal%20Burger%20King%20posing%20with%20a%20Whopper.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;an oil painting portrait of the regal Burger King posing with a Whopper&#34; title=&#34;an oil painting portrait of the regal Burger King posing with a Whopper&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/Eternal%20clock%20powered%20by%20a%20human%20cranium%2C%20artstation.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;Eternal clock powered by a human cranium, artstation&#34; title=&#34;Eternal clock powered by a human cranium, artstation&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/another%20planet%20amazing%20landscape.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;another planet amazing landscape&#34; title=&#34;another planet amazing landscape&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/The%20Decline%20and%20Fall%20of%20the%20Roman%20Empire%20board%20game%20kickstarter.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;The Decline and Fall of the Roman Empire board game kickstarter&#34; title=&#34;The Decline and Fall of the Roman Empire board game kickstarter&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/A%20raccoon%20astronaut%20with%20the%20cosmos%20reflecting%20on%20the%20glass%20of%20his%20helmet%20dreaming%20of%20the%20stars%2C%20digital%20art.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;A raccoon astronaut with the cosmos reflecting on the glass of his helmet dreaming of the stars, digital art&#34; title=&#34;A raccoon astronaut with the cosmos reflecting on the glass of his helmet dreaming of the stars, digital art&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/A%20photograph%20of%20an%20apple%20that%20is%20a%20disco%20ball%2C%2085%20mm%20lens%2C%20studio%20lighting.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;A photograph of an apple that is a disco ball, 85 mm lens, studio lighting&#34; title=&#34;A photograph of an apple that is a disco ball, 85 mm lens, studio lighting&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20cubism%20painting%20Donald%20trump%20happy%20cyberpunk.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a cubism painting Donald trump happy cyberpunk&#34; title=&#34;a cubism painting Donald trump happy cyberpunk&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/oil%20painting%20of%20a%20hamster%20drinking%20tea%20outside.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;oil painting of a hamster drinking tea outside&#34; title=&#34;oil painting of a hamster drinking tea outside&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/Colossus%20of%20Rhodes%20by%20Max%20Ernst.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;Colossus of Rhodes by Max Ernst&#34; title=&#34;Colossus of Rhodes by Max Ernst&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/landscape%20with%20great%20castle%20in%20middle%20of%20forest.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;landscape with great castle in middle of forest&#34; title=&#34;landscape with great castle in middle of forest&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/an%20medieval%20oil%20painting%20of%20Kanye%20west%20feels%20satisfied%20while%20playing%20chess%20in%20the%20style%20of%20Expressionism.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;an medieval oil painting of Kanye west feels satisfied while playing chess in the style of Expressionism&#34; title=&#34;an medieval oil painting of Kanye west feels satisfied while playing chess in the style of Expressionism&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/An%20oil%20pastel%20painting%20of%20an%20annoyed%20cat%20in%20a%20spaceship.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;An oil pastel painting of an annoyed cat in a spaceship&#34; title=&#34;An oil pastel painting of an annoyed cat in a spaceship&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/dinosaurs%20at%20the%20brink%20of%20a%20nuclear%20disaster.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;dinosaurs at the brink of a nuclear disaster&#34; title=&#34;dinosaurs at the brink of a nuclear disaster&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/fantasy%20landscape%20with%20medieval%20city.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;fantasy landscape with medieval city&#34; title=&#34;fantasy landscape with medieval city&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/GPU%20chip%20in%20the%20form%20of%20an%20avocado%2C%20digital%20art.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;GPU chip in the form of an avocado, digital art&#34; title=&#34;GPU chip in the form of an avocado, digital art&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20giant%20rubber%20duck%20in%20the%20ocean.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a giant rubber duck in the ocean&#34; title=&#34;a giant rubber duck in the ocean&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/Paddington%20bear%20as%20austrian%20emperor%20in%20antique%20black%20%26%20white%20photography.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;Paddington bear as austrian emperor in antique black &amp;amp; white photography&#34; title=&#34;Paddington bear as austrian emperor in antique black &amp;amp; white photography&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20rainy%20night%20with%20a%20superhero%20perched%20above%20a%20city%2C%20in%20the%20style%20of%20a%20comic%20book.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a rainy night with a superhero perched above a city, in the style of a comic book&#34; title=&#34;a rainy night with a superhero perched above a city, in the style of a comic book&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/A%20synthwave%20style%20sunset%20above%20the%20reflecting%20water%20of%20the%20sea%2C%20digital%20art.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;A synthwave style sunset above the reflecting water of the sea, digital art&#34; title=&#34;A synthwave style sunset above the reflecting water of the sea, digital art&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/an%20oil%20painting%20of%20ocean%20beach%20front%20in%20the%20style%20of%20Titian.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;an oil painting of ocean beach front in the style of Titian&#34; title=&#34;an oil painting of ocean beach front in the style of Titian&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/an%20oil%20painting%20of%20Klingon%20general%20in%20the%20style%20of%20Rubens.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;an oil painting of Klingon general in the style of Rubens&#34; title=&#34;an oil painting of Klingon general in the style of Rubens&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/city%2C%20top%20view%2C%20cyberpunk%2C%20digital%20realistic%20art.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;city, top view, cyberpunk, digital realistic art&#34; title=&#34;city, top view, cyberpunk, digital realistic art&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/an%20oil%20painting%20of%20a%20medieval%20cyborg%20automaton%20made%20of%20magic%20parts%20and%20old%20steampunk%20mechanics.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;an oil painting of a medieval cyborg automaton made of magic parts and old steampunk mechanics&#34; title=&#34;an oil painting of a medieval cyborg automaton made of magic parts and old steampunk mechanics&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20watercolour%20painting%20of%20a%20top%20view%20of%20a%20pirate%20ship%20sailing%20on%20the%20clouds.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a watercolour painting of a top view of a pirate ship sailing on the clouds&#34; title=&#34;a watercolour painting of a top view of a pirate ship sailing on the clouds&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20knight%20made%20of%20beautiful%20flowers%20and%20fruits%20by%20Rachel%20ruysch%20in%20the%20style%20of%20Syd%20brak.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a knight made of beautiful flowers and fruits by Rachel ruysch in the style of Syd brak&#34; title=&#34;a knight made of beautiful flowers and fruits by Rachel ruysch in the style of Syd brak&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%203D%20render%20of%20a%20rainbow%20colored%20hot%20air%20balloon%20flying%20above%20a%20reflective%20lake.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a 3D render of a rainbow colored hot air balloon flying above a reflective lake&#34; title=&#34;a 3D render of a rainbow colored hot air balloon flying above a reflective lake&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20teddy%20bear%20on%20a%20skateboard%20in%20Times%20Square%20.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a teddy bear on a skateboard in Times Square &#34; title=&#34;a teddy bear on a skateboard in Times Square &#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/cozy%20bedroom%20at%20night.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;cozy bedroom at night&#34; title=&#34;cozy bedroom at night&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/an%20oil%20painting%20of%20monkey%20using%20computer.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;an oil painting of monkey using computer&#34; title=&#34;an oil painting of monkey using computer&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/the%20diagram%20of%20a%20search%20machine%20invented%20by%20Leonardo%20da%20Vinci.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;the diagram of a search machine invented by Leonardo da Vinci&#34; title=&#34;the diagram of a search machine invented by Leonardo da Vinci&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/A%20stained%20glass%20window%20of%20toucans%20in%20outer%20space.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;A stained glass window of toucans in outer space&#34; title=&#34;A stained glass window of toucans in outer space&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20campfire%20in%20the%20woods%20at%20night%20with%20the%20milky-way%20galaxy%20in%20the%20sky.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a campfire in the woods at night with the milky-way galaxy in the sky&#34; title=&#34;a campfire in the woods at night with the milky-way galaxy in the sky&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/Bionic%20killer%20robot%20made%20of%20AI%20scarab%20beetles.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;Bionic killer robot made of AI scarab beetles&#34; title=&#34;Bionic killer robot made of AI scarab beetles&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/The%20Hanging%20Gardens%20of%20Babylon%20in%20the%20middle%20of%20a%20city%2C%20in%20the%20style%20of%20Dal%C3%AD.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;The Hanging Gardens of Babylon in the middle of a city, in the style of Dalí&#34; title=&#34;The Hanging Gardens of Babylon in the middle of a city, in the style of Dalí&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/painting%20oil%20of%20Izhevsk.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;painting oil of Izhevsk&#34; title=&#34;painting oil of Izhevsk&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20hyper%20realistic%20photo%20of%20a%20marshmallow%20office%20chair.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a hyper realistic photo of a marshmallow office chair&#34; title=&#34;a hyper realistic photo of a marshmallow office chair&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/fantasy%20landscape%20with%20city.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;fantasy landscape with city&#34; title=&#34;fantasy landscape with city&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/ocean%20beach%20front%20view%20in%20Van%20Gogh%20style.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;ocean beach front view in Van Gogh style&#34; title=&#34;ocean beach front view in Van Gogh style&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/An%20oil%20painting%20of%20a%20family%20reunited%20inside%20of%20an%20airport%2C%20digital%20art.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;An oil painting of a family reunited inside of an airport, digital art&#34; title=&#34;An oil painting of a family reunited inside of an airport, digital art&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/antique%20photo%20of%20a%20knight%20riding%20a%20T-Rex.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;antique photo of a knight riding a T-Rex&#34; title=&#34;antique photo of a knight riding a T-Rex&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20top%20view%20of%20a%20pirate%20ship%20sailing%20on%20the%20clouds.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a top view of a pirate ship sailing on the clouds&#34; title=&#34;a top view of a pirate ship sailing on the clouds&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/an%20oil%20painting%20of%20a%20humanoid%20robot%20playing%20chess%20in%20the%20style%20of%20Matisse.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;an oil painting of a humanoid robot playing chess in the style of Matisse&#34; title=&#34;an oil painting of a humanoid robot playing chess in the style of Matisse&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20cubism%20painting%20of%20a%20cat%20dressed%20as%20French%20emperor%20Napoleon.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a cubism painting of a cat dressed as French emperor Napoleon&#34; title=&#34;a cubism painting of a cat dressed as French emperor Napoleon&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/a%20husky%20dog%20wearing%20a%20hat%20with%20sunglasses.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;a husky dog wearing a hat with sunglasses&#34; title=&#34;a husky dog wearing a hat with sunglasses&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/A%20mystical%20castle%20appears%20between%20the%20clouds%20in%20the%20style%20of%20Vincent%20di%20Fate.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;A mystical castle appears between the clouds in the style of Vincent di Fate&#34; title=&#34;A mystical castle appears between the clouds in the style of Vincent di Fate&#34;&gt;&lt;img src=&#34;https://github.com/hanxiao/dalle/raw/gallery/.github/gallery/golden%20gucci%20airpods%20realistic%20photo.png?raw=true&#34; width=&#34;32%&#34; alt=&#34;golden gucci airpods realistic photo&#34; title=&#34;golden gucci airpods realistic photo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Client&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Open-in%20Colab-orange?logo=google-colab&amp;amp;style=flat-square&#34; alt=&#34;Open in Google Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Using client is super easy. The following steps are best run in &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/dalle-flow/main/client.ipynb&#34;&gt;Jupyter notebook&lt;/a&gt; or &lt;a href=&#34;https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will need to install &lt;a href=&#34;https://github.com/jina-ai/docarray&#34;&gt;DocArray&lt;/a&gt; and &lt;a href=&#34;https://github.com/jina-ai/jina&#34;&gt;Jina&lt;/a&gt; first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;docarray[common]&amp;gt;=0.13.5&#34; jina&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have provided a demo server for you to play:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;⚠️ &lt;strong&gt;Due to the massive requests, our server may be delay in response. Yet we are &lt;em&gt;very&lt;/em&gt; confident on keeping the uptime high.&lt;/strong&gt; You can also deploy your own server by &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/dalle-flow/main/#server&#34;&gt;following the instruction here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;server_url = &#39;grpc://dalle-flow.jina.ai:51005&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 1: Generate via DALL·E Mega&lt;/h3&gt; &#xA;&lt;p&gt;Now let&#39;s define the prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#39;an oil painting of a humanoid robot playing chess in the style of Matisse&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s submit it to the server and visualize the results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import Document&#xA;&#xA;da = Document(text=prompt).post(server_url, parameters={&#39;num_images&#39;: 8}).matches&#xA;&#xA;da.plot_image_sprites(fig_size=(10,10), show_index=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we generate 16 candidates, 8 from DALLE-mega and 8 from GLID3 XL, this is as defined in &lt;code&gt;num_images&lt;/code&gt;, which takes about ~2 minutes. You can use a smaller value if it is too long for you.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/client-dalle.png?raw=true&#34; width=&#34;70%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Step 2: Select and refinement via GLID3 XL&lt;/h3&gt; &#xA;&lt;p&gt;The 16 candidates are sorted by &lt;a href=&#34;https://github.com/jina-ai/clip-as-service&#34;&gt;CLIP-as-service&lt;/a&gt;, with index-&lt;code&gt;0&lt;/code&gt; as the best candidate judged by CLIP. Of course, you may think differently. Notice the number in the top-left corner? Select the one you like the most and get a better view:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fav_id = 3&#xA;fav = da[fav_id]&#xA;fav.display()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/client-select1.png?raw=true&#34; width=&#34;30%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Now let&#39;s submit the selected candidates to the server for diffusion.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;diffused = fav.post(f&#39;{server_url}&#39;, parameters={&#39;skip_rate&#39;: 0.5, &#39;num_images&#39;: 36}, target_executor=&#39;diffusion&#39;).matches&#xA;&#xA;diffused.plot_image_sprites(fig_size=(10,10), show_index=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will give 36 images based on the selected image. You may allow the model to improvise more by giving &lt;code&gt;skip_rate&lt;/code&gt; a near-zero value, or a near-one value to force its closeness to the given image. The whole procedure takes about ~2 minutes.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/client-glid.png?raw=true&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Select and upscale via SwinIR&lt;/h3&gt; &#xA;&lt;p&gt;Select the image you like the most, and give it a closer look:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dfav_id = 34&#xA;fav = diffused[dfav_id]&#xA;fav.display()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/client-select2.png?raw=true&#34; width=&#34;30%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Finally, submit to the server for the last step: upscaling to 1024 x 1024px.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fav = fav.post(f&#39;{server_url}/upscale&#39;)&#xA;fav.display()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! It is &lt;em&gt;the one&lt;/em&gt;. If not satisfied, please repeat the procedure.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/client-select3.png?raw=true&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Btw, DocArray is a powerful and easy-to-use data structure for unstructured data. It is super productive for data scientists who work in cross-/multi-modal domain. To learn more about DocArray, &lt;a href=&#34;https://docs.jina.ai&#34;&gt;please check out the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Server&lt;/h2&gt; &#xA;&lt;p&gt;You can host your own server by following the instruction below.&lt;/p&gt; &#xA;&lt;h3&gt;Hardware requirements&lt;/h3&gt; &#xA;&lt;p&gt;DALL·E Flow needs one GPU with 21GB memory at its peak. All services are squeezed into this one GPU.&lt;/p&gt; &#xA;&lt;p&gt;It requires at least 40GB free space on the hard drive, mostly for downloading pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;High-speed internet is required. Slow/unstable internet may throw frustrating timeout when downloading models.&lt;/p&gt; &#xA;&lt;p&gt;CPU-only environment is not tested and likely won&#39;t work. Google Colab is likely throwing OOM hence also won&#39;t work.&lt;/p&gt; &#xA;&lt;h3&gt;Server architecture&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/flow.svg?raw=true&#34; width=&#34;70%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If you have installed Jina, the above flowchart can be generated via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# pip install jina&#xA;jina export flowchart flow.yml flow.svg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run in Docker&lt;/h3&gt; &#xA;&lt;p&gt;We have provided &lt;a href=&#34;https://github.com/jina-ai/dalle-flow/raw/main/Dockerfile&#34;&gt;a Dockerfile&lt;/a&gt; which allows you to run a server out of the box.&lt;/p&gt; &#xA;&lt;p&gt;Our Dockerfile is using CUDA 11.6 as the base image, you may want to adjust it according to your system.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/jina-ai/dalle-flow.git&#xA;cd dalle-flow&#xA;&#xA;docker build --build-arg GROUP_ID=$(id -g ${USER}) --build-arg USER_ID=$(id -u ${USER}) -t jinaai/dalle-flow .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The building will take 10 minutes with average internet speed, which results in a 10GB Docker image.&lt;/p&gt; &#xA;&lt;p&gt;To run it, simply do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 51005:51005 -v $HOME/.cache:/home/dalle/.cache --gpus all jinaai/dalle-flow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The first run will take ~10 minutes with average internet speed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-v $HOME/.cache:/root/.cache&lt;/code&gt; avoids repeated model downloading on every docker run.&lt;/li&gt; &#xA; &lt;li&gt;The first part of &lt;code&gt;-p 51005:51005&lt;/code&gt; is your host public port. Make sure people can access this port if you are serving publicly. The second par of it is &lt;a href=&#34;https://github.com/jina-ai/dalle-flow/raw/e7e313522608668daeec1b7cd84afe56e5b19f1e/flow.yml#L4&#34;&gt;the port defined in flow.yml&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should see the screen like following once running:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/docker-run.png?raw=true&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Note that unlike running natively, running inside Docker may give less vivid progressbar, color logs, and prints. This is due to the limitations of the terminal in a Docker container. It does not affect the actual usage.&lt;/p&gt; &#xA;&lt;h3&gt;Run natively&lt;/h3&gt; &#xA;&lt;p&gt;Running natively requires some manual steps, but it is often easier to debug.&lt;/p&gt; &#xA;&lt;h4&gt;Clone repos&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir dalle &amp;amp;&amp;amp; cd dalle&#xA;git clone https://github.com/jina-ai/dalle-flow.git&#xA;git clone https://github.com/JingyunLiang/SwinIR.git&#xA;git clone https://github.com/CompVis/latent-diffusion.git&#xA;git clone https://github.com/hanxiao/glid-3-xl.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should have the following folder structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;dalle/&#xA; |&#xA; |-- dalle-flow/&#xA; |-- SwinIR/&#xA; |-- glid-3-xl/&#xA; |-- latent-diffusion/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install auxiliary repos&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd latent-diffusion &amp;amp;&amp;amp; pip install -e . &amp;amp;&amp;amp; cd -&#xA;cd glid-3-xl &amp;amp;&amp;amp; pip install -e . &amp;amp;&amp;amp; cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are couple models we need to download for GLID-3-XL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd glid-3-xl&#xA;wget https://dall-3.com/models/glid-3-xl/bert.pt&#xA;wget https://dall-3.com/models/glid-3-xl/kl-f8.pt&#xA;wget https://dall-3.com/models/glid-3-xl/finetune.pt&#xA;cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install flow&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd dalle-flow&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Start the server&lt;/h3&gt; &#xA;&lt;p&gt;Now you are under &lt;code&gt;dalle-flow/&lt;/code&gt;, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jina flow --uses flow.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see this screen immediately:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/server-onstart.png?raw=true&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;On the first start it will take ~8 minutes for downloading the DALL·E mega model and other necessary models. The proceeding runs should only take ~1 minute to reach the success message.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/server-wait.png?raw=true&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;When everything is ready, you will see:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/dalle-flow/raw/main/.github/server-success.png?raw=true&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Congrats! Now you should be able to &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/dalle-flow/main/#client&#34;&gt;run the client&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can modify and extend the server flow as you like, e.g. changing the model, adding persistence, or even auto-posting to Instagram/OpenSea. With Jina and DocArray, you can easily make DALL·E Flow &lt;a href=&#34;https://github.com/jina-ai/jina&#34;&gt;cloud-native and ready for production&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- start support-pitch --&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To extend DALL·E Flow you will need to get familiar with &lt;a href=&#34;https://github.com/jina-ai/jina&#34;&gt;Jina&lt;/a&gt; and &lt;a href=&#34;https://github.com/jina-ai/docarray&#34;&gt;DocArray&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://slack.jina.ai&#34;&gt;Slack community&lt;/a&gt; and chat with other community members about ideas.&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne&#34;&gt;Engineering All Hands&lt;/a&gt; meet-up to discuss your use case and learn Jina&#39;s new features. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;When?&lt;/strong&gt; The second Tuesday of every month&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Where?&lt;/strong&gt; Zoom (&lt;a href=&#34;https://calendar.google.com/calendar/embed?src=c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com&amp;amp;ctz=Europe%2FBerlin&#34;&gt;see our public events calendar&lt;/a&gt;/&lt;a href=&#34;https://calendar.google.com/calendar/ical/c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com/public/basic.ics&#34;&gt;.ical&lt;/a&gt;) and &lt;a href=&#34;https://youtube.com/c/jina-ai&#34;&gt;live stream on YouTube&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Subscribe to the latest video tutorials on our &lt;a href=&#34;https://youtube.com/c/jina-ai&#34;&gt;YouTube channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Join Us&lt;/h2&gt; &#xA;&lt;p&gt;DALL·E Flow is backed by &lt;a href=&#34;https://jina.ai&#34;&gt;Jina AI&lt;/a&gt; and licensed under &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/dalle-flow/main/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;. &lt;a href=&#34;https://jobs.jina.ai&#34;&gt;We are actively hiring&lt;/a&gt; AI engineers, solution engineers to build the next neural search ecosystem in open-source.&lt;/p&gt; &#xA;&lt;!-- end support-pitch --&gt;</summary>
  </entry>
  <entry>
    <title>doccano/doccano</title>
    <updated>2022-06-11T01:37:51Z</updated>
    <id>tag:github.com,2022-06-11:/doccano/doccano</id>
    <link href="https://github.com/doccano/doccano" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open source annotation tool for machine learning practitioners.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/doccano/doccano/master/docs/images/logo/doccano.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;doccano&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.codacy.com/gh/doccano/doccano/dashboard?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=doccano/doccano&amp;amp;utm_campaign=Badge_Grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/35ac8625a2bc4eddbff23dbc61bc6abb&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/doccano/doccano/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/doccano/doccano/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;doccano CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;doccano is an open source text annotation tool for humans. It provides annotation features for text classification, sequence labeling and sequence to sequence tasks. So, you can create labeled data for sentiment analysis, named entity recognition, text summarization and so on. Just create a project, upload data and start annotating. You can build a dataset in hours.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;You can try the &lt;a href=&#34;http://doccano.herokuapp.com&#34;&gt;annotation demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/doccano/doccano/master/docs/images/demo/demo.gif&#34; alt=&#34;Demo image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Read the documentation at the &lt;a href=&#34;https://doccano.github.io/doccano/&#34;&gt;https://doccano.github.io/doccano/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Collaborative annotation&lt;/li&gt; &#xA; &lt;li&gt;Multi-language support&lt;/li&gt; &#xA; &lt;li&gt;Mobile support&lt;/li&gt; &#xA; &lt;li&gt;Emoji &lt;span&gt;😄&lt;/span&gt; support&lt;/li&gt; &#xA; &lt;li&gt;Dark theme&lt;/li&gt; &#xA; &lt;li&gt;RESTful API&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Three options to run doccano:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pip (Python 3.8+)&lt;/li&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA; &lt;li&gt;Docker Compose&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;pip&lt;/h3&gt; &#xA;&lt;p&gt;To install doccano, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install doccano&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, SQLite 3 is used for the default database. If you want to use PostgreSQL, install the additional dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#39;doccano[postgresql]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and set &lt;code&gt;DATABASE_URL&lt;/code&gt; environment variable according to your PostgreSQL credentials:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATABASE_URL=&#34;postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}?sslmode=disable&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installation, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Initialize database.&#xA;doccano init&#xA;# Create a super user.&#xA;doccano createuser --username admin --password pass&#xA;# Start a web server.&#xA;doccano webserver --port 8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In another terminal, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Start the task queue to handle file upload/download.&#xA;doccano task&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;http://127.0.0.1:8000/&#34;&gt;http://127.0.0.1:8000/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;As a one-time setup, create a Docker container as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull doccano/doccano&#xA;docker container create --name doccano \&#xA;  -e &#34;ADMIN_USERNAME=admin&#34; \&#xA;  -e &#34;ADMIN_EMAIL=admin@example.com&#34; \&#xA;  -e &#34;ADMIN_PASSWORD=password&#34; \&#xA;  -v doccano-db:/data \&#xA;  -p 8000:8000 doccano/doccano&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, start doccano by running the container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker container start doccano&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;http://127.0.0.1:8000/&#34;&gt;http://127.0.0.1:8000/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To stop the container, run &lt;code&gt;docker container stop doccano -t 5&lt;/code&gt;. All data created in the container will persist across restarts.&lt;/p&gt; &#xA;&lt;h3&gt;Docker Compose&lt;/h3&gt; &#xA;&lt;p&gt;You need to install Git and to clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/doccano/doccano.git&#xA;cd doccano&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note for Windows developers:&lt;/em&gt; Be sure to configure git to correctly handle line endings or you may encounter &lt;code&gt;status code 127&lt;/code&gt; errors while running the services in future steps. Running with the git config options below will ensure your git directory correctly handles line endings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/doccano/doccano.git --config core.autocrlf=input&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, create an &lt;code&gt;.env&lt;/code&gt; file with variables in the following format (see &lt;a href=&#34;https://github.com/doccano/doccano/raw/master/docker/.env.example&#34;&gt;./docker/.env.example&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;# platform settings&#xA;ADMIN_USERNAME=admin&#xA;ADMIN_PASSWORD=password&#xA;ADMIN_EMAIL=admin@example.com&#xA;&#xA;# rabbit mq settings&#xA;RABBITMQ_DEFAULT_USER=doccano&#xA;RABBITMQ_DEFAULT_PASS=doccano&#xA;&#xA;# database settings&#xA;POSTGRES_USER=doccano&#xA;POSTGRES_PASSWORD=doccano&#xA;POSTGRES_DB=doccano&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the following command, access &lt;a href=&#34;http://127.0.0.1/&#34;&gt;http://127.0.0.1/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose -f docker/docker-compose.prod.yml --env-file .env up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;One-click Deployment&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Service&lt;/th&gt; &#xA;   &lt;th&gt;Button&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AWS[^1]&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=doccano&amp;amp;templateURL=https://doccano.s3.amazonaws.com/public/cloudformation/template.aws.yaml&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/buildkite/cloudformation-launch-stack-button-svg/master/launch-stack.svg?sanitize=true&#34; alt=&#34;AWS CloudFormation Launch Stack SVG Button&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Heroku&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dashboard.heroku.com/new?template=https%3A%2F%2Fgithub.com%2Fdoccano%2Fdoccano&#34;&gt;&lt;img src=&#34;https://www.herokucdn.com/deploy/button.svg?sanitize=true&#34; alt=&#34;Deploy&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- | GCP[^2] | [![GCP Cloud Run PNG Button](https://storage.googleapis.com/gweb-cloudblog-publish/images/run_on_google_cloud.max-300x300.png)](https://console.cloud.google.com/cloudshell/editor?shellonly=true&amp;cloudshell_image=gcr.io/cloudrun/button&amp;cloudshell_git_repo=https://github.com/doccano/doccano.git&amp;cloudshell_git_branch=CloudRunButton)  | --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[^1]: (1) EC2 KeyPair cannot be created automatically, so make sure you have an existing EC2 KeyPair in one region. Or &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair&#34;&gt;create one yourself&lt;/a&gt;. (2) If you want to access doccano via HTTPS in AWS, here is an &lt;a href=&#34;https://github.com/doccano/doccano/wiki/HTTPS-setting-for-doccano-in-AWS&#34;&gt;instruction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- &gt; [^2]: Although this is a very cheap option, it is only suitable for very small teams (up to 80 concurrent requests). Read more on [Cloud Run docs](https://cloud.google.com/run/docs/concepts). --&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doccano.github.io/doccano/faq/#how-to-create-a-user&#34;&gt;How to create a user&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doccano.github.io/doccano/faq/#how-to-add-a-user-to-your-project&#34;&gt;How to add a user to your project&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doccano.github.io/doccano/faq/#how-to-change-the-password&#34;&gt;How to change the password&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://doccano.github.io/doccano/&#34;&gt;documentation&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;As with any software, doccano is under continuous development. If you have requests for features, please file an issue describing your request. Also, if you want to see work towards a specific feature, feel free to contribute by working towards it. The standard procedure is to fork the repository, add a feature, fix a bug, then file a pull request that your changes are to be merged into the main repository and included in the next release.&lt;/p&gt; &#xA;&lt;p&gt;Here are some tips might be helpful. &lt;a href=&#34;https://github.com/doccano/doccano/wiki/How-to-Contribute-to-Doccano-Project&#34;&gt;How to Contribute to Doccano Project&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@misc{doccano,&#xA;  title={{doccano}: Text Annotation Tool for Human},&#xA;  url={https://github.com/doccano/doccano},&#xA;  note={Software available from https://github.com/doccano/doccano},&#xA;  author={&#xA;    Hiroki Nakayama and&#xA;    Takahiro Kubo and&#xA;    Junya Kamura and&#xA;    Yasufumi Taniguchi and&#xA;    Xu Liang},&#xA;  year={2018},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For help and feedback, please feel free to contact &lt;a href=&#34;https://github.com/Hironsan&#34;&gt;the author&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>