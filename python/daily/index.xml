<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-13T01:42:03Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>soulteary/docker-prompt-generator</title>
    <updated>2023-04-13T01:42:03Z</updated>
    <id>tag:github.com,2023-04-13:/soulteary/docker-prompt-generator</id>
    <link href="https://github.com/soulteary/docker-prompt-generator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Using a Model to generate prompts for Model applications. / ‰ΩøÁî®Ê®°ÂûãÊù•ÁîüÊàê‰ΩúÂõæÂííËØ≠ÁöÑÂÅ∑ÊáíÂ∑•ÂÖ∑ÔºåÊîØÊåÅ MidJourney„ÄÅStable Diffusion Á≠â„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Docker Prompt Generator&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/soulteary/docker-prompt-generator/main/.github/prompt.png&#34; width=&#34;300px&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/soulteary/docker-prompt-generator/main/README-CN.md&#34;&gt;‰∏≠ÊñáÊñáÊ°£&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Using a Model to generate prompts for Model applications (MidJourney, Stable Diffusion, etc...)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Preview&lt;/h2&gt; &#xA;&lt;p&gt;Like the official Mid Journey function, it supports parsing prompts from images and secondary extensions based on prompts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/soulteary/docker-prompt-generator/main/.github/preview.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Support writing prompts directly in Chinese, and then get prompts text that can be used for better effect generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/soulteary/docker-prompt-generator/main/.github/preview-translate.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;In the past &lt;a href=&#34;https://soulteary.com/tags/python.html&#34;&gt;several articles&lt;/a&gt;, I mentioned my personal habits and recommended development environment, which is based on Docker and Nvidia&#39;s official base container for deep learning environments. I won&#39;t go into details about that here, but if you&#39;re interested, you can check out articles like this one on &lt;a href=&#34;https://soulteary.com/2023/03/22/docker-based-deep-learning-environment-getting-started.html&#34;&gt;getting started with Docker-based deep learning environments&lt;/a&gt;. I believe that long-time readers should already be quite familiar with it.&lt;/p&gt; &#xA;&lt;p&gt;Of course, since this article includes parts that can be played with just a CPU, you can also refer to &lt;a href=&#34;https://soulteary.com/2022/12/10/play-the-stable-diffusion-model-on-macbook-devices-with-m1-and-m2-chips.html&#34;&gt;Playing with the Stable Diffusion Model on MacBook Devices with M1 and M2 chips&lt;/a&gt; from a few months ago to configure your environment.&lt;/p&gt; &#xA;&lt;p&gt;Once you have prepared the Docker environment configuration, we can continue to have fun.&lt;/p&gt; &#xA;&lt;p&gt;Find a suitable directory and use &lt;code&gt;git clone&lt;/code&gt; or download the Zip archive to get the &#34;Docker Prompt Generator&#34; project code onto your local machine.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/soulteary/docker-prompt-generator.git&#xA;# or&#xA;curl -sL -o docker-prompt-generator.zip https://github.com/soulteary/docker-prompt-generator/archive/refs/heads/main.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, enter the project directory and use Nvidia&#39;s official PyTorch Docker base image to build the basic environment. Compared to pulling a pre-made image directly from DockerHub, building it yourself will save a lot of time.&lt;/p&gt; &#xA;&lt;p&gt;Execute the following commands in the project directory to complete the model application build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Build the base image&#xA;docker build -t soulteary/prompt-generator:base . -f docker/Dockerfile.base&#xA;&#xA;# Build the CPU application&#xA;docker build -t soulteary/prompt-generator:cpu . -f docker/Dockerfile.cpu&#xA;&#xA;# Build the GPU application&#xA;docker build -t soulteary/prompt-generator:gpu . -f docker/Dockerfile.gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, depending on your hardware environment, selectively execute the following commands to start a model application with a Web UI interface.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run the CPU image&#xA;docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --rm -it -p 7860:7860 soulteary/prompt-generator:cpu&#xA;&#xA;# Run the GPU image&#xA;docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --rm -it -p 7860:7860 soulteary/prompt-generator:gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Enter the IP address of the host running the container in your browser, and you can start using the tool.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Prompt Model: &lt;a href=&#34;https://huggingface.co/succinctly/text2image-prompt-generator&#34;&gt;succinctly/text2image-prompt-generator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Translator Model: &lt;a href=&#34;https://huggingface.co/Helsinki-NLP/opus-mt-zh-en&#34;&gt;Helsinki-NLP/opus-mt-zh-en&lt;/a&gt; / &lt;a href=&#34;https://github.com/Helsinki-NLP/OPUS-MT-train&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CLIP Model: &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K&#34;&gt;laion/CLIP-ViT-H-14-laion2B-s32B-b79K&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Datasets: &lt;a href=&#34;https://www.kaggle.com/datasets/succinctlyai/midjourney-texttoimage&#34;&gt;succinctlyai/midjourney-texttoimage&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ravenscroftj/turbopilot</title>
    <updated>2023-04-13T01:42:03Z</updated>
    <id>tag:github.com,2023-04-13:/ravenscroftj/turbopilot</id>
    <link href="https://github.com/ravenscroftj/turbopilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Turbopilot is an open source large-language-model based code completion engine that runs locally on CPU&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TurboPilot üöÄ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ravenscroftj/turbopilot/actions/workflows/cmake.yml&#34;&gt;&lt;img src=&#34;https://github.com/ravenscroftj/turbopilot/actions/workflows/cmake.yml/badge.svg?sanitize=true&#34; alt=&#34;CMake&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fosstodon.org/@jamesravey&#34;&gt;&lt;img src=&#34;https://img.shields.io/mastodon/follow/000117012?domain=https%3A%2F%2Ffosstodon.org%2F&amp;amp;style=social&#34; alt=&#34;Mastodon Follow&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/ravenscroftj/turbopilot&#34; alt=&#34;BSD Licensed&#34;&gt; &lt;img src=&#34;https://img.shields.io/endpoint?url=https://wakapi.nopro.be/api/compat/shields/v1/jamesravey/all_time/project%3Aturbopilot&#34; alt=&#34;Time Spent&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;TurboPilot is a self-hosted &lt;a href=&#34;https://github.com/features/copilot&#34;&gt;copilot&lt;/a&gt; clone which uses the library behind &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; to run the &lt;a href=&#34;https://github.com/salesforce/CodeGen&#34;&gt;6 Billion Parameter Salesforce Codegen model&lt;/a&gt; in 4GiB of RAM. It is heavily based and inspired by on the &lt;a href=&#34;https://github.com/fauxpilot/fauxpilot&#34;&gt;fauxpilot&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NB: This is a proof of concept right now rather than a stable tool. Autocompletion is quite slow in this version of the project. Feel free to play with it, but your mileage may vary.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ravenscroftj/turbopilot/main/assets/screenrecording.gif&#34; alt=&#34;a screen recording of turbopilot running through fauxpilot plugin&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;PRs to this project and the corresponding &lt;a href=&#34;https://github.com/ravenscroftj/ggml&#34;&gt;GGML fork&lt;/a&gt; are very welcome.&lt;/p&gt; &#xA;&lt;p&gt;Make a fork, make your changes and then open a &lt;a href=&#34;https://github.com/ravenscroftj/turbopilot/pulls&#34;&gt;PR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üëã Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to try the project out is to grab the pre-processed models and then run the server in docker.&lt;/p&gt; &#xA;&lt;h3&gt;Getting The Models&lt;/h3&gt; &#xA;&lt;p&gt;You have 2 options for getting the model&lt;/p&gt; &#xA;&lt;h4&gt;Option A: Direct Download - Easy, Quickstart&lt;/h4&gt; &#xA;&lt;p&gt;You can download the pre-converted, pre-quantized models from &lt;a href=&#34;https://drive.google.com/drive/folders/1wFy1Y0pqoK23ZeMWWCp8evxWOJQVdaGh?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;. I&#39;ve made the &lt;code&gt;multi&lt;/code&gt; flavour models with 2B and 6B parameters available - these models are pre-trained on &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt;, &lt;code&gt;Go&lt;/code&gt;, &lt;code&gt;Java&lt;/code&gt;, &lt;code&gt;JavaScript&lt;/code&gt;, and &lt;code&gt;Python&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Option B: Convert The Models Yourself - Hard, More Flexible&lt;/h4&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://github.com/ravenscroftj/turbopilot/wiki/Converting-and-Quantizing-The-Models&#34;&gt;this guide&lt;/a&gt; if you want to experiment with quantizing the models yourself.&lt;/p&gt; &#xA;&lt;h3&gt;‚öôÔ∏è Running TurboPilot Server&lt;/h3&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://github.com/ravenscroftj/turbopilot/releases&#34;&gt;latest binary&lt;/a&gt; and extract it to the root project folder. If a binary is not provided for your OS or you&#39;d prefer to build it yourself follow the &lt;a href=&#34;https://raw.githubusercontent.com/ravenscroftj/turbopilot/main/BUILD.md&#34;&gt;build instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./codegen-serve -m ./models/codegen-6B-multi-ggml-4bit-quant.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The application should start a server on port &lt;code&gt;18080&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have a multi-core system you can control how many CPUs are used with the &lt;code&gt;-t&lt;/code&gt; option - for example, on my AMD Ryzen 5000 which has 6 cores/12 threads I use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./codegen-serve -t 6 -m ./models/codegen-6B-multi-ggml-4bit-quant.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üì¶ Running From Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can also run Turbopilot from the pre-built docker image supplied &lt;a href=&#34;https://github.com/users/ravenscroftj/packages/container/package/turbopilot%2Fturbopilot&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You will still need to download the models separately, then you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it \&#xA;  -v ./models:/models \&#xA;  -e THREADS=6 \&#xA;  -e MODEL=&#34;/models/codegen-2B-multi-ggml-4bit-quant.bin&#34; \&#xA;  -p 18080:18080 \&#xA;  ghcr.io/ravenscroftj/turbopilot/turbopilot:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üåê Using the API&lt;/h3&gt; &#xA;&lt;h4&gt;Using the API with FauxPilot Plugin&lt;/h4&gt; &#xA;&lt;p&gt;To use the API from VSCode, I recommend the &lt;a href=&#34;https://github.com/Venthe/vscode-fauxpilot&#34;&gt;vscode-fauxpilot&lt;/a&gt; plugin. Once you install it, you will need to change a few settings in your settings.json file.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open settings (CTRL/CMD + SHIFT + P) and select &lt;code&gt;Preferences: Open User Settings (JSON)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add the following values:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    ... // other settings&#xA;&#xA;    &#34;fauxpilot.enabled&#34;: true,&#xA;    &#34;fauxpilot.server&#34;: &#34;http://localhost:18080/v1/engines&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can enable fauxpilot with &lt;code&gt;CTRL + SHIFT + P&lt;/code&gt; and select &lt;code&gt;Enable Fauxpilot&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The plugin will send API calls to the running &lt;code&gt;codegen-serve&lt;/code&gt; process when you make a keystroke. It will then wait for each request to complete before sending further requests.&lt;/p&gt; &#xA;&lt;h4&gt;Calling the API Directly&lt;/h4&gt; &#xA;&lt;p&gt;You can make requests to &lt;code&gt;http://localhost:18080/v1/engines/codegen/completions&lt;/code&gt; which will behave just like the same Copilot endpoint.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl --request POST \&#xA;  --url http://localhost:18080/v1/engines/codegen/completions \&#xA;  --header &#39;Content-Type: application/json&#39; \&#xA;  --data &#39;{&#xA; &#34;model&#34;: &#34;codegen&#34;,&#xA; &#34;prompt&#34;: &#34;def main():&#34;,&#xA; &#34;max_tokens&#34;: 100&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Should get you something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA; &#34;choices&#34;: [&#xA;  {&#xA;   &#34;logprobs&#34;: null,&#xA;   &#34;index&#34;: 0,&#xA;   &#34;finish_reason&#34;: &#34;length&#34;,&#xA;   &#34;text&#34;: &#34;\n  \&#34;\&#34;\&#34;Main entry point for this script.\&#34;\&#34;\&#34;\n  logging.getLogger().setLevel(logging.INFO)\n  logging.basicConfig(format=(&#39;%(levelname)s: %(message)s&#39;))\n\n  parser = argparse.ArgumentParser(\n      description=__doc__,\n      formatter_class=argparse.RawDescriptionHelpFormatter,\n      epilog=__doc__)\n  &#34;&#xA;  }&#xA; ],&#xA; &#34;created&#34;: 1681113078,&#xA; &#34;usage&#34;: {&#xA;  &#34;total_tokens&#34;: 105,&#xA;  &#34;prompt_tokens&#34;: 3,&#xA;  &#34;completion_tokens&#34;: 102&#xA; },&#xA; &#34;object&#34;: &#34;text_completion&#34;,&#xA; &#34;model&#34;: &#34;codegen&#34;,&#xA; &#34;id&#34;: &#34;01d7a11b-f87c-4261-8c03-8c78cbe4b067&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üëâ Known Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Again I want to set expectations around this being a proof-of-concept project. With that in mind. Here are some current known limitations.&lt;/p&gt; &#xA;&lt;p&gt;As of &lt;strong&gt;v0.0.1&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The models can be quite slow - especially the 6B ones. It can take ~30-40s to make suggestions across 4 CPU cores.&lt;/li&gt; &#xA; &lt;li&gt;I&#39;ve only tested the system on Ubuntu 22.04. Your mileage may vary on other operating systems. Please let me know if you try it elsewhere. I&#39;m particularly interested in performance on Apple Silicon.&lt;/li&gt; &#xA; &lt;li&gt;Sometimes suggestions get truncated in nonsensical places - e.g. part way through a variable name or string name. This is due to a hard limit on suggestion length.&lt;/li&gt; &#xA; &lt;li&gt;Sometimes the server will run out of memory and crash. This is because it will try to use everything above your current location as context during generation. I&#39;m working on a fix.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üëè Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This project would not have been possible without &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;Georgi Gerganov&#39;s work on GGML and llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;It was completely inspired by &lt;a href=&#34;https://github.com/fauxpilot/fauxpilot&#34;&gt;fauxpilot&lt;/a&gt; which I did experiment with for a little while but wanted to try to make the models work without a GPU&lt;/li&gt; &#xA; &lt;li&gt;The frontend of the project is powered by &lt;a href=&#34;https://github.com/Venthe/vscode-fauxpilot&#34;&gt;Venthe&#39;s vscode-fauxpilot plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The project uses the &lt;a href=&#34;https://github.com/salesforce/CodeGen&#34;&gt;Salesforce Codegen&lt;/a&gt; models.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://huggingface.co/moyix&#34;&gt;Moyix&lt;/a&gt; for his work on converting the Salesforce models to run in a GPT-J architecture. Not only does this &lt;a href=&#34;https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566&#34;&gt;confer some speed benefits&lt;/a&gt; but it also made it much easier for me to port the models to GGML using the &lt;a href=&#34;https://github.com/ggerganov/ggml/tree/master/examples/gpt-j&#34;&gt;existing gpt-j example code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The model server uses &lt;a href=&#34;https://crowcpp.org/master/&#34;&gt;CrowCPP&lt;/a&gt; to serve suggestions.&lt;/li&gt; &#xA; &lt;li&gt;Check out the &lt;a href=&#34;https://arxiv.org/pdf/2203.13474.pdf&#34;&gt;original scientific paper&lt;/a&gt; for CodeGen for more info.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mesonbuild/meson</title>
    <updated>2023-04-13T01:42:03Z</updated>
    <id>tag:github.com,2023-04-13:/mesonbuild/meson</id>
    <link href="https://github.com/mesonbuild/meson" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Meson Build System&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://mesonbuild.com/assets/images/meson_logo.png&#34;&gt; &lt;/p&gt; Meson¬Æ is a project to create the best possible next-generation build system. &#xA;&lt;h4&gt;Status&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.python.org/pypi/meson&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/meson.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/jussi0947/jussi/_build/latest?definitionId=1&#34;&gt;&lt;img src=&#34;https://dev.azure.com/jussi0947/jussi/_apis/build/status/mesonbuild.meson&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/mesonbuild/meson/branch/master&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/mesonbuild/meson/coverage.svg?branch=master&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Dependencies&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.org&#34;&gt;Python&lt;/a&gt; (version 3.7 or newer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ninja-build.org&#34;&gt;Ninja&lt;/a&gt; (version 1.8.2 or newer)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Installing from source&lt;/h4&gt; &#xA;&lt;p&gt;Meson is available on &lt;a href=&#34;https://pypi.python.org/pypi/meson&#34;&gt;PyPi&lt;/a&gt;, so it can be installed with &lt;code&gt;pip3 install meson&lt;/code&gt;. The exact command to type to install with &lt;code&gt;pip&lt;/code&gt; can vary between systems, be sure to use the Python 3 version of &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you wish you can install it locally with the standard Python command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;python3 -m pip install meson&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For builds using Ninja, Ninja can be downloaded directly from Ninja &lt;a href=&#34;https://github.com/ninja-build/ninja/releases&#34;&gt;GitHub release page&lt;/a&gt; or via &lt;a href=&#34;https://pypi.python.org/pypi/ninja&#34;&gt;PyPi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;python3 -m pip install ninja&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More on Installing Meson build can be found at the &lt;a href=&#34;https://mesonbuild.com/Getting-meson.html&#34;&gt;getting meson page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Creating a standalone script&lt;/h4&gt; &#xA;&lt;p&gt;Meson can be run as a &lt;a href=&#34;https://docs.python.org/3/library/zipapp.html&#34;&gt;Python zip app&lt;/a&gt;. To generate the executable run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./packaging/create_zipapp.py --outfile meson.pyz --interpreter &#39;/usr/bin/env python3&#39; &amp;lt;source checkout&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running&lt;/h4&gt; &#xA;&lt;p&gt;Meson requires that you have a source directory and a build directory and that these two are different. In your source root must exist a file called &lt;code&gt;meson.build&lt;/code&gt;. To generate the build system run this command:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;meson setup &amp;lt;source directory&amp;gt; &amp;lt;build directory&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Depending on how you obtained Meson the command might also be called &lt;code&gt;meson.py&lt;/code&gt; instead of plain &lt;code&gt;meson&lt;/code&gt;. In the rest of this document we are going to use the latter form.&lt;/p&gt; &#xA;&lt;p&gt;You can omit either of the two directories, and Meson will substitute the current directory and autodetect what you mean. This allows you to do things like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;cd &amp;lt;source root&amp;gt;&#xA;meson setup builddir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compile, cd into your build directory and type &lt;code&gt;ninja&lt;/code&gt;. To run unit tests, type &lt;code&gt;ninja test&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More on running Meson build system commands can be found at the &lt;a href=&#34;https://mesonbuild.com/Running-Meson.html&#34;&gt;running meson page&lt;/a&gt; or by typing &lt;code&gt;meson --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Contributing&lt;/h4&gt; &#xA;&lt;p&gt;We love code contributions. See the &lt;a href=&#34;https://mesonbuild.com/Contributing.html&#34;&gt;contribution page&lt;/a&gt; on the website for details.&lt;/p&gt; &#xA;&lt;h4&gt;IRC&lt;/h4&gt; &#xA;&lt;p&gt;The channel to use is &lt;code&gt;#mesonbuild&lt;/code&gt; either via Matrix (&lt;a href=&#34;https://app.element.io/#/room/#mesonbuild:matrix.org&#34;&gt;web interface&lt;/a&gt;) or &lt;a href=&#34;https://www.oftc.net/&#34;&gt;OFTC IRC&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Further info&lt;/h4&gt; &#xA;&lt;p&gt;More information about the Meson build system can be found at the &lt;a href=&#34;https://mesonbuild.com&#34;&gt;project&#39;s home page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Meson is a registered trademark of &lt;em&gt;&lt;strong&gt;Jussi Pakkanen&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>