<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-05T01:40:31Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>zju3dv/EasyVolcap</title>
    <updated>2024-01-05T01:40:31Z</updated>
    <id>tag:github.com,2024-01-05:/zju3dv/EasyVolcap</id>
    <link href="https://github.com/zju3dv/EasyVolcap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EasyVolcap: Accelerating Neural Volumetric Video Research&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/dendenxu/easyvolcap.github.io.assets/assets/43734697/de41df46-25e6-456c-a253-90d7807b2a9a&#34; alt=&#34;logo&#34; width=&#34;33%&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;: Accelerating Neural Volumetric Video Research&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/languages/top/zju3dv/EasyVolcap&#34; alt=&#34;python&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/zju3dv/EasyVolcap&#34; alt=&#34;star&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/license&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-zju3dv-white&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3610543.3626173&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2312.06575&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/file/d/1XxeO7TnAPvDugnxguEF5Jp89ERS9CAia/view?usp=sharing&#34;&gt;Example Dataset&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/file/d/1OFBFxes9kje02RARFpYpQ6SkmYlulYca/view?usp=sharing&#34;&gt;Pretrained Model&lt;/a&gt; | &lt;a href=&#34;https://zju3dv.github.io/4k4d&#34;&gt;4K4D&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;23.12.13 &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; will be presented at SIGGRAPH Asia 2023, Sydney.&lt;/li&gt; &#xA; &lt;li&gt;23.10.17 &lt;a href=&#34;https://zju3dv.github.io/4k4d&#34;&gt;&lt;em&gt;&lt;strong&gt;4K4D&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;, a real-time 4D view synthesis algorithm developed using &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;, has been made public.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; is a PyTorch library for accelerating neural volumetric video research, particularly in areas of &lt;strong&gt;volumetric video capturing&lt;/strong&gt;, reconstruction, and rendering.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zju3dv/EasyVolcap/assets/43734697/14fdfb46-5277-4963-ba75-067ea574c87a&#34;&gt;https://github.com/zju3dv/EasyVolcap/assets/43734697/14fdfb46-5277-4963-ba75-067ea574c87a&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Copy and paste version of the installation process listed below. For a more thorough explanation, read on.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Prepare conda environment&#xA;conda install -n base mamba -y -c conda-forge&#xA;conda create -n easyvolcap &#34;python&amp;gt;=3.10&#34; -y&#xA;conda activate easyvolcap&#xA;&#xA;# Install conda dependencies&#xA;mamba env update&#xA;&#xA;# Install pip dependencies&#xA;cat requirements.txt | sed -e &#39;/^\s*#.*$/d&#39; -e &#39;/^\s*$/d&#39; | xargs -n 1 pip install&#xA;&#xA;# Register EasyVolcp for imports&#xA;pip install -e . --no-build-isolation --no-deps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We opted to use the latest &lt;code&gt;pyproject.toml&lt;/code&gt; style packing system for exposing command line interfaces. It creates a virtual environment for building dependencies by default, which could be quite slow. Disabled with &lt;code&gt;--no-build-isolation&lt;/code&gt;. You should create a &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;mamba&lt;/code&gt; (recommended) environment for development, and install the dependencies manually. If the existing environment with &lt;code&gt;PyTorch&lt;/code&gt; installed can be utilized, you can jump straight to installing the &lt;code&gt;pip&lt;/code&gt; dependencies. More details about installing on &lt;em&gt;Windows&lt;/em&gt; or compiling &lt;em&gt;CUDA&lt;/em&gt; modules can be found in &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/design/install.md&#34;&gt;&lt;code&gt;install.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note: &lt;code&gt;pip&lt;/code&gt; dependencies can sometimes fail to install &amp;amp; build. However, not all of them are strictly required for &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The core ones include &lt;code&gt;tinycudann&lt;/code&gt; and &lt;code&gt;pytorch3d&lt;/code&gt;. Make sure those are built correctly and you&#39;ll be able to use most of the functionality of &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s also OK to install missing packages manually when &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; reports that they are missing since we lazy load a lot of them (&lt;code&gt;tinycudann&lt;/code&gt;, &lt;code&gt;diff_gauss&lt;/code&gt;, &lt;code&gt;open3d&lt;/code&gt; etc.).&lt;/li&gt; &#xA; &lt;li&gt;Just be sure to check how we listed the missing package in &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/requirements.txt&#34;&gt;&lt;code&gt;requirements.txt&lt;/code&gt;&lt;/a&gt; before performing &lt;code&gt;pip install&lt;/code&gt; on them. Some packages require to be installed from GitHub.&lt;/li&gt; &#xA; &lt;li&gt;If the &lt;code&gt;mamba env update&lt;/code&gt; step fails due to network issues, it is OK to proceed with pip installs since &lt;code&gt;PyTorch&lt;/code&gt; will also be installed by pip.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; now supports direct import from other locations &amp;amp; codebases. After installing, you can not only directly use utility modules and functions from &lt;code&gt;easyvolcap.utils&lt;/code&gt;, but also import and build upon our core modules and classes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the logging and debugging functions&#xA;from easyvolcap.utils.console_utils import * # log, tqdm, @catch_throw&#xA;from easyvolcap.utils.timer_utils import timer  # timer.record&#xA;from easyvolcap.utils.data_utils import export_pts, export_mesh, export_npz&#xA;...&#xA;&#xA;# Import the OpenGL-based viewer and build upon it&#xA;from easyvolcap.runners.volumetric_video_viewer import VolumetricVideoViewer&#xA;&#xA;class CustomViewer(VolumetricVideoViewer):&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The import will work when actually running the code, but it might fail since some of the autocompletion module &lt;a href=&#34;https://code.visualstudio.com/docs/python/editing#_importresolvefailure&#34;&gt;is not fully compatible with the newest editable install&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you see warnings when importing &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; in your editor like VSCode, you might want to add the path of your &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; codebase to the &lt;code&gt;python.autoComplete.extraPaths&lt;/code&gt; and &lt;code&gt;python.analysis.extraPaths&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  ...&#xA;    &#34;python.autoComplete.extraPaths&#34;: [&#34;/home/zju3dv/code/easyvolcap&#34;],&#xA;    &#34;python.analysis.extraPaths&#34;: [&#34;/home/zju3dv/code/easyvolcap&#34;]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another solution is to replace the installation command of &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; with a compatible one &lt;a href=&#34;https://microsoft.github.io/pyright/#/import-resolution?id=editable-installs&#34;&gt;using compatible editable install&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e . --no-build-isolation --no-deps --config-settings editable_mode=compat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that this is &lt;a href=&#34;https://setuptools.pypa.io/en/latest/userguide/development_mode.html#legacy-behavior&#34;&gt;marked deprecated in the PEP specification&lt;/a&gt;. Thus our recommendation is to change the setting of your editor instead.&lt;/p&gt; &#xA;&lt;h3&gt;New Project Based on &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re interested in developing or researching with &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;, the recommended way is to fork the repository and modify or append to our source code directly instead of using &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; as a module.&lt;/p&gt; &#xA;&lt;p&gt;After cloning and forking, add &lt;a href=&#34;https://github.com/zju3dv/EasyVolcap&#34;&gt;https://github.com/zju3dv/EasyVolcap&lt;/a&gt; as an &lt;code&gt;upstream&lt;/code&gt; if you want to receive updates from our side. Use &lt;code&gt;git fetch upstream&lt;/code&gt; to pull and merge our updates to &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; to your new project if needed. The following code block provides an example of this development process.&lt;/p&gt; &#xA;&lt;p&gt;Our recent project &lt;a href=&#34;https://github.com/zju3dv/4K4D&#34;&gt;4K4D&lt;/a&gt; is developed in this fashion.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Prepare name and GitHub repo of your new project&#xA;project=4K4D&#xA;repo=https://github.com/zju3dv/${project}&#xA;&#xA;# Clone EasyVolcap and add our repo as an upstream&#xA;git clone https://github.com/zju3dv/EasyVolcap ${project}&#xA;&#xA;# Setup the remote of your new project&#xA;git set-url origin ${repo}&#xA;&#xA;# Add EasyVolcap as upstream&#xA;git remote add upstream https://github.com/zju3dv/EasyVolcap&#xA;&#xA;# If EasyVolcap updates, fetch the updates and maybe merge with it&#xA;git fetch upstream&#xA;git merge upstream/main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Nevertheless, we still encourage you to read on and possibly follow the tutorials in the &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/#examples&#34;&gt;Examples&lt;/a&gt; section and maybe read our design documents in the &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/#design-docs&#34;&gt;Design Docs&lt;/a&gt; section to grasp an understanding of how &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; works as a project.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;In the following sections, we&#39;ll show examples of how to run &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; on a small multi-view video dataset with several of our implemented algorithms, including Instant-NGP+T, 3DGS+T, and ENeRFi (ENeRF Improved). In the documentation &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/misc/static.md&#34;&gt;&lt;code&gt;static.md&lt;/code&gt;&lt;/a&gt;, we also provide a complete example of how to prepare the dataset using COLMAP and run the above-mentioned three models using &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The example dataset for this section can be downloaded from &lt;a href=&#34;https://drive.google.com/file/d/1XxeO7TnAPvDugnxguEF5Jp89ERS9CAia/view?usp=sharing&#34;&gt;this Google Drive link&lt;/a&gt;. After downloading the example dataset, place the unzipped files inside &lt;code&gt;data/enerf_outdoor&lt;/code&gt; such that you can see files like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;data/enerf_outdoor/actor1_4_subseq/images&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;data/enerf_outdoor/actor1_4_subseq/intri.yml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;data/enerf_outdoor/actor1_4_subseq/extri.yml&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This dataset is a small subset of the &lt;a href=&#34;https://github.com/zju3dv/ENeRF/raw/master/docs/enerf_outdoor.md&#34;&gt;ENeRF-Outdoor&lt;/a&gt; dataset released by our team. For downloading the full dataset, please follow the guide in the &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/(https://github.com/zju3dv/ENeRF/raw/master/docs/enerf_outdoor.md)&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Structure&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;data/dataset/sequence # data_root &amp;amp; datadir&#xA;‚îú‚îÄ‚îÄ intri.yml # required: intrinsics&#xA;‚îú‚îÄ‚îÄ extri.yml # required: extrinsics&#xA;‚îî‚îÄ‚îÄ images # required: source images&#xA; &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ 000000 # camera / frame&#xA; &amp;nbsp;&amp;nbsp; ‚îÇ   ‚îú‚îÄ‚îÄ 000000.jpg # image&#xA; &amp;nbsp;&amp;nbsp; ‚îÇ   ‚îú‚îÄ‚îÄ 000001.jpg # for dynamic dataset, more images can be placed here&#xA;    ‚îÇ   ...&#xA; &amp;nbsp;&amp;nbsp; ‚îÇ   ‚îú‚îÄ‚îÄ 000298.jpg # for dynamic dataset, more images can be placed here&#xA; &amp;nbsp;&amp;nbsp; ‚îÇ   ‚îî‚îÄ‚îÄ 000299.jpg # for dynamic dataset, more images can be placed here&#xA; &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ 000001&#xA; &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ 000002&#xA;    ...&#xA; &amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ 000058&#xA; &amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ 000059&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; is designed to work on the simplest data form: &lt;code&gt;images&lt;/code&gt; and no more. The key data preprocessing are done in the &lt;code&gt;dataloader&lt;/code&gt; and &lt;code&gt;dataset&lt;/code&gt; modules. These steps are done in the dataloader&#39;s initialization&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We might correct the camera pose with their center of attention and world-up vector (&lt;code&gt;dataloader_cfg.dataset_cfg.use_aligned_cameras=True&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;We undistort read images from the disk using the intrinsic poses and store them as jpeg bytes in memory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Before running the model, let&#39;s first prepare some shell variables for easy access.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;expname=actor1_4_subseq&#xA;datadir=data/enerf_outdoor/actor1_4_subseq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Instant-NGP+T&lt;/h3&gt; &#xA;&lt;p&gt;We extend Instant-NGP to be time-aware, as a baseline method. With the data preparation completed, we&#39;ve got an &lt;code&gt;images&lt;/code&gt; folder and a pair of &lt;code&gt;intri.yml&lt;/code&gt; and &lt;code&gt;extri.yml&lt;/code&gt; files, and we can run the l3mhet model. Note that this model is not built for dynamic scenes, we train it here mainly for extracting initialization point clouds and computing a tighter bounding box. Similar procedures can be applied to other datasets if such initialization is required.&lt;/p&gt; &#xA;&lt;p&gt;We need to write a config file for this model&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Write the data-folder-related stuff inside configs/datasets. Just copy and paste &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/configs/datasets/enerf_outdoor/actor1_4_subseq.yaml&#34;&gt;&lt;code&gt;configs/datasets/enerf_outdoor/actor1_4_subseq.yaml&lt;/code&gt;&lt;/a&gt; and modify the &lt;code&gt;data_root&lt;/code&gt; and &lt;code&gt;bounds&lt;/code&gt; (bounding box), or maybe add a camera near-far threshold.&lt;/li&gt; &#xA; &lt;li&gt;Write the experiment config inside configs/exps. Just copy and paste &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/configs/exps/l3mhet/l3mhet_actor1_4_subseq.yaml&#34;&gt;&lt;code&gt;configs/exps/l3mhet/l3mhet_actor1_4_subseq.yaml&lt;/code&gt;&lt;/a&gt; and modify the &lt;code&gt;dataset&lt;/code&gt;-related line in &lt;code&gt;configs&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# With your config files ready, you can run the following command to train the model&#xA;evc -c configs/exps/l3mhet/l3mhet_${expname}.yaml&#xA;&#xA;# Now run the following command to render some output&#xA;evc -t test -c configs/exps/l3mhet/l3mhet_${expname}.yaml,configs/specs/spiral.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/configs/specs/spiral.yaml&#34;&gt;&lt;code&gt;configs/specs/spiral.yaml&lt;/code&gt;&lt;/a&gt;: please check this file for more details, it&#39;s a collection of configs to tell the dataloader and visualizer to generate a spiral path by interpolating the given cameras&lt;/p&gt; &#xA;&lt;h3&gt;Running 3DGS+T&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dendenxu/easyvolcap.github.io.assets/assets/43734697/acd83f13-ba34-449c-96ce-e7b7b0781de4&#34;&gt;https://github.com/dendenxu/easyvolcap.github.io.assets/assets/43734697/acd83f13-ba34-449c-96ce-e7b7b0781de4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The original &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3DGS&lt;/a&gt; uses the sparse reconstruction result of COLMAP for initialization. However, we found that the sparse reconstruction result often contains a lot of floating points, which is hard to prune for 3DGS and could easily make the model fail to converge. Thus, we opted to use the &#34;dense&#34; reconstruction result of our Instant-NGP+T implementation by computing the RGBD image for input views and concatenating them as the input of 3DGS. The script &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/scripts/tools/volume_fusion.py&#34;&gt;&lt;code&gt;volume_fusion.py&lt;/code&gt;&lt;/a&gt; controls this process and it should work similarly on all models that support depth output.&lt;/p&gt; &#xA;&lt;p&gt;The following script block provides an example of how to prepare an initialization for our 3DGS+T implementation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Extract geometry (point cloud) for initialization from the l3mhet model&#xA;# Tune image sample rate and resizing ratio for a denser or sparser estimation&#xA;python scripts/tools/volume_fusion.py -- -c configs/exps/l3mhet/l3mhet_${expname}.yaml val_dataloader_cfg.dataset_cfg.ratio=0.15&#xA;&#xA;# Move the rendering results to the dataset folder&#xA;source_folder=&#34;data/geometry/l3mhet_${expname}/POINT&#34;&#xA;destination_folder=&#34;${datadir}/vhulls&#34;&#xA;&#xA;# Create the destination directory if it doesn&#39;t exist&#xA;mkdir -p ${destination_folder}&#xA;&#xA;# Loop through all .ply files in the source directory&#xA;for file in ${source_folder}/*.ply; do&#xA;    number=$(echo $(basename ${file}) | sed -e &#39;s/frame\([0-9]*\).ply/\1/&#39;)&#xA;    formatted_number=$(printf &#34;%06d&#34; ${number})&#xA;    destination_file=&#34;${destination_folder}/${formatted_number}.ply&#34;&#xA;    cp ${file} ${destination_file}&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our conventions for storing initialization point clouds:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Raw point clouds extracted using Instant-NGP or Space Carving are placed inside the &lt;code&gt;vhulls&lt;/code&gt; folder. These files might be large. It&#39;s OK to directly optimize 3DGS+T on these.&lt;/li&gt; &#xA; &lt;li&gt;We might perform some cleanup of the point clouds and store them in the &lt;code&gt;surfs&lt;/code&gt; folder. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For 3DGS+T, the cleaned-up point clouds might be easier to optimize since 3DGS is good at growing details but not so good at dealing with floaters (removing or splitting).&lt;/li&gt; &#xA;   &lt;li&gt;For other representations, the cleaned-up point clouds work better than the visual hull (from Space Carving) but might not work so well as the raw point clouds of Instant-NGP.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then, prepare an experiment config like &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/configs/exps/gaussiant/gaussiant_actor1_4_subseq.yaml&#34;&gt;&lt;code&gt;configs/exps/gaussiant/gaussiant_actor1_4_subseq.yaml&lt;/code&gt;&lt;/a&gt;. The &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/configs/specs/colmap.yaml&#34;&gt;&lt;code&gt;colmap.yaml&lt;/code&gt;&lt;/a&gt; provides some heuristics for large-scale static scenes. Remove these if you&#39;re not planning on using COLMAP&#39;s parameters directly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Train a 3DGS model on the ${expname} dataset&#xA;evc -c configs/exps/gaussiant/gaussiant_${expname}.yaml # might run out of VRAM, try reducing densify until iter&#xA;&#xA;# Perform rendering on the trained ${expname} dataset&#xA;evc -t test -c configs/exps/gaussiant/gaussiant_${expname}.yaml,configs/specs/superm.yaml,configs/specs/spiral.yaml&#xA;&#xA;# Perform rendering with GUI, do this on a machine with monitor, tested on Windows and Ubuntu&#xA;evc -t gui -c configs/exps/gaussiant/gaussiant_${expname}.yaml,configs/specs/superm.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/configs/specs/superm.yaml&#34;&gt;&lt;code&gt;superm.yaml&lt;/code&gt;&lt;/a&gt; skips the loading of input images and other initializations for network-only rendering since all the information we need is contained inside the trained model.&lt;/p&gt; &#xA;&lt;h3&gt;Inferencing With ENeRFi&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dendenxu/easyvolcap.github.io.assets/assets/43734697/68401485-85fe-477f-9144-976bb2ee8d3c&#34;&gt;https://github.com/dendenxu/easyvolcap.github.io.assets/assets/43734697/68401485-85fe-477f-9144-976bb2ee8d3c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dendenxu/easyvolcap.github.io.assets/assets/43734697/6d60f2a4-6692-43e8-b682-aa27fcdf9516&#34;&gt;https://github.com/dendenxu/easyvolcap.github.io.assets/assets/43734697/6d60f2a4-6692-43e8-b682-aa27fcdf9516&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The pre-trained model for ENeRFi on the DTU dataset can be downloaded from &lt;a href=&#34;https://drive.google.com/file/d/1OFBFxes9kje02RARFpYpQ6SkmYlulYca/view?usp=sharing&#34;&gt;this Google Drive link&lt;/a&gt;. After downloading, rename the model to &lt;code&gt;latest.npz&lt;/code&gt; and place it in &lt;code&gt;data/trained_model/enerfi_dtu&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Render ENeRFi with pretrained model&#xA;evc -t test -c configs/exps/enerfi/enerfi_${expname}.yaml,configs/specs/spiral.yaml,configs/specs/ibr.yaml runner_cfg.visualizer_cfg.save_tag=${expname} exp_name=enerfi_dtu&#xA;&#xA;# Render ENeRFi with GUI&#xA;evc -t gui -c configs/exps/enerfi/enerfi_${expname}.yaml exp_name=enerfi_dtu # 2.5 FPS on 3060&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If more performance is desired:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Fine quality, faster rendering&#xA;evc -t gui -c configs/exps/enerfi/enerfi_actor1_4_subseq.yaml exp_name=enerfi_dtu model_cfg.sampler_cfg.n_planes=32,8 model_cfg.sampler_cfg.n_samples=4,1 # 3.6 FPS on 3060&#xA;&#xA;# Worst quality, fastest rendering&#xA;evc -t gui -c configs/exps/enerfi/enerfi_actor1_4_subseq.yaml,configs/specs/fp16.yaml exp_name=enerfi_dtu model_cfg.sampler_cfg.n_planes=32,8 model_cfg.sampler_cfg.n_samples=4,1 # 5.0 FPS on 3060&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Documentations are still WIP. We&#39;ll gradually add more guides and examples, especially regarding the usage of &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;&#39;s various systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Design Docs&lt;/h3&gt; &#xA;&lt;p&gt;The documentation contained in the &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/design&#34;&gt;&lt;code&gt;docs/design&lt;/code&gt;&lt;/a&gt; directory contains explanations of design choices and various best practices when developing with &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/design/main.md&#34;&gt;&lt;code&gt;docs/design/main.md&lt;/code&gt;&lt;/a&gt;: Gives an overview of the structure of the &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; codebase.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/design/config.md&#34;&gt;&lt;code&gt;docs/design/config.md&lt;/code&gt;&lt;/a&gt;: Thoroughly explains the commandline and configuration API of &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/design/dataset.md&#34;&gt;&lt;code&gt;docs/design/dataset.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/design/logging.md&#34;&gt;&lt;code&gt;docs/design/logging.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/design/model.md&#34;&gt;&lt;code&gt;docs/design/model.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/design/runner.md&#34;&gt;&lt;code&gt;docs/design/runner.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/docs/design/viewer.md&#34;&gt;&lt;code&gt;docs/design/viewer.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Project Docs&lt;/h3&gt; &#xA;&lt;h3&gt;Misc Docs&lt;/h3&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We would like to acknowledge the following inspiring prior work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zju3dv/EasyMocap&#34;&gt;EasyMocap: Make Human Motion Capture Easier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openxrlab/xrnerf&#34;&gt;XRNeRF: OpenXRLab Neural Radiance Field (NeRF) Toolbox and Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nerfstudio-project/nerfstudio&#34;&gt;Nerfstudio: A Modular Framework for Neural Radiance Field Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ocornut/imgui&#34;&gt;Dear ImGui: Bloat-Free Graphical User Interface for C++ With Minimal Dependencies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zju3dv/neuralbody&#34;&gt;Neural Body: Implicit Neural Representations with Structured Latent Codes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zju3dv/ENeRF&#34;&gt;ENeRF: Efficient Neural Radiance Fields for Interactive Free-Viewpoint Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;Instant Neural Graphics Primitives with a Multiresolution Hash Encoding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3D Gaussian Splatting for Real-Time Radiance Field Rendering&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt;&#39;s license can be found &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/license&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that the license of the algorithms or other components implemented in &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; might be different from the license of &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; itself. You will have to install their respective modules to use them in &lt;em&gt;&lt;strong&gt;EasyVolcap&lt;/strong&gt;&lt;/em&gt; following the guide in the &lt;a href=&#34;https://raw.githubusercontent.com/zju3dv/EasyVolcap/main/#installation&#34;&gt;installation section&lt;/a&gt;. Please refer to their respective licensing terms if you&#39;re planning on using them.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code useful for your research, please cite us using the following BibTeX entry. If you used our implementation of other methods, please also cite them separately.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{xu2023easyvolcap,&#xA;  title={EasyVolcap: Accelerating Neural Volumetric Video Research},&#xA;  author={Xu, Zhen and Xie, Tao and Peng, Sida and Lin, Haotong and Shuai, Qing and Yu, Zhiyuan and He, Guangzhao and Sun, Jiaming and Bao, Hujun and Zhou, Xiaowei},&#xA;  booktitle={SIGGRAPH Asia 2023 Technical Communications},&#xA;  year={2023}&#xA;}&#xA;&#xA;@article{xu20234k4d,&#xA;  title={4K4D: Real-Time 4D View Synthesis at 4K Resolution},&#xA;  author={Xu, Zhen and Peng, Sida and Lin, Haotong and He, Guangzhao and Sun, Jiaming and Shen, Yujun and Bao, Hujun and Zhou, Xiaowei},&#xA;  booktitle={arXiv preprint arXiv:2310.11448},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>open-compass/opencompass</title>
    <updated>2024-01-05T01:40:31Z</updated>
    <id>tag:github.com,2024-01-05:/open-compass/opencompass</id>
    <link href="https://github.com/open-compass/opencompass" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenCompass is an LLM evaluation platform, supporting a wide range of models (LLaMA, LLaMa2, ChatGLM2, ChatGPT, Claude, etc) over 50+ datasets.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/docs/en/_static/image/logo.svg?sanitize=true&#34; width=&#34;500px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://opencompass.readthedocs.io/en&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/opencompass/badge&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-compass/opencompass/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/InternLM/opencompass.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!-- [![PyPI](https://badge.fury.io/py/opencompass.svg)](https://pypi.org/project/opencompass/) --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://opencompass.org.cn/&#34;&gt;üåêWebsite&lt;/a&gt; | &lt;a href=&#34;https://opencompass.readthedocs.io/en/latest/&#34;&gt;üìòDocumentation&lt;/a&gt; | &lt;a href=&#34;https://opencompass.readthedocs.io/en/latest/get_started/installation.html&#34;&gt;üõ†Ô∏èInstallation&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-compass/opencompass/issues/new/choose&#34;&gt;ü§îReporting Issues&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/README_zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üëã join us on &lt;a href=&#34;https://discord.gg/KKwfEbFj7U&#34; target=&#34;_blank&#34;&gt;Discord&lt;/a&gt; and &lt;a href=&#34;https://r.vansin.top/?r=opencompass&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üì£ OpenCompass 2023 LLM Annual Leaderboard&lt;/h2&gt; &#xA;&lt;p&gt;We are honored to have witnessed the tremendous progress of artificial general intelligence together with the community in the past year, and we are also very pleased that &lt;strong&gt;OpenCompass&lt;/strong&gt; can help numerous developers and users.&lt;/p&gt; &#xA;&lt;p&gt;We announce the launch of the &lt;strong&gt;OpenCompass 2023 LLM Annual Leaderboard&lt;/strong&gt; plan. We expect to release the annual leaderboard of the LLMs in January 2024, systematically evaluating the performance of LLMs in various capabilities such as language, knowledge, reasoning, creation, long-text, and agents.&lt;/p&gt; &#xA;&lt;p&gt;At that time, we will release rankings for both open-source models and commercial API models, aiming to provide a comprehensive, objective, and neutral reference for the industry and research community.&lt;/p&gt; &#xA;&lt;p&gt;We sincerely invite various large models to join the OpenCompass to showcase their performance advantages in different fields. At the same time, we also welcome researchers and developers to provide valuable suggestions and contributions to jointly promote the development of the LLMs. If you have any questions or needs, please feel free to &lt;a href=&#34;mailto:opencompass@pjlab.org.cn&#34;&gt;contact us&lt;/a&gt;. In addition, relevant evaluation contents, performance statistics, and evaluation methods will be open-source along with the leaderboard release.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s look forward to the release of the OpenCompass 2023 LLM Annual Leaderboard!&lt;/p&gt; &#xA;&lt;h2&gt;üß≠ Welcome&lt;/h2&gt; &#xA;&lt;p&gt;to &lt;strong&gt;OpenCompass&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;p&gt;Just like a compass guides us on our journey, OpenCompass will guide you through the complex landscape of evaluating large language models. With its powerful algorithms and intuitive interface, OpenCompass makes it easy to assess the quality and effectiveness of your NLP models.&lt;/p&gt; &#xA;&lt;p&gt;üö©üö©üö© Explore opportunities at OpenCompass! We&#39;re currently &lt;strong&gt;hiring full-time researchers/engineers and interns&lt;/strong&gt;. If you&#39;re passionate about LLM and OpenCompass, don&#39;t hesitate to reach out to us via &lt;a href=&#34;mailto:zhangsongyang@pjlab.org.cn&#34;&gt;email&lt;/a&gt;. We&#39;d love to hear from you!&lt;/p&gt; &#xA;&lt;p&gt;üî•üî•üî• We are delighted to announce that &lt;strong&gt;the OpenCompass has been recommended by the Meta AI&lt;/strong&gt;, click &lt;a href=&#34;https://ai.meta.com/llama/get-started/#validation&#34;&gt;Get Started&lt;/a&gt; of Llama for more information.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;br&gt; We launch the OpenCompass Collaboration project, welcome to support diverse evaluation benchmarks into OpenCompass! Clike &lt;a href=&#34;https://github.com/open-compass/opencompass/issues/248&#34;&gt;Issue&lt;/a&gt; for more information. Let&#39;s work together to build a more powerful OpenCompass toolkit!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üöÄ What&#39;s New &lt;a&gt;&lt;img width=&#34;35&#34; height=&#34;20&#34; src=&#34;https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-4fe0-bbba-39ffb77730be.png&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12.28]&lt;/strong&gt; We have enabled seamless evaluation of all models developed using &lt;a href=&#34;https://github.com/Alpha-VLLM/LLaMA2-Accessory&#34;&gt;LLaMA2-Accessory&lt;/a&gt;, a powerful toolkit for comprehensive LLM development. üî•üî•üî•.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12.22]&lt;/strong&gt; We have released &lt;a href=&#34;https://github.com/open-compass/T-Eval&#34;&gt;T-Eval&lt;/a&gt;, a step-by-step evaluation benchmark to gauge your LLMs on tool utilization. Welcome to our &lt;a href=&#34;https://open-compass.github.io/T-Eval/leaderboard.html&#34;&gt;Leaderboard&lt;/a&gt; for more details! üî•üî•üî•.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12.10]&lt;/strong&gt; We have released &lt;a href=&#34;https://github.com/open-compass/VLMEvalKit&#34;&gt;VLMEvalKit&lt;/a&gt;, a toolkit for evaluating vision-language models (VLMs), currently support 20+ VLMs and 7 multi-modal benchmarks (including MMBench series). üî•üî•üî•.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12.10]&lt;/strong&gt; We have supported Mistral AI&#39;s MoE LLM: &lt;strong&gt;Mixtral-8x7B-32K&lt;/strong&gt;. Welcome to &lt;a href=&#34;https://github.com/open-compass/MixtralKit&#34;&gt;MixtralKit&lt;/a&gt; for more details about inference and evaluation. üî•üî•üî•.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.11.22]&lt;/strong&gt; We have supported many API-based models, include &lt;strong&gt;Baidu, ByteDance, Huawei, 360&lt;/strong&gt;. Welcome to &lt;a href=&#34;https://opencompass.readthedocs.io/en/latest/user_guides/models.html&#34;&gt;Models&lt;/a&gt; section for more details. üî•üî•üî•.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.11.20]&lt;/strong&gt; Thanks &lt;a href=&#34;https://github.com/helloyongyang&#34;&gt;helloyongyang&lt;/a&gt; for supporting the evaluation with &lt;a href=&#34;https://github.com/ModelTC/lightllm&#34;&gt;LightLLM&lt;/a&gt; as backent. Welcome to &lt;a href=&#34;https://opencompass.readthedocs.io/en/latest/advanced_guides/evaluation_lightllm.html&#34;&gt;Evaluation With LightLLM&lt;/a&gt; for more details. üî•üî•üî•.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.11.13]&lt;/strong&gt; We are delighted to announce the release of OpenCompass v0.1.8. This version enables local loading of evaluation benchmarks, thereby eliminating the need for an internet connection. Please note that with this update, &lt;strong&gt;you must re-download all evaluation datasets&lt;/strong&gt; to ensure accurate and up-to-date results.üî•üî•üî•.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.11.06]&lt;/strong&gt; We have supported several API-based models, include &lt;strong&gt;ChatGLM Pro@Zhipu, ABAB-Chat@MiniMax and Xunfei&lt;/strong&gt;. Welcome to &lt;a href=&#34;https://opencompass.readthedocs.io/en/latest/user_guides/models.html&#34;&gt;Models&lt;/a&gt; section for more details. üî•üî•üî•.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.10.24]&lt;/strong&gt; We release a new benchmark for evaluating LLMs‚Äô capabilities of having multi-turn dialogues. Welcome to &lt;a href=&#34;https://github.com/open-compass/BotChat&#34;&gt;BotChat&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.09.26]&lt;/strong&gt; We update the leaderboard with &lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;Qwen&lt;/a&gt;, one of the best-performing open-source models currently available, welcome to our &lt;a href=&#34;https://opencompass.org.cn&#34;&gt;homepage&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.09.20]&lt;/strong&gt; We update the leaderboard with &lt;a href=&#34;https://github.com/InternLM/InternLM&#34;&gt;InternLM-20B&lt;/a&gt;, welcome to our &lt;a href=&#34;https://opencompass.org.cn&#34;&gt;homepage&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.09.19]&lt;/strong&gt; We update the leaderboard with WeMix-LLaMA2-70B/Phi-1.5-1.3B, welcome to our &lt;a href=&#34;https://opencompass.org.cn&#34;&gt;homepage&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.09.18]&lt;/strong&gt; We have released &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/docs/en/advanced_guides/longeval.md&#34;&gt;long context evaluation guidance&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/docs/en/notes/news.md&#34;&gt;More&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;‚ú® Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/open-compass/opencompass/assets/22607038/f45fe125-4aed-4f8c-8fe8-df4efb41a8ea&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenCompass is a one-stop platform for large model evaluation, aiming to provide a fair, open, and reproducible benchmark for large model evaluation. Its main features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Comprehensive support for models and datasets&lt;/strong&gt;: Pre-support for 20+ HuggingFace and API models, a model evaluation scheme of 70+ datasets with about 400,000 questions, comprehensively evaluating the capabilities of the models in five dimensions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient distributed evaluation&lt;/strong&gt;: One line command to implement task division and distributed evaluation, completing the full evaluation of billion-scale models in just a few hours.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Diversified evaluation paradigms&lt;/strong&gt;: Support for zero-shot, few-shot, and chain-of-thought evaluations, combined with standard or dialogue-type prompt templates, to easily stimulate the maximum performance of various models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular design with high extensibility&lt;/strong&gt;: Want to add new models or datasets, customize an advanced task division strategy, or even support a new cluster management system? Everything about OpenCompass can be easily expanded!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Experiment management and reporting mechanism&lt;/strong&gt;: Use config files to fully record each experiment, and support real-time reporting of results.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìä Leaderboard&lt;/h2&gt; &#xA;&lt;p&gt;We provide &lt;a href=&#34;https://opencompass.org.cn/rank&#34;&gt;OpenCompass Leaderboard&lt;/a&gt; for the community to rank all public models and API models. If you would like to join the evaluation, please provide the model repository URL or a standard API interface to the email address &lt;code&gt;opencompass@pjlab.org.cn&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/#top&#34;&gt;üîùBack to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; &#xA;&lt;p&gt;Below are the steps for quick installation and datasets preparation.&lt;/p&gt; &#xA;&lt;h3&gt;üíª Environment Setup&lt;/h3&gt; &#xA;&lt;h4&gt;Open-source Models with GPU&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name opencompass python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y&#xA;conda activate opencompass&#xA;git clone https://github.com/open-compass/opencompass opencompass&#xA;cd opencompass&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;API Models with CPU-only&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n opencompass python=3.10 pytorch torchvision torchaudio cpuonly -c pytorch -y&#xA;conda activate opencompass&#xA;git clone https://github.com/open-compass/opencompass opencompass&#xA;cd opencompass&#xA;pip install -e .&#xA;# also please install requiresments packages via `pip install -r requirements/api.txt` for API models if needed.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üìÇ Data Preparation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download dataset to data/ folder&#xA;wget https://github.com/open-compass/opencompass/releases/download/0.1.8.rc1/OpenCompassData-core-20231110.zip&#xA;unzip OpenCompassData-core-20231110.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some third-party features, like Humaneval and Llama, may require additional steps to work properly, for detailed steps please refer to the &lt;a href=&#34;https://opencompass.readthedocs.io/en/latest/get_started/installation.html&#34;&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/#top&#34;&gt;üîùBack to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üèóÔ∏è Ô∏èEvaluation&lt;/h2&gt; &#xA;&lt;p&gt;After ensuring that OpenCompass is installed correctly according to the above steps and the datasets are prepared, you can evaluate the performance of the LLaMA-7b model on the MMLU and C-Eval datasets using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py --models hf_llama_7b --datasets mmlu_ppl ceval_ppl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OpenCompass has predefined configurations for many models and datasets. You can list all available model and dataset configurations using the &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/docs/en/tools.md#list-configs&#34;&gt;tools&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# List all configurations&#xA;python tools/list_configs.py&#xA;# List all configurations related to llama and mmlu&#xA;python tools/list_configs.py llama mmlu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also evaluate other HuggingFace models via command line. Taking LLaMA-7b as an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py --datasets ceval_ppl mmlu_ppl \&#xA;--hf-path huggyllama/llama-7b \  # HuggingFace model path&#xA;--model-kwargs device_map=&#39;auto&#39; \  # Arguments for model construction&#xA;--tokenizer-kwargs padding_side=&#39;left&#39; truncation=&#39;left&#39; use_fast=False \  # Arguments for tokenizer construction&#xA;--max-out-len 100 \  # Maximum number of tokens generated&#xA;--max-seq-len 2048 \  # Maximum sequence length the model can accept&#xA;--batch-size 8 \  # Batch size&#xA;--no-batch-padding \  # Don&#39;t enable batch padding, infer through for loop to avoid performance loss&#xA;--num-gpus 1  # Number of minimum required GPUs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;br&gt; To run the command above, you will need to remove the comments starting from &lt;code&gt;# &lt;/code&gt; first.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Through the command line or configuration files, OpenCompass also supports evaluating APIs or custom models, as well as more diversified evaluation strategies. Please read the &lt;a href=&#34;https://opencompass.readthedocs.io/en/latest/get_started/quick_start.html&#34;&gt;Quick Start&lt;/a&gt; to learn how to run an evaluation task.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/#top&#34;&gt;üîùBack to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Dataset Support&lt;/h2&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Language&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Knowledge&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Reasoning&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Examination&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Word Definition&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;WiC&lt;/li&gt; &#xA;      &lt;li&gt;SummEdits&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Idiom Learning&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;CHID&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Semantic Similarity&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;AFQMC&lt;/li&gt; &#xA;      &lt;li&gt;BUSTM&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Coreference Resolution&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;CLUEWSC&lt;/li&gt; &#xA;      &lt;li&gt;WSC&lt;/li&gt; &#xA;      &lt;li&gt;WinoGrande&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Translation&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Flores&lt;/li&gt; &#xA;      &lt;li&gt;IWSLT2017&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Multi-language Question Answering&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;TyDi-QA&lt;/li&gt; &#xA;      &lt;li&gt;XCOPA&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Multi-language Summary&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;XLSum&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Knowledge Question Answering&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;BoolQ&lt;/li&gt; &#xA;      &lt;li&gt;CommonSenseQA&lt;/li&gt; &#xA;      &lt;li&gt;NaturalQuestions&lt;/li&gt; &#xA;      &lt;li&gt;TriviaQA&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Textual Entailment&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;CMNLI&lt;/li&gt; &#xA;      &lt;li&gt;OCNLI&lt;/li&gt; &#xA;      &lt;li&gt;OCNLI_FC&lt;/li&gt; &#xA;      &lt;li&gt;AX-b&lt;/li&gt; &#xA;      &lt;li&gt;AX-g&lt;/li&gt; &#xA;      &lt;li&gt;CB&lt;/li&gt; &#xA;      &lt;li&gt;RTE&lt;/li&gt; &#xA;      &lt;li&gt;ANLI&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Commonsense Reasoning&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;StoryCloze&lt;/li&gt; &#xA;      &lt;li&gt;COPA&lt;/li&gt; &#xA;      &lt;li&gt;ReCoRD&lt;/li&gt; &#xA;      &lt;li&gt;HellaSwag&lt;/li&gt; &#xA;      &lt;li&gt;PIQA&lt;/li&gt; &#xA;      &lt;li&gt;SIQA&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Mathematical Reasoning&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;MATH&lt;/li&gt; &#xA;      &lt;li&gt;GSM8K&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Theorem Application&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;TheoremQA&lt;/li&gt; &#xA;      &lt;li&gt;StrategyQA&lt;/li&gt; &#xA;      &lt;li&gt;SciBench&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Comprehensive Reasoning&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;BBH&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Junior High, High School, University, Professional Examinations&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;C-Eval&lt;/li&gt; &#xA;      &lt;li&gt;AGIEval&lt;/li&gt; &#xA;      &lt;li&gt;MMLU&lt;/li&gt; &#xA;      &lt;li&gt;GAOKAO-Bench&lt;/li&gt; &#xA;      &lt;li&gt;CMMLU&lt;/li&gt; &#xA;      &lt;li&gt;ARC&lt;/li&gt; &#xA;      &lt;li&gt;Xiezhi&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Medical Examinations&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;CMB&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Understanding&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Long Context&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Safety&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Code&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Reading Comprehension&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;C3&lt;/li&gt; &#xA;      &lt;li&gt;CMRC&lt;/li&gt; &#xA;      &lt;li&gt;DRCD&lt;/li&gt; &#xA;      &lt;li&gt;MultiRC&lt;/li&gt; &#xA;      &lt;li&gt;RACE&lt;/li&gt; &#xA;      &lt;li&gt;DROP&lt;/li&gt; &#xA;      &lt;li&gt;OpenBookQA&lt;/li&gt; &#xA;      &lt;li&gt;SQuAD2.0&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Content Summary&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;CSL&lt;/li&gt; &#xA;      &lt;li&gt;LCSTS&lt;/li&gt; &#xA;      &lt;li&gt;XSum&lt;/li&gt; &#xA;      &lt;li&gt;SummScreen&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Content Analysis&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;EPRSTMT&lt;/li&gt; &#xA;      &lt;li&gt;LAMBADA&lt;/li&gt; &#xA;      &lt;li&gt;TNEWS&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Long Context Understanding&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;LEval&lt;/li&gt; &#xA;      &lt;li&gt;LongBench&lt;/li&gt; &#xA;      &lt;li&gt;GovReports&lt;/li&gt; &#xA;      &lt;li&gt;NarrativeQA&lt;/li&gt; &#xA;      &lt;li&gt;Qasper&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Safety&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;CivilComments&lt;/li&gt; &#xA;      &lt;li&gt;CrowsPairs&lt;/li&gt; &#xA;      &lt;li&gt;CValues&lt;/li&gt; &#xA;      &lt;li&gt;JigsawMultilingual&lt;/li&gt; &#xA;      &lt;li&gt;TruthfulQA&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Robustness&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;AdvGLUE&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary&gt;&lt;b&gt;Code&lt;/b&gt;&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;HumanEval&lt;/li&gt; &#xA;      &lt;li&gt;HumanEvalX&lt;/li&gt; &#xA;      &lt;li&gt;MBPP&lt;/li&gt; &#xA;      &lt;li&gt;APPs&lt;/li&gt; &#xA;      &lt;li&gt;DS1000&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;OpenCompass Ecosystem&lt;/h2&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/#top&#34;&gt;üîùBack to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Model Support&lt;/h2&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Open-source Models&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;API Models&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;!-- &lt;td&gt;&#xA;        &lt;b&gt;Custom Models&lt;/b&gt;&#xA;      &lt;/td&gt; --&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM&#34;&gt;InternLM&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/baichuan-inc&#34;&gt;Baichuan&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/nlpxucan/WizardLM&#34;&gt;WizardLM&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;&gt;ChatGLM2&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM3-6B&#34;&gt;ChatGLM3&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/TigerResearch/TigerBot&#34;&gt;TigerBot&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;Qwen&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/vivo-ai-lab/BlueLM&#34;&gt;BlueLM&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;...&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;OpenAI&lt;/li&gt; &#xA;     &lt;li&gt;Claude&lt;/li&gt; &#xA;     &lt;li&gt;ZhipuAI(ChatGLM)&lt;/li&gt; &#xA;     &lt;li&gt;Baichuan&lt;/li&gt; &#xA;     &lt;li&gt;ByteDance(YunQue)&lt;/li&gt; &#xA;     &lt;li&gt;Huawei(PanGu)&lt;/li&gt; &#xA;     &lt;li&gt;360&lt;/li&gt; &#xA;     &lt;li&gt;Baidu(ERNIEBot)&lt;/li&gt; &#xA;     &lt;li&gt;MiniMax(ABAB-Chat)&lt;/li&gt; &#xA;     &lt;li&gt;SenseTime(nova)&lt;/li&gt; &#xA;     &lt;li&gt;Xunfei(Spark)&lt;/li&gt; &#xA;     &lt;li&gt;‚Ä¶‚Ä¶&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/#top&#34;&gt;üîùBack to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üîú Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Subjective Evaluation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release CompassAreana&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Subjective evaluation dataset.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Long-context &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Long-context evaluation with extensive datasets.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Long-context leaderboard.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Coding &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Coding evaluation leaderboard.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Non-python language evaluation service.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Agent &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support various agenet framework.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Evaluation of tool use of the LLMs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Robustness &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support various attack method&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üë∑‚Äç‚ôÇÔ∏è Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improving OpenCompass. Please refer to the &lt;a href=&#34;https://opencompass.readthedocs.io/en/latest/notes/contribution_guide.html&#34;&gt;contributing guideline&lt;/a&gt; for the best practice.&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Some code in this project is cited and modified from &lt;a href=&#34;https://github.com/Shark-NLP/OpenICL&#34;&gt;OpenICL&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some datasets and prompt implementations are modified from &lt;a href=&#34;https://github.com/FranxYao/chain-of-thought-hub&#34;&gt;chain-of-thought-hub&lt;/a&gt; and &lt;a href=&#34;https://github.com/declare-lab/instruct-eval&#34;&gt;instruct-eval&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üñäÔ∏è Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{2023opencompass,&#xA;    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},&#xA;    author={OpenCompass Contributors},&#xA;    howpublished = {\url{https://github.com/open-compass/opencompass}},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/opencompass/main/#top&#34;&gt;üîùBack to top&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Frrrrrrrrank/auto_job__find__chatgpt__rpa</title>
    <updated>2024-01-05T01:40:31Z</updated>
    <id>tag:github.com,2024-01-05:/Frrrrrrrrank/auto_job__find__chatgpt__rpa</id>
    <link href="https://github.com/Frrrrrrrrank/auto_job__find__chatgpt__rpa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is a tool used to automatically generate a cover letter using chatgpt based on your resume and job description and send messages to bosses in China.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;ËøôÊòØ‰∏Ä‰∏™ÂÆåÂÖ®ÂÖçË¥πÁöÑËÑöÊú¨ÔºåÂè™ÈúÄË¶Å‰Ω†‰ª¨Ëá™Â∑±ÈÖçÁΩÆÂ•ΩopenaiÁöÑapiÂç≥ÂèØ Â∏åÊúõÊÇ®ËÉΩÁªôÊàëÁÇπ‰∏™star Â¶ÇÊûúÂú®Ëøô‰∏™ÂØíÂÜ∑ÁöÑÊãõËÅòÂ≠£ÔºåËøô‰∏™ËÑöÊú¨ËÉΩÁªôÊÇ®‰∏Ä‰∫õÂ∏ÆÂä©ÔºåÂ∏¶Êù•‰∏Ä‰∫õÊ∏©ÊöñÔºåÂ∞ÜËÆ©ÊàëÈùûÂ∏∏Ëç£Âπ∏&lt;/p&gt; &#xA;&lt;p&gt;Â∏åÊúõ‰∏çË¶ÅÊúâ‰∫∫ÊãøÁùÄÊàëÁöÑËÑöÊú¨ÂéªÂâ≤Èü≠ËèúÔºåÈÉΩÂ∑≤ÁªèË¢´ÈÄºÂà∞Áî®ËøôÁßçËÑöÊú¨ÊäïÁÆÄÂéÜÁöÑÂú∞Ê≠•‰∫ÜÔºåË∫´‰∏ä‰πüÊ≤°Âï•Ê≤πÊ∞¥ÂèØÊ¶®‰∫ÜÂêß„ÄÇÂèØÂΩì‰∏™‰∫∫Âêß&lt;/p&gt; &#xA;&lt;p&gt;ËØ∑È¶ñÂÖàÈÖçÁΩÆÂ•ΩopenaiÁöÑapiÔºåÈöèÂêéÂ∞ÜpdfÁÆÄÂéÜ‰∏ä‰º†Âà∞Êñá‰ª∂Â§πauto_job_findÈáåÔºåÂëΩÂêç‰∏∫‚Äúmy_cover&#34;.ÈöèÂêéÊâßË°åwrite_response.pyÂç≥ÂèØ ‰ºöËá™Âä®ÁîüÊàêopenaiÁöÑassistantÔºåÂπ∂Âú®Êú¨Âú∞‰∫ßÁîü‰∏Ä‰∏™.jsonÊñá‰ª∂ÔºåÂè™ÊúâÁ¨¨‰∏ÄÊ¨°ËøêË°åÁöÑÊó∂ÂÄôÊâç‰ºö‰∫ßÁîüÔºåÂêéÈù¢ÊØèÊ¨°ËøêË°åÂ¶ÇÊûúÊ£ÄÊµãÂà∞Ëøô‰∏™jsonÔºåÂ∞±‰ºöË∞ÉÁî®Â∑≤ÊúâÁöÑassistant&lt;/p&gt; &#xA;&lt;p&gt;ÂÖ≥‰∫éopenaiÈÉ®ÂàÜÁöÑÂåÖÔºö openai&lt;/p&gt; &#xA;&lt;p&gt;About RPA tutorial video about how to learn rpa: &lt;a href=&#34;https://www.youtube.com/watch?v=65OPFmEgCbM&amp;amp;list=PLx4LEkEdFArgrdD_lvXe_hYBy8zM0Sp3b&amp;amp;index=1&#34;&gt;https://www.youtube.com/watch?v=65OPFmEgCbM&amp;amp;list=PLx4LEkEdFArgrdD_lvXe_hYBy8zM0Sp3b&amp;amp;index=1&lt;/a&gt; Package of RPA selenium robotframework robotframework-seleniumlibrary robotframework-pythonlibcore&lt;/p&gt; &#xA;&lt;p&gt;Plugin: Intellibot@Selenium Library&lt;/p&gt; &#xA;&lt;p&gt;------------------‰∏ãÈù¢ÊòØÁÆÄÂçïÁöÑÊïôÂ≠¶ËßÜÈ¢ë---------------------&lt;/p&gt; &#xA;&lt;p&gt;BÁ´ôÈìæÊé•Ôºö„ÄêËµõÂçöÊäïÁÆÄÂéÜËÑöÊú¨ÊïôÁ®ã„Äë &lt;a href=&#34;https://www.bilibili.com/video/BV1UC4y1N78v/?share_source=copy_web&amp;amp;vd_source=b2608434484091fcc64d4eb85233122d&#34;&gt;https://www.bilibili.com/video/BV1UC4y1N78v/?share_source=copy_web&amp;amp;vd_source=b2608434484091fcc64d4eb85233122d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ê≤πÁÆ°ÈìæÊé•Ôºö&lt;a href=&#34;https://youtu.be/TlnytEi2lD8?si=jfcDj2MZqBptziZc&#34;&gt;https://youtu.be/TlnytEi2lD8?si=jfcDj2MZqBptziZc&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>