<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-28T01:46:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>embedchain/embedchain</title>
    <updated>2023-06-28T01:46:54Z</updated>
    <id>tag:github.com,2023-06-28:/embedchain/embedchain</id>
    <link href="https://github.com/embedchain/embedchain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Framework to easily create LLM powered bots over any dataset.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;embedchain&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/nhvCbCtKV&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/nhvCbCtKV?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/embedchain&#34; alt=&#34;PyPI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;embedchain is a framework to easily create LLM powered bots over any dataset.&lt;/p&gt; &#xA;&lt;p&gt;It abstracts the entire process of loading a dataset, chunking it, creating embeddings and then storing in a vector database.&lt;/p&gt; &#xA;&lt;p&gt;You can add a single or multiple dataset using &lt;code&gt;.add&lt;/code&gt; and &lt;code&gt;.add_local&lt;/code&gt; function and then use &lt;code&gt;.query&lt;/code&gt; function to find an answer from the added datasets.&lt;/p&gt; &#xA;&lt;p&gt;If you want to create a Naval Ravikant bot which has 1 youtube video, 1 book as pdf and 2 of his blog posts, as well as a question and answer pair you supply, all you need to do is add the links to the videos, pdf and blog posts and the QnA pair and embedchain will create a bot for you.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from embedchain import App&#xA;&#xA;naval_chat_bot = App()&#xA;&#xA;# Embed Online Resources&#xA;naval_chat_bot.add(&#34;youtube_video&#34;, &#34;https://www.youtube.com/watch?v=3qHkcs3kG44&#34;)&#xA;naval_chat_bot.add(&#34;pdf_file&#34;, &#34;https://navalmanack.s3.amazonaws.com/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_Final.pdf&#34;)&#xA;naval_chat_bot.add(&#34;web_page&#34;, &#34;https://nav.al/feedback&#34;)&#xA;naval_chat_bot.add(&#34;web_page&#34;, &#34;https://nav.al/agi&#34;)&#xA;&#xA;# Embed Local Resources&#xA;naval_chat_bot.add_local(&#34;qna_pair&#34;, (&#34;Who is Naval Ravikant?&#34;, &#34;Naval Ravikant is an Indian-American entrepreneur and investor.&#34;))&#xA;&#xA;naval_chat_bot.query(&#34;What unique capacity does Naval argue humans possess when it comes to understanding explanations or concepts?&#34;)&#xA;# answer: Naval argues that humans possess the unique capacity to understand explanations or concepts to the maximum extent possible in this physical reality.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First make sure that you have the package installed. If not, then install it using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install embedchain&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;We use OpenAI&#39;s embedding model to create embeddings for chunks and ChatGPT API as LLM to get answer given the relevant docs. Make sure that you have an OpenAI account and an API key. If you have dont have an API key, you can create one by visiting &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once you have the API key, set it in an environment variable called &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;sk-xxxx&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Next import the &lt;code&gt;App&lt;/code&gt; class from embedchain and use &lt;code&gt;.add&lt;/code&gt; function to add any dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from embedchain import App&#xA;&#xA;naval_chat_bot = App()&#xA;&#xA;# Embed Online Resources&#xA;naval_chat_bot.add(&#34;youtube_video&#34;, &#34;https://www.youtube.com/watch?v=3qHkcs3kG44&#34;)&#xA;naval_chat_bot.add(&#34;pdf_file&#34;, &#34;https://navalmanack.s3.amazonaws.com/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_Final.pdf&#34;)&#xA;naval_chat_bot.add(&#34;web_page&#34;, &#34;https://nav.al/feedback&#34;)&#xA;naval_chat_bot.add(&#34;web_page&#34;, &#34;https://nav.al/agi&#34;)&#xA;&#xA;# Embed Local Resources&#xA;naval_chat_bot.add_local(&#34;qna_pair&#34;, (&#34;Who is Naval Ravikant?&#34;, &#34;Naval Ravikant is an Indian-American entrepreneur and investor.&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If there is any other app instance in your script or app, you can change the import as&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from embedchain import App as EmbedChainApp&#xA;&#xA;# or&#xA;&#xA;from embedchain import App as ECApp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now your app is created. You can use &lt;code&gt;.query&lt;/code&gt; function to get the answer for any query.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(naval_chat_bot.query(&#34;What unique capacity does Naval argue humans possess when it comes to understanding explanations or concepts?&#34;))&#xA;# answer: Naval argues that humans possess the unique capacity to understand explanations or concepts to the maximum extent possible in this physical reality.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Format supported&lt;/h2&gt; &#xA;&lt;p&gt;We support the following formats:&lt;/p&gt; &#xA;&lt;h3&gt;Youtube Video&lt;/h3&gt; &#xA;&lt;p&gt;To add any youtube video to your app, use the data_type (first argument to &lt;code&gt;.add&lt;/code&gt;) as &lt;code&gt;youtube_video&lt;/code&gt;. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add(&#39;youtube_video&#39;, &#39;a_valid_youtube_url_here&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;PDF File&lt;/h3&gt; &#xA;&lt;p&gt;To add any pdf file, use the data_type as &lt;code&gt;pdf_file&lt;/code&gt;. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add(&#39;pdf_file&#39;, &#39;a_valid_url_where_pdf_file_can_be_accessed&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that we do not support password protected pdfs.&lt;/p&gt; &#xA;&lt;h3&gt;Web Page&lt;/h3&gt; &#xA;&lt;p&gt;To add any web page, use the data_type as &lt;code&gt;web_page&lt;/code&gt;. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add(&#39;web_page&#39;, &#39;a_valid_web_page_url&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Text&lt;/h3&gt; &#xA;&lt;p&gt;To supply your own text, use the data_type as &lt;code&gt;text&lt;/code&gt; and enter a string. The text is not processed, this can be very versatile. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add_local(&#39;text&#39;, &#39;Seek wealth, not money or status. Wealth is having assets that earn while you sleep. Money is how we transfer time and wealth. Status is your place in the social hierarchy.&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: This is not used in the examples because in most cases you will supply a whole paragraph or file, which did not fit.&lt;/p&gt; &#xA;&lt;h3&gt;QnA Pair&lt;/h3&gt; &#xA;&lt;p&gt;To supply your own QnA pair, use the data_type as &lt;code&gt;qna_pair&lt;/code&gt; and enter a tuple. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add_local(&#39;qna_pair&#39;, (&#34;Question&#34;, &#34;Answer&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;More Formats coming soon&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you want to add any other format, please create an &lt;a href=&#34;https://github.com/embedchain/embedchain/issues&#34;&gt;issue&lt;/a&gt; and we will add it to the list of supported formats.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How does it work?&lt;/h1&gt; &#xA;&lt;p&gt;Creating a chat bot over any dataset needs the following steps to happen&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;load the data&lt;/li&gt; &#xA; &lt;li&gt;create meaningful chunks&lt;/li&gt; &#xA; &lt;li&gt;create embeddigns for each chunk&lt;/li&gt; &#xA; &lt;li&gt;store the chunks in vector database&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Whenever a user asks any query, following process happens to find the answer for the query&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;create the embedding for query&lt;/li&gt; &#xA; &lt;li&gt;find similar documents for this query from vector database&lt;/li&gt; &#xA; &lt;li&gt;pass similar documents as context to LLM to get the final answer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The process of loading the dataset and then querying involves multiple steps and each steps has nuances of it is own.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How should I chunk the data? What is a meaningful chunk size?&lt;/li&gt; &#xA; &lt;li&gt;How should I create embeddings for each chunk? Which embedding model should I use?&lt;/li&gt; &#xA; &lt;li&gt;How should I store the chunks in vector database? Which vector database should I use?&lt;/li&gt; &#xA; &lt;li&gt;Should I store meta data along with the embeddings?&lt;/li&gt; &#xA; &lt;li&gt;How should I find similar documents for a query? Which ranking model should I use?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These questions may be trivial for some but for a lot of us, it needs research, experimentation and time to find out the accurate answers.&lt;/p&gt; &#xA;&lt;p&gt;embedchain is a framework which takes care of all these nuances and provides a simple interface to create bots over any dataset.&lt;/p&gt; &#xA;&lt;p&gt;In the first release, we are making it easier for anyone to get a chatbot over any dataset up and running in less than a minute. All you need to do is create an app instance, add the data sets using &lt;code&gt;.add&lt;/code&gt; function and then use &lt;code&gt;.query&lt;/code&gt; function to get the relevant answer.&lt;/p&gt; &#xA;&lt;h1&gt;Tech Stack&lt;/h1&gt; &#xA;&lt;p&gt;embedchain is built on the following stack:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;Langchain&lt;/a&gt; as an LLM framework to load, chunk and index data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings&#34;&gt;OpenAI&#39;s Ada embedding model&lt;/a&gt; to create embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/chat-completions-api&#34;&gt;OpenAI&#39;s ChatGPT API&lt;/a&gt; as LLM to get answers given the context&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chroma-core/chroma&#34;&gt;Chroma&lt;/a&gt; as the vector database to store embeddings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Author&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Taranjeet Singh (&lt;a href=&#34;https://twitter.com/taranjeetio&#34;&gt;@taranjeetio&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>TermuxHackz/X-osint</title>
    <updated>2023-06-28T01:46:54Z</updated>
    <id>tag:github.com,2023-06-28:/TermuxHackz/X-osint</id>
    <link href="https://github.com/TermuxHackz/X-osint" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is an Open source intelligent framework ie an osint tool which gathers valid information about a phone number, user&#39;s email address, perform VIN Osint, and reverse, perform subdomain enumeration, able to find email from a name, and so much more. Best osint tool for Termux and linux&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;X-osint&lt;/h1&gt; &#xA;&lt;p&gt;This is an osint tool which gathers useful and yet credible valid information about a phone number, user&#39;s email address and ip address and more to come in future updates &lt;img src=&#34;https://raw.githubusercontent.com/TermuxHackz/X-osint/master/images/x-osint_banner_white_texts.png&#34; float=&#34;center&#34;&gt;&lt;/p&gt; &#xA;&lt;center&gt; &#xA; &lt;h2&gt;&lt;img src=&#34;https://img.shields.io/badge/Author-AnonyminHack5-blueviolet&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Followers-1.6k-blue&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Tool-X--osint-red&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Made%20with-Python%20and%20bash-yellowgreen&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Maintained-YES-green&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Version-2.1-9cf&#34;&gt; &lt;/h2&gt;&#xA;&lt;/center&gt;  &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Menu&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/TermuxHackz/X-osint/master/images/X-osintv2.1.1.png&#34; alt=&#34;X-osintv2.1&#34; float=&#34;center&#34;&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;IP Address information gathering&lt;/li&gt; &#xA; &lt;li&gt;Email Address information gathering&lt;/li&gt; &#xA; &lt;li&gt;Phone number information gathering&lt;/li&gt; &#xA; &lt;li&gt;Host finding&lt;/li&gt; &#xA; &lt;li&gt;Ports finding&lt;/li&gt; &#xA; &lt;li&gt;Subdomain Enumeration&lt;/li&gt; &#xA; &lt;li&gt;CVE Exploits Finder&lt;/li&gt; &#xA; &lt;li&gt;Email Finder&lt;/li&gt; &#xA; &lt;li&gt;Exploit Open Source Vulnerability Database&lt;/li&gt; &#xA; &lt;li&gt;DNS Lookup&lt;/li&gt; &#xA; &lt;li&gt;DNS Reverse&lt;/li&gt; &#xA; &lt;li&gt;Vin extractor&lt;/li&gt; &#xA; &lt;li&gt;Protonmail OSINT And many more...&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;b&gt;MANY OTHER FEATURES SOON TO COME &lt;/b&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Report bugs&lt;/h1&gt; &#xA;&lt;p&gt;If you notice issues while installing this tool or running this tool kindly mail to me at &lt;a href=&#34;mailto: AnonyminHack5@protonmail.com&#34;&gt;Gmail&lt;/a&gt; or Open an issue via github.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python-3&#xA;pip&#xA;Internet Connection&#xA;And some other python packages&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;How to Update Manually (For any version)&lt;/h1&gt; &#xA;&lt;p&gt;This tool would be updated regularly or as time progresses to improve it, fix more bugs and add so many other features, I would be showing you how to update it&lt;/p&gt; &#xA;&lt;h5&gt;&lt;u&gt;How to Update For Termux&lt;/u&gt;&lt;/h5&gt; &#xA;&lt;b&gt;ALSO TYPE THE DOLLAR SIGN &lt;/b&gt; &#xA;&lt;h4&gt;1) Type:&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;cd $HOME&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;cd $PREFIX/bin&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;rm xosint&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2) Re-clone from git:&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;cd $HOME&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;git clone &lt;a href=&#34;https://github.com/TermuxHackz/X-osint&#34;&gt;https://github.com/TermuxHackz/X-osint&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;cd X-osint&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;3) Grant permissions and run install file&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;chmod +x *&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;bash setup.sh&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;And your all done!!!..and updated&lt;/p&gt; &#xA;&lt;h3&gt;&lt;u&gt;How to Update for Linux&lt;/u&gt;&lt;/h3&gt; &#xA;&lt;b&gt;ALSO TYPE THE DOLLAR SIGN &lt;/b&gt; &#xA;&lt;h4&gt;1) Type:&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;cd $HOME&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;cd /usr/local/bin&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;sudo rm xosint&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2) Re-clone from GitHub&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;cd $HOME&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;git clone &lt;a href=&#34;https://github.com/TermuxHackz/X-osint&#34;&gt;https://github.com/TermuxHackz/X-osint&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;cd X-osint&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;3) Grant permissions and run install file&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;chmod +x *&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;bash setup.sh&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;And your all done!!!..and updated&lt;/p&gt; &#xA;&lt;h1&gt;Demo Installation&lt;/h1&gt; &#xA;&lt;p&gt;Here is a video demonstration below that shows how to install X-osint in your various terminal(s)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TermuxHackz/X-osint/raw/master/Demo/Install-Xosint.mp4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TermuxHackz/X-osint/master/Demo/install.gif&#34; alt=&#34;Install X-osint2.1&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;Youtube Video Demo here&lt;/h5&gt; &#xA;&lt;a href=&#34;https://www.youtube.com/watch?feature=player_embedded&amp;amp;v=ikU1RHNVVuk&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TermuxHackz/X-osint/master/images/X-osintv2.1.1.png&#34; alt=&#34;Watch the Video installation&#34; width=&#34;340&#34; height=&#34;180&#34; border=&#34;10&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd $HOME&#xA;git clone https://github.com/TermuxHackz/X-osint&#xA;cd X-osint&#xA;chmod +x *&#xA;bash setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;How to update Automatically (if your using version 2.1 of X-Osint and above)&lt;/h1&gt; &#xA;&lt;h4&gt;For Termux&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd $HOME&#xA;&#xA;xosint&#xA;&#xA;And then from the menu Type 99 and proceed to selecting termux&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt; For linux&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd $HOME&#xA;&#xA;sudo xosint&#xA;&#xA;And from the menu Type 99 and proceed to selecting linux&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt; NOTICE &lt;/h3&gt; &#xA;&lt;p&gt;If you are using the Subdomains feature and it ask for a word list, please download from &lt;a href=&#34;https://www.mediafire.com/file/k60ooi301s4vkfo/subdomains.txt/file&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and then extract the zip, make sure you know the location where it is kept, then proceed with using subdomain&lt;/p&gt; &#xA;&lt;h1&gt;API&lt;/h1&gt; &#xA;&lt;p&gt;Get your various API keys&lt;/p&gt; &#xA;&lt;h4&gt;1) Shodan (&lt;a href=&#34;https://shodan.io&#34;&gt;https://shodan.io&lt;/a&gt;) (Number 4 - 9 from my tool will require a shodan API key, Sign up on shodan and paste your API and begin to use flawlessly)&lt;/h4&gt; &#xA;&lt;h4&gt;2) Hunter (&lt;a href=&#34;https://hunter.io&#34;&gt;https://hunter.io&lt;/a&gt;)&lt;/h4&gt; &#xA;&lt;h4&gt;3) Opencagedata (&lt;a href=&#34;https://opencagedata.com&#34;&gt;https://opencagedata.com&lt;/a&gt;): Use this for geolocation of numbers, And get your API from here &lt;b&gt;THIS WOULD BE REQUIRED IN PHONE NUMBER INFORMATION&amp;lt;+, SO SIGN UP AND GET YOUR API TO USE&lt;/b&gt;&lt;/h4&gt;&#xA;&lt;b&gt; &#xA; &lt;hr&gt; &lt;h1&gt;How to create a desktop Launcher for X-osint in Linux&lt;/h1&gt; &lt;h4&gt;1) Go to your home desktop, right click then click on Create Launcher&lt;/h4&gt; &lt;h4&gt;2) Fill the field as follows&lt;/h4&gt; &lt;p&gt;Name: X-osint&lt;/p&gt; &lt;p&gt;Comment: An osint tool made by AnonyminHack5 in python3&lt;/p&gt; &lt;p&gt;Command: sudo xosint&lt;/p&gt; &lt;p&gt;Working Directory: /usr/local/bin&lt;/p&gt; &lt;p&gt;Icon: Click the No icon button and add an icon, and then Go to my github, &lt;a href=&#34;https://github.com/TermuxHackz/X-osint/tree/master/Icons&#34;&gt;and download the .ico image there&lt;/a&gt; then select that as your Icon and thats it&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/TermuxHackz/X-osint/raw/master/Demo/create-launcher.mp4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TermuxHackz/X-osint/master/Demo/create-launcher.gif&#34; alt=&#34;Create Launcher&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &lt;h4&gt;3) For the Options&lt;/h4&gt; &lt;p&gt;Tick&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Run in terminal&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;hr&gt; &lt;pre&gt;&lt;code&gt;Works for Termux and Linux &#xA;Tested &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;License Plate OSINT&lt;/h1&gt; &lt;p&gt;X-osint is able to fetch and provide information about a car license plate easily, which only works for license plates registered in the United States, States that this feature works for include: Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa and Distric of Columbia. KIndly note that this feature doesnt work for license plate registered in another country, how ever i may add such a feature but to do so i need encouragement hence, you supporting this project by Starring it and Buying me a cup of coffee. Thanks&lt;/p&gt; &lt;h1&gt;Google Dork Hacking&lt;/h1&gt; &lt;p&gt;X-osint provides a way by which you can use Google for hacking once you know how to the particular search queries to perform, I have provided some useful google dork queries in this repo code, kindly view and use. Thank you.&lt;/p&gt; &lt;h1&gt;Movie Database&lt;/h1&gt; &lt;p&gt;X-Osint uses the lastest information from IMDB To give search results based on you choice of options to give you information on Actor, Movie Name, Keyword names, Company name of the Movie, and also check if the actor was starred in the Movie or not&lt;/p&gt; &lt;h1&gt;SMTP Analysis&lt;/h1&gt; &lt;p&gt;X-osint is able to perform an SMTP Analysis and enumerate if an SMTP server is vulnerable or not&lt;/p&gt; &lt;h1&gt;VIN Number Identification&lt;/h1&gt; &lt;p&gt;X-osint is able to gather information from a gov database and display the list or infos of vehicles based on their Identification numbers. X-osint is able to do that flawlessly without need for an API. VIN is available to use Via CLI or the GUI&lt;/p&gt; &lt;h1&gt;ProtonMail OSINT&lt;/h1&gt; &lt;p&gt;Credits to pixelbubble, X-Osint is able to perform OSINT investigation on Proton service (for educational purposes only).&lt;br&gt; ProtOSINT is separated in 3 sub-modules:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[1] Test the validity of one protonMail account and get additional information&lt;/li&gt; &#xA;  &lt;li&gt;[2] Try to find if your target have a protonMail account by generating multiple adresses by combining information fields inputted&lt;/li&gt; &#xA;  &lt;li&gt;[3] Find if your IP is currently affiliate to ProtonVPN&lt;/li&gt; &#xA;  &lt;li&gt;[4] Find a protonmail user PGP Key and download it right from your terminal And so many More&lt;/li&gt; &#xA; &lt;/ul&gt; &lt;h1&gt;Demo protonmail OSINT&lt;/h1&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/TermuxHackz/X-osint/raw/master/Demo/protonmail-xosint.mp4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TermuxHackz/X-osint/master/Demo/protonmail-osint.gif&#34; alt=&#34;Protonmail osint&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More features are still to come..Stay Tuned&lt;/p&gt; &lt;h1&gt;ChangeLogs&lt;/h1&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[1] Fixed updating&lt;/li&gt; &#xA;  &lt;li&gt;[2] Fixed Number 3 Option error&lt;/li&gt; &#xA;  &lt;li&gt;[3] Changed Banner&lt;/li&gt; &#xA;  &lt;li&gt;[4] Changed User interface&lt;/li&gt; &#xA;  &lt;li&gt;[5] Added Features&lt;/li&gt; &#xA;  &lt;li&gt;[6] Improved Speed&lt;/li&gt; &#xA;  &lt;li&gt;[7] And thats about it, if you face any errors or bugs kindly mail them to me or open an Issue in github&lt;/li&gt; &#xA; &lt;/ul&gt; &lt;h1&gt;Buy me a coffee&lt;/h1&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TermuxHackz/X-osint/master/images/images.png&#34;&gt;&lt;br&gt; Love my work and wish to support me, Buy me a coffee &lt;a href=&#34;https://www.buymeacoffee.com/AnonyminHack5&#34; target=&#34;_blank&#34;&gt;here &lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Contributing&lt;/h2&gt; &lt;p&gt;Feel free to clone this project. For major changes, please open an issue first to discuss what you would like to change or add, thank you!!.&lt;/p&gt; &lt;h2&gt;Credits&lt;/h2&gt; &lt;p&gt;Some of the modules here and APIs used for the creation of X-osint, got the idea from them, and so i would like to give them credit&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[1] Pixellbubble&lt;/li&gt; &#xA;  &lt;li&gt;[2] C3n7ral051nt4g3ncy&lt;/li&gt; &#xA;  &lt;li&gt;[3] SpiderAnonGreyHat&lt;/li&gt; &#xA; &lt;/ul&gt; &lt;h1&gt;Faqs&lt;/h1&gt; &lt;h2&gt;If your getting the error below which says&lt;/h2&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;sudo xosint Traceback (most recent call last): File &#34;/usr/local/bin/xosint&#34;, line 11, in from googlesearch import search ModuleNotFoundError: No module named &#39;googlesearch&#39;&lt;/li&gt; &#xA; &lt;/ol&gt; &lt;p&gt;&lt;u&gt;Solution: &lt;/u&gt; Kindly make sure you ran the &lt;code&gt;bash setup.sh&lt;/code&gt; file and make sure you don&#39;t interrupt the setup process and after you run that, and doesnt still work type &lt;code&gt;pip install google&lt;/code&gt; and re run xosint. if your still having issue with it run &lt;code&gt;pip install googlesearch-python&lt;/code&gt; and run xosint. That should solve your problem with xosint. Thank you and share to friends.&lt;/p&gt; &lt;/b&gt;</summary>
  </entry>
  <entry>
    <title>hiyouga/ChatGLM-Efficient-Tuning</title>
    <updated>2023-06-28T01:46:54Z</updated>
    <id>tag:github.com,2023-06-28:/hiyouga/ChatGLM-Efficient-Tuning</id>
    <link href="https://github.com/hiyouga/ChatGLM-Efficient-Tuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fine-tuning ChatGLM-6B with PEFT | åŸºäºŽ PEFT çš„é«˜æ•ˆ ChatGLM å¾®è°ƒ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGLM Efficient Tuning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning&#34; alt=&#34;GitHub Code License&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue&#34; alt=&#34;GitHub pull request&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Fine-tuning ðŸ¤–&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt; model with ðŸ¤—&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ‘‹ Join our &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/assets/wechat.jpg&#34;&gt;WeChat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[ English | &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/README_zh.md&#34;&gt;ä¸­æ–‡&lt;/a&gt; ]&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[23/06/25] Now we align the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/src/api_demo.py&#34;&gt;demo API&lt;/a&gt; with the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI&#39;s&lt;/a&gt; format where you can insert the fine-tuned model in arbitrary ChatGPT-based applications.&lt;/p&gt; &#xA;&lt;p&gt;[23/06/25] Now we support fine-tuning the &lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;&gt;ChatGLM2-6B&lt;/a&gt; model with our framework! Try &lt;code&gt;--use_v2&lt;/code&gt; argument to fine-tune that model.&lt;/p&gt; &#xA;&lt;p&gt;[23/06/05] Now we support 4-bit LoRA training (aka &lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt;). Try &lt;code&gt;--quantization_bit 4&lt;/code&gt; argument to work with 4-bit quantized model. (experimental feature)&lt;/p&gt; &#xA;&lt;p&gt;[23/06/01] We implemented a framework supporting the efficient tuning of LLaMA and BLOOM models. Please follow &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Efficient-Tuning&#34;&gt;LLaMA-Efficient-Tuning&lt;/a&gt; if you are interested.&lt;/p&gt; &#xA;&lt;p&gt;[23/05/19] Now we support using the development set to evaluate the model while training. Try &lt;code&gt;--dev_ratio&lt;/code&gt; argument to specify the size of development set.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/29] Now we support training ChatGLM with &lt;strong&gt;Reinforcement Learning with Human Feedback (RLHF)&lt;/strong&gt; ! We provide several examples to run RLHF training, please refer to the &lt;code&gt;examples&lt;/code&gt; folder for details.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/20] Our repo achieved 100 stars within 12 days! Congratulations!&lt;/p&gt; &#xA;&lt;p&gt;[23/04/19] Now we support &lt;strong&gt;merging the weights&lt;/strong&gt; of fine-tuned models trained by LoRA! Try &lt;code&gt;--checkpoint_dir checkpoint1,checkpoint2&lt;/code&gt; argument for continually fine-tuning the models.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/18] Now we support training the &lt;strong&gt;quantized models&lt;/strong&gt; using three fine-tuning methods! Try &lt;code&gt;quantization_bit&lt;/code&gt; argument for training the model in 4/8 bits.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/12] Now we support &lt;strong&gt;training from checkpoints&lt;/strong&gt;! Use &lt;code&gt;--checkpoint_dir&lt;/code&gt; argument to specify the checkpoint model to fine-tune from.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/11] Now we support training with &lt;strong&gt;combined datasets&lt;/strong&gt;! Try &lt;code&gt;--dataset dataset1,dataset2&lt;/code&gt; argument for training with multiple datasets.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Our script now supports the following datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Stanford Alpaca (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_2M_CN&#34;&gt;BELLE 2M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BELLE 1M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BELLE 0.5M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&#34;&gt;BELLE Dialogue 0.4M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&#34;&gt;BELLE School Math 0.25M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BELLE Multiturn Chat 0.8M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;Guanaco Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;Firefly 1.1M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&#34;&gt;CodeAlpaca 20k&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&#34;&gt;Alpaca CoT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/suolyer/webqa&#34;&gt;Web QA (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;UltraChat&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your HuggingFace account using these commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade huggingface_hub&#xA;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine-Tuning Methods&lt;/h2&gt; &#xA;&lt;p&gt;Our script now supports the following fine-tuning methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning the low-rank adapters of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/P-tuning-v2&#34;&gt;P-Tuning V2&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning the prefix encoder of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.14913&#34;&gt;Freeze&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning the MLPs in the last n blocks of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Full Tuning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning all the parameters of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+ and PyTorch 1.13.1&lt;/li&gt; &#xA; &lt;li&gt;ðŸ¤—Transformers, Datasets, Accelerate, PEFT and TRL&lt;/li&gt; &#xA; &lt;li&gt;protobuf, cpm_kernels and sentencepiece&lt;/li&gt; &#xA; &lt;li&gt;jieba, rouge_chinese and nltk (used at evaluation)&lt;/li&gt; &#xA; &lt;li&gt;gradio and mdtex2html (used in web_demo.py)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And &lt;strong&gt;powerful GPUs&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Data Preparation (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;data/example_dataset&lt;/code&gt; for checking the details about the format of dataset files. You can either use a single &lt;code&gt;.json&lt;/code&gt; file or a &lt;a href=&#34;https://huggingface.co/docs/datasets/dataset_script&#34;&gt;dataset loading script&lt;/a&gt; with multiple files to create a custom dataset.&lt;/p&gt; &#xA;&lt;p&gt;Note: please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset. About the format of this file, please refer to &lt;code&gt;data/README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Dependence Installation (optional)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git&#xA;conda create -n chatglm_etuning python=3.10&#xA;conda activate chatglm_etuning&#xA;cd ChatGLM-Efficient-Tuning&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to enable LoRA or Freeze quantization on Windows, you will be required to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.6 or 11.7.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install https://github.com/acpopescu/bitsandbytes/releases/download/v0.37.2-win.1/bitsandbytes-0.37.2-py3-none-any.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning with a Single GPU&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_sft_checkpoint \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 5e-5 \&#xA;    --num_train_epochs 3.0 \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki&#34;&gt;Wiki&lt;/a&gt; about the details of the arguments.&lt;/p&gt; &#xA;&lt;h3&gt;Distributed Fine-tuning with Multiple GPUs&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate config # configure the environment&#xA;accelerate launch src/train_sft.py # arguments (same as above)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: if you are using LoRA method at fine-tuning, please provide &lt;code&gt;--ddp_find_unused_parameters False&lt;/code&gt; argument to avoid the runtime error.&lt;/p&gt; &#xA;&lt;h3&gt;Training Reward Model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_rm.py \&#xA;    --do_train \&#xA;    --dataset comparison_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_rm_checkpoint \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training with RLHF&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_ppo.py \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_sft_checkpoint \&#xA;    --reward_model path_to_rm_checkpoint \&#xA;    --output_dir path_to_ppo_checkpoint \&#xA;    --per_device_train_batch_size 2 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation (BLEU and ROUGE_CHINESE)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --do_eval \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_eval_result \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --max_samples 50 \&#xA;    --predict_with_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Predict&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --do_predict \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_predict_result \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --max_samples 50 \&#xA;    --predict_with_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/cli_demo.py \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Web Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/web_demo.py \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Export model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/export_model.py \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_export&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Hardware Requirements&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Fine-tune method&lt;/th&gt; &#xA;   &lt;th&gt;Batch size&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;28GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;10GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;8GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;P-Tuning (p=16)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;P-Tuning (p=16)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;P-Tuning (p=16)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Freeze (l=3)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Freeze (l=3)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;RM method&lt;/th&gt; &#xA;   &lt;th&gt;Batch size&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + rm&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;22GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + rm&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;11GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;RLHF method&lt;/th&gt; &#xA;   &lt;th&gt;Batch size&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + ppo&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;23GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + ppo&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: &lt;code&gt;r&lt;/code&gt; is the lora rank, &lt;code&gt;p&lt;/code&gt; is the number of prefix tokens, &lt;code&gt;l&lt;/code&gt; is the number of trainable layers, &lt;code&gt;ex/s&lt;/code&gt; is the examples per second at training. The &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; is set to &lt;code&gt;1&lt;/code&gt;. All are evaluated on a single Tesla V100 (32G) GPU, they are approximated values and may vary in different GPUs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Fine-tuning ChatGLM: A Case&lt;/h2&gt; &#xA;&lt;h3&gt;Training Results&lt;/h3&gt; &#xA;&lt;p&gt;We use the whole &lt;code&gt;alpaca_gpt4_zh&lt;/code&gt; dataset to fine-tune the ChatGLM model with LoRA (r=8) for one epoch, using the default hyper-parameters. The loss curve during training is presented below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/assets/trainer_state.jpg&#34; alt=&#34;training loss&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation Results&lt;/h3&gt; &#xA;&lt;p&gt;We select 100 instances in the &lt;code&gt;alpaca_gpt4_zh&lt;/code&gt; dataset to evaluate the fine-tuned ChatGLM model and compute the BLEU and ROUGE scores. The results are presented below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Score&lt;/th&gt; &#xA;   &lt;th&gt;Original&lt;/th&gt; &#xA;   &lt;th&gt;FZ (l=2)&lt;/th&gt; &#xA;   &lt;th&gt;PT (p=16)&lt;/th&gt; &#xA;   &lt;th&gt;LoRA (r=8)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLEU-4&lt;/td&gt; &#xA;   &lt;td&gt;15.75&lt;/td&gt; &#xA;   &lt;td&gt;16.85&lt;/td&gt; &#xA;   &lt;td&gt;16.06&lt;/td&gt; &#xA;   &lt;td&gt;17.01 (&lt;strong&gt;+1.26&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rouge-1&lt;/td&gt; &#xA;   &lt;td&gt;34.51&lt;/td&gt; &#xA;   &lt;td&gt;36.62&lt;/td&gt; &#xA;   &lt;td&gt;34.80&lt;/td&gt; &#xA;   &lt;td&gt;36.77 (&lt;strong&gt;+2.26&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rouge-2&lt;/td&gt; &#xA;   &lt;td&gt;15.11&lt;/td&gt; &#xA;   &lt;td&gt;17.04&lt;/td&gt; &#xA;   &lt;td&gt;15.32&lt;/td&gt; &#xA;   &lt;td&gt;16.83 (&lt;strong&gt;+1.72&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rouge-l&lt;/td&gt; &#xA;   &lt;td&gt;26.18&lt;/td&gt; &#xA;   &lt;td&gt;28.17&lt;/td&gt; &#xA;   &lt;td&gt;26.35&lt;/td&gt; &#xA;   &lt;td&gt;28.86 (&lt;strong&gt;+2.68&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Params (%)&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;4.35%&lt;/td&gt; &#xA;   &lt;td&gt;0.06%&lt;/td&gt; &#xA;   &lt;td&gt;0.06%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;FZ: freeze tuning, PT: P-Tuning V2 (we use &lt;code&gt;pre_seq_len=16&lt;/code&gt; for fair comparison with LoRA), Params: the percentange of trainable parameters.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Compared with Existing Implementations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning&#34;&gt;THUDM/ChatGLM-6B&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Official implementation of fine-tuning ChatGLM with &lt;a href=&#34;https://github.com/THUDM/P-tuning-v2&#34;&gt;P-Tuning v2&lt;/a&gt; on the &lt;a href=&#34;https://aclanthology.org/D19-1321.pdf&#34;&gt;ADGEN&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;Our fine-tuning script is largely depend on it. We further implement the &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; tuning method. Additionally, we &lt;strong&gt;dynamically&lt;/strong&gt; pad the inputs to the longest sequence in the batch instead of the maximum length, to accelerate the fine-tuning.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mymusise/ChatGLM-Tuning&#34;&gt;mymusise/ChatGLM-Tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unoffical implementation of fine-tuning ChatGLM with &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;We borrowed some ideas from it. Our fine-tuning script &lt;strong&gt;integrates&lt;/strong&gt; the data pre-processing part into the training procedure, so we need not generate a pre-processed dataset before training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ssbuild/chatglm_finetuning&#34;&gt;ssbuild/chatglm_finetuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM with several PEFT methods on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;Our fine-tuning script is implemented &lt;strong&gt;purely&lt;/strong&gt; with &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface transformers&lt;/a&gt; and is independent of the &lt;a href=&#34;https://github.com/ssbuild/deep_training&#34;&gt;deep_training&lt;/a&gt; framework.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lich99/ChatGLM-finetune-LoRA&#34;&gt;lich99/ChatGLM-finetune-LoRA&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM with &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;We use the &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;Huggingface PEFT&lt;/a&gt; to provide the state-of-the-art PEFT methods.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liucongg/ChatGLM-Finetuning&#34;&gt;liucongg/ChatGLM-Finetuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM with several methods including Freeze, LoRA and P-Tuning on the industrial dataset.&lt;/li&gt; &#xA;   &lt;li&gt;We are aim to incorporate more instruction-following datasets for fine-tuning the ChatGLM model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yanqiangmiffy/InstructGLM&#34;&gt;yanqiangmiffy/InstructGLM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM that explores the ChatGLM&#39;s ability on the instruction-following datasets.&lt;/li&gt; &#xA;   &lt;li&gt;Our fine-tuning script integrates the data pre-processing part in to the training procedure.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Employing &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; to easily build applications that are capable of leveraging external knowledge upon fine-tuned ChatGLM models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implementing the alignment algorithms to align human preferrences. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat&#34;&gt;RLHF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/GanjinZero/RRHF&#34;&gt;RRHF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/OptimalScale/LMFlow&#34;&gt;RAFT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;Chinese datasets&lt;/a&gt; into the training sets. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/CLUEbenchmark/pCLUE&#34;&gt;pCLUE&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/CLUEbenchmark/CLUECorpus2020&#34;&gt;CLUECorpus&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;GuanacoDataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;FireflyDataset&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://openai.com/blog/chatgpt&#34;&gt;ChatGPT&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://openai.com/research/gpt-4&#34;&gt;GPT-4&lt;/a&gt; self-chat data into the training sets. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/project-baize/baize-chatbot&#34;&gt;Baize&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4-LLM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implementing the Freeze-Tuning and P-Tuning method.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Supporting Multi-GPUs fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Adding script for evaluation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Loading from checkpoint.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fine-tuning the quantized model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Writing a guidebook about how to fine-tune ChatGLM with this framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Combining with state-of-the-art model editing algorithms. (&lt;em&gt;e.g. &lt;a href=&#34;https://arxiv.org/abs/2110.11309&#34;&gt;MEND&lt;/a&gt;&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating the &lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;OpenAssistant Conversations Dataset&lt;/a&gt; for SFT and alignment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating the high quality Chinese instruction dataset &lt;a href=&#34;https://huggingface.co/datasets/BAAI/COIG&#34;&gt;COIG&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;. Please follow the &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/raw/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt; to use ChatGLM-6B model.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Misc{chatglm-efficient-tuning,&#xA;  title = {ChatGLM Efficient Tuning},&#xA;  author = {hiyouga},&#xA;  howpublished = {\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;, &lt;a href=&#34;https://github.com/mymusise/ChatGLM-Tuning&#34;&gt;ChatGLM-Tuning&lt;/a&gt; and &lt;a href=&#34;https://github.com/yuanzhoulvpi2017/zero_nlp&#34;&gt;yuanzhoulvpi2017/zero_nlp&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=hiyouga/ChatGLM-Efficient-Tuning&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>