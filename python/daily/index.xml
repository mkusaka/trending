<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-14T01:33:04Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>open-compass/VLMEvalKit</title>
    <updated>2024-09-14T01:33:04Z</updated>
    <id>tag:github.com,2024-09-14:/open-compass/VLMEvalKit</id>
    <link href="https://github.com/open-compass/VLMEvalKit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source evaluation toolkit of large vision-language models (LVLMs), support ~100 VLMs, 40+ benchmarks&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;http://opencompass.openxlab.space/utils/MMLB.jpg&#34; alt=&#34;LOGO&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;b&gt;A Toolkit for Evaluating Large Vision-Language Models. &lt;/b&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/open-compass/VLMEvalKit/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/open-compass/VLMEvalKit?color=c4f042&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; • &lt;a href=&#34;https://github.com/open-compass/VLMEvalKit/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/open-compass/VLMEvalKit?color=8ae8ff&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; • &lt;a href=&#34;https://github.com/open-compass/VLMEvalKit/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/open-compass/VLMEvalKit?color=ffcb47&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; • &lt;a href=&#34;https://github.com/open-compass/VLMEvalKit/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/open-compass/VLMEvalKit?color=ff80eb&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; • &lt;a href=&#34;https://github.com/open-compass/VLMEvalKit/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-compass/VLMEvalKit?color=white&amp;amp;labelColor=black&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/zh-CN/README_zh-CN.md&#34;&gt;简体中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/ja/README_ja.md&#34;&gt;日本語&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://rank.opencompass.org.cn/leaderboard-multimodal&#34;&gt;🏆 OC Learderboard &lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#-datasets-models-and-evaluation-results&#34;&gt;📊Datasets &amp;amp; Models &lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#%EF%B8%8F-quickstart&#34;&gt;🏗️Quickstart &lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#%EF%B8%8F-development-guide&#34;&gt;🛠️Development &lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#-the-goal-of-vlmevalkit&#34;&gt;🎯Goal &lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#%EF%B8%8F-citation&#34;&gt;🖊️Citation &lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/opencompass/open_vlm_leaderboard&#34;&gt;🤗 HF Leaderboard&lt;/a&gt; • &lt;a href=&#34;https://huggingface.co/datasets/VLMEval/OpenVLMRecords&#34;&gt;🤗 Evaluation Records&lt;/a&gt; • &lt;a href=&#34;https://discord.gg/evDT4GZmxN&#34;&gt;🔊 Discord&lt;/a&gt; • &lt;a href=&#34;https://www.arxiv.org/abs/2407.11691&#34;&gt;📝 Report&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;VLMEvalKit&lt;/strong&gt; (the python package name is &lt;strong&gt;vlmeval&lt;/strong&gt;) is an &lt;strong&gt;open-source evaluation toolkit&lt;/strong&gt; of &lt;strong&gt;large vision-language models (LVLMs)&lt;/strong&gt;. It enables &lt;strong&gt;one-command evaluation&lt;/strong&gt; of LVLMs on various benchmarks, without the heavy workload of data preparation under multiple repositories. In VLMEvalKit, we adopt &lt;strong&gt;generation-based evaluation&lt;/strong&gt; for all LVLMs, and provide the evaluation results obtained with both &lt;strong&gt;exact matching&lt;/strong&gt; and &lt;strong&gt;LLM-based answer extraction&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🆕 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-09-09]&lt;/strong&gt; We have supported &lt;a href=&#34;https://arxiv.org/abs/2408.15556&#34;&gt;&lt;strong&gt;HRBench&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/DreamMr&#34;&gt;&lt;strong&gt;DreamMr&lt;/strong&gt;&lt;/a&gt; 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-09-09]&lt;/strong&gt; We have supported &lt;a href=&#34;https://github.com/yfzhang114/SliME&#34;&gt;&lt;strong&gt;SliME&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/yfzhang114&#34;&gt;&lt;strong&gt;Yifan zhang&lt;/strong&gt;&lt;/a&gt; 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-09-07]&lt;/strong&gt; We have supported &lt;a href=&#34;https://github.com/X-PLUG/mPLUG-Owl&#34;&gt;&lt;strong&gt;mPLUG-Owl3&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/SYuan03&#34;&gt;&lt;strong&gt;SYuan03&lt;/strong&gt;&lt;/a&gt; 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-09-07]&lt;/strong&gt; We have supported &lt;a href=&#34;https://github.com/QwenLM/Qwen2-VL&#34;&gt;&lt;strong&gt;Qwen2-VL&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/kq-chen&#34;&gt;&lt;strong&gt;kq-chen&lt;/strong&gt;&lt;/a&gt; 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-09-03]&lt;/strong&gt; We have supported &lt;a href=&#34;https://huggingface.co/RBDash-Team/RBDash-v1.2-72b&#34;&gt;&lt;strong&gt;RBDash&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/anzhao920&#34;&gt;&lt;strong&gt;anzhao920&lt;/strong&gt;&lt;/a&gt; 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-09-03]&lt;/strong&gt; We have supported &lt;a href=&#34;https://huggingface.co/Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5&#34;&gt;&lt;strong&gt;xGen-MM&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/amitbcp&#34;&gt;&lt;strong&gt;amitbcp&lt;/strong&gt;&lt;/a&gt; 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-09-03]&lt;/strong&gt; In the previous 2 months, 7 new contributors have made 3+ major contributions to the project: &lt;a href=&#34;https://github.com/amitbcp&#34;&gt;amitbcp&lt;/a&gt;, &lt;a href=&#34;https://github.com/czczup&#34;&gt;czczup&lt;/a&gt;, &lt;a href=&#34;https://github.com/DseidLi&#34;&gt;DseidLi&lt;/a&gt;, &lt;a href=&#34;https://github.com/mayubo233&#34;&gt;mayubo233&lt;/a&gt;, &lt;a href=&#34;https://github.com/sun-hailong&#34;&gt;sun-hailong&lt;/a&gt;, &lt;a href=&#34;https://github.com/PhoenixZ810&#34;&gt;PhoenixZ810&lt;/a&gt;, &lt;a href=&#34;https://github.com/Cuiunbo&#34;&gt;Cuiunbo&lt;/a&gt;. We will update the report accordingly in the coming weeks. Check &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/en/advanced_guides/Contributors.md&#34;&gt;contributor list&lt;/a&gt; for their detailed contributions 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-09-03]&lt;/strong&gt; We have supported &lt;a href=&#34;https://arxiv.org/abs/2408.13257&#34;&gt;&lt;strong&gt;MME-RealWorld&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/yfzhang114&#34;&gt;&lt;strong&gt;Yifan zhang&lt;/strong&gt;&lt;/a&gt; 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-09-02]&lt;/strong&gt; We have supported &lt;a href=&#34;https://arxiv.org/abs/2404.19205&#34;&gt;&lt;strong&gt;TableVQABench&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/hkunzhe&#34;&gt;&lt;strong&gt;hkunzhe&lt;/strong&gt;&lt;/a&gt; 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-08-29]&lt;/strong&gt; We have supported &lt;a href=&#34;https://arxiv.org/abs/2404.16994&#34;&gt;&lt;strong&gt;PLLaVA&lt;/strong&gt;&lt;/a&gt;, including three different sizes (7B/13B/34B) 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-08-29]&lt;/strong&gt; We have supported &lt;a href=&#34;https://huggingface.co/papers/2408.03361&#34;&gt;&lt;strong&gt;GMAI-MMBench&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/TousenKaname&#34;&gt;&lt;strong&gt;TousenKaname&lt;/strong&gt;&lt;/a&gt;. Reference Performance: GPT-4o-MINI achieves 42.2% Overall accuracy 🔥🔥🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-08-29]&lt;/strong&gt; We have supported &lt;a href=&#34;https://muirbench.github.io&#34;&gt;&lt;strong&gt;MUIRBench&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;strong&gt;amitbcp&lt;/strong&gt;. Reference Performance: GPT-4o-MINI achieves 64.6% Overall accuracy 🔥🔥🔥&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📊 Datasets, Models, and Evaluation Results&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;The performance numbers on our official multi-modal leaderboards can be downloaded from here!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/opencompass/open_vlm_leaderboard&#34;&gt;&lt;strong&gt;OpenVLM Leaderboard&lt;/strong&gt;&lt;/a&gt;: &lt;a href=&#34;http://opencompass.openxlab.space/assets/OpenVLM.json&#34;&gt;Download All DETAILED Results&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported Image Understanding Dataset&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;By default, all evaluation results are presented in &lt;a href=&#34;https://huggingface.co/spaces/opencompass/open_vlm_leaderboard&#34;&gt;&lt;strong&gt;OpenVLM Leaderboard&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Abbrs: &lt;code&gt;MCQ&lt;/code&gt;: Multi-choice question; &lt;code&gt;Y/N&lt;/code&gt;: Yes-or-No Questions; &lt;code&gt;MTT&lt;/code&gt;: Benchmark with Multi-turn Conversations; &lt;code&gt;MTI&lt;/code&gt;: Benchmark with Multi-Image as Inputs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Names (for run.py)&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Names (for run.py)&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-compass/mmbench/&#34;&gt;&lt;strong&gt;MMBench Series&lt;/strong&gt;&lt;/a&gt;: &lt;br&gt;MMBench, MMBench-CN, CCBench&lt;/td&gt; &#xA;   &lt;td&gt;MMBench_DEV_[EN/CN] &lt;br&gt;MMBench_TEST_[EN/CN]&lt;br&gt;MMBench_DEV_[EN/CN]_V11&lt;br&gt;MMBench_TEST_[EN/CN]_V11&lt;br&gt;CCBench&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MMStar-Benchmark/MMStar&#34;&gt;&lt;strong&gt;MMStar&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MMStar&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation&#34;&gt;&lt;strong&gt;MME&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MME&lt;/td&gt; &#xA;   &lt;td&gt;Y/N&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AILab-CVC/SEED-Bench&#34;&gt;&lt;strong&gt;SEEDBench Series&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SEEDBench_IMG &lt;br&gt;SEEDBench2 &lt;br&gt;SEEDBench2_Plus&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yuweihao/MM-Vet&#34;&gt;&lt;strong&gt;MM-Vet&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MMVet&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mmmu-benchmark.github.io&#34;&gt;&lt;strong&gt;MMMU&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MMMU_[DEV_VAL/TEST]&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mathvista.github.io&#34;&gt;&lt;strong&gt;MathVista&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MathVista_MINI&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scienceqa.github.io&#34;&gt;&lt;strong&gt;ScienceQA_IMG&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ScienceQA_[VAL/TEST]&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cocodataset.org&#34;&gt;&lt;strong&gt;COCO Caption&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;COCO_VAL&lt;/td&gt; &#xA;   &lt;td&gt;Caption&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tianyi-lab/HallusionBench&#34;&gt;&lt;strong&gt;HallusionBench&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HallusionBench&lt;/td&gt; &#xA;   &lt;td&gt;Y/N&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ocr-vqa.github.io&#34;&gt;&lt;strong&gt;OCRVQA&lt;/strong&gt;&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td&gt;OCRVQA_[TESTCORE/TEST]&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://textvqa.org&#34;&gt;&lt;strong&gt;TextVQA&lt;/strong&gt;&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td&gt;TextVQA_VAL&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/vis-nlp/ChartQA&#34;&gt;&lt;strong&gt;ChartQA&lt;/strong&gt;&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td&gt;ChartQA_TEST&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://allenai.org/data/diagrams&#34;&gt;&lt;strong&gt;AI2D&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AI2D_[TEST/TEST_NO_MASK]&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild&#34;&gt;&lt;strong&gt;LLaVABench&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaVABench&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.docvqa.org&#34;&gt;&lt;strong&gt;DocVQA&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;DocVQA_[VAL/TEST]&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.docvqa.org/datasets/infographicvqa&#34;&gt;&lt;strong&gt;InfoVQA&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;InfoVQA_[VAL/TEST]&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Yuliang-Liu/MultimodalOCR&#34;&gt;&lt;strong&gt;OCRBench&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;OCRBench&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://x.ai/blog/grok-1.5v&#34;&gt;&lt;strong&gt;RealWorldQA&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RealWorldQA&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AoiDragon/POPE&#34;&gt;&lt;strong&gt;POPE&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;POPE&lt;/td&gt; &#xA;   &lt;td&gt;Y/N&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/core-mm/core-mm&#34;&gt;&lt;strong&gt;Core-MM&lt;/strong&gt;&lt;/a&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;CORE_MM (MTI)&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mmt-bench.github.io&#34;&gt;&lt;strong&gt;MMT-Bench&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MMT-Bench_[VAL/ALL]&lt;br&gt;MMT-Bench_[VAL/ALL]_MI&lt;/td&gt; &#xA;   &lt;td&gt;MCQ (MTI)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Carol-gutianle/MLLMGuard&#34;&gt;&lt;strong&gt;MLLMGuard&lt;/strong&gt;&lt;/a&gt; -&lt;/td&gt; &#xA;   &lt;td&gt;MLLMGuard_DS&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yipoh/AesBench&#34;&gt;&lt;strong&gt;AesBench&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;AesBench_[VAL/TEST]&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/vcr-org/&#34;&gt;&lt;strong&gt;VCR-wiki&lt;/strong&gt;&lt;/a&gt; +&lt;/td&gt; &#xA;   &lt;td&gt;VCR_[EN/ZH]_[EASY/HARD]_[ALL/500/100]&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mayubo2333.github.io/MMLongBench-Doc/&#34;&gt;&lt;strong&gt;MMLongBench-Doc&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;MMLongBench_DOC&lt;/td&gt; &#xA;   &lt;td&gt;VQA (MTI)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zeyofu.github.io/blink/&#34;&gt;&lt;strong&gt;BLINK&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BLINK&lt;/td&gt; &#xA;   &lt;td&gt;MCQ (MTI)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mathvision-cuhk.github.io&#34;&gt;&lt;strong&gt;MathVision&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;MathVision&lt;br&gt;MathVision_MINI&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bytedance/MTVQA&#34;&gt;&lt;strong&gt;MT-VQA&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MTVQA_TEST&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://liuziyu77.github.io/MMDU/&#34;&gt;&lt;strong&gt;MMDU&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;MMDU&lt;/td&gt; &#xA;   &lt;td&gt;VQA (MTT, MTI)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Q-Future/Q-Bench&#34;&gt;&lt;strong&gt;Q-Bench1&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Q-Bench1_[VAL/TEST]&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Q-Future/A-Bench&#34;&gt;&lt;strong&gt;A-Bench&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A-Bench_[VAL/TEST]&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.08455&#34;&gt;&lt;strong&gt;DUDE&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;DUDE&lt;/td&gt; &#xA;   &lt;td&gt;VQA (MTI)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.04883&#34;&gt;&lt;strong&gt;SlideVQA&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;SLIDEVQA&lt;br&gt;SLIDEVQA_MINI&lt;/td&gt; &#xA;   &lt;td&gt;VQA (MTI)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/weikaih/TaskMeAnything-v1-imageqa-random&#34;&gt;&lt;strong&gt;TaskMeAnything ImageQA Random&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;TaskMeAnything_v1_imageqa_random&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sun-hailong.github.io/projects/Parrot/&#34;&gt;&lt;strong&gt;MMMB and Multilingual MMBench&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;MMMB_[ar/cn/en/pt/ru/tr]&lt;br&gt;MMBench_dev_[ar/cn/en/pt/ru/tr]&lt;br&gt;MMMB&lt;br&gt;MTL_MMBench_DEV&lt;br&gt;PS: MMMB &amp;amp; MTL_MMBench_DEV &lt;br&gt;are &lt;strong&gt;all-in-one&lt;/strong&gt; names for 6 langs&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.01718&#34;&gt;&lt;strong&gt;A-OKVQA&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;A-OKVQA&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://muirbench.github.io&#34;&gt;&lt;strong&gt;MuirBench&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;MUIRBench&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/papers/2408.03361&#34;&gt;&lt;strong&gt;GMAI-MMBench&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;GMAI-MMBench_VAL&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.19205&#34;&gt;&lt;strong&gt;TableVQABench&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;TableVQABench&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.13257&#34;&gt;&lt;strong&gt;MME-RealWorld&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;MME-RealWorld[-CN]&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.15556&#34;&gt;&lt;strong&gt;HRBench&lt;/strong&gt;&lt;/a&gt;+&lt;/td&gt; &#xA;   &lt;td&gt;HRBench[4K/8K]&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;*&lt;/strong&gt; We only provide a subset of the evaluation results, since some VLMs do not yield reasonable results under the zero-shot setting&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;+&lt;/strong&gt; The evaluation results are not available yet&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;-&lt;/strong&gt; Only inference is supported in VLMEvalKit&lt;/p&gt; &#xA;&lt;p&gt;VLMEvalKit will use a &lt;strong&gt;judge LLM&lt;/strong&gt; to extract answer from the output if you set the key, otherwise it uses the &lt;strong&gt;exact matching&lt;/strong&gt; mode (find &#34;Yes&#34;, &#34;No&#34;, &#34;A&#34;, &#34;B&#34;, &#34;C&#34;... in the output strings). &lt;strong&gt;The exact matching can only be applied to the Yes-or-No tasks and the Multi-choice tasks.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported Video Understanding Dataset&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Names (for run.py)&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Names (for run.py)&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mmbench-video.github.io&#34;&gt;&lt;strong&gt;MMBench-Video&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MMBench-Video&lt;/td&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://video-mme.github.io/&#34;&gt;&lt;strong&gt;Video-MME&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Video-MME&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenGVLab/Ask-Anything/raw/main/video_chat2/MVBENCH.md&#34;&gt;&lt;strong&gt;MVBench&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MVBench/MVBench_MP4&lt;/td&gt; &#xA;   &lt;td&gt;MCQ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported API Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/vision&#34;&gt;&lt;strong&gt;GPT-4v (20231106, 20240409)&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://openai.com/index/hello-gpt-4o/&#34;&gt;&lt;strong&gt;GPT-4o&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/vision&#34;&gt;&lt;strong&gt;Gemini-1.0-Pro&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/vision&#34;&gt;&lt;strong&gt;Gemini-1.5-Pro&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://www.stepfun.com/#step1v&#34;&gt;&lt;strong&gt;Step-1V&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.reka.ai&#34;&gt;&lt;strong&gt;Reka-[Edge / Flash / Core]&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen-VL-Max&#34;&gt;&lt;strong&gt;Qwen-VL-[Plus / Max]&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.anthropic.com/news/claude-3-family&#34;&gt;&lt;strong&gt;Claude3-[Haiku / Sonnet / Opus]&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://open.bigmodel.cn/dev/howuse/glm4v&#34;&gt;&lt;strong&gt;GLM-4v&lt;/strong&gt;&lt;/a&gt; 🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mllm.cloudwalk.com/web&#34;&gt;&lt;strong&gt;CongRong&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.anthropic.com/news/claude-3-5-sonnet&#34;&gt;&lt;strong&gt;Claude3.5-Sonnet&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/&#34;&gt;&lt;strong&gt;GPT-4o-Mini&lt;/strong&gt;&lt;/a&gt; 🎞️🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://platform.lingyiwanwu.com&#34;&gt;&lt;strong&gt;Yi-Vision&lt;/strong&gt;&lt;/a&gt;🎞️🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cloud.tencent.com/document/product/1729&#34;&gt;&lt;strong&gt;Hunyuan-Vision&lt;/strong&gt;&lt;/a&gt;🎞️🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported PyTorch / HF Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://huggingface.co/HuggingFaceM4/idefics-9b-instruct&#34;&gt;&lt;strong&gt;IDEFICS-[9B/80B/v2-8B/v3-8B]-Instruct&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS/raw/main/projects/instructblip/README.md&#34;&gt;&lt;strong&gt;InstructBLIP-[7B/13B]&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;&lt;strong&gt;LLaVA-[v1-7B/v1.5-7B/v1.5-13B]&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;&lt;strong&gt;MiniGPT-4-[v1-7B/v1-13B/v2-7B]&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2&#34;&gt;&lt;strong&gt;mPLUG-Owl[2/3]&lt;/strong&gt;&lt;/a&gt;🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mlfoundations/open_flamingo&#34;&gt;&lt;strong&gt;OpenFlamingo-v2&lt;/strong&gt;&lt;/a&gt;🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yxuansu/PandaGPT&#34;&gt;&lt;strong&gt;PandaGPT-13B&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-VL&#34;&gt;&lt;strong&gt;Qwen-VL&lt;/strong&gt;&lt;/a&gt;🚅🎞️ , &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-VL-Chat&#34;&gt;&lt;strong&gt;Qwen-VL-Chat&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM/visualglm-6b&#34;&gt;&lt;strong&gt;VisualGLM-6B&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/internlm/internlm-xcomposer-7b&#34;&gt;&lt;strong&gt;InternLM-XComposer-[1/2]&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sharegpt4v.github.io&#34;&gt;&lt;strong&gt;ShareGPT4V-[7B/13B]&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PCIResearch/TransCore-M&#34;&gt;&lt;strong&gt;TransCore-M&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/xtuner/llava-internlm-7b&#34;&gt;&lt;strong&gt;LLaVA (XTuner)&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM/cogvlm-chat-hf&#34;&gt;&lt;strong&gt;CogVLM-[Chat/Llama3]&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/Lin-Chen/Share-Captioner&#34;&gt;&lt;strong&gt;ShareCaptioner&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf&#34;&gt;&lt;strong&gt;CogVLM-Grounding-Generalist&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Yuliang-Liu/Monkey&#34;&gt;&lt;strong&gt;Monkey&lt;/strong&gt;&lt;/a&gt;🚅, &lt;a href=&#34;https://github.com/Yuliang-Liu/Monkey&#34;&gt;&lt;strong&gt;Monkey-Chat&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/baaivision/Emu&#34;&gt;&lt;strong&gt;EMU2-Chat&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/01-ai/Yi-VL-6B&#34;&gt;&lt;strong&gt;Yi-VL-[6B/34B]&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/DataCanvas/MMAlaya&#34;&gt;&lt;strong&gt;MMAlaya&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer&#34;&gt;&lt;strong&gt;InternLM-XComposer-2.5&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V&#34;&gt;&lt;strong&gt;MiniCPM-[V1/V2/V2.5/V2.6]&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openbmb/OmniLMM-12B&#34;&gt;&lt;strong&gt;OmniLMM-12B&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenGVLab/InternVL&#34;&gt;&lt;strong&gt;InternVL-Chat-[V1-1/V1-2/V1-5/V2]&lt;/strong&gt;&lt;/a&gt;🚅🎞️, &lt;br&gt;&lt;a href=&#34;https://github.com/OpenGVLab/InternVL&#34;&gt;&lt;strong&gt;Mini-InternVL-Chat-[2B/4B]-V1-5&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-VL/tree/main&#34;&gt;&lt;strong&gt;DeepSeek-VL&lt;/strong&gt;&lt;/a&gt;🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://llava-vl.github.io/blog/2024-01-30-llava-next/&#34;&gt;&lt;strong&gt;LLaVA-NeXT&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/BAAI/Bunny-v1_1-Llama-3-8B-V&#34;&gt;&lt;strong&gt;Bunny-Llama3&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xverse-ai/XVERSE-V-13B/raw/main/vxverse/models/vxverse.py&#34;&gt;&lt;strong&gt;XVERSE-V-13B&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google/paligemma-3b-pt-448&#34;&gt;&lt;strong&gt;PaliGemma-3B&lt;/strong&gt;&lt;/a&gt; 🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/qihoo360/360VL-70B&#34;&gt;&lt;strong&gt;360VL-70B&lt;/strong&gt;&lt;/a&gt; 🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft/Phi-3-vision-128k-instruct&#34;&gt;&lt;strong&gt;Phi-3-Vision&lt;/strong&gt;&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/microsoft/Phi-3.5-vision-instruct&#34;&gt;&lt;strong&gt;Phi-3.5-Vision&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/scenarios/WeMM&#34;&gt;&lt;strong&gt;WeMM&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM/glm-4v-9b&#34;&gt;&lt;strong&gt;GLM-4v-9B&lt;/strong&gt;&lt;/a&gt; 🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cambrian-mllm.github.io/&#34;&gt;&lt;strong&gt;Cambrian-[8B/13B/34B]&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/lmms-lab/llava-next-qwen-32b&#34;&gt;&lt;strong&gt;LLaVA-Next-[Qwen-32B]&lt;/strong&gt;&lt;/a&gt; 🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/chameleon-7b&#34;&gt;&lt;strong&gt;Chameleon-[7B/30B]&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA&#34;&gt;&lt;strong&gt;Video-LLaVA-7B-[HF]&lt;/strong&gt;&lt;/a&gt; 🎬&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/NVlabs/VILA/&#34;&gt;&lt;strong&gt;VILA1.5-[3B/8B/13B/40B]&lt;/strong&gt;&lt;/a&gt;🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AIDC-AI/Ovis&#34;&gt;&lt;strong&gt;Ovis1.5-[Llama3-8B/Gemma2-9B]&lt;/strong&gt;&lt;/a&gt; 🚅🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/TIGER-Lab/Mantis-8B-Idefics2&#34;&gt;&lt;strong&gt;Mantis-8B-[siglip-llama3/clip-llama3/Idefics2/Fuyu]&lt;/strong&gt;&lt;/a&gt; 🎞️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Zero-Vision/Llama-3-MixSenseV1_1&#34;&gt;&lt;strong&gt;Llama-3-MixSenseV1_1&lt;/strong&gt;&lt;/a&gt;🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AIDC-AI/Parrot&#34;&gt;&lt;strong&gt;Parrot-7B&lt;/strong&gt;&lt;/a&gt; 🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/omlab/omchat-v2.0-13B-single-beta_hf&#34;&gt;&lt;strong&gt;OmChat-v2.0-13B-sinlge-beta&lt;/strong&gt;&lt;/a&gt; 🚅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT&#34;&gt;&lt;strong&gt;Video-ChatGPT&lt;/strong&gt;&lt;/a&gt; 🎬&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Chat-UniVi&#34;&gt;&lt;strong&gt;Chat-UniVi-7B[-v1.5]&lt;/strong&gt;&lt;/a&gt; 🎬&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dvlab-research/LLaMA-VID&#34;&gt;&lt;strong&gt;LLaMA-VID-7B&lt;/strong&gt;&lt;/a&gt; 🎬&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/OpenGVLab/VideoChat2_HD_stage4_Mistral_7B&#34;&gt;&lt;strong&gt;VideoChat2-HD&lt;/strong&gt;&lt;/a&gt; 🎬&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ermu2001/pllava-7b&#34;&gt;&lt;strong&gt;PLLaVA-[7B/13B/34B]&lt;/strong&gt;&lt;/a&gt; 🎬&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/RBDash-Team/RBDash&#34;&gt;&lt;strong&gt;RBDash_72b&lt;/strong&gt;&lt;/a&gt; 🚅🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5&#34;&gt;&lt;strong&gt;xgen-mm-phi3-[interleave/dpo]-r-v1.5&lt;/strong&gt;&lt;/a&gt; 🚅🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen2-VL&#34;&gt;&lt;strong&gt;Qwen2-VL-[2B/7B]&lt;/strong&gt;&lt;/a&gt;🚅🎞️&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yfzhang114/SliME&#34;&gt;&lt;strong&gt;slime_[7b/8b/13b]&lt;/strong&gt;&lt;/a&gt;🎞️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;🎞️: Support multiple images as inputs.&lt;/p&gt; &#xA;&lt;p&gt;🚅: Models can be used without any additional configuration/operation.&lt;/p&gt; &#xA;&lt;p&gt;🎬: Support Video as inputs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transformers Version Recommendation:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that some VLMs may not be able to run under certain transformer versions, we recommend the following settings to evaluate each VLM:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.33.0&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;Qwen series&lt;/code&gt;, &lt;code&gt;Monkey series&lt;/code&gt;, &lt;code&gt;InternLM-XComposer Series&lt;/code&gt;, &lt;code&gt;mPLUG-Owl2&lt;/code&gt;, &lt;code&gt;OpenFlamingo v2&lt;/code&gt;, &lt;code&gt;IDEFICS series&lt;/code&gt;, &lt;code&gt;VisualGLM&lt;/code&gt;, &lt;code&gt;MMAlaya&lt;/code&gt;, &lt;code&gt;ShareCaptioner&lt;/code&gt;, &lt;code&gt;MiniGPT-4 series&lt;/code&gt;, &lt;code&gt;InstructBLIP series&lt;/code&gt;, &lt;code&gt;PandaGPT&lt;/code&gt;, &lt;code&gt;VXVERSE&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.37.0&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;LLaVA series&lt;/code&gt;, &lt;code&gt;ShareGPT4V series&lt;/code&gt;, &lt;code&gt;TransCore-M&lt;/code&gt;, &lt;code&gt;LLaVA (XTuner)&lt;/code&gt;, &lt;code&gt;CogVLM Series&lt;/code&gt;, &lt;code&gt;EMU2 Series&lt;/code&gt;, &lt;code&gt;Yi-VL Series&lt;/code&gt;, &lt;code&gt;MiniCPM-[V1/V2]&lt;/code&gt;, &lt;code&gt;OmniLMM-12B&lt;/code&gt;, &lt;code&gt;DeepSeek-VL series&lt;/code&gt;, &lt;code&gt;InternVL series&lt;/code&gt;, &lt;code&gt;Cambrian Series&lt;/code&gt;, &lt;code&gt;VILA Series&lt;/code&gt;, &lt;code&gt;Llama-3-MixSenseV1_1&lt;/code&gt;, &lt;code&gt;Parrot-7B&lt;/code&gt;, &lt;code&gt;PLLaVA Series&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.40.0&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;IDEFICS2&lt;/code&gt;, &lt;code&gt;Bunny-Llama3&lt;/code&gt;, &lt;code&gt;MiniCPM-Llama3-V2.5&lt;/code&gt;, &lt;code&gt;360VL-70B&lt;/code&gt;, &lt;code&gt;Phi-3-Vision&lt;/code&gt;, &lt;code&gt;WeMM&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==latest&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;LLaVA-Next series&lt;/code&gt;, &lt;code&gt;PaliGemma-3B&lt;/code&gt;, &lt;code&gt;Chameleon series&lt;/code&gt;, &lt;code&gt;Video-LLaVA-7B-HF&lt;/code&gt;, &lt;code&gt;Ovis series&lt;/code&gt;, &lt;code&gt;Mantis series&lt;/code&gt;, &lt;code&gt;MiniCPM-V2.6&lt;/code&gt;, &lt;code&gt;OmChat-v2.0-13B-sinlge-beta&lt;/code&gt;, &lt;code&gt;Idefics-3&lt;/code&gt;, &lt;code&gt;GLM-4v-9B&lt;/code&gt;, &lt;code&gt;VideoChat2-HD&lt;/code&gt;, &lt;code&gt;RBDash_72b&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Demo&#xA;from vlmeval.config import supported_VLM&#xA;model = supported_VLM[&#39;idefics_9b_instruct&#39;]()&#xA;# Forward Single Image&#xA;ret = model.generate([&#39;assets/apple.jpg&#39;, &#39;What is in this image?&#39;])&#xA;print(ret)  # The image features a red apple with a leaf on it.&#xA;# Forward Multiple Images&#xA;ret = model.generate([&#39;assets/apple.jpg&#39;, &#39;assets/apple.jpg&#39;, &#39;How many apples are there in the provided images? &#39;])&#xA;print(ret)  # There are two apples in the provided images.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🏗️ QuickStart&lt;/h2&gt; &#xA;&lt;p&gt;See [&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/en/get_started/Quickstart.md&#34;&gt;QuickStart&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/zh-CN/get_started/Quickstart.md&#34;&gt;快速开始&lt;/a&gt;] for a quick start guide.&lt;/p&gt; &#xA;&lt;h2&gt;🛠️ Development Guide&lt;/h2&gt; &#xA;&lt;p&gt;To develop custom benchmarks, VLMs, or simply contribute other codes to &lt;strong&gt;VLMEvalKit&lt;/strong&gt;, please refer to [&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/en/advanced_guides/Development.md&#34;&gt;Development_Guide&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/zh-CN/advanced_guides/Development.md&#34;&gt;开发指南&lt;/a&gt;].&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Call for contributions&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To promote the contribution from the community and share the corresponding credit (in the next report update):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All Contributions will be acknowledged in the report.&lt;/li&gt; &#xA; &lt;li&gt;Contributors with 3 or more major contributions (implementing an MLLM, benchmark, or major feature) can join the author list of &lt;a href=&#34;https://www.arxiv.org/abs/2407.11691&#34;&gt;VLMEvalKit Technical Report&lt;/a&gt; on ArXiv. Eligible contributors can create an issue or dm kennyutc in &lt;a href=&#34;https://discord.com/invite/evDT4GZmxN&#34;&gt;VLMEvalKit Discord Channel&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is a &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/en/advanced_guides/Contributors.md&#34;&gt;contributor list&lt;/a&gt; we curated based on the records.&lt;/p&gt; &#xA;&lt;h2&gt;🎯 The Goal of VLMEvalKit&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;The codebase is designed to:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Provide an &lt;strong&gt;easy-to-use&lt;/strong&gt;, &lt;strong&gt;opensource evaluation toolkit&lt;/strong&gt; to make it convenient for researchers &amp;amp; developers to evaluate existing LVLMs and make evaluation results &lt;strong&gt;easy to reproduce&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Make it easy for VLM developers to evaluate their own models. To evaluate the VLM on multiple supported benchmarks, one just need to &lt;strong&gt;implement a single &lt;code&gt;generate_inner()&lt;/code&gt; function&lt;/strong&gt;, all other workloads (data downloading, data preprocessing, prediction inference, metric calculation) are handled by the codebase.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;The codebase is not designed to:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Reproduce the exact accuracy number reported in the original papers of all &lt;strong&gt;3rd party benchmarks&lt;/strong&gt;. The reason can be two-fold: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;VLMEvalKit uses &lt;strong&gt;generation-based evaluation&lt;/strong&gt; for all VLMs (and optionally with &lt;strong&gt;LLM-based answer extraction&lt;/strong&gt;). Meanwhile, some benchmarks may use different approaches (SEEDBench uses PPL-based evaluation, &lt;em&gt;eg.&lt;/em&gt;). For those benchmarks, we compare both scores in the corresponding result. We encourage developers to support other evaluation paradigms in the codebase.&lt;/li&gt; &#xA;   &lt;li&gt;By default, we use the same prompt template for all VLMs to evaluate on a benchmark. Meanwhile, &lt;strong&gt;some VLMs may have their specific prompt templates&lt;/strong&gt; (some may not covered by the codebase at this time). We encourage VLM developers to implement their own prompt template in VLMEvalKit, if that is not covered currently. That will help to improve the reproducibility.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🖊️ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work helpful, please consider to &lt;strong&gt;star🌟&lt;/strong&gt; this repo. Thanks for your support!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/open-compass/VLMEvalKit/stargazers&#34;&gt;&lt;img src=&#34;https://reporoster.com/stars/open-compass/VLMEvalKit&#34; alt=&#34;Stargazers repo roster for @open-compass/VLMEvalKit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use VLMEvalKit in your research or wish to refer to published OpenSource evaluation results, please use the following BibTeX entry and the BibTex entry corresponding to the specific VLM / benchmark you used.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@misc{duan2024vlmevalkit,&#xA;      title={VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models},&#xA;      author={Haodong Duan and Junming Yang and Yuxuan Qiao and Xinyu Fang and Lin Chen and Yuan Liu and Xiaoyi Dong and Yuhang Zang and Pan Zhang and Jiaqi Wang and Dahua Lin and Kai Chen},&#xA;      year={2024},&#xA;      eprint={2407.11691},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2407.11691},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Future-House/paper-qa</title>
    <updated>2024-09-14T01:33:04Z</updated>
    <id>tag:github.com,2024-09-14:/Future-House/paper-qa</id>
    <link href="https://github.com/Future-House/paper-qa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High accuracy RAG for answering questions from scientific documents with citations&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PaperQA2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/whitead/paper-qa&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/whitead/paper-qa&#34;&gt;&lt;img src=&#34;https://github.com/whitead/paper-qa/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/paper-qa&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/paper-qa.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PaperQA2 is a package for doing high-accuracy retrieval augmented generation (RAG) on PDFs or text files, with a focus on the scientific literature. See our &lt;a href=&#34;https://paper.wikicrow.ai&#34;&gt;recent 2024 paper&lt;/a&gt; to see examples of PaperQA2&#39;s superhuman performance in scientific tasks like question answering, summarization, and contradiction detection.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;In this example we take a folder of research paper PDFs, magically get their metadata - including citation counts and a retraction check, then parse and cache PDFs into a full-text search index, and finally answer the user question with an LLM agent.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install paper-qa&#xA;cd my_papers&#xA;pqa ask &#39;How can carbon nanotubes be manufactured at a large scale?&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Output&lt;/h3&gt; &#xA;&lt;p&gt;Question: Has anyone designed neural networks that compute with proteins or DNA?&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The claim that neural networks have been designed to compute with DNA is supported by multiple sources. The work by Qian, Winfree, and Bruck demonstrates the use of DNA strand displacement cascades to construct neural network components, such as artificial neurons and associative memories, using a DNA-based system (Qian2011Neural pages 1-2, Qian2011Neural pages 15-16, Qian2011Neural pages 54-56). This research includes the implementation of a 3-bit XOR gate and a four-neuron Hopfield associative memory, showcasing the potential of DNA for neural network computation. Additionally, the application of deep learning techniques to genomics, which involves computing with DNA sequences, is well-documented. Studies have applied convolutional neural networks (CNNs) to predict genomic features such as transcription factor binding and DNA accessibility (Eraslan2019Deep pages 4-5, Eraslan2019Deep pages 5-6). These models leverage DNA sequences as input data, effectively using neural networks to compute with DNA. While the provided excerpts do not explicitly mention protein-based neural network computation, they do highlight the use of neural networks in tasks related to protein sequences, such as predicting DNA-protein binding (Zeng2016Convolutional pages 1-2). However, the primary focus remains on DNA-based computation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;What is PaperQA2&lt;/h2&gt; &#xA;&lt;p&gt;PaperQA2 is engineered to be the best RAG model for working with scientific papers. Here are some features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A simple interface to get good answers with grounded responses containing in-text citations.&lt;/li&gt; &#xA; &lt;li&gt;State-of-the-art implementation including document metadata-awareness in embeddings and LLM-based re-ranking and contextual summarization (RCS).&lt;/li&gt; &#xA; &lt;li&gt;Support for agentic RAG, where a language agent can iteratively refine queries and answers.&lt;/li&gt; &#xA; &lt;li&gt;Automatic redundant fetching of paper metadata, including citation and journal quality data from multiple providers.&lt;/li&gt; &#xA; &lt;li&gt;A usable full-text search engine for a local repository of PDF/text files.&lt;/li&gt; &#xA; &lt;li&gt;A robust interface for customization, with default support for all &lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;LiteLLM&lt;/a&gt; models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, it uses &lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings&#34;&gt;OpenAI embeddings&lt;/a&gt; and &lt;a href=&#34;https://platform.openai.com/docs/models&#34;&gt;models&lt;/a&gt; with a Numpy vector DB to embed and search documents. However, you can easily use other closed-source, open-source models or embeddings (see details below).&lt;/p&gt; &#xA;&lt;p&gt;PaperQA2 depends on some awesome libraries/APIs that make our repo possible. Here are some in a random order:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.semanticscholar.org/&#34;&gt;Semantic Scholar&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.crossref.org/&#34;&gt;Crossref&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unpaywall.org/&#34;&gt;Unpaywall&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.pydantic.dev/latest/&#34;&gt;Pydantic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/quickwit-oss/tantivy&#34;&gt;tantivy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/&#34;&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pybtex.org/&#34;&gt;pybtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pymupdf.readthedocs.io/en/latest/&#34;&gt;PyMuPDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;For a non-development setup, install PaperQA2 from &lt;a href=&#34;https://pypi.org/project/paper-qa/&#34;&gt;PyPI&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install paper-qa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For development setup, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/Future-House/paper-qa/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;PaperQA2 uses an LLM to operate, so you&#39;ll need to either set an appropriate &lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;API key environment variable&lt;/a&gt; (i.e. &lt;code&gt;export OPENAI_API_KEY=sk-...&lt;/code&gt;) or set up an open source LLM server (i.e. using &lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile&#34;&gt;llamafile&lt;/a&gt;. Any LiteLLM compatible model can be configured to use with PaperQA2.&lt;/p&gt; &#xA;&lt;p&gt;If you need to index a large set of papers (100+), you will likely want an API key for both &lt;a href=&#34;https://www.crossref.org/documentation/metadata-plus/metadata-plus-keys/&#34;&gt;Crossref&lt;/a&gt; and &lt;a href=&#34;https://www.semanticscholar.org/product/api#api-key&#34;&gt;Semantic Scholar&lt;/a&gt;, which will allow you to avoid hitting public rate limits using these metadata services. Those can be exported as &lt;code&gt;CROSSREF_API_KEY&lt;/code&gt; and &lt;code&gt;SEMANTIC_SCHOLAR_API_KEY&lt;/code&gt; variables.&lt;/p&gt; &#xA;&lt;h2&gt;PaperQA2 vs PaperQA&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve been working on hard on fundamental upgrades for a while and mostly followed &lt;a href=&#34;https://semver.org/&#34;&gt;SemVer&lt;/a&gt;. meaning we&#39;ve incremented the major version number on each breaking change. This brings us to the current major version number v5. So why call is the repo now called PaperQA2? We wanted to remark on the fact though that we&#39;ve exceeded human performance on &lt;a href=&#34;https://paper.wikicrow.ai&#34;&gt;many important metrics&lt;/a&gt;. So we arbitrarily call version 5 and onward PaperQA2, and versions before it as PaperQA1 to denote the significant change in performance. We recognize that we are challenged at naming and counting at FutureHouse, so we reserve the right at any time to arbitrarily change the name to PaperCrow.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New in Version 5 (aka PaperQA2)?&lt;/h2&gt; &#xA;&lt;p&gt;Version 5 added:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A CLI &lt;code&gt;pqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Agentic workflows invoking tools for paper search, gathering evidence, and generating an answer&lt;/li&gt; &#xA; &lt;li&gt;Removed much of the statefulness from the &lt;code&gt;Docs&lt;/code&gt; object&lt;/li&gt; &#xA; &lt;li&gt;A migration to LiteLLM for compatibility with many LLM providers as well as centralized rate limits and cost tracking&lt;/li&gt; &#xA; &lt;li&gt;A bundled set of configurations (read &lt;a href=&#34;https://raw.githubusercontent.com/Future-House/paper-qa/main/#bundled-settings&#34;&gt;here&lt;/a&gt;)) containing known-good hyperparameters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;Docs&lt;/code&gt; objects pickled from prior versions of &lt;code&gt;PaperQA&lt;/code&gt; are incompatible with version 5 and will need to be rebuilt. Also, our minimum Python version is now Python 3.11.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To understand PaperQA2, let&#39;s start with the pieces of the underlying algorithm. The default workflow of PaperQA2 is as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Phase&lt;/th&gt; &#xA;   &lt;th&gt;PaperQA2 Actions&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1. Paper Search&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;- Get candidate papers from LLM-generated keyword query&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;- Chunk, embed, and add candidate papers to state&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2. Gather Evidence&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;- Embed query into vector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;- Rank top &lt;em&gt;k&lt;/em&gt; document chunks in current state&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;- Create scored summary of each chunk in the context of the current query&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;- Use LLM to re-score and select most relevant summaries&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3. Generate Answer&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;- Put best summaries into prompt with context&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;- Generate answer with prompt&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The tools can be invoked in any order by a language agent. For example, an LLM agent might do a narrow and broad search, or using different phrasing for the gather evidence step from the generate answer step.&lt;/p&gt; &#xA;&lt;h3&gt;CLI&lt;/h3&gt; &#xA;&lt;p&gt;The fastest way to test PaperQA2 is via the CLI. First navigate to a directory with some papers and use the &lt;code&gt;pqa&lt;/code&gt; cli:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pqa ask &#39;What manufacturing challenges are unique to bispecific antibodies?&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will see PaperQA2 index your local PDF files, gathering the necessary metadata for each of them (using &lt;a href=&#34;https://www.crossref.org/&#34;&gt;Crossref&lt;/a&gt; and &lt;a href=&#34;https://www.semanticscholar.org/&#34;&gt;Semantic Scholar&lt;/a&gt;), search over that index, then break the files into chunked evidence contexts, rank them, and ultimately generate an answer. The next time this directory is queried, your index will already be built (save for any differences detected, like new added papers), so it will skip the indexing and chunking steps.&lt;/p&gt; &#xA;&lt;p&gt;All prior answers will be indexed and stored, you can view them by querying via the &lt;code&gt;search&lt;/code&gt; subcommand, or access them yourself in your &lt;code&gt;PQA_HOME&lt;/code&gt; directory, which defaults to &lt;code&gt;~/.pqa/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pqa search -i &#39;answers&#39; &#39;antibodies&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PaperQA2 is highly configurable, when running from the command line, &lt;code&gt;pqa --help&lt;/code&gt; shows all options and short descriptions. For example to run with a higher temperature:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pqa --temperature 0.5 ask &#39;What manufacturing challenges are unique to bispecific antibodies?&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can view all settings with &lt;code&gt;pqa view&lt;/code&gt;. Another useful thing is to change to other templated settings - for example &lt;code&gt;fast&lt;/code&gt; is a setting that answers more quickly and you can see it with &lt;code&gt;pqa -s fast view&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Maybe you have some new settings you want to save? You can do that with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pqa -s my_new_settings --temperature 0.5 --llm foo-bar-5 save&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then you can use it with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pqa -s my_new_settings ask &#39;What manufacturing challenges are unique to bispecific antibodies?&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you run &lt;code&gt;pqa&lt;/code&gt; with a command which requires a new indexing, say if you change the default chunk_size, a new index will automatically be created for you.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pqa --parsing.chunk_size 5000 ask &#39;What manufacturing challenges are unique to bispecific antibodies?&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use &lt;code&gt;pqa&lt;/code&gt; to do full-text search with use of LLMs view the search command. For example, let&#39;s save the index from a directory and give it a name:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pqa -i nanomaterials index&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now I can search for papers about thermoelectrics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pqa -i nanomaterials search thermoelectrics&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or I can use the normal ask&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pqa -i nanomaterials ask &#39;Are there nm scale features in thermoelectric materials?&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Both the CLI and module have pre-configured settings based on prior performance and our publications, they can be invoked as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pqa --settings &amp;lt;setting name&amp;gt; ask &#39;Are there nm scale features in thermoelectric materials?&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Bundled Settings&lt;/h4&gt; &#xA;&lt;p&gt;Inside &lt;a href=&#34;https://raw.githubusercontent.com/Future-House/paper-qa/main/paperqa/configs&#34;&gt;&lt;code&gt;paperqa/configs&lt;/code&gt;&lt;/a&gt; we bundle known useful settings:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Setting Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;high_quality&lt;/td&gt; &#xA;   &lt;td&gt;Highly performant, relatively expensive (due to having &lt;code&gt;evidence_k&lt;/code&gt; = 15) query using a &lt;code&gt;ToolSelector&lt;/code&gt; agent.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;fast&lt;/td&gt; &#xA;   &lt;td&gt;Setting to get answers cheaply and quickly.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wikicrow&lt;/td&gt; &#xA;   &lt;td&gt;Setting to emulate the Wikipedia article writing used in our WikiCrow publication.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;contracrow&lt;/td&gt; &#xA;   &lt;td&gt;Setting to find contradictions in papers, your query should be a claim that needs to be flagged as a contradiction (or not).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;debug&lt;/td&gt; &#xA;   &lt;td&gt;Setting useful solely for debugging, but not in any actual application beyond debugging.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Module Usage&lt;/h3&gt; &#xA;&lt;p&gt;PaperQA2&#39;s full workflow can be accessed via Python directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Settings, ask&#xA;&#xA;answer = ask(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;    settings=Settings(temperature=0.5),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The answer object has the following attributes: &lt;code&gt;formatted_answer&lt;/code&gt;, &lt;code&gt;answer&lt;/code&gt; (answer alone), &lt;code&gt;question&lt;/code&gt; , and &lt;code&gt;context&lt;/code&gt; (the summaries of passages found for answer). &lt;code&gt;ask&lt;/code&gt; will use the &lt;code&gt;SearchPapers&lt;/code&gt; tool, which will query a local index of files, you can specify this location via the &lt;code&gt;Settings&lt;/code&gt; object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Settings, ask&#xA;&#xA;answer = ask(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;    settings=Settings(temperature=0.5, paper_directory=&#34;my_papers&#34;),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;ask&lt;/code&gt; is just a convenience wrapper around the real entrypoint, which can be accessed if you&#39;d like to run concurrent asynchronous workloads:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Settings, agent_query, QueryRequest&#xA;&#xA;answer = await agent_query(&#xA;    QueryRequest(&#xA;        query=&#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;        settings=Settings(temperature=0.5, paper_directory=&#34;my_papers&#34;),&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default agent will use an LLM based agent, but you can also specify a &lt;code&gt;&#34;fake&#34;&lt;/code&gt; agent to use a hard coded call path of search -&amp;gt; gather evidence -&amp;gt; answer to reduce token usage.&lt;/p&gt; &#xA;&lt;h3&gt;Adding Documents Manually&lt;/h3&gt; &#xA;&lt;p&gt;If you prefer fine grained control, and you wish to add objects to the docs object yourself (rather than using the search tool), then the previously existing &lt;code&gt;Docs&lt;/code&gt; object interface can be used:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Docs, Settings&#xA;&#xA;# valid extensions include .pdf, .txt, and .html&#xA;doc_paths = (&#34;myfile.pdf&#34;, &#34;myotherfile.pdf&#34;)&#xA;&#xA;docs = Docs()&#xA;&#xA;for doc in doc_paths:&#xA;    docs.add(doc)&#xA;&#xA;settings = Settings()&#xA;settings.llm = &#34;claude-3-5-sonnet-20240620&#34;&#xA;settings.answer.answer_max_sources = 3&#xA;&#xA;answer = docs.query(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;    settings=settings,&#xA;)&#xA;&#xA;print(answer.formatted_answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Async&lt;/h3&gt; &#xA;&lt;p&gt;PaperQA2 is written to be used asynchronously. The synchronous API is just a wrapper around the async. Here are the methods and their async equivalents:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Sync&lt;/th&gt; &#xA;   &lt;th&gt;Async&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.add&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.aadd&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.add_file&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.aadd_file&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.add_url&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.aadd_url&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.get_evidence&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.aget_evidence&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.query&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Docs.aquery&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The synchronous version just calls the async version in a loop. Most modern python environments support async natively (including Jupyter notebooks!). So you can do this in a Jupyter Notebook:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from paperqa import Docs&#xA;&#xA;&#xA;async def main():&#xA;    # valid extensions include .pdf, .txt, and .html&#xA;    doc_paths = (&#34;myfile.pdf&#34;, &#34;myotherfile.pdf&#34;)&#xA;&#xA;    docs = Docs()&#xA;&#xA;    for doc in doc_paths:&#xA;        await docs.aadd(doc)&#xA;&#xA;    answer = await docs.aquery(&#xA;        &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;&#xA;    )&#xA;&#xA;    print(answer.formatted_answer)&#xA;&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Choosing Model&lt;/h3&gt; &#xA;&lt;p&gt;By default, it uses OpenAI models with &lt;code&gt;gpt-4o-2024-08-06&lt;/code&gt; for both the re-ranking and summary step, the &lt;code&gt;summary_llm&lt;/code&gt; setting, and for the answering step, the &lt;code&gt;llm&lt;/code&gt; setting. You can adjust this easily:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Settings, ask&#xA;&#xA;answer = ask(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;    settings=Settings(&#xA;        llm=&#34;gpt-4o-mini&#34;, summary_llm=&#34;gpt-4o-mini&#34;, paper_directory=&#34;my_papers&#34;&#xA;    ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use Anthropic or any other model supported by &lt;code&gt;litellm&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Settings, ask&#xA;&#xA;answer = ask(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;    settings=Settings(&#xA;        llm=&#34;claude-3-5-sonnet-20240620&#34;, summary_llm=&#34;claude-3-5-sonnet-20240620&#34;&#xA;    ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Locally Hosted&lt;/h4&gt; &#xA;&lt;p&gt;You can use llama.cpp to be the LLM. Note that you should be using relatively large models, because PaperQA2 requires following a lot of instructions. You won&#39;t get good performance with 7B models.&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to get set-up is to download a &lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile&#34;&gt;llama file&lt;/a&gt; and execute it with &lt;code&gt;-cb -np 4 -a my-llm-model --embedding&lt;/code&gt; which will enable continuous batching and embeddings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Settings, ask&#xA;&#xA;local_llm_config = dict(&#xA;    model_list=dict(&#xA;        model_name=&#34;my_llm_model&#34;,&#xA;        litellm_params=dict(&#xA;            model=&#34;my-llm-model&#34;,&#xA;            api_base=&#34;http://localhost:8080/v1&#34;,&#xA;            api_key=&#34;sk-no-key-required&#34;,&#xA;            temperature=0.1,&#xA;            frequency_penalty=1.5,&#xA;            max_tokens=512,&#xA;        ),&#xA;    )&#xA;)&#xA;&#xA;answer = ask(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;    settings=Settings(&#xA;        llm=&#34;my-llm-model&#34;,&#xA;        llm_config=local_llm_config,&#xA;        summary_llm=&#34;my-llm-model&#34;,&#xA;        summary_llm_config=local_llm_config,&#xA;    ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Changing Embedding Model&lt;/h3&gt; &#xA;&lt;p&gt;PaperQA2 defaults to using OpenAI (&lt;code&gt;text-embedding-3-small&lt;/code&gt;) embeddings, but has flexible options for both vector stores and embedding choices. The simplest way to change an embedding is via the &lt;code&gt;embedding&lt;/code&gt; argument to the &lt;code&gt;Settings&lt;/code&gt; object constructor:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Settings, ask&#xA;&#xA;answer = ask(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;    settings=Settings(embedding=&#34;text-embedding-3-large&#34;),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;embedding&lt;/code&gt; accepts any embedding model name supported by litellm. PaperQA2 also supports an embedding input of &lt;code&gt;&#34;hybrid-&amp;lt;model_name&amp;gt;&#34;&lt;/code&gt; i.e. &lt;code&gt;&#34;hybrid-text-embedding-3-small&#34;&lt;/code&gt; to use a hybrid sparse keyword (based on a token modulo embedding) and dense vector embedding, where any litellm model can be used in the dense model name. &lt;code&gt;&#34;sparse&#34;&lt;/code&gt; can be used to use a sparse keyword embedding only.&lt;/p&gt; &#xA;&lt;p&gt;Embedding models are used to create PaperQA2&#39;s index of the full-text embedding vectors (&lt;code&gt;texts_index&lt;/code&gt; argument). The embedding model can be specified as a setting when you are adding new papers to the &lt;code&gt;Docs&lt;/code&gt; object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Docs, Settings&#xA;&#xA;doc_paths = (&#34;myfile.pdf&#34;, &#34;myotherfile.pdf&#34;)&#xA;&#xA;docs = Docs()&#xA;&#xA;for doc in doc_paths:&#xA;    doc.add(doc_paths, Settings(embedding=&#34;text-embedding-large-3&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that PaperQA2 uses Numpy as a dense vector store. Its design of using a keyword search initially reduces the number of chunks needed for each answer to a relatively small number &amp;lt; 1k. Therefore, &lt;code&gt;NumpyVectorStore&lt;/code&gt; is a good place to start, it&#39;s a simple in-memory store, without an index. However, if a larger-than-memory vector store is needed, we are currently lacking here.&lt;/p&gt; &#xA;&lt;p&gt;The hybrid embeddings can be customized:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import (&#xA;    Docs,&#xA;    HybridEmbeddingModel,&#xA;    SparseEmbeddingModel,&#xA;    LiteLLMEmbeddingModel,&#xA;)&#xA;&#xA;&#xA;doc_paths = (&#34;myfile.pdf&#34;, &#34;myotherfile.pdf&#34;)&#xA;&#xA;model = HybridEmbeddingModel(&#xA;    models=[LiteLLMEmbeddingModel(), SparseEmbeddingModel(ndim=1024)]&#xA;)&#xA;docs = Docs()&#xA;for doc in doc_paths:&#xA;    doc.add(doc_paths, embedding_model=model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The sparse embedding (keyword) models default to having 256 dimensions, but this can be specified via the &lt;code&gt;ndim&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;h3&gt;Adjusting number of sources&lt;/h3&gt; &#xA;&lt;p&gt;You can adjust the numbers of sources (passages of text) to reduce token usage or add more context. &lt;code&gt;k&lt;/code&gt; refers to the top k most relevant and diverse (may from different sources) passages. Each passage is sent to the LLM to summarize, or determine if it is irrelevant. After this step, a limit of &lt;code&gt;max_sources&lt;/code&gt; is applied so that the final answer can fit into the LLM context window. Thus, &lt;code&gt;k&lt;/code&gt; &amp;gt; &lt;code&gt;max_sources&lt;/code&gt; and &lt;code&gt;max_sources&lt;/code&gt; is the number of sources used in the final answer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Settings&#xA;&#xA;settings = Settings()&#xA;settings.answer.answer_max_sources = 3&#xA;settings.answer.k = 5&#xA;&#xA;docs.query(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;    settings=settings,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Code or HTML&lt;/h3&gt; &#xA;&lt;p&gt;You do not need to use papers -- you can use code or raw HTML. Note that this tool is focused on answering questions, so it won&#39;t do well at writing code. One note is that the tool cannot infer citations from code, so you will need to provide them yourself.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import glob&#xA;import os&#xA;from paperqa import Docs&#xA;&#xA;source_files = glob.glob(&#34;**/*.js&#34;)&#xA;&#xA;docs = Docs()&#xA;for f in source_files:&#xA;    # this assumes the file names are unique in code&#xA;    docs.add(f, citation=&#34;File &#34; + os.path.name(f), docname=os.path.name(f))&#xA;answer = docs.query(&#34;Where is the search bar in the header defined?&#34;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using External DB/Vector DB and Caching&lt;/h3&gt; &#xA;&lt;p&gt;You may want to cache parsed texts and embeddings in an external database or file. You can then build a Docs object from those directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Docs, Doc, Text&#xA;&#xA;docs = Docs()&#xA;&#xA;for ... in my_docs:&#xA;    doc = Doc(docname=..., citation=..., dockey=..., citation=...)&#xA;    texts = [Text(text=..., name=..., doc=doc) for ... in my_texts]&#xA;    docs.add_texts(texts, doc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Where do I get papers?&lt;/h2&gt; &#xA;&lt;p&gt;Well that&#39;s a really good question! It&#39;s probably best to just download PDFs of papers you think will help answer your question and start from there.&lt;/p&gt; &#xA;&lt;h3&gt;Zotero&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;It&#39;s been a while since we&#39;ve tested this - so let us know if it runs into issues!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use &lt;a href=&#34;https://www.zotero.org/&#34;&gt;Zotero&lt;/a&gt; to organize your personal bibliography, you can use the &lt;code&gt;paperqa.contrib.ZoteroDB&lt;/code&gt; to query papers from your library, which relies on &lt;a href=&#34;https://github.com/urschrei/pyzotero&#34;&gt;pyzotero&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Install &lt;code&gt;pyzotero&lt;/code&gt; via the &lt;code&gt;zotero&lt;/code&gt; extra for this feature:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install paperqa[zotero]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;First, note that PaperQA2 parses the PDFs of papers to store in the database, so all relevant papers should have PDFs stored inside your database. You can get Zotero to automatically do this by highlighting the references you wish to retrieve, right clicking, and selecting &lt;em&gt;&#34;Find Available PDFs&#34;&lt;/em&gt;. You can also manually drag-and-drop PDFs onto each reference.&lt;/p&gt; &#xA;&lt;p&gt;To download papers, you need to get an API key for your account.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get your library ID, and set it as the environment variable &lt;code&gt;ZOTERO_USER_ID&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For personal libraries, this ID is given &lt;a href=&#34;https://www.zotero.org/settings/keys&#34;&gt;here&lt;/a&gt; at the part &#34;&lt;em&gt;Your userID for use in API calls is XXXXXX&lt;/em&gt;&#34;.&lt;/li&gt; &#xA;   &lt;li&gt;For group libraries, go to your group page &lt;code&gt;https://www.zotero.org/groups/groupname&lt;/code&gt;, and hover over the settings link. The ID is the integer after /groups/. (&lt;em&gt;h/t pyzotero!&lt;/em&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create a new API key &lt;a href=&#34;https://www.zotero.org/settings/keys/new&#34;&gt;here&lt;/a&gt; and set it as the environment variable &lt;code&gt;ZOTERO_API_KEY&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The key will need read access to the library.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;With this, we can download papers from our library and add them to PaperQA2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Docs&#xA;from paperqa.contrib import ZoteroDB&#xA;&#xA;docs = Docs()&#xA;zotero = ZoteroDB(library_type=&#34;user&#34;)  # &#34;group&#34; if group library&#xA;&#xA;for item in zotero.iterate(limit=20):&#xA;    if item.num_pages &amp;gt; 30:&#xA;        continue  # skip long papers&#xA;    docs.add(item.pdf, docname=item.key)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which will download the first 20 papers in your Zotero database and add them to the &lt;code&gt;Docs&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;p&gt;We can also do specific queries of our Zotero library and iterate over the results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for item in zotero.iterate(&#xA;    q=&#34;large language models&#34;,&#xA;    qmode=&#34;everything&#34;,&#xA;    sort=&#34;date&#34;,&#xA;    direction=&#34;desc&#34;,&#xA;    limit=100,&#xA;):&#xA;    print(&#34;Adding&#34;, item.title)&#xA;    docs.add(item.pdf, docname=item.key)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can read more about the search syntax by typing &lt;code&gt;zotero.iterate?&lt;/code&gt; in IPython.&lt;/p&gt; &#xA;&lt;h3&gt;Paper Scraper&lt;/h3&gt; &#xA;&lt;p&gt;If you want to search for papers outside of your own collection, I&#39;ve found an unrelated project called &lt;a href=&#34;https://github.com/blackadad/paper-scraper&#34;&gt;paper-scraper&lt;/a&gt; that looks like it might help. But beware, this project looks like it uses some scraping tools that may violate publisher&#39;s rights or be in a gray area of legality.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Docs&#xA;&#xA;keyword_search = &#34;bispecific antibody manufacture&#34;&#xA;papers = paperscraper.search_papers(keyword_search)&#xA;docs = Docs()&#xA;for path, data in papers.items():&#xA;    try:&#xA;        docs.add(path)&#xA;    except ValueError as e:&#xA;        # sometimes this happens if PDFs aren&#39;t downloaded or readable&#xA;        print(&#34;Could not read&#34;, path, e)&#xA;answer = docs.query(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Callbacks&lt;/h2&gt; &#xA;&lt;p&gt;To execute a function on each chunk of LLM completions, you need to provide a function that can be executed on each chunk. For example, to get a typewriter view of the completions, you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def typewriter(chunk: str) -&amp;gt; None:&#xA;    print(chunk, end=&#34;&#34;)&#xA;&#xA;&#xA;docs = Docs()&#xA;&#xA;# add some docs...&#xA;&#xA;docs.query(&#xA;    &#34;What manufacturing challenges are unique to bispecific antibodies?&#34;,&#xA;    callbacks=[typewriter],&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Caching Embeddings&lt;/h3&gt; &#xA;&lt;p&gt;In general, embeddings are cached when you pickle a &lt;code&gt;Docs&lt;/code&gt; regardless of what vector store you use. So as long as you save your underlying &lt;code&gt;Docs&lt;/code&gt; object, you should be able to avoid re-embedding your documents.&lt;/p&gt; &#xA;&lt;h2&gt;Customizing Prompts&lt;/h2&gt; &#xA;&lt;p&gt;You can customize any of the prompts using settings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paperqa import Docs, Settings&#xA;&#xA;my_qa_prompt = (&#xA;    &#34;Answer the question &#39;{question}&#39; &#34;&#xA;    &#34;Use the context below if helpful. &#34;&#xA;    &#34;You can cite the context using the key &#34;&#xA;    &#34;like (Example2012). &#34;&#xA;    &#34;If there is insufficient context, write a poem &#34;&#xA;    &#34;about how you cannot answer.\n\n&#34;&#xA;    &#34;Context: {context}\n\n&#34;&#xA;)&#xA;&#xA;docs = Docs()&#xA;settings = Settings()&#xA;settings.prompts.qa = my_qa_prompt&#xA;docs.query(&#xA;    &#34;Are covid-19 vaccines effective?&#34;,&#xA;    settings=settings,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pre and Post Prompts&lt;/h3&gt; &#xA;&lt;p&gt;Following the syntax above, you can also include prompts that are executed after the query and before the query. For example, you can use this to critique the answer.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;How come I get different results than your papers?&lt;/h3&gt; &#xA;&lt;p&gt;Internally at FutureHouse, we have a slightly different set of tools. We&#39;re trying to get some of them, like citation traversal, into this repo. However, we have APIs and licenses to access research papers that we cannot share openly. Similarly, in our research papers&#39; results we do not start with the known relevant PDFs. Our agent has to identify them using keyword search over all papers, rather than just a subset. We&#39;re gradually aligning these two versions of PaperQA, but until there is an open-source way to freely access papers (even just open source papers) you will need to provide PDFs yourself.&lt;/p&gt; &#xA;&lt;h3&gt;How is this different from LlamaIndex?&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s not that different! This is similar to the tree response method in LlamaIndex. We also support agentic workflows and local indexes for easier operations with the scientific literature. Another big difference is our strong focus on scientific papers and their underlying metadata.&lt;/p&gt; &#xA;&lt;h3&gt;How is this different from LangChain?&lt;/h3&gt; &#xA;&lt;p&gt;There has been some great work on retrievers in LangChain, and you could say this is an example of a retriever with an LLM-based re-ranking and contextual summary. Another big difference is our strong focus on scientific papers and their underlying metadata.&lt;/p&gt; &#xA;&lt;h3&gt;Can I save or load?&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;Docs&lt;/code&gt; class can be pickled and unpickled. This is useful if you want to save the embeddings of the documents and then load them later.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pickle&#xA;&#xA;# save&#xA;with open(&#34;my_docs.pkl&#34;, &#34;wb&#34;) as f:&#xA;    pickle.dump(docs, f)&#xA;&#xA;# load&#xA;with open(&#34;my_docs.pkl&#34;, &#34;rb&#34;) as f:&#xA;    docs = pickle.load(f)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please read and cite the following papers if you use this software:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{skarlinski2024language,&#xA;  title={Language agents achieve superhuman synthesis of scientific knowledge},&#xA;  author={&#xA;    Michael D. Skarlinski and&#xA;    Sam Cox and&#xA;    Jon M. Laurent and&#xA;    James D. Braza and&#xA;    Michaela Hinks and&#xA;    Michael J. Hammerling and&#xA;    Manvitha Ponnapati and&#xA;    Samuel G. Rodriques and&#xA;    Andrew D. White},&#xA;  year={2024},&#xA;  journal={preprint},&#xA;  url={https://paper.wikicrow.ai}&#xA;}&#xA;&#xA;&#xA;@article{lala2023paperqa,&#xA;  title={PaperQA: Retrieval-Augmented Generative Agent for Scientific Research},&#xA;  author={L{\&#39;a}la, Jakub and O&#39;Donoghue, Odhran and Shtedritski, Aleksandar and Cox, Sam and Rodriques, Samuel G and White, Andrew D},&#xA;  journal={arXiv preprint arXiv:2312.07559},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>