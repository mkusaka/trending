<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-29T01:36:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>exo-explore/exo</title>
    <updated>2024-09-29T01:36:28Z</updated>
    <id>tag:github.com,2024-09-29:/exo-explore/exo</id>
    <link href="https://github.com/exo-explore/exo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;/docs/exo-logo-black-bg.jpg&#34;&gt; &#xA;  &lt;img alt=&#34;exo logo&#34; src=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href=&#34;https://x.com/exolabs&#34;&gt;exo labs&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt; &lt;p&gt;&lt;a href=&#34;https://discord.gg/EUnjGpsmWw&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://t.me/+Kh-KqHTzFYg3MGNk&#34;&gt;Telegram&lt;/a&gt; | &lt;a href=&#34;https://x.com/exolabs&#34;&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/exo-explore/exo/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/exo-explore/exo&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main&#34;&gt;&lt;img src=&#34;https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true&#34; alt=&#34;License: GPL v3&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Forget expensive NVIDIA GPUs, unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, Linux, pretty much any device!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;Update: exo is hiring. See &lt;a href=&#34;https://exolabs.net&#34;&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Get Involved&lt;/h2&gt; &#xA;&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href=&#34;https://x.com/exolabs&#34;&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; &#xA;&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing&#34;&gt;this sheet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Wide Model Support&lt;/h3&gt; &#xA;&lt;p&gt;exo supports different models including LLaMA (&lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py&#34;&gt;MLX&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py&#34;&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen and Deepseek.&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; &#xA;&lt;p&gt;exo &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py&#34;&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; &#xA;&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; &#xA;&lt;p&gt;exo will &lt;a href=&#34;https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/standard_node.py#L154&#34;&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; &#xA;&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; &#xA;&lt;p&gt;exo provides a &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py&#34;&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It&#39;s a &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh&#34;&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; &#xA;&lt;h3&gt;Device Equality&lt;/h3&gt; &#xA;&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href=&#34;https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/standard_node.py#L161&#34;&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; &#xA;&lt;p&gt;Exo supports different &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py&#34;&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py&#34;&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; &#xA;&lt;p&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;ring topology&#34; src=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/docs/ring-topology.png&#34; width=&#34;30%&#34; height=&#34;30%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href=&#34;https://github.com/exo-explore/exo/issues/5&#34;&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; &#xA; &lt;li&gt;Linux (with NVIDIA card): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;NVIDIA driver (test with &lt;code&gt;nvidia-smi&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;CUDA (&lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation&#34;&gt;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation&lt;/a&gt;) (test with &lt;code&gt;nvcc --version&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;cuDNN (&lt;a href=&#34;https://developer.nvidia.com/cudnn-downloads&#34;&gt;https://developer.nvidia.com/cudnn-downloads&lt;/a&gt;) (test with &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older&#34;&gt;link&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;From source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/exo-explore/exo.git&#xA;cd exo&#xA;pip install .&#xA;# alternatively, with venv&#xA;source install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If running on Mac, MLX has an &lt;a href=&#34;https://ml-explore.github.io/mlx/build/html/install.html&#34;&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Upgrade to the latest version of MacOS 15.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Example Usage on Multiple MacOS Devices&lt;/h3&gt; &#xA;&lt;h4&gt;Device 1:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Device 2:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; &#xA;&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href=&#34;https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat&#34;&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href=&#34;http://localhost:8000/v1/chat/completions&#34;&gt;http://localhost:8000/v1/chat/completions&lt;/a&gt;. Example with curls:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;     &#34;model&#34;: &#34;llama-3.1-8b&#34;,&#xA;     &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the meaning of exo?&#34;}],&#xA;     &#34;temperature&#34;: 0.7&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;     &#34;model&#34;: &#34;llava-1.5-7b-hf&#34;,&#xA;     &#34;messages&#34;: [&#xA;      {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;          {&#xA;            &#34;type&#34;: &#34;text&#34;,&#xA;            &#34;text&#34;: &#34;What are these?&#34;&#xA;          },&#xA;          {&#xA;            &#34;type&#34;: &#34;image_url&#34;,&#xA;            &#34;image_url&#34;: {&#xA;              &#34;url&#34;: &#34;http://images.cocodataset.org/val2017/000000039769.jpg&#34;&#xA;            }&#xA;          }&#xA;        ]&#xA;      }&#xA;    ],&#xA;     &#34;temperature&#34;: 0.0&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (MacOS + Linux)&lt;/h3&gt; &#xA;&lt;h4&gt;Device 1 (MacOS):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 main.py --inference-engine tinygrad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; &#xA;&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; &#xA;&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href=&#34;https://docs.tinygrad.org/env_vars/&#34;&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Debugging&lt;/h2&gt; &#xA;&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;DEBUG=9 python3 main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;TINYGRAD_DEBUG=2 python3 main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On some versions of MacOS/Python, certificates are not installed properly which can lead to SSL errors (e.g. SSL error with huggingface.co). To fix this, run the Install Certificates command, usually:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/Applications/Python 3.x/Install Certificates.command&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöß As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it&#39;s ready. If you would like access to the iOS implementation now, please email &lt;a href=&#34;mailto:alex@exolabs.net&#34;&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Inference Engines&lt;/h2&gt; &#xA;&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py&#34;&gt;MLX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py&#34;&gt;tinygrad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/TODO&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Networking Modules&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc&#34;&gt;GRPC&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/TODO&#34;&gt;Radio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/TODO&#34;&gt;Bluetooth&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>make-all/tuya-local</title>
    <updated>2024-09-29T01:36:28Z</updated>
    <id>tag:github.com,2024-09-29:/make-all/tuya-local</id>
    <link href="https://github.com/make-all/tuya-local" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Local support for Tuya devices in Home Assistant&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Home Assistant Tuya Local component&lt;/h1&gt; &#xA;&lt;p&gt;Please report any &lt;a href=&#34;https://github.com/make-all/tuya-local/issues&#34;&gt;issues&lt;/a&gt; and feel free to raise &lt;a href=&#34;https://github.com/make-all/tuya-local/pulls&#34;&gt;pull requests&lt;/a&gt;. &lt;a href=&#34;https://github.com/make-all/tuya-local/raw/main/ACKNOWLEDGEMENTS.md&#34;&gt;Many others&lt;/a&gt; have contributed their help already.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/jasonrumney&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&#34; alt=&#34;BuyMeCoffee&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a Home Assistant integration to support devices running Tuya firmware without going via the Tuya cloud. Devices are supported over WiFi, other technologies need a Tuya gateway device (Zigbee devices will work with other Zigbee gateways, but not via this integration).&lt;/p&gt; &#xA;&lt;p&gt;Note that many Tuya devices seem to support only one local connection. If you have connection issues when using this integration, ensure that other integrations offering local Tuya connections are not configured to use the same device, mobile applications on devices on the local network are closed, and no other software is trying to connect locally to your Tuya devices.&lt;/p&gt; &#xA;&lt;p&gt;Using this integration does not stop your devices from sending status to the Tuya cloud, so this should not be seen as a security measure, rather it improves speed and reliability by using local connections, and may unlock some features of your device, or even unlock whole devices, that are not supported by the Tuya cloud API.&lt;/p&gt; &#xA;&lt;p&gt;A similar but unrelated integration is &lt;a href=&#34;https://github.com/rospogrigio/localtuya/&#34;&gt;rospogrigio/localtuya&lt;/a&gt;, if your device is not supported by this integration, you may find it easier to set up using that as an alternative.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Device support&lt;/h2&gt; &#xA;&lt;p&gt;Note that devices sometimes get firmware upgrades, or incompatible versions are sold under the same model name, so it is possible that the device will not work despite being listed.&lt;/p&gt; &#xA;&lt;p&gt;Battery powered devices such as door and window sensors, smoke alarms etc which do not use a hub will be impossible to support locally, due to the power management that they need to do to get acceptable battery life.&lt;/p&gt; &#xA;&lt;p&gt;Hubs are currently supported, but with limitations. Each connection to a sub device uses a separate network connection, but like other Tuya devices, hubs are usually limited in the number of connections they can handle, with typical limits being 1 or 3, depending on the specific Tuya module they are using. This severely limits the number of sub devices that can be connected through this integration.&lt;/p&gt; &#xA;&lt;p&gt;Tuya Zigbee devices are usually standard zigbee devices, so as an alternative to this integration with a Tuya hub, you can use a supported Zigbee USB stick or Wifi hub with &lt;a href=&#34;https://www.home-assistant.io/integrations/zha/#compatible-hardware&#34;&gt;ZHA&lt;/a&gt; or &lt;a href=&#34;https://www.zigbee2mqtt.io/guide/adapters/&#34;&gt;Zigbee2MQTT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Tuya Bluetooth devices can be supported directly by the &lt;a href=&#34;https://github.com/PlusPlus-ua/ha_tuya_ble/&#34;&gt;tuya_ble&lt;/a&gt; integration.&lt;/p&gt; &#xA;&lt;p&gt;Tuya IR hubs that expose general IR remotes as sub devices usually expose them as one way devices (send only). Due to the way this integration does device detection based on the dps returned by the device, it is not currently able to detect such devices at all. Some specialised IR hubs for air conditioner remote controls do work, as they try to emulate a fully smart air conditioner using internal memory of what settings are currently set, and internal temperature and humidity sensors.&lt;/p&gt; &#xA;&lt;p&gt;A list of currently supported devices can be found in the &lt;a href=&#34;https://github.com/make-all/tuya-local/raw/main/DEVICES.md&#34;&gt;DEVICES.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Documentation on building a device configuration file is in &lt;a href=&#34;https://github.com/make-all/tuya-local/raw/main/custom_components/tuya_local/devices/README.md&#34;&gt;/custom_components/tuya_local/devices/README.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If your device is not listed, you can find the information required to add a configuration for it in the following locations:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;When attempting to add the device, if it is not supported, you will either get a message saying the device cannot be recognised at all, or you will be offered a list of devices (maybe a list of length 1) that are partial matches, often simple switch is among them. You can cancel the process at this point, and look in the Home Assistant log - there should be a message there containing the current data points (dps) returned by the device.&lt;/li&gt; &#xA; &lt;li&gt;If you have signed up for &lt;a href=&#34;https://iot.tuya.com/&#34;&gt;iot.tuya.com&lt;/a&gt; to get your local key, you should also have access to the API Explorer under &#34;Cloud&#34;. Under &#34;Device Control&#34; there is a function called &#34;Query Things Data Model&#34;, which returns the dp_id in addition to range information that is needed for integer and enum data types.&lt;/li&gt; &#xA; &lt;li&gt;By following the method described at the link below, you can find information for all the data points supported by your device, including those not listed by the API explorer method above and those that are only returned under specific conditions. Ignore the requirement for a Tuya Zigbee gateway, that is for Zigbee devices, and this integration does not currently support devices connected via a gateway, but the non-Zigbee/gateway specific parts of the procedure apply also to WiFi devices.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.zigbee2mqtt.io/advanced/support-new-devices/03_find_tuya_data_points.html&#34;&gt;https://www.zigbee2mqtt.io/advanced/support-new-devices/03_find_tuya_data_points.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you file an issue to request support for a new device, please include the following information:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Identification of the device, such as model and brand name.&lt;/li&gt; &#xA; &lt;li&gt;As much information on the datapoints you can gather using the above methods.&lt;/li&gt; &#xA; &lt;li&gt;If manuals or webpages are available online, links to those help understand how to interpret the technical info above - even if they are not in English automatic translations can help, or information in them may help to identify identical devices sold under other brands in other countries that do have English or more detailed information available.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you submit a pull request, please understand that the config file naming and details of the configuration may get modified before release - for example if your name was too generic, I may rename it to a more specific name, or conversely if the device appears to be generic and sold under many brands, I may change the brand specific name to something more general. So it may be necessary to remove and re-add your device once it has been integrated into a release.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hacs/integration&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/HACS-Custom-orange.svg?style=for-the-badge&#34; alt=&#34;hacs_badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Installation is easiest via the &lt;a href=&#34;https://hacs.xyz/&#34;&gt;Home Assistant Community Store (HACS)&lt;/a&gt;, which is the best place to get third-party integrations for Home Assistant. Once you have HACS set up, simply click the button below (requires My Homeassistant configured) or follow the &lt;a href=&#34;https://hacs.xyz/docs/faq/custom_repositories&#34;&gt;instructions for adding a custom repository&lt;/a&gt; and then the integration will be available to install like any other.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://my.home-assistant.io/redirect/hacs_repository/?owner=make-all&amp;amp;repository=tuya-local&amp;amp;category=integration&#34;&gt;&lt;img src=&#34;https://my.home-assistant.io/badges/hacs_repository.svg?sanitize=true&#34; alt=&#34;Open your Home Assistant instance and open a repository inside the Home Assistant Community Store.&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;After installing, you can easily configure your devices using the Integrations configuration UI. Go to Settings / Devices &amp;amp; Services and press the Add Integration button, or click the shortcut button below (requires My Homeassistant configured).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://my.home-assistant.io/redirect/config_flow_start/?domain=tuya_local&#34;&gt;&lt;img src=&#34;https://my.home-assistant.io/badges/config_flow_start.svg?sanitize=true&#34; alt=&#34;Add Integration to your Home Assistant instance.&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Choose your configuration path&lt;/h3&gt; &#xA;&lt;p&gt;There are two options for configuring a device:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can login to Tuya cloud with the Smart Life app and retrieve a list of devices and the necessary local connection data.&lt;/li&gt; &#xA; &lt;li&gt;You can provide all the necessary information manually &lt;a href=&#34;https://raw.githubusercontent.com/make-all/tuya-local/main/#finding-your-device-id-and-local-key&#34;&gt;as per the instructions below&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The first choice essentially automates all the manual steps of the second and without needing to create a Tuya IOT developer account. This is especially important now that Tuya has started time limiting access to a key data access capability in the IOT developer portal to only a month with the ability to refresh the trial of that only every 6 months.&lt;/p&gt; &#xA;&lt;p&gt;The cloud assisted choice will guide you through authenticating, choosing a device to add from the list of devices associated with your Smart Life account, locate the device on your local subnet and then drop you into &lt;a href=&#34;https://raw.githubusercontent.com/make-all/tuya-local/main/#stage-one&#34;&gt;Stage One&lt;/a&gt; with fully populated data necessary to move forward to &lt;a href=&#34;https://raw.githubusercontent.com/make-all/tuya-local/main/#stage-two&#34;&gt;Stage Two&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then Smart Life authentication token expires after a small number of hours and so is not saved by the integration. But, as long as you don&#39;t restart Home Assistant, this allows you to add multiple devices one after another only needing to authenticate once for the first one.&lt;/p&gt; &#xA;&lt;h3&gt;Stage One&lt;/h3&gt; &#xA;&lt;p&gt;The first stage of configuration is to provide the information needed to connect to the device.&lt;/p&gt; &#xA;&lt;p&gt;You will need to provide your device&#39;s IP address or hostname, device ID and local key; the last two can be found using &lt;a href=&#34;https://raw.githubusercontent.com/make-all/tuya-local/main/#finding-your-device-id-and-local-key&#34;&gt;the instructions below&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;host&lt;/h4&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em&gt;(string) (Required)&lt;/em&gt; IP or hostname of the device.&lt;/p&gt; &#xA;&lt;h4&gt;device_id&lt;/h4&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em&gt;(string) (Required)&lt;/em&gt; Device ID retrieved &lt;a href=&#34;https://raw.githubusercontent.com/make-all/tuya-local/main/#finding-your-device-id-and-local-key&#34;&gt;as per the instructions below&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;local_key&lt;/h4&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em&gt;(string) (Required)&lt;/em&gt; Local key retrieved &lt;a href=&#34;https://raw.githubusercontent.com/make-all/tuya-local/main/#finding-your-device-id-and-local-key&#34;&gt;as per the instructions below&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;protocol_version&lt;/h4&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em&gt;(string or float) (Required)&lt;/em&gt; Valid options are &#34;auto&#34;, 3.1, 3.2, 3.3, 3.4. If you aren&#39;t sure, choose &#34;auto&#34;, but some 3.2 and maybe 3.4 devices may be misdetected as 3.3 (or vice-versa), so if your device does not seem to respond to commands reliably, try selecting between those protocol versions.&lt;/p&gt; &#xA;&lt;p&gt;At the end of this step, an attempt is made to connect to the device and see if it returns any data. For tuya protocol version 3.1 devices, the local key is only used for sending commands to the device, so if your local key is incorrect the setup will appear to work, and you will not see any problems until you try to control your device. For more recent Tuya protocol versions, the local key is used to decrypt received data as well, so an incorrect key will be detected at this step and cause an immediate failure. Note that each time you pair the device, the local key changes, so if you obtained the local key using the instructions below, then re-paired with your manufacturer&#39;s app, then the key will have changed already.&lt;/p&gt; &#xA;&lt;h3&gt;Stage Two&lt;/h3&gt; &#xA;&lt;p&gt;The second stage of configuration is to select which device you are connecting. The list of devices offered will be limited to devices which appear to be at least a partial match to the data returned by the device.&lt;/p&gt; &#xA;&lt;h4&gt;type&lt;/h4&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em&gt;(string) (Optional)&lt;/em&gt; The type of Tuya device. Select from the available options.&lt;/p&gt; &#xA;&lt;p&gt;If you pick the wrong type, you will need to delete the device and set it up again. This is because different types of devices create different entities, so changing the device type without deleting everything is not easy.&lt;/p&gt; &#xA;&lt;h3&gt;Stage Three&lt;/h3&gt; &#xA;&lt;p&gt;The final stage is to choose a name for the device in Home Assistant.&lt;/p&gt; &#xA;&lt;p&gt;If you have multiple devices of the same type, you may want to change the name to make it easier to distinguish them.&lt;/p&gt; &#xA;&lt;h4&gt;name&lt;/h4&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em&gt;(string) (Required)&lt;/em&gt; Any unique name for the device. This will be used as the base for the entity names in Home Assistant. Although Home Assistant allows you to change the name later, it will only change the name used in the UI, not the name of the entities.&lt;/p&gt; &#xA;&lt;h2&gt;Offline operation issues&lt;/h2&gt; &#xA;&lt;p&gt;Many Tuya devices will stop responding if unable to connect to the Tuya servers for an extended period. Reportedly, some devices act better offline if DNS as well as TCP connections is blocked.&lt;/p&gt; &#xA;&lt;h2&gt;General issues&lt;/h2&gt; &#xA;&lt;p&gt;Many Tuya devices do not handle multiple commands sent in quick succession. Some will reboot, possibly changing state in the process, others will go offline for 30s to a few minutes if you overload them. There is some rate limiting to try to avoid this, but it is not sufficient for many devices, and may not work across entities where you are sending commands to multiple entities on the same device. The rate limiting also combines commands, which not all devices can handle. If you are sending commands from an automation, it is best to add delays between commands - if your automation is for multiple devices, it might be enough to send commands to other devices first before coming back to send a second command to the first one, or you may still need a delay after that. The exact timing depends on the device, so you may need to experiment to find the minimum delay that gives reliable results.&lt;/p&gt; &#xA;&lt;p&gt;Some devices can handle multiple commands in a single message, so for entity platforms that support it (eg climate &lt;code&gt;set_temperature&lt;/code&gt; can include presets, lights pretty much everything is set through &lt;code&gt;turn_on&lt;/code&gt;) multiple settings are sent at once. But some devices do not like this and require all commands to set only a single dp at a time, so you may need to experiment with your automations to see whether a single command or multiple commands (with delays, see above) work best with your devices.&lt;/p&gt; &#xA;&lt;p&gt;When adding devices, some devices that are detected as protocol version 3.3 at first require version 3.2 to work correctly. Either they cannot be detected, or work as read-only if the pprotocol is set to 3.3.&lt;/p&gt; &#xA;&lt;h2&gt;Heater issues&lt;/h2&gt; &#xA;&lt;p&gt;Goldair GPPH heaters have individual target temperatures for their Comfort and Eco modes, whereas Home Assistant only supports a single target temperature. Therefore, when you&#39;re in Comfort mode you will set the Comfort temperature (&lt;code&gt;5&lt;/code&gt;-&lt;code&gt;35&lt;/code&gt;), and when you&#39;re in Eco mode you will set the Eco temperature (&lt;code&gt;5&lt;/code&gt;-&lt;code&gt;21&lt;/code&gt;), just like you were using the heater&#39;s own control panel. Bear this in mind when writing automations that change the operation mode and set a temperature at the same time: you must change the operation mode &lt;em&gt;before&lt;/em&gt; setting the new target temperature, otherwise you will set the current thermostat rather than the new one.&lt;/p&gt; &#xA;&lt;p&gt;When switching to Anti-freeze mode, the heater will set the current power level to &lt;code&gt;1&lt;/code&gt; as if you had manually chosen it. When you switch back to other modes, you will no longer be in &lt;code&gt;Auto&lt;/code&gt; and will have to set it again if this is what you wanted. This could be worked around in code however it would require storing state that may be cleared if HA is restarted and due to this unreliability it&#39;s probably best that you just factor it into your automations.&lt;/p&gt; &#xA;&lt;p&gt;When child lock is enabled, the heater&#39;s display will flash with the child lock symbol (&lt;code&gt;[]&lt;/code&gt;) whenever you change something in HA. This can be confusing because it&#39;s the same behaviour as when you try to change something via the heater&#39;s own control panel and the change is rejected due to being locked, however rest assured that the changes &lt;em&gt;are&lt;/em&gt; taking effect.&lt;/p&gt; &#xA;&lt;p&gt;When setting the target temperature, different heaters have different behaviour, which you may need to compensate for. From observation, GPPH heaters allow the temperature to reach 3 degrees higher than the set temperature before turning off, and 1 degree lower before turning on again. Kogan Heaters on the other hand turn off when the temperature reaches 1 degree over the target in LOW mode, and turn on again 3 degrees below the target. To make these heaters act the same in LOW power mode, you need to set the Kogan thermostat 2 degrees higher than the GPPH thermostat. In HIGH power mode however, they seem to act the same as the GPPH heaters.&lt;/p&gt; &#xA;&lt;p&gt;The Inkbird thermostat switch does not seem to work for setting anything. If you can figure out how to make setting temperatures and presets work, please leave feedback in Issue #19.&lt;/p&gt; &#xA;&lt;h2&gt;Fan issues&lt;/h2&gt; &#xA;&lt;p&gt;Reportedly, Goldair fans can be a bit flaky. If they become unresponsive, give them about 60 seconds to wake up again.&lt;/p&gt; &#xA;&lt;p&gt;Anko fans mostly work, except setting the speed does not seem to work. If you can figure out how to set the speed through the Tuya protocol for these devices, please leave feedback on Issue #22.&lt;/p&gt; &#xA;&lt;h2&gt;Smart Switch issues&lt;/h2&gt; &#xA;&lt;p&gt;It has been observed after a while that the current and power readings from the switch were returning 0 when there was clearly a load on the switch. After unplugging and replugging, the switch started returning only dps 1 and 2 (switch status and timer). If HomeAssistant is restarted in that state, the switch detection would fail, however as Home Assistant was left running, it continued to work with no readings for the current, power and voltage. I unplugged the switch overnight, and in the morning it was working correctly.&lt;/p&gt; &#xA;&lt;p&gt;Cumulative Energy readings seem to be reset whenever the reading is successfully sent to the server. This leads to the energy usage never moving from the minimum reporting level of 0.1kWh, which isn&#39;t very useful. It may be possible to get useful readings by blocking the switch from accessing the internet, otherwise an integration sensor based on the Power sensor will need to be set up on the Home Assistant side, and the Energy sensor ignored.&lt;/p&gt; &#xA;&lt;p&gt;For the amount of consumed energy, it may be reasonable to use an additional helper - the &lt;a href=&#34;https://www.home-assistant.io/integrations/integration/&#34;&gt;Riemann integral&lt;/a&gt;. Select &lt;code&gt;power&lt;/code&gt; of switch as the sensor for it. The result of the integral will be calculated in &lt;code&gt;(k/M/G/T)W*h&lt;/code&gt; and will correspond to the consumed energy.&lt;/p&gt; &#xA;&lt;h2&gt;Kogan Kettle issues&lt;/h2&gt; &#xA;&lt;p&gt;Although these look like simple devices, their behaviour is not consistent so they are difficult to detect. Sometimes they are misdetected as a simple switch, other times they only output the temperature sensor so are not detected at all.&lt;/p&gt; &#xA;&lt;h2&gt;Beca thermostat issues&lt;/h2&gt; &#xA;&lt;p&gt;Some of these devices support switching between Celcius and Fahrenheit on the control panel, but do not provide any information over the Tuya local protocol about which units are selected. Three configurations for BHP6000 are provided, &lt;code&gt;beca_bhp6000_thermostat_c&lt;/code&gt; and &lt;code&gt;beca_bhp6000_thermostat_f&lt;/code&gt;, which use Celsius and Fahrenheit respectively, and &lt;code&gt;beca_bhp6000_thermostat_mapped&lt;/code&gt; for a buggy looking firmware which displays the temperature on the thermostat in Celsius in increments of half a degree, but uses a slightly offset Fahrenheit for the protocol, as detailed in issue #215. Please select the appropriate config for the temperature units you use. If you change the units on the device control panel, you will need to delete the device from Home Assistant and set it up again.&lt;/p&gt; &#xA;&lt;h2&gt;Saswell C16 thermostat issues&lt;/h2&gt; &#xA;&lt;p&gt;These support configuration as either heating or cooling controllers, but only have one output. The HVAC mode is provided as an indicator of which mode they are in, but are set to readonly so that you cannot accidentally switch the thermostat to the wrong mode from HA.&lt;/p&gt; &#xA;&lt;h2&gt;Finding your device ID and local key&lt;/h2&gt; &#xA;&lt;h3&gt;Tuya IoT developer portal&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to find your local key is with the Tuya Developer portal. If you have previously configured the built in Tuya cloud integration, or localtuya, you probably already have a developer account with the Tuya app linked. Note that you need to use Tuya&#39;s own branded &#34;Tuya Smart&#34; or &#34;SmartLife&#34; apps to access devices through the developer portal. For most devices, your device will work identically with those apps as it does with your manufacturer&#39;s branded app, but there are a few devices where that is not the case and you will need to decide whether you are willing to potentially lose access to some functionality (such as mapping for some vacuum cleaners).&lt;/p&gt; &#xA;&lt;p&gt;If you log on to your Developer Portal account, under Cloud you should be able to get a list of your devices, which contains the &#34;Device ID&#34;. If you don&#39;t see them, check your server is set correctly at the top of the page. Make a note of the Device IDs for all your devices, then select Cloud on the side bar again and go to the API Explorer.&lt;/p&gt; &#xA;&lt;p&gt;Under &#34;Devices Management&#34;, select the &#34;Query Device Details in Bulk&#34; function, and enter your Device IDs, separated by commas. In the results you should see your local_key.&lt;/p&gt; &#xA;&lt;p&gt;The IP address you should be able to get from your router. Using a command line Tuya client like tuyaapi/cli or &lt;a href=&#34;https://github.com/jasonacox/tinytuya&#34;&gt;tinytuya&lt;/a&gt; you may also be able to scan your network for Tuya devices to find the IP address and also automate the above process of connecting to the portal and getting the local key.&lt;/p&gt; &#xA;&lt;h3&gt;Finding device ids and local keys with tinytuya&lt;/h3&gt; &#xA;&lt;p&gt;You can use this component&#39;s underlying library &lt;a href=&#34;https://github.com/jasonacox/tinytuya&#34;&gt;tinytuya&lt;/a&gt; to scan for devices in your network and find the required information about them. In particular, you need to use this procedure to obtain the &lt;code&gt;node_id&lt;/code&gt; value required to connect to hub-dependent devices.&lt;/p&gt; &#xA;&lt;p&gt;Before running tinytuya&#39;s wizard you need to gather your API credentials so head to &lt;a href=&#34;https://iot.tuya.com&#34;&gt;Tuya&#39;s Developer Portal&lt;/a&gt; -&amp;gt; Cloud -&amp;gt; Development -&amp;gt; Open project and make a note of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Access ID/Client ID&lt;/li&gt; &#xA; &lt;li&gt;Access Secret/Client Secret&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Next, go to the &#34;Devices&#34; tab and note your device id (any of them will work). Also note your region (eg. &#34;Central Europe Data Center&#34;) in the combobox at the top right of the page.&lt;/p&gt; &#xA;&lt;p&gt;Then, open a terminal in your HA machine and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m tinytuya wizard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Answer the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enter API Key from tuya.com: your &#34;Access ID/Client ID&#34;&lt;/li&gt; &#xA; &lt;li&gt;Enter API Secret from tuya.com: your &#34;Access Secret/Client Secret&#34;&lt;/li&gt; &#xA; &lt;li&gt;Enter any Device ID currently registered in Tuya App (used to pull full list) or &#39;scan&#39; to scan for one: your device id&lt;/li&gt; &#xA; &lt;li&gt;Enter Your Region: your datacenter&#39;s region&lt;/li&gt; &#xA; &lt;li&gt;Download DP Name mappings? (Y/n): Y&lt;/li&gt; &#xA; &lt;li&gt;Poll local devices? (Y/n): Y&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If your device supports local connections and is in the same network as your HA instance this should find it and report its IP address.&lt;/p&gt; &#xA;&lt;p&gt;In the &lt;code&gt;devices.json&lt;/code&gt; file you will everything you need to add your device:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;id&#34;: the device id&lt;/li&gt; &#xA; &lt;li&gt;&#34;key&#34;: the local key&lt;/li&gt; &#xA; &lt;li&gt;&#34;node_id&#34;: the sub-device id. You need this for hub-dependent devices&lt;/li&gt; &#xA; &lt;li&gt;&#34;mapping&#34;: in the unfortunate case your device is not &lt;a href=&#34;https://raw.githubusercontent.com/make-all/tuya-local/main/DEVICES.md&#34;&gt;yet supported&lt;/a&gt;, this key contains a description of all the datapoints reported by the device, type and expected values. You are more than welcome to create a new device specification following &lt;a href=&#34;https://raw.githubusercontent.com/make-all/tuya-local/main/custom_components/tuya_local/devices/README.md&#34;&gt;the guidelines&lt;/a&gt; and submitting a PR.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Connecting to devices via hubs&lt;/h2&gt; &#xA;&lt;p&gt;If your device connects via a hub (eg. battery powered water timers) you have to provide the following info when adding a new device:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Device id (uuid): this is the &lt;strong&gt;hub&#39;s&lt;/strong&gt; device id&lt;/li&gt; &#xA; &lt;li&gt;IP address or hostname: the &lt;strong&gt;hub&#39;s&lt;/strong&gt; IP address or hostname&lt;/li&gt; &#xA; &lt;li&gt;Local key: the &lt;strong&gt;hub&#39;s&lt;/strong&gt; local key&lt;/li&gt; &#xA; &lt;li&gt;Sub device id: the &lt;strong&gt;actual device you want to control&#39;s&lt;/strong&gt; &lt;code&gt;node_id&lt;/code&gt;. Note this &lt;code&gt;node_id&lt;/code&gt; differs from the device id, you can find it with tinytuya as described below.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Next steps&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;This component is mostly unit-tested thanks to the upstream project, but there are a few more to complete. Feel free to use existing specs as inspiration and the Sonar Cloud analysis to see where the gaps are.&lt;/li&gt; &#xA; &lt;li&gt;Once unit tests are complete, the next task is to complete the Home Assistant quality checklist before considering submission to the HA team for inclusion in standard installations.&lt;/li&gt; &#xA; &lt;li&gt;Discovery seems possible with the new tinytuya library, though the steps to get a local key will most likely remain manual. Discovery also returns a productKey, which might help make the device detection more reliable where different devices use the same dps mapping but different names for the presets for example.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>google-research/circuit_training</title>
    <updated>2024-09-29T01:36:28Z</updated>
    <id>tag:github.com,2024-09-29:/google-research/circuit_training</id>
    <link href="https://github.com/google-research/circuit_training" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AlphaChip: An open-source framework for generating chip floorplans with distributed deep reinforcement learning.&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;AlphaChip&lt;/em&gt; is an open-source framework for generating chip floorplans with distributed deep reinforcement learning. This framework reproduces the methodology published in the Nature 2021 paper:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.nature.com/articles/s41586-021-03544-w&#34;&gt;A graph placement methodology for fast chip design.&lt;/a&gt; Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, Jiwoo Pak, Andy Tong, Kavya Srinivasa, William Hang, Emre Tuncer, Quoc V. Le, James Laudon, Richard Ho, Roger Carpenter &amp;amp; Jeff Dean, 2021. Nature, 594(7862), pp.207-212. &lt;a href=&#34;https://www.nature.com/articles/s41586-021-03544-w.epdf?sharing_token=tYaxh2mR5EozfsSL0WHZLdRgN0jAjWel9jnR3ZoTv0PW0K0NmVrRsFPaMa9Y5We9O4Hqf_liatg-lvhiVcYpHL_YQpqkurA31sxqtmA-E1yNUWVMMVSBxWSp7ZFFIWawYQYnEXoBE4esRDSWqubhDFWUPyI5wK_5B_YIO-D_kS8%3D&#34;&gt;[PDF]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;AlphaChip--one of the first reinforcement learning approaches used to solve a real-world engineering problem--has led to a proliferation of research in AI for chips over the past few years. It is now used to design layouts for chips across Alphabet and outside, and has been extended to various stages of the design process, including logic synthesis, macro selection, timing optimization, and more! We hope that researchers will continue building on top of AlphaChip methodologies and open-source framework. Please see our &lt;a href=&#34;https://deepmind.google/discover/blog/how-alphachip-transformed-computer-chip-design/&#34;&gt;blogpost&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;AlphaChip is built on top of &lt;a href=&#34;https://github.com/tensorflow/agents&#34;&gt;TF-Agents&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow 2.x&lt;/a&gt; with support for eager execution, distributed training across multiple GPUs, and distributed data collection scaling to 100s of actors.&lt;/p&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Features&#34;&gt;Features&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Installation&#34;&gt;Installation&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#QuickStart&#34;&gt;Quick start&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Testing&#34;&gt;Testing&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Releases&#34;&gt;Releases&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#PreTrainedModelCheckpoint&#34;&gt;Pre-Trained Model Checkpoint&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#HowToUseTheCheckpoint&#34;&gt;How to use the checkpoint&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Results&#34;&gt;Results&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#FAQ&#34;&gt;FAQ&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Contributing&#34;&gt;How to contribute&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Principles&#34;&gt;AI Principles&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Contributors&#34;&gt;Contributors&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Citation&#34;&gt;How to cite&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#Disclaimer&#34;&gt;Disclaimer&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Features&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Places netlists with hundreds of macros and millions of stdcells (in clustered format).&lt;/li&gt; &#xA; &lt;li&gt;Computes both macro location and orientation (flipping).&lt;/li&gt; &#xA; &lt;li&gt;Optimizes multiple objectives including wirelength, congestion, and density.&lt;/li&gt; &#xA; &lt;li&gt;Supports alignment of blocks to the grid, to model clock strap or macro blockage.&lt;/li&gt; &#xA; &lt;li&gt;Supports macro-to-macro, macro-to-boundary spacing constraints.&lt;/li&gt; &#xA; &lt;li&gt;Supports fixed macros.&lt;/li&gt; &#xA; &lt;li&gt;Supports &lt;a href=&#34;https://github.com/limbo018/DREAMPlace&#34;&gt;DREAMPlace&lt;/a&gt; as the stdcell placer.&lt;/li&gt; &#xA; &lt;li&gt;Allows users to specify their own technology parameters, e.g. and routing resources (in routes per micron) and macro routing allocation.&lt;/li&gt; &#xA; &lt;li&gt;Generates &lt;a href=&#34;https://github.com/google-research/circuit_training/tree/main/circuit_training/grouping&#34;&gt;clustered netlists&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tilos.ai/&#34;&gt;TILOS-AI-Institute&lt;/a&gt; has created a &lt;a href=&#34;https://github.com/TILOS-AI-Institute/MacroPlacement/tree/main/CodeElements/FormatTranslators&#34;&gt;script&lt;/a&gt; to convert LEF/DEF and Bookshelf to the &lt;a href=&#34;https://github.com/google-research/circuit_training/raw/main/docs/NETLIST_FORMAT.md&#34;&gt;Netlist Protocol Buffer&lt;/a&gt; used as the input for AlphaChip.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Installation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; AlphaChip only supports Linux based OSes.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; AlphaChip requires Python 3.9 or greater.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Stable&lt;/h2&gt; &#xA;&lt;p&gt;AlphaChip is a research project. We are not currently creating PyPi builds. Stable in this instance is relative to HEAD and means that the code was tested at this point in time and branched. With upstream libraires constantly changing; older branches may end up rotting faster than expected.&lt;/p&gt; &#xA;&lt;p&gt;The steps below install the most recent branch and the archive is in the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#releases&#34;&gt;releases section&lt;/a&gt;. There are two methods for installing; but before doing either one you need to run the preliminary setup](#preliminary-setup).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#using-the-docker&#34;&gt;Use the docker&lt;/a&gt; (&lt;strong&gt;Highly Recommended&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#install-locally&#34;&gt;Install locally&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Preliminary Setup&lt;/h3&gt; &#xA;&lt;p&gt;Before following the instructions set the following variables and clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ export CT_VERSION=0.0.4&#xA;# Currently supports python3.9, python3.10, and python3.11&#xA;# The docker is python3.9 only.&#xA;$ export PYTHON_VERSION=python3.9&#xA;$ export DREAMPLACE_PATTERN=dreamplace_20231214_c5a83e5_${PYTHON_VERSION}.tar.gz&#xA;# If the verson of TF-Agents in the table is not current, change this command to&#xA;# match the version tf-agenst that matches the branch of AlphaChip used.&#xA;$ export TF_AGENTS_PIP_VERSION=tf-agents[reverb]&#xA;&#xA;# Clone the Repo and checkout the desired branch.&#xA;$  git clone https://github.com/google-research/circuit_training.git&#xA;$  git -C $(pwd)/circuit_training checkout r${CT_VERSION}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the docker&lt;/h3&gt; &#xA;&lt;p&gt;Do not forget to do the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#preliminary-setup&#34;&gt;prelimary setup&lt;/a&gt;. The cleanest way to use AlphaChip is to use the docker, these commands will create a docker with all the dependencies needed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ export REPO_ROOT=$(pwd)/circuit_training&#xA;&#xA;# Build the docker image.&#xA;$ docker build --pull --no-cache --tag circuit_training:core \&#xA;    --build-arg tf_agents_version=&#34;${TF_AGENTS_PIP_VERSION}&#34; \&#xA;    --build-arg dreamplace_version=&#34;${DREAMPLACE_PATTERN}&#34; \&#xA;    --build-arg placement_cost_binary=&#34;plc_wrapper_main_${CT_VERSION}&#34; \&#xA;    -f &#34;${REPO_ROOT}&#34;/tools/docker/ubuntu_circuit_training ${REPO_ROOT}/tools/docker/&#xA;&#xA;# Run the end2end smoke test using the image. Takes 10-20 minutes.&#xA;$ mkdir -p ${REPO_ROOT}/logs&#xA;$ docker run --rm -v ${REPO_ROOT}:/workspace --workdir /workspace circuit_training:core \&#xA;    bash tools/e2e_smoke_test.sh --root_dir /workspace/logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install locally&lt;/h3&gt; &#xA;&lt;p&gt;Do not forget to do the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#preliminary-setup&#34;&gt;prelimary setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;AlphaChip installation steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install our DREAMPlace binary.&lt;/li&gt; &#xA; &lt;li&gt;Install TF-Agents and The Placement Cost Binary&lt;/li&gt; &#xA; &lt;li&gt;Run a test&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Install DREAMPlace&lt;/h4&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#install-dreamplace&#34;&gt;instructions&lt;/a&gt; for DREAMPlace but do not change the ENV VARS that you already exported previously.&lt;/p&gt; &#xA;&lt;h4&gt;Install TF-Agents and the Placement Cost binary&lt;/h4&gt; &#xA;&lt;p&gt;These commands install TF-Agents and the placement cost binary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Installs TF-Agents with stable versions of Reverb and TensorFlow 2.x.&#xA;$  pip install $TF_AGENTS_PIP_VERSION&#xA;$  pip install tf-keras&#xA;# Using keras-2&#xA;$ export TF_USE_LEGACY_KERAS=1&#xA;# Copies the placement cost binary to /usr/local/bin and makes it executable.&#xA;$  sudo curl https://storage.googleapis.com/rl-infra-public/circuit-training/placement_cost/plc_wrapper_main_${CT_VERSION} \&#xA;     -o  /usr/local/bin/plc_wrapper_main&#xA;$  sudo chmod 555 /usr/local/bin/plc_wrapper_main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run a test.&lt;/h4&gt; &#xA;&lt;p&gt;These commands run a basic unit test; if the current stable tf-agents is not the version you installed, then edit the tox.ini file and change &lt;code&gt;tf-agents[reverb]&lt;/code&gt; to &lt;code&gt;tf-agents[reverb]~=&amp;lt;version you want&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;tox -e py39-stable -- circuit_training/grouping/grouping_test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;HEAD&lt;/h2&gt; &#xA;&lt;p&gt;We recommand using &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#stable&#34;&gt;stable&lt;/a&gt; branches; but our team does work from the &lt;code&gt;HEAD&lt;/code&gt;. The main issue is &lt;code&gt;HEAD&lt;/code&gt; breaks when upstream libraries are broken and our &lt;code&gt;HEAD&lt;/code&gt; utilizes other nightly created libraries adding to the variablity.&lt;/p&gt; &#xA;&lt;p&gt;The steps below install the most recent branch and the archive is in the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#releases&#34;&gt;releases section&lt;/a&gt;. There are two methods for installing; but before doing either one you need to run the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#preliminary-setup-2&#34;&gt;preliminary setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#using-the-docker-2&#34;&gt;Use the docker&lt;/a&gt; (&lt;strong&gt;Highly Recommended&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#install-locally-2&#34;&gt;Install locally&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Preliminary Setup&lt;/h3&gt; &#xA;&lt;p&gt;Before following the instructions set the following variables and clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Currently supports python3.9, python3.10, and python3.11&#xA;# The docker is python3.9 only.&#xA;$ export PYTHON_VERSION=python3.9&#xA;$ export DREAMPLACE_PATTERN=dreamplace_${PYTHON_VERSION}.tar.gz&#xA;&#xA;# Clone the Repo and checkout the desired branch.&#xA;$  git clone https://github.com/google-research/circuit_training.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the docker&lt;/h3&gt; &#xA;&lt;p&gt;Do not forget to do the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#preliminary-setup&#34;&gt;preliminary setup&lt;/a&gt;. The cleanest way to use AlphaChip is to use docker, these commands will create an image with all the dependencies needed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ export REPO_ROOT=$(pwd)/circuit_training&#xA;&#xA;# Builds the image with current DREAMPlace and Placement Cost Binary.&#xA;$ docker build --pull --no-cache --tag circuit_training:core \&#xA;    --build-arg tf_agents_version=&#34;tf-agents-nightly[reverb]&#34; \&#xA;    -f &#34;${REPO_ROOT}&#34;/tools/docker/ubuntu_circuit_training ${REPO_ROOT}/tools/docker/&#xA;&#xA;# Run the end2end smoke test using the image. Takes 10-20 minutes.&#xA;$ mkdir -p ${REPO_ROOT}/logs&#xA;$ docker run --rm -v ${REPO_ROOT}:/workspace --workdir /workspace circuit_training:core \&#xA;    bash tools/e2e_smoke_test.sh --root_dir /workspace/logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install locally&lt;/h3&gt; &#xA;&lt;p&gt;AlphaChip installation steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install our DREAMPlace binary.&lt;/li&gt; &#xA; &lt;li&gt;Install TF-Agents Nightly and the placement cost binary&lt;/li&gt; &#xA; &lt;li&gt;Run a test&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Install DREAMPlace&lt;/h4&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/#install-dreamplace&#34;&gt;instructions&lt;/a&gt; for DREAMPlace but do not change the ENV VARS that you already exported previously.&lt;/p&gt; &#xA;&lt;h4&gt;Install TF-Agents and the Placement Cost binary&lt;/h4&gt; &#xA;&lt;p&gt;These commands install TF-Agents and the placement cost binary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Installs TF-Agents with stable versions of Reverb and TensorFlow 2.x.&#xA;$  pip install tf-agents-nightly[reverb]&#xA;$  pip install tf-keras&#xA;# Using keras-2&#xA;$ export TF_USE_LEGACY_KERAS=1&#xA;# Copies the placement cost binary to /usr/local/bin and makes it executable.&#xA;$  sudo curl https://storage.googleapis.com/rl-infra-public/circuit-training/placement_cost/plc_wrapper_main \&#xA;     -o  /usr/local/bin/plc_wrapper_main&#xA;$  sudo chmod 555 /usr/local/bin/plc_wrapper_main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run a test.&lt;/h4&gt; &#xA;&lt;p&gt;These commands run a basic unit test.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;tox -e py39-nightly -- circuit_training/grouping/grouping_test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install DREAMPlace&lt;/h2&gt; &#xA;&lt;p&gt;DREAMPlace is &lt;strong&gt;not&lt;/strong&gt; provided as a PyPi package and needs to be compiled. We provide compiled versions of DREAMPlace taken from our &lt;a href=&#34;https://github.com/esonghori/DREAMPlace/tree/circuit_training&#34;&gt;branch&lt;/a&gt; for a range of Python versions built for our docker image (Ubuntu 20.4). We also use them for presubmit testing. If our binaries are not compatible with your OS tool chain, you will need to compile your own version. We use this &lt;a href=&#34;https://github.com/google-research/circuit_training/raw/main/tools/bootstrap_dreamplace_build.sh&#34;&gt;script&lt;/a&gt; to create our DREAMPlace binary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# These ENV VARS may have been set above, do not export again if already set.&#xA;$ export PYTHON_VERSION=python3.9&#xA;$ export DREAMPLACE_PATTERN=dreamplace_${PYTHON_VERSION}.tar.gz&#xA;# Installs DREAMPlace into `/dreamplace`. Anywhere is fine as long as PYTHONPATH&#xA;# is set correctly.&#xA;$  mkdir -p /dreamplace&#xA;# Picks the binary that matches your version of Python.&#xA;$  curl https://storage.googleapis.com/rl-infra-public/circuit-training/dreamplace/dreamplace_python3.9.tar.gz -o /dreamplace/dreamplace.tar.gz&#xA;&#xA;# Unpacks the package.&#xA;$  tar xzf /dreamplace/dreamplace.tar.gz -C /dreamplace/&#xA;&#xA;# Sets the python path so we can find Placer with `import dreamplace.Placer`&#xA;# This also needs to put all of DREAMPlace at the root because DREAMPlace python&#xA;# is not setup like a package with imports like `dreamplace.Param`.&#xA;$  export PYTHONPATH=&#34;${PYTHONPATH}:/dreamplace:/dreamplace/dreamplace&#34;&#xA;&#xA;# DREAMPlace requires some additional system and python libraries&#xA;# System packages&#xA;$  apt-get install -y \&#xA;      flex \&#xA;      libcairo2-dev \&#xA;      libboost-all-dev&#xA;&#xA;# Python packages&#xA;$  python3 -mpip install pyunpack&amp;gt;=0.1.2 \&#xA;      patool&amp;gt;=1.12 \&#xA;      timeout-decorator&amp;gt;=0.5.0 \&#xA;      matplotlib&amp;gt;=2.2.2 \&#xA;      cairocffi&amp;gt;=0.9.0 \&#xA;      pkgconfig&amp;gt;=1.4.0 \&#xA;      setuptools&amp;gt;=39.1.0 \&#xA;      scipy&amp;gt;=1.1.0 \&#xA;      numpy&amp;gt;=1.15.4 \&#xA;      torch==1.13.1 \&#xA;      shapely&amp;gt;=1.7.0&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a id=&#34;QuickStart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;The best quick start is to run the &lt;a href=&#34;https://github.com/google-research/circuit_training/tree/main/tools#end-to-end-smoke-test&#34;&gt;end2end smoke test&lt;/a&gt; and then look at the full distributed example &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/docs/ARIANE.md&#34;&gt;AlphaChip for Ariane RISC-V&lt;/a&gt;. For the pre-training on multiple netlists see &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/docs/PRETRAINING.md&#34;&gt;Pre-Training Instruction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Testing&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Runs tests with nightly TF-Agents.&#xA;$  tox -e py39-nightly,py310-nightly,py311-nightly&#xA;# Runs with latest stable TF-Agents.&#xA;$  tox -e py39-stable,py310-stable,py311-stable&#xA;&#xA;# Using our Docker for CI.&#xA;## Build the docker&#xA;$  docker build --tag circuit_training:ci -f tools/docker/ubuntu_ci tools/docker/&#xA;## Runs tests with nightly TF-Agents.&#xA;$  docker run -it --rm -v $(pwd):/workspace --workdir /workspace circuit_training:ci \&#xA;     tox -e py39-nightly,py310-nightly,py311-nightly&#xA;## Runs tests with latest stable TF-Agents.&#xA;$  docker run -it --rm -v $(pwd):/workspace --workdir /workspace circuit_training:ci \&#xA;     tox -e py39-stable,py310-stable,py311-stable&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Releases&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;While running at &lt;code&gt;HEAD&lt;/code&gt; likely works, working from a branch has advantages of being more stable. We have tagged the code base to mark compatibility with stable releases of the underlying libraries. For DREAMPlace the filename pattern can be used to install DREAMPle for the versions of Python supported. For the Placement Cost binary, the ULR is to the version of the PLC used at the time the branch was cut.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Release&lt;/th&gt; &#xA;   &lt;th&gt;Branch / Tag&lt;/th&gt; &#xA;   &lt;th&gt;TF-Agents&lt;/th&gt; &#xA;   &lt;th&gt;DREAMPlace&lt;/th&gt; &#xA;   &lt;th&gt;PL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HEAD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/circuit-training&#34;&gt;main&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;tf-agents-nightly[reverb]&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.0.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/circuit_training/tree/r0.0.4&#34;&gt;v0.0.4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;tf-agents[reverb]~=0.19.0&lt;/td&gt; &#xA;   &lt;td&gt;dreamplace_20231214_c5a83e5_python3.9.tar.gz&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/rl-infra-public/circuit-training/placement_cost/plc_wrapper_main_0.0.4&#34;&gt;plc_wrapper_main_0.0.4&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.0.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/circuit_training/tree/r0.0.3&#34;&gt;v0.0.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;tf-agents[reverb]~=0.16.0&lt;/td&gt; &#xA;   &lt;td&gt;dreamplace_20230414_b31e8af_python3.9.tar.gz&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/rl-infra-public/circuit-training/placement_cost/plc_wrapper_main_0.0.3&#34;&gt;plc_wrapper_main_0.0.3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.0.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/circuit_training/tree/v0.0.2&#34;&gt;v0.0.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;tf-agents[reverb]~=0.16.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a id=&#34;PreTrainedModelCheckpoint&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pre-Trained Model Checkpoint&lt;/h2&gt; &#xA;&lt;p&gt;Unlike prior approaches, our method is a learning-based approach, meaning that it becomes better and faster as it sees and solves more instances of the chip placement problem. This pre-training step significantly improves its speed, reliability, and placement quality, as discussed in the original Nature article and a follow-up study at ISPD 2022 ([Summer Yue, Ebrahim Songhori, Joe Jiang, Toby Boyd, Anna Goldie, Azalia Mirhoseini, Sergio Guadarrama. Scalability and Generalization of Circuit Training for Chip Floorplanning. ISPD, 2022.`] (&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3505170.3511478&#34;&gt;https://dl.acm.org/doi/abs/10.1145/3505170.3511478&lt;/a&gt;)).&lt;/p&gt; &#xA;&lt;p&gt;We release a model checkpoint pre-trained on 20 TPU blocks, which can serve as a starting point for model training and fine-tuning purposes. Please note that, like any other deep learning models (such as large language and vision models), increasing the number of training examples and using in-distribution data during pre-training will improve the quality of results. Therefore, for best results, we strongly recommend &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/docs/PRETRAINING.md&#34;&gt;pre-training on your own chip blocks&lt;/a&gt;, as these will represent the most relevant placement experience for the RL agent.&lt;/p&gt; &#xA;&lt;p&gt;Obviously, not performing any pre-training, i.e., training from scratch, removes the RL agent&#39;s ability to learn from prior experience.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;HowToUseTheCheckpoint&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to use the checkpoint&lt;/h3&gt; &#xA;&lt;p&gt;First, download and untar the checkpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo curl https://storage.googleapis.com/rl-infra-public/circuit-training/tpu_checkpoint_20240815.tar.gz -o $PWD/tpu_checkpoint_20240815.tar.gz&#xA;tar -xvf tpu_checkpoint_20240815.tar.gz&#xA;CHECKPOINT_DIR=$PWD/tpu_checkpoint_20240815/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, set the following flags in the train binary to the provided checkpoint directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3.9 -m circuit_training.learning.train_ppo \&#xA;  ... \&#xA;  --policy_checkpoint_dir=${CHECKPOINT_DIR} \&#xA;  --policy_saved_model_dir=${CHECKPOINT_DIR}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Results&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;The results below are reported for training from scratch, since the pre-trained model cannot be shared at this time.&lt;/p&gt; &#xA;&lt;h3&gt;Ariane RISC-V CPU&lt;/h3&gt; &#xA;&lt;p&gt;View the full details of the Ariane experiment on our &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/docs/ARIANE.md&#34;&gt;details page&lt;/a&gt;. With this code we are able to get comparable or better results training from scratch as fine-tuning a pre-trained model. At the time the paper was published, training from a pre-trained model resulted in better results than training from scratch for the Ariane RISC-V. Improvements to the code have also resulted in 50% less GPU resources needed and a 2x walltime speedup even in training from scratch. Below are the mean and standard deviation for 3 different seeds run 3 times each. This is slightly different than what was used in the paper (8 runs each with a different seed), but better captures the different sources of variability.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Metric&lt;/th&gt; &#xA;   &lt;th&gt;Proxy Wirelength&lt;/th&gt; &#xA;   &lt;th&gt;Proxy Congestion&lt;/th&gt; &#xA;   &lt;th&gt;Proxy Density&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;mean&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.1013&lt;/td&gt; &#xA;   &lt;td&gt;0.9174&lt;/td&gt; &#xA;   &lt;td&gt;0.5502&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;std&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.0036&lt;/td&gt; &#xA;   &lt;td&gt;0.0647&lt;/td&gt; &#xA;   &lt;td&gt;0.0568&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The table below summarizes the &lt;a href=&#34;https://www.nature.com/articles/s41586-021-03544-w.epdf?sharing_token=tYaxh2mR5EozfsSL0WHZLdRgN0jAjWel9jnR3ZoTv0PW0K0NmVrRsFPaMa9Y5We9O4Hqf_liatg-lvhiVcYpHL_YQpqkurA31sxqtmA-E1yNUWVMMVSBxWSp7ZFFIWawYQYnEXoBE4esRDSWqubhDFWUPyI5wK_5B_YIO-D_kS8%3D&#34;&gt;paper&lt;/a&gt; result for fine-tuning from a pre-trained model over 8 runs with each one using a different seed.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Metric&lt;/th&gt; &#xA;   &lt;th&gt;Proxy Wirelength&lt;/th&gt; &#xA;   &lt;th&gt;Proxy Congestion&lt;/th&gt; &#xA;   &lt;th&gt;Proxy Density&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;mean&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.1198&lt;/td&gt; &#xA;   &lt;td&gt;0.9718&lt;/td&gt; &#xA;   &lt;td&gt;0.5729&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;std&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.0019&lt;/td&gt; &#xA;   &lt;td&gt;0.0346&lt;/td&gt; &#xA;   &lt;td&gt;0.0086&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a id=&#34;FAQ&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;p&gt;We wrote this FAQ to answer frequently asked questions about our work. Please reach out to us if you have any other questions!&lt;/p&gt; &#xA;&lt;h4&gt;What is the goal and philosophy of our team?&lt;/h4&gt; &#xA;&lt;p&gt;Our goal is to help chip designers do their jobs better and faster, and we welcome any method that moves us in that direction. To ensure that we are solving real world problems, we work closely with chip designers to understand and address their needs.&lt;/p&gt; &#xA;&lt;h4&gt;What is the impact of our work?&lt;/h4&gt; &#xA;&lt;p&gt;To our knowledge, this is the first deep reinforcement learning (RL) method used in production to design hardware products. More specifically, the RL method described in the Nature paper generated macro placements that were frozen and taped out in Google‚Äôs AI accelerator chip (TPU-v5).&lt;/p&gt; &#xA;&lt;p&gt;We are also excited to see that top EDA and chip design companies (e.g. &lt;a href=&#34;https://www.forbes.com/sites/moorinsights/2020/04/20/using-ai-to-build-better-chips/?sh=63551aef306c&#34;&gt;Synopsys&lt;/a&gt;, &lt;a href=&#34;https://www.zdnet.com/article/ai-on-the-bench-cadence-offers-machine-learning-to-smooth-chip-design/&#34;&gt;Cadence&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/publication/2021-07_NVCell%3A-Standard-Cell&#34;&gt;NVIDIA&lt;/a&gt;, etc.) have announced initiatives to use similar RL-based methods in their tools and chip design efforts.&lt;/p&gt; &#xA;&lt;h4&gt;Have we evaluated our method on open-source benchmarks?&lt;/h4&gt; &#xA;&lt;p&gt;We are focused on modern sub-10nm chips like TPU and Pixel, but we did publish an article in MLCAD 2021 led by Prof. David Pan and his student Zixuan Jiang, where we report results on the open-source ISPD 2015 benchmarks after unfixing macros. In any case, we have open-sourced our method, so the community is free to try it out on any benchmark.&lt;/p&gt; &#xA;&lt;h4&gt;How do we compare to commercial autoplacers?&lt;/h4&gt; &#xA;&lt;p&gt;Due to licensing agreements, we cannot publish any public comparison with commercial autoplacers. However, we can say that our strongest baseline is the physical design team working directly with the assistance of commercial autoplacers, and we outperform this baseline (see ‚Äúmanual‚Äù baseline in Table 1 of our Nature article).&lt;/p&gt; &#xA;&lt;h4&gt;How do we perform clustering of standard cells?&lt;/h4&gt; &#xA;&lt;p&gt;In our Nature paper, we describe how to use hMETIS to cluster standard cells, including all necessary settings. For detailed settings, please see Extended Data Table 3 from our &lt;a href=&#34;http://rdcu.be/cmedX&#34;&gt;Nature article&lt;/a&gt;. Internally, Google pays for a commercial license, but non-commercial entities are welcome to use a free open-source license&lt;/p&gt; &#xA;&lt;p&gt;Regardless, our method runs on unclustered netlists as well, so you can skip the preprocessing step if you wish, though we‚Äôve found clustering to benefit both our RL method and baseline placers. The complexity of our method scales with the number of macros, not the number of standard cells, so the runtime will not be overly affected.&lt;/p&gt; &#xA;&lt;h4&gt;What netlist formats do we support?&lt;/h4&gt; &#xA;&lt;p&gt;Our placer represents netlists in the open-source &lt;a href=&#34;https://developers.google.com/protocol-buffers&#34;&gt;protocol buffer&lt;/a&gt; format. You can learn more about the format &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/docs/NETLIST_FORMAT.md&#34;&gt;here&lt;/a&gt;. To run on netlists in other formats (e.g. LEF/DEF or Bookshelf), you can convert to protocol buffer format. Please see our &lt;a href=&#34;https://github.com/google-research/circuit_training#QuickStart&#34;&gt;quick start guide&lt;/a&gt; for an example of how to use this format on the open-source RISC-V Ariane CPU.&lt;/p&gt; &#xA;&lt;h4&gt;Why do we claim ‚Äúfast chip design‚Äù when RL is slower than analytic solvers?&lt;/h4&gt; &#xA;&lt;p&gt;When we say ‚Äúfast‚Äù, we mean that we actually help chip designers do their jobs faster, not that our algorithm runs fast per se. Our method can, in hours, do what a human chip designer needs weeks or months to perform.&lt;/p&gt; &#xA;&lt;p&gt;If an analytic method optimizes for wirelength and produces a result in ~1 minute, that‚Äôs obviously faster than hours of RL optimization; however, if the result does not meet design criteria and therefore physical design experts must spend weeks further iterating in the loop with commercial EDA tools, then it‚Äôs not faster in any way that matters.&lt;/p&gt; &#xA;&lt;h4&gt;In our Nature experiments, why do we report QoR metrics rather than wirelength alone?&lt;/h4&gt; &#xA;&lt;p&gt;Our goal is to develop methods that help chip designers do their job better and faster. We therefore designed the experiments in our paper to mimic the true production setting as closely as possible, and report QoR (Quality of Result) metrics.&lt;/p&gt; &#xA;&lt;p&gt;QoR metrics can take up to 72 hours to generate with a commercial EDA tool, but are highly accurate measurements of all key metrics, including wirelength, horizontal/vertical congestion, timing (TNS and WNS), power, and area.&lt;/p&gt; &#xA;&lt;p&gt;QoR metrics are closest to physical ground truth and are used by production chip design teams to decide which placements are sent for manufacturing. In contrast, proxy costs like approximate wirelength and congestion can be computed cheaply and are useful for optimization, but are not used to make real world decisions as they can vary significantly from QoR.&lt;/p&gt; &#xA;&lt;p&gt;It is also worth noting that metrics like wirelength and routing congestion directly trade off against each other (e.g. placing nodes close to one another increases congestion, but reduces wirelength), so optimizing or evaluating for wirelength alone is unlikely to result in manufacturable chip layouts.&lt;/p&gt; &#xA;&lt;h4&gt;In our Nature experiments, do we perform any postprocessing on the RL results?&lt;/h4&gt; &#xA;&lt;p&gt;No. In our Nature experiments, we do not apply any postprocessing to the RL results.&lt;/p&gt; &#xA;&lt;p&gt;In our open-source code, we provide an optional 1-5 minute coordinate descent postprocessing step, which we found to slightly improve wirelength. You are welcome to turn it on or off with a flag, and to compare performance with or without it.&lt;/p&gt; &#xA;&lt;h4&gt;What was the process for open-sourcing this code?&lt;/h4&gt; &#xA;&lt;p&gt;Open-sourcing our code involved partnering with another team at Google (&lt;a href=&#34;https://www.tensorflow.org/agents&#34;&gt;TF-Agents&lt;/a&gt;). TF-Agents first replicated the results in our Nature article using our codebase, then reimplemented our method and replicated our results using their own implementation, and then open-sourced their implementation as it does not rely on any internal infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;Getting approval to open-source this code, ensuring compliance with export control restrictions, migrating to TensorFlow 2.x, and removing dependencies from all Google infrastructure was quite time-consuming; but we felt that it was worth the effort to be able to share our method with the community.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Contributing&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to contribute&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re eager to collaborate with you! See &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; for a guide on how to contribute. This project adheres to TensorFlow&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code of conduct.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Principles&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Principles&lt;/h2&gt; &#xA;&lt;p&gt;This project adheres to &lt;a href=&#34;https://raw.githubusercontent.com/google-research/circuit_training/main/PRINCIPLES.md&#34;&gt;Google&#39;s AI principles&lt;/a&gt;. By participating, using or contributing to this project you are expected to adhere to these principles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Main Contributors&lt;/h2&gt; &#xA;&lt;p&gt;We would like to recognize the following individuals for their code contributions, discussions, and other work to make the release of the AlphaChip library possible.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sergio Guadarrama&lt;/li&gt; &#xA; &lt;li&gt;Summer Yue&lt;/li&gt; &#xA; &lt;li&gt;Ebrahim Songhori&lt;/li&gt; &#xA; &lt;li&gt;Joe Jiang&lt;/li&gt; &#xA; &lt;li&gt;Toby Boyd&lt;/li&gt; &#xA; &lt;li&gt;Azalia Mirhoseini&lt;/li&gt; &#xA; &lt;li&gt;Anna Goldie&lt;/li&gt; &#xA; &lt;li&gt;Mustafa Yazgan&lt;/li&gt; &#xA; &lt;li&gt;Shen Wang&lt;/li&gt; &#xA; &lt;li&gt;Terence Tam&lt;/li&gt; &#xA; &lt;li&gt;Young-Joon Lee&lt;/li&gt; &#xA; &lt;li&gt;Roger Carpenter&lt;/li&gt; &#xA; &lt;li&gt;Quoc Le&lt;/li&gt; &#xA; &lt;li&gt;Ed Chi&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Citation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to cite&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code, please cite both:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{mirhoseini2021graph,&#xA;  title={A graph placement methodology for fast chip design},&#xA;  author={Mirhoseini*, Azalia and Goldie*, Anna and Yazgan, Mustafa and Jiang, Joe&#xA;  Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson,&#xA;  Eric and Pathak, Omkar and Nazi, Azade and Pak, Jiwoo and Tong, Andy and&#xA;  Srinivasa, Kavya and Hang, William and Tuncer, Emre and V. Le, Quoc and&#xA;  Laudon, James and Ho, Richard and Carpenter, Roger and Dean, Jeff},&#xA;  journal={Nature},&#xA;  volume={594},&#xA;  number={7862},&#xA;  pages={207--212},&#xA;  year={2021},&#xA;  publisher={Nature Publishing Group}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{CircuitTraining2021,&#xA;  title = {{Circuit Training}: An open-source framework for generating chip&#xA;  floor plans with distributed deep reinforcement learning.},&#xA;  author = {Guadarrama, Sergio and Yue, Summer and Boyd, Toby and Jiang, Joe&#xA;  Wenjie and Songhori, Ebrahim and Tam, Terence, Goldie, Anna and Mirhoseini,&#xA;  Azalia},&#xA;  howpublished = {\url{https://github.com/google_research/circuit_training}},&#xA;  url = &#34;https://github.com/google_research/circuit_training&#34;,&#xA;  year = 2021,&#xA;  note = &#34;[Online; accessed 21-December-2021]&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition, if you used the open-sourced checkpoint, please also cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{AlphaChipCheckpoint2024,&#xA;  title = {A Pre-Trained Checkpoint for {AlphaChip}},&#xA;  author = {Jiang, Joe Wenjie and Songhori, Ebrahim and Mirhoseini, Azalia and&#xA;  Goldie, Anna and Guadarrama, Sergio and Yue, Summer and&#xA;  Boyd, Toby and Tam, Terence, and Wu, Guanhang and Lee, Kuang-Huei and&#xA;  Zhuang, Vincent and Yazgan, Mustafa and and Wang, Shen and Lee, Young-Joon and&#xA;  Johnson, Eric and Pathak, Omkar and Nazi, Azade and Pak, Jiwoo and&#xA;  Tong, Andy and Srinivasa, Kavya and Hang, William and Tuncer, Emre and&#xA;  V. Le, Quoc and Laudon, James and Ho, Richard and Carpenter, Roger and&#xA;  Dean, Jeff},&#xA;  howpublished = {\url{https://github.com/google-research/circuit_training/?tab=readme-ov-file#PreTrainedModelCheckpoint}},&#xA;  url = &#34;https://github.com/google-research/circuit_training/?tab=readme-ov-file#PreTrainedModelCheckpoint&#34;,&#xA;  year = 2024,&#xA;  note = &#34;[Online; accessed 25-September-2024]&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Disclaimer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an official Google product.&lt;/p&gt;</summary>
  </entry>
</feed>