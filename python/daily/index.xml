<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-24T01:42:01Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lucidrains/self-rewarding-lm-pytorch</title>
    <updated>2024-01-24T01:42:01Z</updated>
    <id>tag:github.com,2024-01-24:/lucidrains/self-rewarding-lm-pytorch</id>
    <link href="https://github.com/lucidrains/self-rewarding-lm-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of the training framework proposed in Self-Rewarding Language Model, from MetaAI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/self-rewarding-lm-pytorch/main/diagram.png&#34; width=&#34;450px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Self-Rewarding Language Model (wip)&lt;/h2&gt; &#xA;&lt;p&gt;Implementation of the training framework proposed in &lt;a href=&#34;https://arxiv.org/abs/2401.10020&#34;&gt;Self-Rewarding Language Model&lt;/a&gt;, from MetaAI&lt;/p&gt; &#xA;&lt;p&gt;They really took the &lt;a href=&#34;https://arxiv.org/abs/2305.18290&#34;&gt;title of the DPO paper&lt;/a&gt; to heart.&lt;/p&gt; &#xA;&lt;p&gt;May generalize the framework so one can add &lt;a href=&#34;https://arxiv.org/abs/2401.01335v1&#34;&gt;SPIN&lt;/a&gt; as well.&lt;/p&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://a16z.com/supporting-the-open-source-ai-community/&#34;&gt;A16Z Open Source AI Grant Program&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/&#34;&gt;ðŸ¤— Huggingface&lt;/a&gt; for the generous sponsorships, as well as my other sponsors, for affording me the independence to open source current artificial intelligence research&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{yuan2024selfrewarding,&#xA;    title   = {Self-Rewarding Language Models}, &#xA;    author  = {Weizhe Yuan and Richard Yuanzhe Pang and Kyunghyun Cho and Sainbayar Sukhbaatar and Jing Xu and Jason Weston},&#xA;    year    = {2024},&#xA;    eprint  = {2401.10020},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>InstantID/InstantID</title>
    <updated>2024-01-24T01:42:01Z</updated>
    <id>tag:github.com,2024-01-24:/InstantID/InstantID</id>
    <link href="https://github.com/InstantID/InstantID" rel="alternate"></link>
    <summary type="html">&lt;p&gt;InstantID : Zero-shot Identity-Preserving Generation in Seconds ðŸ”¥&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;InstantID&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://instantid.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.07519&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Technique-Report-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2401.07519&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Paper&amp;amp;message=Huggingface&amp;amp;color=orange&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/InstantX/InstantID&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/zsxkib/instant-id&#34;&gt;&lt;img src=&#34;https://replicate.com/zsxkib/instant-id/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstantID : Zero-shot Identity-Preserving Generation in Seconds&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;InstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/applications.png&#34;&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/1/23] ðŸ”¥ Our pipeline has been merged into &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/examples/community/pipeline_stable_diffusion_xl_instantid.py&#34;&gt;diffusers&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[2024/1/22] ðŸ”¥ We release the &lt;a href=&#34;https://huggingface.co/InstantX/InstantID&#34;&gt;pre-trained checkpoints&lt;/a&gt;, &lt;a href=&#34;https://github.com/InstantID/InstantID/raw/main/infer.py&#34;&gt;inference code&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/InstantX/InstantID&#34;&gt;gradio demo&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[2024/1/15] ðŸ”¥ We release the &lt;a href=&#34;https://arxiv.org/abs/2401.07519&#34;&gt;technical report&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/12/11] ðŸ”¥ We launch the &lt;a href=&#34;https://instantid.github.io/&#34;&gt;project page&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;h3&gt;Stylized Synthesis&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/0.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Comparison with Previous Works&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/compare-a.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Comparison with existing tuning-free state-of-the-art techniques. InstantID achieves better fidelity and retain good text editability (faces and styles blend better).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/compare-c.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Comparison with pre-trained character LoRAs. We don&#39;t need multiple images and still can achieve competitive results as LoRAs without any training.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/compare-b.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Comparison with InsightFace Swapper (also known as ROOP or Refactor). However, in non-realistic style, our work is more flexible on the integration of face and background.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;You can directly download the model from &lt;a href=&#34;https://huggingface.co/InstantX/InstantID&#34;&gt;Huggingface&lt;/a&gt;. You also can download the model in python script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from huggingface_hub import hf_hub_download&#xA;hf_hub_download(repo_id=&#34;InstantX/InstantID&#34;, filename=&#34;ControlNetModel/config.json&#34;, local_dir=&#34;./checkpoints&#34;)&#xA;hf_hub_download(repo_id=&#34;InstantX/InstantID&#34;, filename=&#34;ControlNetModel/diffusion_pytorch_model.safetensors&#34;, local_dir=&#34;./checkpoints&#34;)&#xA;hf_hub_download(repo_id=&#34;InstantX/InstantID&#34;, filename=&#34;ip-adapter.bin&#34;, local_dir=&#34;./checkpoints&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you cannot access to Huggingface, you can use &lt;a href=&#34;https://hf-mirror.com/&#34;&gt;hf-mirror&lt;/a&gt; to download models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export HF_ENDPOINT=https://hf-mirror.com&#xA;huggingface-cli download --resume-download InstantX/InstantID --local-dir checkpoints&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For face encoder, you need to manually download via this &lt;a href=&#34;https://github.com/deepinsight/insightface/issues/1896#issuecomment-1023867304&#34;&gt;URL&lt;/a&gt; to &lt;code&gt;models/antelopev2&lt;/code&gt; as the default link is invalid. Once you have prepared all models, the folder tree should be like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  .&#xA;  â”œâ”€â”€ models&#xA;  â”œâ”€â”€ checkpoints&#xA;  â”œâ”€â”€ ip_adapter&#xA;  â”œâ”€â”€ pipeline_stable_diffusion_xl_instantid.py&#xA;  â””â”€â”€ README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# !pip install opencv-python transformers accelerate insightface&#xA;import diffusers&#xA;from diffusers.utils import load_image&#xA;from diffusers.models import ControlNetModel&#xA;&#xA;import cv2&#xA;import torch&#xA;import numpy as np&#xA;from PIL import Image&#xA;&#xA;from insightface.app import FaceAnalysis&#xA;from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps&#xA;&#xA;# prepare &#39;antelopev2&#39; under ./models&#xA;app = FaceAnalysis(name=&#39;antelopev2&#39;, root=&#39;./&#39;, providers=[&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;])&#xA;app.prepare(ctx_id=0, det_size=(640, 640))&#xA;&#xA;# prepare models under ./checkpoints&#xA;face_adapter = f&#39;./checkpoints/ip-adapter.bin&#39;&#xA;controlnet_path = f&#39;./checkpoints/ControlNetModel&#39;&#xA;&#xA;# load IdentityNet&#xA;controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)&#xA;&#xA;base_model = &#39;wangqixun/YamerMIX_v8&#39;  # from https://civitai.com/models/84040?modelVersionId=196039&#xA;pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(&#xA;    base_model,&#xA;    controlnet=controlnet,&#xA;    torch_dtype=torch.float16&#xA;)&#xA;pipe.cuda()&#xA;&#xA;# load adapter&#xA;pipe.load_ip_adapter_instantid(face_adapter)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can customized your own face images&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load an image&#xA;face_image = load_image(&#34;./examples/yann-lecun_resize.jpg&#34;)&#xA;&#xA;# prepare face emb&#xA;face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))&#xA;face_info = sorted(face_info, key=lambda x:(x[&#39;bbox&#39;][2]-x[&#39;bbox&#39;][0])*x[&#39;bbox&#39;][3]-x[&#39;bbox&#39;][1])[-1]  # only use the maximum face&#xA;face_emb = face_info[&#39;embedding&#39;]&#xA;face_kps = draw_kps(face_image, face_info[&#39;kps&#39;])&#xA;&#xA;# prompt&#xA;prompt = &#34;film noir style, ink sketch|vector, male man, highly detailed, sharp focus, ultra sharpness, monochrome, high contrast, dramatic shadows, 1940s style, mysterious, cinematic&#34;&#xA;negative_prompt = &#34;ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, vibrant, colorful&#34;&#xA;&#xA;# generate image&#xA;pipe.set_ip_adapter_scale(0.8)&#xA;image = pipe(&#xA;    prompt,&#xA;    image_embeds=face_emb,&#xA;    image=face_kps,&#xA;    controlnet_conditioning_scale=0.8,&#xA;).images[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Start a local gradio demo&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python gradio_demo/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For higher similarity, increase the weight of controlnet_conditioning_scale (IdentityNet) and ip_adapter_scale (Adapter).&lt;/li&gt; &#xA; &lt;li&gt;For over-saturation, decrease the ip_adapter_scale. If not work, decrease controlnet_conditioning_scale.&lt;/li&gt; &#xA; &lt;li&gt;For higher text control ability, decrease ip_adapter_scale.&lt;/li&gt; &#xA; &lt;li&gt;For specific styles, choose corresponding base model makes differences.&lt;/li&gt; &#xA; &lt;li&gt;We have not supported multi-person yet, will only use the largest face as reference pose.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/InstantX/InstantID&#34;&gt;Huggingface Space&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://instantid.org/&#34;&gt;instantid.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Replicate Demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/zsxkib/instant-id&#34;&gt;zsxkib/instant-id&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ComfyUI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID&#34;&gt;ZHO-ZHO-ZHO/ComfyUI-InstantID&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huxiuhan/ComfyUI-InstantID&#34;&gt;huxiuhan/ComfyUI-InstantID&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sdbds/InstantID-for-windows&#34;&gt;sdbds/InstantID-for-windows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our work is highly inspired by &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IP-Adapter&lt;/a&gt; and &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;. Thanks for their great works!&lt;/li&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO&#34;&gt;ZHO-ZHO-ZHO&lt;/a&gt;, &lt;a href=&#34;https://github.com/huxiuhan&#34;&gt;huxiuhan&lt;/a&gt;, &lt;a href=&#34;https://github.com/sdbds&#34;&gt;sdbds&lt;/a&gt;, &lt;a href=&#34;https://replicate.com/zsxkib&#34;&gt;zsxkib&lt;/a&gt; for their generous contributions.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the &lt;a href=&#34;https://github.com/huggingface&#34;&gt;HuggingFace&lt;/a&gt; gradio team for their free GPU support!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under &lt;a href=&#34;https://github.com/InstantID/InstantID?tab=Apache-2.0-1-ov-file#readme&#34;&gt;Apache License&lt;/a&gt; and aims to positively impact the field of AI-driven image generation. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;If you find InstantID useful for your research and applications, please cite us using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wang2024instantid,&#xA;  title={InstantID: Zero-shot Identity-Preserving Generation in Seconds},&#xA;  author={Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony},&#xA;  journal={arXiv preprint arXiv:2401.07519},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>MzeroMiko/VMamba</title>
    <updated>2024-01-24T01:42:01Z</updated>
    <id>tag:github.com,2024-01-24:/MzeroMiko/VMamba</id>
    <link href="https://github.com/MzeroMiko/VMamba" rel="alternate"></link>
    <summary type="html">&lt;p&gt;VMamba: Visual State Space Models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;VMamba &lt;/h1&gt; &#xA; &lt;h3&gt;VMamba: Visual State Space Model&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/MzeroMiko&#34;&gt;Yue Liu&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,&lt;a href=&#34;https://sunsmarterjie.github.io/&#34;&gt;Yunjie Tian&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,&lt;a href=&#34;https://scholar.google.com.hk/citations?user=tStQNm4AAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Yuzhong Zhao&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://github.com/yuhongtian17&#34;&gt;Hongtian Yu&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=EEMm7hwAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Lingxi Xie&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=o_DllmIAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Yaowei Wang&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=tjEfgsEAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Qixiang Ye&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=YPL33G0AAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Yunfan Liu&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; University of Chinese Academy of Sciences, &lt;sup&gt;2&lt;/sup&gt; HUAWEI Inc., &lt;sup&gt;3&lt;/sup&gt; PengCheng Lab.&lt;/p&gt; &#xA; &lt;p&gt;Paper: (&lt;a href=&#34;https://arxiv.org/abs/2401.10166&#34;&gt;arXiv 2401.10166&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt; Jan. 23th, 2024&lt;/code&gt;:&lt;/strong&gt; we add an alternative for mamba_ssm and causal_conv1d. Typing &#34;python setup.py install&#34; in classification/models/selective_scan and you can get rid of those two packages. Just turn &#34;self.forward_core = self.forward_corev0&#34; to &#34;self.forward_core = self.forward_corev1&#34; in classification/models/vmamba/vmamba.py#280 to enjoy that feature. This feature is still on testing...&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt; Jan. 22th, 2024&lt;/code&gt;:&lt;/strong&gt; We have released VMamba-T/S pre-trained weights. The ema weights should be converted before transferring to downstream tasks to match the module names using &lt;a href=&#34;https://raw.githubusercontent.com/MzeroMiko/VMamba/main/analyze/get_ckpt.py&#34;&gt;get_ckpt.py&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt; Jan. 19th, 2024&lt;/code&gt;:&lt;/strong&gt; The source code for classification, object detection, and semantic segmentation are provided.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as the two most popular foundation models for visual representation learning. While CNNs exhibit remarkable scalability with linear complexity w.r.t. image resolution, ViTs surpass them in fitting capabilities despite contending with quadratic complexity. A closer inspection reveals that ViTs achieve superior visual modeling performance through the incorporation of global receptive fields and dynamic weights. This observation motivates us to propose a novel architecture that inherits these components while enhancing computational efficiency. To this end, we draw inspiration from the recently introduced state space model and propose the Visual State Space Model (VMamba), which achieves linear complexity without sacrificing global receptive fields. To address the encountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM) to traverse the spatial domain and convert any non-causal visual image into order patch sequences. Extensive experimental results substantiate that VMamba not only demonstrates promising capabilities across various visual perception tasks, but also exhibits more pronounced advantages over established benchmarks as the image resolution increases.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.10166&#34;&gt;&lt;strong&gt;VMamba&lt;/strong&gt;&lt;/a&gt; serves as a general-purpose backbone for computer vision with linear complexity and shows the advantages of global receptive fields and dynamic weights.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MzeroMiko/VMamba/main/assets/acc_flow_comp.png&#34; alt=&#34;accuracy&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2D-Selective-Scan of VMamba&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MzeroMiko/VMamba/main/assets/ss2d.png&#34; alt=&#34;arch&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;VMamba has global effective receptive field&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MzeroMiko/VMamba/main/assets/erf_comp.png&#34; alt=&#34;erf&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Main Results&lt;/h2&gt; &#xA;&lt;p&gt;We will release all the pre-trained models/logs in few days!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Classification on ImageNet-1K&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;pretrain&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;acc@1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;checkpoints/logs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeiT-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.6G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeiT-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeiT-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.7G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1ml7nZM-YPYbQurHiodf4dpXHw88dXFfP/view?usp=sharing&#34;&gt;ckpt&lt;/a&gt;/&lt;a href=&#34;https://drive.google.com/file/d/1mVooWXl1Zj8ZALr1iYuoMLdG_yDbZpRx/view?usp=sharing&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.1G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1gUlRxeHxkn4JG2QR_DoAPbzSFYAoSxDy/view?usp=sharing&#34;&gt;ckpt&lt;/a&gt;/&lt;a href=&#34;https://drive.google.com/file/d/12l81-VsPcCRjyIByWQzyO_EsovVj_00v/view?usp=sharing&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-1K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.2G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;waiting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Object Detection on COCO&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Detector&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;box mAP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mask mAP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;checkpoints/logs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;267G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@1x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;262G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@1x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1SIQFPpXkVBPB4mx1VO9P9nH4ebvTH0W5/view?usp=sharing&#34;&gt;ckpt&lt;/a&gt;/&lt;a href=&#34;https://drive.google.com/file/d/15nd3AZuOkHpqlZhVUEXilnsVzd1qn8Kc/view?usp=sharing&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;354G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@1x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;357G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@1x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1LzytVo2wTKgOxyBadstzacslwol8Dvhq/view?usp=sharing&#34;&gt;ckpt&lt;/a&gt;/&lt;a href=&#34;https://drive.google.com/file/d/1TbYZhban4VqC-9kQ8-kuZOPSBX484sSj/view?usp=sharing&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;107M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;496G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@1x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;482G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@1x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;waiting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;267G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;262G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;waiting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;354G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;357G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MaskRCNN@3x&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;waiting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic Segmentation on ADE20K&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Segmentor&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;checkpoints/logs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;945G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet@160k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;939G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet@160k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1hLAGFBRJfaFSzyPlqsGbKXXN_gQJMLzn/view?usp=sharing&#34;&gt;ckpt&lt;/a&gt;/&lt;a href=&#34;https://drive.google.com/file/d/17nh9_hdF9QQxyqj81U86HoGUnMxZQ4nN/view?usp=sharing&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1039G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet@160k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1037G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet@160k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/18GReI1A6LckwnPrnEFPXp9at7VB8GiJW/view?usp=sharing&#34;&gt;ckpt&lt;/a&gt;/&lt;a href=&#34;https://drive.google.com/file/d/1m-Pd4_kPgF6Dt2E33sfIf_g9jVWxfPnG/view?usp=sharing&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;121M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1188G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet@160k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;110M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1167G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet@160k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;waiting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Swin-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1614G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet@160k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VMamba-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1620G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UperNet@160k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;waiting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install required packages:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda_env=&#34;vmamba&#34;&#xA;nvcc -V&#xA;conda create -n ${conda_env} --clone base&#xA;python -VV&#xA;pip -V&#xA;pip install torch==1.13.0 torchvision==0.14.0 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu117&#xA;# We use py110 cu117 torch113&#xA;pip install packaging&#xA;pip install timm==0.4.12&#xA;pip install pytest chardet yacs termcolor&#xA;pip install submitit tensorboardX&#xA;pip install triton==2.0.0&#xA;pip install causal_conv1d==1.0.0  # causal_conv1d-1.0.0+cu118torch1.13cxx11abiFALSE-cp310-cp310-linux_x86_64.whl&#xA;pip install mamba_ssm==1.0.1  # mamba_ssm-1.0.1+cu118torch1.13cxx11abiFALSE-cp310-cp310-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See more details at &lt;a href=&#34;https://raw.githubusercontent.com/MzeroMiko/VMamba/main/modelcard.sh&#34;&gt;modelcard.sh&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{liu2024vmamba,&#xA;  title={VMamba: Visual State Space Model},&#xA;  author={Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Liu, Yunfan},&#xA;  journal={arXiv preprint arXiv:2401.10166},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on Mamba (&lt;a href=&#34;https://arxiv.org/abs/2312.00752&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/state-spaces/mamba&#34;&gt;code&lt;/a&gt;), Swin-Transformer (&lt;a href=&#34;https://arxiv.org/pdf/2103.14030.pdf&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/Swin-Transformer&#34;&gt;code&lt;/a&gt;), ConvNeXt (&lt;a href=&#34;https://arxiv.org/abs/2201.03545&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/ConvNeXt&#34;&gt;code&lt;/a&gt;), &lt;a href=&#34;https://github.com/open-mmlab&#34;&gt;OpenMMLab&lt;/a&gt;, and the analyze/get_erf.py is adopted from &lt;a href=&#34;https://github.com/DingXiaoH/RepLKNet-pytorch/tree/main/erf&#34;&gt;replknet&lt;/a&gt;, thanks for their excellent works.&lt;/p&gt;</summary>
  </entry>
</feed>