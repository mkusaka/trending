<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-21T03:19:09Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>theowni/Damn-Vulnerable-RESTaurant-API-Game</title>
    <updated>2024-04-21T03:19:09Z</updated>
    <id>tag:github.com,2024-04-21:/theowni/Damn-Vulnerable-RESTaurant-API-Game</id>
    <link href="https://github.com/theowni/Damn-Vulnerable-RESTaurant-API-Game" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Damn Vulnerable Restaurant is an intentionally vulnerable Web API game for learning and training purposes dedicated to developers, ethical hackers and security engineers.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/app/static/img/mad-chef-circle-text.png&#34; alt=&#34;Damn Vulnerable RESTaurant Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;An intentionally vulnerable API service designed for learning and training purposes dedicated to developers, ethical hackers and security engineers. The idea of the project is to provide an environment that can be easily extended with new vulnerable endpoints and mechanisms that could be used in trainings for detecting and exploiting identified vulnerabilities.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s a training playground:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;For Developers&lt;/strong&gt; - engage in a dedicated game where you will identify and fix vulnerabilities interactively.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;For Ethical Hackers&lt;/strong&gt; - exploit vulnerabilities manually or use automated tools. Treat it as a CTF challenge, you can start from low privileged API user and escalate to root user. There is one path to achieve this. API docs are provided to facilitate your hacking adventure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;For Security Engineers&lt;/strong&gt; - utilise various security automation tools such as SAST, DAST, IaC, etc., to test vulnerability detection mechanisms.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üèÜ Hall of Fame&lt;/h2&gt; &#xA;&lt;p&gt;Participants who were able to complete this challenge are listed in &lt;a href=&#34;https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/HALL_OF_FAME.md&#34;&gt;Hall of Fame&lt;/a&gt;. Submit your solution and become one of them!&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Starting the Game&lt;/h2&gt; &#xA;&lt;p&gt;The application can be launched locally in two alternative ways: &lt;strong&gt;Developers&lt;/strong&gt; can play an interactive game where they will investigate and fix vulnerabilities, or &lt;strong&gt;Ethical Hackers&lt;/strong&gt; can identify and exploit vulnerabilities. Furthermore, &lt;a href=&#34;https://github.com/features/codespaces&#34;&gt;GitHub Codespaces&lt;/a&gt; can be used to run the application easily without a local environment! GitHub Codespaces offers up to 60 hours a month for free.&lt;/p&gt; &#xA;&lt;p&gt;The following sections present how to start the game.&lt;/p&gt; &#xA;&lt;h3&gt;üë®‚Äçüíª Developers Approach&lt;/h3&gt; &#xA;&lt;p&gt;For developers, I created a dedicated game where you can identify and fix vulnerabilities in FastAPI based app in an interactive way.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://www.docker.com/get-started/&#34;&gt;Docker&lt;/a&gt; and &lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;Docker Compose V2&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start the game by executing the following commands:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/theowni/Damn-Vulnerable-RESTaurant-API-Game.git&#xA;cd Damn-Vulnerable-RESTaurant-API-Game&#xA;./start_game.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;You should observe the following screen: &lt;img src=&#34;https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/app/static/img/game-screenshot.png&#34; alt=&#34;The Game Entry Screen&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can implement the fixes without needing to restart the Docker instance. Since changes to models are not required, there should be no need for restarts related to migrations.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;At this point, the application is running locally. You can take a look at &lt;strong&gt;step 3&lt;/strong&gt; in next section for more details about accessing the API and documentation.&lt;/p&gt; &#xA;&lt;h3&gt;üëæ Ethical Hackers Approach&lt;/h3&gt; &#xA;&lt;p&gt;You can open RESTaurant easily just with a few steps, you don&#39;t need to have a large budget, rented place, cooks or waitress...&lt;/p&gt; &#xA;&lt;p&gt;You just need to find a Chef and follow the steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://www.docker.com/get-started/&#34;&gt;Docker&lt;/a&gt; and &lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;Docker Compose V2&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy RESTaurant locally with Docker by executing the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/theowni/Damn-Vulnerable-RESTaurant-API-Game.git&#xA;cd Damn-Vulnerable-RESTaurant-API-Game&#xA;./start_app.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The API service will be exposed at &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt; by default. API documentation can be found at the following endpoints:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Swagger - &lt;a href=&#34;http://localhost:8080/docs&#34;&gt;http://localhost:8080/docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Redoc - &lt;a href=&#34;http://localhost:8080/redoc&#34;&gt;http://localhost:8080/redoc&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;To close the restaurant at the end of the hacking day, just run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;./stop_app.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Data will persist between stops and starts.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;re ready to serve the dishes now, &lt;strong&gt;make the Chef mad by exploiting vulns&lt;/strong&gt; or &lt;strong&gt;make the Chef happy by fixing them&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;h3&gt;‚òÅÔ∏è Launch in Github Codespaces&lt;/h3&gt; &#xA;&lt;p&gt;To launch the game in &lt;a href=&#34;https://github.com/features/codespaces&#34;&gt;GitHub Codespaces&lt;/a&gt;, follow the steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Click on: &lt;a href=&#34;https://github.com/new?template_owner=theowni&amp;amp;template_name=Damn-Vulnerable-RESTaurant-API-Game&amp;amp;owner=%40me&amp;amp;name=Damn-Vulnerable-RESTaurant-API-Game&amp;amp;description=My+clone+of+Damn+Vulnerable+RESTaurant+API+Game&amp;amp;visibility=public&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/images/codespace-open-button.png&#34; alt=&#34;Damn Vulnerable RESTaurant Logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To create a codespace, click the &lt;strong&gt;Code&lt;/strong&gt; green drop down button in the upper-right of your repository navigation bar.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click the &lt;strong&gt;Create codespace on main&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After creating a codespace, you can execute &lt;code&gt;./start_game.sh&lt;/code&gt; or &lt;code&gt;./start_app.sh&lt;/code&gt; respectively in the terminal and follow instructions presented in previous sections.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The application can be accessed via a dedicated link. Example codespace environment is shown below (take a look at web app URL scheme):&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/images/codespace-screenshot-2.png&#34; alt=&#34;Damn Vulnerable RESTaurant Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Development Stack&lt;/h2&gt; &#xA;&lt;p&gt;It is developed with Python FastAPI framework and uses PostgreSQL database. The environment is containerised and can be easily deployed locally with Docker. With Python and FastAPI it&#39;s rather simple to extend the application with new vulnerable features in a short amount of time.&lt;/p&gt; &#xA;&lt;p&gt;Damn Vulnerable RESTaurant is not actually limited to any specific type of API, as endpoints may utilize REST API, GraphQL, and others. It&#39;s a restaurant, so various dishes might be served there over a time!&lt;/p&gt; &#xA;&lt;h2&gt;üó∫Ô∏è Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;There are several ideas for improving the project such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; more user-friendly developer&#39;s environment with GitHub Codespaces&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; adding learning resources to each vulnerability to help in better understanding vulns&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; implementing more vulnerabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ideas based on feedback / issues raised in repository&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ù Development / Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Damn Vulnerable RESTaurant was developed with having flexibility in mind. It can be extended with new security issues by following &lt;a href=&#34;https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;‚ö†Ô∏è Warning and Disclaimer!&lt;/h2&gt; &#xA;&lt;p&gt;Damn Vulnerable RESTaurant is damn vulnerable! Do not serve dishes from this RESTaurant on a public streets... ekhm... networks!&lt;/p&gt; &#xA;&lt;p&gt;Do not upload it to your hosting provider&#39;s public html folder or any Internet facing servers, as they will be compromised. It is recommended to deploy RESTaurant locally with Docker.&lt;/p&gt; &#xA;&lt;p&gt;The usage of the Damn Vulnerable RESTaurant application, which is intentionally insecure by design, is at your own risk. We do not assume any responsibility for any potential harm, damage, or security breaches that may arise from using RESTaurant. This application is specifically created for educational and training purposes within controlled environments, such as learning about vulnerabilities and practicing ethical hacking techniques. It is RESTaurant to use RESTaurant responsibly and exclusively in environments that you have the right to access and modify. By using RESTaurant, you acknowledge that any unintended usage or consequences are your sole responsibility, and we disclaim liability for any such actions.&lt;/p&gt; &#xA;&lt;h2&gt;üßæ License&lt;/h2&gt; &#xA;&lt;p&gt;Damn Vulnerable RESTaurant is proudly developed under the terms of the GNU General Public License version 3.0 (GNU GPL v3.0). This license empowers the open-source community by promoting the principles of software freedom, collaboration, and transparency. With GNU GPL v3.0, RESTaurant encourages learning, sharing, and contributions from ethical hackers, security engineers, and developers to collectively enhance their skills and understanding of security vulnerabilities. Please review the LICENSE file for a detailed overview of the rights and responsibilities associated with using and contributing to this project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>meta-llama/llama</title>
    <updated>2024-04-21T03:19:09Z</updated>
    <id>tag:github.com,2024-04-21:/meta-llama/llama</id>
    <link href="https://github.com/meta-llama/llama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference code for Llama models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama 2&lt;/h1&gt; &#xA;&lt;p&gt;We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers, and businesses of all sizes so that they can experiment, innovate, and scale their ideas responsibly.&lt;/p&gt; &#xA;&lt;p&gt;This release includes model weights and starting code for pre-trained and fine-tuned Llama language models ‚Äî ranging from 7B to 70B parameters.&lt;/p&gt; &#xA;&lt;p&gt;This repository is intended as a minimal example to load &lt;a href=&#34;https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/&#34;&gt;Llama 2&lt;/a&gt; models and run inference. For more detailed examples leveraging Hugging Face, see &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes/&#34;&gt;llama-recipes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Updates post-launch&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama/main/UPDATES.md&#34;&gt;UPDATES.md&lt;/a&gt;. Also for a running list of frequently asked questions, see &lt;a href=&#34;https://ai.meta.com/llama/faq/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;In order to download the model weights and tokenizer, please visit the &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;Meta website&lt;/a&gt; and accept our License.&lt;/p&gt; &#xA;&lt;p&gt;Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download.&lt;/p&gt; &#xA;&lt;p&gt;Pre-requisites: Make sure you have &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;md5sum&lt;/code&gt; installed. Then run the script: &lt;code&gt;./download.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as &lt;code&gt;403: Forbidden&lt;/code&gt;, you can always re-request a link.&lt;/p&gt; &#xA;&lt;h3&gt;Access to Hugging Face&lt;/h3&gt; &#xA;&lt;p&gt;We are also providing downloads on &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Hugging Face&lt;/a&gt;. You can request access to the models by acknowledging the license and filling the form in the model card of a repo. After doing so, you should get access to all the Llama models of a version (Code Llama, Llama 2, or Llama Guard) within 1 hour.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You can follow the steps below to quickly get up and running with Llama 2 models. These steps will let you run quick inference locally. For more examples, see the &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes&#34;&gt;Llama 2 recipes repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;In a conda env with PyTorch / CUDA available clone and download this repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the top-level directory run:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Visit the &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;Meta website&lt;/a&gt; and register to download the model/s.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once you get the email, navigate to your downloaded llama repository and run the download.sh script.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure to grant execution permissions to the download.sh script&lt;/li&gt; &#xA;   &lt;li&gt;During this process, you will be prompted to enter the URL from the email.&lt;/li&gt; &#xA;   &lt;li&gt;Do not use the ‚ÄúCopy Link‚Äù option but rather make sure to manually copy the link from the email.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once the model/s you want have been downloaded, you can run the model locally using the command below:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node 1 example_chat_completion.py \&#xA;    --ckpt_dir llama-2-7b-chat/ \&#xA;    --tokenizer_path tokenizer.model \&#xA;    --max_seq_len 512 --max_batch_size 6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Replace &lt;code&gt;llama-2-7b-chat/&lt;/code&gt; with the path to your checkpoint directory and &lt;code&gt;tokenizer.model&lt;/code&gt; with the path to your tokenizer model.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;‚Äìnproc_per_node&lt;/code&gt; should be set to the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama/main/#inference&#34;&gt;MP&lt;/a&gt; value for the model you are using.&lt;/li&gt; &#xA; &lt;li&gt;Adjust the &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; parameters as needed.&lt;/li&gt; &#xA; &lt;li&gt;This example runs the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama/main/example_chat_completion.py&#34;&gt;example_chat_completion.py&lt;/a&gt; found in this repository but you can change that to a different .py file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Different models require different model-parallel (MP) values:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All models support sequence length up to 4096 tokens, but we pre-allocate the cache according to &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; values. So set those according to your hardware.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;These models are not finetuned for chat or Q&amp;amp;A. They should be prompted so that the expected answer is the natural continuation of the prompt.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_text_completion.py&lt;/code&gt; for some examples. To illustrate, see the command below to run it with the llama-2-7b model (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_text_completion.py \&#xA;    --ckpt_dir llama-2-7b/ \&#xA;    --tokenizer_path tokenizer.model \&#xA;    --max_seq_len 128 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuned Chat Models&lt;/h3&gt; &#xA;&lt;p&gt;The fine-tuned models were trained for dialogue applications. To get the expected features and performance for them, a specific formatting defined in &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/llama/generation.py#L212&#34;&gt;&lt;code&gt;chat_completion&lt;/code&gt;&lt;/a&gt; needs to be followed, including the &lt;code&gt;INST&lt;/code&gt; and &lt;code&gt;&amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/code&gt; tags, &lt;code&gt;BOS&lt;/code&gt; and &lt;code&gt;EOS&lt;/code&gt; tokens, and the whitespaces and breaklines in between (we recommend calling &lt;code&gt;strip()&lt;/code&gt; on inputs to avoid double-spaces).&lt;/p&gt; &#xA;&lt;p&gt;You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes/raw/main/examples/inference.py&#34;&gt;an example&lt;/a&gt; of how to add a safety checker to the inputs and outputs of your inference code.&lt;/p&gt; &#xA;&lt;p&gt;Examples using llama-2-7b-chat:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_chat_completion.py \&#xA;    --ckpt_dir llama-2-7b-chat/ \&#xA;    --tokenizer_path tokenizer.model \&#xA;    --max_seq_len 512 --max_batch_size 6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Llama 2 is a new technology that carries potential risks with use. Testing conducted to date has not ‚Äî and could not ‚Äî cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama/main/Responsible-Use-Guide.pdf&#34;&gt;Responsible Use Guide&lt;/a&gt;. More details can be found in our research paper as well.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please report any software ‚Äúbug‚Äù, or other problems with the models through one of the following means:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reporting issues with the model: &lt;a href=&#34;http://github.com/facebookresearch/llama&#34;&gt;github.com/facebookresearch/llama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting risky content generated by the model: &lt;a href=&#34;http://developers.facebook.com/llama_output_feedback&#34;&gt;developers.facebook.com/llama_output_feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting bugs and security concerns: &lt;a href=&#34;http://facebook.com/whitehat/info&#34;&gt;facebook.com/whitehat/info&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Card&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama/main/MODEL_CARD.md&#34;&gt;MODEL_CARD.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file, as well as our accompanying &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama/main/USE_POLICY.md&#34;&gt;Acceptable Use Policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/&#34;&gt;Research Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama&#34;&gt;Llama 2 technical overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/llama/open-innovation-ai-research-community/&#34;&gt;Open Innovation AI Research Community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For common questions, the FAQ can be found &lt;a href=&#34;https://ai.meta.com/llama/faq/&#34;&gt;here&lt;/a&gt; which will be kept up to date over time as new questions arise.&lt;/p&gt; &#xA;&lt;h2&gt;Original Llama&lt;/h2&gt; &#xA;&lt;p&gt;The repo for the original llama release is in the &lt;a href=&#34;https://github.com/facebookresearch/llama/tree/llama_v1&#34;&gt;&lt;code&gt;llama_v1&lt;/code&gt;&lt;/a&gt; branch.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Upsonic/Tiger</title>
    <updated>2024-04-21T03:19:09Z</updated>
    <id>tag:github.com,2024-04-21:/Upsonic/Tiger</id>
    <link href="https://github.com/Upsonic/Tiger" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neuralink for your AI Agents - LangChain - Autogen - CrewAI&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;&lt;img src=&#34;assets/image.svg &#34; height=&#34;28px&#34;&gt; Tiger: Neuralink for your AI Agents&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/#setup&#34;&gt;Setup&lt;/a&gt; ‚Ä¢ &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Integrations:&lt;/b&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/#crewai-integration&#34;&gt;crewAI Integration&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/#langchain-integration&#34;&gt;LangChain Integration&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/#autogen-integration&#34;&gt;AutoGen Integration&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/#signin-to-telegram&#34;&gt;Telegram Integration&lt;/a&gt; ‚Ä¢ &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Sources:&lt;/b&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/#currently-tools&#34;&gt;Currently Tools&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/#creating-your-own-tiger&#34;&gt;Custom Tools (On-Prem Docker)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/#public-dashboard&#34;&gt;Public Dashboard&lt;/a&gt; ‚Ä¢ &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/assets/overview.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;What is Tiger?&lt;/h1&gt; &#xA;&lt;p&gt;Tiger is a community-driven project developing a reusable and integrated tool ecosystem for LLM Agent Revolution. It utilizes Upsonic for isolated tool storage, profillibg and for the automatic generation of documents. With Tiger, you can create a customized environment for your agents or leverage the robust and publicly maintained Tiger üêÖ curated by the community itself.&lt;/p&gt; &#xA;&lt;h2&gt;Details&lt;/h2&gt; &#xA;&lt;p&gt;Tiger, influenced by &lt;a href=&#34;https://neuralink.com/&#34;&gt;Neuralink&lt;/a&gt;, provides an AI-oriented computer interface with threads connected to the LLM interface. It offers a platform for AIs to control a computer by simply &#39;thinking&#39;.&lt;/p&gt; &#xA;&lt;p&gt;With Tiger, your LLM agents can write and execute code, use search engines, manage your calendar, control your mouse and keyboard, speak into your headphones, and much more. Essentially, anything conceived by your agent, Tiger will transform into concrete actions. This embodies the core philosophy of the Tiger project ‚Äì to harness AI intelligence to generate tangible actions and support standard infrastructures. Our goals include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Providing a &lt;strong&gt;Utility point&lt;/strong&gt; for agent tools across any framework that utilizes a function call mechanism,&lt;/li&gt; &#xA; &lt;li&gt;Building and nurturing a &lt;strong&gt;Community of tool support&lt;/strong&gt; across diverse technologies and disciplines,&lt;/li&gt; &#xA; &lt;li&gt;Developing a &lt;strong&gt;Free, Open and MIT&lt;/strong&gt; licensed tool library for the AI agent ecosystem.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Setup&lt;/h1&gt; &#xA;&lt;p&gt;Tiger projects have a general usage public library at &lt;a href=&#34;https://tiger.upsonic.co&#34;&gt;tiger.upsonic.co&lt;/a&gt;. Its include the tools that in &lt;code&gt;tools&lt;/code&gt; library. For usage this you can use the standart connection that in upsonic python library. After installing the &lt;code&gt;upsonic&lt;/code&gt; library we will use the Tiger object wand integrate to your agents.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tiger requires equal or higher python version to 3.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip3 install upsonic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Currently Tools&lt;/h2&gt; &#xA;&lt;p&gt;We are working on Upsonic and the tools that inside the &lt;code&gt;tools&lt;/code&gt; folder is sending to public tiger in each release. We are aiming to create tools without any api key and just like normal human events like searching on google with mouse, keyboard and browser.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Interpreter&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;python &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;check_package&lt;/li&gt; &#xA;     &lt;li&gt;execute&lt;/li&gt; &#xA;     &lt;li&gt;install_package&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;sh &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;execute&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Search&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;google&lt;/li&gt; &#xA;   &lt;li&gt;duckduckgo&lt;/li&gt; &#xA;   &lt;li&gt;read_website&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;System&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;os_name&lt;/li&gt; &#xA;   &lt;li&gt;architecture&lt;/li&gt; &#xA;   &lt;li&gt;python_version&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Knowledge&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;put&lt;/li&gt; &#xA;   &lt;li&gt;pull&lt;/li&gt; &#xA;   &lt;li&gt;delete&lt;/li&gt; &#xA;   &lt;li&gt;index&lt;/li&gt; &#xA;   &lt;li&gt;reset&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Communication&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;telegram &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;as_user &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;delete_message&lt;/li&gt; &#xA;       &lt;li&gt;get_last_dialogs&lt;/li&gt; &#xA;       &lt;li&gt;get_last_messages&lt;/li&gt; &#xA;       &lt;li&gt;send_message&lt;/li&gt; &#xA;       &lt;li&gt;signin&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to add functions to public and strongest Tiger you can see to &lt;a href=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/#adding-tools&#34;&gt;Adding Tools&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Public Dashboard&lt;/h2&gt; &#xA;&lt;p&gt;For the public Tiger you can see the functions and their documentations and readmes in &lt;a href=&#34;https://tiger.upsonic.co&#34;&gt;tiger.upsonic.co&lt;/a&gt;. You can use this place for documentation also.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Auth&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;username: tiger&lt;/li&gt; &#xA; &lt;li&gt;password: tiger&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/assets/dashboard.png&#34; width=&#34;700px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Documentation of Tiger Tools&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to Upsonic we just write the codes and its gives us an storage system with detailed documentation and cpu ram usage for each function. Also you can make search and use functions in your other projects with connection code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Auth&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;username: tiger&lt;/li&gt; &#xA; &lt;li&gt;password: tiger&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Upsonic/Tiger/master/assets/documentation.png&#34; width=&#34;700px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;crewAI Integration&lt;/h1&gt; &#xA;&lt;p&gt;Tiger project aim is being available for most popular agent framworks like &lt;code&gt;crewAI&lt;/code&gt;. In this example you can see the easiest tool integration for an AI agent. We are asking for who is Onur Atakan ULUSOY and waits.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip3 install crewai &#39;crewai[tools]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Geting the tiger tools&#xA;from upsonic import Tiger&#xA;tools = Tiger().crewai()&#xA;&#xA;&#xA;&#xA;from langchain_openai import ChatOpenAI&#xA;llm = ChatOpenAI(model=&#34;gpt-4-0125-preview&#34;, api_key=OPENAI_API_KEY)&#xA;&#xA;&#xA;&#xA;from crewai import Agent, Task, Crew, Process&#xA;&#xA;researcher = Agent(&#xA;  role=&#39;Senior Research Analyst&#39;,&#xA;  goal=&#39;Uncover cutting-edge developments in AI and data science&#39;,&#xA;  backstory=&#34;You are graduated from Research section of University&#34;,&#xA;  verbose=True,&#xA;  allow_delegation=False,&#xA;  tools=tools,&#xA;  llm=llm&#xA;)&#xA;&#xA;&#xA;task1 = Task(&#xA;  description=&#34;&#34;&#34;Who is Onur Atakan ULUSOY&#34;&#34;&#34;,&#xA;  expected_output=&#34;Full analysis report of Onur Atakan ULUSOY and putting the report to knowledge&#34;,&#xA;  agent=researcher&#xA;)&#xA;&#xA;&#xA;crew = Crew(&#xA;  agents=[researcher],&#xA;  tasks=[task1],&#xA;  verbose=2,&#xA;)&#xA;&#xA;&#xA;result = crew.kickoff()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;LangChain Integration&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip3 install langchain langchain-openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Tiger is able to make a collabration for sharing tools with LangChain agents with this your agents will able to use Tiger functions. In this example we are asking for an multiplation question and the agent will use the tiger and after that its write a python code and tiger will give the result in behind. With this agent will able to make mathematical operations in just two lines of code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Geting the tiger tools&#xA;from upsonic import Tiger&#xA;tools = Tiger().langchain()&#xA;&#xA;&#xA;&#xA;# Generating Agent and executor with tiger tool set&#xA;from langchain_openai import ChatOpenAI&#xA;from langchain import hub&#xA;from langchain.agents import AgentExecutor, create_openai_functions_agent&#xA;&#xA;llm = ChatOpenAI(model=&#34;gpt-4-0125-preview&#34;, api_key=OPENAI_API_KEY)&#xA;prompt = hub.pull(&#34;hwchase17/openai-functions-agent&#34;)&#xA;agent = create_openai_functions_agent(llm, tools, prompt)&#xA;agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)&#xA;&#xA;&#xA;# Asking for 15231 * 64231&#xA;agent_executor.invoke({&#34;input&#34;: &#34;What is the result of 15231 * 64231&#34;})&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;AutoGen Integration&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip3 install pyautogen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Tiger is also have a integration with AutoGen agents. You can put a tiger to your AutoGen agents. In this examples we will use the &#39;interpreter.python&#39; module and with this your autogen agent able to run and view result of python codes. With this your agent will able to wait 2 second as we request.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generating Agents with tiger tool set&#xA;from typing_extensions import Annotated&#xA;import autogen&#xA;&#xA;config_list = [&#xA;    {&#xA;        &#39;model&#39;: &#39;gpt-4-0125-preview&#39;,&#xA;        &#39;api_key&#39;: OPENAI_API_KEY,&#xA;    },&#xA;]&#xA;&#xA;llm_config = {&#xA;    &#34;config_list&#34;: config_list,&#xA;    &#34;timeout&#34;: 120,&#xA;}&#xA;chatbot = autogen.AssistantAgent(&#xA;    name=&#34;chatbot&#34;,&#xA;    system_message=&#34;For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.&#34;,&#xA;    llm_config=llm_config,&#xA;)&#xA;&#xA;user_proxy = autogen.UserProxyAgent(&#xA;    name=&#34;user_proxy&#34;,&#xA;    is_termination_msg=lambda x: x.get(&#34;content&#34;, &#34;&#34;) and x.get(&#34;content&#34;, &#34;&#34;).rstrip().endswith(&#34;TERMINATE&#34;),&#xA;    human_input_mode=&#34;NEVER&#34;,&#xA;    max_consecutive_auto_reply=10,&#xA;)&#xA;&#xA;&#xA;&#xA;# Geting the tiger tools&#xA;from upsonic import Tiger&#xA;Tiger().autogen(chatbot, userproxy)&#xA;&#xA;&#xA;&#xA;# Asking sleep 2 second&#xA;user_proxy.initiate_chat(&#xA;        chatbot,&#xA;        message=&#34;What is Upsonic.co&#34;,&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Signin to Telegram&lt;/h2&gt; &#xA;&lt;p&gt;The user who wants to use telegram functionalities in their LLM agents must trig the signin function before all. For this you can use this function and its will ask for phone number and verification code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from upsonic import Tiger&#xA;&#xA;Tiger().get(&#34;communication.telegram.as_user.signin__user&#34;)()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Adding Tools&lt;/h2&gt; &#xA;&lt;p&gt;Tiger project is open to any contribution for public tiger, also in the bottom we have another way to create your own, offline tiger. For adding the public tiger you should create a pull request with your new tool.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a python file in &lt;code&gt;tiger/tools&lt;/code&gt; section. for ex: &lt;code&gt;tiger/tools/interpreter/python/execute.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Write your function in this format&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#imports&#xA;&#xA;def my_function(query:str) -&amp;gt; str:&#xA;    return query + &#34; hi&#34;&#xA;&#xA;&#xA;tool_name = &#34;test.my_function&#34;&#xA;tool_obj = my_function&#xA;tool_requirements = [&#34;beautifulsoup4==4.12.3&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Create the pull request. When its merged its will be available at public Tiger and dashboard.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Creating your Own Tiger&lt;/h2&gt; &#xA;&lt;p&gt;For creating your own tiger you should install a Upsonic On-Prem docker container. Its will give a dashboard for viewing your own tools and will make documentation automatic. After that you should use the Upsonic Client to connect your On-Prem for this you should get the connection code from your dashboard and finaly you use the tiger function in upsonic client.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.upsonic.co/on-prem/getting_started/install_on_prem&#34;&gt;Installation document&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#from upsonic import Tiger&#xA;#Tiger().autogen(chatbot, userproxy)&#xA;&#xA;# to&#xA;&#xA;#Your Upsonic Connection Code&#xA;&#xA;upsonic.autogen(chatbot, userproxy)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://api.star-history.com/svg?repos=Upsonic/Tiger&amp;amp;type=Date&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt;</summary>
  </entry>
</feed>