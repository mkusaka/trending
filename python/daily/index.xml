<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-07T01:42:52Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jupyterlab/jupyter-ai</title>
    <updated>2023-08-07T01:42:52Z</updated>
    <id>tag:github.com,2023-08-07:/jupyterlab/jupyter-ai</id>
    <link href="https://github.com/jupyterlab/jupyter-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A generative AI extension for JupyterLab&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Jupyter AI&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to Jupyter AI, which brings generative AI to Jupyter. Jupyter AI provides a user-friendly and powerful way to explore generative AI models in notebooks and improve your productivity in JupyterLab and the Jupyter Notebook. More specifically, Jupyter AI offers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An &lt;code&gt;%%ai&lt;/code&gt; magic that turns the Jupyter notebook into a reproducible generative AI playground. This works anywhere the IPython kernel runs (JupyterLab, Jupyter Notebook, Google Colab, VSCode, etc.).&lt;/li&gt; &#xA; &lt;li&gt;A native chat UI in JupyterLab that enables you to work with generative AI as a conversational assistant.&lt;/li&gt; &#xA; &lt;li&gt;Support for a wide range of generative model providers and models (AI21, Anthropic, Cohere, Hugging Face, OpenAI, SageMaker, etc.).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Documentation is available on &lt;a href=&#34;https://jupyter-ai.readthedocs.io/en/latest/&#34;&gt;ReadTheDocs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8 - 3.11&lt;/li&gt; &#xA; &lt;li&gt;JupyterLab 4&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Below is a simplified overview of the installation and usage process. See &lt;a href=&#34;https://jupyter-ai.readthedocs.io/en/latest/users/index.html&#34;&gt;our official documentation&lt;/a&gt; for details on installing and using Jupyter AI.&lt;/p&gt; &#xA;&lt;h3&gt;With pip&lt;/h3&gt; &#xA;&lt;p&gt;If you want to install both the &lt;code&gt;%%ai&lt;/code&gt; magic and the JupyterLab extension, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install jupyter_ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are not using JupyterLab and you only want to install the Jupyter AI &lt;code&gt;%%ai&lt;/code&gt; magic, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install jupyter_ai_magics&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;With conda&lt;/h3&gt; &#xA;&lt;p&gt;First, install &lt;a href=&#34;https://conda.io/projects/conda/en/latest/user-guide/install/index.html&#34;&gt;conda&lt;/a&gt; and create an environment that uses Python 3.11:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda create -n jupyter-ai python=3.11&#xA;$ conda activate jupyter-ai&#xA;$ pip install jupyter_ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are not using JupyterLab and you only want to install the Jupyter AI &lt;code&gt;%%ai&lt;/code&gt; magic, skip the &lt;code&gt;pip install jupyter_ai&lt;/code&gt; step above, and instead, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install jupyter_ai_magics&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;The &lt;code&gt;%%ai&lt;/code&gt; magic command&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;%%ai&lt;/code&gt; magic works anywhere the IPython kernel runs (JupyterLab, Jupyter Notebook, Google Colab, Visual Studio Code, etc.).&lt;/p&gt; &#xA;&lt;p&gt;Once you have installed the &lt;code&gt;%%ai&lt;/code&gt; magic, you can enable it in any notebook or the IPython shell by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;%load_ext jupyter_ai_magics&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;%load_ext jupyter_ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The screenshots below are from notebooks in the &lt;code&gt;examples/&lt;/code&gt; directory of this package.&lt;/p&gt; &#xA;&lt;p&gt;Then, you can use the &lt;code&gt;%%ai&lt;/code&gt; magic command to specify a model and natural language prompt:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyterlab/jupyter-ai/main/docs/source/_static/sample-code.png&#34; alt=&#34;Sample with code generation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Jupyter AI can also generate HTML and math to be rendered as cell output.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyterlab/jupyter-ai/main/docs/source/_static/sample-html-math.png&#34; alt=&#34;Sample with HTML and math generation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Jupyter AI can interpolate IPython expressions, allowing you to run prompts that include variable values.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyterlab/jupyter-ai/main/docs/source/_static/sample-markdown.png&#34; alt=&#34;Sample with code interpolation and markdown output&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;JupyterLab extension&lt;/h2&gt; &#xA;&lt;p&gt;The Jupyter AI extension for JupyterLab offers a native UI that enables multiple users to chat with the Jupyter AI conversational assistant. If you have JupyterLab installed, this should be installed and activated when you install the &lt;code&gt;jupyter_ai&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;h2&gt;Using&lt;/h2&gt; &#xA;&lt;p&gt;For help with installing and using Jupyter AI, please see our &lt;a href=&#34;https://jupyter-ai.readthedocs.io/en/latest/users/index.html&#34;&gt;user documentation on ReadTheDocs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to contribute to Jupyter AI, see our &lt;a href=&#34;https://jupyter-ai.readthedocs.io/en/latest/contributors/index.html&#34;&gt;contributor documentation on ReadTheDocs&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mlfoundations/open_flamingo</title>
    <updated>2023-08-07T01:42:52Z</updated>
    <id>tag:github.com,2023-08-07:/mlfoundations/open_flamingo</id>
    <link href="https://github.com/mlfoundations/open_flamingo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source framework for training large multimodal models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸ¦© OpenFlamingo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/open_flamingo&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/open_flamingo.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.01390&#34;&gt;Paper&lt;/a&gt; | Blog posts: &lt;a href=&#34;https://laion.ai/blog/open-flamingo/&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://laion.ai/blog/open-flamingo-v2/&#34;&gt;2&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/openflamingo/OpenFlamingo&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to our open source implementation of DeepMind&#39;s &lt;a href=&#34;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&#34;&gt;Flamingo&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;In this repository, we provide a PyTorch implementation for training and evaluating OpenFlamingo models. If you have any questions, please feel free to open an issue. We also welcome contributions!&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#approach&#34;&gt;Approach&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#model-architecture&#34;&gt;Model architecture&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#initializing-an-openflamingo-model&#34;&gt;Initializing an OpenFlamingo model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#generating-text&#34;&gt;Generating text&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#future-plans&#34;&gt;Future plans&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#team&#34;&gt;Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/#citing&#34;&gt;Citing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;To install the package in an existing environment, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install open-flamingo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to create a conda environment for running OpenFlamingo, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Approach&lt;/h1&gt; &#xA;&lt;p&gt;OpenFlamingo is a multimodal language model that can be used for a variety of tasks. It is trained on a large multimodal dataset (e.g. Multimodal C4) and can be used to generate text conditioned on interleaved images/text. For example, OpenFlamingo can be used to generate a caption for an image, or to generate a question given an image and a text passage. The benefit of this approach is that we are able to rapidly adapt to new tasks using in-context learning.&lt;/p&gt; &#xA;&lt;h2&gt;Model architecture&lt;/h2&gt; &#xA;&lt;p&gt;OpenFlamingo combines a pretrained vision encoder and a language model using cross attention layers. The model architecture is shown below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_flamingo/main/docs/flamingo.png&#34; alt=&#34;OpenFlamingo architecture&#34;&gt; Credit: &lt;a href=&#34;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&#34;&gt;Flamingo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h2&gt;Initializing an OpenFlamingo model&lt;/h2&gt; &#xA;&lt;p&gt;We support pretrained vision encoders from the &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;OpenCLIP&lt;/a&gt; package, which includes OpenAI&#39;s pretrained models. We also support pretrained language models from the &lt;code&gt;transformers&lt;/code&gt; package, such as &lt;a href=&#34;https://huggingface.co/models?search=mosaicml%20mpt&#34;&gt;MPT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=redpajama&#34;&gt;RedPajama&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=llama&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=opt&#34;&gt;OPT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=gpt-neo&#34;&gt;GPT-Neo&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?search=gptj&#34;&gt;GPT-J&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/models?search=pythia&#34;&gt;Pythia&lt;/a&gt; models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from open_flamingo import create_model_and_transforms&#xA;&#xA;model, image_processor, tokenizer = create_model_and_transforms(&#xA;    clip_vision_encoder_path=&#34;ViT-L-14&#34;,&#xA;    clip_vision_encoder_pretrained=&#34;openai&#34;,&#xA;    lang_encoder_path=&#34;anas-awadalla/mpt-1b-redpajama-200b&#34;,&#xA;    tokenizer_path=&#34;anas-awadalla/mpt-1b-redpajama-200b&#34;,&#xA;    cross_attn_every_n_layers=1&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Released OpenFlamingo models&lt;/h2&gt; &#xA;&lt;p&gt;We have trained the following OpenFlamingo models so far.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;# params&lt;/th&gt; &#xA;   &lt;th&gt;Language model&lt;/th&gt; &#xA;   &lt;th&gt;Vision encoder&lt;/th&gt; &#xA;   &lt;th&gt;Xattn interval*&lt;/th&gt; &#xA;   &lt;th&gt;COCO 4-shot CIDEr&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2 4-shot Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;mosaicml/mpt-1b-redpajama-200b&lt;/td&gt; &#xA;   &lt;td&gt;openai CLIP ViT-L/14&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;77.3&lt;/td&gt; &#xA;   &lt;td&gt;45.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;mosaicml/mpt-1b-redpajama-200b-dolly&lt;/td&gt; &#xA;   &lt;td&gt;openai CLIP ViT-L/14&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;82.7&lt;/td&gt; &#xA;   &lt;td&gt;45.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b-langinstruct&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4B&lt;/td&gt; &#xA;   &lt;td&gt;togethercomputer/RedPajama-INCITE-Base-3B-v1&lt;/td&gt; &#xA;   &lt;td&gt;openai CLIP ViT-L/14&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;81.8&lt;/td&gt; &#xA;   &lt;td&gt;49.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openflamingo/OpenFlamingo-4B-vitl-rpj3b&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4B&lt;/td&gt; &#xA;   &lt;td&gt;togethercomputer/RedPajama-INCITE-Instruct-3B-v1&lt;/td&gt; &#xA;   &lt;td&gt;openai CLIP ViT-L/14&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;85.8&lt;/td&gt; &#xA;   &lt;td&gt;49.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openflamingo/OpenFlamingo-4B-vitl-rpj3b-langinstruct&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9B&lt;/td&gt; &#xA;   &lt;td&gt;mosaicml/mpt-7b&lt;/td&gt; &#xA;   &lt;td&gt;openai CLIP ViT-L/14&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;89.0&lt;/td&gt; &#xA;   &lt;td&gt;54.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;* Xattn interval refers to the &lt;code&gt;--cross_attn_every_n_layers&lt;/code&gt; argument.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: as part of our v2 release, we have deprecated a previous LLaMA-based checkpoint. However, you can continue to use our older checkpoint using the new codebase.&lt;/p&gt; &#xA;&lt;h2&gt;Downloading pretrained weights&lt;/h2&gt; &#xA;&lt;p&gt;To instantiate an OpenFlamingo model with one of our released weights, initialize the model as above and use the following code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# grab model checkpoint from huggingface hub&#xA;from huggingface_hub import hf_hub_download&#xA;import torch&#xA;&#xA;checkpoint_path = hf_hub_download(&#34;openflamingo/OpenFlamingo-3B-vitl-mpt1b&#34;, &#34;checkpoint.pt&#34;)&#xA;model.load_state_dict(torch.load(checkpoint_path), strict=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating text&lt;/h2&gt; &#xA;&lt;p&gt;Below is an example of generating text conditioned on interleaved images/text. In particular, let&#39;s try few-shot image captioning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;import requests&#xA;import torch&#xA;&#xA;&#34;&#34;&#34;&#xA;Step 1: Load images&#xA;&#34;&#34;&#34;&#xA;demo_image_one = Image.open(&#xA;    requests.get(&#xA;        &#34;http://images.cocodataset.org/val2017/000000039769.jpg&#34;, stream=True&#xA;    ).raw&#xA;)&#xA;&#xA;demo_image_two = Image.open(&#xA;    requests.get(&#xA;        &#34;http://images.cocodataset.org/test-stuff2017/000000028137.jpg&#34;,&#xA;        stream=True&#xA;    ).raw&#xA;)&#xA;&#xA;query_image = Image.open(&#xA;    requests.get(&#xA;        &#34;http://images.cocodataset.org/test-stuff2017/000000028352.jpg&#34;, &#xA;        stream=True&#xA;    ).raw&#xA;)&#xA;&#xA;&#xA;&#34;&#34;&#34;&#xA;Step 2: Preprocessing images&#xA;Details: For OpenFlamingo, we expect the image to be a torch tensor of shape &#xA; batch_size x num_media x num_frames x channels x height x width. &#xA; In this case batch_size = 1, num_media = 3, num_frames = 1,&#xA; channels = 3, height = 224, width = 224.&#xA;&#34;&#34;&#34;&#xA;vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]&#xA;vision_x = torch.cat(vision_x, dim=0)&#xA;vision_x = vision_x.unsqueeze(1).unsqueeze(0)&#xA;&#xA;&#34;&#34;&#34;&#xA;Step 3: Preprocessing text&#xA;Details: In the text we expect an &amp;lt;image&amp;gt; special token to indicate where an image is.&#xA; We also expect an &amp;lt;|endofchunk|&amp;gt; special token to indicate the end of the text &#xA; portion associated with an image.&#xA;&#34;&#34;&#34;&#xA;tokenizer.padding_side = &#34;left&#34; # For generation padding tokens should be on the left&#xA;lang_x = tokenizer(&#xA;    [&#34;&amp;lt;image&amp;gt;An image of two cats.&amp;lt;|endofchunk|&amp;gt;&amp;lt;image&amp;gt;An image of a bathroom sink.&amp;lt;|endofchunk|&amp;gt;&amp;lt;image&amp;gt;An image of&#34;],&#xA;    return_tensors=&#34;pt&#34;,&#xA;)&#xA;&#xA;&#xA;&#34;&#34;&#34;&#xA;Step 4: Generate text&#xA;&#34;&#34;&#34;&#xA;generated_text = model.generate(&#xA;    vision_x=vision_x,&#xA;    lang_x=lang_x[&#34;input_ids&#34;],&#xA;    attention_mask=lang_x[&#34;attention_mask&#34;],&#xA;    max_new_tokens=20,&#xA;    num_beams=3,&#xA;)&#xA;&#xA;print(&#34;Generated text: &#34;, tokenizer.decode(generated_text[0]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;p&gt;We provide training scripts in &lt;code&gt;open_flamingo/train&lt;/code&gt;. We provide an example Slurm script in &lt;code&gt;open_flamingo/scripts/run_train.py&lt;/code&gt;, as well as the following example command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nnodes=1 --nproc_per_node=4 open_flamingo/train/train.py \&#xA;  --lm_path anas-awadalla/mpt-1b-redpajama-200b \&#xA;  --tokenizer_path anas-awadalla/mpt-1b-redpajama-200b \&#xA;  --cross_attn_every_n_layers 1 \&#xA;  --dataset_resampled \&#xA;  --batch_size_mmc4 32 \&#xA;  --batch_size_laion 64 \&#xA;  --train_num_samples_mmc4 125000\&#xA;  --train_num_samples_laion 250000 \&#xA;  --loss_multiplier_laion 0.2 \&#xA;  --workers=4 \&#xA;  --run_name OpenFlamingo-3B-vitl-mpt1b \&#xA;  --num_epochs 480 \&#xA;  --warmup_steps  1875 \&#xA;  --mmc4_textsim_threshold 0.24 \&#xA;  --laion_shards &#34;/path/to/shards/shard-{0000..0999}.tar&#34; \&#xA;  --mmc4_shards &#34;/path/to/shards/shard-{0000..0999}.tar&#34; \&#xA;  --report_to_wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: The MPT-1B &lt;a href=&#34;https://huggingface.co/mosaicml/mpt-1b-redpajama-200b&#34;&gt;base&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly&#34;&gt;instruct&lt;/a&gt; modeling code does not accept the &lt;code&gt;labels&lt;/code&gt; kwarg or compute cross-entropy loss directly within &lt;code&gt;forward()&lt;/code&gt;, as expected by our codebase. We suggest using a modified version of the MPT-1B models found &lt;a href=&#34;https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b-dolly&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more details, see our &lt;a href=&#34;https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/train&#34;&gt;training README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Evaluation&lt;/h1&gt; &#xA;&lt;p&gt;An example evaluation script is at &lt;code&gt;open_flamingo/scripts/run_eval.sh&lt;/code&gt;. Please see our &lt;a href=&#34;https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/eval&#34;&gt;evaluation README&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;To run evaluations on OKVQA you will need to run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import nltk&#xA;nltk.download(&#39;wordnet&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Future plans&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add support for video input&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Team&lt;/h1&gt; &#xA;&lt;p&gt;OpenFlamingo is developed by:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://anas-awadalla.streamlit.app/&#34;&gt;Anas Awadalla*&lt;/a&gt;, &lt;a href=&#34;https://i-gao.github.io/&#34;&gt;Irena Gao*&lt;/a&gt;, &lt;a href=&#34;https://homes.cs.washington.edu/~jpgard/&#34;&gt;Joshua Gardner&lt;/a&gt;, &lt;a href=&#34;https://jmhessel.com/&#34;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/yusufhanafy/&#34;&gt;Yusuf Hanafy&lt;/a&gt;, &lt;a href=&#34;https://wanrong-zhu.com/&#34;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/uw.edu/kalyanimarathe/home?authuser=0&#34;&gt;Kalyani Marathe&lt;/a&gt;, &lt;a href=&#34;https://yonatanbitton.github.io/&#34;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&#34;https://sagadre.github.io/&#34;&gt;Samir Gadre&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/~ssagawa/&#34;&gt;Shiori Sagawa&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.de/citations?user=p1FuAMkAAAAJ&amp;amp;hl=en&#34;&gt;Jenia Jitsev&lt;/a&gt;, &lt;a href=&#34;https://simonster.com/&#34;&gt;Simon Kornblith&lt;/a&gt;, &lt;a href=&#34;https://koh.pw/&#34;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&#34;https://gabrielilharco.com/&#34;&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href=&#34;https://mitchellnw.github.io/&#34;&gt;Mitchell Wortsman&lt;/a&gt;, &lt;a href=&#34;https://people.csail.mit.edu/ludwigs/&#34;&gt;Ludwig Schmidt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The team is primarily from the University of Washington, Stanford, AI2, UCSB, and Google.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgments&lt;/h1&gt; &#xA;&lt;p&gt;This code is based on Lucidrains&#39; &lt;a href=&#34;https://github.com/lucidrains/flamingo-pytorch&#34;&gt;flamingo implementation&lt;/a&gt; and David Hansmair&#39;s &lt;a href=&#34;https://github.com/dhansmair/flamingo-mini&#34;&gt;flamingo-mini repo&lt;/a&gt;. Thank you for making your code public! We also thank the &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;OpenCLIP&lt;/a&gt; team as we use their data loading code and take inspiration from their library design.&lt;/p&gt; &#xA;&lt;p&gt;We would also like to thank &lt;a href=&#34;https://www.jbalayrac.com&#34;&gt;Jean-Baptiste Alayrac&lt;/a&gt; and &lt;a href=&#34;https://antoine77340.github.io&#34;&gt;Antoine Miech&lt;/a&gt; for their advice, &lt;a href=&#34;https://www.rohantaori.com/&#34;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&#34;https://nicholasschiefer.com/&#34;&gt;Nicholas Schiefer&lt;/a&gt;, &lt;a href=&#34;https://hai.stanford.edu/people/deep-ganguli&#34;&gt;Deep Ganguli&lt;/a&gt;, &lt;a href=&#34;https://thomasliao.com/&#34;&gt;Thomas Liao&lt;/a&gt;, &lt;a href=&#34;https://thashim.github.io/&#34;&gt;Tatsunori Hashimoto&lt;/a&gt;, and &lt;a href=&#34;https://nicholas.carlini.com/&#34;&gt;Nicholas Carlini&lt;/a&gt; for their help with assessing the safety risks of our release, and to &lt;a href=&#34;https://stability.ai&#34;&gt;Stability AI&lt;/a&gt; for providing us with compute resources to train these models.&lt;/p&gt; &#xA;&lt;h1&gt;Citing&lt;/h1&gt; &#xA;&lt;p&gt;If you found this repository useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{anas_awadalla_2023_7733589,&#xA;  author = {Awadalla, Anas and Gao, Irena and Gardner, Joshua and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Jitsev, Jenia and Kornblith, Simon and Koh, Pang Wei and Ilharco, Gabriel and Wortsman, Mitchell and Schmidt, Ludwig},&#xA;  title = {OpenFlamingo},&#xA;  month        = mar,&#xA;  year         = 2023,&#xA;  publisher    = {Zenodo},&#xA;  version      = {v0.1.1},&#xA;  doi          = {10.5281/zenodo.7733589},&#xA;  url          = {https://doi.org/10.5281/zenodo.7733589}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{Alayrac2022FlamingoAV,&#xA;  title={Flamingo: a Visual Language Model for Few-Shot Learning},&#xA;  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},&#xA;  journal={ArXiv},&#xA;  year={2022},&#xA;  volume={abs/2204.14198}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>musabgultekin/functionary</title>
    <updated>2023-08-07T01:42:52Z</updated>
    <id>tag:github.com,2023-08-07:/musabgultekin/functionary</id>
    <link href="https://github.com/musabgultekin/functionary" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat language model that can interpret and execute functions/plugins&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Functionary&lt;/h1&gt; &#xA;&lt;img align=&#34;right&#34; width=&#34;256&#34; height=&#34;256&#34; src=&#34;https://github.com/musabgultekin/functionary/assets/3749407/c7a1972d-6ad7-40dc-8000-dceabe6baabd&#34;&gt; &#xA;&lt;p&gt;Functionary is a language model that can interpret and execute functions/plugins.&lt;/p&gt; &#xA;&lt;p&gt;The model determines when to execute a function and can understand its output. It only triggers functions as needed. Function definitions are given as JSON Schema Objects, similar to OpenAI GPT function calls.&lt;/p&gt; &#xA;&lt;p&gt;Based on &lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Llama 2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;OpenAI compatible server&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you have &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt; installed. Then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;python3 server.py --model &#34;musabgultekin/functionary-7b-v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or start blazing fast &lt;a href=&#34;https://vllm.readthedocs.io/en/latest/getting_started/installation.html&#34;&gt;vLLM&lt;/a&gt; server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 server_vllm.py --model &#34;musabgultekin/functionary-7b-v1&#34; --host 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Server Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;&#xA;openai.api_base = &#34;http://localhost:8000/v1&#34;&#xA;openai.api_key = &#34;functionary&#34; # We just need to set this something other than None, so it works with openai package. No API key is required.&#xA;&#xA;openai.ChatCompletion.create(&#xA;    model=&#34;musabgultekin/functionary-7b-v1&#34;,&#xA;    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the weather for Istanbul?&#34;}],&#xA;    functions=[{&#xA;        &#34;name&#34;: &#34;get_current_weather&#34;,&#xA;        &#34;description&#34;: &#34;Get the current weather&#34;,&#xA;        &#34;parameters&#34;: {&#xA;            &#34;type&#34;: &#34;object&#34;,&#xA;            &#34;properties&#34;: {&#xA;                &#34;location&#34;: {&#xA;                    &#34;type&#34;: &#34;string&#34;,&#xA;                    &#34;description&#34;: &#34;The city and state, e.g. San Francisco, CA&#34;&#xA;                },&#xA;            },&#xA;            &#34;required&#34;: [&#34;location&#34;],&#xA;        },&#xA;    }]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re having trouble with dependencies, and you have &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#setting-up-nvidia-container-toolkit&#34;&gt;nvidia-container-toolkit&lt;/a&gt;, you can start your environment like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo docker run --gpus all -it --shm-size=8g --name functionary -v ${PWD}/functionary_workspace:/workspace -p 8000:8000 nvcr.io/nvidia/pytorch:22.12-py3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Use Cases&lt;/h1&gt; &#xA;&lt;p&gt;Here are a few examples of how you can use this function calling system:&lt;/p&gt; &#xA;&lt;h3&gt;Travel and Hospitality - Trip Planning&lt;/h3&gt; &#xA;&lt;p&gt;The function &lt;code&gt;plan_trip(destination: string, duration: int, interests: list)&lt;/code&gt; can take user input such as &#34;I want to plan a 7-day trip to Paris with a focus on art and culture&#34; and generate an itinerary accordingly.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Details (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;openai.ChatCompletion.create(&#xA;    model=&#34;musabgultekin/functionary-7b-v1&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#39;I want to plan a 7-day trip to Paris with a focus on art and culture&#39;},&#xA;    ], &#xA;    functions=[&#xA;        {&#xA;            &#34;name&#34;: &#34;plan_trip&#34;,&#xA;            &#34;description&#34;: &#34;Plan a trip based on user&#39;s interests&#34;,&#xA;            &#34;parameters&#34;: {&#xA;                &#34;type&#34;: &#34;object&#34;,&#xA;                &#34;properties&#34;: {&#xA;                    &#34;destination&#34;: {&#xA;                        &#34;type&#34;: &#34;string&#34;,&#xA;                        &#34;description&#34;: &#34;The destination of the trip&#34;,&#xA;                    },&#xA;                    &#34;duration&#34;: {&#xA;                        &#34;type&#34;: &#34;integer&#34;,&#xA;                        &#34;description&#34;: &#34;The duration of the trip in days&#34;,&#xA;                    },&#xA;                    &#34;interests&#34;: {&#xA;                        &#34;type&#34;: &#34;array&#34;,&#xA;                        &#34;items&#34;: {&#34;type&#34;: &#34;string&#34;},&#xA;                        &#34;description&#34;: &#34;The interests based on which the trip will be planned&#34;,&#xA;                    },&#xA;                },&#xA;                &#34;required&#34;: [&#34;destination&#34;, &#34;duration&#34;, &#34;interests&#34;],&#xA;            },&#xA;        },&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Response will have:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;role&#34;: &#34;assistant&#34;, &#34;function_call&#34;: {&#34;name&#34;: &#34;plan_trip&#34;, &#34;arguments&#34;: &#39;{\n  &#34;destination&#34;: &#34;Paris&#34;,\n  &#34;duration&#34;: 7,\n  &#34;interests&#34;: [&#34;art&#34;, &#34;culture&#34;]\n}&#39;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then you need to call &lt;code&gt;plan_trip&lt;/code&gt; function with provided arguments. If you would like a commentary from the model, then you&#39;ll call the model again with the response from the function, the model will write necessary commentary.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Real Estate - Property Valuation&lt;/h3&gt; &#xA;&lt;p&gt;A function like estimate_property_value(property_details: dict) could allow users to input details about a property (such as location, size, number of rooms, etc.) and receive an estimated market value.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Details (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;openai.ChatCompletion.create(&#xA;    model=&#34;musabgultekin/functionary-7b-v1&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#39;What is the estimated value of a 3-bedroom house in San Francisco with 2000 sq ft area?&#39;},&#xA;        {&#34;role&#34;: &#34;assistant&#34;, &#34;function_call&#34;: {&#34;name&#34;: &#34;estimate_property_value&#34;, &#34;arguments&#34;: &#39;{\n  &#34;property_details&#34;: {&#34;location&#34;: &#34;San Francisco&#34;, &#34;size&#34;: 2000, &#34;rooms&#34;: 3}\n}&#39;}},&#xA;    ], &#xA;    functions=[&#xA;        {&#xA;            &#34;name&#34;: &#34;estimate_property_value&#34;,&#xA;            &#34;description&#34;: &#34;Estimate the market value of a property&#34;,&#xA;            &#34;parameters&#34;: {&#xA;                &#34;type&#34;: &#34;object&#34;,&#xA;                &#34;properties&#34;: {&#xA;                    &#34;property_details&#34;: {&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: {&#xA;                            &#34;location&#34;: {&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;The location of the property&#34;,&#xA;                            },&#xA;                            &#34;size&#34;: {&#xA;                                &#34;type&#34;: &#34;integer&#34;,&#xA;                                &#34;description&#34;: &#34;The size of the property in square feet&#34;,&#xA;                            },&#xA;                            &#34;rooms&#34;: {&#xA;                                &#34;type&#34;: &#34;integer&#34;,&#xA;                                &#34;description&#34;: &#34;The number of rooms in the property&#34;,&#xA;                            },&#xA;                        },&#xA;                        &#34;required&#34;: [&#34;location&#34;, &#34;size&#34;, &#34;rooms&#34;],&#xA;                    },&#xA;                },&#xA;                &#34;required&#34;: [&#34;property_details&#34;],&#xA;            },&#xA;        },&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Response will have:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;role&#34;: &#34;assistant&#34;, &#34;function_call&#34;: {&#34;name&#34;: &#34;plan_trip&#34;, &#34;arguments&#34;: &#39;{\n  &#34;destination&#34;: &#34;Paris&#34;,\n  &#34;duration&#34;: 7,\n  &#34;interests&#34;: [&#34;art&#34;, &#34;culture&#34;]\n}&#39;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then you need to call &lt;code&gt;plan_trip&lt;/code&gt; function with provided arguments. If you would like a commentary from the model, then you&#39;ll call the model again with the response from the function, the model will write necessary commentary.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Telecommunications - Customer Support&lt;/h3&gt; &#xA;&lt;p&gt;A function &lt;code&gt;parse_customer_complaint(complaint: {issue: string, frequency: string, duration: string})&lt;/code&gt; could help in extracting structured information from a complex, narrative customer complaint, identifying the core issue and potential solutions. The &lt;code&gt;complaint&lt;/code&gt; object could include properties such as &lt;code&gt;issue&lt;/code&gt; (the main problem), &lt;code&gt;frequency&lt;/code&gt; (how often the issue occurs), and &lt;code&gt;duration&lt;/code&gt; (how long the issue has been occurring).&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Details (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;openai.ChatCompletion.create(&#xA;    model=&#34;musabgultekin/functionary-7b-v1&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#39;My internet has been disconnecting frequently for the past week&#39;},&#xA;    ], &#xA;    functions=[&#xA;        {&#xA;            &#34;name&#34;: &#34;parse_customer_complaint&#34;,&#xA;            &#34;description&#34;: &#34;Parse a customer complaint and identify the core issue&#34;,&#xA;            &#34;parameters&#34;: {&#xA;                &#34;type&#34;: &#34;object&#34;,&#xA;                &#34;properties&#34;: {&#xA;                    &#34;complaint&#34;: {&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: {&#xA;                            &#34;issue&#34;: {&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;The main problem&#34;,&#xA;                            },&#xA;                            &#34;frequency&#34;: {&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;How often the issue occurs&#34;,&#xA;                            },&#xA;                            &#34;duration&#34;: {&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;How long the issue has been occurring&#34;,&#xA;                            },&#xA;                        },&#xA;                        &#34;required&#34;: [&#34;issue&#34;, &#34;frequency&#34;, &#34;duration&#34;],&#xA;                    },&#xA;                },&#xA;                &#34;required&#34;: [&#34;complaint&#34;],&#xA;            },&#xA;        },&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Response will have:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;role&#34;: &#34;assistant&#34;, &#34;function_call&#34;: {&#34;name&#34;: &#34;parse_customer_complaint&#34;, &#34;arguments&#34;: &#39;{\n  &#34;complaint&#34;: {&#34;issue&#34;: &#34;internet disconnecting&#34;, &#34;frequency&#34;: &#34;frequently&#34;, &#34;duration&#34;: &#34;past week&#34;}\n}&#39;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then you need to call parse_customer_complaint function with provided arguments. If you would like a commentary from the model, then you&#39;ll call the model again with the response from the function, the model will write necessary commentary.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We use standard HuggingFace Trainer. When calculating the loss, we only calculate the loss on assistant outputs and assistant function calls. Not on function responses and function definitions&lt;/p&gt; &#xA;&lt;p&gt;We use the similar hyperparameters as its used in LLama 2 &lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;paper&lt;/a&gt;. Except we use bigger weight decay (0.3 instead of 0.1) and warmup of 0.03, to reduce overfitting as we sample 2x of the function calling example conversations. But ablation study is required.&lt;/p&gt; &#xA;&lt;p&gt;We use transformers after this &lt;a href=&#34;https://github.com/huggingface/transformers/commit/f4eb459ef25c62c4cc9edde38052da1980977872&#34;&gt;commit&lt;/a&gt;. As it fixes OOM for FSDP training on Llama 2.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hyperparameters&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Batch size: 64&lt;/li&gt; &#xA; &lt;li&gt;Learning rate: 2e-5&lt;/li&gt; &#xA; &lt;li&gt;Epochs: 2&lt;/li&gt; &#xA; &lt;li&gt;Max length: 4096&lt;/li&gt; &#xA; &lt;li&gt;Weight decay: 0.3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More on training: &lt;a href=&#34;https://raw.githubusercontent.com/musabgultekin/functionary/main/train/README.md&#34;&gt;README.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it Works?&lt;/h2&gt; &#xA;&lt;p&gt;We convert function definitions to a similar text like TypeScript definitions. Then we inject these definitions as system prompts. After that, we inject the default system prompt. Then we start the conversation messages.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example prompt that will be provided to the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;system:&#xA;namespace weather {&#xA;&#xA;// Get the current weather&#xA;type get_current_weather  = (_: {&#xA;// The city and state, e.g. San Francisco, CA&#xA;location: string,&#xA;// The temperature unit to use. Infer this from the users location.&#xA;format: &#34;celsius&#34; | &#34;fahrenheit&#34;,&#xA;}) =&amp;gt; any;&#xA;&#xA;} // namespace weather&#xA;system:&#xA;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&#39;s questions. The assistant calls functions with appropriate input when necessary&#xA;user:&#xA;&amp;lt;/s&amp;gt;What is the weather in Istanbul?&amp;lt;/s&amp;gt;&#xA;assistant&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model will output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt; to=weather.get_current_weather:&#xA;{&#34;location&#34;: &#34;Istanbul&#34;, &#34;format&#34;: &#34;celsius&#34;}&amp;lt;/s&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then it will stop.&lt;/p&gt; &#xA;&lt;p&gt;We don&#39;t change the logit probabilities to conform a certain schema, but the model itself knows how to conform. This allows us to use existing tools and caching systems with ease.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;--- Work In Progress ---&lt;/p&gt; &#xA;&lt;p&gt;Due to the unique nature, it requires custom evaluation suite. But we can probably evaluate with gpt-4-0613, likely with a similar approach like &lt;a href=&#34;https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge&#34;&gt;LLM Judge&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;--- Work In Progress ---&lt;/p&gt; &#xA;&lt;p&gt;Dataset preparation process consists of several steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Function Definitions Conversion:&lt;/strong&gt; We begin by selecting multiple function definitions and converting them into TypeScript definitions. This approach benefits from the model&#39;s prior exposure to TypeScript tokens during the pretraining phase. &lt;a href=&#34;https://github.com/musabgultekin/functionary/raw/17a86de9b06acaedd0afab212717205c0484a218/schema.py#L54&#34;&gt;See how we do it&lt;/a&gt; Also see &lt;a href=&#34;https://github.com/microsoft/TypeChat/raw/d2f2de9ca37ef9adeb108d5fc60703b72fec0a22/site/src/blog/introducing-typechat.md#just-add-types&#34;&gt;Microsoft TypeChat&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Human Prompts Generation:&lt;/strong&gt; We then create human prompts that incorporate the converted TypeScript function definitions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Function Calls Generation:&lt;/strong&gt; Following the generation of human prompts, we proceed to generate corresponding function calls.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Function Answers Generation:&lt;/strong&gt; Once function calls have been generated, we derive the outputs of these function calls would produce.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Function Answers Interpretation:&lt;/strong&gt; After procuring function answers, we generate language model answers for the function response. So the model knows how to interpret the function response.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Merging and Training:&lt;/strong&gt; We combine all the generated elements (prompts, function calls, function answers, and their interpretations) using a custom formatting. This consolidated dataset is then used for the model&#39;s training.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Llama 2 70b is capable of doing all synthetic data generation.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;More information about this process will be provided soon as possible.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;v0.1&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Sources:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/ehartford/wizard_vicuna_70k_unfiltered/blob/cfe3f5810110d4d763665c070b4a966fda43e5c5/wizard_vicuna_dataset_unfiltered.json&#34;&gt;ShareGPT 34K&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Synthetic function calling dataset (2.7k examples)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Observations:&lt;/strong&gt; This version showed limitations in handling multi-prompt conversations, likely due to the absence of multiple instructions in the function calling dataset. Also hallucinations are common, we likely need more conversation data.&lt;/p&gt; &#xA;&lt;h3&gt;v0.2&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Sources:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/bcd32a724d8460ebe14e1d05b0195e30e9a46cb1/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json&#34;&gt;ShareGPT 53K&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Synthetic function calling dataset (3.5k examples). Sampled 2 times.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Sources:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Same as v0.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Observations:&lt;/strong&gt; Compared to v0.2, because the model supports 4k context sizes, its much more resilient to the longer conversations and longer function definitions. Also we switched to Llama 2.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; If I can save more money, I&#39;ll train &lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Llama 2&lt;/a&gt; 13B model too, with 2x more data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; OpenAPI specification based plugin support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fast inference server &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;text-generation-inference&lt;/a&gt; ? See: &lt;a href=&#34;https://github.com/huggingface/text-generation-inference/issues/726&#34;&gt;License Issue&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Streaming Support&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; function_call parameter to server&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Python function calling support (Automatic detection of type annotations and calling them automatically)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Real world usage examples, such as creating agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Please consider opening a PR for future requests&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>