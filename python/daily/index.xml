<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-18T01:34:38Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>langchain-ai/ollama-deep-researcher</title>
    <updated>2025-03-18T01:34:38Z</updated>
    <id>tag:github.com,2025-03-18:/langchain-ai/ollama-deep-researcher</id>
    <link href="https://github.com/langchain-ai/ollama-deep-researcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fully local web research and report writing assistant&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ollama Deep Researcher&lt;/h1&gt; &#xA;&lt;p&gt;Ollama Deep Researcher is a fully local web research assistant that uses any LLM hosted by &lt;a href=&#34;https://ollama.com/search&#34;&gt;Ollama&lt;/a&gt;. Give it a topic and it will generate a web search query, gather web search results (via &lt;a href=&#34;https://www.tavily.com/&#34;&gt;Tavily&lt;/a&gt; by default), summarize the results of web search, reflect on the summary to examine knowledge gaps, generate a new search query to address the gaps, search, and improve the summary for a user-defined number of cycles. It will provide the user a final markdown summary with all sources used.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/4308ee9c-abf3-4abb-9d1e-83e7c2c3f187&#34; alt=&#34;research-rabbit&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Short summary: &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/02084902-f067-4658-9683-ff312cab7944&#34; controls&gt;&lt;/video&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ðŸ“º Video Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;See it in action or build it yourself? Check out these helpful video tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sGUjmyfof4Q&#34;&gt;Overview of Ollama Deep Researcher with R1&lt;/a&gt; - Load and test &lt;a href=&#34;https://api-docs.deepseek.com/news/news250120&#34;&gt;DeepSeek R1&lt;/a&gt; &lt;a href=&#34;https://ollama.com/library/deepseek-r1&#34;&gt;distilled models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XGuTzHoqlj8&#34;&gt;Building Ollama Deep Researcher from Scratch&lt;/a&gt; - Overview of how this is built.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ðŸš€ Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Mac&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the Ollama app for Mac &lt;a href=&#34;https://ollama.com/download&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pull a local LLM from &lt;a href=&#34;https://ollama.com/search&#34;&gt;Ollama&lt;/a&gt;. As an &lt;a href=&#34;https://ollama.com/library/deepseek-r1:8b&#34;&gt;example&lt;/a&gt;:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ollama pull deepseek-r1:8b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Clone the repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/langchain-ai/ollama-deep-researcher.git&#xA;cd ollama-deep-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Select a web search tool:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;By default, it will use &lt;a href=&#34;https://duckduckgo.com/&#34;&gt;DuckDuckGo&lt;/a&gt; for web search, which does not require an API key. But you can also use &lt;a href=&#34;https://tavily.com/&#34;&gt;Tavily&lt;/a&gt; or &lt;a href=&#34;https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api&#34;&gt;Perplexity&lt;/a&gt; by adding their API keys to the environment file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following environment variables are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OLLAMA_BASE_URL&lt;/code&gt; - the endpoint of the Ollama service, defaults to &lt;code&gt;http://localhost:11434&lt;/code&gt; if not set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;OLLAMA_MODEL&lt;/code&gt; - the model to use, defaults to &lt;code&gt;llama3.2&lt;/code&gt; if not set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SEARCH_API&lt;/code&gt; - the search API to use, either &lt;code&gt;duckduckgo&lt;/code&gt; (default) or &lt;code&gt;tavily&lt;/code&gt; or &lt;code&gt;perplexity&lt;/code&gt;. You need to set the corresponding API key if tavily or perplexity is used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TAVILY_API_KEY&lt;/code&gt; - the tavily API key to use&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PERPLEXITY_API_KEY&lt;/code&gt; - the perplexity API key to use&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MAX_WEB_RESEARCH_LOOPS&lt;/code&gt; - the maximum number of research loop steps, defaults to &lt;code&gt;3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;FETCH_FULL_PAGE&lt;/code&gt; - fetch the full page content if using &lt;code&gt;duckduckgo&lt;/code&gt; for the search API, defaults to &lt;code&gt;false&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;(Recommended) Create a virtual environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .venv&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Launch the assistant with the LangGraph server:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install uv package manager&#xA;curl -LsSf https://astral.sh/uv/install.sh | sh&#xA;uvx --refresh --from &#34;langgraph-cli[inmem]&#34; --with-editable . --python 3.11 langgraph dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the Ollama app for Windows &lt;a href=&#34;https://ollama.com/download&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pull a local LLM from &lt;a href=&#34;https://ollama.com/search&#34;&gt;Ollama&lt;/a&gt;. As an &lt;a href=&#34;https://ollama.com/library/deepseek-r1:8b&#34;&gt;example&lt;/a&gt;:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;ollama pull deepseek-r1:8b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Clone the repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/langchain-ai/ollama-deep-researcher.git&#xA;cd ollama-deep-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Select a web search tool, as above.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Recommended) Create a virtual environment: Install &lt;code&gt;Python 3.11&lt;/code&gt; (and add to PATH during installation). Restart your terminal to ensure Python is available, then create and activate a virtual environment:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;python -m venv .venv&#xA;.venv\Scripts\Activate.ps1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Launch the assistant with the LangGraph server:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;# Install dependencies&#xA;pip install -e .&#xA;pip install -U &#34;langgraph-cli[inmem]&#34;            &#xA;&#xA;# Start the LangGraph server&#xA;langgraph dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the LangGraph Studio UI&lt;/h3&gt; &#xA;&lt;p&gt;When you launch LangGraph server, you should see the following output and Studio will open in your browser:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Ready!&lt;/p&gt; &#xA; &lt;p&gt;API: &lt;a href=&#34;http://127.0.0.1:2024&#34;&gt;http://127.0.0.1:2024&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Docs: &lt;a href=&#34;http://127.0.0.1:2024/docs&#34;&gt;http://127.0.0.1:2024/docs&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;LangGraph Studio Web UI: &lt;a href=&#34;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&#34;&gt;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Open &lt;code&gt;LangGraph Studio Web UI&lt;/code&gt; via the URL in the output above.&lt;/p&gt; &#xA;&lt;p&gt;In the &lt;code&gt;configuration&lt;/code&gt; tab:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pick your web search tool (DuckDuckGo, Tavily, or Perplexity) (it will by default be &lt;code&gt;DuckDuckGo&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Set the name of your local LLM to use with Ollama (it will by default be &lt;code&gt;llama3.2&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;You can set the depth of the research iterations (it will by default be &lt;code&gt;3&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;1621&#34; alt=&#34;Screenshot 2025-01-24 at 10 08 31 PM&#34; src=&#34;https://github.com/user-attachments/assets/7cfd0e04-28fd-4cfa-aee5-9a556d74ab21&#34;&gt; &#xA;&lt;p&gt;Give the assistant a topic for research, and you can visualize its process!&lt;/p&gt; &#xA;&lt;img width=&#34;1621&#34; alt=&#34;Screenshot 2025-01-24 at 10 08 22 PM&#34; src=&#34;https://github.com/user-attachments/assets/4de6bd89-4f3b-424c-a9cb-70ebd3d45c5f&#34;&gt; &#xA;&lt;h3&gt;Model Compatibility Note&lt;/h3&gt; &#xA;&lt;p&gt;When selecting a local LLM, note that this application relies on the model&#39;s ability to produce structured JSON output. Some models may have difficulty with this requirement:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Working well&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ollama.com/library/llama3.2&#34;&gt;Llama2 3.2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ollama.com/library/deepseek-r1:8b&#34;&gt;DeepSeek R1 (8B)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Known issues&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ollama.com/library/deepseek-llm:7b&#34;&gt;DeepSeek R1 (7B)&lt;/a&gt; - Currently has difficulty producing required JSON output&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you &lt;a href=&#34;https://github.com/langchain-ai/ollama-deep-researcher/issues/18&#34;&gt;encounter JSON-related errors&lt;/a&gt; (e.g., &lt;code&gt;KeyError: &#39;query&#39;&lt;/code&gt;), try switching to one of the confirmed working models.&lt;/p&gt; &#xA;&lt;h3&gt;Browser Compatibility Note&lt;/h3&gt; &#xA;&lt;p&gt;When accessing the LangGraph Studio UI:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Firefox is recommended for the best experience&lt;/li&gt; &#xA; &lt;li&gt;Safari users may encounter security warnings due to mixed content (HTTPS/HTTP)&lt;/li&gt; &#xA; &lt;li&gt;If you encounter issues, try: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Using Firefox or another browser&lt;/li&gt; &#xA;   &lt;li&gt;Disabling ad-blocking extensions&lt;/li&gt; &#xA;   &lt;li&gt;Checking browser console for specific error messages&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;Ollama Deep Researcher is inspired by &lt;a href=&#34;https://arxiv.org/html/2410.04343v1#:~:text=To%20tackle%20this%20issue%2C%20we,used%20to%20generate%20intermediate%20answers.&#34;&gt;IterDRAG&lt;/a&gt;. This approach will decompose a query into sub-queries, retrieve documents for each one, answer the sub-query, and then build on the answer by retrieving docs for the second sub-query. Here, we do similar:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Given a user-provided topic, use a local LLM (via &lt;a href=&#34;https://ollama.com/search&#34;&gt;Ollama&lt;/a&gt;) to generate a web search query&lt;/li&gt; &#xA; &lt;li&gt;Uses a search engine (configured for &lt;a href=&#34;https://duckduckgo.com/&#34;&gt;DuckDuckGo&lt;/a&gt;, &lt;a href=&#34;https://www.tavily.com/&#34;&gt;Tavily&lt;/a&gt;, or &lt;a href=&#34;https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api&#34;&gt;Perplexity&lt;/a&gt;) to find relevant sources&lt;/li&gt; &#xA; &lt;li&gt;Uses LLM to summarize the findings from web search related to the user-provided research topic&lt;/li&gt; &#xA; &lt;li&gt;Then, it uses the LLM to reflect on the summary, identifying knowledge gaps&lt;/li&gt; &#xA; &lt;li&gt;It generates a new search query to address the knowledge gaps&lt;/li&gt; &#xA; &lt;li&gt;The process repeats, with the summary being iteratively updated with new information from web search&lt;/li&gt; &#xA; &lt;li&gt;It will repeat down the research rabbit hole&lt;/li&gt; &#xA; &lt;li&gt;Runs for a configurable number of iterations (see &lt;code&gt;configuration&lt;/code&gt; tab)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Outputs&lt;/h2&gt; &#xA;&lt;p&gt;The output of the graph is a markdown file containing the research summary, with citations to the sources used.&lt;/p&gt; &#xA;&lt;p&gt;All sources gathered during research are saved to the graph state.&lt;/p&gt; &#xA;&lt;p&gt;You can visualize them in the graph state, which is visible in LangGraph Studio:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/e8ac1c0b-9acb-4a75-8c15-4e677e92f6cb&#34; alt=&#34;Screenshot 2024-12-05 at 4 08 59 PM&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The final summary is saved to the graph state as well:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/f6d997d5-9de5-495f-8556-7d3891f6bc96&#34; alt=&#34;Screenshot 2024-12-05 at 4 10 11 PM&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Deployment Options&lt;/h2&gt; &#xA;&lt;p&gt;There are &lt;a href=&#34;https://langchain-ai.github.io/langgraph/concepts/#deployment-options&#34;&gt;various ways&lt;/a&gt; to deploy this graph.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/langchain-ai/langchain-academy/tree/main/module-6&#34;&gt;Module 6&lt;/a&gt; of LangChain Academy for a detailed walkthrough of deployment options with LangGraph.&lt;/p&gt; &#xA;&lt;h2&gt;TypeScript Implementation&lt;/h2&gt; &#xA;&lt;p&gt;A TypeScript port of this project (without Perplexity search) is available at: &lt;a href=&#34;https://github.com/PacoVK/ollama-deep-researcher-ts&#34;&gt;https://github.com/PacoVK/ollama-deep-researcher-ts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running as a Docker container&lt;/h2&gt; &#xA;&lt;p&gt;The included &lt;code&gt;Dockerfile&lt;/code&gt; only runs LangChain Studio with ollama-deep-researcher as a service, but does not include Ollama as a dependant service. You must run Ollama separately and configure the &lt;code&gt;OLLAMA_BASE_URL&lt;/code&gt; environment variable. Optionally you can also specify the Ollama model to use by providing the &lt;code&gt;OLLAMA_MODEL&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;p&gt;Clone the repo and build an image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker build -t ollama-deep-researcher .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker run --rm -it -p 2024:2024 \&#xA;  -e SEARCH_API=&#34;tavily&#34; \ &#xA;  -e TAVILY_API_KEY=&#34;tvly-***YOUR_KEY_HERE***&#34; \&#xA;  -e OLLAMA_BASE_URL=&#34;http://host.docker.internal:11434/&#34; \&#xA;  -e OLLAMA_MODEL=&#34;llama3.2&#34; \  &#xA;  ollama-deep-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: You will see log message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;2025-02-10T13:45:04.784915Z [info     ] ðŸŽ¨ Opening Studio in your browser... [browser_opener] api_variant=local_dev message=ðŸŽ¨ Opening Studio in your browser...&#xA;URL: https://smith.langchain.com/studio/?baseUrl=http://0.0.0.0:2024&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...but the browser will not launch from the container.&lt;/p&gt; &#xA;&lt;p&gt;Instead, visit this link with the correct baseUrl IP address: &lt;a href=&#34;https://smith.langchain.com/studio/thread?baseUrl=http://127.0.0.1:2024&#34;&gt;&lt;code&gt;https://smith.langchain.com/studio/thread?baseUrl=http://127.0.0.1:2024&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>