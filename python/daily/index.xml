<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-14T01:40:15Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>open-compass/MixtralKit</title>
    <updated>2023-12-14T01:40:15Z</updated>
    <id>tag:github.com,2023-12-14:/open-compass/MixtralKit</id>
    <link href="https://github.com/open-compass/MixtralKit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A toolkit for inference and evaluation of &#39;mixtral-8x7b-32kseqlen&#39; from Mistral AI&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/open-compass/MixtralKit/assets/7881589/149f8930-3a34-49b6-b27d-79dc192aeac7&#34; width=&#34;500px&#34;&gt; &#xA; &lt;h1&gt;MixtralKit&lt;/h1&gt; &#xA; &lt;p&gt;A Toolkit for Mixtral Model&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-compass/MixtralKit/main/#-performance&#34;&gt;üìäPerformance &lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/MixtralKit/main/#-resources&#34;&gt;‚ú®Resources &lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/MixtralKit/main/#-model-architecture&#34;&gt;üìñArchitecture &lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/MixtralKit/main/#-model-weights&#34;&gt;üìÇWeights &lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/MixtralKit/main/#-install&#34;&gt; üî® Install &lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/MixtralKit/main/#-inference&#34;&gt;üöÄInference &lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/MixtralKit/main/#-acknowledgement&#34;&gt;ü§ù Acknowledgement &lt;/a&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-compass/MixtralKit/main/README_zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Important]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt; üì¢ Welcome to try &lt;a href=&#34;https://github.com/open-compass/opencompass&#34;&gt;OpenCompass&lt;/a&gt; for model evaluation üì¢ &lt;/b&gt; &#xA;  &lt;br&gt; &#xA;  &lt;b&gt; ü§ó Request for update your mixtral-related projects is open! &lt;/b&gt; &#xA;  &lt;br&gt; &#xA;  &lt;b&gt; üôè This repo is an **experimental** implementation of inference code. &lt;/b&gt; &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;üìä Performance&lt;/h1&gt; &#xA;&lt;h2&gt;Comparison with Other Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All data generated from &lt;a href=&#34;https://github.com/open-compass/opencompass&#34;&gt;OpenCompass&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Performances generated from different evaluation toolkits are different due to the prompts, settings and implementation details.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datasets&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;Mistral-7B-v0.1&lt;/th&gt; &#xA;   &lt;th&gt;Mixtral-8x7B(MoE)&lt;/th&gt; &#xA;   &lt;th&gt;Llama2-70B&lt;/th&gt; &#xA;   &lt;th&gt;DeepSeek-67B-Base&lt;/th&gt; &#xA;   &lt;th&gt;Qwen-72B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Active Params&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;12B&lt;/td&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;67B&lt;/td&gt; &#xA;   &lt;td&gt;72B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU&lt;/td&gt; &#xA;   &lt;td&gt;PPL&lt;/td&gt; &#xA;   &lt;td&gt;64.1&lt;/td&gt; &#xA;   &lt;td&gt;71.3&lt;/td&gt; &#xA;   &lt;td&gt;69.7&lt;/td&gt; &#xA;   &lt;td&gt;71.9&lt;/td&gt; &#xA;   &lt;td&gt;77.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BIG-Bench-Hard&lt;/td&gt; &#xA;   &lt;td&gt;GEN&lt;/td&gt; &#xA;   &lt;td&gt;56.7&lt;/td&gt; &#xA;   &lt;td&gt;67.1&lt;/td&gt; &#xA;   &lt;td&gt;64.9&lt;/td&gt; &#xA;   &lt;td&gt;71.7&lt;/td&gt; &#xA;   &lt;td&gt;63.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GSM-8K&lt;/td&gt; &#xA;   &lt;td&gt;GEN&lt;/td&gt; &#xA;   &lt;td&gt;47.5&lt;/td&gt; &#xA;   &lt;td&gt;65.7&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;66.5&lt;/td&gt; &#xA;   &lt;td&gt;77.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MATH&lt;/td&gt; &#xA;   &lt;td&gt;GEN&lt;/td&gt; &#xA;   &lt;td&gt;11.3&lt;/td&gt; &#xA;   &lt;td&gt;22.7&lt;/td&gt; &#xA;   &lt;td&gt;12.0&lt;/td&gt; &#xA;   &lt;td&gt;15.9&lt;/td&gt; &#xA;   &lt;td&gt;35.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HumanEval&lt;/td&gt; &#xA;   &lt;td&gt;GEN&lt;/td&gt; &#xA;   &lt;td&gt;27.4&lt;/td&gt; &#xA;   &lt;td&gt;32.3&lt;/td&gt; &#xA;   &lt;td&gt;26.2&lt;/td&gt; &#xA;   &lt;td&gt;40.9&lt;/td&gt; &#xA;   &lt;td&gt;33.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MBPP&lt;/td&gt; &#xA;   &lt;td&gt;GEN&lt;/td&gt; &#xA;   &lt;td&gt;38.6&lt;/td&gt; &#xA;   &lt;td&gt;47.8&lt;/td&gt; &#xA;   &lt;td&gt;39.6&lt;/td&gt; &#xA;   &lt;td&gt;55.2&lt;/td&gt; &#xA;   &lt;td&gt;51.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARC-c&lt;/td&gt; &#xA;   &lt;td&gt;PPL&lt;/td&gt; &#xA;   &lt;td&gt;74.2&lt;/td&gt; &#xA;   &lt;td&gt;85.1&lt;/td&gt; &#xA;   &lt;td&gt;78.3&lt;/td&gt; &#xA;   &lt;td&gt;86.8&lt;/td&gt; &#xA;   &lt;td&gt;92.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARC-e&lt;/td&gt; &#xA;   &lt;td&gt;PPL&lt;/td&gt; &#xA;   &lt;td&gt;83.6&lt;/td&gt; &#xA;   &lt;td&gt;91.4&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;93.7&lt;/td&gt; &#xA;   &lt;td&gt;96.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CommonSenseQA&lt;/td&gt; &#xA;   &lt;td&gt;PPL&lt;/td&gt; &#xA;   &lt;td&gt;67.4&lt;/td&gt; &#xA;   &lt;td&gt;70.4&lt;/td&gt; &#xA;   &lt;td&gt;78.3&lt;/td&gt; &#xA;   &lt;td&gt;70.7&lt;/td&gt; &#xA;   &lt;td&gt;73.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NaturalQuestion&lt;/td&gt; &#xA;   &lt;td&gt;GEN&lt;/td&gt; &#xA;   &lt;td&gt;24.6&lt;/td&gt; &#xA;   &lt;td&gt;29.4&lt;/td&gt; &#xA;   &lt;td&gt;34.2&lt;/td&gt; &#xA;   &lt;td&gt;29.9&lt;/td&gt; &#xA;   &lt;td&gt;27.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TrivialQA&lt;/td&gt; &#xA;   &lt;td&gt;GEN&lt;/td&gt; &#xA;   &lt;td&gt;56.5&lt;/td&gt; &#xA;   &lt;td&gt;66.1&lt;/td&gt; &#xA;   &lt;td&gt;70.7&lt;/td&gt; &#xA;   &lt;td&gt;67.4&lt;/td&gt; &#xA;   &lt;td&gt;60.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HellaSwag&lt;/td&gt; &#xA;   &lt;td&gt;PPL&lt;/td&gt; &#xA;   &lt;td&gt;78.9&lt;/td&gt; &#xA;   &lt;td&gt;82.0&lt;/td&gt; &#xA;   &lt;td&gt;82.3&lt;/td&gt; &#xA;   &lt;td&gt;82.3&lt;/td&gt; &#xA;   &lt;td&gt;85.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PIQA&lt;/td&gt; &#xA;   &lt;td&gt;PPL&lt;/td&gt; &#xA;   &lt;td&gt;81.6&lt;/td&gt; &#xA;   &lt;td&gt;82.9&lt;/td&gt; &#xA;   &lt;td&gt;82.5&lt;/td&gt; &#xA;   &lt;td&gt;82.6&lt;/td&gt; &#xA;   &lt;td&gt;85.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIQA&lt;/td&gt; &#xA;   &lt;td&gt;GEN&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;64.8&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;78.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Performance Mixtral-8x7b&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;dataset                                 version    metric         mode    mixtral-8x7b-32k&#xA;--------------------------------------  ---------  -------------  ------  ------------------&#xA;mmlu                                    -          naive_average     ppl     71.34&#xA;ARC-c                                   2ef631     accuracy          ppl     85.08&#xA;ARC-e                                   2ef631     accuracy          ppl     91.36&#xA;BoolQ                                   314797     accuracy          ppl     86.27&#xA;commonsense_qa                          5545e2     accuracy          ppl     70.43&#xA;triviaqa                                2121ce     score             gen     66.05&#xA;nq                                      2121ce     score             gen     29.36&#xA;openbookqa_fact                         6aac9e     accuracy          ppl     85.40&#xA;AX_b                                    6db806     accuracy          ppl     48.28&#xA;AX_g                                    66caf3     accuracy          ppl     48.60&#xA;hellaswag                               a6e128     accuracy          ppl     82.01&#xA;piqa                                    0cfff2     accuracy          ppl     82.86&#xA;siqa                                    e8d8c5     accuracy          ppl     64.28&#xA;math                                    265cce     accuracy          gen     22.74&#xA;gsm8k                                   1d7fe4     accuracy          gen     65.66&#xA;openai_humaneval                        a82cae     humaneval_pass@1  gen     32.32&#xA;mbpp                                    1e1056     score             gen     47.80&#xA;bbh                                     -          naive_average     gen     67.14&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;‚ú® Resources&lt;/h1&gt; &#xA;&lt;h2&gt;Blog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/moe&#34;&gt;MoE Blog from Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-efficient&#34;&gt;Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More Efficient&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Title&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Venue&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14705&#34;&gt;Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Arxiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.15841&#34;&gt;MegaBlocks: Efficient Sparse Training with Mixture-of-Experts&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Arxiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/stanford-futuredata/megablocks&#34;&gt;megablocks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.08906&#34;&gt;ST-MoE: Designing Stable and Transferable Sparse Expert Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Arxiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.03961&#34;&gt;Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Arxiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.06905&#34;&gt;GLaM: Efficient Scaling of Language Models with Mixture-of-Experts&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ICML 2022&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.16668&#34;&gt;GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Arxiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.4314&#34;&gt;Learning Factored Representations in a Deep Mixture of Experts&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Arxiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.13262&#34;&gt;FastMoE: A Fast Mixture-of-Expert Training System&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Arxiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/laekov/FastMoE&#34;&gt;FastMoE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3503221.3508418&#34;&gt;FasterMoE: Modeling and Optimizing Training of Large-scale Dynamic Pre-trained Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACM SIGPLAN PPoPP 2022&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/laekov/FastMoE&#34;&gt;FasterMoE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.usenix.org/conference/atc23/presentation/zhai&#34;&gt;SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;USENIX ATC 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zms1999/SmartMoE&#34;&gt;SmartMoE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf&#34;&gt;Adaptive Mixture of Local Experts&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Neural Computation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1991&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Evaluation Toolkit: &lt;a href=&#34;https://github.com/open-compass/opencompass&#34;&gt;OpenCompass&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Megablocks: &lt;a href=&#34;https://github.com/stanford-futuredata/megablocks&#34;&gt;https://github.com/stanford-futuredata/megablocks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FairSeq: &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm&#34;&gt;https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenMoE: &lt;a href=&#34;https://github.com/XueFuzhao/OpenMoE&#34;&gt;https://github.com/XueFuzhao/OpenMoE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ColossalAI MoE: &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe&#34;&gt;https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastMoE(FasterMoE): &lt;a href=&#34;https://github.com/laekov/FastMoE&#34;&gt;https://github.com/laekov/FastMoE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SmartMoE: &lt;a href=&#34;https://github.com/zms1999/SmartMoE&#34;&gt;https://github.com/zms1999/SmartMoE&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Finetuning script (Full-parameters or QLoRA) from &lt;a href=&#34;https://github.com/InternLM/xtuner/tree/main/xtuner/configs/mixtral&#34;&gt;XTuner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Finetuned Mixtral-8x7B from DiscoResearch: &lt;a href=&#34;https://huggingface.co/DiscoResearch/DiscoLM-mixtral-8x7b-v2&#34;&gt;DiscoLM-mixtral-8x7b-v2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;Inference with vLLM&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìñ Model Architecture&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The Mixtral-8x7B-32K MoE model is mainly composed of 32 identical MoEtransformer blocks. The main difference between the MoEtransformer block and the ordinary transformer block is that the FFN layer is replaced by the &lt;strong&gt;MoE FFN&lt;/strong&gt; layer. In the MoE FFN layer, the tensor first goes through a gate layer to calculate the scores of each expert, and then selects the top-k experts from the 8 experts based on the expert scores. The tensor is aggregated through the outputs of the top-k experts, thereby obtaining the final output of the MoE FFN layer. Each expert consists of 3 linear layers. It is worth noting that all Norm Layers of Mixtral MoE also use RMSNorm, which is the same as LLama. In the attention layer, the QKV matrix in the Mixtral MoE has a Q matrix shape of (4096,4096) and K and V matrix shapes of (4096,1024).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We plot the architecture as the following:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/open-compass/MixtralKit/assets/7881589/0bd59661-4799-4e39-8a92-95fd559679e9&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;üìÇ Model Weights&lt;/h1&gt; &#xA;&lt;h2&gt;Hugging Face Format&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-v0.1&#34;&gt;Official Base Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;&gt;Official Chat Model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Raw Format&lt;/h2&gt; &#xA;&lt;p&gt;You can download the checkpoints by magnet or Hugging Face&lt;/p&gt; &#xA;&lt;h3&gt;Download via HF&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen&#34;&gt;mixtral-8x7b-32kseqlen&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you are unable to access Hugging Face, please try &lt;a href=&#34;https://hf-mirror.com/someone13574/mixtral-8x7b-32kseqlen&#34;&gt;hf-mirror&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download the Hugging Face&#xA;git lfs install&#xA;git clone https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen&#xA;&#xA;# Merge Files(Only for HF)&#xA;cd mixtral-8x7b-32kseqlen/&#xA;&#xA;# Merge the checkpoints&#xA;cat consolidated.00.pth-split0 consolidated.00.pth-split1 consolidated.00.pth-split2 consolidated.00.pth-split3 consolidated.00.pth-split4 consolidated.00.pth-split5 consolidated.00.pth-split6 consolidated.00.pth-split7 consolidated.00.pth-split8 consolidated.00.pth-split9 consolidated.00.pth-split10 &amp;gt; consolidated.00.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download via Magnet Link&lt;/h3&gt; &#xA;&lt;p&gt;Please use this link to download the original files&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;magnet:?xt=urn:btih:5546272da9065eddeb6fcd7ffddeef5b75be79a7&amp;amp;dn=mixtral-8x7b-32kseqlen&amp;amp;tr=udp%3A%2F%http://2Fopentracker.i2p.rocks%3A6969%2Fannounce&amp;amp;tr=http%3A%2F%http://2Ftracker.openbittorrent.com%3A80%2Fannounce&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;MD5 Validation&lt;/h3&gt; &#xA;&lt;p&gt;Please check the MD5 to make sure the files are completed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;md5sum consolidated.00.pth&#xA;md5sum tokenizer.model&#xA;&#xA;# Once verified, you can delete the splited files.&#xA;rm consolidated.00.pth-split*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Official MD5&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; ‚ïì‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïñ&#xA; ‚ïë                                                                            ‚ïë&#xA; ‚ïë                               ¬∑¬∑ md5sum ¬∑¬∑                                 ‚ïë&#xA; ‚ïë                                                                            ‚ïë&#xA; ‚ïë        1faa9bc9b20fcfe81fcd4eb7166a79e6  consolidated.00.pth               ‚ïë&#xA; ‚ïë        37974873eb68a7ab30c4912fc36264ae  tokenizer.model                   ‚ïë&#xA; ‚ïô‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïú&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üî® Install&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name mixtralkit python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y&#xA;conda activate mixtralkit&#xA;&#xA;git clone https://github.com/open-compass/MixtralKit&#xA;cd MixtralKit/&#xA;pip install -r requirements.txt&#xA;pip install -e .&#xA;&#xA;ln -s path/to/checkpoints_folder/ ckpts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üöÄ Inference&lt;/h1&gt; &#xA;&lt;h2&gt;Text Completion&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/example.py -m ./ckpts -t ckpts/tokenizer.model --num-gpus 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Expected Results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;==============================Example START==============================&#xA;&#xA;[Prompt]:&#xA;Who are you?&#xA;&#xA;[Response]:&#xA;I am a designer and theorist; a lecturer at the University of Malta and a partner in the firm Barbagallo and Baressi Design, which won the prestig&#xA;ious Compasso d‚ÄôOro award in 2004. I was educated in industrial and interior design in the United States&#xA;&#xA;==============================Example END==============================&#xA;&#xA;==============================Example START==============================&#xA;&#xA;[Prompt]:&#xA;1 + 1 -&amp;gt; 3&#xA;2 + 2 -&amp;gt; 5&#xA;3 + 3 -&amp;gt; 7&#xA;4 + 4 -&amp;gt;&#xA;&#xA;[Response]:&#xA;9&#xA;5 + 5 -&amp;gt; 11&#xA;6 + 6 -&amp;gt; 13&#xA;&#xA;#include &amp;lt;iostream&amp;gt;&#xA;&#xA;using namespace std;&#xA;&#xA;int addNumbers(int x, int y)&#xA;{&#xA;        return x + y;&#xA;}&#xA;&#xA;int main()&#xA;{&#xA;&#xA;==============================Example END==============================&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üèóÔ∏è Evaluation&lt;/h1&gt; &#xA;&lt;h2&gt;Step-1: Setup OpenCompass&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone and Install OpenCompass&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# assume you have already create the conda env named mixtralkit &#xA;conda activate mixtralkit&#xA;&#xA;git clone https://github.com/open-compass/opencompass opencompass&#xA;cd opencompass&#xA;&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Prepare Evaluation Dataset&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download dataset to data/ folder&#xA;wget https://github.com/open-compass/opencompass/releases/download/0.1.8.rc1/OpenCompassData-core-20231110.zip&#xA;unzip OpenCompassData-core-20231110.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you need to evaluate the &lt;strong&gt;humaneval&lt;/strong&gt;, please go to &lt;a href=&#34;https://opencompass.readthedocs.io/en/latest/get_started/installation.html&#34;&gt;Installation Guide&lt;/a&gt; for more information&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Step-2: Pre-pare evaluation config and weights&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd opencompass/&#xA;# link the example config into opencompass&#xA;ln -s path/to/MixtralKit/playground playground&#xA;&#xA;# link the model weights into opencompass&#xA;mkdir -p ./models/mixtral/&#xA;ln -s path/to/checkpoints_folder/ ./models/mixtral/mixtral-8x7b-32kseqlen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently, you should have the files structure like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;opencompass/&#xA;‚îú‚îÄ‚îÄ configs&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ .....&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ .....&#xA;‚îú‚îÄ‚îÄ models&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ mixtral&#xA;‚îÇ&amp;nbsp;&amp;nbsp;     ‚îî‚îÄ‚îÄ mixtral-8x7b-32kseqlen&#xA;‚îú‚îÄ‚îÄ data/&#xA;‚îú‚îÄ‚îÄ playground&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ eval_mixtral.py&#xA;‚îÇ‚îÄ‚îÄ ......&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Step-3: Run evaluation experiments&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;HF_EVALUATE_OFFLINE=1 HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python run.py playground/eval_mixtral.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;ü§ù Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dzhulgakov/llama-mistral&#34;&gt;llama-mistral&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;llama&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üñäÔ∏è Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{2023opencompass,&#xA;    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},&#xA;    author={OpenCompass Contributors},&#xA;    howpublished = {\url{https://github.com/open-compass/opencompass}},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>allenai/papermage</title>
    <updated>2023-12-14T01:40:15Z</updated>
    <id>tag:github.com,2023-12-14:/allenai/papermage</id>
    <link href="https://github.com/allenai/papermage" rel="alternate"></link>
    <summary type="html">&lt;p&gt;library supporting NLP and CV research on scientific papers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;papermage&lt;/h1&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conda create -n papermage python=3.9&#xA;conda activate papermage&#xA;pip install -e &#39;.[dev,predictors,visualizers]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re on MacOSX, you&#39;ll also want to run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install poppler&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Unit testing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pytest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for latest failed test&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pytest --lf --no-cov -n0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for specific test name of class name&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pytest -k &#39;TestPDFPlumberParser&#39; --no-cov -n0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;h4&gt;1. Create a Document for the first time from a PDF&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;from papermage.recipes import CoreRecipe&#xA;&#xA;recipe = CoreRecipe()&#xA;doc = recipe.run(&#34;tests/fixtures/papermage.pdf&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Understanding the output: the &lt;code&gt;Document&lt;/code&gt; class&lt;/h4&gt; &#xA;&lt;p&gt;What is a &lt;code&gt;Document&lt;/code&gt;? At minimum, it is some text, saved under the &lt;code&gt;.symbols&lt;/code&gt; field, which is just a &lt;code&gt;&amp;lt;str&amp;gt;&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt; doc.symbols&#xA;&#34;PaperMage: A Unified Toolkit for Processing, Representing, and\nManipulating Visually-...&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;But this library is really useful when you have multiple different ways of segmenting &lt;code&gt;.symbols&lt;/code&gt;. For example, segmenting the paper into Pages, and then each page into Rows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for page in doc.pages:&#xA;    print(f&#39;\n=== PAGE: {page.id} ===\n\n&#39;)&#xA;    for row in page.rows:&#xA;        print(row.text)&#xA;        &#xA;...&#xA;=== PAGE: 5 ===&#xA;&#xA;4&#xA;Vignette: Building an Attributed QA&#xA;System for Scientific Papers&#xA;How could researchers leverage papermage for&#xA;their research? Here, we walk through a user sce-&#xA;nario in which a researcher (Lucy) is prototyping&#xA;an attributed QA system for science.&#xA;System Design.&#xA;Drawing inspiration from Ko&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This shows two nice aspects of this library:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Document&lt;/code&gt; provides iterables for different segmentations of &lt;code&gt;symbols&lt;/code&gt;. Options include things like &lt;code&gt;pages, tokens, rows, sentences, paragraphs, sections, ...&lt;/code&gt;. Not every Parser will provide every segmentation, though.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Each one of these segments (in our library, we call them &lt;code&gt;Entity&lt;/code&gt; objects) is aware of (and can access) other segment types. For example, you can call &lt;code&gt;page.rows&lt;/code&gt; to get all Rows that intersect a particular Page. Or you can call &lt;code&gt;sent.tokens&lt;/code&gt; to get all Tokens that intersect a particular Sentence. Or you can call &lt;code&gt;sent.rows&lt;/code&gt; to get the Row(s) that intersect a particular Sentence. These indexes are built &lt;em&gt;dynamically&lt;/em&gt; when the &lt;code&gt;Document&lt;/code&gt; is created and each time a new &lt;code&gt;Entity&lt;/code&gt; type is added. In the extreme, as long as those fields are available in the Document, you can write:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for page in doc.pages:&#xA;    for paragraph in page.paragraphs:&#xA;        for sent in paragraph.sentences:&#xA;            for row in sent.rows: &#xA;                ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can check which fields are available in a Document via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt; doc.fields&#xA;[&#39;tokens&#39;,&#xA; &#39;rows&#39;,&#xA; &#39;pages&#39;,&#xA; &#39;words&#39;,&#xA; &#39;sentences&#39;,&#xA; &#39;blocks&#39;,&#xA; &#39;vila_entities&#39;,&#xA; &#39;titles&#39;,&#xA; &#39;paragraphs&#39;,&#xA; &#39;authors&#39;,&#xA; &#39;abstracts&#39;,&#xA; &#39;keywords&#39;,&#xA; &#39;sections&#39;,&#xA; &#39;lists&#39;,&#xA; &#39;bibliographies&#39;,&#xA; &#39;equations&#39;,&#xA; &#39;algorithms&#39;,&#xA; &#39;figures&#39;,&#xA; &#39;tables&#39;,&#xA; &#39;captions&#39;,&#xA; &#39;headers&#39;,&#xA; &#39;footers&#39;,&#xA; &#39;footnotes&#39;,&#xA; &#39;symbols&#39;,&#xA; &#39;images&#39;,&#xA; &#39;metadata&#39;,&#xA; &#39;entities&#39;,&#xA; &#39;relations&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Understanding intersection of Entities&lt;/h4&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;Entity&lt;/code&gt;s don&#39;t necessarily perfectly nest each other. For example, what happens if you run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for sent in doc.sentences:&#xA;    for row in sent.rows:&#xA;        print([token.text for token in row.tokens])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Tokens that are &lt;em&gt;outside&lt;/em&gt; each sentence can still be printed. This is because when we jump from a sentence to its rows, we are looking for &lt;em&gt;all&lt;/em&gt; rows that have &lt;em&gt;any&lt;/em&gt; overlap with the sentence. Rows can extend beyond sentence boundaries, and as such, can contain tokens outside that sentence.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s another example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for page in doc.pages:&#xA;    print([sent.text for sent in page.sentences])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sentences can cross page boundaries. As such, adjacent pages may end up printing the same sentence.&lt;/p&gt; &#xA;&lt;p&gt;But rows and tokens adhere strictly to page boundaries, and thus will not repeat when printed across pages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for page in doc.pages:&#xA;    print([row.text for row in page.rows])&#xA;    print([token.text for token in page.tokens])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A key aspect of using this library is understanding how these different fields are defined &amp;amp; anticipating how they might interact with each other. We try to make decisions that are intuitive, but we do ask users to experiment with fields to build up familiarity.&lt;/p&gt; &#xA;&lt;h4&gt;4. What&#39;s in an &lt;code&gt;Entity&lt;/code&gt;?&lt;/h4&gt; &#xA;&lt;p&gt;Each &lt;code&gt;Entity&lt;/code&gt; object stores information about its contents and position:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;.spans: List[Span]&lt;/code&gt;, A &lt;code&gt;Span&lt;/code&gt; is a pointer into &lt;code&gt;Document.symbols&lt;/code&gt; (that is, &lt;code&gt;Span(start=0, end=5)&lt;/code&gt; corresponds to &lt;code&gt;symbols[0:5]&lt;/code&gt;). By default, when you iterate over an &lt;code&gt;Entity&lt;/code&gt;, you iterate over its &lt;code&gt;.spans&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;.boxes: List[Box]&lt;/code&gt;, A &lt;code&gt;Box&lt;/code&gt; represents a rectangular region on the page. Each span is associated a Box.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;.metadata: Metadata&lt;/code&gt;, A free form dictionary-like object to store extra metadata about that &lt;code&gt;Entity&lt;/code&gt;. These are usually empty.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;5. How can I manually create my own &lt;code&gt;Document&lt;/code&gt;?&lt;/h4&gt; &#xA;&lt;p&gt;A &lt;code&gt;Document&lt;/code&gt; is created by stitching together 3 types of tools: &lt;code&gt;Parsers&lt;/code&gt;, &lt;code&gt;Rasterizers&lt;/code&gt; and &lt;code&gt;Predictors&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Parsers&lt;/code&gt; take a PDF as input and return a &lt;code&gt;Document&lt;/code&gt; compared of &lt;code&gt;.symbols&lt;/code&gt; and other fields. The example one we use is a wrapper around &lt;a href=&#34;https://github.com/jsvine/pdfplumber&#34;&gt;PDFPlumber&lt;/a&gt; - MIT License utility.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Rasterizers&lt;/code&gt; take a PDF as input and return an &lt;code&gt;Image&lt;/code&gt; per page that is added to &lt;code&gt;Document.images&lt;/code&gt;. The example one we use is &lt;a href=&#34;https://github.com/Belval/pdf2image&#34;&gt;PDF2Image&lt;/a&gt; - MIT License.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Predictors&lt;/code&gt; take a &lt;code&gt;Document&lt;/code&gt; and apply some operation to compute a new set of &lt;code&gt;Entity&lt;/code&gt; objects that we can insert into our &lt;code&gt;Document&lt;/code&gt;. These are all built in-house and can be either simple heuristics or full machine-learning models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;6. How can I save my &lt;code&gt;Document&lt;/code&gt;?&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;with open(&#39;filename.json&#39;, &#39;w&#39;) as f_out:&#xA;    json.dump(doc.to_json(with_images=True), f_out, indent=4)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will produce something akin to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;    &#34;symbols&#34;: &#34;PaperMage: A Unified Toolkit for Processing, Representing, an...&#34;,&#xA;    &#34;entities&#34;: {&#xA;        &#34;images&#34;: [...],&#xA;        &#34;rows&#34;: [...],&#xA;        &#34;tokens&#34;: [...],&#xA;        &#34;words&#34;: [...],&#xA;        &#34;blocks&#34;: [...],&#xA;        &#34;sentences&#34;: [...]&#xA;    },&#xA;    &#34;metadata&#34;: {...}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;Images&lt;/code&gt; are serialized to &lt;code&gt;base64&lt;/code&gt; if you include &lt;code&gt;with_images&lt;/code&gt; flag. Otherwise, it&#39;s left out of JSON serialization by default.&lt;/p&gt; &#xA;&lt;h4&gt;7. How can I load my &lt;code&gt;Document&lt;/code&gt;?&lt;/h4&gt; &#xA;&lt;p&gt;These can be used to reconstruct a &lt;code&gt;Document&lt;/code&gt; again via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with open(&#39;filename.json&#39;) as f_in:&#xA;    doc_dict = json.load(f_in)&#xA;    doc = Document.from_json(doc_dict)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: A common pattern for adding fields to a document is to load in a previously saved document, run some additional &lt;code&gt;Predictors&lt;/code&gt; on it, and save the result.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;papermage/predictors/README.md&lt;/code&gt; for more information about training custom predictors on your own data.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;papermage/examples/quick_start_demo.ipynb&lt;/code&gt; for a notebook walking through some more usage patterns.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>guoqincode/AnimateAnyone-unofficial</title>
    <updated>2023-12-14T01:40:15Z</updated>
    <id>tag:github.com,2023-12-14:/guoqincode/AnimateAnyone-unofficial</id>
    <link href="https://github.com/guoqincode/AnimateAnyone-unofficial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unofficial Implementation of Animate Anyone&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Unofficial Implementation of Animate Anyone&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains an simple and unofficial implementation of &lt;a href=&#34;https://humanaigc.github.io/animate-anyone/&#34;&gt;Animate Anyone&lt;/a&gt;. This project is built upon &lt;a href=&#34;https://github.com/magic-research/magic-animate/tree/main&#34;&gt;magic-animate&lt;/a&gt; and &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;AnimateDiff&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Note !!!&lt;/h2&gt; &#xA;&lt;p&gt;This project is under continuous development in part-time, there may be bugs in the code, welcome to correct them, I will optimize the code after the pre-trained model is released!&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Same as &lt;a href=&#34;https://github.com/magic-research/magic-animate/tree/main&#34;&gt;magic-animate&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ToDo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Release Training Code.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; DeepSpeed + Accelerator Training&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;strong&gt;Release Inference Code and unofficial pre-trained weights.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h4&gt;First Stage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torchrun --nnodes=1 --nproc_per_node=1 train.py --config configs/training/train_stage_1.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Second Stage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torchrun --nnodes=1 --nproc_per_node=1 train.py --config configs/training/train_stage_2.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h4&gt;First Stage&lt;/h4&gt; &#xA;&lt;h4&gt;Second Stage&lt;/h4&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Special thanks to the original authors of the &lt;a href=&#34;https://humanaigc.github.io/animate-anyone/&#34;&gt;Animate Anyone&lt;/a&gt; project and the contributors to the &lt;a href=&#34;https://github.com/magic-research/magic-animate/tree/main&#34;&gt;magic-animate&lt;/a&gt; and &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;AnimateDiff&lt;/a&gt; repository for their open research and foundational work that inspired this unofficial implementation.&lt;/p&gt;</summary>
  </entry>
</feed>