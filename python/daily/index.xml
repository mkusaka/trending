<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-08T01:39:48Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>docker/genai-stack</title>
    <updated>2023-10-08T01:39:48Z</updated>
    <id>tag:github.com,2023-10-08:/docker/genai-stack</id>
    <link href="https://github.com/docker/genai-stack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Langchain + Docker + Neo4j&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GenAI Stack&lt;/h1&gt; &#xA;&lt;p&gt;This GenAI application stack will get you started building your own GenAI application in no time. The demo applications can serve as inspiration or as a starting point.&lt;/p&gt; &#xA;&lt;h1&gt;Configure&lt;/h1&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file from the environment template file &lt;code&gt;env.example&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;LLM Configuration&lt;/h2&gt; &#xA;&lt;p&gt;MacOS and Linux users can use any LLM that&#39;s available via Ollama. Check the &#34;tags&#34; section under the model page you want to use on &lt;a href=&#34;https://ollama.ai/library&#34;&gt;https://ollama.ai/library&lt;/a&gt; and write the tag for the value of the environment variable &lt;code&gt;LLM=&lt;/code&gt; in th e&lt;code&gt;.env&lt;/code&gt; file. All platforms can use GPT-3.5-turbo and GPT-4 (bring your own API keys for OpenAIs models).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MacOS&lt;/strong&gt; Install &lt;a href=&#34;https://ollama.ai&#34;&gt;Ollama&lt;/a&gt; in MacOS and start it before running &lt;code&gt;docker compose up&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Linux&lt;/strong&gt; No need to install Ollama manually, it will run in a container as part of the stack when running with the Linus profile: &lt;code&gt;run docker compose --profile linux up&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt; Not supported by Ollama, so Windows users need to generate a OpenAI API key and configure the stack to use &lt;code&gt;gpt-3.5&lt;/code&gt; or &lt;code&gt;gpt-4&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h1&gt;Develop&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] There is a performance issue that impacts python applications in the latest release of Docker Desktop. Until a fix is available, please use &lt;a href=&#34;https://docs.docker.com/desktop/release-notes/#4230&#34;&gt;version &lt;code&gt;4.23.0&lt;/code&gt;&lt;/a&gt; or earlier.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;To start everything&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If changes to build scripts has been made, &lt;strong&gt;rebuild&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enter &lt;strong&gt;watch mode&lt;/strong&gt; (auto rebuild on file changes). First start everything, then in new terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose alpha watch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shutdown&lt;/strong&gt; Is health check fails or containers doesn&#39;t start up as expected, shutdown completely to start up again.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose down&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Applications&lt;/h1&gt; &#xA;&lt;h2&gt;App 1 - Support Agent Bot&lt;/h2&gt; &#xA;&lt;p&gt;UI: &lt;a href=&#34;http://localhost:8501&#34;&gt;http://localhost:8501&lt;/a&gt; DB client: &lt;a href=&#34;http://localhost:7474&#34;&gt;http://localhost:7474&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;answer support question based on recent entries&lt;/li&gt; &#xA; &lt;li&gt;provide summarized answers with sources&lt;/li&gt; &#xA; &lt;li&gt;demonstrate difference between &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RAG Disabled (pure LLM reponse)&lt;/li&gt; &#xA;   &lt;li&gt;RAG Enabled (vector + knowledge graph context)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;allow to generate a high quality support ticket for the current conversation based on the style of highly rated questions in the database.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/docker/genai-stack/main/.github/media/app1-rag-selector.png&#34; alt=&#34;&#34;&gt; &lt;em&gt;(Chat input + RAG mode selector)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/docker/genai-stack/main/.github/media/app1-generate.png&#34; alt=&#34;&#34;&gt; &lt;em&gt;(CTA to auto generate support ticket draft)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/docker/genai-stack/main/.github/media/app1-ticket.png&#34; alt=&#34;&#34;&gt; &lt;em&gt;(UI of the auto generated support ticket draft)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;App 2 Loader&lt;/h2&gt; &#xA;&lt;p&gt;UI: &lt;a href=&#34;http://localhost:8502&#34;&gt;http://localhost:8502&lt;/a&gt; DB client: &lt;a href=&#34;http://localhost:7474&#34;&gt;http://localhost:7474&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;import recent SO data for certain tags into a KG&lt;/li&gt; &#xA; &lt;li&gt;embed questions and answers and store in vector index&lt;/li&gt; &#xA; &lt;li&gt;UI: choose tags, run import, see progress, some stats of data in the database&lt;/li&gt; &#xA; &lt;li&gt;Load high ranked questions (regardless of tags) to support the ticket generation feature of App 1.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/docker/genai-stack/main/.github/media/app2-ui-1.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/docker/genai-stack/main/.github/media/app2-model.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;App 3 Question / Answer with a local PDF&lt;/h2&gt; &#xA;&lt;p&gt;UI: &lt;a href=&#34;http://localhost:8503&#34;&gt;http://localhost:8503&lt;/a&gt;&lt;br&gt; DB client: &lt;a href=&#34;http://localhost:7474&#34;&gt;http://localhost:7474&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This application let&#39;s you load a local PDF into text chunks and embed it into Neo4j so you can ask questions about its contents and have the LLM answer them using vector similarity search.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/docker/genai-stack/main/.github/media/app3-ui.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hotshotco/Hotshot-XL</title>
    <updated>2023-10-08T01:39:48Z</updated>
    <id>tag:github.com,2023-10-08:/hotshotco/Hotshot-XL</id>
    <link href="https://github.com/hotshotco/Hotshot-XL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚ú® Hotshot-XL: State-of-the-art AI text-to-GIF model trained to work alongside Stable Diffusion XL&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;&lt;img src=&#34;https://i.imgur.com/HsWXQTW.png&#34; width=&#34;24px&#34; alt=&#34;logo&#34;&gt; Hotshot-XL&lt;/h1&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.hotshot.co&#34;&gt;üåê Try it&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://huggingface.co/hotshotco/Hotshot-XL&#34;&gt;üÉè Model card&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://discord.gg/85pqA3GG&#34;&gt;üí¨ Discord&lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/image-gen/gif_e8a50e1e-0b2e-4ebc-8229-817703585405.gif&#34; alt=&#34;a barbie doll smiling in kitchen, oven on fire, disaster, pink wes anderson vibes, cinematic&#34; width=&#34;195px&#34; height=&#34;111.42px&#34;&gt; &amp;nbsp; &lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/image-gen/gif_f6ca56a3-30b8-4b2a-9342-111353e85b96.gif&#34; alt=&#34;a teddy bear writing a letter&#34; width=&#34;195px&#34; height=&#34;111.42px&#34;&gt; &amp;nbsp; &lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/image-gen/gif_6c219102-7f72-45e9-b4fa-b7a07c004ae1.gif&#34; alt=&#34;dslr photo of mark zuckerberg happy, pulling on threads, lots of threads everywhere, laughing, hd, 8k&#34; width=&#34;195px&#34; height=&#34;111.42px&#34;&gt; &amp;nbsp; &lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/image-gen/gif_2dd3c30f-42c5-4f37-8fa6-b2494fcac4b4.gif&#34; alt=&#34;a cat laughing&#34; width=&#34;195px&#34; height=&#34;111.42px&#34;&gt; &amp;nbsp; &lt;/p&gt; &#xA;&lt;p&gt;Hotshot-XL is an AI text-to-GIF model trained to work alongside &lt;a href=&#34;https://stability.ai/stable-diffusion&#34;&gt;Stable Diffusion XL&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Hotshot-XL can generate GIFs with any fine-tuned SDXL model. This means two things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You‚Äôll be able to make GIFs with any existing or newly fine-tuned SDXL model you may want to use.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;d like to make GIFs of personalized subjects, you can load your own SDXL based LORAs, and not have to worry about fine-tuning Hotshot-XL. This is awesome because it‚Äôs usually much easier to find suitable images for training data than it is to find videos. It also hopefully fits into everyone&#39;s existing LORA usage/workflows :) See more &lt;a href=&#34;https://raw.githubusercontent.com/hotshotco/Hotshot-XL/main/#text-to-gif-with-personalized-loras&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Hotshot-XL is compatible with SDXL ControlNet to make GIFs in the composition/layout you‚Äôd like. See the &lt;a href=&#34;https://raw.githubusercontent.com/hotshotco/Hotshot-XL/main/#text-to-gif-with-controlnet&#34;&gt;ControlNet&lt;/a&gt; section below.&lt;/p&gt; &#xA;&lt;p&gt;Hotshot-XL was trained to generate 1 second GIFs at 8 FPS.&lt;/p&gt; &#xA;&lt;p&gt;Hotshot-XL was trained on various aspect ratios. For best results with the base Hotshot-XL model, we recommend using it with an SDXL model that has been fine-tuned with 512x512 images. You can find an SDXL model we fine-tuned for 512x512 resolutions &lt;a href=&#34;https://huggingface.co/hotshotco/SDXL-512&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;üåê Try It&lt;/h1&gt; &#xA;&lt;p&gt;Try Hotshot-XL yourself here: &lt;a href=&#34;https://www.hotshot.co&#34;&gt;https://www.hotshot.co&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or, if you&#39;d like to run Hotshot-XL yourself locally, continue on to the sections below.&lt;/p&gt; &#xA;&lt;p&gt;If you‚Äôre running Hotshot-XL yourself, you are going to be able to have a lot more flexibility/control with the model. As a very simple example, you‚Äôll be able to change the sampler. We‚Äôve seen best results with Euler-A so far, but you may find interesting results with some other ones.&lt;/p&gt; &#xA;&lt;h1&gt;üîß Setup&lt;/h1&gt; &#xA;&lt;h3&gt;Environment Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install virtualenv --upgrade&#xA;virtualenv -p $(which python3) venv&#xA;source venv/bin/activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download the Hotshot-XL Weights&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make sure you have git-lfs installed (https://git-lfs.com)&#xA;git lfs install&#xA;git clone https://huggingface.co/hotshotco/Hotshot-XL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or visit &lt;a href=&#34;https://huggingface.co/hotshotco/Hotshot-XL&#34;&gt;https://huggingface.co/hotshotco/Hotshot-XL&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Download our fine-tuned SDXL model (or BYOSDXL)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Note&lt;/em&gt;: To maximize data and training efficiency, Hotshot-XL was trained at various aspect ratios around 512x512 resolution. For best results with the base Hotshot-XL model, we recommend using it with an SDXL model that has been fine-tuned with images around the 512x512 resolution. You can download an SDXL model we trained with images at 512x512 resolution below, or bring your own SDXL base model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make sure you have git-lfs installed (https://git-lfs.com)&#xA;git lfs install&#xA;git clone https://huggingface.co/hotshotco/SDXL-512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or visit &lt;a href=&#34;https://huggingface.co/hotshotco/SDXL-512&#34;&gt;https://huggingface.co/hotshotco/SDXL-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üîÆ Inference&lt;/h1&gt; &#xA;&lt;h3&gt;Text-to-GIF&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py \&#xA;  --prompt=&#34;a bulldog in the captains chair of a spaceship, hd, high quality&#34; \&#xA;  --output=&#34;output.gif&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;What to Expect:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Sasquatch scuba diving&lt;/th&gt; &#xA;   &lt;th&gt;a camel smoking a cigarette&lt;/th&gt; &#xA;   &lt;th&gt;Ronald McDonald sitting at a vanity mirror putting on lipstick&lt;/th&gt; &#xA;   &lt;th&gt;drake licking his lips and staring through a window at a cupcake&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/image-gen/gif_441b7ea2-9887-4124-a52b-14c9db1d15aa.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/image-gen/gif_7956a022-0464-4441-88b8-15a6de953335.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/image-gen/gif_35f55a64-7ed9-498e-894e-6ec7a8026fba.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/image-gen/gif_df5f52cb-d74d-40b5-a066-2ce567dae512.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Text-to-GIF with personalized LORAs&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py \&#xA;  --prompt=&#34;a bulldog in the captains chair of a spaceship, hd, high quality&#34; \&#xA;  --output=&#34;output.gif&#34; \&#xA;  --spatial_unet_base=&#34;path/to/stabilityai/stable-diffusion-xl-base-1.0/unet&#34; \&#xA;  --lora=&#34;path/to/lora&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;What to Expect:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The outputs below use the DDIMScheduler.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;sks person screaming at a capri sun&lt;/th&gt; &#xA;   &lt;th&gt;sks person kissing kermit the frog&lt;/th&gt; &#xA;   &lt;th&gt;sks person wearing a tuxedo holding up a glass of champagne, fireworks in background, hd, high quality, 4K&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/inf-temp/79a20eae-ffeb-4d24-8d22-609fa77c292f.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/r/aakash.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/inf-temp/4fa34a16-2835-4a12-8c59-348caa4f3891.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Text-to-GIF with ControlNet&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py \&#xA;  --prompt=&#34;a girl jumping up and down and pumping her fist, hd, high quality&#34; \&#xA;  --output=&#34;output.gif&#34; \&#xA;  --control_type=&#34;depth&#34; \&#xA;  --gif=&#34;https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExbXNneXJicG1mOHJ2dzQ2Y2JteDY1ZWlrdjNjMjl3ZWxyeWFxY2EzdyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/YOTAoXBgMCmFeQQzuZ/giphy.gif&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, Hotshot-XL will create key frames from your source gif using 8 equally spaced frames and crop the keyframes to the default aspect ratio. For finer grained control, learn how to &lt;a href=&#34;https://raw.githubusercontent.com/hotshotco/Hotshot-XL/main/#varying-aspect-ratios&#34;&gt;vary aspect ratios&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/hotshotco/Hotshot-XL/main/#varying-frame-rates--lengths-experimental&#34;&gt;vary frame rates/lengths&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Hotshot-XL currently supports the use of one ControlNet model at a time; supporting Multi-ControlNet would be &lt;a href=&#34;https://raw.githubusercontent.com/hotshotco/Hotshot-XL/main/#-further-work&#34;&gt;exciting&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;What to Expect:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;pixar style girl putting two thumbs up, happy, high quality, 8k, 3d, animated disney render&lt;/th&gt; &#xA;   &lt;th&gt;keanu reaves holding a sign that says &#34;HELP&#34;, hd, high quality&lt;/th&gt; &#xA;   &lt;th&gt;a woman laughing, hd, high quality&lt;/th&gt; &#xA;   &lt;th&gt;barack obama making a rainbow with their hands, the word &#34;MAGIC&#34; in front of them, wearing a blue and white striped hoodie, hd, high quality&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/inf-temp/387d8b68-7289-45e3-9b21-1a9e6ad8a782.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot%2Finf-temp/047543b2-d499-4de8-8fd2-3712c3a6c446.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/inf-temp/8f50f4d8-4b86-4df7-a643-aae3e9d8634d.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/inf-temp/c133d8b7-46ad-4469-84fd-b7f7444a47a0.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Control&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://media1.giphy.com/media/3o6Zt8qDiPE2d3kayI/giphy.gif?cid=ecf05e47igskj73xpl62pv8kyk9m39brlualxcz1j68vk8ul&amp;amp;ep=v1_gifs_related&amp;amp;rid=giphy.gif&amp;amp;ct=g&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://media2.giphy.com/media/IoXVrbzUIuvTy/giphy.gif?cid=ecf05e47ill5r35i1bhxk0tr7quqbpruqivjtuy7gcgkfmx5&amp;amp;ep=v1_gifs_search&amp;amp;rid=giphy.gif&amp;amp;ct=g&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://media0.giphy.com/media/12msOFU8oL1eww/giphy.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://media4.giphy.com/media/3o84U6421OOWegpQhq/giphy.gif?cid=ecf05e47eufup08cz2up9fn9bitkgltb88ez37829mxz43cc&amp;amp;ep=v1_gifs_related&amp;amp;rid=giphy.gif&amp;amp;ct=g&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Varying Aspect Ratios&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Note&lt;/em&gt;: The base SDXL model is trained to best create images around 1024x1024 resolution. To maximize data and training efficiency, Hotshot-XL was trained at aspect ratios around 512x512 resolution. Please see &lt;a href=&#34;https://raw.githubusercontent.com/hotshotco/Hotshot-XL/main/#supported-aspect-ratios&#34;&gt;Additional Notes&lt;/a&gt; for a list of aspect ratios the base Hotshot-XL model was trained with.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Like SDXL, Hotshot-XL was trained at various aspect ratios with aspect ratio bucketing, and includes support for SDXL parameters like target-size and original-size. This means you can create GIFs at several different aspect ratios and resolutions, just with the base Hotshot-XL model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py \&#xA;  --prompt=&#34;a bulldog in the captains chair of a spaceship, hd, high quality&#34; \&#xA;  --output=&#34;output.gif&#34; \&#xA;  --width=&amp;lt;WIDTH&amp;gt; \&#xA;  --height=&amp;lt;HEIGHT&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;What to Expect:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;512x512&lt;/th&gt; &#xA;   &lt;th&gt;672x384&lt;/th&gt; &#xA;   &lt;th&gt;384x672&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;a monkey playing guitar, nature footage, hd, high quality&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/inf-temp/2295c6af-c345-47a4-8afe-62e77f84141b.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/inf-temp/909a86c5-60df-459a-b662-ce4e85706303.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://dvfx9cgvtgnyd.cloudfront.net/hotshot/inf-temp/8512854d-66ea-41ff-919e-6e36d6e6a541.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Varying frame rates &amp;amp; lengths (&lt;em&gt;Experimental&lt;/em&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;By default, Hotshot-XL is trained to generate GIFs that are 1 second long with 8FPS. If you&#39;d like to play with generating GIFs with varying frame rates and time lengths, you can try out the parameters &lt;code&gt;video_length&lt;/code&gt; and &lt;code&gt;video_duration&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;video_length&lt;/code&gt; sets the number of frames. The default value is 8.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;video_duration&lt;/code&gt; sets the runtime of the output gif in milliseconds. The default value is 1000.&lt;/p&gt; &#xA;&lt;p&gt;Please note that you should expect unstable/&#34;jittery&#34; results when modifying these parameters as the model was only trained with 1s videos @ 8fps. You&#39;ll be able to improve the stability of results for different time lengths and frame rates by &lt;a href=&#34;https://raw.githubusercontent.com/hotshotco/Hotshot-XL/main/#-fine-tuning&#34;&gt;fine-tuning Hotshot-XL&lt;/a&gt;. Please let us know if you do!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py \&#xA;  --prompt=&#34;a bulldog in the captains chair of a spaceship, hd, high quality&#34; \&#xA;  --output=&#34;output.gif&#34; \&#xA;  --video_length=16 \&#xA;  --video_duration=2000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spatial Layers Only&lt;/h3&gt; &#xA;&lt;p&gt;Hotshot-XL is trained to generate GIFs alongside SDXL. If you&#39;d like to generate just an image, you can simply set &lt;code&gt;video_length=1&lt;/code&gt; in your inference call and the Hotshot-XL temporal layers will be ignored, as you&#39;d expect.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py \&#xA;  --prompt=&#34;a bulldog in the captains chair of a spaceship, hd, high quality&#34; \&#xA;  --output=&#34;output.jpg&#34; \&#xA;  --video_length=1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Additional Notes&lt;/h3&gt; &#xA;&lt;h4&gt;Supported Aspect Ratios&lt;/h4&gt; &#xA;&lt;p&gt;Hotshot-XL was trained at the following aspect ratios; to reliably generate GIFs outside the range of these aspect ratios, you will want to fine-tune Hotshot-XL with videos at the resolution of your desired aspect ratio.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Aspect Ratio&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.42&lt;/td&gt; &#xA;   &lt;td&gt;320 x 768&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.57&lt;/td&gt; &#xA;   &lt;td&gt;384 x 672&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.68&lt;/td&gt; &#xA;   &lt;td&gt;416 x 608&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.00&lt;/td&gt; &#xA;   &lt;td&gt;512 x 512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.46&lt;/td&gt; &#xA;   &lt;td&gt;608 x 416&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.75&lt;/td&gt; &#xA;   &lt;td&gt;672 x 384&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.40&lt;/td&gt; &#xA;   &lt;td&gt;768 x 320&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;üí™ Fine-Tuning&lt;/h1&gt; &#xA;&lt;p&gt;The following section relates to fine-tuning the Hotshot-XL temporal model with additional text/video pairs. If you&#39;re trying to generate GIFs of personalized concepts/subjects, we&#39;d recommend not fine-tuning Hotshot-XL, but instead training your own SDXL based LORAs and &lt;a href=&#34;https://raw.githubusercontent.com/hotshotco/Hotshot-XL/main/#text-to-gif-with-personalized-loras&#34;&gt;just loading those&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-Tuning Hotshot-XL&lt;/h3&gt; &#xA;&lt;h4&gt;Dataset Preparation&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;fine_tune.py&lt;/code&gt; script expects your samples to be structured like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fine_tune_dataset&#xA;‚îú‚îÄ‚îÄ sample_001&#xA;‚îÇ  ‚îú‚îÄ‚îÄ 0.jpg&#xA;‚îÇ  ‚îú‚îÄ‚îÄ 1.jpg&#xA;‚îÇ  ‚îú‚îÄ‚îÄ 2.jpg&#xA;...&#xA;...&#xA;‚îÇ  ‚îú‚îÄ‚îÄ n.jpg&#xA;‚îÇ  ‚îî‚îÄ‚îÄ prompt.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each sample directory should contain your &lt;strong&gt;n key frames&lt;/strong&gt; and a &lt;code&gt;prompt.txt&lt;/code&gt; file which contains the prompt. The final checkpoint will be saved to &lt;code&gt;output_dir&lt;/code&gt;. We&#39;ve found it useful to send validation GIFs to &lt;a href=&#34;https://raw.githubusercontent.com/hotshotco/Hotshot-XL/main/www.wandb.ai&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; every so often. If you choose to use validation with Weights &amp;amp; Biases, you can set how often this runs with the &lt;code&gt;validate_every_steps&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch fine_tune.py \&#xA;    --output_dir=&#34;&amp;lt;OUTPUT_DIR&amp;gt;&#34; \&#xA;    --data_dir=&#34;fine_tune_dataset&#34; \&#xA;    --report_to=&#34;wandb&#34; \&#xA;    --run_validation_at_start \&#xA;    --resolution=512 \&#xA;    --mixed_precision=fp16 \&#xA;    --train_batch_size=4 \&#xA;    --learning_rate=1.25e-05 \&#xA;    --lr_scheduler=&#34;constant&#34; \&#xA;    --lr_warmup_steps=0 \&#xA;    --max_train_steps=1000 \&#xA;    --save_n_steps=20 \&#xA;    --validate_every_steps=50 \&#xA;    --vae_b16 \&#xA;    --gradient_checkpointing \&#xA;    --noise_offset=0.05 \&#xA;    --snr_gamma \&#xA;    --test_prompts=&#34;man sits at a table in a cafe, he greets another man with a smile and a handshakes&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üìù Further work&lt;/h1&gt; &#xA;&lt;p&gt;There are lots of ways we are excited about improving Hotshot-XL. For example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine-Tuning Hotshot-XL at larger frame rates to create longer/higher frame-rate GIFs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine-Tuning Hotshot-XL at larger resolutions to create higher resolution GIFs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training temporal layers for a latent upscaler to produce higher resolution GIFs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training an image conditioned &#34;frame prediction&#34; model for more coherent, longer GIFs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training temporal layers for a VAE to mitigate flickering/dithering in outputs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Supporting Multi-ControlNet for greater control over GIF generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training &amp;amp; integrating different ControlNet models for further control over GIF generation (finer facial expression control would be very cool)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Moving Hotshot-XL into &lt;a href=&#34;https://github.com/facebookincubator/AITemplate&#34;&gt;AITemplate&lt;/a&gt; for faster inference times&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We üíó contributions from the open-source community! Please let us know in the issues or PRs if you&#39;re interested in working on these improvements or anything else!&lt;/p&gt; &#xA;&lt;h1&gt;üôè Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Text-to-Video models are improving quickly and the development of Hotshot-XL has been greatly inspired by the following amazing works and teams:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stability.ai/stable-diffusion&#34;&gt;SDXL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/VideoLDM/&#34;&gt;Align Your Latents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://makeavideo.studio/&#34;&gt;Make-A-Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://animatediff.github.io/&#34;&gt;AnimateDiff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagen.research.google/video/&#34;&gt;Imagen Video&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We hope that releasing this model/codebase helps the community to continue pushing these creative tools forward in an open and responsible way.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fail2ban/fail2ban</title>
    <updated>2023-10-08T01:39:48Z</updated>
    <id>tag:github.com,2023-10-08:/fail2ban/fail2ban</id>
    <link href="https://github.com/fail2ban/fail2ban" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Daemon to ban hosts that cause multiple authentication errors&lt;/p&gt;&lt;hr&gt;&lt;pre&gt;&lt;code&gt;                     __      _ _ ___ _               &#xA;                    / _|__ _(_) |_  ) |__  __ _ _ _  &#xA;                   |  _/ _` | | |/ /| &#39;_ \/ _` | &#39; \ &#xA;                   |_| \__,_|_|_/___|_.__/\__,_|_||_|&#xA;                   v1.1.0.dev1            20??/??/??&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fail2Ban: ban hosts that cause multiple authentication errors&lt;/h2&gt; &#xA;&lt;p&gt;Fail2Ban scans log files like &lt;code&gt;/var/log/auth.log&lt;/code&gt; and bans IP addresses conducting too many failed login attempts. It does this by updating system firewall rules to reject new connections from those IP addresses, for a configurable amount of time. Fail2Ban comes out-of-the-box ready to read many standard log files, such as those for sshd and Apache, and is easily configured to read any log file of your choosing, for any error you wish.&lt;/p&gt; &#xA;&lt;p&gt;Though Fail2Ban is able to reduce the rate of incorrect authentication attempts, it cannot eliminate the risk presented by weak authentication. Set up services to use only two factor, or public/private authentication mechanisms if you really want to protect services.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;http://www.worldipv6launch.org/wp-content/themes/ipv6/downloads/World_IPv6_launch_logo.svg?sanitize=true&#34; height=&#34;52pt&#34;&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Since v0.10 fail2ban supports the matching of IPv6 addresses.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;This README is a quick introduction to Fail2Ban. More documentation, FAQ, and HOWTOs to be found on fail2ban(1) manpage, &lt;a href=&#34;https://github.com/fail2ban/fail2ban/wiki&#34;&gt;Wiki&lt;/a&gt;, &lt;a href=&#34;https://fail2ban.readthedocs.io/&#34;&gt;Developers documentation&lt;/a&gt; and the website: &lt;a href=&#34;https://www.fail2ban.org&#34;&gt;https://www.fail2ban.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation:&lt;/h2&gt; &#xA;&lt;p&gt;Fail2Ban is likely already packaged for your Linux distribution and &lt;a href=&#34;https://github.com/fail2ban/fail2ban/wiki/How-to-install-fail2ban-packages&#34;&gt;can installed with a simple command&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If your distribution is not listed, you can install from GitHub:&lt;/p&gt; &#xA;&lt;p&gt;Required:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org&#34;&gt;Python &amp;gt;= 3.5&lt;/a&gt; or &lt;a href=&#34;https://pypy.org&#34;&gt;PyPy3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;python-setuptools, python-distutils (or python3-setuptools) for installation from source&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Optional:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/seb-m/pyinotify&#34;&gt;pyinotify &amp;gt;= 0.8.3&lt;/a&gt;, may require: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux &amp;gt;= 2.6.13&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.freedesktop.org/wiki/Software/systemd&#34;&gt;systemd &amp;gt;= 204&lt;/a&gt; and python bindings: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.freedesktop.org/software/systemd/python-systemd/index.html&#34;&gt;python-systemd package&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.dnspython.org/&#34;&gt;dnspython&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tar xvfj fail2ban-master.tar.bz2&#xA;cd fail2ban-master&#xA;sudo python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can clone the source from GitHub to a directory of Your choice, and do the install from there. Pick the correct branch, for example, master or 0.11&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/fail2ban/fail2ban.git&#xA;cd fail2ban&#xA;sudo python setup.py install &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will install Fail2Ban into the python library directory. The executable scripts are placed into &lt;code&gt;/usr/bin&lt;/code&gt;, and configuration in &lt;code&gt;/etc/fail2ban&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Fail2Ban should be correctly installed now. Just type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fail2ban-client -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to see if everything is alright. You should always use fail2ban-client and never call fail2ban-server directly. You can verify that you have the correct version installed with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fail2ban-client version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that the system init/service script is not automatically installed. To enable fail2ban as an automatic service, simply copy the script for your distro from the &lt;code&gt;files&lt;/code&gt; directory to &lt;code&gt;/etc/init.d&lt;/code&gt;. Example (on a Debian-based system):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cp files/debian-initd /etc/init.d/fail2ban&#xA;update-rc.d fail2ban defaults&#xA;service fail2ban start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuration:&lt;/h2&gt; &#xA;&lt;p&gt;You can configure Fail2Ban using the files in &lt;code&gt;/etc/fail2ban&lt;/code&gt;. It is possible to configure the server using commands sent to it by &lt;code&gt;fail2ban-client&lt;/code&gt;. The available commands are described in the fail2ban-client(1) manpage. Also see fail2ban(1) and jail.conf(5) manpages for further references.&lt;/p&gt; &#xA;&lt;h2&gt;Code status:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fail2ban/fail2ban/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/fail2ban/fail2ban/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact:&lt;/h2&gt; &#xA;&lt;h3&gt;Bugs, feature requests, discussions?&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/fail2ban/fail2ban/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;You just appreciate this program:&lt;/h3&gt; &#xA;&lt;p&gt;Send kudos to the original author (&lt;a href=&#34;mailto:cyril.jaquier@fail2ban.org&#34;&gt;Cyril Jaquier&lt;/a&gt;) or &lt;em&gt;better&lt;/em&gt; to the &lt;a href=&#34;https://lists.sourceforge.net/lists/listinfo/fail2ban-users&#34;&gt;mailing list&lt;/a&gt; since Fail2Ban is &#34;community-driven&#34; for years now.&lt;/p&gt; &#xA;&lt;h2&gt;Thanks:&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/fail2ban/fail2ban/raw/master/THANKS&#34;&gt;THANKS&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;License:&lt;/h2&gt; &#xA;&lt;p&gt;Fail2Ban is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.&lt;/p&gt; &#xA;&lt;p&gt;Fail2Ban is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.&lt;/p&gt; &#xA;&lt;p&gt;You should have received a copy of the GNU General Public License along with Fail2Ban; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110, USA&lt;/p&gt;</summary>
  </entry>
</feed>