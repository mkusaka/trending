<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-07T01:31:38Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>judahpaul16/gpt-home</title>
    <updated>2024-05-07T01:31:38Z</updated>
    <id>tag:github.com,2024-05-07:/judahpaul16/gpt-home</id>
    <link href="https://github.com/judahpaul16/gpt-home" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatGPT at home! Basically a better Google Nest Hub or Amazon Alexa home assistant. Built on the Raspberry Pi using the OpenAI API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üè† GPT Home ü§ñüí¨&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Ubuntu_Server-v23.04-orange?style=flat-square&amp;amp;logo=ubuntu&#34; alt=&#34;Ubuntu Server Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Raspberry_Pi-4B-red?style=flat-square&amp;amp;logo=raspberry-pi&#34; alt=&#34;Raspberry Pi Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Python-v3.11-blue?style=flat-square&amp;amp;logo=python&#34; alt=&#34;Python Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Node.js-v18.17.1-green?style=flat-square&amp;amp;logo=node.js&#34; alt=&#34;Node.js Version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ChatGPT at home! Basically a better Google Nest Hub or Amazon Alexa home assistant. Built on the Raspberry Pi using the OpenAI API.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/judahpaul16/gpt-home/main/screenshots/my_build.jpg&#34; alt=&#34;My Build&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This guide will explain how to build your own. It&#39;s pretty straight forward. You can also use this as a reference for building other projects on the Raspberry Pi.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;This guide assumes you&#39;re using Ubuntu Server as your Raspberry Pi&#39;s operating system. You may need to make certain modifications to accommodate other operating systems. See Issue &lt;a href=&#34;https://github.com/judahpaul16/gpt-home/issues/12&#34;&gt;#12&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ö†Ô∏è Schematics / Wiring Diagram&lt;/h2&gt; &#xA;&lt;h3&gt;Caution: Battery Connection&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;: Before connecting the battery, ensure that the polarity is correct to avoid damage to your Raspberry Pi or other components. Disconnect power sources before making changes.&lt;/p&gt; &#xA;&lt;table style=&#34;border-collapse: collapse; border: 0;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;border: none;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/judahpaul16/gpt-home/main/screenshots/schematic_bb.png&#34; alt=&#34;Schematics Breadboard&#34; height=&#34;350px&#34;&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;border: none;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/judahpaul16/gpt-home/main/screenshots/schematic_schem.png&#34; alt=&#34;Schematics Schematic&#34; height=&#34;350px&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;span style=&#34;font-size: 1em; display:block;&#34;&gt;[click to enlarge]&lt;/span&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üõ† My Parts List&lt;/h2&gt; &#xA;&lt;h3&gt;Core Components&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Raspberry Pi 4B&lt;/strong&gt;: &lt;a href=&#34;https://a.co/d/aH6YCXY&#34;&gt;Link&lt;/a&gt; - $50-$70&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mini Speaker&lt;/strong&gt;: &lt;a href=&#34;https://a.co/d/9bN8LZ2&#34;&gt;Link&lt;/a&gt; - $18&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;128x32 OLED Display&lt;/strong&gt;: &lt;a href=&#34;https://a.co/d/4Scrfjq&#34;&gt;Link&lt;/a&gt; - $13-$14&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;128 GB MicroSD card&lt;/strong&gt;: &lt;a href=&#34;https://a.co/d/0SxSg7O&#34;&gt;Link&lt;/a&gt; - $13&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;USB 2.0 Mini Microphone&lt;/strong&gt;: &lt;a href=&#34;https://a.co/d/eIrQUXC&#34;&gt;Link&lt;/a&gt; - $8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üåü Optional Components&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Standoff Spacer Column M3x40mm&lt;/strong&gt;: &lt;a href=&#34;https://a.co/d/ees6oEA&#34;&gt;Link&lt;/a&gt; - $14&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;M1.4 M1.7 M2 M2.5 M3 Screw Kit&lt;/strong&gt;: &lt;a href=&#34;https://a.co/d/4XJwiBY&#34;&gt;Link&lt;/a&gt; - $15&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Raspberry Pi UPS Power Supply with Battery&lt;/strong&gt;: &lt;a href=&#34;https://a.co/d/1rMMCPR&#34;&gt;Link&lt;/a&gt; - $30&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cool Case for Raspberry Pi 4B&lt;/strong&gt;: &lt;a href=&#34;https://a.co/d/idSKJIG&#34;&gt;Link&lt;/a&gt; - $16&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üí≤ Total Price Range&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Core Components&lt;/strong&gt;: $102-$123&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optional Components&lt;/strong&gt;: $75&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Total (Without Optional)&lt;/strong&gt;: $102-$123&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Total (With Optional)&lt;/strong&gt;: $177-$198&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üì∂ Configuring Wi-Fi via wpa_supplicant&lt;/h2&gt; &#xA;&lt;p&gt;To configure Wi-Fi on your Raspberry Pi, you&#39;ll need to edit the &lt;code&gt;wpa_supplicant.conf&lt;/code&gt; file and ensure the wireless interface is enabled at boot.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;net-tools&lt;/code&gt; to get the &lt;code&gt;ifconfig&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install net-tools&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To enable the wireless interface (&lt;code&gt;wlan0&lt;/code&gt; in most cases) at boot, add the following command to &lt;code&gt;/etc/rc.local&lt;/code&gt; before the &lt;code&gt;exit 0&lt;/code&gt; line:&lt;br&gt; &lt;em&gt;Create the file if it doesn&#39;t exist&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo vim /etc/rc.local&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the following contents:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash&#xA;sudo ifconfig wlan0 up &amp;amp;&#xA;sudo wpa_supplicant -i wlan0 -c /etc/wpa_supplicant/wpa_supplicant.conf -B &amp;amp;&#xA;sudo dhclient wlan0 &amp;amp;&#xA;exit 0&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ensure the file has executable permissions and is enabled as a service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo chmod +x /etc/rc.local&#xA;sudo systemctl enable rc-local.service&#xA;sudo systemctl start rc-local.service&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the configuration file in a text editor:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo vim /etc/wpa_supplicant/wpa_supplicant.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add the following lines at the end of the file:&lt;br&gt; &lt;em&gt;You can define multiple &lt;code&gt;network&lt;/code&gt; blocks for multiple Wi-Fi networks&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;network={&#xA;    ssid=&#34;Your_Wi-Fi_Name&#34;&#xA;    psk=&#34;Your_Wi-Fi_Password&#34;&#xA;    key_mgmt=WPA-PSK&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace &lt;code&gt;Your_Wi-Fi_Name&lt;/code&gt; and &lt;code&gt;Your_Wi-Fi_Password&lt;/code&gt; with your actual Wi-Fi credentials.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;wpa_supplicant&lt;/code&gt; service starts at boot:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo systemctl enable wpa_supplicant.service&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start &lt;code&gt;wpa_supplicant&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo systemctl start wpa_supplicant.service&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Your Raspberry Pi should now connect to the Wi-Fi network automatically on boot. If you face issues, refer to the &lt;a href=&#34;https://www.raspberrypi.com/documentation/computers/configuration.html#setting-up-a-wireless-lan-via-the-command-line&#34;&gt;official Raspberry Pi documentation on wireless connectivity&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üõ† System Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Before running this project on your Raspberry Pi, you&#39;ll need to install some system-level dependencies in addition to the Python packages.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Synchoronize your system clock:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo timedatectl set-ntp on&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Update your package list:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure the Universe repository is enabled:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo add-apt-repository universe&#xA;sudo apt update&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Installing Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use the &lt;a href=&#34;https://raw.githubusercontent.com/judahpaul16/gpt-home/main/#-example-setup-script&#34;&gt;setup.sh&lt;/a&gt; script, you can skip this section. Otherwise, you can install the dependencies manually.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OpenAI API Key&lt;/strong&gt;: Required for OpenAI&#39;s GPT API.&lt;br&gt; Setup: Set up as an environment variable.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Python 3.x&lt;/strong&gt;: Required for running the Python code.&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y python3 python3-dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PortAudio&lt;/strong&gt;: Required for &lt;code&gt;pyttsx3&lt;/code&gt; (text-to-speech).&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y portaudio19-dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ALSA Utilities&lt;/strong&gt;: Required for audio configuration.&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y alsa-utils&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;JPEG Library&lt;/strong&gt;: Required for Pillow.&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y libjpeg-dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Build Essentials&lt;/strong&gt;: Required for building packages.&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y build-essential&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;vcgencmd&lt;/strong&gt;: Comes pre-installed on Raspberry Pi OS. Used for fetching CPU temperature.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Speech Recognition Libraries&lt;/strong&gt;: Required for &lt;code&gt;speech_recognition&lt;/code&gt;.&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y libasound2-dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I2C Support&lt;/strong&gt;: Required for &lt;code&gt;adafruit_ssd1306&lt;/code&gt; (OLED display).&lt;br&gt; Enable via &lt;code&gt;raspi-config&lt;/code&gt; or install packages:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install -y i2c-tools&#xA;sudo apt-get install -y python3-smbus&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;eSpeak Library&lt;/strong&gt;: Required for text-to-speech (&lt;code&gt;pyttsx3&lt;/code&gt;).&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y libespeak1&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;JACK Audio Connection Kit&lt;/strong&gt;: Required for handling audio.&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y jackd2&lt;/code&gt;&lt;br&gt; Select &lt;code&gt;Yes&lt;/code&gt; when prompted to enable realtime privileges.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FLAC Libraries&lt;/strong&gt;: Required for handling FLAC audio formats.&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y flac libflac12:armhf&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Git&lt;/strong&gt;: Required for cloning the repository.&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y git&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Node.js and npm&lt;/strong&gt;: Required for the web interface.&lt;br&gt; Installation: &lt;a href=&#34;https://github.com/nodesource/distributions#installation-instructions&#34;&gt;Follow NodeSource Installation Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;NGINX&lt;/strong&gt;: Required for reverse proxy for the web interface. Installation: &lt;code&gt;sudo apt-get install -y nginx&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rust and Cargo&lt;/strong&gt;: Required for installing &lt;code&gt;spotifyd&lt;/code&gt;.&lt;br&gt; Installation: &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;Follow Rust Installation Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Spotifyd&lt;/strong&gt;: Required for Spotify Connect.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Virtual Environment&lt;/strong&gt;: Recommended for Python package management.&lt;br&gt; Installation: &lt;code&gt;sudo apt-get install -y python3-venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üìú Example Setup script:&lt;/h2&gt; &#xA;&lt;p&gt;This script will install all the dependencies and completely set up the project for you. The first time you run it, it will take a while to install all the dependencies. After that, it will be much faster and you can just run it to reinstall the project if you make any changes to the code or want the latest version of the project.&lt;/p&gt; &#xA;&lt;p&gt;You will need to initialize an environment variable with your OpenAI API Key.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Note: Executing &lt;code&gt;export&lt;/code&gt; directly in the terminal does not persist after reboot.&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#34;your_openai_api_key_here&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you set up the variable in .bashrc file. (recommended)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Put this at the end of your &lt;code&gt;~/.bashrc&lt;/code&gt; file&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# export your OpenAI API Key in here to initialize it at boot&#xA;export OPENAI_API_KEY=&#34;your_openai_api_key_here&#34;&#xA;&#xA;# Optional: Add these aliases to your .bashrc file for easier management&#xA;alias gpt-start=&#34;sudo systemctl start gpt-home&#34;&#xA;alias gpt-restart=&#34;sudo systemctl restart gpt-home&#34;&#xA;alias gpt-stop=&#34;sudo systemctl stop gpt-home&#34;&#xA;alias gpt-disable=&#34;sudo systemctl disable gpt-home&#34;&#xA;alias gpt-status=&#34;sudo systemctl status gpt-home&#34;&#xA;alias gpt-enable=&#34;sudo systemctl enable gpt-home&#34;&#xA;alias gpt-log=&#34;tail -n 100 -f /home/$(whoami)/gpt-home/events.log&#34;&#xA;&#xA;alias web-start=&#34;sudo systemctl start gpt-web&#34;&#xA;alias web-restart=&#34;sudo systemctl restart gpt-web &amp;amp;&amp;amp; sudo systemctl restart nginx&#34;&#xA;alias web-stop=&#34;sudo systemctl stop gpt-web&#34;&#xA;alias web-disable=&#34;sudo systemctl disable gpt-web&#34;&#xA;alias web-status=&#34;sudo systemctl status gpt-web&#34;&#xA;alias web-enable=&#34;sudo systemctl enable gpt-web&#34;&#xA;alias web-log=&#34;tail -n 100 -f /var/log/nginx/access.log&#34;&#xA;alias web-error=&#34;tail -n 100 -f /var/log/nginx/error.log&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;setup.sh&lt;/h2&gt; &#xA;&lt;p&gt;Create a script outside the local repo folder with &lt;code&gt;vim setup.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash&#xA;&#xA;# Function to check and install a package if it&#39;s not installed&#xA;check_and_install() {&#xA;    package=$1&#xA;    install_cmd=$2&#xA;&#xA;    if ! dpkg -l | grep -q $package; then&#xA;        echo &#34;Installing $package...&#34;&#xA;        eval $install_cmd&#xA;    else&#xA;        echo &#34;$package is already installed.&#34;&#xA;    fi&#xA;}&#xA;&#xA;# Function to update the system time&#xA;update_system_time() {&#xA;    echo &#34;Updating system time...&#34;&#xA;    check_and_install &#34;ntpdate&#34; &#34;sudo apt-get install -y ntpdate&#34;&#xA;    sudo ntpdate -u ntp.ubuntu.com&#xA;}&#xA;&#xA;# Update system time&#xA;update_system_time&#xA;&#xA;# Update package list&#xA;yes | sudo add-apt-repository universe&#xA;sudo apt update&#xA;&#xA;# Set permissions&#xA;sudo chown -R $(whoami):$(whoami) $HOME&#xA;sudo chmod -R 755 $HOME&#xA;&#xA;# Check and install missing dependencies&#xA;check_and_install &#34;python3&#34; &#34;sudo apt-get install -y python3 python3-dev python3-venv&#34;&#xA;check_and_install &#34;portaudio19-dev&#34; &#34;sudo apt-get install -y portaudio19-dev&#34;&#xA;check_and_install &#34;alsa-utils&#34; &#34;sudo apt-get install -y alsa-utils&#34;&#xA;check_and_install &#34;libjpeg-dev&#34; &#34;sudo apt-get install -y libjpeg-dev&#34;&#xA;check_and_install &#34;build-essential&#34; &#34;sudo apt-get install -y build-essential&#34;&#xA;check_and_install &#34;libasound2-dev&#34; &#34;sudo apt-get install -y libasound2-dev&#34;&#xA;check_and_install &#34;i2c-tools&#34; &#34;sudo apt-get install -y i2c-tools&#34;&#xA;check_and_install &#34;python3-smbus&#34; &#34;sudo apt-get install -y python3-smbus&#34;&#xA;check_and_install &#34;libespeak1&#34; &#34;sudo apt-get install -y libespeak1&#34;&#xA;check_and_install &#34;jackd2&#34; &#34;sudo apt-get install -y jackd2&#34;&#xA;check_and_install &#34;flac&#34; &#34;sudo apt-get install -y flac&#34;&#xA;check_and_install &#34;libflac12:armhf&#34; &#34;sudo apt-get install -y libflac12:armhf&#34;&#xA;check_and_install &#34;cmake&#34; &#34;sudo apt-get install -y cmake&#34;&#xA;check_and_install &#34;openssl&#34; &#34;sudo apt-get install -y openssl&#34;&#xA;check_and_install &#34;git&#34; &#34;sudo apt-get install -y git&#34;&#xA;check_and_install &#34;nginx&#34; &#34;sudo apt-get install -y nginx&#34;&#xA;check_and_install &#34;expect&#34; &#34;sudo apt-get install -y expect&#34;&#xA;check_and_install &#34;avahi-daemon&#34; &#34;sudo apt-get install -y avahi-daemon avahi-utils&#34;&#xA;check_and_install &#34;nodejs&#34; &#34;curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash - &amp;amp;&amp;amp; sudo apt-get install -y nodejs&#34;&#xA;&#xA;# Install cargo and rust&#xA;if ! command -v cargo &amp;amp;&amp;gt; /dev/null; then&#xA;    echo &#34;Installing cargo and rust...&#34;&#xA;    curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y&#xA;&#xA;    # Source the environment for cargo and rust&#xA;    if [ -f &#34;$HOME/.cargo/env&#34; ]; then&#xA;        source $HOME/.cargo/env&#xA;    else&#xA;        echo &#34;Error: Unable to source Rust environment. Installation may have failed or path is incorrect.&#34;&#xA;    fi&#xA;else&#xA;    echo &#34;cargo is already installed.&#34;&#xA;fi&#xA;&#xA;# Ensure directory exists for the configuration&#xA;mkdir -p $HOME/.config/spotifyd&#xA;&#xA;# Install spotifyd using Rust&#39;s Cargo&#xA;if ! command -v spotifyd &amp;amp;&amp;gt; /dev/null; then&#xA;    echo &#34;Installing spotifyd...&#34;&#xA;    cargo install spotifyd&#xA;    sudo mv $HOME/.cargo/bin/spotifyd /usr/local/bin/&#xA;else&#xA;    echo &#34;spotifyd is already installed.&#34;&#xA;fi&#xA;&#xA;# Create Spotifyd configuration (this is just a basic config; adjust accordingly)&#xA;cat &amp;lt;&amp;lt;EOF &amp;gt; $HOME/.config/spotifyd/spotifyd.conf&#xA;[global]&#xA;backend = &#34;alsa&#34; # Or pulseaudio if you use it&#xA;device_name = &#34;GPT Home&#34; # Name your device shows in Spotify Connect&#xA;bitrate = 320 # Choose bitrate from 96/160/320 kbps&#xA;cache_path = &#34;/home/$(whoami)/.spotifyd/cache&#34;&#xA;discovery = false&#xA;EOF&#xA;&#xA;# Function to setup a systemd service&#xA;setup_service() {&#xA;    # Parameters&#xA;    local SERVICE_NAME=$1&#xA;    local EXEC_START=$2&#xA;    local DEPENDS=$3&#xA;    local ENV=$4&#xA;    local HOSTNAME=$5&#xA;    local TYPE=$6&#xA;    local LMEMLOCK=$7&#xA;    local RESTART=$8&#xA;&#xA;    # Stop the service if it&#39;s already running&#xA;    sudo systemctl stop &#34;$SERVICE_NAME&#34; &amp;amp;&amp;gt;/dev/null&#xA;&#xA;    echo &#34;Creating and enabling $SERVICE_NAME...&#34;&#xA;    # Create systemd service file&#xA;    cat &amp;lt;&amp;lt;EOF | sudo tee &#34;/etc/systemd/system/$SERVICE_NAME&#34; &amp;gt;/dev/null&#xA;[Unit]&#xA;Description=$SERVICE_NAME&#xA;$DEPENDS&#xA;StartLimitIntervalSec=10&#xA;StartLimitBurst=10&#xA;&#xA;[Service]&#xA;User=$(whoami)&#xA;WorkingDirectory=/home/$(whoami)/gpt-home&#xA;$EXEC_START&#xA;$ENV&#xA;$HOSTNAME&#xA;$RESTART&#xA;$TYPE&#xA;$LMEMLOCK&#xA;&#xA;[Install]&#xA;WantedBy=multi-user.target&#xA;EOF&#xA;&#xA;    # Reload systemd to recognize the new service, then enable and restart it&#xA;    sudo systemctl daemon-reload&#xA;    sudo systemctl enable &#34;$SERVICE_NAME&#34;&#xA;    sudo systemctl restart &#34;$SERVICE_NAME&#34;&#xA;&#xA;    # Wait for 5 seconds and then show the service status&#xA;    echo &#34;&#34;&#xA;    sleep 5&#xA;    sudo systemctl status &#34;$SERVICE_NAME&#34; --no-pager&#xA;    echo &#34;&#34;&#xA;}&#xA;&#xA;# Setup UFW Firewall&#xA;sudo ufw allow ssh&#xA;sudo ufw allow 80,443/tcp&#xA;sudo ufw allow 5353/udp&#xA;echo &#34;y&#34; | sudo ufw enable&#xA;&#xA;# Setup NGINX for reverse proxy&#xA;echo &#34;Setting up NGINX...&#34;&#xA;sudo tee /etc/nginx/sites-available/gpt-home &amp;lt;&amp;lt;EOF&#xA;server {&#xA;    listen 80;&#xA;&#xA;    location / {&#xA;        proxy_pass http://localhost:8000/;&#xA;        proxy_set_header Host \$host;&#xA;        proxy_set_header X-Real-IP \$remote_addr;&#xA;        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;&#xA;    }&#xA;}&#xA;EOF&#xA;&#xA;# Remove existing symlink if it exists&#xA;[ -L &#34;/etc/nginx/sites-enabled/gpt-home&#34; ] &amp;amp;&amp;amp; sudo unlink /etc/nginx/sites-enabled/gpt-home&#xA;&#xA;# Symlink the site configuration&#xA;sudo ln -s /etc/nginx/sites-available/gpt-home /etc/nginx/sites-enabled&#xA;&#xA;# Test the NGINX configuration&#xA;sudo nginx -t&#xA;&#xA;# Remove the default site if it exists&#xA;[ -L &#34;/etc/nginx/sites-enabled/default&#34; ] &amp;amp;&amp;amp; sudo unlink /etc/nginx/sites-enabled/default&#xA;&#xA;# Reload NGINX to apply changes&#xA;sudo systemctl reload nginx&#xA;&#xA;# Remove existing local repo if it exists&#xA;[ -d &#34;gpt-home&#34; ] &amp;amp;&amp;amp; rm -rf gpt-home&#xA;&#xA;# Clone gpt-home repo and navigate into its directory&#xA;git clone https://github.com/judahpaul16/gpt-home.git&#xA;cd gpt-home&#xA;&#xA;## Setup main app&#xA;# Create and activate a virtual environment, then install dependencies&#xA;python3 -m venv env&#xA;source env/bin/activate&#xA;pip install --upgrade pip setuptools&#xA;pip install --use-pep517 -r requirements.txt&#xA;&#xA;## Setup Web Interface&#xA;# Navigate to gpt-web and install dependencies&#xA;cd gpt-web&#xA;npm install&#xA;&#xA;# Configure Avahi for gpt-home.local&#xA;sudo sed -i &#39;s/#host-name=.*$/host-name=gpt-home/g&#39; /etc/avahi/avahi-daemon.conf&#xA;sudo systemctl restart avahi-daemon&#xA;&#xA;# Build the React App&#xA;npm run build&#xA;&#xA;## Setup Services&#xA;# Setup spotifyd service&#xA;setup_service \&#xA;    &#34;spotifyd.service&#34; \&#xA;    &#34;ExecStart=/usr/local/bin/spotifyd --no-daemon&#34; \&#xA;    &#34;Wants=sound.target&#xA;    After=sound.target&#xA;    Wants=network-online.target&#xA;    After=network-online.target&#34; \&#xA;    &#34;&#34; \&#xA;    &#34;&#34; \&#xA;    &#34;&#34; \&#xA;    &#34;&#34; \&#xA;    &#34;Restart=always&#xA;    RestartSec=12&#34;&#xA;&#xA;# Setup gpt-home service&#xA;setup_service \&#xA;    &#34;gpt-home.service&#34; \&#xA;    &#34;ExecStart=/bin/bash -c &#39;source /home/$(whoami)/gpt-home/env/bin/activate &amp;amp;&amp;amp; python /home/$(whoami)/gpt-home/app.py&#39;&#34; \&#xA;    &#34;&#34; \&#xA;    &#34;Environment=\&#34;OPENAI_API_KEY=$OPENAI_API_KEY\&#34;&#34; \&#xA;    &#34;Environment=\&#34;HOSTNAME=$HOSTNAME\&#34;&#34; \&#xA;    &#34;Type=simple&#34; \&#xA;    &#34;LimitMEMLOCK=infinity&#34; \&#xA;    &#34;Restart=always&#34;&#xA;&#xA;# Setup FastAPI service for web interface backend&#xA;setup_service \&#xA;    &#34;gpt-web.service&#34; \&#xA;    &#34;ExecStart=/bin/bash -c &#39;source /home/$(whoami)/gpt-home/env/bin/activate &amp;amp;&amp;amp; uvicorn gpt-web.backend:app --host 0.0.0.0 --port 8000&#39;&#34; \&#xA;    &#34;&#34; \&#xA;    &#34;Environment=\&#34;OPENAI_API_KEY=$OPENAI_API_KEY\&#34;&#34; \&#xA;    &#34;Environment=\&#34;HOSTNAME=$HOSTNAME\&#34;&#34; \&#xA;    &#34;Type=simple&#34; \&#xA;    &#34;&#34; \&#xA;    &#34;Restart=always&#34;&#xA;&#xA;# Mask systemd-networkd-wait-online.service to prevent boot delays&#xA;sudo systemctl mask systemd-networkd-wait-online.service&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Be sure to make the script executable to run it&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x setup.sh&#xA;./setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üìö Useful Documentation&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://beta.openai.com/docs/introduction&#34;&gt;OpenAI API Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.raspberrypi.com/documentation&#34;&gt;Raspberry Pi Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://nodejs.org/en/docs/&#34;&gt;Node.js Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://docs.npmjs.com/&#34;&gt;npm Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://nginx.org/en/docs/&#34;&gt;NGINX Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://reactjs.org/docs/getting-started.html&#34;&gt;React Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://ubuntu.com/server/docs&#34;&gt;Ubuntu Server Docs&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://docs.python.org/3/&#34;&gt;Python3 Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.raspberrypi.com/documentation/computers/images/GPIO-Pinout-Diagram-2.png&#34;&gt;GPIO Pinout&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://circuitpython.readthedocs.io/projects/ssd1306/en/latest/&#34;&gt;Adafruit SSD1306 Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pyttsx3/&#34;&gt;pyttsx3 Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://i2c.readthedocs.io/en/latest/&#34;&gt;I2C Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.alsa-project.org/wiki/Documentation&#34;&gt;ALSA Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;http://www.portaudio.com/docs/v19-doxydocs/index.html&#34;&gt;PortAudio Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/SpeechRecognition/&#34;&gt;SpeechRecognition Docs&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://developer.spotify.com/documentation/web-api/&#34;&gt;Spotify API Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://spotipy.readthedocs.io/&#34;&gt;Spotify API Python Docs (Spotipy)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/Spotifyd/spotifyd&#34;&gt;Spotifyd Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://developers.meethue.com/develop/get-started-2/&#34;&gt;Phillips Hue API Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/studioimaginaire/phue&#34;&gt;Phillips Hue Python API Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://openweathermap.org/api/one-call-3&#34;&gt;OpenWeatherMap API Docs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://fritzing.org/&#34;&gt;Fritzing Schematics&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are certainly welcome! Please read the &lt;a href=&#34;https://raw.githubusercontent.com/judahpaul16/gpt-home/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for more information on how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;üìú License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the GNU GPL v3.0 License - see the &lt;a href=&#34;https://raw.githubusercontent.com/judahpaul16/gpt-home/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Blealtan/efficient-kan</title>
    <updated>2024-05-07T01:31:38Z</updated>
    <id>tag:github.com,2024-05-07:/Blealtan/efficient-kan</id>
    <link href="https://github.com/Blealtan/efficient-kan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;An Efficient Implementation of Kolmogorov-Arnold Network&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains an efficient implementation of Kolmogorov-Arnold Network (KAN). The original implementation of KAN is available &lt;a href=&#34;https://github.com/KindXiaoming/pykan&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The performance issue of the original implementation is mostly because it needs to expand all intermediate variables to perform the different activation functions. For a layer with &lt;code&gt;in_features&lt;/code&gt; input and &lt;code&gt;out_features&lt;/code&gt; output, the original implementation needs to expand the input to a tensor with shape &lt;code&gt;(batch_size, out_features, in_features)&lt;/code&gt; to perform the activation functions. However, all activation functions are linear combination of a fixed set of basis functions which are B-splines; given that, we can reformulate the computation as activate the input with different basis functions and then combine them linearly. This reformulation can significantly reduce the memory cost and make the computation a straightforward matrix multiplication, and works with both forward and backward pass naturally.&lt;/p&gt; &#xA;&lt;p&gt;The problem is in the &lt;strong&gt;sparsification&lt;/strong&gt; which is claimed to be critical to KAN&#39;s interpretability. The authors proposed a L1 regularization defined on the input samples, which requires non-linear operations on the &lt;code&gt;(batch_size, out_features, in_features)&lt;/code&gt; tensor, and is thus not compatible with the reformulation. I instead replace the L1 regularization with a L1 regularization on the weights, which is more common in neural networks and is compatible with the reformulation. The author&#39;s implementation indeed include this kind of regularization alongside the one described in the paper as well, so I think it might help. More experiments are needed to verify this; but at least the original approach is infeasible if efficiency is wanted.&lt;/p&gt; &#xA;&lt;p&gt;Another difference is that, beside the learnable activation functions (B-splines), the original implementation also includes a learnable scale on each activation function. I provided an option &lt;code&gt;enable_standalone_scale_spline&lt;/code&gt; that defaults to &lt;code&gt;True&lt;/code&gt; to include this feature; disable it will make the model more efficient, but potentially hurts results. It needs more experiments.&lt;/p&gt; &#xA;&lt;p&gt;2024-05-04 Update: @xiaol hinted that the constant initialization of &lt;code&gt;base_weight&lt;/code&gt; parameters can be a problem on MNIST. For now I&#39;ve changed both the &lt;code&gt;base_weight&lt;/code&gt; and &lt;code&gt;spline_scaler&lt;/code&gt; matrices to be initialized with &lt;code&gt;kaiming_uniform_&lt;/code&gt;, following &lt;code&gt;nn.Linear&lt;/code&gt;&#39;s initialization. It seems to work much much better on MNIST (~20% to ~97%), but I&#39;m not sure if it&#39;s a good idea in general.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Efficient-Large-Model/VILA</title>
    <updated>2024-05-07T01:31:38Z</updated>
    <id>tag:github.com,2024-05-07:/Efficient-Large-Model/VILA</id>
    <link href="https://github.com/Efficient-Large-Model/VILA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;VILA - a multi-image visual language model with training, inference and evaluation recipe, deployable from cloud to edge (Jetson Orin and laptops)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/demo_images/vila-logo.jpg&#34; width=&#34;20%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;VILA: On Pre-training for Visual Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/CODE_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/MODEL_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/MODEL%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Model License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.10+&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.07533&#34;&gt;VILA arxiv&lt;/a&gt; / &lt;a href=&#34;https://vila-demo.hanlab.ai/&#34;&gt;VILA Demo&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/collections/Efficient-Large-Model/vila-on-pre-training-for-visual-language-models-65d8022a3a52cd9bcd62698e&#34;&gt;VILA Huggingface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üí° Introduction&lt;/h2&gt; &#xA;&lt;p&gt;VILA is a visual language model (VLM) pretrained with interleaved image-text data at scale, enabling &lt;strong&gt;video understanding&lt;/strong&gt; and &lt;strong&gt;multi-image understanding&lt;/strong&gt; capabilities. VILA is deployable on the edge by &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt; 4bit quantization and &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; framework. We find: (1) image-text pairs are not enough, interleaved image-text is essential; (2) unfreezing LLM during interleaved image-text pre-training enables in-context learning; (3)re-blending text-only instruction data is crucial to boost both VLM and text-only performance; (4) token compression extends #video frames. VILA unveils appealing capabilities, including: video reasoning, in-context learning, visual chain-of-thought, and better world knowledge.&lt;/p&gt; &#xA;&lt;h2&gt;üí° News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/05] We release VILA-1.5, which offers &lt;strong&gt;video understanding capability&lt;/strong&gt;. VILA-1.5 comes with four model sizes: 3B/8B/13B/40B.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] We release &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt;-quantized 4bit VILA-1.5 models. VILA-1.5 is efficiently deployable on diverse NVIDIA GPUs (A100, 4090, 4070 Laptop, Orin, Orin Nano) by &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/demo_trt_llm&#34;&gt;TensorRT-LLM&lt;/a&gt; backends.&lt;/li&gt; &#xA; &lt;li&gt;[2024/03] VILA has been accepted by CVPR 2024!&lt;/li&gt; &#xA; &lt;li&gt;[2024/02] We release &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt;-quantized 4bit VILA models, deployable on Jetson Orin and laptops through &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; and &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine&#34;&gt;TinyChatEngine&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/02] VILA is released. We propose interleaved image-text pretraining that enables &lt;strong&gt;multi-image&lt;/strong&gt; VLM. VILA comes with impressive in-context learning capabilities. We open source everything: including training code, evaluation code, datasets, model ckpts.&lt;/li&gt; &#xA; &lt;li&gt;[2023/12] &lt;a href=&#34;https://arxiv.org/abs/2312.07533&#34;&gt;Paper&lt;/a&gt; is on Arxiv!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;h3&gt;Image QA Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Prec.&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;VizWiz&lt;/th&gt; &#xA;   &lt;th&gt;SQA-I&lt;/th&gt; &#xA;   &lt;th&gt;VQA-T&lt;/th&gt; &#xA;   &lt;th&gt;POPE&lt;/th&gt; &#xA;   &lt;th&gt;MME&lt;/th&gt; &#xA;   &lt;th&gt;MMB&lt;/th&gt; &#xA;   &lt;th&gt;MMB-CN&lt;/th&gt; &#xA;   &lt;th&gt;SEED&lt;/th&gt; &#xA;   &lt;th&gt;SEED-I&lt;/th&gt; &#xA;   &lt;th&gt;MMMU (val)&lt;/th&gt; &#xA;   &lt;th&gt;MMMU (test)&lt;/th&gt; &#xA;   &lt;th&gt;llava-bench&lt;/th&gt; &#xA;   &lt;th&gt;MM-Vet&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;80.4&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;53.5&lt;/td&gt; &#xA;   &lt;td&gt;69.0&lt;/td&gt; &#xA;   &lt;td&gt;60.4&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;1442.44&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;52.7&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;   &lt;td&gt;67.9&lt;/td&gt; &#xA;   &lt;td&gt;33.3&lt;/td&gt; &#xA;   &lt;td&gt;30.8&lt;/td&gt; &#xA;   &lt;td&gt;75.9&lt;/td&gt; &#xA;   &lt;td&gt;35.4&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;80.0&lt;/td&gt; &#xA;   &lt;td&gt;61.1&lt;/td&gt; &#xA;   &lt;td&gt;53.8&lt;/td&gt; &#xA;   &lt;td&gt;67.8&lt;/td&gt; &#xA;   &lt;td&gt;60.4&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;1437.34&lt;/td&gt; &#xA;   &lt;td&gt;63.3&lt;/td&gt; &#xA;   &lt;td&gt;51.4&lt;/td&gt; &#xA;   &lt;td&gt;59.8&lt;/td&gt; &#xA;   &lt;td&gt;66.6&lt;/td&gt; &#xA;   &lt;td&gt;32.7&lt;/td&gt; &#xA;   &lt;td&gt;31.1&lt;/td&gt; &#xA;   &lt;td&gt;75.0&lt;/td&gt; &#xA;   &lt;td&gt;37.3&lt;/td&gt; &#xA;   &lt;td&gt;59.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;79.8&lt;/td&gt; &#xA;   &lt;td&gt;61.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;69.6&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;85.3&lt;/td&gt; &#xA;   &lt;td&gt;1431.65&lt;/td&gt; &#xA;   &lt;td&gt;62.8&lt;/td&gt; &#xA;   &lt;td&gt;52.2&lt;/td&gt; &#xA;   &lt;td&gt;60.0&lt;/td&gt; &#xA;   &lt;td&gt;66.4&lt;/td&gt; &#xA;   &lt;td&gt;32.8&lt;/td&gt; &#xA;   &lt;td&gt;31.3&lt;/td&gt; &#xA;   &lt;td&gt;76.7&lt;/td&gt; &#xA;   &lt;td&gt;38.6&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;79.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;62.3&lt;/td&gt; &#xA;   &lt;td&gt;69.2&lt;/td&gt; &#xA;   &lt;td&gt;63.0&lt;/td&gt; &#xA;   &lt;td&gt;85.8&lt;/td&gt; &#xA;   &lt;td&gt;1417.06&lt;/td&gt; &#xA;   &lt;td&gt;61.6&lt;/td&gt; &#xA;   &lt;td&gt;51.5&lt;/td&gt; &#xA;   &lt;td&gt;59.1&lt;/td&gt; &#xA;   &lt;td&gt;65.7&lt;/td&gt; &#xA;   &lt;td&gt;33.4&lt;/td&gt; &#xA;   &lt;td&gt;30.4&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;36.7&lt;/td&gt; &#xA;   &lt;td&gt;60.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;80.9&lt;/td&gt; &#xA;   &lt;td&gt;61.9&lt;/td&gt; &#xA;   &lt;td&gt;58.7&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;84.4&lt;/td&gt; &#xA;   &lt;td&gt;1577.01&lt;/td&gt; &#xA;   &lt;td&gt;72.3&lt;/td&gt; &#xA;   &lt;td&gt;66.2&lt;/td&gt; &#xA;   &lt;td&gt;64.2&lt;/td&gt; &#xA;   &lt;td&gt;71.4&lt;/td&gt; &#xA;   &lt;td&gt;36.9&lt;/td&gt; &#xA;   &lt;td&gt;36.0&lt;/td&gt; &#xA;   &lt;td&gt;80.0&lt;/td&gt; &#xA;   &lt;td&gt;38.3&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;80.3&lt;/td&gt; &#xA;   &lt;td&gt;61.7&lt;/td&gt; &#xA;   &lt;td&gt;59.3&lt;/td&gt; &#xA;   &lt;td&gt;79.0&lt;/td&gt; &#xA;   &lt;td&gt;65.4&lt;/td&gt; &#xA;   &lt;td&gt;82.9&lt;/td&gt; &#xA;   &lt;td&gt;1593.65&lt;/td&gt; &#xA;   &lt;td&gt;71.0&lt;/td&gt; &#xA;   &lt;td&gt;64.9&lt;/td&gt; &#xA;   &lt;td&gt;64.0&lt;/td&gt; &#xA;   &lt;td&gt;71.1&lt;/td&gt; &#xA;   &lt;td&gt;36.0&lt;/td&gt; &#xA;   &lt;td&gt;36.1&lt;/td&gt; &#xA;   &lt;td&gt;79.0&lt;/td&gt; &#xA;   &lt;td&gt;37.2&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;82.8&lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;65.0&lt;/td&gt; &#xA;   &lt;td&gt;86.3&lt;/td&gt; &#xA;   &lt;td&gt;1569.55&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;   &lt;td&gt;33.6&lt;/td&gt; &#xA;   &lt;td&gt;80.8&lt;/td&gt; &#xA;   &lt;td&gt;44.3&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;82.7&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;   &lt;td&gt;63.3&lt;/td&gt; &#xA;   &lt;td&gt;79.7&lt;/td&gt; &#xA;   &lt;td&gt;64.7&lt;/td&gt; &#xA;   &lt;td&gt;86.7&lt;/td&gt; &#xA;   &lt;td&gt;1531.35&lt;/td&gt; &#xA;   &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;td&gt;66.7&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;37.8&lt;/td&gt; &#xA;   &lt;td&gt;34.0&lt;/td&gt; &#xA;   &lt;td&gt;81.9&lt;/td&gt; &#xA;   &lt;td&gt;46.4&lt;/td&gt; &#xA;   &lt;td&gt;66.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;84.3&lt;/td&gt; &#xA;   &lt;td&gt;64.6&lt;/td&gt; &#xA;   &lt;td&gt;62.2&lt;/td&gt; &#xA;   &lt;td&gt;87.2&lt;/td&gt; &#xA;   &lt;td&gt;73.6&lt;/td&gt; &#xA;   &lt;td&gt;87.3&lt;/td&gt; &#xA;   &lt;td&gt;1726.82&lt;/td&gt; &#xA;   &lt;td&gt;82.4&lt;/td&gt; &#xA;   &lt;td&gt;80.2&lt;/td&gt; &#xA;   &lt;td&gt;69.1&lt;/td&gt; &#xA;   &lt;td&gt;75.8&lt;/td&gt; &#xA;   &lt;td&gt;51.9&lt;/td&gt; &#xA;   &lt;td&gt;46.9&lt;/td&gt; &#xA;   &lt;td&gt;81.3&lt;/td&gt; &#xA;   &lt;td&gt;53.0&lt;/td&gt; &#xA;   &lt;td&gt;72.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;84.1&lt;/td&gt; &#xA;   &lt;td&gt;64.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;86.7&lt;/td&gt; &#xA;   &lt;td&gt;73.2&lt;/td&gt; &#xA;   &lt;td&gt;88.2&lt;/td&gt; &#xA;   &lt;td&gt;1714.79&lt;/td&gt; &#xA;   &lt;td&gt;83.2&lt;/td&gt; &#xA;   &lt;td&gt;79.6&lt;/td&gt; &#xA;   &lt;td&gt;68.9&lt;/td&gt; &#xA;   &lt;td&gt;75.6&lt;/td&gt; &#xA;   &lt;td&gt;49.3&lt;/td&gt; &#xA;   &lt;td&gt;46.2&lt;/td&gt; &#xA;   &lt;td&gt;83.0&lt;/td&gt; &#xA;   &lt;td&gt;51.4&lt;/td&gt; &#xA;   &lt;td&gt;72.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: VQAV2 and VizWiz are test-dev, the average accuracy is calculated over all datasets and MME numbers are divided by 20.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Video QA Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Prec.&lt;/th&gt; &#xA;   &lt;th&gt;Perception Test&lt;/th&gt; &#xA;   &lt;th&gt;ActivityNet&lt;/th&gt; &#xA;   &lt;th&gt;MSVD&lt;/th&gt; &#xA;   &lt;th&gt;MSRVTT&lt;/th&gt; &#xA;   &lt;th&gt;TGIF&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;39.3&lt;/td&gt; &#xA;   &lt;td&gt;50.2&lt;/td&gt; &#xA;   &lt;td&gt;76.6&lt;/td&gt; &#xA;   &lt;td&gt;57.5&lt;/td&gt; &#xA;   &lt;td&gt;51.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;39&lt;/td&gt; &#xA;   &lt;td&gt;50.7&lt;/td&gt; &#xA;   &lt;td&gt;76.9&lt;/td&gt; &#xA;   &lt;td&gt;57.6&lt;/td&gt; &#xA;   &lt;td&gt;51.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;41.8&lt;/td&gt; &#xA;   &lt;td&gt;54.3&lt;/td&gt; &#xA;   &lt;td&gt;78.3&lt;/td&gt; &#xA;   &lt;td&gt;60.1&lt;/td&gt; &#xA;   &lt;td&gt;54.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;39.3&lt;/td&gt; &#xA;   &lt;td&gt;54.7&lt;/td&gt; &#xA;   &lt;td&gt;77.9&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;   &lt;td&gt;56&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;41.7&lt;/td&gt; &#xA;   &lt;td&gt;58&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;63&lt;/td&gt; &#xA;   &lt;td&gt;58.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference speed ( Token/sec )&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;A100&lt;/th&gt; &#xA;   &lt;th&gt;4090&lt;/th&gt; &#xA;   &lt;th&gt;Orin&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;104.6&lt;/td&gt; &#xA;   &lt;td&gt;137.6&lt;/td&gt; &#xA;   &lt;td&gt;25.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;182.8&lt;/td&gt; &#xA;   &lt;td&gt;215.5&lt;/td&gt; &#xA;   &lt;td&gt;42.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;104.3&lt;/td&gt; &#xA;   &lt;td&gt;137.2&lt;/td&gt; &#xA;   &lt;td&gt;24.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;180.2&lt;/td&gt; &#xA;   &lt;td&gt;219.3&lt;/td&gt; &#xA;   &lt;td&gt;40.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;57.4&lt;/td&gt; &#xA;   &lt;td&gt;10.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;168.9&lt;/td&gt; &#xA;   &lt;td&gt;150.2&lt;/td&gt; &#xA;   &lt;td&gt;28.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;50.9&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;6.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;115.9&lt;/td&gt; &#xA;   &lt;td&gt;105.7&lt;/td&gt; &#xA;   &lt;td&gt;20.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;57.0&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: Measured using the &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tinychat&#34;&gt;TinyChat&lt;/a&gt; backend at batch size = 1.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h2&gt;VILA Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Video captioning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/156256291/c9520943-2478-4f97-bc95-121d625018a6&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/156256291/c9520943-2478-4f97-bc95-121d625018a6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: Elaborate on the visual and narrative elements of the video in detail.&lt;/p&gt; &#xA;&lt;p&gt;Caption: The video shows a person&#39;s hands working on a white surface. They are folding a piece of fabric with a checkered pattern in shades of blue and white. The fabric is being folded into a smaller, more compact shape. The person&#39;s fingernails are painted red, and they are wearing a black and red garment. There are also a ruler and a pencil on the surface, suggesting that measurements and precision are involved in the process.&lt;/p&gt; &#xA;&lt;h3&gt;In context learning&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/demo_images/demo_img_1.png&#34; height=&#34;239&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/demo_images/demo_img_2.png&#34; height=&#34;250&#34;&gt; &#xA;&lt;h3&gt;Multi-image reasoning&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/demo_images/demo_img_3.png&#34; height=&#34;193&#34;&gt; &#xA;&lt;h3&gt;VILA on Jetson Orin&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/7783214/6079374c-0787-4bc4-b9c6-e1524b4c9dc4&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/7783214/6079374c-0787-4bc4-b9c6-e1524b4c9dc4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;VILA on RTX 4090&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/7783214/80c47742-e873-4080-ad7d-d17c4700539f&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/7783214/80c47742-e873-4080-ad7d-d17c4700539f&lt;/a&gt;&lt;/p&gt;  &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./environment_setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or follow the instructions below in order.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n vila python=3.10 -y&#xA;conda activate vila&#xA;&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;# this is optional if you prefer to system built-in nvcc.&#xA;conda install -c nvidia cuda-toolkit -y&#xA;wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.4.2/flash_attn-2.4.2+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl&#xA;pip install flash_attn-2.4.2+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl&#xA;pip install -e .&#xA;pip install -e &#34;.[train]&#34;&#xA;&#xA;pip install git+https://github.com/huggingface/transformers@v4.36.2&#xA;site_pkg_path=$(python -c &#39;import site; print(site.getsitepackages()[0])&#39;)&#xA;cp -rv ./llava/train/transformers_replace/* $site_pkg_path/transformers/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;VILA training contains three steps, for specific hyperparameters, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/scripts/v1_5&#34;&gt;scripts/v1_5&lt;/a&gt; folder:&lt;/p&gt; &#xA;&lt;h3&gt;Step-1: Alignment&lt;/h3&gt; &#xA;&lt;p&gt;We utilize LLaVA-CC3M-Pretrain-595K dataset to align the textual and visual modalities.&lt;/p&gt; &#xA;&lt;p&gt;The stage 1 script takes in two parameters and it can run on a single 8xA100 node. &lt;code&gt;BASE_MODEL_PATH&lt;/code&gt; points to a online or local huggingface repository, such as &lt;code&gt;NousResearch/Llama-2-7b-hf&lt;/code&gt;. &lt;code&gt;OUTPUT_NAME&lt;/code&gt; points to a target directory under &lt;code&gt;checkpoints&lt;/code&gt;, which will save the trained multimodal projector afterwards.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/1_mm_align.sh [BASE_MODEL_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step-2: Pretraining&lt;/h3&gt; &#xA;&lt;p&gt;We use MMC4 and Coyo dataset to train VLM with interleaved image-text pairs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/2_pretrain_mmc4_coyo.sh [CODE_PATH] [BASE_MODEL_PATH] [STAGE1_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The stage 2 script takes in four arguments. &lt;code&gt;CODE_PATH&lt;/code&gt; is the absolute path to our VILA codebase, &lt;code&gt;BASE_MODEL_PATH&lt;/code&gt; has similar meaning to what is presented in the stage 1 script. &lt;code&gt;STAGE1_PATH&lt;/code&gt; points to the &lt;code&gt;OUTPUT_NAME&lt;/code&gt; of stage 1 (i.e. where the stage 1 checkpoint is stored). &lt;code&gt;OUTPUT_NAME&lt;/code&gt; is the desired folder name under &lt;code&gt;checkpoints&lt;/code&gt; that saves the pretraining checkpoint. The script we provided for this stage is executed on slurm, and we expect it to execute on 16 nodes (128 GPUs).&lt;/p&gt; &#xA;&lt;h3&gt;Step-3: Supervised fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;This is the last stage of VILA training, in which we tune the model to follow multimodal instructions on a subset of M3IT, FLAN and ShareGPT4V. This stage runs on a 8xA100 node.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/3_sft.sh [STAGE2_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The stage 3 script takes in two arguments. &lt;code&gt;STAGE2_PATH&lt;/code&gt; points to the &lt;code&gt;OUTPUT_NAME&lt;/code&gt; of the stage 2 script (i.e. where the stage 2 checkpoint is stored). &lt;code&gt;OUTPUT_NAME&lt;/code&gt; is the desired folder name under &lt;code&gt;checkpoints&lt;/code&gt; that stores the final checkpoint.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations&lt;/h2&gt; &#xA;&lt;h3&gt;Image Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;You can follow &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;Llava1.5 eval&lt;/a&gt; to download all datasets. After downloading all datasets, please put them under &lt;code&gt;playground/data/eval&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please make the following changes to the MME evaluation script. Please search for:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_path=&#39;MME_Benchmark_release_version&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and replace it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_path=os.path.join(script_dir, &#39;MME_Benchmark_release_version&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide a push-the-button script to perform evaluation on all 10 datasets that do not require GPT-assisted evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/v1_5/eval/eval_all.sh [CHECKPOINT_PATH] [MODEL_NAME] [CONV_MODE]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script takes in two parameters, &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; points to the stage 3 model checkpoint, and &lt;code&gt;MODEL_NAME&lt;/code&gt; will be the name of evaluation results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/830/my-submission&#34;&gt;VQAv2&lt;/a&gt; and &lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/2185/my-submission&#34;&gt;Vizwiz&lt;/a&gt; evaluations are hosted on eval.ai. You need to register an account and create a team to be able to submit eval.&lt;/p&gt; &#xA;&lt;p&gt;MMBench and MMBench_CN eval are hosted on another &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;evaluation server&lt;/a&gt;. Make sure you change the name of the file before submitting, otherwise the server caches results and will always return wrong result to you.&lt;/p&gt; &#xA;&lt;p&gt;We provide a quick script to automatically organize the prediction files that need to be submitted to servers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/v1_5/eval/copy_predictions.py [MODEL_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will be able to find the predictions under &lt;code&gt;playground/data/predictions_upload/[MODEL_NAME]&lt;/code&gt; after executing this script.&lt;/p&gt; &#xA;&lt;h3&gt;Video Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the evaluation steps in &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA/raw/main/TRAIN_AND_VALIDATE.md#data-for-validating&#34;&gt;Video-LLaVA&lt;/a&gt; for dataset preparation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/v1_5/eval/video_chatgpt/run_all.sh [CHECKPOINT_PATH] [MODEL_NAME] [CONV_MODE]&#xA;./scripts/v1_5/eval/video_chatgpt/eval_all.sh [MODEL_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;We provide snippets for quick inference with user prompts and images.&lt;/p&gt; &#xA;&lt;p&gt;Llama-3-VILA1.5-8B inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/Llama-3-VILA1.5-8b \&#xA;    --conv-mode vicuna_v1 \&#xA;    --query &#34;&amp;lt;image&amp;gt;\n Please describe the traffic condition.&#34; \&#xA;    --image-file &#34;av.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;VILA1.5-3B video inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/VILA1.5-3b \&#xA;    --conv-mode vicuna_v1 \&#xA;    --query &#34;&amp;lt;video&amp;gt;\n Please describe this video.&#34; \&#xA;    --video-file &#34;demo.mp4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quantization and Deployment&lt;/h2&gt; &#xA;&lt;p&gt;Our VILA models are quantized by &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt; into 4 bits for efficient inference on the edge. We provide a push-the-button &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/raw/main/scripts/vila_example.sh&#34;&gt;script&lt;/a&gt; to quantize VILA with AWQ.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA on desktop GPUs and edge GPUs&lt;/h3&gt; &#xA;&lt;p&gt;We support AWQ-quantized 4bit VILA on GPU platforms via &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt;. We provide a &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat#support-vlm-models-vila--llava&#34;&gt;tutorial&lt;/a&gt; to run the model with TinyChat after quantization. We also provide an &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat/serve&#34;&gt;instruction&lt;/a&gt; to launch a Gradio server (powered by TinyChat and AWQ) to serve 4-bit quantized VILA models.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA on laptops&lt;/h3&gt; &#xA;&lt;p&gt;We further support our AWQ-quantized 4bit VILA models on various CPU platforms with both x86 and ARM architectures with our &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine&#34;&gt;TinyChatEngine&lt;/a&gt;. We also provide a detailed &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine/tree/main?tab=readme-ov-file#deploy-vision-language-model-vlm-chatbot-with-tinychatengine&#34;&gt;tutorial&lt;/a&gt; to help the users deploy VILA on different CPUs.&lt;/p&gt; &#xA;&lt;h2&gt;Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;We release &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b&#34;&gt;VILA1.5-3B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-s2&#34;&gt;VILA1.5-3B-S2&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/Llama-3-VILA1.5-8b&#34;&gt;Llama-3-VILA1.5-8B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-13b&#34;&gt;VILA1.5-13B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-40b&#34;&gt;VILA1.5-40B&lt;/a&gt; and the 4-bit &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt;-quantized models &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-AWQ&#34;&gt;VILA1.5-3B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-s2-AWQ&#34;&gt;VILA1.5-3B-S2-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/Llama-3-VILA1.5-8b-AWQ&#34;&gt;Llama-3-VILA1.5-8B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-13b-AWQ&#34;&gt;VILA1.5-13B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-40b-AWQ&#34;&gt;VILA1.5-40B-AWQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üîí License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code is released under the Apache 2.0 license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;The pretrained weights are released under the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en&#34;&gt;CC-BY-NC-SA-4.0 license&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The service is a research preview intended for non-commercial use only, and is subject to the following licenses and terms: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;Model License&lt;/a&gt; of LLaMA. For LLAMA3-VILA checkpoints terms of use, please refer to the &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;LLAMA3 License&lt;/a&gt; for additional details.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;Terms of Use&lt;/a&gt; of the data generated by OpenAI&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/data_prepare/LICENSE&#34;&gt;Dataset Licenses&lt;/a&gt; for each one used during training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Team&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=OI7zFmwAAAAJ&amp;amp;hl=en&#34;&gt;*Yao Lu&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hongxu-yin.github.io/&#34;&gt;*Hongxu Yin&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linji.me/&#34;&gt;*Ji Lin&lt;/a&gt;: OpenAI (work done at Nvidia and MIT)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=6gKEYRgAAAAJ&amp;amp;hl=en&#34;&gt;Wei Ping&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.pmolchanov.com/&#34;&gt;Pavlo Molchanov&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=Wel9l1wAAAAJ&amp;amp;hl=en&#34;&gt;Andrew Tao&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://kentang.net/&#34;&gt;Haotian Tang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ys-2020.github.io/&#34;&gt;Shang Yang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lzhu.me/&#34;&gt;Ligeng Zhu&lt;/a&gt;: Nvidia, MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://weichenwang.me/&#34;&gt;Wei-Chen Wang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://xuefuzhao.github.io/&#34;&gt;Fuzhao Xue&lt;/a&gt;: Nvidia, NUS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://seerkfang.github.io/&#34;&gt;Yunhao Fang&lt;/a&gt;: Nvidia, UCSD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://yukangchen.com/&#34;&gt;Yukang Chen&lt;/a&gt;: Nvidia, CUHK&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/profile?id=~Zhuoyang_Zhang1&#34;&gt;Zhuoyang Zhang&lt;/a&gt;: Nvidia, Tsinghua Univ.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linkedin.com/in/yue-james-shen/&#34;&gt;Yue Shen&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=6xFvyJwAAAAJ&amp;amp;hl=en&#34;&gt;Wei-Ming Chen&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=r5WezOYAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Huizi Mao&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bfshi.github.io/&#34;&gt;Baifeng Shi&lt;/a&gt;: Nvidia, UC Berkeley&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://jankautz.com/&#34;&gt;Jan Kautz&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=62ElavIAAAAJ&amp;amp;hl=en&#34;&gt;Mohammad Shoeybi&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://songhan.mit.edu/&#34;&gt;Song Han&lt;/a&gt;: Nvidia, MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lin2023vila,&#xA;      title={VILA: On Pre-training for Visual Language Models},&#xA;      author={Ji Lin and Hongxu Yin and Wei Ping and Yao Lu and Pavlo Molchanov and Andrew Tao and Huizi Mao and Jan Kautz and Mohammad Shoeybi and Song Han},&#xA;      year={2023},&#xA;      eprint={2312.07533},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;: the codebase we built upon. Thanks for their wonderful work.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenGVLab/InternVL&#34;&gt;InternVL&lt;/a&gt;: for open-sourcing InternViT (used in VILA1.5-40b) and the &lt;a href=&#34;https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets&#34;&gt;InternVL-SFT&lt;/a&gt; data blend (inspired by LLaVA-1.6) used in all VILA1.5 models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the amazing open-sourced large language model!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT&#34;&gt;Video-ChatGPT&lt;/a&gt;: we borrowed video evaluation script from this repository.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/allenai/mmc4&#34;&gt;MMC4&lt;/a&gt;, &lt;a href=&#34;https://github.com/kakaobrain/coyo-dataset&#34;&gt;COYO-700M&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/MMInstruction/M3IT&#34;&gt;M3IT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/FLAN&#34;&gt;OpenORCA/FLAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/google-research-datasets/wit&#34;&gt;WIT&lt;/a&gt;, &lt;a href=&#34;https://github.com/OFA-Sys/gsm8k-ScRel/raw/main/data/train_use.jsonl&#34;&gt;GSM8K-ScRel&lt;/a&gt;, &lt;a href=&#34;https://visualgenome.org/api/v0/api_home.html&#34;&gt;VisualGenome&lt;/a&gt;, &lt;a href=&#34;https://visualcommonsense.com/download/&#34;&gt;VCR&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/derek-thomas/ScienceQA&#34;&gt;ScienceQA&lt;/a&gt;, &lt;a href=&#34;https://github.com/bytedance/Shot2Story/raw/master/DATA.md&#34;&gt;Shot2Story&lt;/a&gt;, &lt;a href=&#34;http://youcook2.eecs.umich.edu/&#34;&gt;Youcook2&lt;/a&gt;, &lt;a href=&#34;https://eric-xw.github.io/vatex-website/download.html&#34;&gt;Vatex&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction&#34;&gt;ShareGPT-Video&lt;/a&gt; for providing datasets used in this research.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>