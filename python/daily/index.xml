<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-05T01:40:02Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>codecov/codecov-api</title>
    <updated>2023-08-05T01:40:02Z</updated>
    <id>tag:github.com,2023-08-05:/codecov/codecov-api</id>
    <link href="https://github.com/codecov/codecov-api" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the API of Codecov&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Codecov API&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We believe that everyone should have access to quality software (like Sentry), that‚Äôs why we have always offered Codecov for free to open source maintainers.&lt;/p&gt; &#xA; &lt;p&gt;By making our code public, we‚Äôre not only joining the community that‚Äôs supported us from the start ‚Äî but also want to make sure that every developer can contribute to and build on the Codecov experience.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;A private Django REST Framework API intended to serve Codecov&#39;s front end.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;This project contains a makefile. To build the docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;requirements.txt&lt;/code&gt; is used in the base image. If you make changes to &lt;code&gt;requirements.txt&lt;/code&gt; you will need to rebuild.&lt;/p&gt; &#xA;&lt;p&gt;Note, you&#39;ll need to install Rust to build &lt;code&gt;ribs&lt;/code&gt; which is a dependency of &lt;code&gt;shared&lt;/code&gt;. Go here for more info on how to do this: &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;https://www.rust-lang.org/tools/install&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Running Standalone&lt;/h3&gt; &#xA;&lt;p&gt;This project contains a &lt;code&gt;docker-compose.yml&lt;/code&gt; file that is intended to run the api standalone. In this configuration it &lt;strong&gt;does not&lt;/strong&gt; share codecov.io&#39;s development database; so don&#39;t expect parity there.&lt;/p&gt; &#xA;&lt;p&gt;To start the service, do&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker-compose up&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Utilizing its own database provides a convenient way for the REST API to provide its own helpful seeds and migrations for active development without potentially destroying/modifying your development database for codecov.io.&lt;/p&gt; &#xA;&lt;p&gt;Once running, the api will be available at &lt;code&gt;http://localhost:5100&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Running with codecov.io&lt;/h3&gt; &#xA;&lt;p&gt;This service will startup when you run codecov.io normally. It is under that &lt;code&gt;api&lt;/code&gt; block of codecov.io&#39;s &lt;code&gt;docker-compose.yml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to run tests (that doesn&#39;t require installing postgres and other dependencies) is to run inside of docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose up&#xA;docker exec -it codecov-api_api_1 pytest -rf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Testing standalone&lt;/h3&gt; &#xA;&lt;p&gt;If you would like to use pytest directly (Either through an IDE like PyCharm or with the CLI), you will need to change the settings file used by pytest. Run this command to have the tests executed (You will need an instance of postgres running locally):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;RUN_ENV=TESTING DJANGO_SETTINGS_MODULE=codecov.settings_test pytest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure to have all the requirements from &lt;code&gt;requirements.txt&lt;/code&gt; installed.&lt;/p&gt; &#xA;&lt;h3&gt;Deploying&lt;/h3&gt; &#xA;&lt;p&gt;All work merged into the &lt;code&gt;main&lt;/code&gt; branch is immediately deployed to the production environment. More context on this strategy can be found &lt;a href=&#34;https://codecovio.atlassian.net/wiki/spaces/ENG/pages/507445249/Branching+and+Continuous+Delivery+Strategy+Proposal&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Deploying to Staging environment&lt;/h3&gt; &#xA;&lt;p&gt;To deploy to our staging environment it&#39;s crucial to follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Check in Slack to see if anyone is currently using the staging environment&lt;/li&gt; &#xA; &lt;li&gt;If not, delete the current &lt;code&gt;staging&lt;/code&gt; branch&lt;/li&gt; &#xA; &lt;li&gt;Create a new &lt;code&gt;staging&lt;/code&gt; branch and merge your feature branch into it&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Steps 2 and 3 are important to limit interaction between features not yet merged into main. This approach was inspired by this document: &lt;a href=&#34;https://codecovio.atlassian.net/wiki/spaces/ENG/pages/507445249/Branching+and+Continuous+Delivery+Strategy+Proposal&#34;&gt;https://codecovio.atlassian.net/wiki/spaces/ENG/pages/507445249/Branching+and+Continuous+Delivery+Strategy+Proposal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Secret and Credential Management&lt;/h3&gt; &#xA;&lt;p&gt;This project should store no secrets or credentials in its source. If you need to add to / modify / setup secrets for this project, contact Eli and he&#39;ll get you started..&lt;/p&gt; &#xA;&lt;h3&gt;Adding dependencies&lt;/h3&gt; &#xA;&lt;p&gt;This repository uses &lt;code&gt;pip-tools&lt;/code&gt; to manage dependencies, so make sure you&#39;ve installed it with &lt;code&gt;pip install pip-tools&lt;/code&gt;. To add or update dependencies, change &lt;code&gt;requirements.in&lt;/code&gt;, Then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip-compile requirements.in&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Do not change &lt;code&gt;requirements.txt&lt;/code&gt; directly.&lt;/p&gt; &#xA;&lt;h3&gt;Formatting&lt;/h3&gt; &#xA;&lt;p&gt;This project uses &lt;code&gt;black&lt;/code&gt; for formatting.&lt;br&gt; You can run the linter using the command &lt;code&gt;make lint&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Migrations&lt;/h3&gt; &#xA;&lt;p&gt;We leverage Django&#39;s migration system to keep the state of our models in sync with the state of our database. You can read more about how we work with migrations at &lt;a href=&#34;https://codecovio.atlassian.net/wiki/spaces/ENG/pages/1696530442/Migrations&#34;&gt;https://codecovio.atlassian.net/wiki/spaces/ENG/pages/1696530442/Migrations&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This repository, like all of Codecov&#39;s repositories, strives to follow our general &lt;a href=&#34;https://github.com/codecov/contributing&#34;&gt;Contributing guidlines&lt;/a&gt;. If you&#39;re considering making a contribution to this repository, we encourage review of our Contributing guidelines first.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenBMB/ToolBench</title>
    <updated>2023-08-05T01:40:02Z</updated>
    <id>tag:github.com,2023-08-05:/OpenBMB/ToolBench</id>
    <link href="https://github.com/OpenBMB/ToolBench" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open platform for training, serving, and evaluating large language model for tool learning.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; üõ†Ô∏èToolBenchü§ñ&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Tool_Num-3451-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/API_Num-16464-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Current_Dataset_Size-12K-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Total_API_Call-37K-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Average_Reasoning_Traces-4.1-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Tool_LLaMA-Released-green?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#model&#34;&gt;Model&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#data&#34;&gt;Data Release&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#web-ui&#34;&gt;Web Demo&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#tool-eval&#34;&gt;Tool Eval&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/paper.pdf&#34;&gt;Paper&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#citation&#34;&gt;Citation&lt;/a&gt; &lt;/p&gt;  &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://cdn.discordapp.com/attachments/941582479117127680/1111543600879259749/20230526075532.png&#34; width=&#34;350px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üî®This project (ToolLLM) aims to construct &lt;strong&gt;open-source, large-scale, high-quality&lt;/strong&gt; instruction tuning SFT data to facilitate the construction of powerful LLMs with general &lt;strong&gt;tool-use&lt;/strong&gt; capability. We aim to empower open-source LLMs to master thousands of diverse real-world APIs. We achieve this by collecting a high-quality instruction-tuning dataset. It is constructed automatically using the latest ChatGPT (gpt-3.5-turbo-16k), which is upgraded with enhanced &lt;a href=&#34;https://openai.com/blog/function-calling-and-other-api-updates&#34;&gt;function call&lt;/a&gt; capabilities. We provide the dataset, the corresponding training and evaluation scripts, and a capable model ToolLLaMA fine-tuned on ToolBench.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üíÅ‚Äç‚ôÇÔ∏èüíÅüíÅ‚Äç‚ôÄÔ∏èJoint Us on &lt;a href=&#34;https://discord.gg/QSC6yTtu&#34;&gt;Discord&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023/8/4]&lt;/strong&gt; We provide &lt;strong&gt;RapidAPI backend service&lt;/strong&gt; to free you from using your own RapidAPI key and subscribing the APIs. Please fill out our &lt;a href=&#34;https://forms.gle/oCHHc8DQzhGfiT9r6&#34;&gt;form&lt;/a&gt;. We will review it as soon as possible and send you the ToolBench key to get start on it!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023/8/1]&lt;/strong&gt; Our &lt;a href=&#34;https://arxiv.org/abs/2307.16789&#34;&gt;paper&lt;/a&gt; is released.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023/7/27]&lt;/strong&gt; New version ToolBench is released.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;‚ú®Here is an overview of the dataset construction, training, and evaluation.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/overview.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;‚ú®‚ú®Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Collection&lt;/strong&gt;: we gather &lt;strong&gt;16464&lt;/strong&gt; representational state transfer (REST) APIs from &lt;a href=&#34;https://rapidapi.com/hub&#34;&gt;RapidAPI&lt;/a&gt;, a platform that hosts massive real-world APIs provided by developers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Instruction Generation&lt;/strong&gt;: we curate instructions that involve both &lt;strong&gt;single-tool&lt;/strong&gt; and &lt;strong&gt;multi-tool&lt;/strong&gt; scenarios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Answer Annotation&lt;/strong&gt;: we develop a novel &lt;strong&gt;depth-first search based decision tree&lt;/strong&gt; (DFSDT) to bolster the planning and reasoning ability of LLMs, which significantly improves the annotation efficiency and successfully annotates those complex instructions that cannot be answered with CoT or ReACT. We provide responses that not only include the final answer but also incorporate the model&#39;s &lt;strong&gt;reasoning process, tool execution, and tool execution results&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Retriver&lt;/strong&gt;: we incorporate API retrieval to equip ToolLLaMA with open-domain tool-using abilities.&lt;/li&gt; &#xA; &lt;li&gt;All the data is automatically generated by OpenAI API and filtered by us, the whole data creation process is easy to scale up.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/comparison.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;We also provide &lt;strong&gt;A demo of using ToolLLaMA&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/OpenBMB/ToolBench/assets/25274507/f1151d85-747b-4fac-92ff-6c790d8d9a31&#34;&gt;https://github.com/OpenBMB/ToolBench/assets/25274507/f1151d85-747b-4fac-92ff-6c790d8d9a31&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Currently, our ToolLLaMA has reached the performance of ChatGPT (turbo-16k) in tool use, in the future, &lt;em&gt;we will continually improve the data quality and increase the coverage of real-world tools.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/performance.png&#34; width=&#34;300px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Here is the &lt;em&gt;&lt;a href=&#34;https://github.com/OpenBMB/ToolBench/tree/legacy&#34;&gt;Old version&lt;/a&gt;&lt;/em&gt; of ToolBench.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;üëêToolBench is intended solely for research and educational purposes and should not be construed as reflecting the opinions or views of the creators, owners, or contributors of this dataset. It is distributed under &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34;&gt;CC BY NC 4.0 License&lt;/a&gt;. Below is the statistics of the data :&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Tool Nums&lt;/th&gt; &#xA;   &lt;th&gt;API Nums&lt;/th&gt; &#xA;   &lt;th&gt;Instance Nums&lt;/th&gt; &#xA;   &lt;th&gt;Real API Call&lt;/th&gt; &#xA;   &lt;th&gt;Reasoning Traces&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3451&lt;/td&gt; &#xA;   &lt;td&gt;16464&lt;/td&gt; &#xA;   &lt;td&gt;12657&lt;/td&gt; &#xA;   &lt;td&gt;37204&lt;/td&gt; &#xA;   &lt;td&gt;4.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We crawl 16000+ real-world APIs from &lt;a href=&#34;https://rapidapi.com/hub&#34;&gt;RapidAPI&lt;/a&gt;, and curate realistic human instructions that involve them. Below we present a hierarchy of RapidAPI and our instruction generation process.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/instructiongeneration.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;ToolBench contains both single-tool and multi-tool scenarios. The multi-tool scenarios can be further categorized into intra-category multi-tool and intra-collection multi-tool. We utilize DFSDT method for all scenarios to our data creation. Here is an illustration for the data creation process using DFSDT method:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/answer_anno.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Data Release&lt;/h3&gt; &#xA;&lt;p&gt;Please download our dataset using the following link: &lt;a href=&#34;https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://cloud.tsinghua.edu.cn/f/c9e50625743b40bfbe10/&#34;&gt;Tsinghua Cloud&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;G1&lt;/code&gt;,&lt;code&gt;G2&lt;/code&gt;, &lt;code&gt;G3&lt;/code&gt;data refers to single-tool, intra-category multi-tool and intra-collection multi-tool data respectively. We also have an &lt;a href=&#34;https://atlas.nomic.ai/map/58aca169-c29a-447a-8f01-0d418fc4d341/030ddad7-5305-461c-ba86-27e1ca79d899&#34;&gt;Atlas Explorer&lt;/a&gt; for visualization.&lt;/li&gt; &#xA; &lt;li&gt;We split the G1, G2 and G3 data into train, eval and test parts respectively and combine the train data for training in our main experiments. &lt;code&gt;toolllama_G123_dfs_train.json&lt;/code&gt; refers to the combined train data.&lt;/li&gt; &#xA; &lt;li&gt;The tool environment related data is in &lt;code&gt;toolenv&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;We sample 100 instances from every test set. The &lt;code&gt;test_query_ids&lt;/code&gt; directory contains query ids of the test instances in each test set.&lt;/li&gt; &#xA; &lt;li&gt;The data used for tool retrieval is included in the &lt;code&gt;retrieval&lt;/code&gt; directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ñModel&lt;/h2&gt; &#xA;&lt;p&gt;We release the &lt;a href=&#34;https://huggingface.co/ToolBench/ToolLLaMA-7b&#34;&gt;ToolLLaMA-7b&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/ToolBench/ToolLLaMA-7b-LoRA&#34;&gt;ToolLLaMA-7b-LoRA&lt;/a&gt; models, which are both trained on the released dataset in a multi-task fashion. We also release the &lt;a href=&#34;https://huggingface.co/ToolBench/ToolBench_IR_bert_based_uncased&#34;&gt;tool retriever&lt;/a&gt; trained under our experimental setting.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄFine-tuning&lt;/h2&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repository and navigate to the ToolBench folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:OpenBMB/ToolBench.git&#xA;cd ToolBench&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install Package (python&amp;gt;=3.9)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for ToolEval only&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r toolbench/tooleval/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Prepare the data and tool environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget --no-check-certificate &#39;https://docs.google.com/uc?export=download&amp;amp;id=1Vis-RxBstXLKC1W1agIQUJNuumPJrrw0&amp;amp;confirm=yes&#39; -O data.zip&#xA;unzip data.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training Retriever&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data preprocessing:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python data/preprocess_retriever_data.py \&#xA;    --query_file data/instruction/G1_query.json \&#xA;    --index_file data/test_query_ids/G1_instruction_test_query_ids.json \&#xA;    --dataset_name G1 \&#xA;    --output_dir data/retrieval/G1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Then run the following command to train the tool retriever:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/retrieval/train.py \&#xA;    --data_path data/retrieval/G1/ \&#xA;    --model_name bert-base-uncased \&#xA;    --output_path retrieval_model \&#xA;    --num_epochs 5 \&#xA;    --train_batch_size 32 \&#xA;    --learning_rate 2e-5 \&#xA;    --warmup_steps 500 \&#xA;    --max_seq_length 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training ToolLLaMA&lt;/h3&gt; &#xA;&lt;p&gt;Our training code is based on &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;. You can use the following command to train ToolLLaMA-7b with 2 x A100 (80GB), with the preprocessed data in our &lt;a href=&#34;https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J&#34;&gt;data link&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;torchrun --nproc_per_node=2 --master_port=20001 toolbench/train/train_long_seq.py \&#xA;    --model_name_or_path huggyllama/llama-7b  \&#xA;    --data_path  data/toolllama_G123_dfs_train.json \&#xA;    --eval_data_path  data/toolllama_G123_dfs_eval.json \&#xA;    --conv_template tool-llama-single-round \&#xA;    --bf16 True \&#xA;    --output_dir toolllama \&#xA;    --num_train_epochs 2 \&#xA;    --per_device_train_batch_size 2 \&#xA;    --per_device_eval_batch_size 2 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;epoch&#34; \&#xA;    --prediction_loss_only \&#xA;    --save_strategy &#34;epoch&#34; \&#xA;    --save_total_limit 8 \&#xA;    --learning_rate 5e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.04 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --tf32 True \&#xA;    --model_max_length 8192 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to none&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also preprocess and split the data in your own way with this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python preprocess/preprocess_toolllama_data.py \&#xA;    --tool_data_dir data/answer/G1_answer \&#xA;    --method DFS_woFilter_w2 \&#xA;    --output_file data/answer/toolllama_G1_dfs.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train lora version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;deepspeed --master_port=20001 toolbench/train/train_long_seq_lora.py \&#xA;    --model_name_or_path huggyllama/llama-7b  \&#xA;    --data_path  data/toolllama_G123_dfs_train.json \&#xA;    --eval_data_path  data/toolllama_G123_dfs_eval.json \&#xA;    --conv_template tool-llama-single-round \&#xA;    --bf16 True \&#xA;    --output_dir toolllama_lora \&#xA;    --num_train_epochs 5 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 2 \&#xA;    --gradient_accumulation_steps 2 \&#xA;    --evaluation_strategy &#34;epoch&#34; \&#xA;    --prediction_loss_only \&#xA;    --save_strategy &#34;epoch&#34; \&#xA;    --save_total_limit 8 \&#xA;    --learning_rate 5e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.04 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --model_max_length 8192 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \    &#xA;    --deepspeed ds_configs/stage2.json \&#xA;    --report_to none&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference With Our RapidAPI Server&lt;/h2&gt; &#xA;&lt;p&gt;Please fill out the &lt;a href=&#34;https://forms.gle/oCHHc8DQzhGfiT9r6&#34;&gt;form&lt;/a&gt; first and after reviewing we will send you the toolbench key. Then prepare your toolbench key by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export TOOLBENCH_KEY=&#34;your_toolbench_key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For ToolLLaMA&lt;/h3&gt; &#xA;&lt;p&gt;To inference with ToolLLaMA, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model toolllama \&#xA;    --model_path ToolBench/ToolLLaMA-7b \&#xA;    --max_observation_length 1024 \&#xA;    --observ_compress_method truncate \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/toolllama_dfs \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;strong&gt;ToolLLaMA-LoRA&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model toolllama \&#xA;    --model_path huggyllama/llama-7b \&#xA;    --lora \&#xA;    --lora_path /path/to/your/downloaded/ToolLLaMA-7b-LoRA \&#xA;    --max_observation_length 1024 \&#xA;    --observ_compress_method truncate \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/toolllama_lora_dfs \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For ToolLLaMA-LoRA under &lt;strong&gt;open-domain&lt;/strong&gt; setting, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline_open_domain.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --corpus_tsv_path data/retrieval/G1/corpus.tsv \&#xA;    --retrieval_model_path /path/to/your/retrival_model \&#xA;    --retrieved_api_nums 5 \&#xA;    --backbone_model toolllama \&#xA;    --model_path huggyllama/llama-7b \&#xA;    --lora \&#xA;    --lora_path /path/to/your/toolllama_lora \&#xA;    --max_observation_length 1024 \&#xA;    --observ_compress_method truncate \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo_open_domain.json \&#xA;    --output_answer_file data/answer/toolllama_lora_dfs_open_domain \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For OpenAI Models&lt;/h3&gt; &#xA;&lt;p&gt;To use ChatGPT, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export TOOLBENCH_KEY=&#34;&#34;&#xA;export OPENAI_KEY=&#34;&#34;&#xA;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model chatgpt_function \&#xA;    --openai_key $OPENAI_KEY \&#xA;    --max_observation_length 1024 \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/chatgpt_dfs \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use Text-Davinci-003, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export TOOLBENCH_KEY=&#34;&#34;&#xA;export OPENAI_KEY=&#34;&#34;&#xA;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model davinci \&#xA;    --openai_key $OPENAI_KEY \&#xA;    --max_observation_length 1024 \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/davinci_dfs \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference With Your Own RapidAPI Account&lt;/h2&gt; &#xA;&lt;p&gt;To do inference with customized RapidAPI account, pass your &lt;strong&gt;rapidapi key&lt;/strong&gt; through &lt;code&gt;rapidapi_key&lt;/code&gt; and specify the &lt;code&gt;use_rapidapi_key&lt;/code&gt; argument in the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export RAPIDAPI_KEY=&#34;&#34;&#xA;export OPENAI_KEY=&#34;&#34;&#xA;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model chatgpt_function \&#xA;    --openai_key $OPENAI_KEY \&#xA;    --max_observation_length 1024 \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/chatgpt_dfs \&#xA;    --rapidapi_key $RAPIDAPI_KEY \&#xA;    --use_rapidapi_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setting up and running the interface&lt;/h2&gt; &#xA;&lt;p&gt;ToolBench contains a Web UI based on &lt;a href=&#34;https://github.com/mckaywrigley/chatbot-ui&#34;&gt;Chatbot UI&lt;/a&gt;, forked to include the use of tools in the interface. It comes in two parts: the backend server, and &lt;a href=&#34;https://github.com/lilbillybiscuit/chatbot-ui-toolllama&#34;&gt;chatbot-ui-toolllama&lt;/a&gt;. Here is a &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/toolbench-demo.mp4&#34;&gt;video demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/lilbillybiscuit/chatbot-ui-toolllama&#xA;cd chatbot-ui-toolllama&#xA;npm install&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The app will be available on &lt;code&gt;http://localhost:3000/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Backend server&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/inference/toolbench_server.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --corpus_tsv_path data/retrieval/G1/corpus.tsv \&#xA;    --retrieval_model_path /path/to/your/retrival_model \&#xA;    --retrieved_api_nums 5 \&#xA;    --backbone_model toolllama \&#xA;    --model_path huggyllama/llama-7b \&#xA;    --lora \&#xA;    --lora_path /path/to/your/toolllama_lora \&#xA;    --max_observation_length 1024 \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo_open_domain.json \&#xA;    --output_answer_file data/answer/toolllama_lora_dfs_open_domain \&#xA;    --rapidapi_key $RAPIDAPIKEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This server will be available on &lt;code&gt;http://localhost:5000/&lt;/code&gt;. To start a request, call &lt;code&gt;http://localhost:5000/stream&lt;/code&gt; with a GET or POST request containing a JSON object with the following fields:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;text&#34;: &#34;What is the weather in New York today?&#34;,&#xA;    &#34;top_k&#34;: 5,&#xA;    &#34;method&#34;: &#34;DFS_woFilter_w2&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ToolEval&lt;/h2&gt; &#xA;&lt;p&gt;By fine-tuning LLaMA on ToolBench, we obtain &lt;strong&gt;ToolLLaMA&lt;/strong&gt;. Considering that human evaluation can be time-consuming, we follow &lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;AlpacaEval&lt;/a&gt; to develop an efficient machine evaluator &lt;strong&gt;ToolEval&lt;/strong&gt;, which incorporates two evaluation metrics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pass Rate&lt;/strong&gt;: Calculates the proportion of successfully completing an instruction within limited OpenAI API calls.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Preference&lt;/strong&gt;: Measured by comparing two answers (action sequences) for a given instruction. We pre-define a set of criteria for a better answer, which are organized as prompts for ChatGPT. We provide the test instruction and two candidate answers to the evaluator and obtain its preference. We evaluate each answer pair multiple times to improve the reliability of our system. Then we calculate the &lt;strong&gt;Win Rate&lt;/strong&gt; (percentage of being preferred by the evaluator) and &lt;strong&gt;Standard Error&lt;/strong&gt; (the standard error of the Win Rate). More details can be found in our paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To validate the effectiveness of the metric &lt;strong&gt;Preference&lt;/strong&gt;, we sample among three different methods (ChatGPT+ReACT, GPT4+ReACT, and ChatGPT+DFSDT) to obtain answer pairs for &lt;em&gt;600&lt;/em&gt; test instructions. Then we engage humans to annotate human preference for them (&lt;em&gt;4&lt;/em&gt; annotations for each answer pair, &lt;em&gt;2400&lt;/em&gt; annotations in total). Our automatic evaluator, developed using ChatGPT, demonstrates a significant correlation of &lt;strong&gt;75.8%&lt;/strong&gt; with human annotators. We also obtain the agreement among different human annotators &lt;strong&gt;83.54%&lt;/strong&gt;, and the agreement between humans and our evaluator &lt;strong&gt;80.21%&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More details about ToolEval can be found in our paper.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation with ToolEval&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate a model on G1-Inst. test set, for example, run the following commands.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pass rate:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python toolbench/tooleval/pass_rate.py --answer_dir data/answer/toolllama_dfs/G1_instruction&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Win rate (Reference model: ChatGPT-ReACT):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_KEY=&#34;&#34;&#xA;export REF_MODEL_DATA=&#34;data/answer/chatgpt_cot/G1_instruction&#34;&#xA;export REF_MODEL_METHOD=&#34;CoT&#34;&#xA;export TEST_MODEL_DATA=&#34;data/answer/toolllama_dfs/G1_instruction&#34;&#xA;export TEST_MODEL_METHOD=&#34;DFS&#34;&#xA;python ./toolbench/tooleval/convert_to_answer_format.py \&#xA;    --method CoT \&#xA;    --answer_dir $REF_MODEL_DATA \&#xA;    --output ${REF_MODEL_DATA}_converted&#xA;&#xA;python ./toolbench/tooleval/convert_to_answer_format.py \&#xA;    --method DFS \&#xA;    --answer_dir $TEST_MODEL_DATA \&#xA;    --output ${TEST_MODEL_DATA}_converted&#xA;&#xA;python ./toolbench/tooleval/automatic_eval_sample.py \&#xA;    --output ${REF_MODEL_DATA}_converted \&#xA;    --ref_output ${TEST_MODEL_DATA}_converted \&#xA;    --method $REF_MODEL_METHOD \&#xA;    --use_existed_output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üìä Model Experiments Results&lt;/h3&gt; &#xA;&lt;p&gt;In our main experiments, ToolLLaMA demonstrates a compelling capability to handle both single-tool and complex multi-tool instructions. Below are the main results compared with ChatGPT and Text-Davinci-003.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pass Rate:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;I1-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Tool.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I3-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;78&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;84&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;89&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;58&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;57&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;69.6&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT-ReACT&lt;/td&gt; &#xA;   &lt;td&gt;56&lt;/td&gt; &#xA;   &lt;td&gt;62&lt;/td&gt; &#xA;   &lt;td&gt;66&lt;/td&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;44.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-Davinci-003-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;53&lt;/td&gt; &#xA;   &lt;td&gt;58&lt;/td&gt; &#xA;   &lt;td&gt;61&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;39&lt;/td&gt; &#xA;   &lt;td&gt;47.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-Davinci-003-ReACT&lt;/td&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;18.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA&lt;/td&gt; &#xA;   &lt;td&gt;68&lt;/td&gt; &#xA;   &lt;td&gt;80&lt;/td&gt; &#xA;   &lt;td&gt;75&lt;/td&gt; &#xA;   &lt;td&gt;47&lt;/td&gt; &#xA;   &lt;td&gt;56&lt;/td&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;   &lt;td&gt;61.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;51&lt;/td&gt; &#xA;   &lt;td&gt;63&lt;/td&gt; &#xA;   &lt;td&gt;61&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;42&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;50.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-API Retriever&lt;/td&gt; &#xA;   &lt;td&gt;62&lt;/td&gt; &#xA;   &lt;td&gt;62&lt;/td&gt; &#xA;   &lt;td&gt;72&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;55&lt;/td&gt; &#xA;   &lt;td&gt;47&lt;/td&gt; &#xA;   &lt;td&gt;57.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Win Rate:&lt;/strong&gt; (Reference model: ChatGPT-DFSDT)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;I1-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Tool.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I3-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;50.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT-ReACT&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;41&lt;/td&gt; &#xA;   &lt;td&gt;43&lt;/td&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;td&gt;30.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-Davinci-003-ReACT&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;13.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-Davinci-003-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;34&lt;/td&gt; &#xA;   &lt;td&gt;43&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;31.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;59&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;48&lt;/td&gt; &#xA;   &lt;td&gt;46&lt;/td&gt; &#xA;   &lt;td&gt;48.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;43&lt;/td&gt; &#xA;   &lt;td&gt;36.4&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;42&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;41.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-API Retriever&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;39&lt;/td&gt; &#xA;   &lt;td&gt;44&lt;/td&gt; &#xA;   &lt;td&gt;49&lt;/td&gt; &#xA;   &lt;td&gt;49&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;55&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;47.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ToolLLaMA will reach GPT-4&#39;s tool-use capability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; We will train a ToolLLaMa-2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources of Tool Learning&lt;/h2&gt; &#xA;&lt;p&gt;With the powerful capabilities of foundation models, we are eager to see their applications in manipulating various tools. For more resources, please refer to the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;BMTools&lt;/strong&gt;. [&lt;a href=&#34;https://github.com/OpenBMB/BMTools&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Learning Survey&lt;/strong&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2304.08354&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Learning Paper List&lt;/strong&gt;. [&lt;a href=&#34;https://github.com/thunlp/ToolLearningPapers&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebCPM&lt;/strong&gt;. [&lt;a href=&#34;https://github.com/thunlp/WebCPM&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to cite us if you like ToolBench.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{qin2023toolllm,&#xA;      title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, &#xA;      author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},&#xA;      year={2023},&#xA;      eprint={2307.16789},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{qin2023tool,&#xA;      title={Tool Learning with Foundation Models}, &#xA;      author={Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},&#xA;      year={2023},&#xA;      eprint={2304.08354},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>segmind/distill-sd</title>
    <updated>2023-08-05T01:40:02Z</updated>
    <id>tag:github.com,2023-08-05:/segmind/distill-sd</id>
    <link href="https://github.com/segmind/distill-sd" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Segmind Distilled diffusion&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/logo.png&#34; alt=&#34;logo&#34; width=&#34;400&#34; height=&#34;auto&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt; 50% Smaller, Faster Stable Diffusion üöÄ &lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/main.png&#34; width=&#34;950&#34; height=&#34;auto&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Knowledge-distilled, smaller versions of Stable Diffusion. Unofficial implementation as described in &lt;a href=&#34;https://arxiv.org/abs/2305.15798&#34;&gt;BK-SDM&lt;/a&gt;.&lt;br&gt; These distillation-trained models produce images of similar quality to the full-sized Stable-Diffusion model while being significantly faster and smaller.&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Components of this Repository:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/data.py&#34;&gt;data.py&lt;/a&gt;&lt;/strong&gt; contains scripts to download data for training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/distill_training.py&#34;&gt;distill_training.py&lt;/a&gt;&lt;/strong&gt; trains the U-net using the methods described in the paper. This might need additional configuration depending on what model type you want to train (sd_small/sd_tiny),batch size, hyperparameters etc. The basic training code was sourced from the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;Huggingface ü§ó diffusers library&lt;/a&gt;.&lt;br&gt; LoRA Training and Training from checkpoints can be done by simply using the standard diffusers scripts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training Details:&lt;/h2&gt; &#xA;&lt;p&gt;Knowledge-Distillation training a neural network is akin to a teacher guiding a student step-by-step (a somewhat loose example). A large teacher model is trained on large amounts of data and then a smaller model is trained on a smaller dataset, with the objective of aping the outputs of the larger model along with classical training on the dataset.&lt;br&gt; For the Knowledge-Distillation training, we used &lt;a href=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/SG161222/Realistic_Vision_V4.0&#34;&gt;SG161222/Realistic_Vision_V4.0&#39;s&lt;/a&gt; U-net as the teacher model with a subset of &lt;a href=&#34;https://huggingface.co/datasets/recastai/LAION-art-EN-improved-captions&#34;&gt;recastai/LAION-art-EN-improved-captions&lt;/a&gt; as training data.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;The final training loss is the sum of the MSE loss between the noise predicted by the teacher U-net and the noise predicted by the student U-net, the MSE Loss between the actual added noise and the predicted noise, and the sum of MSE Losses between the predictions of the teacher and student U-nets after every block.&lt;br&gt; Total Loss:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/eq1.png&#34; alt=&#34;image&#34;&gt;&lt;br&gt; Task Loss (i.e MSE Loss between added noise and actual noise):&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/eq2.png&#34; alt=&#34;image&#34;&gt;&lt;br&gt; Knowledge Distillation Output Loss (i.e MSE Loss between final output of teacher U-net and student U-net):&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/eq3.png&#34; alt=&#34;image&#34;&gt;&lt;br&gt; Feature-level Knowledge Distillation Loss (i.e MSE Loss between outputs of each block in the U-net):&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/eq4.png&#34; alt=&#34;image&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here are the settings we used for training:&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lr = 1e-5&#xA;scheduler = &#34;cosine&#34;&#xA;batch_size = 32&#xA;output_weight = 0.5 # Lambda Out in the final loss equation&#xA;feature_weight = 0.5 # Lambda Feat in the final loss equation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Parameters:&lt;/h2&gt; &#xA;&lt;p&gt;Normal Stable Diffusion U-net:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/base.png&#34; alt=&#34;image&#34;&gt; Number of parameters: 859,520,964&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;SD_Small U-net:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/small.png&#34; alt=&#34;image&#34;&gt;&lt;br&gt; Number of parameters: 579,384,964&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;SD_Tiny U-net:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/tiny.png&#34; alt=&#34;image&#34;&gt;&lt;br&gt; Number of parameters: 323,384,964&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Model parameters reported using torchinfo)&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import DiffusionPipeline&#xA;from diffusers import DPMSolverMultistepScheduler&#xA;from torch import Generator&#xA;&#xA;&#xA;path = &#39;segmind/small-sd&#39; # Path to the appropriate model-type&#xA;# Insert your prompt below.&#xA;prompt = &#34;Faceshot Portrait of pretty young (18-year-old) Caucasian wearing a high neck sweater, (masterpiece, extremely detailed skin, photorealistic, heavy shadow, dramatic and cinematic lighting, key light, fill light), sharp focus, BREAK epicrealism&#34;&#xA;# Insert negative prompt below. We recommend using this negative prompt for best results.&#xA;negative_prompt = &#34;(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck&#34; &#xA;&#xA;torch.set_grad_enabled(False)&#xA;torch.backends.cudnn.benchmark = True&#xA;&#xA;# Below code will run on gpu, please pass cpu everywhere as the device and set &#39;dtype&#39; to torch.float32 for cpu inference.&#xA;with torch.inference_mode():&#xA;    gen = Generator(&#34;cuda&#34;)&#xA;    gen.manual_seed(1674753452)&#xA;    pipe = DiffusionPipeline.from_pretrained(path, torch_dtype=torch.float16, safety_checker=None, requires_safety_checker=False)&#xA;    pipe.to(&#39;cuda&#39;)&#xA;    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)&#xA;    pipe.unet.to(device=&#39;cuda&#39;, dtype=torch.float16, memory_format=torch.channels_last)&#xA;&#xA;    img = pipe(prompt=prompt,negative_prompt=negative_prompt, width=512, height=512, num_inference_steps=25, guidance_scale = 7, num_images_per_prompt=1, generator = gen).images[0]&#xA;    img.save(&#34;image.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training the Model:&lt;/h2&gt; &#xA;&lt;p&gt;Training instructions for knowledge distillation are similar to those of the diffusers text-to-image finetuning script, apart from some extra parameters:&lt;br&gt; &lt;code&gt;--distill_level&lt;/code&gt;: One of &#34;sd_small&#34; or &#34;sd_tiny&#34;, depending on which type of model is to be trained.&lt;br&gt; &lt;code&gt;--output_weight&lt;/code&gt;: A floating point number representing the amount the output-level KD loss is to be scaled by.&lt;br&gt; &lt;code&gt;--feature-weight&lt;/code&gt;: A floating point number representing the amount the feautre-level KD loss is to be scaled by.&lt;br&gt; Also, &lt;code&gt;snr_gamma&lt;/code&gt; has been removed. We suggest using a standard Stable Diffusion model to distillation train, since the script has been configured for those architectures.&lt;/p&gt; &#xA;&lt;p&gt;An example:&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export MODEL_NAME=&#34;SG161222/Realistic_Vision_V4.0&#34;&#xA;export DATASET_NAME=&#34;fantasyfish/laion-art&#34;&#xA;&#xA;accelerate launch --mixed_precision=&#34;fp16&#34;  distill_training.py \&#xA;  --pretrained_model_name_or_path=$MODEL_NAME \&#xA;  --dataset_name=$DATASET_NAME \&#xA;  --use_ema \&#xA;  --resolution=512 --center_crop --random_flip \&#xA;  --train_batch_size=1 \&#xA;  --gradient_accumulation_steps=4 \&#xA;  --gradient_checkpointing \&#xA;  --max_train_steps=15000 \&#xA;  --distill_level=&#34;sd_small&#34;\&#xA;  --output_weight=0.5\&#xA;  --feature_weight=0.5\&#xA;  --learning_rate=1e-05 \&#xA;  --max_grad_norm=1 \&#xA;  --lr_scheduler=&#34;constant&#34; --lr_warmup_steps=0 \&#xA;  --output_dir=&#34;sd-laion-art&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train from the huggingface checkpoints, use the checkpoint_training script and just replace MODEL_NAME with &#34;segmind/small-sd&#34; or &#34;segmind/tiny-sd&#34;, like so:&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export MODEL_NAME=&#34;segmind/small-sd&#34;&#xA;export DATASET_NAME=&#34;fantasyfish/laion-art&#34;&#xA;&#xA;accelerate launch --mixed_precision=&#34;fp16&#34;  checkpoint_training.py \&#xA;  --pretrained_model_name_or_path=$MODEL_NAME \&#xA;  --dataset_name=$DATASET_NAME \&#xA;  --use_ema \&#xA;  --resolution=512 --center_crop --random_flip \&#xA;  --train_batch_size=1 \&#xA;  --gradient_accumulation_steps=4 \&#xA;  --gradient_checkpointing \&#xA;  --max_train_steps=15000 \&#xA;  --learning_rate=1e-05 \&#xA;  --max_grad_norm=1 \&#xA;  --lr_scheduler=&#34;constant&#34; --lr_warmup_steps=0 \&#xA;  --output_dir=&#34;sd-laion-art&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To LoRA-train:&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export MODEL_NAME=&#34;segmind/small-sd&#34;&#xA;export DATASET_NAME=&#34;fantasyfish/laion-art&#34;&#xA;&#xA;accelerate launch --mixed_precision=&#34;fp16&#34; lora_training.py \&#xA;  --pretrained_model_name_or_path=$MODEL_NAME \&#xA;  --dataset_name=$DATASET_NAME --caption_column=&#34;text&#34; \&#xA;  --resolution=512 --random_flip \&#xA;  --train_batch_size=1 \&#xA;  --num_train_epochs=100 --checkpointing_steps=5000 \&#xA;  --learning_rate=1e-04 --lr_scheduler=&#34;constant&#34; --lr_warmup_steps=0 \&#xA;  --seed=42 \&#xA;  --output_dir=&#34;fantasyfish/laion-art&#34; \&#xA;  --validation_prompt=&#34;A man in a suit&#34; --report_to=&#34;wandb&#34;&#xA;  --use_peft \&#xA;  --lora_r=4 --lora_alpha=32 \&#xA;  --lora_text_encoder_r=4 --lora_text_encoder_alpha=32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The latter two scripts are taken from the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;ü§ó diffusers github&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained checkpoints:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The trained &#34;sd-small&#34; version of the model is available at &lt;a href=&#34;https://huggingface.co/segmind/small-sd&#34;&gt;this Huggingface ü§ó repo&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;The trained &#34;sd-tiny&#34; version of the model is available at &lt;a href=&#34;https://huggingface.co/segmind/tiny-sd&#34;&gt;this Huggingface ü§ó repo&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuned version of the &#34;sd-tiny model&#34; on portrait images is available at &lt;a href=&#34;https://huggingface.co/segmind/portrait-finetuned&#34;&gt;this Huggingface ü§ó repo&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Here are some generated examples:&lt;/h2&gt; &#xA;&lt;h3&gt;SD-tiny model fine-tuned on portrait images&lt;/h3&gt; &#xA;&lt;p&gt;Below are some of the images generated with the sd-tiny model, fine-tuned on portrait images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Closeup portrait photo of a 28-year-old man, wearing a rugged leather jacket, with a five o&#39;clock shadow and prominent laugh lines around his eyes, captured in soft, golden hour lighting.&#xA;RAW photo, (closeup:1.2), portrait of a 35-year-old woman, wearing minimal makeup, showcasing her freckles, with a serene expression in a lush botanical garden, illuminated by gentle dappled sunlight.&#xA;High-quality, face portrait photo of a 40-year-old European man, wearing glasses, revealing the fine lines and character on his forehead, while his salt-and-pepper beard adds a touch of sophistication, with a subtle hint of sunlight peeking through dense foliage.&#xA;B&amp;amp;W photo of a 48-year-old woman, shot from the side, highlighting her elegant profile and the delicate lines etched across her cheeks, capturing her in a serene moment on a windy, overcast beach.&#xA;High-quality, closeup portrait photo of a 30-year-old Asian woman, wearing traditional clothing, emphasizing the flawless porcelain-like texture of her skin, with intricate details of her traditional headpiece.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Link to the model -&amp;gt; &lt;a href=&#34;https://huggingface.co/segmind/portrait-finetuned&#34;&gt;Huggingface ü§ó repo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/grid_2.png&#34; alt=&#34;grid_2&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;SD-tiny model LoRA trained on abstract images&lt;/h3&gt; &#xA;&lt;p&gt;Below are some of the images generated with the LoRA trained sd-tiny model, on abstract concepts0.&lt;/p&gt; &#xA;&lt;p&gt;Link to the model -&amp;gt; &lt;a href=&#34;https://huggingface.co/segmind/tiny_lora_mxtun3_style&#34;&gt;Huggingface ü§ó repo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/grid_3.png&#34; alt=&#34;grid_3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Speed comparision of inference on NVIDIA A100 80GB:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/graph.png&#34; alt=&#34;graph.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/segmind/distill-sd/master/assets/comparision1.png&#34; alt=&#34;compare&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Advantages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Upto 100% Faster inferences&lt;/li&gt; &#xA; &lt;li&gt;Upto 30% lower VRAM footprint&lt;/li&gt; &#xA; &lt;li&gt;Faster dreambooth and LoRA training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The distilled models are in early phase and the outputs may not be at a production quality yet.&lt;/li&gt; &#xA; &lt;li&gt;These models may not be the best general models. They are best used as fine-tuned or LoRA trained on specific concepts/styles.&lt;/li&gt; &#xA; &lt;li&gt;Distilled models are not very good at composibility or multiconcepts yet.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Research Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; SDXL distilled models and code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Further fine-tuned SD-1.5 base models for better composibility and generalization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Apply Flash Attention-2 for faster training/fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Apply TensorRT and/or AITemplate for further accelerations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Look at Quantization-Aware-Training(QAT) during distillation process.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This work would not have been possible without Nota AI&#39;s &lt;a href=&#34;https://arxiv.org/pdf/2305.15798.pdf&#34;&gt;paper&lt;/a&gt; on compression models. We express our gratitude for their research work in this area.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors:&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/segmind/distill-sd/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=segmind/distill-sd&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{kim2023architectural,&#xA;  title={On Architectural Compression of Text-to-Image Diffusion Models},&#xA;  author={Kim, Bo-Kyeong and Song, Hyoung-Kyu and Castells, Thibault and Choi, Shinkook},&#xA;  journal={arXiv preprint arXiv:2305.15798},&#xA;  year={2023},&#xA;  url={https://arxiv.org/abs/2305.15798}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Kim_2023_ICMLW,&#xA;  title={BK-SDM: Architecturally Compressed Stable Diffusion for Efficient Text-to-Image Generation},&#xA;  author={Kim, Bo-Kyeong and Song, Hyoung-Kyu and Castells, Thibault and Choi, Shinkook},&#xA;  journal={ICML Workshop on Efficient Systems for Foundation Models (ES-FoMo)},&#xA;  year={2023},&#xA;  url={https://openreview.net/forum?id=bOVydU0XKC}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>