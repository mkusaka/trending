<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-22T01:31:32Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kongzhecn/OMG</title>
    <updated>2024-03-22T01:31:32Z</updated>
    <id>tag:github.com,2024-03-22:/kongzhecn/OMG</id>
    <link href="https://github.com/kongzhecn/OMG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OMG: Occlusion-friendly Personalized Multi-concept Generation In Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;OMG: Occlusion-friendly Personalized Multi-concept Generation In Diffusion Models&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=4X3yLwsAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Zhe Kong&lt;/a&gt; ¬∑ &lt;a href=&#34;https://yzhang2016.github.io/&#34;&gt;Yong Zhang*&lt;/a&gt; ¬∑ &lt;a href=&#34;https://tianyu-yang.com/&#34;&gt;Tianyu Yang&lt;/a&gt; ¬∑ &lt;a href=&#34;https://taowangzj.github.io/&#34;&gt;Tao Wang&lt;/a&gt;¬∑ &lt;a href=&#34;https://zhangkaihao.github.io/&#34;&gt;Kaihao Zhang&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=u7nZ3bgAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Bizhu Wu&lt;/a&gt; ¬∑ &lt;a href=&#34;https://guanyingc.github.io/&#34;&gt;Guanying Chen&lt;/a&gt; ¬∑ &lt;a href=&#34;https://scholar.google.com/citations?user=AjxoEpIAAAAJ&amp;amp;hl=en&#34;&gt;Wei Liu&lt;/a&gt; ¬∑ &lt;a href=&#34;https://whluo.github.io/&#34;&gt;Wenhan Luo*&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;sup&gt;*&lt;/sup&gt;Corresponding Authors&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://kongzhecn.github.io/omg-project/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.10983&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Technique-Report-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kongzhecn/OMG&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/kongzhecn/OMG?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/Fucius/OMG&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;TL; DR:&lt;/strong&gt; OMG is a framework for multi-concept image generation, supporting character and style LoRAs on &lt;a href=&#34;https://civitai.com/&#34;&gt;Civitai.com&lt;/a&gt;. It also can be combined with &lt;a href=&#34;https://github.com/InstantID/InstantID&#34;&gt;InstantID&lt;/a&gt; for multiple IDs with using a single image for each ID.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/teaser.png&#34;&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Introduction of OMG:&lt;/strong&gt; A tool for high-quality multi-character image generation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/5BI_a7nTb8Q&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/5BI_a7nTb8Q/0.jpg&#34; alt=&#34;IMAGE ALT TEXT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Trailor Demo:&lt;/strong&gt; A short trailor &#34;Home Defense&#34; created by using OMG + SVD.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/embed/c-dYmPo7rVM&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/c-dYmPo7rVM/0.jpg&#34; alt=&#34;IMAGE ALT TEXT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üè∑&lt;/span&gt; Change Log&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/3/19] üî• We release the &lt;a href=&#34;https://arxiv.org/abs/2403.10983&#34;&gt;technical report&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/Fucius/OMG&#34;&gt;Hugging Face demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023/3/18] üî• We release the source code and gradio demo of OMG.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîÜ Introduction&lt;/h2&gt; &#xA;&lt;h3&gt;1. OMG + LoRA (ID with multiple images)&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/lora.png&#34; height=&#34;390&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;2. OMG + InstantID (ID with single image)&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/instantid.png&#34; height=&#34;390&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;3. OMG + ControlNet (Layout Control )&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/controlnet.png&#34; height=&#34;1024&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;4. OMG + style LoRAs (Style Control)&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/style.png&#34; height=&#34;390&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üîß&lt;/span&gt; Dependencies and Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The code requires &lt;code&gt;python==3.10.6&lt;/code&gt;, as well as &lt;code&gt;pytorch==2.0.1&lt;/code&gt; and &lt;code&gt;torchvision==0.15.2&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n OMG python=3.10.6&#xA;conda activate OMG&#xA;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2&#xA;pip install -r requirements.txt&#xA;pip install git+https://github.com/facebookresearch/segment-anything.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;For Visual comprehension, you can choose &lt;code&gt;YoloWorld + EfficientViT SAM&lt;/code&gt; or &lt;code&gt;GroundingDINO + SAM&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;(Recommend) YoloWorld + EfficientViT SAM:&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;pip install inference[yolo-world]==0.9.13&#xA;pip install  onnxsim==0.4.35&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;ol start=&#34;2&#34;&gt; &#xA;   &lt;li&gt;(Optional) If you can not install &lt;code&gt;inference[yolo-world]&lt;/code&gt;. You can install &lt;code&gt;GroundingDINO&lt;/code&gt; for visual comprehension.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;GroundingDINO&lt;/code&gt; requires manual installation.&lt;/p&gt; &#xA;&lt;p&gt;Run this so the environment variable will be set under current shell.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;export CUDA_HOME=/path/to/cuda-11.3&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this example, &lt;code&gt;/path/to/cuda-11.3&lt;/code&gt; should be replaced with the path where your CUDA toolkit is installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;git clone https://github.com/IDEA-Research/GroundingDINO.git&#xA;&#xA;cd GroundingDINO/&#xA;&#xA;pip install -e .&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More installation details can be found in &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO#install&#34;&gt;GroundingDINO&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚è¨ Pretrained Model Preparation&lt;/h2&gt; &#xA;&lt;h3&gt;1) DownLoad Models&lt;/h3&gt; &#xA;&lt;h4&gt;1. Required download:&lt;/h4&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;stable-diffusion-xl-base-1.0&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/thibaud/controlnet-openpose-sdxl-1.0&#34;&gt;controlnet-openpose-sdxl-1.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;InstantID + OMG&lt;/code&gt; download: &lt;a href=&#34;https://huggingface.co/InstantX/InstantID/tree/main&#34;&gt;InstantID&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/18wEUfMNohBJ4K3Ly5wpTejPfDzp-8fI8/view?usp=sharing&#34;&gt;antelopev2&lt;/a&gt;,&lt;/p&gt; &#xA;&lt;h4&gt;2. For Visual comprehension, you can choose &#34;YoloWorld + EfficientViT SAM&#34; or &#34;GroundingDINO + SAM&#34;.&lt;/h4&gt; &#xA;&lt;p&gt;For &lt;code&gt;YoloWorld + EfficientViT SAM&lt;/code&gt;: &lt;a href=&#34;https://huggingface.co/han-cai/efficientvit-sam/resolve/main/xl1.pt&#34;&gt;EfficientViT-SAM-XL1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Fucius/OMG/blob/main/yolo-world.pt&#34;&gt;yolo-world&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;GroundingDINO + SAM&lt;/code&gt;: &lt;a href=&#34;https://huggingface.co/ShilongLiu/GroundingDINO&#34;&gt;GroundingDINO&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&#34;&gt;SAM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;3. For Character LoRAs, download at least one character for man and another character for woman.&lt;/h4&gt; &#xA;&lt;p&gt;For &lt;code&gt;Character LoRAs for man&lt;/code&gt;: &lt;a href=&#34;https://civitai.com/models/253793?modelVersionId=286084&#34;&gt;Chris Evans&lt;/a&gt;, &lt;a href=&#34;https://civitai.com/models/309562/gleb-savchenko-dancing-with-the-stars-sdxl&#34;&gt;Gleb Savchenko&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Fucius/OMG/blob/main/lora/Harry_Potter.safetensors&#34;&gt;Harry Potter&lt;/a&gt;, &lt;a href=&#34;https://civitai.com/models/132387/jordan-torres-15-sdxl?modelVersionId=366964&#34;&gt;Jordan Torres&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;Character LoRAs for woman&lt;/code&gt;: &lt;a href=&#34;https://civitai.com/models/164284/taylor-swift?modelVersionId=185041&#34;&gt;Taylor Swift&lt;/a&gt;, &lt;a href=&#34;https://civitai.com/models/122657/jennifer-lawrence-sdxl&#34;&gt;Jennifer Lawrence&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Fucius/OMG/blob/main/lora/Hermione_Granger.safetensors&#34;&gt;Hermione Granger&lt;/a&gt;, &lt;a href=&#34;https://civitai.com/models/172431/keira-knightley-sdxl?modelVersionId=193658&#34;&gt;Keira Knightley&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;4. (Optional) If using ControlNet, download:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/lllyasviel/ControlNet/blob/main/annotator/ckpts/body_pose_model.pth&#34;&gt;ControlNet&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0&#34;&gt;controlnet-canny-sdxl-1.0&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0&#34;&gt;controlnet-depth-sdxl-1.0&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Intel/dpt-hybrid-midas&#34;&gt;dpt-hybrid-midas&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;5. (Optional) If using Style LoRAs, download:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://civitai.com/models/202764/anime-sketch-style-sdxl-and-sd15?modelVersionId=258108&#34;&gt;Anime Sketch Style&lt;/a&gt;, &lt;a href=&#34;https://civitai.com/models/336656?modelVersionId=376999&#34;&gt;Oil Painting Style&lt;/a&gt;, &lt;a href=&#34;https://civitai.com/models/214956/cinematic-photography-style-xl&#34;&gt;Cinematic Photography Style&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;2) Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Put the models under &lt;code&gt;checkpoint&lt;/code&gt; as follow:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;OMG&#xA;‚îú‚îÄ‚îÄ checkpoint&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ antelopev2&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ ControlNet&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ controlnet-openpose-sdxl-1.0&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ controlnet-canny-sdxl-1.0&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ controlnet-depth-sdxl-1.0&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ dpt-hybrid-midas&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ style&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ EldritchPaletteKnife.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ Cinematic Hollywood Film.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ Anime_Sketch_SDXL.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ InstantID&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ GroundingDINO&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ lora&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ chris-evans.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ Harry_Potter.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ Hermione_Granger.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ jordan_torres_v2_xl.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ keira_lora_sdxl_v1-000008.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ lawrence_dh128_v1-step00012000.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ Gleb-Savchenko_Liam-Hemsworth.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ TaylorSwiftSDXL.safetensors&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ sam&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ sam_vit_h_4b8939.pth&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ xl1.pt&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ stable-diffusion-xl-base-1.0&#xA;‚îú‚îÄ‚îÄ gradio_demo&#xA;‚îú‚îÄ‚îÄ src&#xA;‚îú‚îÄ‚îÄ inference_instantid.py&#xA;‚îî‚îÄ‚îÄ inference_lora.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Put &lt;code&gt;ViT-B-32.pt&lt;/code&gt; (download from &lt;a href=&#34;https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt&#34;&gt;openai&lt;/a&gt;) to &lt;code&gt;~/.cache/clip/ViT-B-32.pt&lt;/code&gt;. If using &lt;code&gt;YoloWorld&lt;/code&gt;, put &lt;code&gt;yolo-world.pt&lt;/code&gt; to &lt;code&gt;/tmp/cache/yolo_world/l/yolo-world.pt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Or you can manually set the checkpoint path as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_lora.py  \&#xA;--pretrained_sdxl_model &amp;lt;path to stable-diffusion-xl-base-1.0&amp;gt; \&#xA;--controlnet_checkpoint &amp;lt;path to controlnet-openpose-sdxl-1.0&amp;gt; \&#xA;--efficientViT_checkpoint &amp;lt;path to efficientViT-SAM-XL1&amp;gt; \&#xA;--dino_checkpoint &amp;lt;path to GroundingDINO&amp;gt; \&#xA;--sam_checkpoint &amp;lt;path to sam&amp;gt; \&#xA;--lora_path &amp;lt;Lora path to character1|Lora path to character1&amp;gt; \&#xA;--style_lora &amp;lt;Path to style LoRA&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For OMG + InstantID:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_instantid.py  \&#xA;--pretrained_model &amp;lt;path to stable-diffusion-xl-base-1.0&amp;gt; \&#xA;--controlnet_path &amp;lt;path to InstantID controlnet&amp;gt; \&#xA;--face_adapter_path &amp;lt;path to InstantID face adapter&amp;gt; \&#xA;--efficientViT_checkpoint &amp;lt;path to efficientViT-SAM-XL1&amp;gt; \&#xA;--dino_checkpoint &amp;lt;path to GroundingDINO&amp;gt; \&#xA;--sam_checkpoint &amp;lt;path to sam&amp;gt; \&#xA;--antelopev2_path &amp;lt;path to antelopev2&amp;gt; \&#xA;--style_lora &amp;lt;Path to style LoRA&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üíª&lt;/span&gt; Usage&lt;/h2&gt; &#xA;&lt;h3&gt;1: OMG + LoRA&lt;/h3&gt; &#xA;&lt;p&gt;The &amp;lt;TOK&amp;gt; for &lt;code&gt;Harry_Potter.safetensors&lt;/code&gt; is &lt;code&gt;Harry Potter&lt;/code&gt; and for &lt;code&gt;Hermione_Granger.safetensors&lt;/code&gt; is &lt;code&gt;Hermione Granger&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For visual comprehension, you can set &lt;code&gt;--segment_type &#39;yoloworld&#39;&lt;/code&gt; for &lt;code&gt;YoloWorld + EfficientViT SAM&lt;/code&gt;, or &lt;code&gt;--segment_type &#39;GroundingDINO&#39;&lt;/code&gt; for &lt;code&gt;GroundingDINO + SAM&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_lora.py \&#xA;    --prompt &amp;lt;prompt for the two person&amp;gt; \&#xA;    --negative_prompt &amp;lt;negative prompt&amp;gt; \&#xA;    --prompt_rewrite &#34;[&amp;lt;prompt for person 1&amp;gt;]-*-[&amp;lt;negative prompt&amp;gt;]|[&amp;lt;prompt for person 2&amp;gt;]-*-[negative prompt]&#34; \&#xA;    --lora_path &#34;[&amp;lt;Lora path for character1|Lora path for character1&amp;gt;]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_lora.py \&#xA;    --prompt &#34;Close-up photo of the happy smiles on the faces of the cool man and beautiful woman as they leave the island with the treasure, sail back to the vacation beach, and begin their love story, 35mm photograph, film, professional, 4k, highly detailed.&#34; \&#xA;    --negative_prompt &#39;noisy, blurry, soft, deformed, ugly&#39; \&#xA;    --prompt_rewrite &#39;[Close-up photo of the Chris Evans in surprised expressions as he wear Hogwarts uniform, 35mm photograph, film, professional, 4k, highly detailed.]-*-[noisy, blurry, soft, deformed, ugly]|[Close-up photo of the TaylorSwift in surprised expressions as she wear Hogwarts uniform, 35mm photograph, film, professional, 4k, highly detailed.]-*-[noisy, blurry, soft, deformed, ugly]&#39; \&#xA;    --lora_path &#39;./checkpoint/lora/chris-evans.safetensors|./checkpoint/lora/TaylorSwiftSDXL.safetensors&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For OMG + LoRA + ControlNet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_lora.py \&#xA;    --prompt &#34;Close-up photo of the happy smiles on the faces of the cool man and beautiful woman as they leave the island with the treasure, sail back to the vacation beach, and begin their love story, 35mm photograph, film, professional, 4k, highly detailed.&#34; \&#xA;    --negative_prompt &#39;noisy, blurry, soft, deformed, ugly&#39; \&#xA;    --prompt_rewrite &#39;[Close-up photo of the Chris Evans in surprised expressions as he wear Hogwarts uniform, 35mm photograph, film, professional, 4k, highly detailed.]-*-[noisy, blurry, soft, deformed, ugly]|[Close-up photo of the TaylorSwift in surprised expressions as she wear Hogwarts uniform, 35mm photograph, film, professional, 4k, highly detailed.]-*-[noisy, blurry, soft, deformed, ugly]&#39; \&#xA;    --lora_path &#39;./checkpoint/lora/chris-evans.safetensors|./checkpoint/lora/TaylorSwiftSDXL.safetensors&#39; \&#xA;    --spatial_condition &#39;./example/pose.png&#39; \&#xA;    --controlnet_checkpoint &#39;./checkpoint/controlnet-openpose-sdxl-1.0&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For OMG + LoRA + Style:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_lora.py \&#xA;    --prompt &#34;Close-up photo of the happy smiles on the faces of the cool man and beautiful woman as they leave the island with the treasure, sail back to the vacation beach, and begin their love story, 35mm photograph, film, professional, 4k, highly detailed, Pencil_Sketch:1.2, messy lines, greyscale, traditional media, sketch.&#34; \&#xA;    --negative_prompt &#39;noisy, blurry, soft, deformed, ugly&#39; \&#xA;    --prompt_rewrite &#39;[Close-up photo of the Chris Evans in surprised expressions as he wear Hogwarts uniform, 35mm photograph, film, professional, 4k, highly detailed, Pencil_Sketch:1.2, messy lines, greyscale, traditional media, sketch.]-*-[noisy, blurry, soft, deformed, ugly]|[Close-up photo of the TaylorSwift in surprised expressions as she wear Hogwarts uniform, 35mm photograph, film, professional, 4k, highly detailed, Pencil_Sketch:1.2, messy lines, greyscale, traditional media, sketch.]-*-[noisy, blurry, soft, deformed, ugly]&#39; \&#xA;    --lora_path &#39;./checkpoint/lora/chris-evans.safetensors|./checkpoint/lora/TaylorSwiftSDXL.safetensors&#39; \&#xA;    --style_lora &#39;./checkpoint/style/Anime_Sketch_SDXL.safetensors&#39; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2: OMG + InstantID&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_instantid.py \&#xA;    --prompt &amp;lt;prompt for the two person&amp;gt; \&#xA;    --negative_prompt &amp;lt;negative prompt&amp;gt; \&#xA;    --prompt_rewrite &#34;[&amp;lt;prompt for person 1&amp;gt;]-*-[&amp;lt;negative prompt&amp;gt;]-*-&amp;lt;path to reference image1&amp;gt;|[&amp;lt;prompt for person 2&amp;gt;]-*-[negative prompt]-*-&amp;lt;path to reference image2&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_instantid.py \&#xA;    --prompt &#39;Close-up photo of the happy smiles on the faces of the cool man and beautiful woman as they leave the island with the treasure, sail back to the vacation beach, and begin their love story, 35mm photograph, film, professional, 4k, highly detailed.&#39; \&#xA;    --negative_prompt &#39;noisy, blurry, soft, deformed, ugly&#39; \&#xA;    --prompt_rewrite &#39;[Close-up photo of the a man, 35mm photograph, professional, 4k, highly detailed.]-*-[noisy, blurry, soft, deformed, ugly]-*-./example/chris-evans.jpg|[Close-up photo of the a woman, 35mm photograph, professional, 4k, highly detailed.]-*-[noisy, blurry, soft, deformed, ugly]-*-./example/TaylorSwift.png&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Local gradio demo with OMG + LoRA&lt;/h3&gt; &#xA;&lt;p&gt;If you choose &lt;code&gt;YoloWorld + EfficientViT SAM&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_demo/app.py --segment_type yoloworld&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;GroundingDINO + SAM&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_demo/app.py --segment_type GroundingDINO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Connect to the public URL displayed after the startup process is completed.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Kiteretsu77/APISR</title>
    <updated>2024-03-22T01:31:32Z</updated>
    <id>tag:github.com,2024-03-22:/Kiteretsu77/APISR</id>
    <link href="https://github.com/Kiteretsu77/APISR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;APISR: Anime Production Inspired Real-World Anime Super-Resolution (CVPR 2024)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/logo.png&#34; height=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;APISR: Anime Production Inspired Real-World Anime Super-Resolution (CVPR 2024)&lt;/h2&gt; &#xA;&lt;p&gt;APISR aims at restoring and enhancing low-quality low-resolution anime images and video sources with various degradations from real-world scenarios.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.01598&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;Arxiv&#34;&gt;&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://huggingface.co/spaces/HikariDawn/APISR&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=HuggingFace&amp;amp;color=orange&#34; alt=&#34;HF Demo&#34;&gt;&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://colab.research.google.com/github/camenduru/APISR-jupyter/blob/main/APISR_jupyter.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üëÄ &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/#Visualization&#34;&gt;&lt;strong&gt;Visualization&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üî• &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/#Update&#34;&gt;Update&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üîß &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/#installation&#34;&gt;Installation&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üè∞ &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/docs/model_zoo.md&#34;&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; ‚ö° &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/#inference&#34;&gt;Inference&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üß© &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/#dataset_curation&#34;&gt;Dataset Curation&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üíª &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/#train&#34;&gt;Train&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/workflow.png&#34; style=&#34;border-radius: 15px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; If you like APISR, please help star this repo. Thanks! &lt;span&gt;ü§ó&lt;/span&gt;&lt;/p&gt; &#xA;&lt;!-- Visualization ----------------------------------------&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;Visualization&#34;&gt;&lt;/a&gt; Visualization (Click them for the best view!) üëÄ&lt;/h2&gt; &#xA;&lt;!-- Kiteret: https://imgsli.com/MjQ1NzE0 --&gt; &#xA;&lt;!-- EVA: https://imgsli.com/MjQ1NzIx --&gt; &#xA;&lt;!-- Pokemon: https://imgsli.com/MjQ1NzIy --&gt; &#xA;&lt;!-- Pokemon2: https://imgsli.com/MjQ1NzM5 --&gt; &#xA;&lt;!-- Gundam0079: https://imgsli.com/MjQ1NzIz --&gt; &#xA;&lt;!-- Gundam0079 #2: https://imgsli.com/MjQ1NzMw --&gt; &#xA;&lt;!-- f91: https://imgsli.com/MjQ1NzMx --&gt; &#xA;&lt;!-- wataru: https://imgsli.com/MjQ1NzMy --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MjQ1NzIz&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/0079_visual.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MjQ1NzMw&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/0079_2_visual.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MjQ1NzIy&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/pokemon_visual.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MjQ1NzM5&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/pokemon2_visual.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MjQ1NzIx&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/eva_visual.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MjQ1NzE0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/kiteret_visual.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MjQ1NzMx&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/f91_visual.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MjQ1NzMy&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/wataru_visual.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/AVC_RealLQ_comparison.png&#34;&gt; &lt;/p&gt; &#xA;&lt;!--  ---------------------------------------------------&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;Update&#34;&gt;&lt;/a&gt;Update üî•üî•üî•&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release Paper version implementation of APISR&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release different upscaler factor weight (for 2x, 4x and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio demo (with online)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Provide weight with different architecture&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create a Project Page&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;installation&#34;&gt;&lt;/a&gt; Installation üîß&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:Kiteretsu77/APISR.git&#xA;cd APISR&#xA;&#xA;# Create conda env&#xA;conda create -n APISR python=3.10&#xA;conda activate APISR&#xA;&#xA;# Install Pytorch and other packages needed&#xA;pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118&#xA;pip install -r requirements.txt&#xA;&#xA;&#xA;# To be absolutely sure that the tensorboard can execute. I recommend the following CMD from &#34;https://github.com/pytorch/pytorch/issues/22676#issuecomment-534882021&#34;&#xA;pip uninstall tb-nightly tensorboard tensorflow-estimator tensorflow-gpu tf-estimator-nightly&#xA;pip install tensorflow&#xA;&#xA;# Install FFMPEG [Only needed for training and dataset curation stage; inference only does not need ffmpeg] (the following is for the linux system, Windows users can download ffmpeg from https://ffmpeg.org/download.html)&#xA;sudo apt install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;inference&#34;&gt;&lt;/a&gt; Gradio Fast Inference ‚ö°‚ö°‚ö°&lt;/h2&gt; &#xA;&lt;p&gt;Gradio option doesn&#39;t need to prepare the weight from the user side but they can only process one image each time.&lt;/p&gt; &#xA;&lt;p&gt;Online demo can be found at &lt;a href=&#34;https://huggingface.co/spaces/HikariDawn/APISR&#34;&gt;https://huggingface.co/spaces/HikariDawn/APISR&lt;/a&gt; (HuggingFace) or &lt;a href=&#34;https://colab.research.google.com/github/camenduru/APISR-jupyter/blob/main/APISR_jupyter.ipynb&#34;&gt;https://colab.research.google.com/github/camenduru/APISR-jupyter/blob/main/APISR_jupyter.ipynb&lt;/a&gt; (Colab)&lt;/p&gt; &#xA;&lt;p&gt;Local Gradio can be created by running the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python gradio_apisr.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;regular_inference&#34;&gt;&lt;/a&gt; Regular Inference ‚ö°‚ö°&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the model weight from &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/docs/model_zoo.md&#34;&gt;&lt;strong&gt;model zoo&lt;/strong&gt;&lt;/a&gt; and &lt;strong&gt;put the weight to &#34;pretrained&#34; folder&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then, Execute&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python test_code/inference.py --input_dir XXX  --weight_path XXX  --store_dir XXX&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the weight you download is paper weight, the default argument of test_code/inference.py is capable of executing sample images from &#34;&lt;strong&gt;assets&lt;/strong&gt;&#34; folder&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;dataset_curation&#34;&gt;&lt;/a&gt; Dataset Curation üß©&lt;/h2&gt; &#xA;&lt;p&gt;Our dataset curation pipeline is under &lt;strong&gt;dataset_curation_pipeline&lt;/strong&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;You can collect your own dataset by sending videos into the pipeline and get the least compressed and the most informative images from the video sources.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download &lt;a href=&#34;https://github.com/tinglyfeng/IC9600?tab=readme-ov-file&#34;&gt;IC9600&lt;/a&gt; weight (ck.pth) from &lt;a href=&#34;https://drive.google.com/drive/folders/1N3FSS91e7FkJWUKqT96y_zcsG9CRuIJw&#34;&gt;https://drive.google.com/drive/folders/1N3FSS91e7FkJWUKqT96y_zcsG9CRuIJw&lt;/a&gt; and place it at &#34;pretrained/&#34; folder (else, you can define a different &lt;strong&gt;--IC9600_pretrained_weight_path&lt;/strong&gt; in the following collect.py execution)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;With a folder with video sources, you can execute the following to get a basic dataset (with &lt;strong&gt;ffmpeg&lt;/strong&gt; installed):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dataset_curation_pipeline/collect.py --video_folder_dir XXX --save_dir XXX&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once you get an image dataset with various aspect ratios and resolutions, you can run the following scripts&lt;/p&gt; &lt;p&gt;Be careful to check &lt;strong&gt;full_patch_source&lt;/strong&gt; &amp;amp;&amp;amp; &lt;strong&gt;degrade_hr_dataset_path&lt;/strong&gt; &amp;amp;&amp;amp; &lt;strong&gt;train_hr_dataset_path&lt;/strong&gt; (we will use these path in &lt;strong&gt;opt.py&lt;/strong&gt; setting during training stage)&lt;/p&gt; &lt;p&gt;In order to decrease memory utilization and increase training efficiency, we pre-process all time-consuming pseudo-GT (&lt;strong&gt;train_hr_dataset_path&lt;/strong&gt;) at the dataset preparation stage.&lt;/p&gt; &lt;p&gt;But in order to create a natural input for prediction-oriented compression, in every epoch, the degradation started from the uncropped GT (&lt;strong&gt;full_patch_source&lt;/strong&gt;), and LR synthetic images are concurrently stored. The cropped HR GT dataset (&lt;strong&gt;degrade_hr_dataset_path&lt;/strong&gt;) and cropped pseudo-GT (&lt;strong&gt;train_hr_dataset_path&lt;/strong&gt;) are fixed in the dataset preparation stage and won&#39;t be modified during training.&lt;/p&gt; &lt;p&gt;Be careful to check if there is any OOM. If there is, it will be impossible to get correct dataset preparation. Usually, this is because &lt;strong&gt;num_workers&lt;/strong&gt; in &lt;strong&gt;scripts/anime_strong_usm.py&lt;/strong&gt; is too big!&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/prepare_datasets.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;train&#34;&gt;&lt;/a&gt; Train üíª&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;The whole training process can be done in one RTX3090/4090!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Prepare a dataset (AVC/API) which follows step 2 &amp;amp; 3 in &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/#dataset_curation&#34;&gt;&lt;strong&gt;Dataset Curation&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In total, you will have 3 folders prepared before executing the following commands:&lt;/p&gt; &lt;p&gt;--&amp;gt; &lt;strong&gt;full_patch_source&lt;/strong&gt;: uncropped GT&lt;/p&gt; &lt;p&gt;--&amp;gt; &lt;strong&gt;degrade_hr_dataset_path&lt;/strong&gt;: cropped GT&lt;/p&gt; &lt;p&gt;--&amp;gt; &lt;strong&gt;train_hr_dataset_path&lt;/strong&gt;: cropped Pseudo-GT&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train: Please check &lt;strong&gt;opt.py&lt;/strong&gt; carefully to set the hyperparameters you want (modifying &lt;strong&gt;Frequently Changed Setting&lt;/strong&gt; is usually enough).&lt;/p&gt; &lt;p&gt;When you execute the following, we will create a &#34;&lt;strong&gt;tmp&lt;/strong&gt;&#34; folder to hold generated lr images for sanity check. You can modify the code to delete it if you want.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step1&lt;/strong&gt; (Net &lt;strong&gt;L1&lt;/strong&gt; loss training): Run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_code/train.py &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The trained model weights will be inside the folder &#39;saved_models&#39; (same as checkpoints)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step2&lt;/strong&gt; (GAN &lt;strong&gt;Adversarial&lt;/strong&gt; Training):&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt; &lt;p&gt;Change opt[&#39;architecture&#39;] in &lt;strong&gt;opt.py&lt;/strong&gt; to &#34;GRLGAN&#34; and change &lt;strong&gt;batch size&lt;/strong&gt; if you need. BTW, I don&#39;t think that, for personal training, it is needed to train 300K iter for GAN. I did that in order to follow the same setting as in AnimeSR and VQDSR, but &lt;strong&gt;100K ~ 130K&lt;/strong&gt; should have a decent visual result.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Following previous works, GAN should start from L1 loss pre-trained network, so please carry a &lt;strong&gt;pretrained_path&lt;/strong&gt; (the default path below should be fine)&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_code/train.py --pretrained_path saved_models/grl_best_generator.pth &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite us if our work is useful for your research.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wang2024apisr,&#xA;  title={APISR: Anime Production Inspired Real-World Anime Super-Resolution},&#xA;  author={Wang, Boyang and Yang, Fengyu and Yu, Xihang and Zhang, Chao and Zhao, Hanbin},&#xA;  journal={arXiv preprint arXiv:2403.01598},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project is released for academic use only. We disclaim responsibility for the distribution of the dataset. Users are solely liable for their actions. The project contributors are not legally affiliated with, nor accountable for, users&#39; behaviors.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/Kiteretsu77/APISR/main/LICENSE&#34;&gt;GPL 3.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please feel free to contact me at &lt;a href=&#34;mailto:hikaridawn412316@gmail.com&#34;&gt;hikaridawn412316@gmail.com&lt;/a&gt; or &lt;a href=&#34;mailto:boyangwa@umich.edu&#34;&gt;boyangwa@umich.edu&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üß© Projects that use APISR&lt;/h2&gt; &#xA;&lt;p&gt;If you develop/use APISR in your projects, welcome to let me know. I will write all of them here. Thanks!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast Anime SR acceleration: &lt;a href=&#34;https://github.com/Kiteretsu77/FAST_Anime_VSR&#34;&gt;https://github.com/Kiteretsu77/FAST_Anime_VSR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ComfyUI: &lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-APISR&#34;&gt;https://github.com/ZHO-ZHO-ZHO/ComfyUI-APISR&lt;/a&gt; and &lt;a href=&#34;https://github.com/kijai/ComfyUI-APISR&#34;&gt;https://github.com/kijai/ComfyUI-APISR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Jupyter Notebook: &lt;a href=&#34;https://github.com/camenduru/APISR-jupyter&#34;&gt;https://github.com/camenduru/APISR-jupyter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ó Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Kiteretsu77/VCISR-official&#34;&gt;VCISR&lt;/a&gt;: My code base is based on my previous paper (WACV 2024).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tinglyfeng/IC9600&#34;&gt;IC9600&lt;/a&gt;: The dataset curation pipeline uses IC9600 code to score image complexity level.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/camenduru/APISR-jupyter&#34;&gt;Jupyter Demo&lt;/a&gt;: The jupter notebook demo is from &lt;a href=&#34;https://github.com/camenduru&#34;&gt;camenduru&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mrphrazer/reverser_ai</title>
    <updated>2024-03-22T01:31:32Z</updated>
    <id>tag:github.com,2024-03-22:/mrphrazer/reverser_ai</id>
    <link href="https://github.com/mrphrazer/reverser_ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Provides automated reverse engineering assistance through the use of local large language models (LLMs) on consumer hardware.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ReverserAI (v1.0.1)&lt;/h1&gt; &#xA;&lt;p&gt;Author: &lt;strong&gt;Tim Blazytko&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Provides automated reverse engineering assistance through the use of local large language models (LLMs) on consumer hardware.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Description:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;ReverserAI&lt;/em&gt; is a research project designed to automate and enhance reverse engineering tasks through the use of locally-hosted large language models (LLMs). Operating entirely offline, this initial release features the automatic suggestion of high-level, semantically meaningful function names derived from decompiler output. &lt;em&gt;ReverserAI&lt;/em&gt; is provided as a Binary Ninja plugin; however, its architecture is designed to be extended to other reverse engineering platforms such as IDA and Ghidra.&lt;/p&gt; &#xA;&lt;p&gt;While local LLMs do not match the performance and capabilities of their cloud-based counterparts like ChatGPT4 and require substantial computing resources, they represent a significant step forward in balancing performance with confidentiality requirements.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;ReverserAI&lt;/em&gt; serves as an initial exploration into the potential of local LLMs as aids in reverse engineering on consumer-grade hardware. It showcases what is currently achievable and plans to be a playground for future developments in the realm of AI-assisted reverse engineering.&lt;/p&gt; &#xA;&lt;p&gt;Some example use cases can be found in &lt;a href=&#34;https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/examples&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Disclaimer: My expertise in machine learning and LLMs is limited. There may exist more efficient models or methods to achieve similar tasks with greater performance. This project represents a culmination of research into viable configurations, offering a stable foundation with acceptable performance. Feedback and contributions to improve &lt;em&gt;ReverserAI&lt;/em&gt; are highly encouraged.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Core Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Offline Operation&lt;/strong&gt;: Runs LLMs entirely on local CPU/GPU, ensuring data privacy and security.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automatic Function Naming&lt;/strong&gt;: Automatically suggests semantically meaningful function names from decompiler output.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Binary Ninja Integration&lt;/strong&gt;: Seamlessly integrates as a plugin with Binary Ninja.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Architecture&lt;/strong&gt;: Designed for easy extension to support other reverse engineering tools like IDA and Ghidra.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consumer Hardware Compatibility&lt;/strong&gt;: Optimized to run on consumer-grade hardware, such as Apple silicon architectures.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;ReverserAI&lt;/em&gt; can be easily integrated via Binary Ninja&#39;s plugin manager. Alternatively, for those preferring command line installation, execute in Binary Ninja&#39;s &lt;code&gt;plugins&lt;/code&gt; folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/mrphrazer/reverser_ai.git&#xA;cd reverser_ai&#xA;&#xA;# install requirements&#xA;pip3 install -r requirements.txt&#xA;&#xA;# install ReverserAI&#xA;pip3 install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Upon initial launch, the tool will automatically download the &lt;code&gt;mistral-7b-instruct-v0.2.Q4_K_M.gguf&lt;/code&gt; large language model file (~5GB). The download time varies based on internet connection speed. To manually initiate the download, execute the &lt;a href=&#34;https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/scripts/model_download.py&#34;&gt;&lt;code&gt;model_download.py&lt;/code&gt;&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;h2&gt;Hardware Requirements&lt;/h2&gt; &#xA;&lt;p&gt;For optimal LLM performance on consumer-grade hardware, a setup with multiple CPU threads or a powerful GPU is advised. &lt;em&gt;ReverserAI&lt;/em&gt; runs efficiently on systems with at least 16 GB of RAM and 12 CPU threads, with queries taking about 20 to 30 seconds. GPU optimizations, especially on Apple silicon devices, can reduce this to 2 to 5 seconds per query.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;ReverserAI&lt;/em&gt; is accessible through Binary Ninja&#39;s user interface and via command line.&lt;/p&gt; &#xA;&lt;h3&gt;User Interface&lt;/h3&gt; &#xA;&lt;p&gt;To invoke the plugin within Binary Ninja, navigate to &lt;code&gt;Plugins -&amp;gt; ReverserAI&lt;/code&gt; and, for example, run &#34;Rename All Functions&#34;:&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img alt=&#34;Plugin Menu&#34; src=&#34;https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/imgs/plugin_menu.png&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Depending on the total number of functions in the binary, this may take a while. The AI-assisted function name suggestions will appear in the Log window:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Binary Ninja Log&#34; src=&#34;https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/imgs/plugin_results.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Configuring &lt;em&gt;ReverserAI&lt;/em&gt; to match your hardware setup optimizes its performance. Key configuration parameters include CPU and GPU utilization preferences: For powerful GPUs, configure &lt;em&gt;ReverserAI&lt;/em&gt; to primarily use GPU, reducing CPU threads to minimize overhead. Without a strong GPU, increase CPU thread usage to maximize processing power. For systems with balanced resources, allocate tasks between CPU and GPU for efficient operation. Further details on these parameters follow:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;use_mmap&lt;/code&gt;: Enables loading the entire model into memory (~5GB) when set to &lt;code&gt;true&lt;/code&gt;. Recommended for performance improvement.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;n_threads&lt;/code&gt;: Specifies the number of CPU threads to utilize. Maximize CPU thread count to the number of available CPU threads for full utilization, or set to 0 to disable.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;n_gpu_layers&lt;/code&gt;: Determines GPU layer usage. Enter values up to 99 for powerful GPUs, or 0 to disable GPU processing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;seed&lt;/code&gt;: A fixed seed ensures deterministic behavior for debugging (consistent output for identical inputs). Modify the seed for varied responses.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;verbose&lt;/code&gt;: Enabling &lt;code&gt;verbose&lt;/code&gt; mode provides detailed logs about the model and configuration settings.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The default configuration prioritizes GPU performance and minimizes verbose output.&lt;/p&gt; &#xA;&lt;h4&gt;Binary Ninja&lt;/h4&gt; &#xA;&lt;p&gt;To adjust settings in Binary Ninja, open &lt;code&gt;Settings&lt;/code&gt; and search for &lt;code&gt;reverser_ai&lt;/code&gt;. Changes require Binary Ninja to be restarted.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Plugin Settings&#34; src=&#34;https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/imgs/plugin_settings.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Each change requires a restart of Binary Ninja.&lt;/p&gt; &#xA;&lt;h4&gt;Parameter Tuning&lt;/h4&gt; &#xA;&lt;p&gt;For detailed parameter adjustment, utilize the &lt;a href=&#34;https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/scripts/gpt_function_namer.py&#34;&gt;&lt;code&gt;gpt_function_namer.py&lt;/code&gt;&lt;/a&gt; script with a configuration file, starting with the provided &lt;a href=&#34;https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/example_config.toml&#34;&gt;&lt;code&gt;example_config.toml&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ time python3 scripts/gpt_function_namer.py example_config.toml&#xA;Suggested name: xor_two_numbers&#xA;&#xA;real&#x9;0m1.550s&#xA;user&#x9;0m0.268s&#xA;sys&#x9;0m0.223s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Code Organization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;ReverserAI&lt;/em&gt;&#39;s codebase maintains a clear separation between generic LLM functionalities and tool-specific integration, ensuring modularity and ease of extension. Below is an overview of the primary components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;gpt&lt;/code&gt; Folder&lt;/strong&gt;: Contains code for interacting with large language models (LLMs). This includes:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A generic agent (&lt;code&gt;agent.py&lt;/code&gt;) for model-agnostic operations.&lt;/li&gt; &#xA;   &lt;li&gt;A specialized module (&lt;code&gt;function_name_gpt.py&lt;/code&gt;) for generating function name suggestions.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;binary_ninja&lt;/code&gt; Folder&lt;/strong&gt;: Hosts wrapper instances that:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Utilize Binary Ninja features to produce decompiler outputs.&lt;/li&gt; &#xA;   &lt;li&gt;Interface with the &lt;code&gt;gpt&lt;/code&gt; folder&#39;s agents, enabling LLM-powered function naming within Binary Ninja.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Limitations and Future Work&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;ReverserAI&lt;/em&gt; serves as a proof of concept that demonstrates the potential of leveraging local LLMs for reverse engineering tasks on consumer-grade hardware. Currently, its primary functionality is to offer function name suggestions, but there exists significant scope for enhancement and expansion. Future directions could include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Investigating additional interaction methods and parameters with LLMs to enhance quality and processing speed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Adding network communication for hosting the &lt;em&gt;ReverserAI&lt;/em&gt; agent on a powerful server, circumventing local hardware constraints.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fine-tuning existing models or developing specialized models tailored to reverse engineering needs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Expanding functionality to include code explanations, analysis, and bug detection, subject to scalability and feasibility.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Extending support to other reverse engineering platforms such as IDA and Ghidra.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This project welcomes further contributions, suggestions, and enhancements, including pull requests.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For more information, contact &lt;a href=&#34;https://twitter.com/mr_phrazer&#34;&gt;@mr_phrazer&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>