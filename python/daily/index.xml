<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-01T01:36:44Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>k4yt3x/video2x</title>
    <updated>2022-09-01T01:36:44Z</updated>
    <id>tag:github.com,2022-09-01:/k4yt3x/video2x</id>
    <link href="https://github.com/k4yt3x/video2x" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A lossless video/GIF/image upscaler achieved with waifu2x, Anime4K, SRMD and RealSR. Started in Hack the Valley 2, 2018.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/21986859/102733190-872a7880-4334-11eb-8e9e-0ca747f130b1.png&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/k4yt3x/video2x?style=flat-square&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/k4yt3x/video2x/CI?label=CI&amp;amp;style=flat-square&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/downloads/k4yt3x/video2x/total?style=flat-square&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/k4yt3x/video2x?style=flat-square&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/dynamic/json?color=%23e85b46&amp;amp;label=Patreon&amp;amp;query=data.attributes.patron_count&amp;amp;suffix=%20patrons&amp;amp;url=https%3A%2F%2Fwww.patreon.com%2Fapi%2Fcampaigns%2F4507807&amp;amp;style=flat-square&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://t.me/video2x&#34;&gt;üí¨ Telegram Discussion Group&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Join our Telegram discussion group to ask any questions you have about Video2X, chat directly with the developers, or discuss about upscaling technologies and the future of Video2X in general.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/k4yt3x/video2x/releases/tag/4.8.1&#34;&gt;ü™ü Download Windows Releases&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The latest Windows update is built based on version 4.8.1. GUI is not available for 5.0.0 yet, but is already under development. Go to the &lt;a href=&#34;https://github.com/k4yt3x/video2x/wiki/GUI&#34;&gt;GUI&lt;/a&gt; page to see the basic usages of the GUI. Try the &lt;a href=&#34;https://files.k4yt3x.com/Projects/Video2X/latest&#34;&gt;mirror&lt;/a&gt; if you can&#39;t download releases directly from GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1gWEwcA9y57EsxwOjmLNmNMXPsafw0kGo&#34;&gt;üìî Google Colab&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;You can use Video2X on &lt;a href=&#34;https://colab.research.google.com/&#34;&gt;Google Colab&lt;/a&gt; &lt;strong&gt;for free&lt;/strong&gt; if you don&#39;t have a powerful GPU of your own. You can borrow a powerful GPU (Tesla K80, T4, P4, or P100) on Google&#39;s server for free for a maximum of 12 hours per session. &lt;strong&gt;Please use the free resource fairly&lt;/strong&gt; and do not create sessions back-to-back and run upscaling 24/7. This might result in you getting banned. You can get &lt;a href=&#34;https://colab.research.google.com/signup/pricing&#34;&gt;Colab Pro/Pro+&lt;/a&gt; if you&#39;d like to use better GPUs and get longer runtimes. Usage instructions are embedded in the &lt;a href=&#34;https://colab.research.google.com/drive/1gWEwcA9y57EsxwOjmLNmNMXPsafw0kGo&#34;&gt;Colab Notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/k4yt3x/video2x/actions/workflows/ci.yml&#34;&gt;üåô Download Nightly Releases&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Nightly releases are automatically created by the GitHub Actions CI/CD pipelines. They usually contain more experimental features and bug fixes. However, they are much less stable to the stable releases. &lt;strong&gt;You must log in to GitHub to download CI build artifacts.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/k4yt3x/video2x/pkgs/container/video2x&#34;&gt;üì¶ Container Image&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Video2X container images are available on the GitHub Container Registry for easy deployment on Linux and macOS. If you already have Docker/Podman installed, only one command is needed to start upscaling a video. For more information on how to use Video2X&#39;s Docker image, please refer to the &lt;a href=&#34;https://github.com/K4YT3X/video2x/wiki/Container&#34;&gt;documentations&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/k4yt3x/video2x/wiki&#34;&gt;üìñ Documentations&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Video2X&#39;s documentations are hosted on this repository&#39;s &lt;a href=&#34;https://github.com/k4yt3x/video2x/wiki&#34;&gt;Wiki page&lt;/a&gt;. It includes comprehensive explanations for how to use the &lt;a href=&#34;https://github.com/k4yt3x/video2x/wiki/GUI&#34;&gt;GUI&lt;/a&gt;, the &lt;a href=&#34;https://github.com/k4yt3x/video2x/wiki/CLI&#34;&gt;CLI&lt;/a&gt;, the &lt;a href=&#34;https://github.com/K4YT3X/video2x/wiki/Container&#34;&gt;container image&lt;/a&gt;, the &lt;a href=&#34;https://github.com/k4yt3x/video2x/wiki/Library&#34;&gt;library&lt;/a&gt;, and more. The Wiki is open to edits by the community, so you, yes you, can also correct errors or add new contents to the documentations.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Video2X is a video/GIF/image upscaling and frame interpolation software written in Python. It can use these following state-of-the-art algorithms to increase the resolution and frame rate of your video/GIF/image. More information about the algorithms that it supports can be found in &lt;a href=&#34;https://github.com/k4yt3x/video2x/wiki/Algorithms&#34;&gt;the documentations&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Video Upscaling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/21986859/49412428-65083280-f73a-11e8-8237-bb34158a545e.png&#34; alt=&#34;Spirited Away Demo&#34;&gt;&lt;br&gt; &lt;em&gt;Upscale demo: Spirited Away&#39;s movie trailer&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spirited Away&lt;/strong&gt;: &lt;a href=&#34;https://youtu.be/mGEfasQl2Zo&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://www.bilibili.com/video/BV1V5411471i/&#34;&gt;Bilibili&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;360P to 4K&lt;/li&gt; &#xA;   &lt;li&gt;The &lt;a href=&#34;https://www.youtube.com/watch?v=ByXuk9QqQkk&#34;&gt;original video&lt;/a&gt;&#39;s copyright belongs to Ê†™Âºè‰ºöÁ§æ„Çπ„Çø„Ç∏„Ç™„Ç∏„Éñ„É™&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bad Apple!!&lt;/strong&gt;: &lt;a href=&#34;https://youtu.be/A81rW_FI3cw&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://www.bilibili.com/video/BV16K411K7ue&#34;&gt;Bilibili&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;384P to 4K 120FPS&lt;/li&gt; &#xA;   &lt;li&gt;The &lt;a href=&#34;https://www.nicovideo.jp/watch/sm8628149&#34;&gt;original video&lt;/a&gt;&#39;s copyright belongs to „ÅÇ„Å´„Çâ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Pet Girl of Sakurasou&lt;/strong&gt;: &lt;a href=&#34;https://youtu.be/M0vDI1HH2_Y&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://www.bilibili.com/video/BV14k4y167KP/&#34;&gt;Bilibili&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;240P to 1080P 60FPS&lt;/li&gt; &#xA;   &lt;li&gt;The original video&#39;s copyright belongs to ASCII Media Works&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;GIF Upscaling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/21986859/81631069-96d4fc80-93f6-11ea-92fb-33d6545055e7.gif&#34; alt=&#34;catfru&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/21986859/81631070-976d9300-93f6-11ea-9137-072a3b386110.gif&#34; alt=&#34;catfru4x&#34;&gt;&lt;br&gt; &lt;em&gt;Catfru scaled up to 4x its original size using waifu2x &lt;a href=&#34;https://gfycat.com/craftyeasygoingankole-capoo-bug-cat&#34;&gt;(original image)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Upscaling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/21986859/81631903-79a12d80-93f8-11ea-9c3c-f340240cf08c.png&#34; alt=&#34;Jill Comparison&#34;&gt;&lt;br&gt; &lt;em&gt;Image 8x upscaling demo (&lt;a href=&#34;https://72915.tumblr.com/post/173793265673&#34;&gt;original image&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/nananicu&#34;&gt;nananicu&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Standard Test Clip&lt;/h3&gt; &#xA;&lt;p&gt;The following clip can be used to test if your setup works properly. This is also the standard clip used for running performance benchmarks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://files.k4yt3x.com/Resources/Videos/standard-test.mp4&#34;&gt;Standard Test Clip (240P)&lt;/a&gt; 4.54 MiB&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://files.k4yt3x.com/Resources/Videos/standard-waifu2x.mp4&#34;&gt;waifu2x Upscaled Sample (1080P)&lt;/a&gt; 4.54 MiB&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://files.k4yt3x.com/Resources/Videos/standard-original.mp4&#34;&gt;Original Ground Truth (1080P)&lt;/a&gt; 22.2 MiB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The original clip came from the anime &#34;„Åï„Åè„ÇâËçò„ÅÆ„Éö„ÉÉ„Éà„Å™ÂΩºÂ•≥.&#34;&lt;br&gt; Copyright of this clip belongs to Ê†™Âºè‰ºöÁ§æ„Ç¢„Éã„Éó„É¨„ÉÉ„ÇØ„Çπ.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://www.gnu.org/licenses/agpl-3.0.txt&#34;&gt;GNU Affero General Public License Version 3 (GNU AGPL v3)&lt;/a&gt;&lt;br&gt; Copyright (c) 2018-2022 K4YT3X and contributors.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://www.gnu.org/graphics/agplv3-155x51.png&#34; alt=&#34;AGPLv3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project includes or depends on these following projects:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ffmpeg.org/&#34;&gt;FFmpeg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LGPLv2.1, GPLv2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nihui/waifu2x-ncnn-vulkan&#34;&gt;waifu2x-ncnn-vulkan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nihui/srmd-ncnn-vulkan&#34;&gt;srmd-ncnn-vulkan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nihui/realsr-ncnn-vulkan&#34;&gt;realsr-ncnn-vulkan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nihui/rife-ncnn-vulkan&#34;&gt;rife-ncnn-vulkan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nihui/realcugan-ncnn-vulkan&#34;&gt;realcugan-ncnn-vulkan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kkroening/ffmpeg-python&#34;&gt;ffmpeg-python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache-2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Delgan/loguru&#34;&gt;Loguru&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/opencv/opencv-python&#34;&gt;opencv-python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/python-pillow/Pillow&#34;&gt;Pillow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HPND License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Textualize/rich&#34;&gt;Rich&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/moses-palmer/pynput&#34;&gt;pynput&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LGPLv3.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Legacy versions of this project includes or depends on these following projects:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lltcggie/waifu2x-caffe&#34;&gt;waifu2x-caffe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/DeadSix27/waifu2x-converter-cpp&#34;&gt;waifu2x-converter-cpp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bloc97/Anime4K&#34;&gt;Anime4K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/TianZerL/Anime4KCPP&#34;&gt;Anime4KCPP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ImageOptim/gifski&#34;&gt;Gifski&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AGPLv3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tqdm/tqdm&#34;&gt;tqdm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MPLv2.0, MIT License&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;More licensing information can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/k4yt3x/video2x/master/NOTICES&#34;&gt;NOTICES&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Special Thanks&lt;/h2&gt; &#xA;&lt;p&gt;Appreciations given to the following personnel who have contributed significantly to the project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BrianPetkovsek&#34;&gt;@BrianPetkovsek&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sat3ll&#34;&gt;@sat3ll&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ddouglas87&#34;&gt;@ddouglas87&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lhanjian&#34;&gt;@lhanjian&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/archiemeng&#34;&gt;@ArchieMeng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nihui&#34;&gt;@nihui&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Similar Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CardinalPanda/dandere2x&#34;&gt;Dandere2x&lt;/a&gt;: A lossy video upscaler also built around &lt;code&gt;waifu2x&lt;/code&gt;, but with video compression techniques to shorten the time needed to process a video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AaronFeng753/Waifu2x-Extension-GUI&#34;&gt;Waifu2x-Extension-GUI&lt;/a&gt;: A similar project that focuses more and only on building a better graphical user interface. It is built using C++ and Qt5, and currently only supports the Windows platform.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/tutorials</title>
    <updated>2022-09-01T01:36:44Z</updated>
    <id>tag:github.com,2022-09-01:/pytorch/tutorials</id>
    <link href="https://github.com/pytorch/tutorials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch tutorials.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PyTorch Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;All the tutorials are now presented as sphinx style documentation at:&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://pytorch.org/tutorials&#34;&gt;https://pytorch.org/tutorials&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We use sphinx-gallery&#39;s &lt;a href=&#34;https://sphinx-gallery.github.io/stable/tutorials/index.html&#34;&gt;notebook styled examples&lt;/a&gt; to create the tutorials. Syntax is very simple. In essence, you write a slightly well formatted python file and it shows up as documentation page.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s how to create a new tutorial or recipe:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a notebook styled python file. If you want it executed while inserted into documentation, save the file with suffix &lt;code&gt;tutorial&lt;/code&gt; so that file name is &lt;code&gt;your_tutorial.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Put it in one of the beginner_source, intermediate_source, advanced_source based on the level. If it is a recipe, add to recipes_source.&lt;/li&gt; &#xA; &lt;li&gt;For Tutorials (except if it is a prototype feature), include it in the TOC tree at index.rst&lt;/li&gt; &#xA; &lt;li&gt;For Tutorials (except if it is a prototype feature), create a thumbnail in the &lt;a href=&#34;https://github.com/pytorch/tutorials/raw/master/index.rst&#34;&gt;index.rst file&lt;/a&gt; using a command like &lt;code&gt;.. customcarditem:: beginner/your_tutorial.html&lt;/code&gt;. For Recipes, create a thumbnail in the &lt;a href=&#34;https://github.com/pytorch/tutorials/raw/master/recipes_source/recipes_index.rst&#34;&gt;recipes_index.rst&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;In case you prefer to write your tutorial in jupyter, you can use &lt;a href=&#34;https://gist.github.com/chsasank/7218ca16f8d022e02a9c0deb94a310fe&#34;&gt;this script&lt;/a&gt; to convert the notebook to python file. After conversion and addition to the project, please make sure the sections headings etc are in logical order.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with installing torch, torchvision, and your GPUs latest drivers. Install other requirements using &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you want to use &lt;code&gt;virtualenv&lt;/code&gt;, make your environment in a &lt;code&gt;venv&lt;/code&gt; directory like: &lt;code&gt;virtualenv ./venv&lt;/code&gt;, then &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Then you can build using &lt;code&gt;make docs&lt;/code&gt;. This will download the data, execute the tutorials and build the documentation to &lt;code&gt;docs/&lt;/code&gt; directory. This will take about 60-120 min for systems with GPUs. If you do not have a GPU installed on your system, then see next step.&lt;/li&gt; &#xA; &lt;li&gt;You can skip the computationally intensive graph generation by running &lt;code&gt;make html-noplot&lt;/code&gt; to build basic html documentation to &lt;code&gt;_build/html&lt;/code&gt;. This way, you can quickly preview your tutorial.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you get &lt;strong&gt;ModuleNotFoundError: No module named &#39;pytorch_sphinx_theme&#39; make: *** [html-noplot] Error 2&lt;/strong&gt; from /tutorials/src/pytorch-sphinx-theme or /venv/src/pytorch-sphinx-theme (while using virtualenv), run &lt;code&gt;python setup.py install&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;About contributing to PyTorch Documentation and Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can find information about contributing to PyTorch documentation in the PyTorch Repo &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/README.md&#34;&gt;README.md&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;Additional information can be found in &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/CONTRIBUTING.md&#34;&gt;PyTorch CONTRIBUTING.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>hlky/sd-enable-textual-inversion</title>
    <updated>2022-09-01T01:36:44Z</updated>
    <id>tag:github.com,2022-09-01:/hlky/sd-enable-textual-inversion</id>
    <link href="https://github.com/hlky/sd-enable-textual-inversion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Copy these files to your stable-diffusion to enable text-inversion&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;sd-enable-textual-inversion&lt;/h1&gt; &#xA;&lt;p&gt;Copy the files into your stable-diffusion folder to enable text-inversion in the Web UI.&lt;/p&gt; &#xA;&lt;p&gt;If you experience any problems &lt;strong&gt;with webui integration&lt;/strong&gt;, please create an issue &lt;a href=&#34;https://github.com/hlky/stable-diffusion-webui&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are having problems installing or running textual-inversion, see FAQ below. If your problem is not listed, the official repo is &lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;How does it work?&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;textual-inversion&lt;/a&gt; - An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion (credit: Tel Aviv University, NVIDIA)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://textual-inversion.github.io/static/images/editing/teaser.JPG&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We learn to generate specific concepts, like personal objects or artistic styles, by describing them using new &#34;words&#34; in the embedding space of pre-trained text-to-image models. These can be used in new sentences, just like any other word.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Essentially, this model will take some pictures of an object, style, etc. and learn how to describe it in a way that can be understood by text-to-image models such as Stable Diffusion. This allows you to reference specific things in your prompts, or concepts that are easier to express with pictures rather than words.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;How to Use&lt;/h1&gt; &#xA;&lt;p&gt;Before you can do anything with the WebUI, you must first create an embedding file by training the Textual-Inversion model. Alternatively, you can test with one of the pre-made embeddings from &lt;a href=&#34;https://github.com/hlky/sd-embeddings&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In the WebUI, place the embeddings file in the embeddings file upload box. Then you can reference the embedding by using &lt;code&gt;*&lt;/code&gt; in your prompt.&lt;/p&gt; &#xA;&lt;p&gt;Examples from the paper: &lt;img src=&#34;https://textual-inversion.github.io/static/images/editing/puppet.JPG&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;How to Train Textual-Inversion&lt;/h1&gt; &#xA;&lt;p&gt;** Training is best done by using the &lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;original repo&lt;/a&gt; **&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;WARNING:&lt;/strong&gt; This is a very memory-intensive model and, as of writing, is not optimized to work with SD. You will need an Nvidia GPU with at least 10GB of VRAM to even get this to train at all on your local device, and a GPU with 20GB+ to train in a reasonable amount of time. If you do not have the system resources, you should use Colab or stick with pretrained embeddings until SD is better supported.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note that these instructions are for training on your local device, instructions may vary for training in Colab.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You will need 3-5 images of what you want the model to describe. You can use more images, but the paper recommends 5. For the best results, the images should be visually similar, and each image should be cropped to 512x512. Any other sizes will be rescaled (stretched) and may produce strange results.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Step 1:&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Place 3-5 images of the object/artstyle/scene/etc. into an empty folder.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; In Anaconda run&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;python main.py --base configs/stable-diffusion/v1-finetune.yaml -t --actual_resume models/ldm/text2img-large/model.ckpt -n &amp;lt;name this run&amp;gt; --data_root path/to/image/folder --gpus 1 --init-word &amp;lt;your init word&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;--base points the script at the training configuration file&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;--actual_resume points the script at the Textual-Inversion model&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;--n gives the training run a name, which will also be used as the output folder name.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;--gpus Leave at 1 unless you know what you&#39;re doing.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;--init-word is a single word the model will start with when looking at your images for the first time. Should be simple, ie: &#34;sculpture&#34;, &#34;girl&#34;, &#34;mountains&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Step 3:&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The model will continue to train until you stop it by entering CTRL+C. The recommended training time is 3000-7000 iterations (global steps). You can see what step the run is on in the progress bar. You can also monitor progress by reviewing the images at &lt;code&gt;logs/&amp;lt;your run name&amp;gt;/images&lt;/code&gt;. I recommend sorting that folder by date modified, they tend to get jumbled up otherwise.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Step 4:&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Once stopped, you will find a number of embedding files under &lt;code&gt;logs/&amp;lt;your run name&amp;gt;/checkpoints&lt;/code&gt;. The one you want is &lt;strong&gt;embeddings.pt&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Step 5:&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;In the WebUI, upload the embedding file you just created. Now, when writing a prompt, you can use &lt;code&gt;*&lt;/code&gt; to reference the whatever the embedding file describes.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;A picture of * in the style of Rembrandt&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;A photo of * as a corgi&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;A coffee mug in the style of *&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Pro tips&lt;/h1&gt; &#xA;&lt;p&gt;Reminder: Official Repo here ==&amp;gt; &lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;rinongal/textual_inversion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unofficial fork, more stable on Windows (8/28/22) ==&amp;gt; &lt;a href=&#34;https://github.com/nicolai256/Stable-textual-inversion_win&#34;&gt;nicolai256/Stable-textual-inversion_win&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When using embeddings in your prompts, the authors note that markers (&lt;code&gt;*&lt;/code&gt;) are sensitive to puncutation. Avoid using periods or commas directly after &lt;code&gt;*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The model tends to converge faster and provide more accurate results when using language such as &#34;a photo of&#34; or &#34;* as a photograph&#34; in your prompts&lt;/li&gt; &#xA; &lt;li&gt;When training Textual-Inversion, the paper says that using more than 5 images leads to less cohesive results. Some users seem to disagree. Try experimenting.&lt;/li&gt; &#xA; &lt;li&gt;When training, more than one init-word can be specified by adding them to the list at &lt;code&gt;initializer_words: [&#34;sculpture&#34;, &#34;ice&#34;]&lt;/code&gt; in &lt;code&gt;v1-finetune.yaml&lt;/code&gt;. Order may matter (unconfirmed)&lt;/li&gt; &#xA; &lt;li&gt;You can train multiple embedding files, then merge them with &lt;code&gt;merge_embeddings.py -sd&lt;/code&gt; to reference multiple things. See official repo for more details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h3&gt;Q: How much VRAM does this require, why am I receiving a CUDA Out of Memory Error?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This model is very VRAM heavy, with 20GB being the recommended amount. It is possible to run this model on a GPU with &amp;lt;12GB of VRAM, but no guarantee. Try changing &lt;code&gt;size: 512&lt;/code&gt; to &lt;code&gt;size: 448&lt;/code&gt; in &lt;code&gt;v1-finetune.yaml -&amp;gt; data: -&amp;gt; params:&lt;/code&gt; for both &lt;code&gt;train:&lt;/code&gt; and &lt;code&gt;validation:&lt;/code&gt;. If that is not enough, then it&#39;s probably best to use a Colab notebook or other GPU hosting service to do your training.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Q: Why am I receiving a &#34;SIGUSR1&#34; error? Why am I receiving an NCNN error? Why am I receiving &lt;code&gt;OSError: cannot open resource&lt;/code&gt;?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The script &lt;code&gt;main.py&lt;/code&gt; was written without Windows in mind.&lt;/p&gt; &#xA;&lt;p&gt;You will need to open &lt;code&gt;main.py&lt;/code&gt; and add the following line after the last import near the top of the script:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;os.environ[&#34;PL_TORCH_DISTRIBUTED_BACKEND&#34;] = &#34;gloo&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Next, find the following lines near the end of the script. Change &lt;code&gt;SIGUSR1&lt;/code&gt; and &lt;code&gt;SIGUSR2&lt;/code&gt; to &lt;code&gt;SIGTERM&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import signal&#xA;&#xA;signal.signal(signal.SIGUSR1, melk)&#xA;signal.signal(signal.SIGUSR2, divein)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, open the file &lt;code&gt;ldm/utils.py&lt;/code&gt; and find this line: &lt;code&gt;font = ImageFont.truetype(&#39;data/DejaVuSans.ttf&#39;, size=size)&lt;/code&gt;. Comment it out and replace it with this: &lt;code&gt;font = ImageFont.load_default()&lt;/code&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Q: Why am I receiving an error about multiple devices detected?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Make sure you are using the &lt;code&gt;--gpus 1&lt;/code&gt; argument. If you are still receiving the error, open &lt;code&gt;main.py&lt;/code&gt; and find the following lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;if not cpu:&#xA;    ngpu = len(lightning_config.trainer.gpus.strip(&#34;,&#34;).split(&#39;,&#39;))&#xA;else:&#xA;    ngpu = 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Comment these lines out, then below them add &lt;code&gt;ngpu = 1&lt;/code&gt; (Or whatever # of GPUs you want to use). Make sure that it is at the same indentation level as the line below it.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Q: Why am I receiving an error about &lt;code&gt;if trainer.global_rank == 0:&lt;/code&gt;?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Open &lt;code&gt;main.py&lt;/code&gt; and scroll to the end of the file. On the last few lines, comment out the line where it says &lt;code&gt;if trainer.global_rank == 0:&lt;/code&gt; and the line below it.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Q: Why am I receiving errors about shapes? (IE: value tensor of shape [1280] cannot be broadcast to indexing result of shape [0, 768])&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; There are two known reasons for shape errors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The sanity check is failing when starting Textual-Inversion training. Try leaving out the &lt;code&gt;--actual-resume&lt;/code&gt; argument when launching main.py. Chances are, the next error you receive will be an Out of Memory Error. See earlier in the FAQ for that.&lt;/li&gt; &#xA; &lt;li&gt;Stable Diffusion is erroring out when you try to use an embeddings file. This is likely because you ran the Textual-Inversion training with the wrong configuration. As of writing, TI and SD are not integrated. Make sure you have downloaded the &lt;code&gt;config/v1-finetune.yaml&lt;/code&gt; file from this repo and that you use &lt;code&gt;--base configs/stable-diffusion/v1-finetune.yaml&lt;/code&gt; when training embeddings. Retrain and try again.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt;</summary>
  </entry>
</feed>