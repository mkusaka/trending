<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-24T01:34:48Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>subzeroid/instagrapi</title>
    <updated>2025-06-24T01:34:48Z</updated>
    <id>tag:github.com,2025-06-24:/subzeroid/instagrapi</id>
    <link href="https://github.com/subzeroid/instagrapi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🔥 The fastest and powerful Python library for Instagram Private API 2025 with HikerAPI SaaS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;If you want to work with Instagrapi (business interests), we strongly advise you to prefer &lt;a href=&#34;https://hikerapi.com/p/bkXQlaVe&#34;&gt;HikerAPI SaaS&lt;/a&gt; project. However, you won&#39;t need to spend weeks or even months setting it up. The best service available today is &lt;a href=&#34;https://hikerapi.com/p/bkXQlaVe&#34;&gt;HikerAPI SaaS&lt;/a&gt;, which handles 4–5 million daily requests, provides support around-the-clock, and offers partners a special rate. In many instances, our clients tried to save money and preferred instagrapi, but in our experience, they ultimately returned to &lt;a href=&#34;https://hikerapi.com/p/bkXQlaVe&#34;&gt;HikerAPI SaaS&lt;/a&gt; after spending much more time and money. It will be difficult to find good accounts, good proxies, or resolve challenges, and IG will ban your accounts.&lt;/p&gt; &#xA;&lt;p&gt;The instagrapi more suits for testing or research than a working business!&lt;/p&gt; &#xA;&lt;p&gt;✨ &lt;a href=&#34;https://github.com/subzeroid/aiograpi&#34;&gt;aiograpi - Asynchronous Python library for Instagram Private API&lt;/a&gt; ✨&lt;/p&gt; &#xA;&lt;h3&gt;We recommend using our services:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lamatok.com/p/B9ScEYIQ&#34;&gt;LamaTok&lt;/a&gt; for TikTok API 🔥&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hikerapi.com/p/bkXQlaVe&#34;&gt;HikerAPI&lt;/a&gt; for Instagram API ⚡⚡⚡&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datalikers.com/p/S9Lv5vBy&#34;&gt;DataLikers&lt;/a&gt; for Instagram Datasets 🚀&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/subzeroid/instagrapi/actions/workflows/python-package.yml&#34;&gt;&lt;img src=&#34;https://github.com/subzeroid/instagrapi/actions/workflows/python-package.yml/badge.svg?branch=master&amp;amp;1&#34; alt=&#34;Package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/instagrapi/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/instagrapi&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/instagrapi&#34; alt=&#34;PyPI - Python Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/mypy-checked-blue&#34; alt=&#34;Checked with mypy&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To run instagrapi you may need a &lt;a href=&#34;https://powervps.net/?from=96837&#34;&gt;cheap and powerful server&lt;/a&gt;, I recommend using my promo you will support the author of this library!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Getting public data of user, posts, stories, highlights, followers and following users&lt;/li&gt; &#xA; &lt;li&gt;Getting public email and phone number, if the user specified them in his business profile&lt;/li&gt; &#xA; &lt;li&gt;Getting public data of post, story, album, Reels, IGTV data and the ability to download content&lt;/li&gt; &#xA; &lt;li&gt;Getting public data of hashtag and location data, as well as a list of posts for them&lt;/li&gt; &#xA; &lt;li&gt;Getting public data of all comments on a post and a list of users who liked it&lt;/li&gt; &#xA; &lt;li&gt;Management of &lt;a href=&#34;https://soax.com?afmc=sEysufQI&#34;&gt;proxy servers&lt;/a&gt;, mobile devices and challenge resolver&lt;/li&gt; &#xA; &lt;li&gt;Login by username and password, sessionid and support 2FA&lt;/li&gt; &#xA; &lt;li&gt;Managing messages and threads for Direct and attach files&lt;/li&gt; &#xA; &lt;li&gt;Download and upload a Photo, Video, IGTV, Reels, Albums and Stories&lt;/li&gt; &#xA; &lt;li&gt;Work with Users, Posts, Comments, Insights, Collections, Location and Hashtag&lt;/li&gt; &#xA; &lt;li&gt;Insights by account, posts and stories&lt;/li&gt; &#xA; &lt;li&gt;Like, following, commenting, editing account (Bio) and much more else&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;instagrapi - Unofficial Instagram API for Python&lt;/h1&gt; &#xA;&lt;p&gt;Fast and effective Instagram Private API wrapper (public+private requests and challenge resolver) without selenium. Use the most recent version of the API from Instagram, which was obtained using reverse-engineering with Charles Proxy and &lt;a href=&#34;https://proxyman.io/&#34;&gt;Proxyman&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Instagram API valid for &lt;strong&gt;25 May 2025&lt;/strong&gt; (last reverse-engineering check)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Support &lt;strong&gt;Python &amp;gt;= 3.9&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For any other languages (e.g. C++, C#, F#, D, &lt;a href=&#34;https://github.com/subzeroid/instagrapi-rest/tree/main/golang&#34;&gt;Golang&lt;/a&gt;, Erlang, Elixir, Nim, Haskell, Lisp, Closure, Julia, R, Java, Kotlin, Scala, OCaml, JavaScript, Crystal, Ruby, Rust, &lt;a href=&#34;https://github.com/subzeroid/instagrapi-rest/tree/main/swift&#34;&gt;Swift&lt;/a&gt;, Objective-C, Visual Basic, .NET, Pascal, Perl, Lua, PHP and others), I suggest using &lt;a href=&#34;https://github.com/subzeroid/instagrapi-rest&#34;&gt;instagrapi-rest&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://t.me/instagrapi&#34;&gt;Support Chat in Telegram&lt;/a&gt; &lt;img src=&#34;https://gist.githubusercontent.com/m8rge/4c2b36369c9f936c02ee883ca8ec89f1/raw/c03fd44ee2b63d7a2a195ff44e9bb071e87b4a40/telegram-single-path-24px.svg?sanitize=true&#34; alt=&#34;&#34;&gt; and &lt;a href=&#34;https://github.com/subzeroid/instagrapi/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Performs &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/fundamentals.html&#34;&gt;Web API&lt;/a&gt; or &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/fundamentals.html&#34;&gt;Mobile API&lt;/a&gt; requests depending on the situation (to avoid Instagram limits)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/interactions.html&#34;&gt;Login&lt;/a&gt; by username and password, including 2FA and by sessionid (and uses Authorization header instead Cookies)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/challenge_resolver.html&#34;&gt;Challenge Resolver&lt;/a&gt; have Email and SMS handlers&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/media.html&#34;&gt;upload&lt;/a&gt; a Photo, Video, IGTV, Reels, Albums and Stories&lt;/li&gt; &#xA; &lt;li&gt;Support work with &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/user.html&#34;&gt;User&lt;/a&gt;, &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/media.html&#34;&gt;Media&lt;/a&gt;, &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/comment.html&#34;&gt;Comment&lt;/a&gt;, &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/insight.html&#34;&gt;Insights&lt;/a&gt;, &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/collection.html&#34;&gt;Collections&lt;/a&gt;, &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/location.html&#34;&gt;Location&lt;/a&gt; (Place), &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/hashtag.html&#34;&gt;Hashtag&lt;/a&gt; and &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/direct.html&#34;&gt;Direct Message&lt;/a&gt; objects&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/media.html&#34;&gt;Like&lt;/a&gt;, &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/user.html&#34;&gt;Follow&lt;/a&gt;, &lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/account.html&#34;&gt;Edit account&lt;/a&gt; (Bio) and much more else&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/insight.html&#34;&gt;Insights&lt;/a&gt; by account, posts and stories&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/story.html&#34;&gt;Build stories&lt;/a&gt; with custom background, font animation, link sticker and mention users&lt;/li&gt; &#xA; &lt;li&gt;In the next release, account registration and captcha passing will appear&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Examples of apps that use instagrapi&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t.me/instagram_load_bot&#34;&gt;Telegram Bot for Download Posts, Stories and Highlights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install instagrapi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Basic Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from instagrapi import Client&#xA;&#xA;cl = Client()&#xA;cl.login(ACCOUNT_USERNAME, ACCOUNT_PASSWORD)&#xA;&#xA;user_id = cl.user_id_from_username(ACCOUNT_USERNAME)&#xA;medias = cl.user_medias(user_id, 20)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Session with persistence&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from instagrapi import Client&#xA;&#xA;cl = Client()&#xA;cl.login(USERNAME, PASSWORD)&#xA;cl.dump_settings(&#34;session.json&#34;)&#xA;&#xA;# reload later without entering credentials again&#xA;cl = Client()&#xA;cl.load_settings(&#34;session.json&#34;)&#xA;cl.login(USERNAME, PASSWORD)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Login using a sessionid&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from instagrapi import Client&#xA;&#xA;cl = Client()&#xA;cl.login_by_sessionid(&#34;&amp;lt;your_sessionid&amp;gt;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List and download another user&#39;s posts&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from instagrapi import Client&#xA;&#xA;cl = Client()&#xA;cl.login(USERNAME, PASSWORD)&#xA;&#xA;target_id = cl.user_id_from_username(&#34;target_user&#34;)&#xA;posts = cl.user_medias(target_id, amount=10)&#xA;for media in posts:&#xA;    # download photos to the current folder&#xA;    cl.photo_download(media.pk)&#xA;&#xA;See [examples/session_login.py](examples/session_login.py) for a standalone script demonstrating these login methods.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Additional example&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from instagrapi import Client&#xA;from instagrapi.types import StoryMention, StoryMedia, StoryLink, StoryHashtag&#xA;&#xA;cl = Client()&#xA;cl.login(USERNAME, PASSWORD, verification_code=&#34;&amp;lt;2FA CODE HERE&amp;gt;&#34;)&#xA;&#xA;media_pk = cl.media_pk_from_url(&#39;https://www.instagram.com/p/CGgDsi7JQdS/&#39;)&#xA;media_path = cl.video_download(media_pk)&#xA;subzeroid = cl.user_info_by_username(&#39;subzeroid&#39;)&#xA;hashtag = cl.hashtag_info(&#39;dhbastards&#39;)&#xA;&#xA;cl.video_upload_to_story(&#xA;    media_path,&#xA;    &#34;Credits @subzeroid&#34;,&#xA;    mentions=[StoryMention(user=subzeroid, x=0.49892962, y=0.703125, width=0.8333333333333334, height=0.125)],&#xA;    links=[StoryLink(webUri=&#39;https://github.com/subzeroid/instagrapi&#39;)],&#xA;    hashtags=[StoryHashtag(hashtag=hashtag, x=0.23, y=0.32, width=0.5, height=0.22)],&#xA;    medias=[StoryMedia(media_pk=media_pk, x=0.5, y=0.5, width=0.6, height=0.8)]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/&#34;&gt;Index&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/getting-started.html&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/fundamentals.html&#34;&gt;Usage Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/interactions.html&#34;&gt;Interactions&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/media.html&#34;&gt;&lt;code&gt;Media&lt;/code&gt;&lt;/a&gt; - Publication (also called post): Photo, Video, Album, IGTV and Reels&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/media.html&#34;&gt;&lt;code&gt;Resource&lt;/code&gt;&lt;/a&gt; - Part of Media (for albums)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/media.html&#34;&gt;&lt;code&gt;MediaOembed&lt;/code&gt;&lt;/a&gt; - Short version of Media&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/account.html&#34;&gt;&lt;code&gt;Account&lt;/code&gt;&lt;/a&gt; - Full private info for your account (e.g. email, phone_number)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/totp.html&#34;&gt;&lt;code&gt;TOTP&lt;/code&gt;&lt;/a&gt; - 2FA TOTP helpers (generate seed, enable/disable TOTP, generate code as Google Authenticator)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/user.html&#34;&gt;&lt;code&gt;User&lt;/code&gt;&lt;/a&gt; - Full public user data&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/user.html&#34;&gt;&lt;code&gt;UserShort&lt;/code&gt;&lt;/a&gt; - Short public user data (used in Usertag, Comment, Media, Direct Message)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/user.html&#34;&gt;&lt;code&gt;Usertag&lt;/code&gt;&lt;/a&gt; - Tag user in Media (coordinates + UserShort)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/location.html&#34;&gt;&lt;code&gt;Location&lt;/code&gt;&lt;/a&gt; - GEO location (GEO coordinates, name, address)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/hashtag.html&#34;&gt;&lt;code&gt;Hashtag&lt;/code&gt;&lt;/a&gt; - Hashtag object (id, name, picture)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/collection.html&#34;&gt;&lt;code&gt;Collection&lt;/code&gt;&lt;/a&gt; - Collection of medias (name, picture and list of medias)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/comment.html&#34;&gt;&lt;code&gt;Comment&lt;/code&gt;&lt;/a&gt; - Comments to Media&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/highlight.html&#34;&gt;&lt;code&gt;Highlight&lt;/code&gt;&lt;/a&gt; - Highlights&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/notes.html&#34;&gt;&lt;code&gt;Notes&lt;/code&gt;&lt;/a&gt; - Notes&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/story.html&#34;&gt;&lt;code&gt;Story&lt;/code&gt;&lt;/a&gt; - Story&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/story.html&#34;&gt;&lt;code&gt;StoryLink&lt;/code&gt;&lt;/a&gt; - Link Sticker&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/story.html&#34;&gt;&lt;code&gt;StoryLocation&lt;/code&gt;&lt;/a&gt; - Tag Location in Story (as sticker)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/story.html&#34;&gt;&lt;code&gt;StoryMention&lt;/code&gt;&lt;/a&gt; - Mention users in Story (user, coordinates and dimensions)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/story.html&#34;&gt;&lt;code&gt;StoryHashtag&lt;/code&gt;&lt;/a&gt; - Hashtag for story (as sticker)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/story.html&#34;&gt;&lt;code&gt;StorySticker&lt;/code&gt;&lt;/a&gt; - Tag sticker to story (for example from giphy)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/story.html&#34;&gt;&lt;code&gt;StoryBuild&lt;/code&gt;&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/subzeroid/instagrapi/master/instagrapi/story.py&#34;&gt;StoryBuilder&lt;/a&gt; return path to photo/video and mention co-ordinates&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/direct.html&#34;&gt;&lt;code&gt;DirectThread&lt;/code&gt;&lt;/a&gt; - Thread (topic) with messages in Direct Message&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/direct.html&#34;&gt;&lt;code&gt;DirectMessage&lt;/code&gt;&lt;/a&gt; - Message in Direct Message&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/insight.html&#34;&gt;&lt;code&gt;Insight&lt;/code&gt;&lt;/a&gt; - Insights for a post&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/track.html&#34;&gt;&lt;code&gt;Track&lt;/code&gt;&lt;/a&gt; - Music track (for Reels/Clips)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/best-practices.html&#34;&gt;Best Practices&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/development-guide.html&#34;&gt;Development Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/handle_exception.html&#34;&gt;Handle Exceptions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/usage-guide/challenge_resolver.html&#34;&gt;Challenge Resolver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://subzeroid.github.io/instagrapi/exceptions.html&#34;&gt;Exceptions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/subzeroid/instagrapi/graphs/contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/instagrapi/contributors.svg?width=890&amp;amp;button=0&#34; alt=&#34;List of contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To release, you need to call the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py sdist&#xA;twine upload dist/*&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/BitNet</title>
    <updated>2025-06-24T01:34:48Z</updated>
    <id>tag:github.com,2025-06-24:/microsoft/BitNet</id>
    <link href="https://github.com/microsoft/BitNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/version-1.0-blue&#34; alt=&#34;version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png&#34; alt=&#34;BitNet Model on Hugging Face&#34; width=&#34;800&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try it out via this &lt;a href=&#34;https://bitnet-demo.azurewebsites.net/&#34;&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href=&#34;https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source&#34;&gt;CPU&lt;/a&gt; or &lt;a href=&#34;https://github.com/microsoft/BitNet/raw/main/gpu/README.md&#34;&gt;GPU&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; &#xA;&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href=&#34;https://arxiv.org/abs/2410.16144&#34;&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg&#34; alt=&#34;m2_performance&#34; width=&#34;800&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg&#34; alt=&#34;m2_performance&#34; width=&#34;800&#34;&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&#34;&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;05/20/2025 &lt;a href=&#34;https://github.com/microsoft/BitNet/raw/main/gpu/README.md&#34;&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/NEW-red&#34; alt=&#34;NEW&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;04/14/2025 &lt;a href=&#34;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&#34;&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;02/18/2025 &lt;a href=&#34;https://arxiv.org/abs/2502.11880&#34;&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;11/08/2024 &lt;a href=&#34;https://arxiv.org/abs/2411.04965&#34;&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;10/21/2024 &lt;a href=&#34;https://arxiv.org/abs/2410.16144&#34;&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; &#xA; &lt;li&gt;03/21/2024 &lt;a href=&#34;https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf&#34;&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;02/27/2024 &lt;a href=&#34;https://arxiv.org/abs/2402.17764&#34;&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;10/17/2023 &lt;a href=&#34;https://arxiv.org/abs/2310.11453&#34;&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp&#39;s kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href=&#34;https://github.com/microsoft/T-MAC/&#34;&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; &#xA;&lt;h2&gt;Official Models&lt;/h2&gt; &#xA;&lt;table&gt;  &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;CPU&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;Kernel&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;I2_S&lt;/th&gt; &#xA;   &lt;th&gt;TL1&lt;/th&gt; &#xA;   &lt;th&gt;TL2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&#34;&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;2.4B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;❗️&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt;  &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;CPU&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;Kernel&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;I2_S&lt;/th&gt; &#xA;   &lt;th&gt;TL1&lt;/th&gt; &#xA;   &lt;th&gt;TL2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/1bitLLM/bitnet_b1_58-large&#34;&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;0.7B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&#34;&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;3.3B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens&#34;&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;8.0B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026&#34;&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;1B-10B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130&#34;&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;1B-3B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; &#xA; &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; &#xA; &lt;li&gt;clang&amp;gt;=18 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Desktop-development with C++&lt;/li&gt; &#xA;     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; &#xA;     &lt;li&gt;Git for Windows&lt;/li&gt; &#xA;     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; &#xA;     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href=&#34;https://apt.llvm.org/&#34;&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c &#34;$(wget -O - https://apt.llvm.org/llvm.sh)&#34;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;conda (highly recommend)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/microsoft/BitNet.git&#xA;cd BitNet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install the dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# (Recommended) Create a new conda environment&#xA;conda create -n bitnet-cpp python=3.9&#xA;conda activate bitnet-cpp&#xA;&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Build the project&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Manually download the model and run with local path&#xA;huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T&#xA;python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&#xA;usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]&#xA;                    [--use-pretuned]&#xA;&#xA;Setup the environment for running inference&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}&#xA;                        Model used for inference&#xA;  --model-dir MODEL_DIR, -md MODEL_DIR&#xA;                        Directory to save/load the model&#xA;  --log-dir LOG_DIR, -ld LOG_DIR&#xA;                        Directory to save the logging info&#xA;  --quant-type {i2_s,tl1}, -q {i2_s,tl1}&#xA;                        Quantization type&#xA;  --quant-embd          Quantize the embeddings to f16&#xA;  --use-pretuned, -p    Use the pretuned kernel parameters&#xA;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Basic usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run inference with the quantized model&#xA;python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p &#34;You are a helpful assistant&#34; -cnv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&#xA;usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]&#xA;&#xA;Run inference&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  -m MODEL, --model MODEL&#xA;                        Path to model file&#xA;  -n N_PREDICT, --n-predict N_PREDICT&#xA;                        Number of tokens to predict when generating text&#xA;  -p PROMPT, --prompt PROMPT&#xA;                        Prompt to generate text from&#xA;  -t THREADS, --threads THREADS&#xA;                        Number of threads to use&#xA;  -c CTX_SIZE, --ctx-size CTX_SIZE&#xA;                        Size of the prompt context&#xA;  -temp TEMPERATURE, --temperature TEMPERATURE&#xA;                        Temperature, a hyperparameter that controls the randomness of the generated text&#xA;  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)&#xA;                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)&#xA;&lt;/pre&gt; &#xA;&lt;h3&gt;Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  &#xA;   &#xA;Setup the environment for running the inference  &#xA;   &#xA;required arguments:  &#xA;  -m MODEL, --model MODEL  &#xA;                        Path to the model file. &#xA;   &#xA;optional arguments:  &#xA;  -h, --help  &#xA;                        Show this help message and exit. &#xA;  -n N_TOKEN, --n-token N_TOKEN  &#xA;                        Number of generated tokens. &#xA;  -p N_PROMPT, --n-prompt N_PROMPT  &#xA;                        Prompt to generate text from. &#xA;  -t THREADS, --threads THREADS  &#xA;                        Number of threads to use. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s a brief explanation of each argument:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; &#xA;&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M&#xA;&#xA;# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate&#xA;python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Prepare the .safetensors model file&#xA;huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16&#xA;&#xA;# Convert to gguf model&#xA;python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;FAQ (Frequently Asked Questions)📌&lt;/h3&gt; &#xA;&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href=&#34;https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323&#34;&gt;commit&lt;/a&gt; in the &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python/issues/1942&#34;&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; &#xA;&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;clang -v&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;clang&#39; is not recognized as an internal or external command, operable program or batch file.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; &#xA;&lt;p&gt;• If you are using Command Prompt, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat&#34; -startdir=none -arch=x64 -host_arch=x64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;• If you are using Windows PowerShell, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Import-Module &#34;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll&#34; Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments &#34;-arch=x64 -host_arch=x64&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</summary>
  </entry>
</feed>