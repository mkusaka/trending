<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-10T01:37:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Weixin-Liang/LLM-scientific-feedback</title>
    <updated>2023-10-10T01:37:46Z</updated>
    <id>tag:github.com,2023-10-10:/Weixin-Liang/LLM-scientific-feedback</id>
    <link href="https://github.com/Weixin-Liang/LLM-scientific-feedback" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Can large language models provide useful feedback on research papers? A large-scale empirical analysis.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Can large language models provide useful feedback on research papers? A large-scale empirical analysis.&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10-blue.svg?sanitize=true&#34; alt=&#34;Python 3.10&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ambv/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.01783&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2310.01783-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo provides the Python source code of our paper: &lt;a href=&#34;https://arxiv.org/abs/2310.01783&#34;&gt;Can large language models provide useful feedback on research papers? A large-scale empirical analysis.&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2310.01783.pdf&#34;&gt;[PDF]&lt;/a&gt;&lt;a href=&#34;https://twitter.com/james_y_zou/status/1709608909395357946&#34;&gt;[Twitter]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{LLM-Research-Feedback-2023,&#xA;  title={{Can large language models provide useful feedback on research papers? A large-scale empirical analysis}},&#xA;  author={Liang, Weixin and Zhang, Yuhui and Cao, Hancheng and Wang, Binglu and Ding, Daisy and Yang, Xinyu and Vodrahalli, Kailas and He, Siyu and Smith, Daniel and Yin, Yian and McFarland, Daniel and Zou, James},&#xA;  booktitle={arXiv preprint arXiv:2310.01783},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4&#39;s feedback through two large-scale studies. We first quantitatively compared GPT-4&#39;s generated feedback with human peer reviewer feedback in 15 &lt;em&gt;Nature&lt;/em&gt; family journals (3,096 papers in total) and the &lt;em&gt;ICLR&lt;/em&gt; machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for &lt;em&gt;Nature&lt;/em&gt; journals, 39.23% for &lt;em&gt;ICLR&lt;/em&gt;) is comparable to the overlap between two human reviewers (average overlap 28.58% for &lt;em&gt;Nature&lt;/em&gt; journals, 35.25% for &lt;em&gt;ICLR&lt;/em&gt;). The overlap between GPT-4 and human reviewers is larger for the weaker papers (i.e., rejected &lt;em&gt;ICLR&lt;/em&gt; papers; average overlap 43.80%). We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations. For example, GPT-4 tends to focus on certain aspects of scientific feedback (e.g., `add experiments on more datasets&#39;), and often struggles to provide in-depth critique of method design. Together our results suggest that LLM and human feedback can complement each other. While human expert review is and should continue to be the foundation of rigorous scientific process, LLM feedback could benefit researchers, especially when timely expert feedback is not available and in earlier stages of manuscript preparation before peer-review.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Weixin-Liang/LLM-scientific-feedback/assets/32794044/8958eb56-a652-45bb-9347-e9578f432ae0&#34; alt=&#34;1&#34;&gt; &lt;img src=&#34;https://github.com/Weixin-Liang/LLM-scientific-feedback/assets/32794044/6228288b-9a54-4c90-8510-32bb823f1e05&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To run the code, you need to 1) create a PDF parsing server and run in the background, 2) create the LLM feedback server, 3) open the web browser and upload your paper.&lt;/p&gt; &#xA;&lt;h3&gt;Create and Run PDF Parsing Server&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f conda_environment.yml&#xA;conda activate ScienceBeam&#xA;python -m sciencebeam_parser.service.server --port=8080  # Make sure this is running in the background&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create and Run LLM Feedback Server&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n llm python=3.10&#xA;conda activate llm&#xA;pip install -r requirements.txt&#xA;cat YOUR_OPENAI_API_KEY &amp;gt; key.txt  # Replace YOUR_OPENAI_API_KEY with your OpenAI API key starting with &#34;sk-&#34;&#xA;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Open the Web Browser and Upload Your Paper&lt;/h3&gt; &#xA;&lt;p&gt;Open &lt;a href=&#34;http://0.0.0.0:7799&#34;&gt;http://0.0.0.0:7799&lt;/a&gt; and upload your paper. The feedback will be generated in around 120 seconds.&lt;/p&gt; &#xA;&lt;p&gt;You should get the following output:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Weixin-Liang/LLM-scientific-feedback/main/demo.png&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you encounter any error, please first check the server log and then open an issue.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/ToRA</title>
    <updated>2023-10-10T01:37:46Z</updated>
    <id>tag:github.com,2023-10-10:/microsoft/ToRA</id>
    <link href="https://github.com/microsoft/ToRA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ToRA is a series of Tool-integrated Reasoning LLM Agents designed to solve challenging mathematical reasoning problems by interacting with tools.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/tora_logo.png&#34; width=&#34;100&#34; alt=&#34;ToRA&#34;&gt; &lt;br&gt; ToRA: A Tool-Integrated Reasoning Agent &lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Task-Mathematical%20Reasoning-orange&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Model-Released-blue&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Code%20License-MIT-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://microsoft.github.io/ToRA/&#34;&gt;&lt;b&gt;[üåê Website]&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://arxiv.org/abs/2309.17452&#34;&gt;&lt;b&gt;[üìú Paper]&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://huggingface.co/llm-agents&#34;&gt;&lt;b&gt;[ü§ó HF Models]&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/microsoft/ToRA&#34;&gt;&lt;b&gt;[üê± GitHub]&lt;/b&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://twitter.com/zhs05232838/status/1708860992631763092&#34;&gt;&lt;b&gt;[üê¶ Twitter]&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1703k6d/tora_a_toolintegrated_reasoning_agent_for/&#34;&gt;&lt;b&gt;[üí¨ Reddit]&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://notes.aimodels.fyi/researchers-announce-tora-training-language-models-to-better-understand-math-using-external-tools/&#34;&gt;[üçÄ Unofficial Blog]&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;#-quick-start&#34;&gt;Quick Start&lt;/a&gt; ‚Ä¢ --&gt; &#xA; &lt;!-- &lt;a href=&#34;#%EF%B8%8F-citation&#34;&gt;Citation&lt;/a&gt; --&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Repo for &#34;&lt;a href=&#34;https://arxiv.org/abs/2309.17452&#34; target=&#34;_blank&#34;&gt;ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving&lt;/a&gt;&#34; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/math_gsm_hist.png&#34; width=&#34;1000&#34;&gt; &lt;br&gt; &lt;em&gt;Figure 1: Comparing ToRA with baselines on LLaMA-2 base models from 7B to 70B.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üî• News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/10/08] üî•üî•üî• All ToRA models released at &lt;a href=&#34;https://huggingface.co/llm-agents&#34;&gt;ü§ó HuggingFace&lt;/a&gt;!!!&lt;/li&gt; &#xA; &lt;li&gt;[2023/09/29] ToRA paper, repo, and website released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üí° Introduction&lt;/h2&gt; &#xA;&lt;p&gt;ToRA is a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical reasoning problems by interacting with tools, e.g., computation libraries and symbolic solvers. ToRA series seamlessly integrate natural language reasoning with the utilization of external tools, thereby amalgamating the analytical prowess of language and the computational efficiency of external tools.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;GSM8k&lt;/th&gt; &#xA;   &lt;th&gt;MATH&lt;/th&gt; &#xA;   &lt;th&gt;AVG@10 math tasks&lt;sup&gt;‚Ä†&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;92.0&lt;/td&gt; &#xA;   &lt;td&gt;42.5&lt;/td&gt; &#xA;   &lt;td&gt;78.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4 (PAL)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;94.2&lt;/td&gt; &#xA;   &lt;td&gt;51.8&lt;/td&gt; &#xA;   &lt;td&gt;86.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llm-agents/tora-7b-v1.0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/tora_logo.png&#34; width=&#34;16&#34; alt=&#34;ToRA&#34;&gt; ToRA-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;68.8&lt;/td&gt; &#xA;   &lt;td&gt;40.1&lt;/td&gt; &#xA;   &lt;td&gt;62.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llm-agents/tora-code-7b-v1.0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/tora_logo.png&#34; width=&#34;16&#34; alt=&#34;ToRA&#34;&gt; ToRA-Code-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;44.6&lt;/td&gt; &#xA;   &lt;td&gt;66.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llm-agents/tora-13b-v1.0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/tora_logo.png&#34; width=&#34;16&#34; alt=&#34;ToRA&#34;&gt; ToRA-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;72.7&lt;/td&gt; &#xA;   &lt;td&gt;43.0&lt;/td&gt; &#xA;   &lt;td&gt;65.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llm-agents/tora-code-13b-v1.0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/tora_logo.png&#34; width=&#34;16&#34; alt=&#34;ToRA&#34;&gt; ToRA-Code-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;75.8&lt;/td&gt; &#xA;   &lt;td&gt;48.1&lt;/td&gt; &#xA;   &lt;td&gt;71.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llm-agents/tora-code-34b-v1.0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/tora_logo.png&#34; width=&#34;16&#34; alt=&#34;ToRA&#34;&gt; ToRA-Code-34B&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;34B&lt;/td&gt; &#xA;   &lt;td&gt;80.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;74.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llm-agents/tora-70b-v1.0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/tora_logo.png&#34; width=&#34;16&#34; alt=&#34;ToRA&#34;&gt; ToRA-70B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;84.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;49.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;76.9&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;sup&gt;*&lt;/sup&gt;ToRA-Code-34B is currently the first and only open-source model to achieve over 50% accuracy (pass@1) on the MATH dataset, which significantly outperforms GPT-4‚Äôs CoT result (51.0 vs. 42.5), and is competitive with GPT-4 solving problems with programs. By open-sourcing our codes and models, we hope more breakthroughs will come!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;sup&gt;‚Ä†&lt;/sup&gt;10 math tasks include GSM8k, MATH, GSM-Hard, SVAMP, TabMWP, ASDiv, SingleEQ, SingleOP, AddSub, and MultiArith.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tool-Integrated Reasoning&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/example.png&#34; width=&#34;800&#34;&gt; &lt;br&gt; &lt;em&gt;Figure 2: A basic example of single-round tool interaction, which interleaves rationales with program-based tool use.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h3&gt;ToRA Training Pipeline&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/docs/static/images/pipeline.png&#34; width=&#34;800&#34;&gt; &lt;br&gt; &lt;em&gt;Figure 3: Training ToRA contains ‚ë† Imitation Learning, and ‚ë° output space shaping.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;‚öôÔ∏è Setup&lt;/h3&gt; &#xA;&lt;p&gt;We recommend using &lt;a href=&#34;https://docs.conda.io/projects/miniconda&#34;&gt;Conda&lt;/a&gt; to manage your environment. We use &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; (0.1.4) to accelerate inference. Run the following commands to setup your environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/microsoft/ToRA.git &amp;amp;&amp;amp; cd ToRA/src&#xA;conda create -n tora python=3.10&#xA;conda activate tora&#xA;pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118 # CUDA 11.8 for example&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ü™Å Inference&lt;/h3&gt; &#xA;&lt;p&gt;We provide a script for inference, simply config the &lt;code&gt;MODEL_NAME_OR_PATH&lt;/code&gt; and &lt;code&gt;DATA&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/src/scripts/infer.sh&#34;&gt;src/scripts/infer.sh&lt;/a&gt; and run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash scritps/infer.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also open-source the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/src/outputs/llm-agents/&#34;&gt;model outputs&lt;/a&gt; from our best models (ToRA-Code-34B and ToRA-70B) in the &lt;code&gt;src/outputs/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;‚öñÔ∏è Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ToRA/main/src/eval/&#34;&gt;src/eval/grader.py&lt;/a&gt; file contains the grading logic that assesses the accuracy of the predicted answer by comparing it to the ground truth. This logic is developed based on the Hendrycks&#39; MATH grading system, which we have manually verified on the MATH dataset to minimize false positives and false negatives.&lt;/p&gt; &#xA;&lt;p&gt;To evaluate the predicted answer, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m eval.evaluate \&#xA;    --data_name &#34;math&#34; \&#xA;    --prompt_type &#34;tora&#34; \&#xA;    --file_path &#34;outputs/llm-agents/tora-code-34b-v1.0/math/test_tora_-1_seed0_t0.0_s0_e5000.jsonl&#34; \&#xA;    --execute&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then you will get:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Num samples: 5000&#xA;Num scores: 5000&#xA;Timeout samples: 0&#xA;Empty samples: 2&#xA;Mean score: [51.0]&#xA;Type scores: {&#39;Algebra&#39;: 67.3, &#39;Counting &amp;amp; Probability&#39;: 42.2, &#39;Geometry&#39;: 26.1, &#39;Intermediate Algebra&#39;: 40.0, &#39;Number Theory&#39;: 59.3, &#39;Prealgebra&#39;: 63.8, &#39;Precalculus&#39;: 34.2}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;‚ö°Ô∏è Training&lt;/h3&gt; &#xA;&lt;p&gt;Due to some restrictions, ToRA-Corpus 16k is under review and cannot be released immediately. However, we open-source our complete training scripts as well as &lt;em&gt;output space shaping&lt;/em&gt; pipelines for the community, and you may construct your own dataset for training.&lt;/p&gt; &#xA;&lt;p&gt;To train a model, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash scritps/train.sh codellama 7b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚òïÔ∏è Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository helpful, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{gou2023tora,&#xA;      title={ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving}, &#xA;      author={Zhibin Gou and Zhihong Shao and Yeyun Gong and yelong shen and Yujiu Yang and Minlie Huang and Nan Duan and Weizhu Chen},&#xA;      year={2023},&#xA;      eprint={2309.17452},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üçÄ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;üåü Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#microsoft/ToRA&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=microsoft/ToRA&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cvg/glue-factory</title>
    <updated>2023-10-10T01:37:46Z</updated>
    <id>tag:github.com,2023-10-10:/cvg/glue-factory</id>
    <link href="https://github.com/cvg/glue-factory" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Training library for local feature detection and matching&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Glue Factory&lt;/h1&gt; &#xA;&lt;p&gt;Glue Factory is CVG&#39;s library for training and evaluating deep neural network that extract and match local visual feature. It enables you to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reproduce the training of state-of-the-art models for point and line matching, like &lt;a href=&#34;https://github.com/cvg/LightGlue&#34;&gt;LightGlue&lt;/a&gt; and &lt;a href=&#34;https://github.com/cvg/GlueStick&#34;&gt;GlueStick&lt;/a&gt; (ICCV 2023)&lt;/li&gt; &#xA; &lt;li&gt;Train these models on multiple datasets using your own local features or lines&lt;/li&gt; &#xA; &lt;li&gt;Evaluate feature extractors or matchers on standard benchmarks like HPatches or MegaDepth-1500&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/cvg/LightGlue&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cvg/glue-factory/main/docs/lightglue_matches.svg?sanitize=true&#34; width=&#34;60%&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/cvg/GlueStick&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cvg/glue-factory/main/docs/gluestick_img.svg?sanitize=true&#34; width=&#34;60%&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;em&gt;Point and line matching with LightGlue and GlueStick.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Glue Factory runs with Python 3 and &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;. The following installs the library and its basic dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/cvg/glue-factory&#xA;cd glue-factory&#xA;python3 -m pip install -e .  # editable mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some advanced features might require installing the full set of dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m pip install -e .[extra]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All models and datasets in gluefactory have auto-downloaders, so you can get started right away!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code and trained models in Glue Factory are released with an Apache-2.0 license. This includes LightGlue trained with an &lt;a href=&#34;https://github.com/rpautrat/SuperPoint&#34;&gt;open version of SuperPoint&lt;/a&gt;. Third-party models that are not compatible with this license, such as SuperPoint (original) and SuperGlue, are provided in &lt;code&gt;gluefactory_nonfree&lt;/code&gt;, where each model might follow its own, restrictive license.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h4&gt;HPatches&lt;/h4&gt; &#xA;&lt;p&gt;Running the evaluation commands automatically downloads the dataset, by default to the directory &lt;code&gt;data/&lt;/code&gt;. You will need about 1.8 GB of free disk space.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Evaluating LightGlue]&lt;/summary&gt; &#xA; &lt;p&gt;To evaluate the pre-trained SuperPoint+LightGlue model on HPatches, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.hpatches --conf superpoint+lightglue-official --overwrite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You should expect the following results&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;{&#39;H_error_dlt@1px&#39;: 0.3515,&#xA; &#39;H_error_dlt@3px&#39;: 0.6723,&#xA; &#39;H_error_dlt@5px&#39;: 0.7756,&#xA; &#39;H_error_ransac@1px&#39;: 0.3428,&#xA; &#39;H_error_ransac@3px&#39;: 0.5763,&#xA; &#39;H_error_ransac@5px&#39;: 0.6943,&#xA; &#39;mnum_keypoints&#39;: 1024.0,&#xA; &#39;mnum_matches&#39;: 560.756,&#xA; &#39;mprec@1px&#39;: 0.337,&#xA; &#39;mprec@3px&#39;: 0.89,&#xA; &#39;mransac_inl&#39;: 130.081,&#xA; &#39;mransac_inl%&#39;: 0.217,&#xA; &#39;ransac_mAA&#39;: 0.5378}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The default robust estimator is &lt;code&gt;opencv&lt;/code&gt;, but we strongly recommend to use &lt;code&gt;poselib&lt;/code&gt; instead:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.hpatches --conf superpoint+lightglue-official --overwrite \&#xA;    eval.estimator=poselib eval.ransac_th=-1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Setting &lt;code&gt;eval.ransac_th=-1&lt;/code&gt; auto-tunes the RANSAC inlier threshold by running the evaluation with a range of thresholds and reports results for the optimal value. Here are the results as Area Under the Curve (AUC) of the homography error at 1/3/5 pixels:&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Methods&lt;/th&gt; &#xA;    &lt;th&gt;DLT&lt;/th&gt; &#xA;    &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/robust_estimators/homography/opencv.py&#34;&gt;OpenCV&lt;/a&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/robust_estimators/homography/poselib.py&#34;&gt;PoseLib&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/configs/superpoint+superglue.yaml&#34;&gt;SuperPoint + SuperGlue&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;32.1 / 65.0 / 75.7&lt;/td&gt; &#xA;    &lt;td&gt;32.9 / 55.7 / 68.0&lt;/td&gt; &#xA;    &lt;td&gt;37.0 / 68.2 / 78.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/configs/superpoint+lightglue.yaml&#34;&gt;SuperPoint + LightGlue&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;35.1 / 67.2 / 77.6&lt;/td&gt; &#xA;    &lt;td&gt;34.2 / 57.9 / 69.9&lt;/td&gt; &#xA;    &lt;td&gt;37.1 / 67.4 / 77.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Evaluating GlueStick]&lt;/summary&gt; &#xA; &lt;p&gt;To evaluate GlueStick on HPatches, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.hpatches --conf gluefactory/configs/superpoint+lsd+gluestick.yaml --overwrite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You should expect the following results&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;{&#34;mprec@1px&#34;: 0.245,&#xA; &#34;mprec@3px&#34;: 0.838,&#xA; &#34;mnum_matches&#34;: 1290.5,&#xA; &#34;mnum_keypoints&#34;: 2287.5,&#xA; &#34;mH_error_dlt&#34;: null,&#xA; &#34;H_error_dlt@1px&#34;: 0.3355,&#xA; &#34;H_error_dlt@3px&#34;: 0.6637,&#xA; &#34;H_error_dlt@5px&#34;: 0.7713,&#xA; &#34;H_error_ransac@1px&#34;: 0.3915,&#xA; &#34;H_error_ransac@3px&#34;: 0.6972,&#xA; &#34;H_error_ransac@5px&#34;: 0.7955,&#xA; &#34;H_error_ransac_mAA&#34;: 0.62806,&#xA; &#34;mH_error_ransac&#34;: null}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Since we use points and lines to solve for the homography, we use a different robust estimator here: &lt;a href=&#34;https://github.com/rpautrat/homography_est/&#34;&gt;Hest&lt;/a&gt;. Here are the results as Area Under the Curve (AUC) of the homography error at 1/3/5 pixels:&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Methods&lt;/th&gt; &#xA;    &lt;th&gt;DLT&lt;/th&gt; &#xA;    &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/main/gluefactory/robust_estimators/homography/homography_est.py&#34;&gt;Hest&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/main/gluefactory/configs/superpoint+lsd+gluestick.yaml&#34;&gt;SP + LSD + GlueStick&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;33.6 / 66.4 / 77.1&lt;/td&gt; &#xA;    &lt;td&gt;39.2 / 69.7 / 79.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;MegaDepth-1500&lt;/h4&gt; &#xA;&lt;p&gt;Running the evaluation commands automatically downloads the dataset, which takes about 1.5 GB of disk space.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Evaluating LightGlue]&lt;/summary&gt; &#xA; &lt;p&gt;To evaluate the pre-trained SuperPoint+LightGlue model on MegaDepth-1500, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.megadepth1500 --conf superpoint+lightglue-official&#xA;# or the adaptive variant&#xA;python -m gluefactory.eval.megadepth1500 --conf superpoint+lightglue-official \&#xA;    model.matcher.{depth_confidence=0.95,width_confidence=0.95}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The first command should print the following results&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;{&#39;mepi_prec@1e-3&#39;: 0.795,&#xA; &#39;mepi_prec@1e-4&#39;: 0.15,&#xA; &#39;mepi_prec@5e-4&#39;: 0.567,&#xA; &#39;mnum_keypoints&#39;: 2048.0,&#xA; &#39;mnum_matches&#39;: 613.287,&#xA; &#39;mransac_inl&#39;: 280.518,&#xA; &#39;mransac_inl%&#39;: 0.442,&#xA; &#39;rel_pose_error@10¬∞&#39;: 0.681,&#xA; &#39;rel_pose_error@20¬∞&#39;: 0.8065,&#xA; &#39;rel_pose_error@5¬∞&#39;: 0.5102,&#xA; &#39;ransac_mAA&#39;: 0.6659}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To use the PoseLib estimator:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.megadepth1500 --conf superpoint+lightglue-official \&#xA;    eval.estimator=poselib eval.ransac_th=2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Evaluating GlueStick]&lt;/summary&gt; &#xA; &lt;p&gt;To evaluate the pre-trained SuperPoint+GlueStick model on MegaDepth-1500, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.megadepth1500 --conf gluefactory/configs/superpoint+lsd+gluestick.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;Here are the results as Area Under the Curve (AUC) of the pose error at 5/10/20 degrees:&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Methods&lt;/th&gt; &#xA;    &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/robust_estimators/relative_pose/pycolmap.py&#34;&gt;pycolmap&lt;/a&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/robust_estimators/relative_pose/opencv.py&#34;&gt;OpenCV&lt;/a&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/robust_estimators/relative_pose/poselib.py&#34;&gt;PoseLib&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/configs/superpoint+superglue.yaml&#34;&gt;SuperPoint + SuperGlue&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;54.4 / 70.4 / 82.4&lt;/td&gt; &#xA;    &lt;td&gt;48.7 / 65.6 / 79.0&lt;/td&gt; &#xA;    &lt;td&gt;64.8 / 77.9 / 87.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/configs/superpoint+lightglue.yaml&#34;&gt;SuperPoint + LightGlue&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;56.7 / 72.4 / 83.7&lt;/td&gt; &#xA;    &lt;td&gt;51.0 / 68.1 / 80.7&lt;/td&gt; &#xA;    &lt;td&gt;66.8 / 79.3 / 87.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/gluefactory/configs/superpoint+lsd+gluestick.yaml&#34;&gt;SuperPoint + GlueStick&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;53.2 / 69.8 / 81.9&lt;/td&gt; &#xA;    &lt;td&gt;46.3 / 64.2 / 78.1&lt;/td&gt; &#xA;    &lt;td&gt;64.4 / 77.5 / 86.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;ETH3D&lt;/h4&gt; &#xA;&lt;p&gt;The dataset will be auto-downloaded if it is not found on disk, and will need about 6 GB of free disk space.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Evaluating GlueStick]&lt;/summary&gt; &#xA; &lt;p&gt;To evaluate GlueStick on ETH3D, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.eth3d --conf gluefactory/configs/superpoint+lsd+gluestick.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You should expect the following results&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;AP: 77.92&#xA;AP_lines: 69.22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Image Matching Challenge 2021&lt;/h4&gt; &#xA;&lt;p&gt;Coming soon!&lt;/p&gt; &#xA;&lt;h4&gt;Image Matching Challenge 2023&lt;/h4&gt; &#xA;&lt;p&gt;Coming soon!&lt;/p&gt; &#xA;&lt;h4&gt;Visual inspection&lt;/h4&gt; &#xA;&lt;details&gt;&#xA;  To inspect the evaluation visually, you can run: &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.inspect hpatches superpoint+lightglue-official&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Click on a point to visualize matches on this pair.&lt;/p&gt; &#xA; &lt;p&gt;To compare multiple methods on a dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.inspect hpatches superpoint+lightglue-official superpoint+superglue-official&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;All current benchmarks are supported by the viewer.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Detailed evaluation instructions can be found &lt;a href=&#34;https://raw.githubusercontent.com/cvg/glue-factory/main/docs/evaluation.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We generally follow a two-stage training:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pre-train on a large dataset of synthetic homographies applied to internet images. We use the 1M-image distractor set of the Oxford-Paris retrieval dataset. It requires about 450 GB of disk space.&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune on the MegaDepth dataset, which is based on PhotoTourism pictures of popular landmarks around the world. It exhibits more complex and realistic appearance and viewpoint changes. It requires about 420 GB of disk space.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;All training commands automatically download the datasets.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Training LightGlue]&lt;/summary&gt; &#xA; &lt;p&gt;We show how to train LightGlue with &lt;a href=&#34;https://github.com/rpautrat/SuperPoint&#34;&gt;SuperPoint open&lt;/a&gt;. We first pre-train LightGlue on the homography dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.train sp+lg_homography \  # experiment name&#xA;    --conf gluefactory/configs/superpoint-open+lightglue_homography.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Feel free to use any other experiment name. By default the checkpoints are written to &lt;code&gt;outputs/training/&lt;/code&gt;. The default batch size of 128 corresponds to the results reported in the paper and requires 2x 3090 GPUs with 24GB of VRAM each as well as PyTorch &amp;gt;= 2.0 (FlashAttention). Configurations are managed by &lt;a href=&#34;https://omegaconf.readthedocs.io/&#34;&gt;OmegaConf&lt;/a&gt; so any entry can be overridden from the command line. If you have PyTorch &amp;lt; 2.0 or weaker GPUs, you may thus need to reduce the batch size via:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.train sp+lg_homography \&#xA;    --conf gluefactory/configs/superpoint-open+lightglue_homography.yaml  \&#xA;    data.batch_size=32  # for 1x 1080 GPU&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Be aware that this can impact the overall performance. You might need to adjust the learning rate accordingly.&lt;/p&gt; &#xA; &lt;p&gt;We then fine-tune the model on the MegaDepth dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.train sp+lg_megadepth \&#xA;    --conf gluefactory/configs/superpoint-open+lightglue_megadepth.yaml \&#xA;    train.load_experiment=sp+lg_homography&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Here the default batch size is 32. To speed up training on MegaDepth, we suggest to cache the local features before training (requires around 150 GB of disk space):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# extract features&#xA;python -m gluefactory.scripts.export_megadepth --method sp_open --num_workers 8&#xA;# run training with cached features&#xA;python -m gluefactory.train sp+lg_megadepth \&#xA;    --conf gluefactory/configs/superpoint-open+lightglue_megadepth.yaml \&#xA;    train.load_experiment=sp+lg_homography \&#xA;    data.load_features.do=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The model can then be evaluated using its experiment name:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.eval.megadepth1500 --checkpoint sp+lg_megadepth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can also run all benchmarks after each training epoch with the option &lt;code&gt;--run_benchmarks&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Training GlueStick]&lt;/summary&gt; &#xA; &lt;p&gt;We first pre-train GlueStick on the homography dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.train gluestick_H --conf gluefactory/configs/superpoint+lsd+gluestick-homography.yaml --distributed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Feel free to use any other experiment name. Configurations are managed by &lt;a href=&#34;https://omegaconf.readthedocs.io/&#34;&gt;OmegaConf&lt;/a&gt; so any entry can be overridden from the command line.&lt;/p&gt; &#xA; &lt;p&gt;We then fine-tune the model on the MegaDepth dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m gluefactory.train gluestick_MD --conf gluefactory/configs/superpoint+lsd+gluestick-megadepth.yaml --distributed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Note that we used the training splits &lt;code&gt;train_scenes.txt&lt;/code&gt; and &lt;code&gt;valid_scenes.txt&lt;/code&gt; to train the original model, which contains some overlap with the IMC challenge. The new default splits are now &lt;code&gt;train_scenes_clean.txt&lt;/code&gt; and &lt;code&gt;valid_scenes_clean.txt&lt;/code&gt;, without this overlap.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Available models&lt;/h3&gt; &#xA;&lt;p&gt;Glue Factory supports training and evaluating the following deep matchers:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Training?&lt;/th&gt; &#xA;   &lt;th&gt;Evaluation?&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cvg/LightGlue&#34;&gt;LightGlue&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cvg/GlueStick&#34;&gt;GlueStick&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/magicleap/SuperGluePretrainedNetwork&#34;&gt;SuperGlue&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zju3dv/LoFTR&#34;&gt;LoFTR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Using the following local feature extractors:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LightGlue config&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rpautrat/SuperPoint&#34;&gt;SuperPoint (open)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;superpoint-open+lightglue_{homography,megadepth}.yaml&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/magicleap/SuperPointPretrainedNetwork&#34;&gt;SuperPoint (official)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå TODO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIFT (via &lt;a href=&#34;https://github.com/colmap/pycolmap&#34;&gt;pycolmap&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sift+lightglue_{homography,megadepth}.yaml&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Shiaoming/ALIKED&#34;&gt;ALIKED&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;aliked+lightglue_{homography,megadepth}.yaml&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cvlab-epfl/disk&#34;&gt;DISK&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå TODO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Key.Net + HardNet&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå TODO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Coming soon&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More baselines (LoFTR, ASpanFormer, MatchFormer, SGMNet, DKM, RoMa)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training deep detectors and descriptors like SuperPoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; IMC evaluations&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Better documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please consider citing the following papers if you found this library useful:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{lindenberger_2023_lightglue,&#xA;  title     = {{LightGlue: Local Feature Matching at Light Speed}},&#xA;  author    = {Philipp Lindenberger and&#xA;               Paul-Edouard Sarlin and&#xA;               Marc Pollefeys},&#xA;  booktitle = {International Conference on Computer Vision (ICCV)},&#xA;  year      = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{pautrat_suarez_2023_gluestick,&#xA;  title     = {{GlueStick: Robust Image Matching by Sticking Points and Lines Together}},&#xA;  author    = {R{\&#39;e}mi Pautrat* and&#xA;               Iago Su{\&#39;a}rez* and&#xA;               Yifan Yu and&#xA;               Marc Pollefeys and&#xA;               Viktor Larsson},&#xA;  booktitle = {International Conference on Computer Vision (ICCV)},&#xA;  year      = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>