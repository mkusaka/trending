<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-17T01:35:08Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>open-mmlab/OpenPCDet</title>
    <updated>2022-08-17T01:35:08Z</updated>
    <id>tag:github.com,2022-08-17:/open-mmlab/OpenPCDet</id>
    <link href="https://github.com/open-mmlab/OpenPCDet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenPCDet Toolbox for LiDAR-based 3D Object Detection.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/open_mmlab.png&#34; align=&#34;right&#34; width=&#34;30%&#34;&gt; &#xA;&lt;h1&gt;OpenPCDet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;OpenPCDet&lt;/code&gt; is a clear, simple, self-contained open source project for LiDAR-based 3D object detection.&lt;/p&gt; &#xA;&lt;p&gt;It is also the official code release of &lt;a href=&#34;https://arxiv.org/abs/1812.04244&#34;&gt;&lt;code&gt;[PointRCNN]&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1907.03670&#34;&gt;&lt;code&gt;[Part-A2-Net]&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1912.13192&#34;&gt;&lt;code&gt;[PV-RCNN]&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2012.15712&#34;&gt;&lt;code&gt;[Voxel R-CNN]&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2102.00463&#34;&gt;&lt;code&gt;[PV-RCNN++]&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OpenPCDet&lt;/code&gt; has been updated to &lt;code&gt;v0.5.2&lt;/code&gt; (Jan. 2022).&lt;/li&gt; &#xA; &lt;li&gt;The codes of PV-RCNN++ has been supported.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#openpcdet-design-pattern&#34;&gt;Design Pattern&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#model-zoo&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/INSTALL.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/DEMO.md&#34;&gt;Quick Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/GETTING_STARTED.md&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[2022-07-05] Added support for the 3D object detection backbone network &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Focal_Sparse_Convolutional_Networks_for_3D_Object_Detection_CVPR_2022_paper.pdf&#34;&gt;&lt;code&gt;Focals Conv&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2022-02-12] Added support for using docker. Please refer to the guidance in &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docker&#34;&gt;./docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2022-02-07] Added support for Centerpoint models on Nuscenes Dataset.&lt;/p&gt; &#xA;&lt;p&gt;[2022-01-14] Added support for dynamic pillar voxelization, following the implementation proposed in &lt;a href=&#34;https://arxiv.org/abs/2107.14391&#34;&gt;H^23D R-CNN&lt;/a&gt; with unique operation and &lt;a href=&#34;https://github.com/rusty1s/pytorch_scatter&#34;&gt;&lt;code&gt;torch_scatter&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt; &#xA;&lt;p&gt;[2022-01-05] &lt;strong&gt;NEW:&lt;/strong&gt; Update &lt;code&gt;OpenPCDet&lt;/code&gt; to v0.5.2:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code of &lt;a href=&#34;https://arxiv.org/abs/2102.00463&#34;&gt;PV-RCNN++&lt;/a&gt; has been released to this repo, with higher performance, faster training/inference speed and less memory consumption than PV-RCNN.&lt;/li&gt; &#xA; &lt;li&gt;Add performance of several models trained with full training set of &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#waymo-open-dataset-baselines&#34;&gt;Waymo Open Dataset&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support Lyft dataset, see the pull request &lt;a href=&#34;https://github.com/open-mmlab/OpenPCDet/pull/720&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[2021-12-09] &lt;strong&gt;NEW:&lt;/strong&gt; Update &lt;code&gt;OpenPCDet&lt;/code&gt; to v0.5.1:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add PointPillar related baseline configs/results on &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#waymo-open-dataset-baselines&#34;&gt;Waymo Open Dataset&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support Pandaset dataloader, see the pull request &lt;a href=&#34;https://github.com/open-mmlab/OpenPCDet/pull/396&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support a set of new augmentations, see the pull request &lt;a href=&#34;https://github.com/open-mmlab/OpenPCDet/pull/653&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[2021-12-01] &lt;strong&gt;NEW:&lt;/strong&gt; &lt;code&gt;OpenPCDet&lt;/code&gt; v0.5.0 is released with the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improve the performance of all models on &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#waymo-open-dataset-baselines&#34;&gt;Waymo Open Dataset&lt;/a&gt;. Note that you need to re-prepare the training/validation data and ground-truth database of Waymo Open Dataset (see &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/GETTING_STARTED.md&#34;&gt;GETTING_STARTED.md&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Support anchor-free &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/pcdet/models/dense_heads/center_head.py&#34;&gt;CenterHead&lt;/a&gt;, add configs of &lt;code&gt;CenterPoint&lt;/code&gt; and &lt;code&gt;PV-RCNN with CenterHead&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support lastest &lt;strong&gt;PyTorch 1.1~1.10&lt;/strong&gt; and &lt;strong&gt;spconv 1.0~2.x&lt;/strong&gt;, where &lt;strong&gt;spconv 2.x&lt;/strong&gt; should be easy to install with pip and faster than previous version (see the official update of spconv &lt;a href=&#34;https://github.com/traveller59/spconv&#34;&gt;here&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Support config &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/dataset_configs/waymo_dataset.yaml&#34;&gt;&lt;code&gt;USE_SHARED_MEMORY&lt;/code&gt;&lt;/a&gt; to use shared memory to potentially speed up the training process in case you suffer from an IO problem.&lt;/li&gt; &#xA; &lt;li&gt;Support better and faster &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/visual_utils/open3d_vis_utils.py&#34;&gt;visualization script&lt;/a&gt;, and you need to install &lt;a href=&#34;https://github.com/isl-org/Open3D&#34;&gt;Open3D&lt;/a&gt; firstly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[2021-06-08] Added support for the voxel-based 3D object detection model &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#KITTI-3D-Object-Detection-Baselines&#34;&gt;&lt;code&gt;Voxel R-CNN&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2021-05-14] Added support for the monocular 3D object detection model &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#KITTI-3D-Object-Detection-Baselines&#34;&gt;&lt;code&gt;CaDDN&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2020-11-27] Bugfixed: Please re-prepare the validation infos of Waymo dataset (version 1.2) if you would like to use our provided Waymo evaluation tool (see &lt;a href=&#34;https://github.com/open-mmlab/OpenPCDet/pull/383&#34;&gt;PR&lt;/a&gt;). Note that you do not need to re-prepare the training data and ground-truth database.&lt;/p&gt; &#xA;&lt;p&gt;[2020-11-10] The &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#waymo-open-dataset-baselines&#34;&gt;Waymo Open Dataset&lt;/a&gt; has been supported with state-of-the-art results. Currently we provide the configs and results of &lt;code&gt;SECOND&lt;/code&gt;, &lt;code&gt;PartA2&lt;/code&gt; and &lt;code&gt;PV-RCNN&lt;/code&gt; on the Waymo Open Dataset, and more models could be easily supported by modifying their dataset configs.&lt;/p&gt; &#xA;&lt;p&gt;[2020-08-10] Bugfixed: The provided NuScenes models have been updated to fix the loading bugs. Please redownload it if you need to use the pretrained NuScenes models.&lt;/p&gt; &#xA;&lt;p&gt;[2020-07-30] &lt;code&gt;OpenPCDet&lt;/code&gt; v0.3.0 is released with the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Point-based and Anchor-Free models (&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#KITTI-3D-Object-Detection-Baselines&#34;&gt;&lt;code&gt;PointRCNN&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#KITTI-3D-Object-Detection-Baselines&#34;&gt;&lt;code&gt;PartA2-Free&lt;/code&gt;&lt;/a&gt;) are supported now.&lt;/li&gt; &#xA; &lt;li&gt;The NuScenes dataset is supported with strong baseline results (&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#NuScenes-3D-Object-Detection-Baselines&#34;&gt;&lt;code&gt;SECOND-MultiHead (CBGS)&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/#NuScenes-3D-Object-Detection-Baselines&#34;&gt;&lt;code&gt;PointPillar-MultiHead&lt;/code&gt;&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;High efficiency than last version, support &lt;strong&gt;PyTorch 1.1~1.7&lt;/strong&gt; and &lt;strong&gt;spconv 1.0~1.2&lt;/strong&gt; simultaneously.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[2020-07-17] Add simple visualization codes and a quick demo to test with custom data.&lt;/p&gt; &#xA;&lt;p&gt;[2020-06-24] &lt;code&gt;OpenPCDet&lt;/code&gt; v0.2.0 is released with pretty new structures to support more models and datasets.&lt;/p&gt; &#xA;&lt;p&gt;[2020-03-16] &lt;code&gt;OpenPCDet&lt;/code&gt; v0.1.0 is released.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;h3&gt;What does &lt;code&gt;OpenPCDet&lt;/code&gt; toolbox do?&lt;/h3&gt; &#xA;&lt;p&gt;Note that we have upgrated &lt;code&gt;PCDet&lt;/code&gt; from &lt;code&gt;v0.1&lt;/code&gt; to &lt;code&gt;v0.2&lt;/code&gt; with pretty new structures to support various datasets and models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;OpenPCDet&lt;/code&gt; is a general PyTorch-based codebase for 3D object detection from point cloud. It currently supports multiple state-of-the-art 3D object detection methods with highly refactored codes for both one-stage and two-stage 3D detection frameworks.&lt;/p&gt; &#xA;&lt;p&gt;Based on &lt;code&gt;OpenPCDet&lt;/code&gt; toolbox, we win the Waymo Open Dataset challenge in &lt;a href=&#34;https://waymo.com/open/challenges/3d-detection/&#34;&gt;3D Detection&lt;/a&gt;, &lt;a href=&#34;https://waymo.com/open/challenges/3d-tracking/&#34;&gt;3D Tracking&lt;/a&gt;, &lt;a href=&#34;https://waymo.com/open/challenges/domain-adaptation/&#34;&gt;Domain Adaptation&lt;/a&gt; three tracks among all LiDAR-only methods, and the Waymo related models will be released to &lt;code&gt;OpenPCDet&lt;/code&gt; soon.&lt;/p&gt; &#xA;&lt;p&gt;We are actively updating this repo currently, and more datasets and models will be supported soon. Contributions are also welcomed.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;OpenPCDet&lt;/code&gt; design pattern&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data-Model separation with unified point cloud coordinate for easily extending to custom datasets:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/dataset_vs_model.png&#34; width=&#34;95%&#34; height=&#34;320&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Unified 3D box definition: (x, y, z, dx, dy, dz, heading).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Flexible and clear model structure to easily support various 3D detection models:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/model_framework.png&#34; width=&#34;95%&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support various models within one framework as:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/multiple_models_demo.png&#34; width=&#34;95%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Currently Supported Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support both one-stage and two-stage 3D object detection frameworks&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support distributed training &amp;amp; testing with multiple GPUs and multiple machines&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support multiple heads on different scales to detect different classes&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support stacked version set abstraction to encode various number of points in different scenes&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support Adaptive Training Sample Selection (ATSS) for target assignment&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support RoI-aware point cloud pooling &amp;amp; RoI-grid point cloud pooling&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support GPU version 3D IoU calculation and rotated NMS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;h3&gt;KITTI 3D Object Detection Baselines&lt;/h3&gt; &#xA;&lt;p&gt;Selected supported methods are shown in the below table. The results are the 3D detection performance of moderate difficulty on the &lt;em&gt;val&lt;/em&gt; set of KITTI dataset.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All LiDAR-based models are trained with 8 GTX 1080Ti GPUs and are available for download.&lt;/li&gt; &#xA; &lt;li&gt;The training time is measured with 8 TITAN XP GPUs and PyTorch 1.5.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;training time&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Car@R11&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Pedestrian@R11&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cyclist@R11&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/pointpillar.yaml&#34;&gt;PointPillar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~1.2 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1wMxWTpU1qUoY3DsCH31WJmvJxcjFXKlm/view?usp=sharing&#34;&gt;model-18M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/second.yaml&#34;&gt;SECOND&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~1.7 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1-01zsPOsqanZQqIIyy7FpNXStL3y4jdR/view?usp=sharing&#34;&gt;model-20M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/second_iou.yaml&#34;&gt;SECOND-IoU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1AQkeNs4bxhvhDQ-5sEo_yvQUlfo73lsW/view?usp=sharing&#34;&gt;model-46M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/pointrcnn.yaml&#34;&gt;PointRCNN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~3 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.70&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1BCX9wMn-GYAfSOPpyxf6Iv6fc0qKLSiU/view?usp=sharing&#34;&gt;model-16M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/pointrcnn_iou.yaml&#34;&gt;PointRCNN-IoU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~3 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.75&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.34&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1V0vNZ3lAHpEEt0MlT80eL2f41K2tHm_D/view?usp=sharing&#34;&gt;model-16M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/PartA2_free.yaml&#34;&gt;Part-A2-Free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~3.8 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1lcUUxF8mJgZ_e-tZhP1XNQtTBuC-R0zr/view?usp=sharing&#34;&gt;model-226M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/PartA2.yaml&#34;&gt;Part-A2-Anchor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~4.3 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.90&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/10GK1aCkLqxGNeX3lVu8cLZyE0G8002hY/view?usp=sharing&#34;&gt;model-244M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/pv_rcnn.yaml&#34;&gt;PV-RCNN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~5 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.61&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.90&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.47&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1lIOq4Hxr0W3qsX83ilQv0nk1Cls6KAr-/view?usp=sharing&#34;&gt;model-50M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/voxel_rcnn_car.yaml&#34;&gt;Voxel R-CNN (Car)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~2.2 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/19_jiAeGLz7V0wNjSJw4cKmMjdm5EW5By/view?usp=sharing&#34;&gt;model-28M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/voxel_rcnn_car_focal_multimodal.yaml&#34;&gt;Focals Conv - F&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~4 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.66&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1u2Vcg7gZPOI-EqrHy7_6fqaibvRt2IjQ/view?usp=sharing&#34;&gt;model-30M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/kitti_models/CaDDN.yaml&#34;&gt;CaDDN (Mono)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~15 hours&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.76&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1OQTO2PtXT8GGr35W9m2GZGuqgb6fyU1V/view?usp=sharing&#34;&gt;model-774M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Waymo Open Dataset Baselines&lt;/h3&gt; &#xA;&lt;p&gt;We provide the setting of &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/dataset_configs/waymo_dataset.yaml&#34;&gt;&lt;code&gt;DATA_CONFIG.SAMPLED_INTERVAL&lt;/code&gt;&lt;/a&gt; on the Waymo Open Dataset (WOD) to subsample partial samples for training and evaluation, so you could also play with WOD by setting a smaller &lt;code&gt;DATA_CONFIG.SAMPLED_INTERVAL&lt;/code&gt; even if you only have limited GPU resources.&lt;/p&gt; &#xA;&lt;p&gt;By default, all models are trained with &lt;strong&gt;a single frame&lt;/strong&gt; of &lt;strong&gt;20% data (~32k frames)&lt;/strong&gt; of all the training samples on 8 GTX 1080Ti GPUs, and the results of each cell here are mAP/mAPH calculated by the official Waymo evaluation metrics on the &lt;strong&gt;whole&lt;/strong&gt; validation set (version 1.2).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Performance@(train with 20% Data)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Vec_L1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Vec_L2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Ped_L1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Ped_L2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cyc_L1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cyc_L2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/second.yaml&#34;&gt;SECOND&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;70.96/70.34&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.58/62.02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.23/54.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.22/47.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.13/55.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.97/53.53&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/pointpillar_1x.yaml&#34;&gt;PointPillar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;70.43/69.83&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.18/61.64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.21/46.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.18/40.64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.26/51.75&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.18/49.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/centerpoint_pillar_1x.yaml&#34;&gt;CenterPoint-Pillar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;70.50/69.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.18/61.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.11/61.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.06/55.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.44/63.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.98/61.46&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/centerpoint_dyn_pillar_1x.yaml&#34;&gt;CenterPoint-Dynamic-Pillar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;70.46/69.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.06/61.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.92/63.35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.91/56.33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.24/64.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.73/62.24&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/centerpoint_without_resnet.yaml&#34;&gt;CenterPoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;71.33/70.76&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.16/62.65&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.09/65.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.27/58.23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.68/67.39&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.11/64.87&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/centerpoint.yaml&#34;&gt;CenterPoint (ResNet)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;72.76/72.23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.91/64.42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.19/67.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.03/60.34&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.04/69.79&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.49/67.28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/PartA2.yaml&#34;&gt;Part-A2-Anchor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;74.66/74.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.82/65.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.71/62.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.46/54.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.53/65.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.05/62.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/pv_rcnn.yaml&#34;&gt;PV-RCNN (AnchorHead)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75.41/74.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.44/66.80&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.98/61.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.70/53.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.88/64.25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.39/61.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/pv_rcnn_with_centerhead_rpn.yaml&#34;&gt;PV-RCNN (CenterHead)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75.95/75.43&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.02/67.54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.94/69.40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.66/61.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.18/68.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.73/66.57&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/voxel_rcnn_with_centerhead_dyn_voxel.yaml&#34;&gt;Voxel R-CNN (CenterHead)-Dynamic-Voxel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;76.13/75.66&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.18/67.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.20/71.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.29/63.59&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.75/69.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.25/67.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/pv_rcnn_plusplus.yaml&#34;&gt;PV-RCNN++&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.82/77.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.07/68.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.99/71.36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.92/63.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.80/70.71&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.31/68.26&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/pv_rcnn_plusplus_resnet.yaml&#34;&gt;PV-RCNN++ (ResNet)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.61/77.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.18/68.75&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.42/73.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.88/65.21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.50/71.39&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.84/68.77&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Here we also provide the performance of several models trained on the full training set (refer to the paper of &lt;a href=&#34;https://arxiv.org/abs/2102.00463&#34;&gt;PV-RCNN++&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Performance@(train with 100% Data)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Vec_L1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Vec_L2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Ped_L1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Ped_L2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cyc_L1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cyc_L2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/second.yaml&#34;&gt;SECOND&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;72.27/71.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.85/63.33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.70/58.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.72/51.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.62/59.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.34/57.05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/PartA2.yaml&#34;&gt;Part-A2-Anchor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.05/76.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.47/67.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.24/66.87&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.18/58.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.60/67.36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.13/64.93&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/pv_rcnn_with_centerhead_rpn.yaml&#34;&gt;PV-RCNN (CenterHead)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;78.00/77.50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.43/68.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.21/73.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.42/64.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.46/70.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.95/67.79&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/pv_rcnn_plusplus.yaml&#34;&gt;PV-RCNN++&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.10/78.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.34/69.91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.62/74.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.86/66.30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.49/72.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.70/69.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/waymo_models/pv_rcnn_plusplus_resnet.yaml&#34;&gt;PV-RCNN++ (ResNet)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.25/78.78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.61/70.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.83/76.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.17/68.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.72/72.66&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.21/70.19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We could not provide the above pretrained models due to &lt;a href=&#34;https://waymo.com/open/terms/&#34;&gt;Waymo Dataset License Agreement&lt;/a&gt;, but you could easily achieve similar performance by training with the default configs.&lt;/p&gt; &#xA;&lt;h3&gt;NuScenes 3D Object Detection Baselines&lt;/h3&gt; &#xA;&lt;p&gt;All models are trained with 8 GTX 1080Ti GPUs and are available for download.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;mATE&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mASE&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAOE&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAVE&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAAE&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;NDS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/nuscenes_models/cbgs_pp_multihead.yaml&#34;&gt;PointPillar-MultiHead&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;33.87&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1p-501mTWsq0G9RzroTWSXreIMyTUUpBM/view?usp=sharing&#34;&gt;model-23M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/nuscenes_models/cbgs_second_multihead.yaml&#34;&gt;SECOND-MultiHead (CBGS)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;31.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.46&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.59&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1bNzcOnE3u9iooBFMk2xK7HqhdeQ_nwTq/view?usp=sharing&#34;&gt;model-35M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/nuscenes_models/cbgs_dyn_pp_centerpoint.yaml&#34;&gt;CenterPoint-PointPillar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;31.13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.92&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.90&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.70&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1UvGm6mROMyJzeSRu7OD1leU_YWoAZG7v/view?usp=sharing&#34;&gt;model-23M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/nuscenes_models/cbgs_voxel01_res3d_centerpoint.yaml&#34;&gt;CenterPoint (voxel_size=0.1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;30.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.55&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.94&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.87&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Cz-J1c3dw7JAWc25KRG1XQj8yCaOlexQ/view?usp=sharing&#34;&gt;model-34M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/tools/cfgs/nuscenes_models/cbgs_voxel0075_res3d_centerpoint.yaml&#34;&gt;CenterPoint (voxel_size=0.075)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;28.80&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.43&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.55&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.48&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1XOHAWm1MPkCKr1gqmc3TWi5AYZgPsgxU/view?usp=sharing&#34;&gt;model-34M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Other datasets&lt;/h3&gt; &#xA;&lt;p&gt;Welcome to support other datasets by submitting pull request.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt; for the installation of &lt;code&gt;OpenPCDet&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Demo&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/DEMO.md&#34;&gt;DEMO.md&lt;/a&gt; for a quick demo to test with a pretrained model and visualize the predicted results on your custom data or the original KITTI data.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/docs/GETTING_STARTED.md&#34;&gt;GETTING_STARTED.md&lt;/a&gt; to learn more usage about this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;OpenPCDet&lt;/code&gt; is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/OpenPCDet/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;OpenPCDet&lt;/code&gt; is an open source project for LiDAR-based 3D scene perception that supports multiple LiDAR-based perception models as shown above. Some parts of &lt;code&gt;PCDet&lt;/code&gt; are learned from the official released codes of the above supported methods. We would like to thank for their proposed methods and the official implementation.&lt;/p&gt; &#xA;&lt;p&gt;We hope that this repo could serve as a strong and flexible codebase to benefit the research community by speeding up the process of reimplementing previous works and/or developing new methods.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{openpcdet2020,&#xA;    title={OpenPCDet: An Open-source Toolbox for 3D Object Detection from Point Clouds},&#xA;    author={OpenPCDet Development Team},&#xA;    howpublished = {\url{https://github.com/open-mmlab/OpenPCDet}},&#xA;    year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to be a member of the OpenPCDet development team by contributing to this repo, and feel free to contact us for any potential contributions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vnhacker1337/CVE-2022-27925-PoC</title>
    <updated>2022-08-17T01:35:08Z</updated>
    <id>tag:github.com,2022-08-17:/vnhacker1337/CVE-2022-27925-PoC</id>
    <link href="https://github.com/vnhacker1337/CVE-2022-27925-PoC" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Zimbra RCE simple poc&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2022-27925-PoC&lt;/h1&gt; &#xA;&lt;p&gt;Zimbra RCE simple poc&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/KeypointNeRF</title>
    <updated>2022-08-17T01:35:08Z</updated>
    <id>tag:github.com,2022-08-17:/facebookresearch/KeypointNeRF</id>
    <link href="https://github.com/facebookresearch/KeypointNeRF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;KeypointNeRF Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints&lt;/h1&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;font-size:16px&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://markomih.github.io/&#34;&gt;&lt;strong&gt;Marko Mihajlovic&lt;/strong&gt;&lt;/a&gt; · &lt;a target=&#34;_blank&#34; href=&#34;https://www.aayushbansal.xyz/&#34;&gt;&lt;strong&gt;Aayush Bansal&lt;/strong&gt;&lt;/a&gt; · &lt;a target=&#34;_blank&#34; href=&#34;https://zollhoefer.com/&#34;&gt;&lt;strong&gt;Michael Zollhoefer&lt;/strong&gt;&lt;/a&gt; . &lt;a target=&#34;_blank&#34; href=&#34;https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html&#34;&gt;&lt;strong&gt;Siyu Tang&lt;/strong&gt;&lt;/a&gt; · &lt;a target=&#34;_blank&#34; href=&#34;http://www-scf.usc.edu/~saitos/&#34;&gt;&lt;strong&gt;Shunsuke Saito&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;ECCV 2022&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&lt;/div&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/KeypointNeRF/main/assets/teaser.gif&#34; alt=&#34;Logo&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;&lt;img alt=&#34;PyTorch&#34; src=&#34;https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytorchlightning.ai/&#34;&gt;&lt;img alt=&#34;Lightning&#34; src=&#34;https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.04992&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-green?style=for-the-badge&amp;amp;logo=arXiv&amp;amp;logoColor=green&#34; alt=&#34;Paper PDF&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://markomih.github.io/KeypointNeRF/&#34; style=&#34;padding-left: 0.5rem;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/KeypointNeRF-Page-orange?style=for-the-badge&amp;amp;logo=Google%20chrome&amp;amp;logoColor=orange&#34; alt=&#34;Project Page&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://youtu.be/RMs1S5k9vrk&#34;&gt;&lt;img alt=&#34;youtube views&#34; title=&#34;Subscribe to my YouTube channel&#34; src=&#34;https://img.shields.io/youtube/views/RMs1S5k9vrk?logo=youtube&amp;amp;labelColor=ce4630&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://paperswithcode.com/sota/generalizable-novel-view-synthesis-on-zju?p=keypointnerf-generalizing-image-based&#34;&gt; &lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/keypointnerf-generalizing-image-based/generalizable-novel-view-synthesis-on-zju&#34; alt=&#34;Generalizable Novel View Synthesis&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;KeypointNeRF is a generalizable neural radiance field for virtual avatars. Given as input 2-3 images, KeypointNeRF generates volumetric radiance representation that can be rendered from novel views.&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please install python dependencies specified in &lt;code&gt;environment.yml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yml&#xA;conda activate KeypointNeRF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/KeypointNeRF/main/DATA_PREP.md&#34;&gt;DATA_PREP.md&lt;/a&gt; to setup the ZJU-MoCap dataset.&lt;/p&gt; &#xA;&lt;p&gt;After this step the data directory follows the structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./data/zju_mocap&#xA;├── CoreView_313&#xA;├── CoreView_315&#xA;├── CoreView_377&#xA;├── CoreView_386&#xA;├── CoreView_387&#xA;├── CoreView_390&#xA;├── CoreView_392&#xA;├── CoreView_393&#xA;├── CoreView_394&#xA;└── CoreView_396&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train your own model on the ZJU dataset&lt;/h2&gt; &#xA;&lt;p&gt;Execute &lt;code&gt;train.py&lt;/code&gt; script to train the model on the ZJU dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --config ./configs/zju.json --data_root ./data/zju_mocap&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the training, the model checkpoint will be stored under &lt;code&gt;./EXPERIMENTS/zju/ckpts/last.ckpt&lt;/code&gt;, which is equivalent to the one provided &lt;a href=&#34;https://drive.google.com/file/d/1rsMb3DFFXaFw0iK7yoUmoDEaCW_XqfaN/view?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To extract render and evaluate images, execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --config ./configs/zju.json --data_root ./data/zju_mocap --run_val&#xA;python eval_zju.py --src_dir ./EXPERIMENTS/zju/images_v3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To visualize the dynamic results, execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python render_dynamic.py --config ./configs/zju.json --data_root ./data/zju_mocap --model_ckpt ./EXPERIMENTS/zju/ckpts/last.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&lt;/div&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/KeypointNeRF/main/assets/zju_result_sub393.gif&#34; alt=&#34;Logo&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;font-size:12px&#34;&gt; (The first three views of an unseen subject are the input to KeypointNeRF; the last image is a rendered novel view) &lt;/p&gt; &#xA;&lt;p&gt;We compare KeypointNeRF with recent state-of-the-art methods. The evaluation metric is SSIM and PSNR.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;PSNR ↑&lt;/th&gt; &#xA;   &lt;th&gt;SSIM ↑&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pixelNeRF &lt;font size=&#34;1&#34;&gt;(Yu et al., CVPR&#39;21)&lt;/font&gt;&lt;/td&gt; &#xA;   &lt;td&gt;23.17&lt;/td&gt; &#xA;   &lt;td&gt;86.93&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PVA &lt;font size=&#34;1&#34;&gt;(Raj et al., CVPR&#39;21)&lt;/font&gt;&lt;/td&gt; &#xA;   &lt;td&gt;23.15&lt;/td&gt; &#xA;   &lt;td&gt;86.63&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NHP &lt;font size=&#34;1&#34;&gt;(Kwon et al., NeurIPS&#39;21)&lt;/font&gt;&lt;/td&gt; &#xA;   &lt;td&gt;24.75&lt;/td&gt; &#xA;   &lt;td&gt;90.58&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KeypointNeRF* &lt;font size=&#34;1&#34;&gt;(Mihajlovic et al., ECCV&#39;22)&lt;/font&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;25.86&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;91.07&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;left&#34; style=&#34;font-size:10px&#34;&gt; (*Note that results of KeypointNeRF are slightly higher compared to the numbers reported in the original paper due to training views not beeing shuffled during training.) &lt;/p&gt; &#xA;&lt;h2&gt;Publication&lt;/h2&gt; &#xA;&lt;p&gt;If you find our code or paper useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Mihajlovic:ECCV2022,&#xA;  title = {{KeypointNeRF}: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints},&#xA;  author = {Mihajlovic, Marko and Bansal, Aayush and Zollhoefer, Michael and Tang, Siyu and Saito, Shunsuke},&#xA;  booktitle={European conference on computer vision},&#xA;  year={2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/legalcode&#34;&gt;CC-BY-NC 4.0&lt;/a&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/KeypointNeRF/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
</feed>