<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-28T01:43:44Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sahil280114/codealpaca</title>
    <updated>2023-03-28T01:43:44Z</updated>
    <id>tag:github.com,2023-03-28:/sahil280114/codealpaca</id>
    <link href="https://github.com/sahil280114/codealpaca" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Code Alpaca: An Instruction-following LLaMA Model trained on code generation instructions&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the repo for the Code Alpaca project, which aims to build and share an instruction-following LLaMA model for code generation. This repo is fully based on &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; ,and only changes the data used for training. Training approach is the same.&lt;/p&gt; &#xA;&lt;p&gt;The repo contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/#data-release&#34;&gt;20K data&lt;/a&gt; used for fine-tuning the model&lt;/li&gt; &#xA; &lt;li&gt;The code for &lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/#data-generation-process&#34;&gt;generating the data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The code for &lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/#fine-tuning&#34;&gt;fine-tuning the model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Demo for the model can be found &lt;a href=&#34;https://code-alpaca-demo.vercel.app/&#34;&gt;https://code-alpaca-demo.vercel.app/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The Code Alpaca models are fine-tuned from a 7B and 13B LLaMA model on 20K instruction-following data generated by the techniques in the Self-Instruct [1] paper, with some modifications that we discuss in the next section. Evals are still a todo.&lt;/p&gt; &#xA;&lt;p&gt;The model is not finetuned to be safe and harmless, so be cautious.&lt;/p&gt; &#xA;&lt;p&gt;Current release contains the data generation procedure, dataset, and training code. Model weights aren&#39;t part of the release for now, to respect OpenAI TOS and LLaMA license.&lt;/p&gt; &#xA;&lt;p&gt;[1]: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;https://arxiv.org/abs/2212.10560&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Data Release&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/data/code_alpaca_20k.json&#34;&gt;&lt;code&gt;data/code_alpaca_20k.json&lt;/code&gt;&lt;/a&gt; contains 20K instruction-following data used for fine-tuning the Code Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;instruction&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, describes the task the model should perform. Each of the 20K instructions is unique.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;input&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, optional context or input for the task. For example, when the instruction is &#34;Amend the following SQL query to select distinct elements&#34;, the input is the SQL query. Around 40% of the examples have an input.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, the answer to the instruction as generated by &lt;code&gt;text-davinci-003&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We used the following prompts for fine-tuning the model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with a non-empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Input:&#xA;{input}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with an empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During inference (eg for the web demo), we use the user instruction with an empty input field (second option).&lt;/p&gt; &#xA;&lt;h2&gt;Data Generation Process&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;strong&gt; Running the code &lt;/strong&gt; &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set environment variables &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to your OpenAI API key.&lt;/li&gt; &#xA;  &lt;li&gt;Install the dependencies with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;python -m generate_instruction generate_instruction_following_data&lt;/code&gt; to generate the data.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; Data generation pipeline had minor changes from [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) - Modified prompt to focus on code generation/editing/optimization tasks instead of general tasks. - Modified seed tasks to only be related to code generation. &#xA;&lt;p&gt;This produced an instruction-following dataset with 20K examples obtained at a much lower cost (less than $200). Also including a smaller 2k samples dataset which was used to derisk the approach and quality of the model.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;Finetuned the models using standard Hugging Face training code and deepspeed with the following hyperparameters:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Learning rate&lt;/td&gt; &#xA;   &lt;td&gt;2e-5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Epochs&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Max length&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Weight decay&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Given Hugging Face hasn&#39;t officially supported the LLaMA models, we fine-tuned LLaMA with Hugging Face&#39;s transformers library by installing it from a particular fork (i.e. this &lt;a href=&#34;https://github.com/huggingface/transformers/pull/21955&#34;&gt;PR&lt;/a&gt; to be merged). The hash of the specific commit we installed was &lt;code&gt;68d640f7c368bcaaaecfc678f11908ebbd3d6176&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The code runs on a 8xA100 80GB, but can also run on 8xA10040GB or 4xA100 with lower batch size and gradient accumulation steps. To get the GPUs, I suggest using &lt;a href=&#34;https://cloud.lambdalabs.com/login?redirect_to=/instances?&#34;&gt;Lambda Labs&lt;/a&gt;, best pricing for the best hardware.&lt;/p&gt; &#xA;&lt;p&gt;To reproduce the fine-tuning runs for LLaMA, first install the requirements&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install the particular fork of Hugging Face&#39;s transformers library.&lt;/p&gt; &#xA;&lt;p&gt;Below is a command that fine-tunes LLaMA-7B with our dataset on a machine with 4 A100 80G GPUs in FSDP &lt;code&gt;full_shard&lt;/code&gt; mode. We were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using &lt;strong&gt;Python 3.10&lt;/strong&gt;. Replace &lt;code&gt;&amp;lt;your_random_port&amp;gt;&lt;/code&gt; with a port of your own, &lt;code&gt;&amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt;&lt;/code&gt; with the path to your converted checkpoint and tokenizer (following instructions in the PR), and &lt;code&gt;&amp;lt;your_output_dir&amp;gt;&lt;/code&gt; with where you want to store your outputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=8 --master_port=&amp;lt;your_random_port&amp;gt; train.py \&#xA;    --model_name_or_path &amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt;&#xA;    --data_path ./data/code_alpaca_20k.json \&#xA;    --fp16 True \&#xA;    --output_dir &amp;lt;your_output_dir&amp;gt; \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 8 \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 500 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --deepspeed ds_config.json&#xA;    --tf32 False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the given training script is meant to be simple and easy to use, and is not particularly optimized.&lt;/p&gt; &#xA;&lt;p&gt;For convenience I have included the &lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/convert_to_hf.py&#34;&gt;&lt;code&gt;convert_to_hf.py&lt;/code&gt;&lt;/a&gt; to covnert llama checkpoints to huggingface compatible checkpoints. (This file is taken from the hugginface transformers repo)&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;Cite this repo if you want to, or don&#39;t, both are fine.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{codealpaca,&#xA;  author = {Sahil Chaudhary},&#xA;  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/sahil280114/codealpaca}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Naturally, you should also cite the original LLaMA paper [1] and the Self-Instruct paper [2] and the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca repo&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>GammaTauAI/reflexion-human-eval</title>
    <updated>2023-03-28T01:43:44Z</updated>
    <id>tag:github.com,2023-03-28:/GammaTauAI/reflexion-human-eval</id>
    <link href="https://github.com/GammaTauAI/reflexion-human-eval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An implementation of a Reflexion agent for SOTA Human-Eval Python results.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mastering HumanEval with Reflexion&lt;/h1&gt; &#xA;&lt;p&gt;This is a spin-off project inspired by the paper: &lt;a href=&#34;https://arxiv.org/abs/2303.11366&#34;&gt;Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath. &lt;em&gt;Preprint&lt;/em&gt;, 2023&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Read more about this project in this &lt;a href=&#34;https://nanothoughts.substack.com/p/reflecting-on-reflexion&#34;&gt;post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Check out an interesting type-inference implementation here: &lt;a href=&#34;https://github.com/GammaTauAI/opentau&#34;&gt;OpenTau&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Check out the code for the original paper &lt;a href=&#34;https://github.com/noahshinn024/reflexion&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions, please contact &lt;a href=&#34;https://raw.githubusercontent.com/GammaTauAI/reflexion-human-eval/main/noahshinn024@gmail.com&#34;&gt;noahshinn024@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GammaTauAI/reflexion-human-eval/main/media/architecture.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GammaTauAI/reflexion-human-eval/main/media/performance.png&#34; alt=&#34;result&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Note&lt;/h3&gt; &#xA;&lt;p&gt;Due to the nature of these experiments, it may not be feasible for individual developers to rerun the results due to limited access to GPT-4 and significant API charges. Due to recent requests, both trials have been rerun once more and are dumped in &lt;code&gt;./root&lt;/code&gt; with a script &lt;a href=&#34;https://github.com/noahshinn024/reflexion-human-eval/raw/main/validate_py_results.py&#34;&gt;here&lt;/a&gt; to validate the solutions with the unit tests provided by &lt;a href=&#34;https://github.com/openai/human-eval&#34;&gt;HumanEval&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run the validation on your log files or the provided log files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./validate_py_results.py &amp;lt;path to jsonlines file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Warning&lt;/h3&gt; &#xA;&lt;p&gt;Please do not run the Reflexion agent in an unsecure environment as the generated code is not validated before execution.&lt;/p&gt; &#xA;&lt;h3&gt;Cite&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This is a spin-off implementation that implements a relaxation on the internal success criteria proposed in the &lt;a href=&#34;https://arxiv.org/abs/2303.11366&#34;&gt;original paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{shinn2023reflexion,&#xA;  title={Reflexion: an autonomous agent with dynamic memory and self-reflection},&#xA;  author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},&#xA;  journal={arXiv preprint arXiv:2303.11366},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>stochasticai/xturing</title>
    <updated>2023-03-28T01:43:44Z</updated>
    <id>tag:github.com,2023-03-28:/stochasticai/xturing</id>
    <link href="https://github.com/stochasticai/xturing" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build and control your own LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/.github/stochastic_logo_light.svg#gh-light-mode-only&#34; width=&#34;250&#34; alt=&#34;Stochastic.ai&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/.github/stochastic_logo_dark.svg#gh-dark-mode-only&#34; width=&#34;250&#34; alt=&#34;Stochastic.ai&#34;&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;Build and control your own LLMs&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;code&gt;xturing&lt;/code&gt; provides fast, efficient and simple fine-tuning of LLMs, such as LLaMA, GPT-J, GPT-2, and more. It supports both single GPU and multi-GPU training. Leverage memory-efficient fine-tuning techniques like LoRA to reduce your hardware costs by up to 90% and train your models in a fraction of the time.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Documentation - &lt;a href=&#34;https://xturing.stochastic.ai/&#34;&gt;https://xturing.stochastic.ai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install xturing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;üöÄ Quickstart&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from xturing.datasets import InstructionDataset&#xA;from xturing.models import BaseModel&#xA;&#xA;# Load the dataset&#xA;instruction_dataset = InstructionDataset(&#34;./alpaca_data&#34;)&#xA;&#xA;# Initialize the model&#xA;model = BaseModel.create(&#34;llama_lora&#34;)&#xA;&#xA;# Finetune the model&#xA;model.finetune(dataset=instruction_dataset)&#xA;&#xA;# Perform inference&#xA;output = model.generate(texts=[&#34;Why LLM models are becoming so important?&#34;])&#xA;&#xA;print(&#34;Generated output by the model: {}&#34;.format(output))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find the data folder &lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/alpaca_data&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;üìö Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/preparing_your_dataset.py&#34;&gt;Preparing your dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/llama_lora.py&#34;&gt;LLaMA efficient fine-tuning with LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/llama.py&#34;&gt;LLaMA fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/gptj/gptj_lora.py&#34;&gt;GPT-J efficient fine-tuning with LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/gpt2/gpt2_lora.py&#34;&gt;GPT-2 efficient fine-tuning with LoRA&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://drive.google.com/file/d/1Sh-ocNpKn9pS7jv6oBb_Q8DitFyj1avL/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;üìä Performance&lt;/h2&gt; &#xA;&lt;p&gt;Here is a comparison for the performance of different fine-tuning techniques on the LLaMA 7B model. We use the &lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/alpaca_data/&#34;&gt;Alpaca dataset&lt;/a&gt; for fine-tuning. The dataset contains 52K instructions.&lt;/p&gt; &#xA;&lt;p&gt;Hardware:&lt;/p&gt; &#xA;&lt;p&gt;4xA100 40GB GPU, 335GB CPU RAM&lt;/p&gt; &#xA;&lt;p&gt;Fine-tuning parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;{&#xA;  &#39;maximum sequence length&#39;: 512,&#xA;  &#39;batch size&#39;: 1,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;LLaMA 7B&lt;/th&gt; &#xA;   &lt;th&gt;DeepSpeed + CPU Offloading&lt;/th&gt; &#xA;   &lt;th&gt;LoRA + DeepSpeed&lt;/th&gt; &#xA;   &lt;th&gt;LoRA + DeepSpeed + CPU Offloading&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;   &lt;td&gt;33.5 GB&lt;/td&gt; &#xA;   &lt;td&gt;23.7 GB&lt;/td&gt; &#xA;   &lt;td&gt;21.9 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt;190 GB&lt;/td&gt; &#xA;   &lt;td&gt;10.2 GB&lt;/td&gt; &#xA;   &lt;td&gt;14.9 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Time per epoch&lt;/td&gt; &#xA;   &lt;td&gt;21 hours&lt;/td&gt; &#xA;   &lt;td&gt;20 mins&lt;/td&gt; &#xA;   &lt;td&gt;20 mins&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please submit your performance results on other GPUs. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìà Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support for LLaMA, GPT-J, GPT-2&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Dataset generation using self-instruction&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 2x more memory-efficient fine-tuning vs LoRA and unsupervised fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Evaluation of LLM models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support for Stable Diffusion&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;ü§ù Help and Support&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, you can create an issue on this repository.&lt;/p&gt; &#xA;&lt;p&gt;You can also join our &lt;a href=&#34;https://discord.gg/TgHXuSJEk6&#34;&gt;Discord server&lt;/a&gt; and start a discussion in the &lt;code&gt;#xturing&lt;/code&gt; channel.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;üìù License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;üåé Contributing&lt;/h2&gt; &#xA;&lt;p&gt;As an open source project in a rapidly evolving field, we welcome contributions of all kinds, including new features and better documentation. Please read our &lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to learn how you can get involved.&lt;/p&gt;</summary>
  </entry>
</feed>