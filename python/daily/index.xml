<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-11T01:34:02Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>opendatalab/PDF-Extract-Kit</title>
    <updated>2024-11-11T01:34:02Z</updated>
    <id>tag:github.com,2024-11-11:/opendatalab/PDF-Extract-Kit</id>
    <link href="https://github.com/opendatalab/PDF-Extract-Kit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Comprehensive Toolkit for High-Quality PDF Content Extraction&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/opendatalab/PDF-Extract-Kit/main/assets/readme/pdf-extract-kit_logo.png&#34; width=&#34;220px&#34; style=&#34;vertical-align:middle;&#34;&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/opendatalab/PDF-Extract-Kit/main/README_zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pdf-extract-kit.readthedocs.io/en/latest/get_started/pretrained_model.html&#34;&gt;PDF-Extract-Kit-1.0 Tutorial&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/opendatalab/PDF-Extract-Kit&#34;&gt;[Models (ü§óHugging Face)]&lt;/a&gt; | &lt;a href=&#34;https://www.modelscope.cn/models/OpenDataLab/PDF-Extract-Kit&#34;&gt;[Models(&lt;img src=&#34;https://raw.githubusercontent.com/opendatalab/PDF-Extract-Kit/main/assets/readme/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;ModelScope)]&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;üî•üî•üî• &lt;a href=&#34;https://github.com/opendatalab/MinerU&#34;&gt;MinerU: Efficient Document Content Extraction Tool Based on PDF-Extract-Kit&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üëã join us on &lt;a href=&#34;https://discord.gg/Tdedn9GTXq&#34; target=&#34;_blank&#34;&gt;Discord&lt;/a&gt; and &lt;a href=&#34;https://r.vansin.top/?r=MinerU&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;PDF-Extract-Kit&lt;/code&gt; is a powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents. Here are its main features and advantages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integration of Leading Document Parsing Models&lt;/strong&gt;: Incorporates state-of-the-art models for layout detection, formula detection, formula recognition, OCR, and other core document parsing tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-Quality Parsing Across Diverse Documents&lt;/strong&gt;: Fine-tuned with diverse document annotation data to deliver high-quality results across various complex document types.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modular Design&lt;/strong&gt;: The flexible modular design allows users to easily combine and construct various applications by modifying configuration files and minimal code, making application building as straightforward as stacking blocks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive Evaluation Benchmarks&lt;/strong&gt;: Provides diverse and comprehensive PDF evaluation benchmarks, enabling users to choose the most suitable model based on evaluation results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Experience PDF-Extract-Kit now and unlock the limitless potential of PDF documents!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; PDF-Extract-Kit is designed for high-quality document processing and functions as a model toolbox.&lt;br&gt; If you are interested in extracting high-quality document content (e.g., converting PDFs to Markdown), please use &lt;a href=&#34;https://github.com/opendatalab/MinerU&#34;&gt;MinerU&lt;/a&gt;, which combines the high-quality predictions from PDF-Extract-Kit with specialized engineering optimizations for more convenient and efficient content extraction.&lt;br&gt; If you&#39;re a developer looking to create engaging applications such as document translation, document Q&amp;amp;A, or document assistants, you&#39;ll find it very convenient to build your own projects using PDF-Extract-Kit. In particular, we will periodically update the PDF-Extract-Kit/project directory with interesting applications, so stay tuned!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;We welcome researchers and engineers from the community to contribute outstanding models and innovative applications by submitting PRs to become contributors to the PDF-Extract-Kit project.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model Overview&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Task Type&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Layout Detection&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Locate different elements in a document: including images, tables, text, titles, formulas&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;DocLayout-YOLO_ft&lt;/code&gt;, &lt;code&gt;YOLO-v10_ft&lt;/code&gt;, &lt;code&gt;LayoutLMv3_ft&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Formula Detection&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Locate formulas in documents: including inline and block formulas&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;YOLOv8_ft&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Formula Recognition&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Recognize formula images into LaTeX source code&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;UniMERNet&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OCR&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Extract text content from images (including location and recognition)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PaddleOCR&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Table Recognition&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Recognize table images into corresponding source code (LaTeX/HTML/Markdown)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PaddleOCR+TableMaster&lt;/code&gt;, &lt;code&gt;StructEqTable&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Reading Order&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sort and concatenate discrete text paragraphs&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;News and Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2024.10.22&lt;/code&gt; üéâüéâüéâ We are excited to announce that table recognition model &lt;a href=&#34;https://huggingface.co/U4R/StructTable-InternVL2-1B&#34;&gt;StructTable-InternVL2-1B&lt;/a&gt;, which supports output LaTeX, HTML and MarkdDown formats has been officially integrated into &lt;code&gt;PDF-Extract-Kit 1.0&lt;/code&gt;. Please refer to the &lt;a href=&#34;https://pdf-extract-kit.readthedocs.io/en/latest/algorithm/table_recognition.html&#34;&gt;table recognition algorithm documentation&lt;/a&gt; for usage instructions!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024.10.17&lt;/code&gt; üéâüéâüéâ We are excited to announce that the more accurate and faster layout detection model, &lt;a href=&#34;https://github.com/opendatalab/DocLayout-YOLO&#34;&gt;DocLayout-YOLO&lt;/a&gt;, has been officially integrated into &lt;code&gt;PDF-Extract-Kit 1.0&lt;/code&gt;. Please refer to the &lt;a href=&#34;https://pdf-extract-kit.readthedocs.io/en/latest/algorithm/layout_detection.html&#34;&gt;layout detection algorithm documentation&lt;/a&gt; for usage instructions!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024.10.10&lt;/code&gt; üéâüéâüéâ The official release of &lt;code&gt;PDF-Extract-Kit 1.0&lt;/code&gt;, rebuilt with modularity for more convenient and flexible model usage! Please switch to the &lt;a href=&#34;https://github.com/opendatalab/PDF-Extract-Kit/tree/release/0.1.1&#34;&gt;release/0.1.1&lt;/a&gt; branch for the old version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024.08.01&lt;/code&gt; üéâüéâüéâ Added the &lt;a href=&#34;https://raw.githubusercontent.com/opendatalab/PDF-Extract-Kit/main/demo/TabRec/StructEqTable/README_TABLE.md&#34;&gt;StructEqTable&lt;/a&gt; module for table content extraction. Welcome to use it!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024.07.01&lt;/code&gt; üéâüéâüéâ We released &lt;code&gt;PDF-Extract-Kit&lt;/code&gt;, a comprehensive toolkit for high-quality PDF content extraction, including &lt;code&gt;Layout Detection&lt;/code&gt;, &lt;code&gt;Formula Detection&lt;/code&gt;, &lt;code&gt;Formula Recognition&lt;/code&gt;, and &lt;code&gt;OCR&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance Demonstration&lt;/h2&gt; &#xA;&lt;p&gt;Many current open-source SOTA models are trained and evaluated on academic datasets, achieving high-quality results only on single document types. To enable models to achieve stable and robust high-quality results on diverse documents, we constructed diverse fine-tuning datasets and fine-tuned some SOTA models to obtain practical parsing models. Below are some visual results of the models.&lt;/p&gt; &#xA;&lt;h3&gt;Layout Detection&lt;/h3&gt; &#xA;&lt;p&gt;We trained robust &lt;code&gt;Layout Detection&lt;/code&gt; models using diverse PDF document annotations. Our fine-tuned models achieve accurate extraction results on diverse PDF documents such as papers, textbooks, research reports, and financial reports, and demonstrate high robustness to challenges like blurring and watermarks. The visualization example below shows the inference results of the fine-tuned LayoutLMv3 model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/opendatalab/PDF-Extract-Kit/main/assets/readme/layout_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Formula Detection&lt;/h3&gt; &#xA;&lt;p&gt;Similarly, we collected and annotated documents containing formulas in both English and Chinese, and fine-tuned advanced formula detection models. The visualization result below shows the inference results of the fine-tuned YOLO formula detection model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/opendatalab/PDF-Extract-Kit/main/assets/readme/mfd_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Formula Recognition&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/opendatalab/UniMERNet&#34;&gt;UniMERNet&lt;/a&gt; is an algorithm designed for diverse formula recognition in real-world scenarios. By constructing large-scale training data and carefully designed results, it achieves excellent recognition performance for complex long formulas, handwritten formulas, and noisy screenshot formulas.&lt;/p&gt; &#xA;&lt;h3&gt;Table Recognition&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/UniModal4Reasoning/StructEqTable-Deploy&#34;&gt;StructEqTable&lt;/a&gt; is a high efficiency toolkit that can converts table images into LaTeX/HTML/MarkDown. The latest version, powered by the InternVL2-1B foundation model, improves Chinese recognition accuracy and expands multi-format output options.&lt;/p&gt; &#xA;&lt;h4&gt;For more visual and inference results of the models, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/opendatalab/PDF-Extract-Kit/main/xxx&#34;&gt;PDF-Extract-Kit tutorial documentation&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;h2&gt;Evaluation Metrics&lt;/h2&gt; &#xA;&lt;p&gt;Coming Soon!&lt;/p&gt; &#xA;&lt;h2&gt;Usage Guide&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n pdf-extract-kit-1.0 python=3.10&#xA;conda activate pdf-extract-kit-1.0&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If your device does not support GPU, please install the CPU version dependencies using &lt;code&gt;requirements-cpu.txt&lt;/code&gt; instead of &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NoteÔºö&lt;/strong&gt; Current Doclayout-YOLO only supports installation from pypiÔºåif error raises during DocLayout-YOLO installationÔºåplease install through &lt;code&gt;pip3 install doclayout-yolo==0.0.2 --extra-index-url=https://pypi.org/simple&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Model Download&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://pdf-extract-kit.readthedocs.io/en/latest/get_started/pretrained_model.html&#34;&gt;Model Weights Download Tutorial&lt;/a&gt; to download the required model weights. Note: You can choose to download all the weights or select specific ones. For detailed instructions, please refer to the tutorial.&lt;/p&gt; &#xA;&lt;h3&gt;Running Demos&lt;/h3&gt; &#xA;&lt;h4&gt;Layout Detection Model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/layout_detection.py --config=configs/layout_detection.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Layout detection models support &lt;strong&gt;DocLayout-YOLO&lt;/strong&gt; (default model), YOLO-v10, and LayoutLMv3. For YOLO-v10 and LayoutLMv3, please refer to &lt;a href=&#34;https://pdf-extract-kit.readthedocs.io/en/latest/algorithm/layout_detection.html&#34;&gt;Layout Detection Algorithm&lt;/a&gt;. You can view the layout detection results in the &lt;code&gt;outputs/layout_detection&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h4&gt;Formula Detection Model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/formula_detection.py --config=configs/formula_detection.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can view the formula detection results in the &lt;code&gt;outputs/formula_detection&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h4&gt;OCR Model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/ocr.py --config=configs/ocr.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can view the OCR results in the &lt;code&gt;outputs/ocr&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h4&gt;Formula Recognition Model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/formula_recognition.py --config=configs/formula_recognition.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can view the formula recognition results in the &lt;code&gt;outputs/formula_recognition&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h4&gt;Table Recognition Model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/table_parsing.py --config configs/table_parsing.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can view the table recognition results in the &lt;code&gt;outputs/table_parsing&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For more details on using the model, please refer to the&lt;a href=&#34;https://pdf-extract-kit.readthedocs.io/en/latest/get_started/pretrained_model.html&#34;&gt;PDF-Extract-Kit-1.0 Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This project focuses on using models for &lt;code&gt;high-quality&lt;/code&gt; content extraction from &lt;code&gt;diverse&lt;/code&gt; documents and does not involve reconstructing extracted content into new documents, such as PDF to Markdown. For such needs, please refer to our other GitHub project: &lt;a href=&#34;https://github.com/opendatalab/MinerU&#34;&gt;MinerU&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;To-Do List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Table Parsing&lt;/strong&gt;: Develop functionality to convert table images into corresponding LaTeX/Markdown format source code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;strong&gt;Chemical Equation Detection&lt;/strong&gt;: Implement automatic detection of chemical equations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;strong&gt;Chemical Equation/Diagram Recognition&lt;/strong&gt;: Develop models to recognize and parse chemical equations and diagrams.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;strong&gt;Reading Order Sorting Model&lt;/strong&gt;: Build a model to determine the correct reading order of text in documents.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;PDF-Extract-Kit&lt;/strong&gt; aims to provide high-quality PDF content extraction capabilities. We encourage the community to propose specific and valuable needs and welcome everyone to participate in continuously improving the PDF-Extract-Kit tool to advance research and industry development.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is open-sourced under the &lt;a href=&#34;https://raw.githubusercontent.com/opendatalab/PDF-Extract-Kit/main/LICENSE&#34;&gt;AGPL-3.0&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;p&gt;Since this project uses YOLO code and PyMuPDF for file processing, these components require compliance with the AGPL-3.0 license. Therefore, to ensure adherence to the licensing requirements of these dependencies, this repository as a whole adopts the AGPL-3.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/layoutlmv3&#34;&gt;LayoutLMv3&lt;/a&gt;: Layout detection model&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendatalab/UniMERNet&#34;&gt;UniMERNet&lt;/a&gt;: Formula recognition model&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UniModal4Reasoning/StructEqTable-Deploy&#34;&gt;StructEqTable&lt;/a&gt;: Table recognition model&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;YOLO&lt;/a&gt;: Formula detection model&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34;&gt;PaddleOCR&lt;/a&gt;: OCR model&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendatalab/DocLayout-YOLO&#34;&gt;DocLayout-YOLO&lt;/a&gt;: Layout detection model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our models / code / papers useful in your research, please consider giving ‚≠ê and citations üìù, thx :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wang2024mineru,&#xA;  title={MinerU: An Open-Source Solution for Precise Document Content Extraction},&#xA;  author={Wang, Bin and Xu, Chao and Zhao, Xiaomeng and Ouyang, Linke and Wu, Fan and Zhao, Zhiyuan and Xu, Rui and Liu, Kaiwen and Qu, Yuan and Shang, Fukai and others},&#xA;  journal={arXiv preprint arXiv:2409.18839},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{zhao2024doclayoutyoloenhancingdocumentlayout,&#xA;      title={DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception}, &#xA;      author={Zhiyuan Zhao and Hengrui Kang and Bin Wang and Conghui He},&#xA;      year={2024},&#xA;      eprint={2410.12628},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2410.12628}, &#xA;}&#xA;&#xA;@misc{wang2024unimernet,&#xA;      title={UniMERNet: A Universal Network for Real-World Mathematical Expression Recognition}, &#xA;      author={Bin Wang and Zhuangcheng Gu and Chao Xu and Bo Zhang and Botian Shi and Conghui He},&#xA;      year={2024},&#xA;      eprint={2404.15254},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@article{he2024opendatalab,&#xA;  title={Opendatalab: Empowering general artificial intelligence with open datasets},&#xA;  author={He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},&#xA;  journal={arXiv preprint arXiv:2407.13773},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;a&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=opendatalab/PDF-Extract-Kit&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=opendatalab/PDF-Extract-Kit&amp;amp;type=Date&#34;&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=opendatalab/PDF-Extract-Kit&amp;amp;type=Date&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Related Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendatalab/UniMERNet&#34;&gt;UniMERNet (Real-World Formula Recognition Algorithm)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendatalab/labelU&#34;&gt;LabelU (Lightweight Multimodal Annotation Tool)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendatalab/LabelLLM&#34;&gt;LabelLLM (Open Source LLM Dialogue Annotation Platform)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendatalab/MinerU&#34;&gt;MinerU (One-Stop High-Quality Data Extraction Tool)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>projectmesa/mesa</title>
    <updated>2024-11-11T01:34:02Z</updated>
    <id>tag:github.com,2024-11-11:/projectmesa/mesa</id>
    <link href="https://github.com/projectmesa/mesa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mesa is an open-source Python library for agent-based modeling, ideal for simulating complex systems and exploring emergent behaviors.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mesa: Agent-based modeling in Python&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CI/CD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/projectmesa/mesa/actions&#34;&gt;&lt;img src=&#34;https://github.com/projectmesa/mesa/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/projectmesa/mesa&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/projectmesa/mesa/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Coverage status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Package&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/Mesa/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mesa.svg?logo=pypi&amp;amp;label=PyPI&amp;amp;logoColor=gold&#34; alt=&#34;PyPI - Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/Mesa/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/mesa.svg?color=blue&amp;amp;label=Downloads&amp;amp;logo=pypi&amp;amp;logoColor=gold&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/Mesa/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/mesa.svg?logo=python&amp;amp;label=Python&amp;amp;logoColor=gold&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/astral-sh/ruff&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&#34; alt=&#34;linting - Ruff&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pypa/hatch&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A5%9A-Hatch-4051b5.svg?sanitize=true&#34; alt=&#34;Hatch project&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chat&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://matrix.to/#/%23project-mesa:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/project-mesa:matrix.org?label=chat&amp;amp;logo=Matrix&#34; alt=&#34;chat&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Mesa allows users to quickly create agent-based models using built-in core components (such as spatial grids and agent schedulers) or customized implementations; visualize them using a browser-based interface; and analyze their results using Python&#39;s data analysis tools. Its goal is to be the Python-based alternative to NetLogo, Repast, or MASON.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/projectmesa/mesa/main/docs/images/wolf_sheep.png&#34; alt=&#34;A screenshot of the WolfSheep Model in Mesa&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Above: A Mesa implementation of the WolfSheep model, this can be displayed in browser windows or Jupyter.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modular components&lt;/li&gt; &#xA; &lt;li&gt;Browser-based visualization&lt;/li&gt; &#xA; &lt;li&gt;Built-in tools for analysis&lt;/li&gt; &#xA; &lt;li&gt;Example model library&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using Mesa&lt;/h2&gt; &#xA;&lt;p&gt;To install our latest stable release (3.0.x), run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U mesa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install our latest pre-release, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U --pre mesa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Starting with Mesa 3.0, we don&#39;t install all our dependencies anymore by default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# You can customize the additional dependencies you need, if you want. Available are:&#xA;pip install -U --pre mesa[network,viz]&#xA;&#xA;# This is equivalent to our recommended dependencies:&#xA;pip install -U --pre mesa[rec]&#xA;&#xA;# To install all, including developer, dependencies:&#xA;pip install -U --pre mesa[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use &lt;code&gt;pip&lt;/code&gt; to install the latest GitHub version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U -e git+https://github.com/projectmesa/mesa@main#egg=mesa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or any other (development) branch on this repo or your own fork:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U -e git+https://github.com/YOUR_FORK/mesa@YOUR_BRANCH#egg=mesa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;p&gt;For resources or help on using Mesa, check out the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mesa.readthedocs.org/en/stable/tutorials/intro_tutorial.html&#34;&gt;Intro to Mesa Tutorial&lt;/a&gt; (An introductory model, the Boltzmann Wealth Model, for beginners or those new to Mesa.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mesa.readthedocs.io/stable/tutorials/visualization_tutorial.html&#34;&gt;Visualization Tutorial&lt;/a&gt; (An introduction into our Solara visualization)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.complexityexplorer.org/courses/172-agent-based-models-with-python-an-introduction-to-mesa&#34;&gt;Complexity Explorer Tutorial&lt;/a&gt; (An advanced-beginner model, SugarScape with Traders, with instructional videos)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/projectmesa/mesa-examples&#34;&gt;Mesa Examples&lt;/a&gt; (A repository of seminal ABMs using Mesa and examples of employing specific Mesa Features)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mesa.readthedocs.org/&#34;&gt;Docs&lt;/a&gt; (Mesa&#39;s documentation, API and useful snippets) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mesa.readthedocs.io/latest/&#34;&gt;Development version docs&lt;/a&gt; (the latest version docs if you&#39;re using a pre-release Mesa version)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/projectmesa/mesa/discussions&#34;&gt;Discussions&lt;/a&gt; (GitHub threaded discussions about Mesa)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matrix.to/#/%23project-mesa:matrix.org&#34;&gt;Matrix Chat&lt;/a&gt; (Chat Forum via Matrix to talk about Mesa)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running Mesa in Docker&lt;/h2&gt; &#xA;&lt;p&gt;You can run Mesa in a Docker container in a few ways.&lt;/p&gt; &#xA;&lt;p&gt;If you are a Mesa developer, first &lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;install Docker Compose&lt;/a&gt; and then, in the folder containing the Mesa Git repository, you run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker compose up&#xA;# If you want to make it run in the background, you instead run&#xA;$ docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This runs the Schelling model, as an example.&lt;/p&gt; &#xA;&lt;p&gt;With the docker-compose.yml file in this Git repository, the &lt;code&gt;docker compose up&lt;/code&gt; command does two important things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It mounts the mesa root directory (relative to the docker-compose.yml file) into /opt/mesa and runs pip install -e on that directory so your changes to mesa should be reflected in the running container.&lt;/li&gt; &#xA; &lt;li&gt;It binds the docker container&#39;s port 8765 to your host system&#39;s port 8765 so you can interact with the running model as usual by visiting localhost:8765 on your browser&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are a model developer that wants to run Mesa on a model, you need to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;make sure that your model folder is inside the folder containing the docker-compose.yml file&lt;/li&gt; &#xA; &lt;li&gt;change the &lt;code&gt;MODEL_DIR&lt;/code&gt; variable in docker-compose.yml to point to the path of your model&lt;/li&gt; &#xA; &lt;li&gt;make sure that the model folder contains an app.py file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then, you just need to run &lt;code&gt;docker compose up -d&lt;/code&gt; to have it accessible from &lt;code&gt;localhost:8765&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to Mesa&lt;/h2&gt; &#xA;&lt;p&gt;Want to join the Mesa team or just curious about what is happening with Mesa? You can...&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Join our &lt;a href=&#34;https://matrix.to/#/%23project-mesa:matrix.org&#34;&gt;Matrix chat room&lt;/a&gt; in which questions, issues, and ideas can be (informally) discussed.&lt;/li&gt; &#xA;  &lt;li&gt;Come to a monthly dev session (you can find dev session times, agendas and notes on &lt;a href=&#34;https://github.com/projectmesa/mesa/discussions&#34;&gt;Mesa discussions&lt;/a&gt;).&lt;/li&gt; &#xA;  &lt;li&gt;Just check out the code on &lt;a href=&#34;https://github.com/projectmesa/mesa/&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you run into an issue, please file a &lt;a href=&#34;https://github.com/projectmesa/mesa/issues&#34;&gt;ticket&lt;/a&gt; for us to discuss. If possible, follow up with a pull request.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to add a feature, please reach out via &lt;a href=&#34;https://github.com/projectmesa/mesa/issues&#34;&gt;ticket&lt;/a&gt; or join a dev session (see &lt;a href=&#34;https://github.com/projectmesa/mesa/discussions&#34;&gt;Mesa discussions&lt;/a&gt;). A feature is most likely to be added if you build it!&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t forget to checkout the &lt;a href=&#34;https://github.com/projectmesa/mesa/raw/main/CONTRIBUTING.md&#34;&gt;Contributors guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing Mesa&lt;/h2&gt; &#xA;&lt;p&gt;To cite Mesa in your publication, you can use the &lt;a href=&#34;https://github.com/projectmesa/mesa/raw/main/CITATION.bib&#34;&gt;CITATION.bib&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>