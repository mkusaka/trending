<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-31T01:45:27Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>clovaai/donut</title>
    <updated>2023-05-31T01:45:27Z</updated>
    <id>tag:github.com,2023-05-31:/clovaai/donut</id>
    <link href="https://github.com/clovaai/donut" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Implementation of OCR-free Document Understanding Transformer (Donut) and Synthetic Document Generator (SynthDoG), ECCV 2022&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Donut ğŸ© : Document Understanding Transformer&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.15664&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-arxiv.2111.15664-red&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/clovaai/donut/master/#how-to-cite&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ECCV-2022-blue&#34; alt=&#34;Conference&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/clovaai/donut/master/#demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-Gradio-brightgreen&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/clovaai/donut/master/#demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-Colab-orange&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/donut-python&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/donut-python?color=green&amp;amp;label=pip%20install%20donut-python&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/donut-python&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/donut-python?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=lightgreen&amp;amp;left_text=Downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Official Implementation of Donut and SynthDoG | &lt;a href=&#34;https://arxiv.org/abs/2111.15664&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://docs.google.com/presentation/d/1gv3A7t4xpwwNdpxV_yeHzEOMy-exJCAz6AlAI9O5fS8/edit?usp=sharing&#34;&gt;Slide&lt;/a&gt; | &lt;a href=&#34;https://docs.google.com/presentation/d/1m1f8BbAm5vxPcqynn_MbFfmQAlHQIR5G72-hQUFS2sk/edit?usp=sharing&#34;&gt;Poster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Donut&lt;/strong&gt; ğŸ©, &lt;strong&gt;Do&lt;/strong&gt;cume&lt;strong&gt;n&lt;/strong&gt;t &lt;strong&gt;u&lt;/strong&gt;nderstanding &lt;strong&gt;t&lt;/strong&gt;ransformer, is a new method of document understanding that utilizes an OCR-free end-to-end Transformer model. Donut does not require off-the-shelf OCR engines/APIs, yet it shows state-of-the-art performances on various visual document understanding tasks, such as visual document classification or information extraction (a.k.a. document parsing). In addition, we present &lt;strong&gt;SynthDoG&lt;/strong&gt; ğŸ¶, &lt;strong&gt;Synth&lt;/strong&gt;etic &lt;strong&gt;Do&lt;/strong&gt;cument &lt;strong&gt;G&lt;/strong&gt;enerator, that helps the model pre-training to be flexible on various languages and domains.&lt;/p&gt; &#xA;&lt;p&gt;Our academic paper, which describes our method in detail and provides full experimental results and analyses, can be found here:&lt;br&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.15664&#34;&gt;&lt;strong&gt;OCR-free Document Understanding Transformer&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt; &lt;a href=&#34;https://geewook.kim&#34;&gt;Geewook Kim&lt;/a&gt;, &lt;a href=&#34;https://dblp.org/pid/183/0952.html&#34;&gt;Teakgyu Hong&lt;/a&gt;, &lt;a href=&#34;https://github.com/moonbings&#34;&gt;Moonbin Yim&lt;/a&gt;, &lt;a href=&#34;https://github.com/long8v&#34;&gt;JeongYeon Nam&lt;/a&gt;, &lt;a href=&#34;https://github.com/jyp1111&#34;&gt;Jinyoung Park&lt;/a&gt;, &lt;a href=&#34;https://jinyeong.github.io&#34;&gt;Jinyeong Yim&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=M13_WdcAAAAJ&#34;&gt;Wonseok Hwang&lt;/a&gt;, &lt;a href=&#34;https://sangdooyun.github.io&#34;&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href=&#34;https://dongyoonhan.github.io&#34;&gt;Dongyoon Han&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=iowjmTwAAAAJ&#34;&gt;Seunghyun Park&lt;/a&gt;. In ECCV 2022.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;img width=&#34;946&#34; alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/clovaai/donut/master/misc/overview.png&#34;&gt; &#xA;&lt;h2&gt;Pre-trained Models and Web Demos&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Gradio web demos are available! &lt;a href=&#34;https://raw.githubusercontent.com/clovaai/donut/master/#demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-Gradio-brightgreen&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/clovaai/donut/master/#demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-Colab-orange&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/clovaai/donut/master/misc/screenshot_gradio_demos.png&#34; alt=&#34;image&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can run the demo with &lt;code&gt;./app.py&lt;/code&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;Sample images are available at &lt;code&gt;./misc&lt;/code&gt; and more receipt images are available at &lt;a href=&#34;https://huggingface.co/datasets/naver-clova-ix/cord-v2&#34;&gt;CORD dataset link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Web demos are available from the links in the following table.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Sec/Img&lt;/th&gt; &#xA;   &lt;th&gt;Score&lt;/th&gt; &#xA;   &lt;th&gt;Trained Model&lt;/th&gt; &#xA;   &lt;th&gt;&#xA;    &lt;div id=&#34;demo&#34;&gt;&#xA;     Demo&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/clovaai/cord&#34;&gt;CORD&lt;/a&gt; (Document Parsing)&lt;/td&gt; &#xA;   &lt;td&gt;0.7 /&lt;br&gt; 0.7 /&lt;br&gt; 1.2&lt;/td&gt; &#xA;   &lt;td&gt;91.3 /&lt;br&gt; 91.1 /&lt;br&gt; 90.9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v2/tree/official&#34;&gt;donut-base-finetuned-cord-v2&lt;/a&gt; (1280) /&lt;br&gt; &lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v1/tree/official&#34;&gt;donut-base-finetuned-cord-v1&lt;/a&gt; (1280) /&lt;br&gt; &lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v1-2560/tree/official&#34;&gt;donut-base-finetuned-cord-v1-2560&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/naver-clova-ix/donut-base-finetuned-cord-v2&#34;&gt;gradio space web demo&lt;/a&gt;,&lt;br&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1o07hty-3OQTvGnc_7lgQFLvvKQuLjqiw?usp=sharing&#34;&gt;google colab demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/beacandler/EATEN&#34;&gt;Train Ticket&lt;/a&gt; (Document Parsing)&lt;/td&gt; &#xA;   &lt;td&gt;0.6&lt;/td&gt; &#xA;   &lt;td&gt;98.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-base-finetuned-zhtrainticket/tree/official&#34;&gt;donut-base-finetuned-zhtrainticket&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/16O-hMvGiXrYZnlXA_tfJ9_q760YcLoOj?usp=sharing&#34;&gt;google colab demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~aharley/rvl-cdip&#34;&gt;RVL-CDIP&lt;/a&gt; (Document Classification)&lt;/td&gt; &#xA;   &lt;td&gt;0.75&lt;/td&gt; &#xA;   &lt;td&gt;95.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-base-finetuned-rvlcdip/tree/official&#34;&gt;donut-base-finetuned-rvlcdip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/nielsr/donut-rvlcdip&#34;&gt;gradio space web demo&lt;/a&gt;,&lt;br&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1xUDmLqlthx8A8rWKLMSLThZ7oeRJkDuU?usp=sharing&#34;&gt;google colab demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rrc.cvc.uab.es/?ch=17&#34;&gt;DocVQA Task1&lt;/a&gt; (Document VQA)&lt;/td&gt; &#xA;   &lt;td&gt;0.78&lt;/td&gt; &#xA;   &lt;td&gt;67.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-base-finetuned-docvqa/tree/official&#34;&gt;donut-base-finetuned-docvqa&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/nielsr/donut-docvqa&#34;&gt;gradio space web demo&lt;/a&gt;,&lt;br&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Z4WG8Wunj3HE0CERjt608ALSgSzRC9ig?usp=sharing&#34;&gt;google colab demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The links to the pre-trained backbones are here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-base/tree/official&#34;&gt;&lt;code&gt;donut-base&lt;/code&gt;&lt;/a&gt;: trained with 64 A100 GPUs (~2.5 days), number of layers (encoder: {2,2,14,2}, decoder: 4), input size 2560x1920, swin window size 10, IIT-CDIP (11M) and SynthDoG (English, Chinese, Japanese, Korean, 0.5M x 4).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-proto/tree/official&#34;&gt;&lt;code&gt;donut-proto&lt;/code&gt;&lt;/a&gt;: (preliminary model) trained with 8 V100 GPUs (~5 days), number of layers (encoder: {2,2,18,2}, decoder: 4), input size 2048x1536, swin window size 8, and SynthDoG (English, Japanese, Korean, 0.4M x 3).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/clovaai/donut/master/#how-to-cite&#34;&gt;our paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;SynthDoG datasets&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/clovaai/donut/master/misc/sample_synthdog.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The links to the SynthDoG-generated datasets are here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/naver-clova-ix/synthdog-en&#34;&gt;&lt;code&gt;synthdog-en&lt;/code&gt;&lt;/a&gt;: English, 0.5M.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/naver-clova-ix/synthdog-zh&#34;&gt;&lt;code&gt;synthdog-zh&lt;/code&gt;&lt;/a&gt;: Chinese, 0.5M.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/naver-clova-ix/synthdog-ja&#34;&gt;&lt;code&gt;synthdog-ja&lt;/code&gt;&lt;/a&gt;: Japanese, 0.5M.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/naver-clova-ix/synthdog-ko&#34;&gt;&lt;code&gt;synthdog-ko&lt;/code&gt;&lt;/a&gt;: Korean, 0.5M.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To generate synthetic datasets with our SynthDoG, please see &lt;code&gt;./synthdog/README.md&lt;/code&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/clovaai/donut/master/#how-to-cite&#34;&gt;our paper&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;2022-11-14&lt;/em&gt;&lt;/strong&gt; New version 1.0.9 is released (&lt;code&gt;pip install donut-python --upgrade&lt;/code&gt;). See &lt;a href=&#34;https://github.com/clovaai/donut/releases/tag/1.0.9&#34;&gt;1.0.9 Release Notes&lt;/a&gt;.&lt;br&gt; &lt;strong&gt;&lt;em&gt;2022-08-12&lt;/em&gt;&lt;/strong&gt; Donut ğŸ© is also available at &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/model_doc/donut&#34;&gt;huggingface/transformers ğŸ¤—&lt;/a&gt; (contributed by &lt;a href=&#34;https://github.com/NielsRogge&#34;&gt;@NielsRogge&lt;/a&gt;). &lt;code&gt;donut-python&lt;/code&gt; loads the pre-trained weights from the &lt;code&gt;official&lt;/code&gt; branch of the model repositories. See &lt;a href=&#34;https://github.com/clovaai/donut/releases/tag/1.0.5&#34;&gt;1.0.5 Release Notes&lt;/a&gt;.&lt;br&gt; &lt;strong&gt;&lt;em&gt;2022-08-05&lt;/em&gt;&lt;/strong&gt; A well-executed hands-on tutorial on donut ğŸ© is published at &lt;a href=&#34;https://towardsdatascience.com/ocr-free-document-understanding-with-donut-1acfbdf099be&#34;&gt;Towards Data Science&lt;/a&gt; (written by &lt;a href=&#34;https://github.com/estaudere&#34;&gt;@estaudere&lt;/a&gt;).&lt;br&gt; &lt;strong&gt;&lt;em&gt;2022-07-20&lt;/em&gt;&lt;/strong&gt; First Commit, We release our code, model weights, synthetic data and generator.&lt;/p&gt; &#xA;&lt;h2&gt;Software installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/donut-python&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/donut-python?color=green&amp;amp;label=pip%20install%20donut-python&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/donut-python&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/donut-python?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=lightgreen&amp;amp;left_text=Downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install donut-python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or clone this repository and install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/clovaai/donut.git&#xA;cd donut/&#xA;conda create -n donut_official python=3.7&#xA;conda activate donut_official&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We tested &lt;a href=&#34;https://github.com/clovaai/donut&#34;&gt;donut&lt;/a&gt; with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;torch&lt;/a&gt; == 1.11.0+cu113&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/vision&#34;&gt;torchvision&lt;/a&gt; == 0.12.0+cu113&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lightning-AI/lightning&#34;&gt;pytorch-lightning&lt;/a&gt; == 1.6.4&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; == 4.11.3&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;timm&lt;/a&gt; == 0.5.4&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;This repository assumes the following structure of dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; tree dataset_name&#xA;dataset_name&#xA;â”œâ”€â”€ test&#xA;â”‚   â”œâ”€â”€ metadata.jsonl&#xA;â”‚   â”œâ”€â”€ {image_path0}&#xA;â”‚   â”œâ”€â”€ {image_path1}&#xA;â”‚             .&#xA;â”‚             .&#xA;â”œâ”€â”€ train&#xA;â”‚   â”œâ”€â”€ metadata.jsonl&#xA;â”‚   â”œâ”€â”€ {image_path0}&#xA;â”‚   â”œâ”€â”€ {image_path1}&#xA;â”‚             .&#xA;â”‚             .&#xA;â””â”€â”€ validation&#xA;    â”œâ”€â”€ metadata.jsonl&#xA;    â”œâ”€â”€ {image_path0}&#xA;    â”œâ”€â”€ {image_path1}&#xA;              .&#xA;              .&#xA;&#xA;&amp;gt; cat dataset_name/test/metadata.jsonl&#xA;{&#34;file_name&#34;: {image_path0}, &#34;ground_truth&#34;: &#34;{\&#34;gt_parse\&#34;: {ground_truth_parse}, ... {other_metadata_not_used} ... }&#34;}&#xA;{&#34;file_name&#34;: {image_path1}, &#34;ground_truth&#34;: &#34;{\&#34;gt_parse\&#34;: {ground_truth_parse}, ... {other_metadata_not_used} ... }&#34;}&#xA;     .&#xA;     .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The structure of &lt;code&gt;metadata.jsonl&lt;/code&gt; file is in &lt;a href=&#34;https://jsonlines.org&#34;&gt;JSON Lines text format&lt;/a&gt;, i.e., &lt;code&gt;.jsonl&lt;/code&gt;. Each line consists of &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;file_name&lt;/code&gt; : relative path to the image file.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ground_truth&lt;/code&gt; : string format (json dumped), the dictionary contains either &lt;code&gt;gt_parse&lt;/code&gt; or &lt;code&gt;gt_parses&lt;/code&gt;. Other fields (metadata) can be added to the dictionary but will not be used.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;donut&lt;/code&gt; interprets all tasks as a JSON prediction problem. As a result, all &lt;code&gt;donut&lt;/code&gt; model training share a same pipeline. For training and inference, the only thing to do is preparing &lt;code&gt;gt_parse&lt;/code&gt; or &lt;code&gt;gt_parses&lt;/code&gt; for the task in format described below.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;For Document Classification&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;gt_parse&lt;/code&gt; follows the format of &lt;code&gt;{&#34;class&#34; : {class_name}}&lt;/code&gt;, for example, &lt;code&gt;{&#34;class&#34; : &#34;scientific_report&#34;}&lt;/code&gt; or &lt;code&gt;{&#34;class&#34; : &#34;presentation&#34;}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Google colab demo is available &lt;a href=&#34;https://colab.research.google.com/drive/1xUDmLqlthx8A8rWKLMSLThZ7oeRJkDuU?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Gradio web demo is available &lt;a href=&#34;https://huggingface.co/spaces/nielsr/donut-rvlcdip&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;For Document Information Extraction&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;gt_parse&lt;/code&gt; is a JSON object that contains full information of the document image, for example, the JSON object for a receipt may look like &lt;code&gt;{&#34;menu&#34; : [{&#34;nm&#34;: &#34;ICE BLACKCOFFEE&#34;, &#34;cnt&#34;: &#34;2&#34;, ...}, ...], ...}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More examples are available at &lt;a href=&#34;https://huggingface.co/datasets/naver-clova-ix/cord-v2&#34;&gt;CORD dataset&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Google colab demo is available &lt;a href=&#34;https://colab.research.google.com/drive/1o07hty-3OQTvGnc_7lgQFLvvKQuLjqiw?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Gradio web demo is available &lt;a href=&#34;https://huggingface.co/spaces/naver-clova-ix/donut-base-finetuned-cord-v2&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;For Document Visual Question Answering&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;gt_parses&lt;/code&gt; follows the format of &lt;code&gt;[{&#34;question&#34; : {question_sentence}, &#34;answer&#34; : {answer_candidate_1}}, {&#34;question&#34; : {question_sentence}, &#34;answer&#34; : {answer_candidate_2}}, ...]&lt;/code&gt;, for example, &lt;code&gt;[{&#34;question&#34; : &#34;what is the model name?&#34;, &#34;answer&#34; : &#34;donut&#34;}, {&#34;question&#34; : &#34;what is the model name?&#34;, &#34;answer&#34; : &#34;document understanding transformer&#34;}]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DocVQA Task1 has multiple answers, hence &lt;code&gt;gt_parses&lt;/code&gt; should be a list of dictionary that contains a pair of question and answer.&lt;/li&gt; &#xA; &lt;li&gt;Google colab demo is available &lt;a href=&#34;https://colab.research.google.com/drive/1Z4WG8Wunj3HE0CERjt608ALSgSzRC9ig?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Gradio web demo is available &lt;a href=&#34;https://huggingface.co/spaces/nielsr/donut-docvqa&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;For (Pseudo) Text Reading Task&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;gt_parse&lt;/code&gt; looks like &lt;code&gt;{&#34;text_sequence&#34; : &#34;word1 word2 word3 ... &#34;}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This task is also a pre-training task of Donut model.&lt;/li&gt; &#xA; &lt;li&gt;You can use our &lt;strong&gt;SynthDoG&lt;/strong&gt; ğŸ¶ to generate synthetic images for the text reading task with proper &lt;code&gt;gt_parse&lt;/code&gt;. See &lt;code&gt;./synthdog/README.md&lt;/code&gt; for details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;This is the configuration of Donut model training on &lt;a href=&#34;https://github.com/clovaai/cord&#34;&gt;CORD&lt;/a&gt; dataset used in our experiment. We ran this with a single NVIDIA A100 GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py --config config/train_cord.yaml \&#xA;                --pretrained_model_name_or_path &#34;naver-clova-ix/donut-base&#34; \&#xA;                --dataset_name_or_paths &#39;[&#34;naver-clova-ix/cord-v2&#34;]&#39; \&#xA;                --exp_version &#34;test_experiment&#34;    &#xA;  .&#xA;  .                                                                                                                                                                                                                                         &#xA;Prediction: &amp;lt;s_menu&amp;gt;&amp;lt;s_nm&amp;gt;Lemon Tea (L)&amp;lt;/s_nm&amp;gt;&amp;lt;s_cnt&amp;gt;1&amp;lt;/s_cnt&amp;gt;&amp;lt;s_price&amp;gt;25.000&amp;lt;/s_price&amp;gt;&amp;lt;/s_menu&amp;gt;&amp;lt;s_total&amp;gt;&amp;lt;s_total_price&amp;gt;25.000&amp;lt;/s_total_price&amp;gt;&amp;lt;s_cashprice&amp;gt;30.000&amp;lt;/s_cashprice&amp;gt;&amp;lt;s_changeprice&amp;gt;5.000&amp;lt;/s_changeprice&amp;gt;&amp;lt;/s_total&amp;gt;&#xA;Answer: &amp;lt;s_menu&amp;gt;&amp;lt;s_nm&amp;gt;Lemon Tea (L)&amp;lt;/s_nm&amp;gt;&amp;lt;s_cnt&amp;gt;1&amp;lt;/s_cnt&amp;gt;&amp;lt;s_price&amp;gt;25.000&amp;lt;/s_price&amp;gt;&amp;lt;/s_menu&amp;gt;&amp;lt;s_total&amp;gt;&amp;lt;s_total_price&amp;gt;25.000&amp;lt;/s_total_price&amp;gt;&amp;lt;s_cashprice&amp;gt;30.000&amp;lt;/s_cashprice&amp;gt;&amp;lt;s_changeprice&amp;gt;5.000&amp;lt;/s_changeprice&amp;gt;&amp;lt;/s_total&amp;gt;&#xA;Normed ED: 0.0&#xA;Prediction: &amp;lt;s_menu&amp;gt;&amp;lt;s_nm&amp;gt;Hulk Topper Package&amp;lt;/s_nm&amp;gt;&amp;lt;s_cnt&amp;gt;1&amp;lt;/s_cnt&amp;gt;&amp;lt;s_price&amp;gt;100.000&amp;lt;/s_price&amp;gt;&amp;lt;/s_menu&amp;gt;&amp;lt;s_total&amp;gt;&amp;lt;s_total_price&amp;gt;100.000&amp;lt;/s_total_price&amp;gt;&amp;lt;s_cashprice&amp;gt;100.000&amp;lt;/s_cashprice&amp;gt;&amp;lt;s_changeprice&amp;gt;0&amp;lt;/s_changeprice&amp;gt;&amp;lt;/s_total&amp;gt;&#xA;Answer: &amp;lt;s_menu&amp;gt;&amp;lt;s_nm&amp;gt;Hulk Topper Package&amp;lt;/s_nm&amp;gt;&amp;lt;s_cnt&amp;gt;1&amp;lt;/s_cnt&amp;gt;&amp;lt;s_price&amp;gt;100.000&amp;lt;/s_price&amp;gt;&amp;lt;/s_menu&amp;gt;&amp;lt;s_total&amp;gt;&amp;lt;s_total_price&amp;gt;100.000&amp;lt;/s_total_price&amp;gt;&amp;lt;s_cashprice&amp;gt;100.000&amp;lt;/s_cashprice&amp;gt;&amp;lt;s_changeprice&amp;gt;0&amp;lt;/s_changeprice&amp;gt;&amp;lt;/s_total&amp;gt;&#xA;Normed ED: 0.0&#xA;Prediction: &amp;lt;s_menu&amp;gt;&amp;lt;s_nm&amp;gt;Giant Squid&amp;lt;/s_nm&amp;gt;&amp;lt;s_cnt&amp;gt;x 1&amp;lt;/s_cnt&amp;gt;&amp;lt;s_price&amp;gt;Rp. 39.000&amp;lt;/s_price&amp;gt;&amp;lt;s_sub&amp;gt;&amp;lt;s_nm&amp;gt;C.Finishing - Cut&amp;lt;/s_nm&amp;gt;&amp;lt;s_price&amp;gt;Rp. 0&amp;lt;/s_price&amp;gt;&amp;lt;sep/&amp;gt;&amp;lt;s_nm&amp;gt;B.Spicy Level - Extreme Hot Rp. 0&amp;lt;/s_price&amp;gt;&amp;lt;/s_sub&amp;gt;&amp;lt;sep/&amp;gt;&amp;lt;s_nm&amp;gt;A.Flavour - Salt &amp;amp; Pepper&amp;lt;/s_nm&amp;gt;&amp;lt;s_price&amp;gt;Rp. 0&amp;lt;/s_price&amp;gt;&amp;lt;/s_sub&amp;gt;&amp;lt;/s_menu&amp;gt;&amp;lt;s_sub_total&amp;gt;&amp;lt;s_subtotal_price&amp;gt;Rp. 39.000&amp;lt;/s_subtotal_price&amp;gt;&amp;lt;/s_sub_total&amp;gt;&amp;lt;s_total&amp;gt;&amp;lt;s_total_price&amp;gt;Rp. 39.000&amp;lt;/s_total_price&amp;gt;&amp;lt;s_cashprice&amp;gt;Rp. 50.000&amp;lt;/s_cashprice&amp;gt;&amp;lt;s_changeprice&amp;gt;Rp. 11.000&amp;lt;/s_changeprice&amp;gt;&amp;lt;/s_total&amp;gt;&#xA;Answer: &amp;lt;s_menu&amp;gt;&amp;lt;s_nm&amp;gt;Giant Squid&amp;lt;/s_nm&amp;gt;&amp;lt;s_cnt&amp;gt;x1&amp;lt;/s_cnt&amp;gt;&amp;lt;s_price&amp;gt;Rp. 39.000&amp;lt;/s_price&amp;gt;&amp;lt;s_sub&amp;gt;&amp;lt;s_nm&amp;gt;C.Finishing - Cut&amp;lt;/s_nm&amp;gt;&amp;lt;s_price&amp;gt;Rp. 0&amp;lt;/s_price&amp;gt;&amp;lt;sep/&amp;gt;&amp;lt;s_nm&amp;gt;B.Spicy Level - Extreme Hot&amp;lt;/s_nm&amp;gt;&amp;lt;s_price&amp;gt;Rp. 0&amp;lt;/s_price&amp;gt;&amp;lt;sep/&amp;gt;&amp;lt;s_nm&amp;gt;A.Flavour- Salt &amp;amp; Pepper&amp;lt;/s_nm&amp;gt;&amp;lt;s_price&amp;gt;Rp. 0&amp;lt;/s_price&amp;gt;&amp;lt;/s_sub&amp;gt;&amp;lt;/s_menu&amp;gt;&amp;lt;s_sub_total&amp;gt;&amp;lt;s_subtotal_price&amp;gt;Rp. 39.000&amp;lt;/s_subtotal_price&amp;gt;&amp;lt;/s_sub_total&amp;gt;&amp;lt;s_total&amp;gt;&amp;lt;s_total_price&amp;gt;Rp. 39.000&amp;lt;/s_total_price&amp;gt;&amp;lt;s_cashprice&amp;gt;Rp. 50.000&amp;lt;/s_cashprice&amp;gt;&amp;lt;s_changeprice&amp;gt;Rp. 11.000&amp;lt;/s_changeprice&amp;gt;&amp;lt;/s_total&amp;gt;&#xA;Normed ED: 0.039603960396039604                                                                                                                                  &#xA;Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:49&amp;lt;00:00,  1.82it/s, loss=0.00327, exp_name=train_cord, exp_version=test_experiment]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some important arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--config&lt;/code&gt; : config file path for model training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--pretrained_model_name_or_path&lt;/code&gt; : string format, model name in Hugging Face modelhub or local path.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--dataset_name_or_paths&lt;/code&gt; : string format (json dumped), list of dataset names in Hugging Face datasets or local paths.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--result_path&lt;/code&gt; : file path to save model outputs/artifacts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--exp_version&lt;/code&gt; : used for experiment versioning. The output files are saved at &lt;code&gt;{result_path}/{exp_version}/*&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Test&lt;/h3&gt; &#xA;&lt;p&gt;With the trained model, test images and ground truth parses, you can get inference results and accuracy scores.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python test.py --dataset_name_or_path naver-clova-ix/cord-v2 --pretrained_model_name_or_path ./result/train_cord/test_experiment --save_path ./result/output.json&#xA;100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:35&amp;lt;00:00,  2.80it/s]&#xA;Total number of samples: 100, Tree Edit Distance (TED) based accuracy score: 0.9129639764131697, F1 accuracy score: 0.8406020841373987&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some important arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--dataset_name_or_path&lt;/code&gt; : string format, the target dataset name in Hugging Face datasets or local path.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--pretrained_model_name_or_path&lt;/code&gt; : string format, the model name in Hugging Face modelhub or local path.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--save_path&lt;/code&gt;: file path to save predictions and scores.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Cite&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful to you, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{kim2022donut,&#xA;  title     = {OCR-Free Document Understanding Transformer},&#xA;  author    = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},&#xA;  booktitle = {European Conference on Computer Vision (ECCV)},&#xA;  year      = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;MIT license&#xA;&#xA;Copyright (c) 2022-present NAVER Corp.&#xA;&#xA;Permission is hereby granted, free of charge, to any person obtaining a copy&#xA;of this software and associated documentation files (the &#34;Software&#34;), to deal&#xA;in the Software without restriction, including without limitation the rights&#xA;to use, copy, modify, merge, publish, distribute, sublicense, and/or sell&#xA;copies of the Software, and to permit persons to whom the Software is&#xA;furnished to do so, subject to the following conditions:&#xA;&#xA;The above copyright notice and this permission notice shall be included in&#xA;all copies or substantial portions of the Software.&#xA;&#xA;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR&#xA;IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,&#xA;FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE&#xA;AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER&#xA;LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,&#xA;OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN&#xA;THE SOFTWARE.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>WankkoRee/eaio</title>
    <updated>2023-05-31T01:45:27Z</updated>
    <id>tag:github.com,2023-05-31:/WankkoRee/eaio</id>
    <link href="https://github.com/WankkoRee/eaio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ä¸€ä¸ªé€šè¿‡å°†ç£ç›˜ä¸Šæ‰€æœ‰ Electron åº”ç”¨ä¸­ç›¸åŒæ–‡ä»¶ç¡¬é“¾æ¥åˆ°ç»Ÿä¸€ä½ç½®æ¥å‡å°‘ç£ç›˜å ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå°±åƒ pnpm ä¸€æ ·ã€‚&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;eaio (Electron All in One)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pdm.fming.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pdm-managed-blueviolet&#34; alt=&#34;pdm-managed&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä¸€ä¸ªé€šè¿‡å°†ç£ç›˜ä¸Šæ‰€æœ‰&lt;a href=&#34;https://github.com/electron/electron&#34;&gt;Electron&lt;/a&gt;åº”ç”¨ä¸­ç›¸åŒæ–‡ä»¶ç¡¬é“¾æ¥åˆ°ç»Ÿä¸€ä½ç½®æ¥å‡å°‘ç£ç›˜å ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå°±åƒ&lt;code&gt;pnpm&lt;/code&gt;ä¸€æ ·ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/WankkoRee/eaio/master/docs/gui-app.png&#34; alt=&#34;GUI åº”ç”¨ç®¡ç†&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/WankkoRee/eaio/master/docs/gui-repo.png&#34; alt=&#34;GUI ä»“åº“ç®¡ç†&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ä½¿ç”¨ä»‹ç»&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WankkoRee/eaio/master/docs/guide/zh-Hans.md&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;åŸç†&lt;/h2&gt; &#xA;&lt;p&gt;ç¡¬é“¾æ¥ä¼šå°†å¤šä¸ªæ–‡ä»¶æŒ‡å‘åŒä¸€ç£ç›˜ä½ç½®ï¼Œä½¿å¾—å¤šä¸ªç›¸åŒçš„æ–‡ä»¶åªå ç”¨ä¸€ä»½ç©ºé—´ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Q: ä¸ºä»€ä¹ˆä¸ç”¨æ›´ä¼˜é›…çš„è½¯é“¾æ¥ï¼Ÿ&lt;/p&gt; &lt;p&gt;A: è½¯é“¾æ¥çŠ¶æ€ä¸‹çš„&lt;code&gt;electron.exe&lt;/code&gt;æ— æ³•æ­£ç¡®åˆ¤æ–­è¿è¡Œç›®å½•(å¦‚æœ‰è§£å†³æ–¹æ³•æ¬¢è¿è®¨è®º)ï¼Œä¸”å¯èƒ½å› ä¸ºä¸€äº›åŸå› é€ æˆè¯¯åˆ ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Q: ä¸ºä»€ä¹ˆä¸ç”¨&lt;code&gt;electron&lt;/code&gt;å‘½ä»¤è¡ŒæŒ‡å®š&lt;code&gt;resources&lt;/code&gt;è·¯å¾„ï¼Ÿ&lt;/p&gt; &lt;p&gt;A: ä¸€äº›åº”ç”¨ä¼šåœ¨è¿è¡Œç›®å½•ä¸‹æ”¾ç½®é¢å¤–çš„&lt;code&gt;.exe&lt;/code&gt;æˆ–&lt;code&gt;.dll&lt;/code&gt;æ–‡ä»¶ï¼ŒæŒ‡å®šåº”ç”¨è·¯å¾„å¯èƒ½ä¼šé€ æˆåº”ç”¨æ— æ³•æ‰¾åˆ°è¿™äº›æ–‡ä»¶ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Q: åªæ”¯æŒ Windows å—ï¼Ÿ&lt;/p&gt; &lt;p&gt;A: å…¶ä»–ç³»ç»Ÿæš‚æœªæµ‹è¯•æœ‰æ•ˆæ€§ï¼Œå¦‚æœ¬æ–¹æ¡ˆå¯ç”¨äºå…¶å®ƒç³»ç»Ÿï¼Œåç»­ä¼šæ”¯æŒã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;æ³¨æ„äº‹é¡¹&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æœ¬å·¥å…·ä¼šåœ¨æ‰§è¡Œ&lt;code&gt;link&lt;/code&gt;æˆ–&lt;code&gt;check&lt;/code&gt;æ“ä½œæ—¶ï¼Œåœ¨ç›®æ ‡åº”ç”¨æ‰€åœ¨çš„ç£ç›˜åˆ†åŒºä¸‹åˆ›å»º&lt;code&gt;.eaio&lt;/code&gt;ä»“åº“ï¼Œç”¨äºå­˜å‚¨ç¡¬é“¾æ¥çš„æºæ–‡ä»¶ï¼Œå¦‚æ— ç‰¹æ®Šæƒ…å†µè¯·&lt;strong&gt;ä¸è¦åˆ ã€æ”¹&lt;/strong&gt;ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æœ¬å·¥å…·çš„&lt;code&gt;status&lt;/code&gt;æ“ä½œå¯ä»¥æ£€æŸ¥æ‰€æœ‰ç£ç›˜åˆ†åŒºä¸‹&lt;code&gt;.eaio&lt;/code&gt;ä»“åº“ä¸­æ‰€æœ‰ç‰ˆæœ¬çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå¯ç”¨äºæ£€æŸ¥&lt;strong&gt;ä¸‹è½½å®Œæˆ&lt;/strong&gt;æƒ…å†µã€æºæ–‡ä»¶&lt;strong&gt;å­˜åœ¨&lt;/strong&gt;æƒ…å†µã€æºæ–‡ä»¶&lt;strong&gt;æ”¹åŠ¨&lt;/strong&gt;æƒ…å†µã€‚&lt;/li&gt; &#xA; &lt;li&gt;æœ¬å·¥å…·çš„&lt;code&gt;download&lt;/code&gt;æ“ä½œå¯ä»¥ä¸‹è½½ç›®æ ‡ç‰ˆæœ¬å’Œæ¶æ„çš„&lt;code&gt;Electron&lt;/code&gt;é¢„ç¼–è¯‘ç¨‹åºåˆ°æŒ‡å®šç£ç›˜åˆ†åŒºçš„&lt;code&gt;.eaio&lt;/code&gt;ä»“åº“ä¸­ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ä¼šè¿›è¡Œ&lt;strong&gt;è¦†ç›–&lt;/strong&gt;ï¼Œæ‰€ä»¥ä¹Ÿå¯ç”¨äºå¯¹æºæ–‡ä»¶çš„&lt;strong&gt;æ¢å¤/ä¿®å¤&lt;/strong&gt;ã€‚&lt;/li&gt; &#xA; &lt;li&gt;è¯·ä¸è¦å¯¹å·²é“¾æ¥çš„&lt;code&gt;Electron&lt;/code&gt;åº”ç”¨è¿›è¡Œ&lt;strong&gt;æ–‡ä»¶ç²‰ç¢&lt;/strong&gt;æ“ä½œï¼Œå¯èƒ½ä¼šå¯¼è‡´æºæ–‡ä»¶æ”¹åŠ¨ã€‚&lt;/li&gt; &#xA; &lt;li&gt;è¯·ä¸è¦å¯¹&lt;code&gt;.eaio&lt;/code&gt;ä»“åº“è¿›è¡Œ&lt;strong&gt;æ–‡ä»¶ç²‰ç¢&lt;/strong&gt;æ“ä½œï¼Œå¯èƒ½ä¼šå¯¼è‡´å·²é“¾æ¥çš„&lt;code&gt;Electron&lt;/code&gt;åº”ç”¨æ–‡ä»¶æ”¹åŠ¨ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;å‚ä¸å¼€å‘&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æ‹‰å–æœ¬ä»“åº“çš„ä»£ç &lt;/li&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®ä½¿ç”¨&lt;a href=&#34;https://github.com/pdm-project/pdm&#34;&gt;pdm&lt;/a&gt;ä½œä¸ºåŒ…ç®¡ç†å·¥å…·ï¼Œç¡®ä¿ä½ å·²å®‰è£…&lt;code&gt;pdm&lt;/code&gt;å¹¶é…ç½®å¥½å…¶ç¯å¢ƒ&lt;/li&gt; &#xA; &lt;li&gt;æ‰§è¡Œ&lt;code&gt;pdm sync&lt;/code&gt;ä»¥å®‰è£…ä¾èµ–åˆ°è™šæ‹Ÿç¯å¢ƒ&lt;/li&gt; &#xA; &lt;li&gt;ä¿®æ”¹ä»£ç &lt;/li&gt; &#xA; &lt;li&gt;æ¨é€ä¿®æ”¹å¹¶æäº¤pr&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å…¨ç›˜æ‰«æ Electron åº”ç”¨&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; éªŒè¯å…¶ä»–ç³»ç»Ÿæœ‰æ•ˆæ€§&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;è¶‹åŠ¿&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://seladb.github.io/StarTrack-js/#/preload?r=WankkoRee,eaio&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=WankkoRee/eaio&amp;amp;type=Timeline&#34; alt=&#34;Star è¶‹åŠ¿&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>threestudio-project/threestudio</title>
    <updated>2023-05-31T01:45:27Z</updated>
    <id>tag:github.com,2023-05-31:/threestudio-project/threestudio</id>
    <link href="https://github.com/threestudio-project/threestudio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A unified framework for 3D content generation.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;threestudio&#34; src=&#34;https://user-images.githubusercontent.com/19284678/236847132-219999d0-4ffa-4240-a262-c2c025d15d9e.png&#34; width=&#34;50%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt; threestudio is a unified framework for 3D content creation from text prompts, single images, and few-shot images, by lifting 2D text-to-image generation models. &lt;/b&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;threestudio&#34; src=&#34;https://user-images.githubusercontent.com/3117031/236739017-365626d9-bb35-4c47-b71d-b9de767b0644.gif&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt; ğŸ‘† Results obtained from methods implemented by threestudio ğŸ‘† &lt;br&gt; | &lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;DreamFusion&lt;/a&gt; | &lt;a href=&#34;https://research.nvidia.com/labs/dir/magic3d/&#34;&gt;Magic3D&lt;/a&gt; | &lt;a href=&#34;https://pals.ttic.edu/p/score-jacobian-chaining&#34;&gt;SJC&lt;/a&gt; | &lt;a href=&#34;https://github.com/eladrich/latent-nerf&#34;&gt;Latent-NeRF&lt;/a&gt; | &lt;a href=&#34;https://fantasia3d.github.io/&#34;&gt;Fantasia3D&lt;/a&gt; | &lt;/b&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/threestudio-project/threestudio/blob/main/threestudio.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Did not find what you want? Submit a feature request or upvote others&#39; requests &lt;a href=&#34;https://github.com/threestudio-project/threestudio/discussions/46&#34;&gt;here&lt;/a&gt;! &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;05/26/2023: An experimental implementation of ProlificDreamer! Following the instruction &lt;a href=&#34;https://github.com/threestudio-project/threestudio#prolificdreamer-&#34;&gt;here&lt;/a&gt; to have a try.&lt;/li&gt; &#xA; &lt;li&gt;05/14/2023: You can experiment with the SDS loss on 2D images using our &lt;a href=&#34;https://raw.githubusercontent.com/threestudio-project/threestudio/main/2dplayground.ipynb&#34;&gt;2dplayground&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;05/13/2023: You can now try threestudio on &lt;a href=&#34;https://colab.research.google.com/github/threestudio-project/threestudio/blob/main/threestudio.ipynb&#34;&gt;Google Colab&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;05/11/2023: We now support exporting textured meshes! See &lt;a href=&#34;https://github.com/threestudio-project/threestudio#export-meshes&#34;&gt;here&lt;/a&gt; for instructions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/threestudio-project/threestudio/assets/19284678/ccae2820-e702-484c-a43f-81678a365427&#34; alt=&#34;export-blender&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The following steps have been tested on Ubuntu20.04.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You must have a NVIDIA graphics card with at least 6GB VRAM and have &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA&lt;/a&gt; installed.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(Optional, Recommended) Create a virtual environment:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 -m virtualenv venv&#xA;. venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;code&gt;PyTorch &amp;gt;= 1.12&lt;/code&gt;. We have tested on &lt;code&gt;torch1.12.1+cu113&lt;/code&gt; and &lt;code&gt;torch2.0.0+cu118&lt;/code&gt;, but other versions should also work fine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# torch1.12.1+cu113&#xA;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;# or torch2.0.0+cu118&#xA;pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(Optional, Recommended) Install ninja to speed up the compilation of CUDA extensions:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install ninja&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional, Recommended) The best-performing models in threestudio uses the newly-released T2I model &lt;a href=&#34;https://github.com/deep-floyd/IF&#34;&gt;DeepFloyd IF&lt;/a&gt; which currently requires signing a license agreement. If you would like use these models, you need to &lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-I-XL-v1.0&#34;&gt;accept the license on the model card of DeepFloyd IF&lt;/a&gt;, and login in the Hugging Face hub in terminal by &lt;code&gt;huggingface-cli login&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For contributors, see &lt;a href=&#34;https://github.com/threestudio-project/threestudio#contributing-to-threestudio&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Here we show some basic usage of threestudio. First let&#39;s train a DreamFusion model to create a classic pancake bunny.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you are experiencing unstable connections with Hugging Face, we suggest you either (1) setting environment variable &lt;code&gt;TRANSFORMERS_OFFLINE=1 DIFFUSERS_OFFLINE=1&lt;/code&gt; before your running command after all needed files have been fetched on the first run, to prevent from connecting to Hugging Face each time you run, or (2) downloading the guidance model you used to a local folder following &lt;a href=&#34;https://huggingface.co/docs/huggingface_hub/v0.14.1/guides/download#download-an-entire-repository&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/huggingface_hub/v0.14.1/guides/download#download-files-to-local-folder&#34;&gt;here&lt;/a&gt;, and set &lt;code&gt;pretrained_model_name_or_path&lt;/code&gt; of the guidance and the prompt processor to the local path.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# if you have agreed the license of DeepFloyd IF and have &amp;gt;20GB VRAM&#xA;# please try this configuration for higher quality&#xA;python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&#34;&#xA;# otherwise you could try with the Stable Diffusion model, which fits in 6GB VRAM&#xA;python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;threestudio uses &lt;a href=&#34;https://github.com/omry/omegaconf&#34;&gt;OmegaConf&lt;/a&gt; for flexible configurations. You can easily change any configuration in the YAML file by specifying arguments without &lt;code&gt;--&lt;/code&gt;, for example the specified prompt in the above cases. For all supported configurations, please see our &lt;a href=&#34;https://github.com/threestudio-project/threestudio/raw/main/DOCUMENTATION.md&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The training lasts for 10,000 iterations. You can find visualizations of the current status in the trial directory which defaults to &lt;code&gt;[exp_root_dir]/[name]/[tag]@[timestamp]&lt;/code&gt;, where &lt;code&gt;exp_root_dir&lt;/code&gt; (&lt;code&gt;outputs/&lt;/code&gt; by default), &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;tag&lt;/code&gt; can be set in the configuration file. A 360-degree video will be generated after the training is completed. In training, press &lt;code&gt;ctrl+c&lt;/code&gt; one time will stop training and head directly to the test stage which generates the video. Press &lt;code&gt;ctrl+c&lt;/code&gt; the second time to fully quit the program.&lt;/p&gt; &#xA;&lt;h3&gt;Multi-GPU training&lt;/h3&gt; &#xA;&lt;p&gt;Multi-GPU training is supported. Note that &lt;code&gt;data.batch_size&lt;/code&gt; is the batch size &lt;strong&gt;per rank (device)&lt;/strong&gt;. Also remember to&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set &lt;code&gt;data.n_val_views&lt;/code&gt; to be a multiple of the number of GPUs.&lt;/li&gt; &#xA; &lt;li&gt;Set a unique &lt;code&gt;tag&lt;/code&gt; as timestamp is disabled in multi-GPU training and will not be appended after the tag. If you the same tag as previous trials, saved config files, code and visualizations will be overriden.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# this results in an effective batch size of 4 (number of GPUs) * 2 (data.batch_size) = 8&#xA;python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0,1,2,3 system.prompt_processor.prompt=&#34;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&#34; data.batch_size=2 data.n_val_views=4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Resume from checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;If you want to resume from a checkpoint, do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# resume training from the last checkpoint, you may replace last.ckpt with any other checkpoints&#xA;python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/configs/last.ckpt&#xA;# if the training has completed, you can still continue training for a longer time by setting trainer.max_steps&#xA;python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/configs/last.ckpt trainer.max_steps=20000&#xA;# you can also perform testing using resumed checkpoints&#xA;python launch.py --config path/to/trial/dir/configs/parsed.yaml --test --gpu 0 resume=path/to/trial/configs/last.ckpt&#xA;# note that the above commands use parsed configuration files from previous trials&#xA;# which will continue using the same trial directory&#xA;# if you want to save to a new trial directory, replace parsed.yaml with raw.yaml in the command&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Export Meshes&lt;/h3&gt; &#xA;&lt;p&gt;To export the scene to texture meshes, use the &lt;code&gt;--export&lt;/code&gt; option. We currently support exporting to obj+mtl, or obj with vertex colors.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# this uses default mesh-exporter configurations which exports obj+mtl&#xA;python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/configs/last.ckpt system.exporter_type=mesh-exporter&#xA;# specify system.exporter.fmt=obj to get obj with vertex colors&#xA;# you may also add system.exporter.save_uv=false to accelerate the process, suitable for a quick peek of the result&#xA;python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/configs/last.ckpt system.exporter_type=mesh-exporter system.exporter.fmt=obj&#xA;# for NeRF-based methods (DreamFusion, Magic3D coarse, Latent-NeRF, SJC)&#xA;# you may need to adjust the isosurface threshold (25 by default) to get satisfying outputs&#xA;# decrease the threshold if the extracted model is incomplete, increase if it is extruded&#xA;python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/configs/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_threshold=10.&#xA;# use marching cubes of higher resolutions to get more detailed models&#xA;python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a delicious hamburger&#34; system.from_coarse=path/to/coarse/stage/trial/ckpts/last.ckpt system.geometry.isosurface_method=mc-cpu system.geometry.isosurface_resolution=256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For all the options you can specify when exporting, see &lt;a href=&#34;https://github.com/threestudio-project/threestudio/raw/main/DOCUMENTATION.md#exporters&#34;&gt;the documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/threestudio-project/threestudio#supported-models&#34;&gt;here&lt;/a&gt; for example running commands of all our supported models. Please refer to &lt;a href=&#34;https://github.com/threestudio-project/threestudio#tips-on-improving-quality&#34;&gt;here&lt;/a&gt; for tips on getting higher-quality results, and &lt;a href=&#34;https://github.com/threestudio-project/threestudio#vram-optimization&#34;&gt;here&lt;/a&gt; for reducing VRAM usage.&lt;/p&gt; &#xA;&lt;p&gt;For feature requests, bug reports, or discussions about technical problems, please &lt;a href=&#34;https://github.com/threestudio-project/threestudio/issues/new&#34;&gt;file an issue&lt;/a&gt;. In case you want to discuss the generation quality or showcase your generation results, please feel free to participate in the &lt;a href=&#34;https://github.com/threestudio-project/threestudio/discussions&#34;&gt;discussion panel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;h3&gt;DreamFusion &lt;a href=&#34;https://arxiv.org/abs/2209.14988&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2209.14988-b31b1b.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results obtained by threestudio (DeepFloyd IF, batch size 8)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/19284678/236694848-38ae4ea4-554b-4c9d-b4c7-fba5bee3acb3.mp4&#34;&gt;https://user-images.githubusercontent.com/19284678/236694848-38ae4ea4-554b-4c9d-b4c7-fba5bee3acb3.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notable differences from the paper&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We use open-source T2I models (StableDiffusion, DeepFloyd IF), while the paper uses Imagen.&lt;/li&gt; &#xA; &lt;li&gt;We use a guiandance scale of 20 for DeepFloyd IF, while the paper uses 100 for Imagen.&lt;/li&gt; &#xA; &lt;li&gt;We do not use sigmoid to normalize the albedo color but simply scale the color from &lt;code&gt;[-1,1]&lt;/code&gt; to &lt;code&gt;[0,1]&lt;/code&gt;, as we find this help convergence.&lt;/li&gt; &#xA; &lt;li&gt;We use HashGrid encoding and uniformly sample points along rays, while the paper uses Integrated Positional Encoding and sampling strategy from MipNeRF360.&lt;/li&gt; &#xA; &lt;li&gt;We adopt camera settings and density initialization strategy from Magic3D, which is slightly different from the DreamFusion paper.&lt;/li&gt; &#xA; &lt;li&gt;Some hyperparameters are different, such as the weighting of loss terms.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example running commands&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training&#xA;# here we adopt random background augmentation to improve geometry quality&#xA;python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a delicious hamburger&#34; system.background.random_aug=true&#xA;# uses StableDiffusion, requires ~6GB VRAM in training&#xA;python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a delicious hamburger&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DeepFloyd IF performs &lt;strong&gt;way better than&lt;/strong&gt; StableDiffusion.&lt;/li&gt; &#xA; &lt;li&gt;Validation shows albedo color before &lt;code&gt;system.material.ambient_only_steps&lt;/code&gt; and shaded color after that.&lt;/li&gt; &#xA; &lt;li&gt;Try increasing/decreasing &lt;code&gt;system.loss.lambda_sparsity&lt;/code&gt; if your scene is stuffed with floaters/becoming empty.&lt;/li&gt; &#xA; &lt;li&gt;Try increasing/decreasing &lt;code&gt;system.loss.lambda_orient&lt;/code&gt; if you object is foggy/over-smoothed.&lt;/li&gt; &#xA; &lt;li&gt;Try replacing the background to random colors with a probability 0.5 by setting &lt;code&gt;system.background.random_aug=true&lt;/code&gt; if you find the model incorrectly treats the background as part of the object.&lt;/li&gt; &#xA; &lt;li&gt;DeepFloyd IF uses T5-XXL as its text encoder, which consumes ~15GB VRAM even when using 8-bit quantization. This is currently the bottleneck for training with less VRAM. If anyone knows how to run the text encoder with less VRAM, please file an issue. We&#39;re also trying to push the text encoder to &lt;a href=&#34;https://replicate.com/&#34;&gt;Replicate&lt;/a&gt; to enable extracting text embeddings via API, but are having some network connection issues. Please &lt;a href=&#34;mailto:imbennyguo@gmail.com&#34;&gt;contact bennyguo&lt;/a&gt; if you would like to help out.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Magic3D &lt;a href=&#34;https://arxiv.org/abs/2211.10440&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2211.10440-b31b1b.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results obtained by threestudio (DeepFloyd IF, batch size 8; first row: coarse, second row: refine)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/19284678/236694858-0ed6939e-cd7a-408f-a94b-406709ae90c0.mp4&#34;&gt;https://user-images.githubusercontent.com/19284678/236694858-0ed6939e-cd7a-408f-a94b-406709ae90c0.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notable differences from the paper&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We use open-source T2I models (StableDiffusion, DeepFloyd IF) for the coarse stage, while the paper uses eDiff-I.&lt;/li&gt; &#xA; &lt;li&gt;In the coarse stage, we use a guiandance scale of 20 for DeepFloyd IF, while the paper uses 100 for eDiff-I.&lt;/li&gt; &#xA; &lt;li&gt;In the coarse stage, we use analytic normal, while the paper uses predicted normal.&lt;/li&gt; &#xA; &lt;li&gt;In the coarse stage, we use orientation loss as in DreamFusion, while the paper does not.&lt;/li&gt; &#xA; &lt;li&gt;There are many things that are ommited from the paper such as the weighting of loss terms and the DMTet grid resolution, which could be different.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example running commands&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;First train the coarse stage NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training&#xA;python launch.py --config configs/magic3d-coarse-if.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a delicious hamburger&#34;&#xA;# uses StableDiffusion, requires ~6GB VRAM in training&#xA;python launch.py --config configs/magic3d-coarse-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a delicious hamburger&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then convert the NeRF from the coarse stage to DMTet and train with differentiable rasterization:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# the refinement stage uses StableDiffusion, requires ~5GB VRAM in training&#xA;# NOTE: the meaning of system.from_coarse has changed from cfff05, it is now the path to the coarse stage weights instead of a boolean value&#xA;python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a delicious hamburger&#34; system.from_coarse=path/to/coarse/stage/trial/ckpts/last.ckpt&#xA;# if you&#39;re unsatisfied with the surface extraced using the default threshold (25)&#xA;# you can specify a threshold value using `system.coarse_geometry_override`&#xA;# decrease the value if the extracted surface is incomplete, increate if it is extruded&#xA;python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a delicious hamburger&#34; system.from_coarse=path/to/coarse/stage/trial/ckpts/last.ckpt system.coarse_geometry_override.isosurface_threshold=10.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the coarse stage, DeepFloyd IF performs &lt;strong&gt;way better than&lt;/strong&gt; StableDiffusion.&lt;/li&gt; &#xA; &lt;li&gt;Magic3D uses a neural network to predict the surface normal, which may not resemble the true geometric normal and degrade geometry quality, so we use analytic normal instead.&lt;/li&gt; &#xA; &lt;li&gt;Try increasing/decreasing &lt;code&gt;system.loss.lambda_sparsity&lt;/code&gt; if your scene is stuffed with floaters/becoming empty.&lt;/li&gt; &#xA; &lt;li&gt;Try increasing/decreasing &lt;code&gt;system.loss.lambda_orient&lt;/code&gt; if you object is foggy/over-smoothed.&lt;/li&gt; &#xA; &lt;li&gt;Try replacing the background to random colors with a probability 0.5 by setting &lt;code&gt;system.background.random_aug=true&lt;/code&gt; if you find the model incorrectly treats the background as part of the object.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Score Jacobian Chaining &lt;a href=&#34;https://arxiv.org/abs/2212.00774&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2212.00774-b31b1b.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results obtained by threestudio (Stable Diffusion)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/19284678/236694871-87a247c1-2d3d-4cbf-89df-450bfeac3aca.mp4&#34;&gt;https://user-images.githubusercontent.com/19284678/236694871-87a247c1-2d3d-4cbf-89df-450bfeac3aca.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notable differences from the paper: N/A.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example running commands&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# train with sjc guidance in latent space&#xA;python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;A high quality photo of a delicious burger&#34;&#xA;# train with sjc guidance in latent space, trump figure&#xA;python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;Trump figure&#34; trainer.max_steps=30000 system.loss.lambda_emptiness=&#34;[15000,10000.0,200000.0,15001]&#34; system.optimizer.params.background.lr=0.05 seed=42&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SJC uses subpixel rendering which decodes a &lt;code&gt;128x128&lt;/code&gt; latent feature map for better visualization quality. You can turn off this feature by &lt;code&gt;system.subpixel_rendering=false&lt;/code&gt; to save VRAM in validation/testing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Latent-NeRF &lt;a href=&#34;https://arxiv.org/abs/2211.07600&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2211.07600-b31b1b.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results obtained by threestudio (Stable Diffusion)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/19284678/236694876-5a270347-6a41-4429-8909-44c90c554e06.mp4&#34;&gt;https://user-images.githubusercontent.com/19284678/236694876-5a270347-6a41-4429-8909-44c90c554e06.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notable differences from the paper: N/A.&lt;/p&gt; &#xA;&lt;p&gt;We currently only implement Latent-NeRF for text-guided and Sketch-Shape for (text,shape)-guided 3D generation. Latent-Paint is not implemented yet.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example running commands&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# train Latent-NeRF in Stable Diffusion latent space&#xA;python launch.py --config configs/latentnerf.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a delicious hamburger&#34;&#xA;# refine Latent-NeRF in RGB space&#xA;python launch.py --config configs/latentnerf-refine.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a delicious hamburger&#34; system.weights=path/to/latent/stage/trial/ckpts/last.ckpt&#xA;&#xA;# train Sketch-Shape in Stable Diffusion latent space&#xA;python launch.py --config configs/sketchshape.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=&#34;a teddy bear in a tuxedo&#34;&#xA;# refine Sketch-Shape in RGB space&#xA;python launch.py --config configs/sketchshape-refine.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=&#34;a teddy bear in a tuxedo&#34; system.weights=path/to/latent/stage/trial/ckpts/last.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fantasia3D &lt;a href=&#34;https://arxiv.org/abs/2303.13873&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2303.13873-b31b1b.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results obtained by threestudio (Stable Diffusion)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/19284678/236694880-33b0db21-4530-47f1-9c3b-c70357bc84b3.mp4&#34;&gt;https://user-images.githubusercontent.com/19284678/236694880-33b0db21-4530-47f1-9c3b-c70357bc84b3.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notable differences from the paper: N/A.&lt;/p&gt; &#xA;&lt;p&gt;We currently only implement the geometry stage of Fantasia3D.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example running commands&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a DSLR photo of an ice cream sundae&#34;&#xA;# Fantasia3D highly relies on the initialized SDF shape&#xA;# the default shape is a sphere with radius 0.5&#xA;# change the shape initialization to match your input prompt&#xA;python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;The leaning tower of Pisa&#34; system.geometry.shape_init=ellipsoid system.geometry.shape_init_params=&#34;[0.3,0.3,0.8]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you find the shape easily diverge in early training stages, you may use a lower guidance scale by setting &lt;code&gt;system.guidance.guidance_scale=30.&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ProlificDreamer &lt;a href=&#34;https://arxiv.org/abs/2305.16213&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2305.16213-b31b1b.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results obtained by threestudio (Stable Diffusion, 256x256, 25000 iterations)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/threestudio-project/threestudio/assets/19284678/1f0081bf-c877-4e7a-9047-a8aa6431a561&#34;&gt;https://github.com/threestudio-project/threestudio/assets/19284678/1f0081bf-c877-4e7a-9047-a8aa6431a561&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT NOTE: This is an unofficial experimental implementation! The quality is still far from the paper. Please refer to &lt;a href=&#34;https://github.com/thu-ml/prolificdreamer&#34;&gt;https://github.com/thu-ml/prolificdreamer&lt;/a&gt; for official code release.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We currently only experiment on the first stage (NeRF training), although the third stage is already implemented (mesh texture refinement), and the second stage is easy to implement too (mesh normal optimization). Some other important design factors that are not implemented:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;multiple particles&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# object geneartion with 64x64 NeRF rendering, ~14GB VRAM&#xA;python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a DSLR photo of a delicious croissant&#34; data.width=64 data.height=64&#xA;# object generation with 512x512 NeRF rendering (original paper), &amp;gt;24GB VRAM&#xA;python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;a DSLR photo of a delicious croissant&#34; data.width=512 data.height=512&#xA;# scene generation&#xA;python launch.py --config configs/prolificdreamer-scene.yaml --train --gpu 0 system.prompt_processor.prompt=&#34;Inside of a smart home, realistic detailed photo, 4k&#34; data.width=64 data.height=64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;More to come, please stay tuned.&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://bluestyle97.github.io/dream3d/&#34;&gt;Dream3D&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.14704&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2212.14704-b31b1b.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://yukangcao.github.io/DreamAvatar/&#34;&gt;DreamAvatar&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.00916&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2304.00916-b31b1b.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you would like to contribute a new method to threestudio, see &lt;a href=&#34;https://github.com/threestudio-project/threestudio#contributing-to-threestudio&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Prompt Library&lt;/h2&gt; &#xA;&lt;p&gt;For easier comparison, we collect the 397 preset prompts from the website of &lt;a href=&#34;https://dreamfusion3d.github.io/gallery.html&#34;&gt;DreamFusion&lt;/a&gt; in &lt;a href=&#34;https://github.com/threestudio-project/threestudio/raw/main/load/prompt_library.json&#34;&gt;this file&lt;/a&gt;. You can use these prompts by setting &lt;code&gt;system.prompt_processor.prompt=lib:keyword1_keyword2_..._keywordN&lt;/code&gt;. Note that the prompt should starts with &lt;code&gt;lib:&lt;/code&gt; and all the keywords are separated by &lt;code&gt;_&lt;/code&gt;. The prompt processor will match the keywords to all the prompts in the library, and will only succeed if there&#39;s &lt;strong&gt;exactly one match&lt;/strong&gt;. The used prompt will be printed to console. Also note that you can&#39;t use this syntax to point to every prompt in the library, as there are prompts that are subset of other prompts lmao. We will enhance the use of this feature.&lt;/p&gt; &#xA;&lt;h2&gt;Tips on Improving Quality&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s important to note that existing techniques that lift 2D T2I models to 3D cannot consistently produce satisfying results. Results from the great papers like DreamFusion and Magic3D are (to some extend) cherry-pickled, so don&#39;t be frustrated if you did not get what you expected on your first trial. Here are some tips that may help you improve the generation quality:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Increase batch size&lt;/strong&gt;. Large batch sizes help convergence and improve the 3D consistency of the geometry. State-of-the-art methods claims using large batch sizes: DreamFusion uses a batch size of 4; Magic3D uses a batch size of 32; Fantasia3D uses a batch size of 24; some results shown above uses a batch size of 8. You can easily change the batch size by setting &lt;code&gt;data.batch_size=N&lt;/code&gt;. Increasing the batch size requires more VRAM. If you have limited VRAM but still want the benefit of large batch sizes, you may use &lt;a href=&#34;https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#accumulate-gradients&#34;&gt;gradient accumulation provided by PyTorch Lightning&lt;/a&gt; by setting &lt;code&gt;trainer.accumulate_grad_batches=N&lt;/code&gt;. This will accumulate the gradient of several batches and achieve a large effective batch size. Note that if you use gradient accumulation, you may need to multiply all step values by N times in your config, such as values that have the name &lt;code&gt;X_steps&lt;/code&gt; and &lt;code&gt;trainer.val_check_interval&lt;/code&gt;, since now N batches equal to a large batch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Train longer.&lt;/strong&gt; This helps if you can already obtain reasonable results and would like to enhance the details. If the result is still a mess after several thousand steps, training for a longer time often won&#39;t help. You can set the total training iterations by &lt;code&gt;trainer.max_steps=N&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Try different seeds.&lt;/strong&gt; This is a simple solution if your results have correct overall geometry but suffer from the multi-face Janus problem. You can change the seed by setting &lt;code&gt;seed=N&lt;/code&gt;. Good luck!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tuning regularization weights.&lt;/strong&gt; Some methods have regularizaion terms which can be essential to obtaining good geometry. Try tuning the weights of these regularizations by setting &lt;code&gt;system.loss.lambda_X=value&lt;/code&gt;. The specific values depend on your situation, you may refer to &lt;a href=&#34;https://github.com/threestudio-project/threestudio#supported-models&#34;&gt;tips for each supported model&lt;/a&gt; for more detailed instructions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;VRAM Optimization&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter CUDA OOM error, try the following in order (roughly sorted by recommendation) to meet your VRAM requirement.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you only encounter OOM at validation/test time, you can set &lt;code&gt;system.cleanup_after_validation_step=true&lt;/code&gt; and &lt;code&gt;system.cleanup_after_test_step=true&lt;/code&gt; to free memory after each validation/test step. This will slow down validation/testing.&lt;/li&gt; &#xA; &lt;li&gt;Use a smaller batch size or use gradient accumulation as demonstrated &lt;a href=&#34;https://github.com/threestudio-project/threestudio#tips-on-improving-quality&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you are using PyTorch1.x, enable &lt;a href=&#34;https://huggingface.co/docs/diffusers/optimization/fp16#memory-efficient-attention&#34;&gt;memory efficient attention&lt;/a&gt; by setting &lt;code&gt;system.guidance.enable_memory_efficient_attention=true&lt;/code&gt;. PyTorch2.0 has built-in support for this optimization and is enabled by default.&lt;/li&gt; &#xA; &lt;li&gt;Enable &lt;a href=&#34;https://huggingface.co/docs/diffusers/optimization/fp16#sliced-attention-for-additional-memory-savings&#34;&gt;attention slicing&lt;/a&gt; by setting &lt;code&gt;system.guidance.enable_attention_slicing=true&lt;/code&gt;. This will slow down training by ~20%.&lt;/li&gt; &#xA; &lt;li&gt;If you are using StableDiffusionGuidance, you can use &lt;a href=&#34;https://github.com/dbolya/tomesd&#34;&gt;Token Merging&lt;/a&gt; to &lt;strong&gt;drastically&lt;/strong&gt; speed up computation and save memory. You can easily enable Token Merging by setting &lt;code&gt;system.guidance.token_merging=true&lt;/code&gt;. You can also customize the Token Merging behavior by setting the parameters &lt;a href=&#34;https://github.com/dbolya/tomesd/raw/main/tomesd/patch.py#L183-L213&#34;&gt;here&lt;/a&gt; to &lt;code&gt;system.guidance.token_merging_params&lt;/code&gt;. Note that Token Merging may degrade generation quality.&lt;/li&gt; &#xA; &lt;li&gt;Enable &lt;a href=&#34;https://huggingface.co/docs/diffusers/optimization/fp16#offloading-to-cpu-with-accelerate-for-memory-savings&#34;&gt;sequential CPU offload&lt;/a&gt; by setting &lt;code&gt;system.guidance.enable_sequential_cpu_offload=true&lt;/code&gt;. This could save a lot of VRAM but will make the training &lt;strong&gt;extremely slow&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;threestudio use &lt;a href=&#34;https://github.com/omry/omegaconf&#34;&gt;OmegaConf&lt;/a&gt; to manage configurations. You can literally change anything inside the yaml configuration file or by adding command line arguments without &lt;code&gt;--&lt;/code&gt;. We list all arguments that you can change in the configuration in our &lt;a href=&#34;https://github.com/threestudio-project/threestudio/raw/main/DOCUMENTATION.md&#34;&gt;documentation&lt;/a&gt;. Happy experimenting!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to threestudio&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork the repository and create your branch from &lt;code&gt;main&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install development dependencies:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements-dev.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are using VSCode as the text editor: (1) Install &lt;code&gt;editorconfig&lt;/code&gt; extension. (2) Set the default linter to mypy to enable static type checking. (3) Set the default formatter to black. You could either manually format the document or let the editor format the document each time it is saved by setting &lt;code&gt;&#34;editor.formatOnSave&#34;: true&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;pre-commit install&lt;/code&gt; to install pre-commit hooks which will automatically format the files before commit.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make changes to the code, update README and DOCUMENTATION if needed, and open a pull request.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Code Structure&lt;/h3&gt; &#xA;&lt;p&gt;Here we just briefly introduce the code structure of this project. We will make more detailed documentation about this in the future.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All methods are implemented as a subclass of &lt;code&gt;BaseSystem&lt;/code&gt; (in &lt;code&gt;systems/base.py&lt;/code&gt;). There typically are six modules inside a system: geometry, material, background, renderer, guidance, and prompt_processor. All modules are subclass of &lt;code&gt;BaseModule&lt;/code&gt; (in &lt;code&gt;utils/base.py&lt;/code&gt;) except for guidance, and prompt_processor, which are subclass of &lt;code&gt;BaseObject&lt;/code&gt; to prevent them from being treated as model parameters and better control their behavior in multi-GPU settings.&lt;/li&gt; &#xA; &lt;li&gt;All systems, modules, and data modules have their configurations in their own dataclasses.&lt;/li&gt; &#xA; &lt;li&gt;Base configurations for the whole project can be found in &lt;code&gt;utils/config.py&lt;/code&gt;. In the &lt;code&gt;ExperimentConfig&lt;/code&gt; dataclass, &lt;code&gt;data&lt;/code&gt;, &lt;code&gt;system&lt;/code&gt;, and module configurations under &lt;code&gt;system&lt;/code&gt; are parsed to configurations of each class mentioned above. These configurations are strictly typed, which means you can only use defined properties in the dataclass and stick to the defined type of each property. This configuration paradigm (1) natually supports default values for properties; (2) effectively prevents wrong assignments of these properties (say typos in the yaml file) or inappropriate usage at runtime.&lt;/li&gt; &#xA; &lt;li&gt;This projects use both static and runtime type checking. For more details, see &lt;code&gt;utils/typing.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To update anything of a module at each training step, simply make it inherit to &lt;code&gt;Updateable&lt;/code&gt; (see &lt;code&gt;utils/base.py&lt;/code&gt;). At the beginning of each iteration, an &lt;code&gt;Updateable&lt;/code&gt; will update itself, and update all its attributes that are also &lt;code&gt;Updateable&lt;/code&gt;. Note that subclasses of &lt;code&gt;BaseSystem&lt;/code&gt;, &lt;code&gt;BaseModule&lt;/code&gt; and &lt;code&gt;BaseObject&lt;/code&gt; are by default inherit to &lt;code&gt;Updateable&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known Problems&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradients of Vanilla MLP parameters are empty in AMP (temporarily fixed by disabling autocast).&lt;/li&gt; &#xA; &lt;li&gt;FullyFused MLP may cause NaNs in 32 precision.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;threestudio is built on the following amazing open-source projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Lightning-AI/lightning&#34;&gt;Lightning&lt;/a&gt;&lt;/strong&gt; Framework for creating highly organized PyTorch code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/omry/omegaconf&#34;&gt;OmegaConf&lt;/a&gt;&lt;/strong&gt; Flexible Python configuration system.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/KAIR-BAIR/nerfacc&#34;&gt;NerfAcc&lt;/a&gt;&lt;/strong&gt; Plug-and-play NeRF acceleration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following repositories greatly inspire threestudio:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;Stable-DreamFusion&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/eladrich/latent-nerf&#34;&gt;Latent-NeRF&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/pals-ttic/sjc&#34;&gt;Score Jacobian Chaining&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/ashawkey/fantasia3d.unofficial&#34;&gt;Fantasia3D.unofficial&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to the maintainers of these projects for their contribution to the community!&lt;/p&gt; &#xA;&lt;h2&gt;Citing threestudio&lt;/h2&gt; &#xA;&lt;p&gt;If you find threestudio helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Misc{threestudio2023,&#xA;  author =       {Yuan-Chen Guo and Ying-Tian Liu and Chen Wang and Zi-Xin Zou and Guan Luo and Chia-Hao Chen and Yan-Pei Cao and Song-Hai Zhang},&#xA;  title =        {threestudio: A unified framework for 3D content generation},&#xA;  howpublished = {\url{https://github.com/threestudio-project/threestudio}},&#xA;  year =         {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>