<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-26T01:38:50Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kyutai-labs/moshi</title>
    <updated>2025-01-26T01:38:50Z</updated>
    <id>tag:github.com,2025-01-26:/kyutai-labs/moshi</id>
    <link href="https://github.com/kyutai-labs/moshi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Moshi: a speech-text foundation model for real time dialogue&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kyutai-labs/moshi/workflows/precommit/badge.svg?sanitize=true&#34; alt=&#34;precommit badge&#34;&gt; &lt;img src=&#34;https://github.com/kyutai-labs/moshi/workflows/Rust%20CI/badge.svg?sanitize=true&#34; alt=&#34;rust ci badge&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.00037&#34;&gt;[Read the paper]&lt;/a&gt; &lt;a href=&#34;https://moshi.chat&#34;&gt;[Demo]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/kyutai/moshi-v01-release-66eaeaf3302bef6bd9ad7acd&#34;&gt;[Hugging Face]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.00037&#34;&gt;Moshi&lt;/a&gt; is a speech-text foundation model and &lt;strong&gt;full-duplex&lt;/strong&gt; spoken dialogue framework. It uses &lt;a href=&#34;https://arxiv.org/abs/2410.00037&#34;&gt;Mimi&lt;/a&gt;, a state-of-the-art streaming neural audio codec. Mimi processes 24 kHz audio, down to a 12.5 Hz representation with a bandwidth of 1.1 kbps, in a fully streaming manner (latency of 80ms, the frame size), yet performs better than existing, non-streaming, codecs like &lt;a href=&#34;https://github.com/ZhangXInFD/SpeechTokenizer&#34;&gt;SpeechTokenizer&lt;/a&gt; (50 Hz, 4kbps), or &lt;a href=&#34;https://github.com/haoheliu/SemantiCodec-inference&#34;&gt;SemantiCodec&lt;/a&gt; (50 Hz, 1.3kbps).&lt;/p&gt; &#xA;&lt;p&gt;Moshi models &lt;strong&gt;two streams of audio&lt;/strong&gt;: one corresponds to Moshi, and the other one to the user. At inference, the stream from the user is taken from the audio input, and the one for Moshi is sampled from the model&#39;s output. Along these two audio streams, Moshi predicts text tokens corresponding to its own speech, its &lt;strong&gt;inner monologue&lt;/strong&gt;, which greatly improves the quality of its generation. A small Depth Transformer models inter codebook dependencies for a given time step, while a large, 7B parameter Temporal Transformer models the temporal dependencies. Moshi achieves a theoretical latency of 160ms (80ms for the frame size of Mimi + 80ms of acoustic delay), with a practical overall latency as low as 200ms on an L4 GPU.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://moshi.chat&#34;&gt;Talk to Moshi&lt;/a&gt; now on our live demo.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kyutai-labs/moshi/main/moshi.png&#34; alt=&#34;Schema representing the structure of Moshi. Moshi models two streams of audio:&#xA;    one corresponds to Moshi, and the other one to the user. At inference, the audio stream of the user is taken from the audio input, and the audio stream for Moshi is sampled from the model&#39;s output. Along that, Moshi predicts text tokens corresponding to its own speech for improved accuracy. A small Depth Transformer models inter codebook dependencies for a given step.&#34; width=&#34;650px&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Mimi builds on previous neural audio codecs such as &lt;a href=&#34;https://arxiv.org/abs/2107.03312&#34;&gt;SoundStream&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;EnCodec&lt;/a&gt;, adding a Transformer both in the encoder and decoder, and adapting the strides to match an overall frame rate of 12.5 Hz. This allows Mimi to get closer to the average frame rate of text tokens (~3-4 Hz), and limit the number of autoregressive steps in Moshi. Similarly to SpeechTokenizer, Mimi uses a distillation loss so that the first codebook tokens match a self-supervised representation from &lt;a href=&#34;https://arxiv.org/abs/2110.13900&#34;&gt;WavLM&lt;/a&gt;, which allows modeling semantic and acoustic information with a single model. Interestingly, while Mimi is fully causal and streaming, it learns to match sufficiently well the non-causal representation from WavLM, without introducing any delays. Finally, and similarly to &lt;a href=&#34;https://arxiv.org/pdf/2210.14090&#34;&gt;EBEN&lt;/a&gt;, Mimi uses &lt;strong&gt;only an adversarial training loss&lt;/strong&gt;, along with feature matching, showing strong improvements in terms of subjective quality despite its low bitrate.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kyutai-labs/moshi/main/mimi.png&#34; alt=&#34;Schema representing the structure of Mimi, our proposed neural codec. Mimi contains a Transformer&#xA;in both its encoder and decoder, and achieves a frame rate closer to that of text tokens. This allows us to reduce&#xA;the number of auto-regressive steps taken by Moshi, thus reducing the latency of the model.&#34; width=&#34;800px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Organisation of the repository&lt;/h2&gt; &#xA;&lt;p&gt;There are three separate versions of the moshi inference stack in this repo.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Python version using PyTorch is in the &lt;a href=&#34;https://raw.githubusercontent.com/kyutai-labs/moshi/main/moshi/&#34;&gt;&lt;code&gt;moshi/&lt;/code&gt;&lt;/a&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;The Python version using MLX for M series Macs is in the &lt;a href=&#34;https://raw.githubusercontent.com/kyutai-labs/moshi/main/moshi_mlx/&#34;&gt;&lt;code&gt;moshi_mlx/&lt;/code&gt;&lt;/a&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;The Rust version used in production is in the &lt;a href=&#34;https://raw.githubusercontent.com/kyutai-labs/moshi/main/rust/&#34;&gt;&lt;code&gt;rust/&lt;/code&gt;&lt;/a&gt; directory. This contains in particular a Mimi implementation in Rust, with Python bindings available as &lt;code&gt;rustymimi&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Finally, the code for the live demo is provided in the &lt;a href=&#34;https://raw.githubusercontent.com/kyutai-labs/moshi/main/client/&#34;&gt;&lt;code&gt;client/&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;We release three models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;our speech codec Mimi,&lt;/li&gt; &#xA; &lt;li&gt;Moshi fine-tuned on a male synthetic voice (Moshiko),&lt;/li&gt; &#xA; &lt;li&gt;Moshi fine-tuned on a female synthetic voice (Moshika).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Depending on the backend, the file format and quantization available will vary. Here is the list of the HuggingFace repo with each model. Mimi is bundled in each of those, and always use the same checkpoint format.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Moshika for PyTorch (bf16): &lt;a href=&#34;https://huggingface.co/kyutai/moshika-pytorch-bf16&#34;&gt;kyutai/moshika-pytorch-bf16&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Moshiko for PyTorch (bf16): &lt;a href=&#34;https://huggingface.co/kyutai/moshiko-pytorch-bf16&#34;&gt;kyutai/moshiko-pytorch-bf16&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Moshika for MLX (int4, int8, bf16): &lt;a href=&#34;https://huggingface.co/kyutai/moshika-mlx-q4&#34;&gt;kyutai/moshika-mlx-q4&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/kyutai/moshika-mlx-q8&#34;&gt;kyutai/moshika-mlx-q8&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/kyutai/moshika-mlx-bf16&#34;&gt;kyutai/moshika-mlx-bf16&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Moshiko for MLX (int4, int8, bf16): &lt;a href=&#34;https://huggingface.co/kyutai/moshiko-mlx-q4&#34;&gt;kyutai/moshiko-mlx-q4&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/kyutai/moshiko-mlx-q8&#34;&gt;kyutai/moshiko-mlx-q8&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/kyutai/moshiko-mlx-bf16&#34;&gt;kyutai/moshiko-mlx-bf16&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Moshika for Rust/Candle (int8, bf16): &lt;a href=&#34;https://huggingface.co/kyutai/moshika-candle-q8&#34;&gt;kyutai/moshika-candle-q8&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/kyutai/moshika-candle-bf16&#34;&gt;kyutai/moshika-mlx-bf16&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Moshiko for Rust/Candle (int8, bf16): &lt;a href=&#34;https://huggingface.co/kyutai/moshiko-candle-q8&#34;&gt;kyutai/moshiko-candle-q8&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/kyutai/moshiko-candle-bf16&#34;&gt;kyutai/moshiko-mlx-bf16&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All models are released under the CC-BY 4.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You will need at least Python 3.10, with 3.12 recommended. For specific requirements, please check the individual backends directories. You can install the PyTorch and MLX clients with the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install moshi      # moshi PyTorch, from PyPI&#xA;pip install moshi_mlx  # moshi MLX, from PyPI, best with Python 3.12.&#xA;# Or the bleeding edge versions for Moshi and Moshi-MLX.&#xA;pip install -e &#34;git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi&amp;amp;subdirectory=moshi&#34;&#xA;pip install -e &#34;git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi_mlx&amp;amp;subdirectory=moshi_mlx&#34;&#xA;&#xA;pip install rustymimi  # mimi, rust implementation with Python bindings from PyPI&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are not using Python 3.12, you might get an error when installing &lt;code&gt;moshi_mlx&lt;/code&gt; or &lt;code&gt;rustymimi&lt;/code&gt; (which &lt;code&gt;moshi_mlx&lt;/code&gt; depends on). Then, you will need to install the &lt;a href=&#34;https://rustup.rs/&#34;&gt;Rust toolchain&lt;/a&gt;, or switch to Python 3.12.&lt;/p&gt; &#xA;&lt;p&gt;While we hope that the present codebase will work on Windows, we do not provide official support for it. We have tested the MLX version on a MacBook Pro M3. At the moment, we do not support quantization for the PyTorch version, so you will need a GPU with a significant amount of memory (24GB).&lt;/p&gt; &#xA;&lt;p&gt;For using the Rust backend, you will need a recent version of the &lt;a href=&#34;https://rustup.rs/&#34;&gt;Rust toolchain&lt;/a&gt;. To compile GPU support, you will also need the &lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA&lt;/a&gt; properly installed for your GPU, in particular with &lt;code&gt;nvcc&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Python (PyTorch)&lt;/h2&gt; &#xA;&lt;p&gt;The PyTorch based API can be found in the &lt;code&gt;moshi&lt;/code&gt; directory. It provides a streaming version of the audio tokenizer (mimi) and the language model (moshi).&lt;/p&gt; &#xA;&lt;p&gt;In order to run in interactive mode, you need to start a server which will run the model, you can then use either the web UI or a command line client.&lt;/p&gt; &#xA;&lt;p&gt;Start the server with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m moshi.server [--gradio-tunnel] [--hf-repo kyutai/moshika-pytorch-bf16]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then access the web UI on &lt;a href=&#34;http://localhost:8998&#34;&gt;localhost:8998&lt;/a&gt;. If your GPU is on a distant machine this will not work as websites using http are not allowed to use the audio worklet api. There are two ways to get around this:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forward the remote 8998 port to your localhost using ssh &lt;code&gt;-L&lt;/code&gt; flag. Then connects to &lt;a href=&#34;http://localhost:8998&#34;&gt;localhost:8998&lt;/a&gt; as mentionned previously.&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;code&gt;--gradio-tunnel&lt;/code&gt; argument, this sets up a tunnel with a URL accessible from anywhere. Keep in mind that this tunnel goes through the US and can add significant latency (up to 500ms from Europe). You can use &lt;code&gt;--gradio-tunnel-token&lt;/code&gt; to set a fixed secret token and reuse the same address over time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;--hf-repo&lt;/code&gt; to select a different pretrained model, by setting the proper Hugging Face repository.&lt;/p&gt; &#xA;&lt;p&gt;Accessing a server that is not localhost via http may cause issues with using the microphone in the web UI (in some browsers this is only allowed using https).&lt;/p&gt; &#xA;&lt;p&gt;A local client is also available, as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m moshi.client [--url URL_TO_GRADIO]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However note that, unlike the web browser, this client is barebone: it does not perform any echo cancellation, nor does it try to compensate for a growing lag by skipping frames.&lt;/p&gt; &#xA;&lt;p&gt;For more information, in particular on how to use the API directly, please checkout &lt;a href=&#34;https://raw.githubusercontent.com/kyutai-labs/moshi/main/moshi/README.md&#34;&gt;moshi/README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Python (MLX) for local inference on macOS&lt;/h2&gt; &#xA;&lt;p&gt;Once you have installed &lt;code&gt;moshi_mlx&lt;/code&gt;, you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m moshi_mlx.local -q 4   # weights quantized to 4 bits&#xA;python -m moshi_mlx.local -q 8   # weights quantized to 8 bits&#xA;# And using a different pretrained model:&#xA;python -m moshi_mlx.local -q 4 --hf-repo kyutai/moshika-mlx-q4&#xA;python -m moshi_mlx.local -q 8 --hf-repo kyutai/moshika-mlx-q8&#xA;# be careful to always match the `-q` and `--hf-repo` flag.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command line interface is also barebone. It does not perform any echo cancellation, nor does it try to compensate for a growing lag by skipping frames.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively you can run &lt;code&gt;python -m moshi_mlx.local_web&lt;/code&gt; to use the web UI, the connection is via http and will be at &lt;a href=&#34;http://localhost:8998&#34;&gt;localhost:8998&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Rust&lt;/h2&gt; &#xA;&lt;p&gt;In order to run the Rust inference server, use the following command from within the &lt;code&gt;rust&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo run --features cuda --bin moshi-backend -r -- --config moshi-backend/config.json standalone&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When using macOS, you can replace &lt;code&gt;--features cuda&lt;/code&gt; with &lt;code&gt;--features metal&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively you can use &lt;code&gt;config-q8.json&lt;/code&gt; rather than &lt;code&gt;config.json&lt;/code&gt; to use the quantized q8 model. You can select a different pretrained model, e.g. Moshika, by changing the &lt;code&gt;&#34;hf_repo&#34;&lt;/code&gt; key in either file.&lt;/p&gt; &#xA;&lt;p&gt;Once the server has printed &#39;standalone worker listening&#39;, you can use the web UI. By default the Rust server uses https so it will be at &lt;a href=&#34;https://localhost:8998&#34;&gt;localhost:8998&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will get warnings about the site being unsafe. When using chrome you can bypass these by selecting &#34;Details&#34; or &#34;Advanced&#34;, then &#34;Visit this unsafe site&#34; or &#34;Proceed to localhost (unsafe)&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Clients&lt;/h2&gt; &#xA;&lt;p&gt;We recommend using the web UI as it provides additional echo cancellation that helps the overall model quality. Note that most commands will directly serve this UI in the provided URL, and there is in general nothing more to do.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, we provide command line interfaces for the Rust and Python versions, the protocol is the same as with the web UI so there is nothing to change on the server side.&lt;/p&gt; &#xA;&lt;p&gt;For reference, here is the list of clients for Moshi.&lt;/p&gt; &#xA;&lt;h3&gt;Rust Command Line&lt;/h3&gt; &#xA;&lt;p&gt;From within the &lt;code&gt;rust&lt;/code&gt; directory, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo run --bin moshi-cli -r -- tui --host localhost&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python with PyTorch&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m moshi.client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA;&lt;p&gt;You can launch a Gradio demo locally with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m moshi.client_gradio --url &amp;lt;moshi-server-url&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Prior to running the Gradio demo, please install &lt;code&gt;gradio-webrtc&amp;gt;=0.0.18&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Docker Compose (CUDA only)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Requires &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;WebUI&lt;/h3&gt; &#xA;&lt;p&gt;The web UI can be built from this repo via the following steps (these will require &lt;code&gt;npm&lt;/code&gt; being installed).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd client&#xA;npm install&#xA;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The web UI can then be found in the &lt;code&gt;client/dist&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to install from a clone of this repository, maybe to further develop Moshi, you can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# From the root of the clone of the repo&#xA;pip install -e &#39;moshi[dev]&#39;&#xA;pip install -e &#39;moshi_mlx[dev]&#39;&#xA;pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you wish to build locally &lt;code&gt;rustymimi&lt;/code&gt; (assuming you have Rust properly installed):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install maturin&#xA;maturin dev -r -m rust/mimi-pyo3/Cargo.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;Checkout the &lt;a href=&#34;https://raw.githubusercontent.com/kyutai-labs/moshi/main/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt; section before opening an issue.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The present code is provided under the MIT license for the Python parts, and Apache license for the Rust backend. The web client code is provided under the MIT license. Note that parts of this code is based on &lt;a href=&#34;https://github.com/facebookresearch/audiocraft&#34;&gt;AudioCraft&lt;/a&gt;, released under the MIT license.&lt;/p&gt; &#xA;&lt;p&gt;The weights for the models are released under the CC-BY 4.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use either Mimi or Moshi, please cite the following paper,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@techreport{kyutai2024moshi,&#xA;      title={Moshi: a speech-text foundation model for real-time dialogue},&#xA;      author={Alexandre D\&#39;efossez and Laurent Mazar\&#39;e and Manu Orsini and&#xA;      Am\&#39;elie Royer and Patrick P\&#39;erez and Herv\&#39;e J\&#39;egou and Edouard Grave and Neil Zeghidour},&#xA;      year={2024},&#xA;      eprint={2410.00037},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={eess.AS},&#xA;      url={https://arxiv.org/abs/2410.00037},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>THUDM/CogAgent</title>
    <updated>2025-01-26T01:38:50Z</updated>
    <id>tag:github.com,2025-01-26:/THUDM/CogAgent</id>
    <link href="https://github.com/THUDM/CogAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-sourced end-to-end VLM-based GUI Agent&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CogAgent: An open-sourced VLM-based GUI Agent&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/README_zh.md&#34;&gt;中文文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 🆕 &lt;strong&gt;December 2024:&lt;/strong&gt; We open-sourced &lt;strong&gt;the latest version of the CogAgent-9B-20241220 model&lt;/strong&gt;. Compared to the previous version of CogAgent, &lt;code&gt;CogAgent-9B-20241220&lt;/code&gt; features significant improvements in GUI perception, reasoning accuracy, action space completeness, task universality, and generalization. It supports bilingual (Chinese and English) interaction through both screen captures and natural language.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🏆 &lt;strong&gt;June 2024:&lt;/strong&gt; CogAgent was accepted by &lt;strong&gt;CVPR 2024&lt;/strong&gt; and recognized as a conference Highlight (top 3%).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 2023:&lt;/strong&gt; We &lt;strong&gt;open-sourced the first GUI Agent&lt;/strong&gt;: &lt;strong&gt;CogAgent&lt;/strong&gt; (with the former repository available &lt;a href=&#34;https://github.com/THUDM/CogVLM&#34;&gt;here&lt;/a&gt;) and &lt;strong&gt;published the corresponding paper: 📖 &lt;a href=&#34;https://arxiv.org/abs/2312.08914&#34;&gt;CogAgent Paper&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Introduction&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Download Links&lt;/th&gt; &#xA;   &lt;th&gt;Technical Documentation&lt;/th&gt; &#xA;   &lt;th&gt;Online Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;cogagent-9b-20241220&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/THUDM/cogagent-9b-20241220&#34;&gt;🤗 HuggingFace&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://modelscope.cn/models/ZhipuAI/cogagent-9b-20241220&#34;&gt;🤖 ModelScope&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://wisemodel.cn/models/ZhipuAI/cogagent-9b-20241220&#34;&gt;🟣 WiseModel&lt;/a&gt; &lt;br&gt;&lt;a href=&#34;https://modelers.cn/models/zhipuai/cogagent-9b-20241220&#34;&gt;🧩 Modelers (Ascend)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cogagent.aminer.cn/blog#/articles/cogagent-9b-20241220-technical-report&#34;&gt;📄 Official Technical Blog&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://zhipu-ai.feishu.cn/wiki/MhPYwtpBhinuoikNIYYcyu8dnKv?fromScene=spaceOverview&#34;&gt;📘 Practical Guide (Chinese)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/THUDM-HF-SPACE/CogAgent-Demo&#34;&gt;🤗 HuggingFace Space&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://modelscope.cn/studios/ZhipuAI/CogAgent-Demo&#34;&gt;🤖 ModelScope Space&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://modelers.cn/spaces/zhipuai/CogAgent&#34;&gt;🧩 Modelers Space (Ascend)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Model Overview&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;CogAgent-9B-20241220&lt;/code&gt; model is based on &lt;a href=&#34;https://huggingface.co/THUDM/glm-4v-9b&#34;&gt;GLM-4V-9B&lt;/a&gt;, a bilingual open-source VLM base model. Through data collection and optimization, multi-stage training, and strategy improvements, &lt;code&gt;CogAgent-9B-20241220&lt;/code&gt; achieves significant advancements in GUI perception, inference prediction accuracy, action space completeness, and generalizability across tasks. The model supports bilingual (Chinese and English) interaction with both screenshots and language input. This version of the CogAgent model has already been applied in ZhipuAI&#39;s &lt;a href=&#34;https://cogagent.aminer.cn/home&#34;&gt;GLM-PC product&lt;/a&gt;. We hope the release of this model can assist researchers and developers in advancing the research and applications of GUI agents based on vision-language models.&lt;/p&gt; &#xA;&lt;h3&gt;Capability Demonstrations&lt;/h3&gt; &#xA;&lt;p&gt;The CogAgent-9b-20241220 model has achieved state-of-the-art results across multiple platforms and categories in GUI Agent tasks and GUI Grounding Benchmarks. In the &lt;a href=&#34;https://cogagent.aminer.cn/blog#/articles/cogagent-9b-20241220-technical-report&#34;&gt;CogAgent-9b-20241220 Technical Blog&lt;/a&gt;, we compared it against API-based commercial models (GPT-4o-20240806, Claude-3.5-Sonnet), commercial API + GUI Grounding models (GPT-4o + UGround, GPT-4o + OS-ATLAS), and open-source GUI Agent models (Qwen2-VL, ShowUI, SeeClick). The results demonstrate that &lt;strong&gt;CogAgent leads in GUI localization (Screenspot), single-step operations (OmniAct), the Chinese step-wise in-house benchmark (CogAgentBench-basic-cn), and multi-step operations (OSWorld)&lt;/strong&gt;, with only a slight disadvantage in OSWorld compared to Claude-3.5-Sonnet, which specializes in Computer Use, and GPT-4o combined with external GUI Grounding models.&lt;/p&gt; &#xA;&lt;div style=&#34;display: flex; flex-direction: column; width: 100%; align-items: center; margin-top: 20px;&#34;&gt; &#xA; &lt;div style=&#34;text-align: center; margin-bottom: 20px; width: 100%; max-width: 600px; height: auto;&#34;&gt; &#xA;  &lt;video src=&#34;https://github.com/user-attachments/assets/4d39fe6a-d460-427c-a930-b7cbe0d082f5&#34; width=&#34;100%&#34; height=&#34;auto&#34; controls autoplay loop&gt;&lt;/video&gt; &#xA;  &lt;p&gt;CogAgent wishes you a Merry Christmas! Let the large model automatically send Christmas greetings to your friends.&lt;/p&gt; &#xA; &lt;/div&gt; &#xA; &lt;div style=&#34;text-align: center; width: 100%; max-width: 600px; height: auto;&#34;&gt; &#xA;  &lt;video src=&#34;https://github.com/user-attachments/assets/87f00f97-1c4f-4152-b7c0-d145742cb910&#34; width=&#34;100%&#34; height=&#34;auto&#34; controls autoplay loop&gt;&lt;/video&gt; &#xA;  &lt;p&gt;Want to open an issue? Let CogAgent help you send an email.&lt;/p&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#cogagent&#34;&gt;CogAgent&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#model-introduction&#34;&gt;Model Introduction&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#model-overview&#34;&gt;Model Overview&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#capability-demonstrations&#34;&gt;Capability Demonstrations&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#inference-and-fine-tuning-costs&#34;&gt;Inference and Fine-tuning Costs&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#model-inputs-and-outputs&#34;&gt;Model Inputs and Outputs&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#user-input&#34;&gt;User Input&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#model-output&#34;&gt;Model Output&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#an-example&#34;&gt;An Example&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#notes&#34;&gt;Notes&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#running-the-model&#34;&gt;Running the Model&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#environment-setup&#34;&gt;Environment Setup&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#running-an-agent-app-example&#34;&gt;Running an Agent APP Example&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#fine-tuning-the-model&#34;&gt;Fine-tuning the Model&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#previous-work&#34;&gt;Previous Work&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/#research-and-development-team---acknowledgements&#34;&gt;Research and Development Team &amp;amp; Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference and Fine-tuning Costs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The model requires at least 29GB of VRAM for inference at &lt;code&gt;BF16&lt;/code&gt; precision. Using &lt;code&gt;INT4&lt;/code&gt; precision for inference is not recommended due to significant performance loss. The VRAM usage for &lt;code&gt;INT4&lt;/code&gt; inference is about 8GB, while for &lt;code&gt;INT8&lt;/code&gt; inference it is about 15GB. In the &lt;code&gt;inference/cli_demo.py&lt;/code&gt; file, we have commented out these two lines. You can uncomment them and use &lt;code&gt;INT4&lt;/code&gt; or &lt;code&gt;INT8&lt;/code&gt; inference. This solution is only supported on NVIDIA devices.&lt;/li&gt; &#xA; &lt;li&gt;All GPU references above refer to A100 or H100 GPUs. For other devices, you need to calculate the required GPU/CPU memory accordingly.&lt;/li&gt; &#xA; &lt;li&gt;During SFT (Supervised Fine-Tuning), this codebase freezes the &lt;code&gt;Vision Encoder&lt;/code&gt;, uses a batch size of 1, and trains on &lt;code&gt;8 * A100&lt;/code&gt; GPUs. The total input tokens (including images, which account for &lt;code&gt;1600&lt;/code&gt; tokens) add up to 2048 tokens. This codebase cannot conduct SFT fine-tuning without freezing the &lt;code&gt;Vision Encoder&lt;/code&gt;.&lt;br&gt; For LoRA fine-tuning, &lt;code&gt;Vision Encoder&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; frozen; the batch size is 1, using &lt;code&gt;1 * A100&lt;/code&gt; GPU. The total input tokens (including images, &lt;code&gt;1600&lt;/code&gt; tokens) also amount to 2048 tokens. In the above setup, SFT fine-tuning requires at least &lt;code&gt;60GB&lt;/code&gt; of GPU memory per GPU (with 8 GPUs), while LoRA fine-tuning requires at least &lt;code&gt;70GB&lt;/code&gt; of GPU memory on a single GPU (cannot be split).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Ascend devices&lt;/code&gt; have not been tested for SFT fine-tuning. We have only tested them on the &lt;code&gt;Atlas800&lt;/code&gt; training server cluster. You need to modify the inference code accordingly based on the loading mechanism described in the &lt;code&gt;Ascend device&lt;/code&gt; download link.&lt;/li&gt; &#xA; &lt;li&gt;Currently, we do &lt;strong&gt;not&lt;/strong&gt; support inference with the &lt;code&gt;vLLM&lt;/code&gt; framework. We will submit a PR as soon as possible to enable it.&lt;/li&gt; &#xA; &lt;li&gt;The online demo link does &lt;strong&gt;not&lt;/strong&gt; support controlling computers; it only allows you to view the model&#39;s inference results. We recommend deploying the model locally.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Inputs and Outputs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;cogagent-9b-20241220&lt;/code&gt; is an agent-type execution model rather than a conversational model. It does not support continuous dialogue, but it &lt;strong&gt;does&lt;/strong&gt; support a continuous execution history. (In other words, each time a new conversation session needs to be started, and the past history should be provided to the model.) The workflow of CogAgent is illustrated as following:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/assets/cogagent_workflow_en.png&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;To achieve optimal GUI Agent performance, we have adopted a strict input-output format.&lt;/strong&gt; Below is how users should format their inputs and feed them to the model, and how to interpret the model’s responses.&lt;/p&gt; &#xA;&lt;h3&gt;User Input&lt;/h3&gt; &#xA;&lt;p&gt;You can refer to &lt;a href=&#34;https://github.com/THUDM/CogAgent/raw/e3ca6f4dc94118d3dfb749f195cbb800ee4543ce/app/client.py#L115&#34;&gt;app/client.py#L115&lt;/a&gt; for constructing user input prompts. A minimal example of user input concatenation code is shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;current_platform = identify_os() # &#34;Mac&#34; or &#34;WIN&#34; or &#34;Mobile&#34;. Pay attention to case sensitivity.&#xA;platform_str = f&#34;(Platform: {current_platform})\n&#34;&#xA;format_str = &#34;(Answer in Action-Operation-Sensitive format.)\n&#34; # You can use other format to replace &#34;Action-Operation-Sensitive&#34;&#xA;&#xA;history_str = &#34;\nHistory steps: &#34;&#xA;for index, (grounded_op_func, action) in enumerate(zip(history_grounded_op_funcs, history_actions)):&#xA;   history_str += f&#34;\n{index}. {grounded_op_func}\t{action}&#34; # start from 0. &#xA;&#xA;query = f&#34;Task: {task}{history_str}\n{platform_str}{format_str}&#34; # Be careful about the \n&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The concatenated Python string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#34;Task: Search for doors, click doors on sale and filter by brands \&#34;Mastercraft\&#34;.\nHistory steps: \n0. CLICK(box=[[352,102,786,139]], element_info=&#39;Search&#39;)\tLeft click on the search box located in the middle top of the screen next to the Menards logo.\n1. TYPE(box=[[352,102,786,139]], text=&#39;doors&#39;, element_info=&#39;Search&#39;)\tIn the search input box at the top, type &#39;doors&#39;.\n2. CLICK(box=[[787,102,809,139]], element_info=&#39;SEARCH&#39;)\tLeft click on the magnifying glass icon next to the search bar to perform the search.\n3. SCROLL_DOWN(box=[[0,209,998,952]], step_count=5, element_info=&#39;[None]&#39;)\tScroll down the page to see the available doors.\n4. CLICK(box=[[280,708,710,809]], element_info=&#39;Doors on Sale&#39;)\tClick the \&#34;Doors On Sale\&#34; button in the middle of the page to view the doors that are currently on sale.\n(Platform: WIN)\n(Answer in Action-Operation format.)\n&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Printed prompt:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Task: Search for doors, click doors on sale and filter by brands &#34;Mastercraft&#34;.&lt;/p&gt; &#xA; &lt;p&gt;History steps:&lt;/p&gt; &#xA; &lt;ol start=&#34;0&#34;&gt; &#xA;  &lt;li&gt;CLICK(box=[[352,102,786,139]], element_info=&#39;Search&#39;) Left click on the search box located in the middle top of the screen next to the Menards logo.&lt;/li&gt; &#xA;  &lt;li&gt;TYPE(box=[[352,102,786,139]], text=&#39;doors&#39;, element_info=&#39;Search&#39;) In the search input box at the top, type &#39; doors&#39;.&lt;/li&gt; &#xA;  &lt;li&gt;CLICK(box=[[787,102,809,139]], element_info=&#39;SEARCH&#39;) Left click on the magnifying glass icon next to the search bar to perform the search.&lt;/li&gt; &#xA;  &lt;li&gt;SCROLL_DOWN(box=[[0,209,998,952]], step_count=5, element_info=&#39;[None]&#39;) Scroll down the page to see the available doors.&lt;/li&gt; &#xA;  &lt;li&gt;CLICK(box=[[280,708,710,809]], element_info=&#39;Doors on Sale&#39;) Click the &#34;Doors On Sale&#34; button in the middle of the page to view the doors that are currently on sale.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;(Platform: WIN)&lt;/p&gt; &#xA; &lt;p&gt;(Answer in Action-Operation format.)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you want to understand the meaning and representation of each field in detail, please continue reading or refer to the &lt;a href=&#34;https://zhipu-ai.feishu.cn/wiki/D9FTwQ78fitS3CkZHUjcKEWTned&#34;&gt;Practical Documentation (in Chinese), &#34;Prompt Concatenation&#34; section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;task&lt;/code&gt; field&lt;/strong&gt;&lt;br&gt; The user’s task description, in text format similar to a prompt. This input instructs the &lt;code&gt;cogagent-9b-20241220&lt;/code&gt; model on how to carry out the user’s request. Keep it concise and clear.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;platform&lt;/code&gt; field&lt;/strong&gt;&lt;br&gt; &lt;code&gt;cogagent-9b-20241220&lt;/code&gt; supports agent operations on multiple platforms with graphical interfaces. We currently support three systems:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Windows 10, 11: Use the &lt;code&gt;WIN&lt;/code&gt; field.&lt;/li&gt; &#xA;   &lt;li&gt;macOS 14, 15: Use the &lt;code&gt;Mac&lt;/code&gt; field.&lt;/li&gt; &#xA;   &lt;li&gt;Android 13, 14, 15 (and other Android UI variants with similar GUI operations): Use the &lt;code&gt;Mobile&lt;/code&gt; field.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;If your system is not among these, the effectiveness may be suboptimal. You can try using &lt;code&gt;Mobile&lt;/code&gt; for mobile devices, &lt;code&gt;WIN&lt;/code&gt; for Windows, or &lt;code&gt;Mac&lt;/code&gt; for Mac.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;format&lt;/code&gt; field&lt;/strong&gt;&lt;br&gt; The format in which the user wants &lt;code&gt;cogagent-9b-20241220&lt;/code&gt; to return data. We provide several options:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Answer in Action-Operation-Sensitive format.&lt;/code&gt;: The default demo return type in this repo. Returns the model’s actions, corresponding operations, and the sensitivity level.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Answer in Status-Plan-Action-Operation format.&lt;/code&gt;: Returns the model’s status, plan, and corresponding operations.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Answer in Status-Action-Operation-Sensitive format.&lt;/code&gt;: Returns the model’s status, actions, corresponding operations, and sensitivity.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Answer in Status-Action-Operation format.&lt;/code&gt;: Returns the model’s status and actions.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Answer in Action-Operation format.&lt;/code&gt;: Returns the model’s actions and corresponding operations.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;history&lt;/code&gt; field&lt;/strong&gt;&lt;br&gt; This should be concatenated in the following order:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;query = f&#39;{task}{history}{platform}{format}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;Continue&lt;/code&gt; field&lt;/strong&gt;&lt;br&gt; CogAgent allows users to let the model &lt;code&gt;continue answering&lt;/code&gt;. This requires users to append the &lt;code&gt;[Continue]\n&lt;/code&gt; field after &lt;code&gt;{task}&lt;/code&gt;. In such cases, the concatenation sequence and result should be as follows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;query = f&#39;{task}[Continue]\n{history}{platform}{format}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Model Output&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sensitive operations&lt;/strong&gt;: Includes &lt;code&gt;&amp;lt;&amp;lt;敏感操作&amp;gt;&amp;gt;&lt;/code&gt; (“sensitive operation”) and &lt;code&gt;&amp;lt;&amp;lt;一般操作&amp;gt;&amp;gt;&lt;/code&gt; (“general operation”). These are only returned if you request the &lt;code&gt;Sensitive&lt;/code&gt; format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Plan&lt;/code&gt;, &lt;code&gt;Status&lt;/code&gt;, &lt;code&gt;Action&lt;/code&gt; fields&lt;/strong&gt;: Used to describe the model’s behavior and operations. Only returned if you request the corresponding fields. For example, if the format includes &lt;code&gt;Action&lt;/code&gt;, then the model returns the &lt;code&gt;Action&lt;/code&gt; field.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;General answer section&lt;/strong&gt;: A summary that appears prior to the formatted answer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Grounded Operation&lt;/code&gt; field&lt;/strong&gt;:&lt;br&gt; Describes the model’s specific operations, including the location of the operation, the operation type, and the action details. The &lt;code&gt;box&lt;/code&gt; attribute indicates the coordinate region for execution, &lt;code&gt;element_type&lt;/code&gt; indicates the element type, and &lt;code&gt;element_info&lt;/code&gt; describes the element. These details are wrapped within a “操作指令” (operation command). For the definition of the action space, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/Action_space.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;An Example&lt;/h3&gt; &#xA;&lt;p&gt;Suppose the user wants to mark all emails as read. The user is on a Mac, and the user wants the model to return in &lt;code&gt;Action-Operation-Sensitive&lt;/code&gt; format. The properly &lt;strong&gt;concatenated prompt&lt;/strong&gt; should be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Task: Please mark all my emails as read&#xA;History steps:&#xA;(Platform: Mac)&#xA;(Answer in Action-Operation-Sensitive format.)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: even if there are no historical actions, &#34;History steps:&#34; still needs to be appended in the prompt. Below are * &lt;em&gt;sample outputs&lt;/em&gt;* for different format requirements:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Answer in Action-Operation-Sensitive format&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Action: Click the &#39;Mark all as read&#39; button in the top toolbar of the page to mark all emails as read.&#xA;Grounded Operation: CLICK(box=[[219,186,311,207]], element_type=&#39;Clickable text&#39;, element_info=&#39;Mark all emails as read&#39;)&#xA;&amp;lt;&amp;lt;一般操作&amp;gt;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Answer in Status-Plan-Action-Operation format&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Status: Currently in the email interface [[0, 2, 998, 905]], with the email categories on the left [[1, 216, 144, 570]], and the inbox in the center [[144, 216, 998, 903]]. The &#34;Mark all as read&#34; button has been clicked [[223, 178, 311, 210]].&#xA;Plan: Future tasks: 1. Click the &#39;Mark all as read&#39; button; 2. Task complete.&#xA;Action: Click the &#34;Mark all as read&#34; button at the top center of the inbox page to mark all emails as read.&#xA;Grounded Operation: CLICK(box=[[219,186,311,207]], element_type=&#39;Clickable text&#39;, element_info=&#39;Mark all emails as read&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Answer in Status-Action-Operation-Sensitive format&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Status: Currently in the email interface [[0, 2, 998, 905]], with the email categories on the left [[1, 216, 144, 570]], and the inbox in the center [[144, 216, 998, 903]]. The &#34;Mark all as read&#34; button has been clicked [[223, 178, 311, 210]].&#xA;Action: Click the &#34;Mark all as read&#34; button at the top center of the inbox page to mark all emails as read.&#xA;Grounded Operation: CLICK(box=[[219,186,311,207]], element_type=&#39;Clickable text&#39;, element_info=&#39;Mark all emails as read&#39;)&#xA;&amp;lt;&amp;lt;一般操作&amp;gt;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Answer in Status-Action-Operation format&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Status: Currently in the email interface [[0, 2, 998, 905]], with the email categories on the left [[1, 216, 144, 570]], and the inbox in the center [[144, 216, 998, 903]]. The &#34;Mark all as read&#34; button has been clicked [[223, 178, 311, 210]].&#xA;Action: Click the &#34;Mark all as read&#34; button at the top center of the inbox page to mark all emails as read.&#xA;Grounded Operation: CLICK(box=[[219,186,311,207]], element_type=&#39;Clickable text&#39;, element_info=&#39;Mark all emails as read&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Answer in Action-Operation format&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Action: Right-click the first email in the left email list to open the action menu.&#xA;Grounded Operation: RIGHT_CLICK(box=[[154,275,343,341]], element_info=&#39;[AXCell]&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Notes&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;This model is &lt;strong&gt;not&lt;/strong&gt; a conversational model and does &lt;strong&gt;not&lt;/strong&gt; support continuous dialogue. Please send specific commands and reference our recommended method for concatenating the history.&lt;/li&gt; &#xA; &lt;li&gt;The model &lt;strong&gt;requires&lt;/strong&gt; images as input; pure text conversation cannot achieve GUI Agent tasks.&lt;/li&gt; &#xA; &lt;li&gt;The model’s output adheres to a strict format. Please parse it strictly according to our requirements. The output is in &lt;strong&gt;string&lt;/strong&gt; format; JSON output is &lt;strong&gt;not&lt;/strong&gt; supported.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Running the Model&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Setup&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you have installed &lt;strong&gt;Python 3.10.16&lt;/strong&gt; or above, and then install the following dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run local inference based on &lt;code&gt;transformers&lt;/code&gt;, you can run the command below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference/cli_demo.py --model_dir THUDM/cogagent-9b-20241220 --platform &#34;Mac&#34; --max_length 4096 --top_k 1 --output_image_path ./results --format_key status_action_op_sensitive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is a command-line interactive code. You will need to provide the path to your images. If the model returns results containing bounding boxes, it will output an image with those bounding boxes, indicating the region where the operation should be executed. The image is saved to &lt;code&gt;output_image_path&lt;/code&gt;, with the file name &lt;code&gt;{your_input_image_name}_{round}.png&lt;/code&gt;. The &lt;code&gt;format_key&lt;/code&gt; indicates in which format you want the model to respond. The &lt;code&gt;platform&lt;/code&gt; field specifies which platform you are using (e.g., &lt;code&gt;Mac&lt;/code&gt;). Therefore, all uploaded screenshots must be from macOS if &lt;code&gt;platform&lt;/code&gt; is set to &lt;code&gt;Mac&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run an online web demo, which supports continuous image uploads for interactive inference, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference/web_demo.py --host 0.0.0.0 --port 7860 --model_dir THUDM/cogagent-9b-20241220 --format_key status_action_op_sensitive --platform &#34;Mac&#34; --output_dir ./results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This code provides the same experience as the &lt;code&gt;HuggingFace Space&lt;/code&gt; online demo. The model will return the corresponding bounding boxes and execution categories.&lt;/p&gt; &#xA;&lt;h3&gt;Running an Agent APP Example&lt;/h3&gt; &#xA;&lt;p&gt;We have prepared a basic demo app for developers to illustrate the GUI capabilities of &lt;code&gt;cogagent-9b-20241220&lt;/code&gt;. The demo shows how to deploy the model on a GPU-equipped server and run the &lt;code&gt;cogagent-9b-20241220&lt;/code&gt; model locally to perform automated GUI operations.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We cannot guarantee the safety of AI behavior; please exercise caution when using it.&lt;br&gt; This example is only for academic reference. We assume no legal responsibility for any issues resulting from this example.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you are interested in this APP, feel free to check out the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/app/README.md&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuning the Model&lt;/h3&gt; &#xA;&lt;p&gt;If you are interested in fine-tuning the &lt;code&gt;cogagent-9b-20241220&lt;/code&gt; model, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/finetune/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Previous Work&lt;/h2&gt; &#xA;&lt;p&gt;In November 2023, we released the first generation of CogAgent. You can find related code and model weights in the &lt;a href=&#34;https://github.com/THUDM/CogVLM&#34;&gt;CogVLM &amp;amp; CogAgent Official Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/assets/cogagent_function.jpg&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;h2&gt; CogVLM &lt;/h2&gt; &lt;p&gt; 📖 Paper: &lt;a href=&#34;https://arxiv.org/abs/2311.03079&#34;&gt;CogVLM: Visual Expert for Pretrained Language Models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;b&gt;CogVLM&lt;/b&gt; is a powerful open-source Vision-Language Model (VLM). CogVLM-17B has 10B visual parameters and 7B language parameters, supporting image understanding at a resolution of 490x490, as well as multi-round dialogue.&lt;/p&gt; &lt;p&gt;&lt;b&gt;CogVLM-17B&lt;/b&gt; achieves state-of-the-art performance on 10 classic multimodal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA, and TDIUC.&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;h2&gt; CogAgent &lt;/h2&gt; &lt;p&gt; 📖 Paper: &lt;a href=&#34;https://arxiv.org/abs/2312.08914&#34;&gt;CogAgent: A Visual Language Model for GUI Agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;b&gt;CogAgent&lt;/b&gt; is an open-source vision-language model improved upon CogVLM. CogAgent-18B has 11B visual parameters and 7B language parameters. &lt;b&gt;It supports image understanding at a resolution of 1120x1120. Building on CogVLM’s capabilities, CogAgent further incorporates a GUI image agent ability.&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;b&gt;CogAgent-18B&lt;/b&gt; delivers state-of-the-art general performance on 9 classic vision-language benchmarks, including VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. It also significantly outperforms existing models on GUI operation datasets such as AITW and Mind2Web.&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/LICENSE&#34;&gt;Apache2.0 LICENSE&lt;/a&gt; applies to the use of the code in this GitHub repository.&lt;/li&gt; &#xA; &lt;li&gt;For the model weights, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogAgent/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please consider citing the following papers&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{hong2023cogagent,&#xA;      title={CogAgent: A Visual Language Model for GUI Agents}, &#xA;      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},&#xA;      year={2023},&#xA;      eprint={2312.08914},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Research and Development Team &amp;amp; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;R&amp;amp;D Institutions&lt;/strong&gt;: Tsinghua University, Zhipu AI&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Team members&lt;/strong&gt;: Wenyi Hong, Junhui Ji, Lihang Pan, Yuanchang Yue, Changyu Pang, Siyan Xue, Guo Wang, Weihan Wang, Jiazheng Xu, Shen Yang, Xiaotao Gu, Yuxiao Dong, Jie Tang&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Acknowledgement&lt;/strong&gt;: We would like to thank the Zhipu AI data team for their strong support, including Xiaohan Zhang, Zhao Xue, Lu Chen, Jingjie Du, Siyu Wang, Ying Zhang, and all annotators. They worked hard to collect and annotate the training and testing data of the CogAgent model. We also thank Yuxuan Zhang, Xiaowei Hu, and Hao Chen from the Zhipu AI open source team for their engineering efforts in open sourcing the model.&lt;/p&gt;</summary>
  </entry>
</feed>