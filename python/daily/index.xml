<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-18T01:37:36Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kijai/ComfyUI-DiffusersStableCascade</title>
    <updated>2024-02-18T01:37:36Z</updated>
    <id>tag:github.com,2024-02-18:/kijai/ComfyUI-DiffusersStableCascade</id>
    <link href="https://github.com/kijai/ComfyUI-DiffusersStableCascade" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple inference with StableCascade using diffusers in ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI StableCascade using diffusers&lt;/h1&gt; &#xA;&lt;h1&gt;Update: This repo is already deprecated as the native support is in ComfyUI, more info: &lt;a href=&#34;https://gist.github.com/comfyanonymous/0f09119a342d0dd825bb2d99d19b781c&#34;&gt;https://gist.github.com/comfyanonymous/0f09119a342d0dd825bb2d99d19b781c&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Simple quick wrapper for &lt;a href=&#34;https://huggingface.co/stabilityai/stable-cascade&#34;&gt;https://huggingface.co/stabilityai/stable-cascade&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Comfy is going to implement this properly soon, this repo is just for quick testing for the impatient!&lt;/p&gt; &#xA;&lt;p&gt;Currently requires this diffusers branch: &lt;code&gt;pip install git+https://github.com/kashif/diffusers.git@wuerstchen-v3&lt;/code&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rushter/MLAlgorithms</title>
    <updated>2024-02-18T01:37:36Z</updated>
    <id>tag:github.com,2024-02-18:/rushter/MLAlgorithms</id>
    <link href="https://github.com/rushter/MLAlgorithms" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Minimal and clean examples of machine learning algorithms implementations&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine learning algorithms&lt;/h1&gt; &#xA;&lt;p&gt;A collection of minimal and clean implementations of machine learning algorithms.&lt;/p&gt; &#xA;&lt;h3&gt;Why?&lt;/h3&gt; &#xA;&lt;p&gt;This project is targeting people who want to learn internals of ml algorithms or implement them from scratch.&lt;br&gt; The code is much easier to follow than the optimized libraries and easier to play with.&lt;br&gt; All algorithms are implemented in Python, using numpy, scipy and autograd.&lt;/p&gt; &#xA;&lt;h3&gt;Implemented:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/neuralnet&#34;&gt;Deep learning (MLP, CNN, RNN, LSTM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/linear_models.py&#34;&gt;Linear regression, logistic regression&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/ensemble/random_forest.py&#34;&gt;Random Forests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/svm&#34;&gt;Support vector machine (SVM) with kernels (Linear, Poly, RBF)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/kmeans.py&#34;&gt;K-Means&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/gaussian_mixture.py&#34;&gt;Gaussian Mixture Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/knn.py&#34;&gt;K-nearest neighbors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/naive_bayes.py&#34;&gt;Naive bayes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/pca.py&#34;&gt;Principal component analysis (PCA)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/fm.py&#34;&gt;Factorization machines&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/rbm.py&#34;&gt;Restricted Boltzmann machine (RBM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/tsne.py&#34;&gt;t-Distributed Stochastic Neighbor Embedding (t-SNE)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/ensemble/gbm.py&#34;&gt;Gradient Boosting trees (also known as GBDT, GBRT, GBM, XGBoost)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rushter/MLAlgorithms/master/mla/rl&#34;&gt;Reinforcement learning (Deep Q learning)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;        git clone https://github.com/rushter/MLAlgorithms&#xA;        cd MLAlgorithms&#xA;        pip install scipy numpy&#xA;        python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to run examples without installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;        cd MLAlgorithms&#xA;        python -m examples.linear_models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to run examples within Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;        cd MLAlgorithms&#xA;        docker build -t mlalgorithms .&#xA;        docker run --rm -it mlalgorithms bash&#xA;        python -m examples.linear_models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;Your contributions are always welcome!&lt;br&gt; Feel free to improve existing code, documentation or implement new algorithm.&lt;br&gt; Please open an issue to propose your changes if they are big enough.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/jepa</title>
    <updated>2024-02-18T01:37:36Z</updated>
    <id>tag:github.com,2024-02-18:/facebookresearch/jepa</id>
    <link href="https://github.com/facebookresearch/jepa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch code and models for V-JEPA self-supervised learning from video.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;V-JEPA: Video Joint Embedding Predictive Architecture&lt;/h1&gt; &#xA;&lt;p&gt;Official PyTorch codebase for the &lt;em&gt;video joint-embedding predictive architecture&lt;/em&gt;, V-JEPA, a method for self-supervised learning of visual representations from video.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran*, Nicolas Ballas*&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/&#34;&gt;[Blog]&lt;/a&gt; &lt;a href=&#34;https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/&#34;&gt;[Paper]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;V-JEPA models are trained by passively watching video pixels from the VideoMix2M dataset, and produce versatile visual representations that perform well on downstream video and image tasks, without adaption of the model’s parameters; e.g., using a frozen backbone and only a light-weight task-specific attentive probe.&lt;/p&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;V-JEPA pretraining is based solely on an unsupervised feature prediction objective, and does not utilize pretrained image encoders, text, negative examples, human annotations, or pixel-level reconstruction.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/facebookresearch/jepa/assets/7530871/72df7ef0-2ef5-48bb-be46-27963db91f3d&#34; width=&#34;40%&#34;&gt;       &#xA;&lt;img src=&#34;https://github.com/facebookresearch/jepa/assets/7530871/f26b2e96-0227-44e2-b058-37e7bf1e10db&#34; width=&#34;40%&#34;&gt; &#xA;&lt;h2&gt;Visualizations&lt;/h2&gt; &#xA;&lt;p&gt;As opposed to generative methods that have a pixel decoder, V-JEPA has a predictor that makes predictions in latent space. We train a conditional diffusion model to decode the V-JEPA feature-space predictions to interpretable pixels; the pretrained V-JEPA encoder and predictor networks are kept frozen in this process. The decoder is only fed the representations predicted for the missing regions of the video, and does not have access to the unmasked regions of the video.&lt;/p&gt; &#xA;&lt;p&gt;The V-JEPA feature predictions are indeed grounded, and exhibit spatio-temporal consistency with the unmasked regions of the video.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/facebookresearch/jepa/assets/7530871/8bb5e338-0db8-4532-ba6f-fc62729acc26&#34; width=&#34;90%&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://github.com/facebookresearch/jepa/assets/7530871/93e15a3b-9119-4149-ac88-4e6288f2043d&#34; width=&#34;22%&#34;&gt; &#xA;&lt;img src=&#34;https://github.com/facebookresearch/jepa/assets/7530871/7efd2ee2-2aa0-4065-a4a6-12f1d9d0499c&#34; width=&#34;22%&#34;&gt; &#xA;&lt;img src=&#34;https://github.com/facebookresearch/jepa/assets/7530871/06626018-cd5a-4536-9d0e-de58506ce5ed&#34; width=&#34;22%&#34;&gt; &#xA;&lt;img src=&#34;https://github.com/facebookresearch/jepa/assets/7530871/766da53a-e6b8-4f94-82c8-9a53b4764358&#34; width=&#34;22%&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;MODEL ZOO&lt;/h2&gt; &#xA;&lt;h4&gt;Pretrained models&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;patch size&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;iterations&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;batch size&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;data&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L&lt;/td&gt; &#xA;   &lt;td&gt;2x16x16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;90K&lt;/td&gt; &#xA;   &lt;td&gt;3072&lt;/td&gt; &#xA;   &lt;td&gt;VideoMix2M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vitl16/vitl16.pth.tar&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/pretrain/vitl16.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H&lt;/td&gt; &#xA;   &lt;td&gt;2x16x16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;90K&lt;/td&gt; &#xA;   &lt;td&gt;3072&lt;/td&gt; &#xA;   &lt;td&gt;VideoMix2M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16/vith16.pth.tar&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/pretrain/vith16.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H&lt;/td&gt; &#xA;   &lt;td&gt;2x16x16&lt;/td&gt; &#xA;   &lt;td&gt;384x384&lt;/td&gt; &#xA;   &lt;td&gt;90K&lt;/td&gt; &#xA;   &lt;td&gt;2400&lt;/td&gt; &#xA;   &lt;td&gt;VideoMix2M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16-384/vith16-384.pth.tar&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/pretrain/vith16_384.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;K400 Attentive probes&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;accuracy (16x8x3)&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;80.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vitl16/k400-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_384_k400_16x8x3.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;82.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16/k400-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_k400_16x8x3.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;81.9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16-384/k400-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vitl16_k400_16x8x3.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;SSv2 Attentive probes&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;accuracy (16x2x3)&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;69.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vitl16/ssv2-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vitl16_ssv2_16x2x3.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;71.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16/ssv2-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_ssv2_16x2x3.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;384x384&lt;/td&gt; &#xA;   &lt;td&gt;72.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16-384/ssv2-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_384_ssv2_16x2x3.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;ImageNet1K Attentive probes&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;accuracy&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;74.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vitl16/in1k-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vitl16_in1k.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;75.9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16/in1k-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_in1k.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;77.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16-384/in1k-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_384_in1k.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Places205 Attentive probes&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;accuracy&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;60.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vitl16/places-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vitl16_places.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;61.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16/places-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_places.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;62.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16-384/places-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_384_places.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;iNat21 Attentive probes&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;resolution&lt;/th&gt; &#xA;   &lt;th colspan=&#34;1&#34;&gt;accuracy&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;67.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vitl16/inat-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vitl16_inat.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;67.9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16/inat-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_inat.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-H/16&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/jepa/vith16-384/inat-probe.pth.tar&#34;&gt;attentive probe checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/jepa/raw/master/configs/evals/vith16_384_inat.yaml&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Code Structure&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Config files:&lt;/strong&gt; All experiment parameters are specified in config files (as opposed to command-line-arguments). See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/configs/&#34;&gt;configs/&lt;/a&gt; directory for example config files. Note, before launching an experiment, you must update the paths in the config file to point to your own directories: indicating where to save the logs and checkpoints, and where to find the training data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── app                       # the only place where training loops are allowed&#xA;│   ├── vjepa                 #   Video JEPA pre-training&#xA;│   ├── main_distributed.py   #   entrypoint for launching app on slurm cluster&#xA;│   └── main.py               #   entrypoint for launching app locally on your machine for debugging&#xA;├── evals                     # the only place where evaluation of &#39;apps&#39; are allowed&#xA;│   ├── image_classification  #   training an attentive probe for image classification with frozen backbone&#xA;│   ├── video_classification  #   training an attentive probe for video classification with frozen backbone&#xA;│   ├── main_distributed.py   #   entrypoint for launching distributed evaluations on slurm cluster&#xA;│   └── main.py               #   entrypoint for launching evaluations locally on your machine for debugging&#xA;├── src                       # the package&#xA;│   ├── datasets              #   datasets, data loaders, ...&#xA;│   ├── models                #   model definitions&#xA;│   ├── masks                 #   mask collators, masking utilities, ...&#xA;│   └── utils                 #   shared utilities&#xA;└── configs                   # the only place where config files are allowed (specify experiment params for app/eval runs)&#xA;    ├── evals                 #   configs for launching vjepa frozen evaluations&#xA;    └── pretrain              #   configs for launching vjepa pretraining&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Video Datasets&lt;/h3&gt; &#xA;&lt;p&gt;V-JEPA pretraining and evaluations works with many standard video formats. To make a video dataset compatible with the V-JEPA codebase, you simply need to create a &lt;code&gt;.csv&lt;/code&gt; file with the following format, and then specify the path to this csv file in your config.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/absolute_file_path.[mp4, webvid, etc.] $integer_class_label&#xA;/absolute_file_path.[mp4, webvid, etc.] $integer_class_label&#xA;/absolute_file_path.[mp4, webvid, etc.] $integer_class_label&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since V-JEPA is entirely unsupervised, the pretraining code will disregard the &lt;code&gt;$integer_class_label&lt;/code&gt; in the csv file. Thus, feel free to put a random value in this column. However, if you wish to run a supervised video classification evaluation on your video dataset, you must replace &lt;code&gt;$integer_class_label&lt;/code&gt; with the ground truth label for each video.&lt;/p&gt; &#xA;&lt;h3&gt;Image Datasets&lt;/h3&gt; &#xA;&lt;p&gt;We use the standard PyTorch &lt;code&gt;ImageFolder&lt;/code&gt; class in our image classification evals. Thus, to setup an image dataset for the image classification evaluation, first create a directory to store your image datasets &lt;code&gt;$your_directory_containing_image_datasets&lt;/code&gt;. Next, download your image datasets into this directory in a format compatible with &lt;a href=&#34;https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html&#34;&gt;PyTorch ImageFolder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, suppose we have a directory called &lt;code&gt;my_image_datasets&lt;/code&gt;. We would then download our image datasets into this directory so that we end up with the following file tree&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;└── /my_image_datasets/                # where we store image datasets&#xA;    ├── places205/121517/pytorch/      #   Places205&#xA;    │   └── [...]&#xA;    ├── iNaturalist-2021/110421/       #   iNaturalist21&#xA;    │   └── [...]&#xA;    ├── [...]                          #   Other Image Datasets&#xA;    │   └── [...]&#xA;    └── imagenet_full_size/061417/     #   ImageNet1k&#xA;        └── train&#xA;        │   ├── $class_1&#xA;        │   │    ├── xxx.[png, jpeg, etc.]&#xA;        │   │    ├── [...]&#xA;        │   │    └── xxz.[png, jpeg, etc.]&#xA;        │   ├── [...]&#xA;        │   └── $class_n&#xA;        │       ├── abc.[png, jpeg, etc.]&#xA;        │       ├── [...]&#xA;        │       └── abz.[png, jpeg, etc.]&#xA;        └── val&#xA;            ├── $class_1&#xA;            │    ├── xxx.[png, jpeg, etc.]&#xA;            │    ├── [...]&#xA;            │    └── xxz.[png, jpeg, etc.]&#xA;            ├── [...]&#xA;            └── $class_n&#xA;                ├── abc.[png, jpeg, etc.]&#xA;                ├── [...]&#xA;                └── abz.[png, jpeg, etc.]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Launching V-JEPA pretraining&lt;/h2&gt; &#xA;&lt;h3&gt;Local training&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to debug your code or setup before launching a distributed training run, we provide the functionality to do so by running the pretraining script locally on a multi-GPU (or single-GPU) machine, however, reproducing our results requires launching distributed training.&lt;/p&gt; &#xA;&lt;p&gt;The single machine implementation starts from the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/appmain.py&#34;&gt;app/main.py&lt;/a&gt;, which parses the experiment config file and runs the pretraining locally on a multi-GPU (or single-GPU) machine. For example, to run V-JEPA pretraining on GPUs &#34;0&#34;,&#34;1&#34;, and &#34;2&#34; on a local machine using the config &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/configs/pretrain/vitl16.yaml&#34;&gt;configs/pretrain/vitl16.yaml&lt;/a&gt;, type the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m app.main \&#xA;  --fname configs/pretrain/vitl16.yaml \&#xA;  --devices cuda:0 cuda:1 cuda:2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Distributed training&lt;/h3&gt; &#xA;&lt;p&gt;To launch a distributed training run, the implementation starts from &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/app/main_distributed.py&#34;&gt;app/main_distributed.py&lt;/a&gt;, which, in addition to parsing the config file, also allows for specifying details about distributed training. For distributed training, we use the popular open-source &lt;a href=&#34;https://github.com/facebookincubator/submitit&#34;&gt;submitit&lt;/a&gt; tool and provide examples for a SLURM cluster.&lt;/p&gt; &#xA;&lt;p&gt;For example, to launch a distributed pre-training experiment using the config &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/configs/pretrain/vitl16.yaml&#34;&gt;configs/pretrain/vitl16.yaml&lt;/a&gt;, type the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m app.main_distributed \&#xA;  --fname configs/pretrain/vitl16.yaml \&#xA;  --folder $path_to_save_stderr_and_stdout \&#xA;  --partition $slurm_partition&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Launching Evaluations&lt;/h2&gt; &#xA;&lt;h3&gt;Local training&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to debug your eval code or setup before launching a distributed training run, we provide the functionality to do so by running the pretraining script locally on a multi-GPU (or single-GPU) machine, however, reproducing the full eval would require launching distributed training. The single machine implementation starts from the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/eval/main.py&#34;&gt;eval/main.py&lt;/a&gt;, which parses the experiment config file and runs the eval locally on a multi-GPU (or single-GPU) machine.&lt;/p&gt; &#xA;&lt;p&gt;For example, to run ImageNet image classification on GPUs &#34;0&#34;,&#34;1&#34;, and &#34;2&#34; on a local machine using the config &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/configs/eval/vitl16_in1k.yaml&#34;&gt;configs/eval/vitl16_in1k.yaml&lt;/a&gt;, type the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m eval.main \&#xA;  --fname configs/eval/vitl16_in1k.yaml \&#xA;  --devices cuda:0 cuda:1 cuda:2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Distributed training&lt;/h3&gt; &#xA;&lt;p&gt;To launch a distributed evaluation run, the implementation starts from &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/eval/main_distributed.py&#34;&gt;eval/main_distributed.py&lt;/a&gt;, which, in addition to parsing the config file, also allows for specifying details about distributed training. For distributed training, we use the popular open-source &lt;a href=&#34;https://github.com/facebookincubator/submitit&#34;&gt;submitit&lt;/a&gt; tool and provide examples for a SLURM cluster.&lt;/p&gt; &#xA;&lt;p&gt;For example, to launch a distributed ImageNet image classification experiment using the config &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/configs/eval/vitl16_in1k.yaml&#34;&gt;configs/eval/vitl16_in1k.yaml&lt;/a&gt;, type the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m eval.main_distributed \&#xA;  --fname configs/eval/vitl16_in1k.yaml \&#xA;  --folder $path_to_save_stderr_and_stdout \&#xA;  --partition $slurm_partition&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly, to launch a distributed K400 video classificaiton experiment using the config &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/configs/eval/vitl16_k400.yaml&#34;&gt;configs/eval/vitl16_k400.yaml&lt;/a&gt;, type the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m eval.main_distributed \&#xA;  --fname configs/eval/vitl16_k400.yaml \&#xA;  --folder $path_to_save_stderr_and_stdout \&#xA;  --partition $slurm_partition&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;Create a new anaconda environment, activate it, and run the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/setup.py&#34;&gt;setup.py&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/jepa/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details about the license under which this code is made available.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and a citation&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{bardes2024revisiting,&#xA;  title={Revisiting Feature Prediction for Learning Visual Representations from Video},&#xA;  author={Bardes, Adrien and Garrido, Quentin and Ponce, Jean and Rabbat, Michael, and LeCun, Yann and Assran, Mahmoud and Ballas, Nicolas},&#xA;  journal={arXiv preprint},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>