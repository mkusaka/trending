<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-08T01:30:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/ParlAI</title>
    <updated>2022-08-08T01:30:56Z</updated>
    <id>tag:github.com,2022-08-08:/facebookresearch/ParlAI</id>
    <link href="https://github.com/facebookresearch/ParlAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A framework for training and evaluating AI models on a variety of openly available dialogue datasets.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;70%&#34; src=&#34;docs/source/\_static/img/parlai.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;CircleCI&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/parlai/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/parlai?color=blue&amp;amp;label=release&#34; alt=&#34;CircleCI&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://circleci.com/gh/facebookresearch/ParlAI/tree/main&#34;&gt; &lt;img src=&#34;https://img.shields.io/circleci/build/github/facebookresearch/ParlAI/main&#34; alt=&#34;Coverage&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/facebookresearch/ParlAI&#34;&gt; &lt;img src=&#34;https://img.shields.io/codecov/c/github/facebookresearch/ParlAI&#34; alt=&#34;GitHub contributors&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://img.shields.io/github/contributors/facebookresearch/ParlAI&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/contributors/facebookresearch/ParlAI&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/parlai_parley&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/parlai_parley?label=Twitter&amp;amp;style=social&#34; alt=&#34;Twitter&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://parl.ai&#34;&gt;ParlAI&lt;/a&gt; (pronounced “par-lay”) is a python framework for sharing, training and testing dialogue models, from open-domain chitchat, to task-oriented dialogue, to visual question answering.&lt;/p&gt; &#xA;&lt;p&gt;Its goal is to provide researchers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;100+ popular datasets available all in one place, with the same API&lt;/strong&gt;, among them &lt;a href=&#34;https://arxiv.org/abs/1801.07243&#34;&gt;PersonaChat&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1710.03957&#34;&gt;DailyDialog&lt;/a&gt;, &lt;a href=&#34;https://openreview.net/forum?id=r1l73iRqKm&#34;&gt;Wizard of Wikipedia&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1811.00207&#34;&gt;Empathetic Dialogues&lt;/a&gt;, &lt;a href=&#34;https://rajpurkar.github.io/SQuAD-explorer/&#34;&gt;SQuAD&lt;/a&gt;, &lt;a href=&#34;http://www.msmarco.org/&#34;&gt;MS MARCO&lt;/a&gt;, &lt;a href=&#34;https://www.aclweb.org/anthology/D18-1241&#34;&gt;QuAC&lt;/a&gt;, &lt;a href=&#34;https://hotpotqa.github.io/&#34;&gt;HotpotQA&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1506.03340&#34;&gt;QACNN &amp;amp; QADailyMail&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1511.02301&#34;&gt;CBT&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1610.00956&#34;&gt;BookTest&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1605.07683&#34;&gt;bAbI Dialogue tasks&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1506.08909&#34;&gt;Ubuntu Dialogue&lt;/a&gt;, &lt;a href=&#34;http://opus.lingfil.uu.se/OpenSubtitles.php&#34;&gt;OpenSubtitles&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1811.00945&#34;&gt;Image Chat&lt;/a&gt;, &lt;a href=&#34;http://visualqa.org/&#34;&gt;VQA&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1611.08669&#34;&gt;VisDial&lt;/a&gt; and &lt;a href=&#34;http://cs.stanford.edu/people/jcjohns/clevr/&#34;&gt;CLEVR&lt;/a&gt;. See the complete list &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/raw/main/parlai/tasks/task_list.py&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;a wide set of &lt;a href=&#34;https://parl.ai/docs/agents_list.html&#34;&gt;&lt;strong&gt;reference models&lt;/strong&gt;&lt;/a&gt; -- from retrieval baselines to Transformers.&lt;/li&gt; &#xA; &lt;li&gt;a large &lt;a href=&#34;https://parl.ai/docs/zoo.html&#34;&gt;zoo of &lt;strong&gt;pretrained models&lt;/strong&gt;&lt;/a&gt; ready to use off-the-shelf&lt;/li&gt; &#xA; &lt;li&gt;seamless &lt;strong&gt;integration of &lt;a href=&#34;https://www.mturk.com/mturk/welcome&#34;&gt;Amazon Mechanical Turk&lt;/a&gt;&lt;/strong&gt; for data collection and human evaluation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;integration with &lt;a href=&#34;https://parl.ai/docs/tutorial_chat_service.html&#34;&gt;Facebook Messenger&lt;/a&gt;&lt;/strong&gt; to connect agents with humans in a chat interface&lt;/li&gt; &#xA; &lt;li&gt;a large range of &lt;strong&gt;helpers to create your own agents&lt;/strong&gt; and train on several tasks with &lt;strong&gt;multitasking&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;multimodality&lt;/strong&gt;, some tasks use text and images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ParlAI is described in the following paper: &lt;a href=&#34;https://arxiv.org/abs/1705.06476&#34;&gt;“ParlAI: A Dialog Research Software Platform&#34;, arXiv:1705.06476&lt;/a&gt; or see these &lt;a href=&#34;https://drive.google.com/file/d/1JfUW4AVrjSp8X8Fp0_rTTRoLxUfW0aUm/view?usp=sharing&#34;&gt;more up-to-date slides&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Follow us on &lt;a href=&#34;https://twitter.com/parlai_parley&#34;&gt;Twitter&lt;/a&gt; and check out our &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/releases&#34;&gt;Release notes&lt;/a&gt; to see the latest information about new features &amp;amp; updates, and the website &lt;a href=&#34;http://parl.ai&#34;&gt;http://parl.ai&lt;/a&gt; for further docs. For an archived list of updates, check out &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/raw/main/NEWS.md&#34;&gt;NEWS.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;90%&#34; src=&#34;https://raw.githubusercontent.com/facebookresearch/ParlAI/main/docs/source/_static/img/parlai_example.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Interactive Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;For those who want to start with ParlAI now, you can try our &lt;a href=&#34;https://colab.research.google.com/drive/1bRMvN0lGXaTF5fuTidgvlAl-Lb41F7AD#scrollTo=KtVz5dCUmFkN&#34;&gt;Colab Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installing ParlAI&lt;/h2&gt; &#xA;&lt;p&gt;ParlAI currently requires Python3.8+ and &lt;a href=&#34;https://pytorch.org&#34;&gt;Pytorch&lt;/a&gt; 1.6 or higher. Dependencies of the core modules are listed in &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/raw/main/requirements.txt&#34;&gt;&lt;code&gt;requirements.txt&lt;/code&gt;&lt;/a&gt;. Some models included (in &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/agents&#34;&gt;&lt;code&gt;parlai/agents&lt;/code&gt;&lt;/a&gt;) have additional requirements. We &lt;em&gt;strongly&lt;/em&gt; recommend you install ParlAI in a &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;venv&lt;/a&gt; or &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;conda&lt;/a&gt; environment.&lt;/p&gt; &#xA;&lt;p&gt;We do not support Windows at this time, but many users &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/issues/3989&#34;&gt;report success on Windows using Python 3.8&lt;/a&gt; and issues with Python 3.9. We are happy to accept patches that improve Windows support.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Standard Installation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to use ParlAI without modifications, you can install it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install parlai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Development Installation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Many users will want to modify some parts of ParlAI. To set up a development environment, run the following commands to clone the repository and install ParlAI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI&#xA;cd ~/ParlAI; python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All needed data will be downloaded to &lt;code&gt;~/ParlAI/data&lt;/code&gt;. If you need to clear out the space used by these files, you can safely delete these directories and any files needed will be downloaded again.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://parl.ai/docs/tutorial_quick.html&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://parl.ai/docs/tutorial_basic.html&#34;&gt;Basics: world, agents, teachers, action and observations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://parl.ai/docs/tutorial_task.html&#34;&gt;Creating a new dataset/task&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://parl.ai/docs/tasks.html&#34;&gt;List of available tasks/datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://parl.ai/docs/tutorial_torch_generator_agent.html&#34;&gt;Creating a seq2seq agent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://parl.ai/docs/agents_list.html&#34;&gt;List of available agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://parl.ai/docs/zoo.html&#34;&gt;Model zoo (list pretrained models)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://parl.ai/docs/tutorial_crowdsourcing.html&#34;&gt;Running crowdsourcing tasks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://parl.ai/docs/tutorial_chat_service.html&#34;&gt;Plug into Facebook Messenger&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;A large set of scripts can be found in &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/scripts&#34;&gt;&lt;code&gt;parlai/scripts&lt;/code&gt;&lt;/a&gt;. Here are a few of them. Note: If any of these examples fail, check the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/ParlAI/main/#installing-parlai&#34;&gt;installation section&lt;/a&gt; to see if you have missed something.&lt;/p&gt; &#xA;&lt;p&gt;Display 10 random examples from the SQuAD task&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;parlai display_data -t squad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Evaluate an IR baseline model on the validation set of the Personachat task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;parlai eval_model -m ir_baseline -t personachat -dt valid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train a single layer transformer on PersonaChat (requires pytorch and torchtext). Detail: embedding size 300, 4 attention heads, 2 epochs using batchsize 64, word vectors are initialized with fasttext and the other elements of the batch are used as negative during training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;parlai train_model -t personachat -m transformer/ranker -mf /tmp/model_tr6 --n-layers 1 --embedding-size 300 --ffn-size 600 --n-heads 4 --num-epochs 2 -veps 0.25 -bs 64 -lr 0.001 --dropout 0.1 --embedding-type fasttext_cc --candidates batch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Code Organization&lt;/h2&gt; &#xA;&lt;p&gt;The code is set up into several main directories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/core&#34;&gt;&lt;strong&gt;core&lt;/strong&gt;&lt;/a&gt;: contains the primary code for the framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/agents&#34;&gt;&lt;strong&gt;agents&lt;/strong&gt;&lt;/a&gt;: contains agents which can interact with the different tasks (e.g. machine learning models)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/scripts&#34;&gt;&lt;strong&gt;scripts&lt;/strong&gt;&lt;/a&gt;: contains a number of useful scripts, like training, evaluating, interactive chatting, ...&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks&#34;&gt;&lt;strong&gt;tasks&lt;/strong&gt;&lt;/a&gt;: contains code for the different tasks available from within ParlAI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/mturk&#34;&gt;&lt;strong&gt;mturk&lt;/strong&gt;&lt;/a&gt;: contains code for setting up Mechanical Turk, as well as sample MTurk tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/chat_service/services/messenger&#34;&gt;&lt;strong&gt;messenger&lt;/strong&gt;&lt;/a&gt;: contains code for interfacing with Facebook Messenger&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/utils&#34;&gt;&lt;strong&gt;utils&lt;/strong&gt;&lt;/a&gt;: contains a wide number of frequently used utility methods&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/crowdsourcing&#34;&gt;&lt;strong&gt;crowdsourcing&lt;/strong&gt;&lt;/a&gt;: contains code for running crowdsourcing tasks, such as on Amazon Mechanical Turk&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/chat_service/services/messenger&#34;&gt;&lt;strong&gt;chat_service&lt;/strong&gt;&lt;/a&gt;: contains code for interfacing with services such as Facebook Messenger&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/tree/main/parlai/zoo&#34;&gt;&lt;strong&gt;zoo&lt;/strong&gt;&lt;/a&gt;: contains code to directly download and use pretrained models from our model zoo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, bug reports or feature requests, please don&#39;t hesitate to post on our &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/issues&#34;&gt;Github Issues page&lt;/a&gt;. You may also be interested in checking out our &lt;a href=&#34;https://parl.ai/docs/faq.html&#34;&gt;FAQ&lt;/a&gt; and our &lt;a href=&#34;https://parl.ai/docs/tutorial_tipsntricks.html&#34;&gt;Tips n Tricks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please remember to follow our &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/raw/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome PRs from the community!&lt;/p&gt; &#xA;&lt;p&gt;You can find information about contributing to ParlAI in our &lt;a href=&#34;https://github.com/facebookresearch/ParlAI/raw/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;ParlAI is currently maintained by Moya Chen, Emily Dinan, Dexter Ju, Mojtaba Komeili, Spencer Poff, Pratik Ringshia, Stephen Roller, Kurt Shuster, Eric Michael Smith, Megan Ung, Jack Urbanek, Jason Weston, Mary Williamson, and Jing Xu. Kurt Shuster is the current Tech Lead.&lt;/p&gt; &#xA;&lt;p&gt;Former major contributors and maintainers include Alexander H. Miller, Margaret Li, Will Feng, Adam Fisch, Jiasen Lu, Antoine Bordes, Devi Parikh, Dhruv Batra, Filipe de Avila Belbute Peres, Chao Pan, and Vedant Puri.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the &lt;a href=&#34;https://arxiv.org/abs/1705.06476&#34;&gt;arXiv paper&lt;/a&gt; if you use ParlAI in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{miller2017parlai,&#xA;  title={ParlAI: A Dialog Research Software Platform},&#xA;  author={{Miller}, A.~H. and {Feng}, W. and {Fisch}, A. and {Lu}, J. and {Batra}, D. and {Bordes}, A. and {Parikh}, D. and {Weston}, J.},&#xA;  journal={arXiv preprint arXiv:{1705.06476}},&#xA;  year={2017}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;ParlAI is MIT licensed. See the &lt;strong&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;&lt;/strong&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Womsxd/AutoMihoyoBBS</title>
    <updated>2022-08-08T01:30:56Z</updated>
    <id>tag:github.com,2022-08-08:/Womsxd/AutoMihoyoBBS</id>
    <link href="https://github.com/Womsxd/AutoMihoyoBBS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;米游社自动签到，支持：崩坏二、崩坏三、原神、未定事件簿，米游币自动获取&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;米游社辅助签到&lt;/h1&gt; &#xA;&lt;p&gt;基于Python3的米游社辅助签到项目&lt;/p&gt; &#xA;&lt;p&gt;禁止大范围宣传本项目，谢谢配合&lt;/p&gt; &#xA;&lt;p&gt;本项目米游币部分参考&lt;a href=&#34;https://github.com/XiaoMiku01/miyoubiAuto&#34;&gt;XiaoMiku01/miyoubiAuto&lt;/a&gt;进行编写&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;此项目的用途&lt;/p&gt; &lt;p&gt;这是一个米游社的辅助签到项目，包含了米游币、崩坏2、崩坏3、原神、未定事件簿 已经支持米哈游国内正在运营的全部游戏的米游社签到(2022-7-19)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;如何使用程序&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;部署方法&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt; &lt;p&gt;使用&lt;a href=&#34;https://git-scm.com/&#34;&gt;Git&lt;/a&gt;或&lt;a href=&#34;https://github.com/Womsxd/AutoMihoyoBBS/archive/refs/heads/master.zip&#34;&gt;点击此处&lt;/a&gt;下载本项目&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;下载&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python3&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;解压本项目压缩包,在解压目录中&lt;strong&gt;Shift+右键&lt;/strong&gt; 打开你的命令提示符cmd或powershell&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Womsxd/AutoMihoyoBBS/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; 是所需第三方模块，执行 &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; 安装模块&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;打开目录中的&lt;strong&gt;config文件夹&lt;/strong&gt;复制&lt;code&gt;config.json.example&lt;/code&gt;并改名为&lt;code&gt;config.json&lt;/code&gt;，脚本的多用户功能靠读取不同的配置文件实现，你可以创建无数个&lt;code&gt;自定义名字.json&lt;/code&gt;，脚本会扫描&lt;strong&gt;config&lt;/strong&gt;目录下&lt;code&gt;json&lt;/code&gt;为拓展名的文件，并按照名称顺序依次执行。&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;请使用vscode/notepad++等文本编辑器打开上一步复制好的配置文件&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;使用&lt;a href=&#34;https://raw.githubusercontent.com/Womsxd/AutoMihoyoBBS/master/#%E8%8E%B7%E5%8F%96%E7%B1%B3%E6%B8%B8%E7%A4%BECookie&#34;&gt;获取Cookie&lt;/a&gt;里面的方法来获取米游社Cookie&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;将复制的Cookie粘贴到&lt;code&gt;config.json&lt;/code&gt;的&lt;code&gt;&#34;cookie&#34;:&#34; &#34;&lt;/code&gt;中(在&lt;code&gt;account&lt;/code&gt;里面)&lt;/p&gt; &lt;p&gt;例子&lt;/p&gt; &#xA;    &lt;blockquote&gt; &#xA;     &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&#34;cookie&#34;: &#34;你复制的cookie&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;/blockquote&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;检查&lt;code&gt;config.json&lt;/code&gt;的&lt;code&gt;&#34;enable&#34;:&lt;/code&gt;的值为true&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;在命令提示符(cmd)/powershell，输入&lt;code&gt;python main.py&lt;/code&gt;来进行执行&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;多用户的请使用&lt;code&gt;python main_multi.py&lt;/code&gt;，多用户在需要自动执行的情况下请使用&lt;code&gt;python main_multi.py autorun&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;获取米游社Cookie&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;打开你的浏览器,进入&lt;strong&gt;无痕/隐身模式&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;由于米哈游修改了bbs可以获取的Cookie，导致一次获取的Cookie缺失，所以需要增加步骤&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;打开&lt;code&gt;http://bbs.mihoyo.com/ys/&lt;/code&gt;并进行登入操作&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;在上一步登入完成后新建标签页，打开&lt;code&gt;http://user.mihoyo.com/&lt;/code&gt;并进行登入操作 (如果你不需要自动获取米游币可以忽略这个步骤，并把&lt;code&gt;mihoyobbs&lt;/code&gt;的&lt;code&gt;enable&lt;/code&gt;改为&lt;code&gt;false&lt;/code&gt;即可)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;按下键盘上的&lt;code&gt;F12&lt;/code&gt;或右键检查,打开开发者工具,点击Console&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;输入&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var cookie=document.cookie;var ask=confirm(&#39;Cookie:&#39;+cookie+&#39;\n\nDo you want to copy the cookie to the clipboard?&#39;);if(ask==true){copy(cookie);msg=cookie}else{msg=&#39;Cancel&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;回车执行，并在确认无误后点击确定。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;此时Cookie已经复制到你的粘贴板上了&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;使用Docker运行&lt;/h2&gt; &#xA;&lt;p&gt;Docker的运行脚本基于Linux平台编写，暂未在Win平台测试。&lt;/p&gt; &#xA;&lt;p&gt;将本项目Clone至本地后，请先按照上述步骤添加或修改配置文件。随后执行&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;启动docker容器。&lt;br&gt; &amp;nbsp;&lt;br&gt; 容器运行成功后可用&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;docker-compose logs -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;命令来查看程序输出。&lt;/p&gt; &#xA;&lt;p&gt;若需要添加配置文件或修改配置文件，可直接在主机config文件夹中修改，修改的内容将实时同步在容器中。&lt;/p&gt; &#xA;&lt;p&gt;每次运行Docker容器后，容器内将自动按照参数执行签到活动，签到完成后容器将默认在每天上午9:30运行一次，如果想自行修改时间可自行编辑&lt;code&gt;docker-compose.yml&lt;/code&gt;文件中的&lt;code&gt;CRON_SIGNIN&lt;/code&gt;，将其修改成想运行的时间。&lt;/p&gt; &#xA;&lt;h2&gt;使用python运行(screen)&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;将本项目Clone至本地后，安装好依赖直接运行&lt;code&gt;python3 server.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;在后台运行时请安装screen&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;使用&lt;code&gt;screen -S automhy&lt;/code&gt;进入后台线程&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ctrl+A组合键再按下d键回到主线程&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;screen -r automhy&lt;/code&gt;回到线程&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果不能回到线程请先&lt;code&gt;screen -d automhy&lt;/code&gt;挂起线程&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;命令窗口如下&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;stop: 关闭程序&lt;br&gt; mulit: 测试多用户签到&lt;br&gt; single: 测试多用户签到&lt;br&gt; reload: 重载配置文件&lt;br&gt; mod x: mod 1为单用户模式 mod 2为多用户模式&lt;br&gt; add &#39;yourcookie&#39;: 直接 add cookie 添加Cookie，根据提示输入用户存档名称&lt;br&gt; time x: 设置任务巡查时间,默认720分钟(12小时)&lt;br&gt; set user enable true(设置user.json 的enable属性为true)&lt;br&gt; show true/false: 开启/关闭20秒的倒计时提示&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;使用云函数运行&lt;/h2&gt; &#xA;&lt;p&gt;腾讯云函数服务免费额度近期有变化，为了&lt;strong&gt;避免产生费用&lt;/strong&gt;，建议切换到阿里云 函数计算 FC&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;腾讯云&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;在本地完整运行一次。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;打开并登录&lt;a href=&#34;https://console.cloud.tencent.com/scf/list&#34;&gt;云函数控制台&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;新建云函数 - 自定义创建，函数类型选&lt;code&gt;事件函数&lt;/code&gt;，部署方式选&lt;code&gt;代码部署&lt;/code&gt;，运行环境选 &lt;code&gt;Python3.6&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;提交方法选&lt;code&gt;本地上传文件夹&lt;/code&gt;，并在下方的函数代码处上传整个项目文件夹。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;执行方法填写 &lt;code&gt;index.main_handler&lt;/code&gt;,多用户请填写&lt;code&gt;index.main_handler_mulit&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;展开高级配置，将执行超时时间修改为 &lt;code&gt;300 秒&lt;/code&gt;，其他保持默认。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;展开触发器配置，选中自定义创建，触发周期选择&lt;code&gt;自定义触发周期&lt;/code&gt;，并填写表达式&lt;code&gt;0 0 10 * * * *&lt;/code&gt;（此处为每天上午 10 时运行一次，可以自行修改）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;完成，enjoy it！&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;阿里云&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;在本地完整运行一次。&lt;/li&gt; &#xA;   &lt;li&gt;打开并登录&lt;a href=&#34;https://fcnext.console.aliyun.com/cn-hangzhou/services&#34;&gt;函数计算 FC&lt;/a&gt;。注意左上方显示的地区，可点击切换其他地区。&lt;/li&gt; &#xA;   &lt;li&gt;创建服务 （日志功能可能产生费用，建议关闭） &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;创建函数&lt;/li&gt; &#xA;     &lt;li&gt;从零开始创建 &#xA;      &lt;ol&gt; &#xA;       &lt;li&gt;&lt;code&gt;请求处理程序类型：处理事件请求&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;推荐设置运行环境为&lt;code&gt;Python3.9&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;根据&lt;a href=&#34;https://help.aliyun.com/document_detail/422183.html&#34;&gt;官方文档&lt;/a&gt; 进行安装模块并打包&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;请求处理程序：index.main_handler&lt;/code&gt;，多用户请填写&lt;code&gt;index.main_handler_mulit&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;配置触发器：触发器类型 定时触发器 异步调用。建议触发方式设为&lt;code&gt;指定时间&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;点击创建&lt;/li&gt; &#xA;      &lt;/ol&gt; &lt;/li&gt; &#xA;     &lt;li&gt;进入函数详情 &#xA;      &lt;ol&gt; &#xA;       &lt;li&gt;打开函数配置&lt;/li&gt; &#xA;       &lt;li&gt;修改 &lt;code&gt;环境信息&lt;/code&gt; - &lt;code&gt;执行超时时间&lt;/code&gt; 为300秒。&lt;/li&gt; &#xA;      &lt;/ol&gt; &lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;测试运行 &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;打开 &lt;code&gt;函数详情&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;点击&lt;code&gt;测试函数&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;完成&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用的第三方库&lt;/h2&gt; &#xA;&lt;p&gt;requests: &lt;a href=&#34;https://github.com/psf/requests&#34;&gt;github&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/requests/&#34;&gt;pypi&lt;/a&gt; (当httpx无法使用时使用)&lt;/p&gt; &#xA;&lt;p&gt;httpx: &lt;a href=&#34;https://github.com/encode/httpx&#34;&gt;github&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/httpx/&#34;&gt;pypi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;crontab: &lt;a href=&#34;https://github.com/josiahcarlson/parse-crontab&#34;&gt;github&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/crontab/&#34;&gt;pypi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;关于使用 Github Actions 运行&lt;/h2&gt; &#xA;&lt;p&gt;本项目&lt;strong&gt;不支持&lt;/strong&gt;也&lt;strong&gt;不推荐&lt;/strong&gt;使用&lt;code&gt;Github Actions&lt;/code&gt;来每日自动执行！&lt;/p&gt; &#xA;&lt;p&gt;也&lt;strong&gt;不会&lt;/strong&gt;处理使用&lt;code&gt;Github Actions&lt;/code&gt;执行有关的issues！&lt;/p&gt; &#xA;&lt;p&gt;推荐使用 阿里云/腾讯云 的云函数来进行每日自动执行脚本。&lt;/p&gt; &#xA;&lt;h2&gt;Stargazers over time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/Womsxd/AutoMihoyoBBS&#34;&gt;&lt;img src=&#34;https://starchart.cc/Womsxd/AutoMihoyoBBS.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Womsxd/AutoMihoyoBBS/raw/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;鸣谢&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jb.gg/OpenSource&#34;&gt;JetBrains&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>karpathy/minGPT</title>
    <updated>2022-08-08T01:30:56Z</updated>
    <id>tag:github.com,2022-08-08:/karpathy/minGPT</id>
    <link href="https://github.com/karpathy/minGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;minGPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt.jpg&#34; alt=&#34;mingpt&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A PyTorch re-implementation of &lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;GPT&lt;/a&gt;, both training and inference. minGPT tries to be small, clean, interpretable and educational, as most of the currently available GPT model implementations can a bit sprawling. GPT is not a complicated model and this implementation is appropriately about 300 lines of code (see &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/model.py&#34;&gt;mingpt/model.py&lt;/a&gt;). All that&#39;s going on is that a sequence of indices feeds into a &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Transformer&lt;/a&gt;, and a probability distribution over the next index in the sequence comes out. The majority of the complexity is just being clever with batching (both across examples and over sequence length) for efficiency.&lt;/p&gt; &#xA;&lt;p&gt;The minGPT library is three files: &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/model.py&#34;&gt;mingpt/model.py&lt;/a&gt; contains the actual Transformer model definition, &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/bpe.py&#34;&gt;mingpt/bpe.py&lt;/a&gt; contains a mildly refactored Byte Pair Encoder that translates between text and sequences of integers exactly like OpenAI did in GPT, &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/trainer.py&#34;&gt;mingpt/trainer.py&lt;/a&gt; is (GPT-independent) PyTorch boilerplate code that trains the model. Then there are a number of demos and projects that use the library in the &lt;code&gt;projects&lt;/code&gt; folder:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;projects/adder&lt;/code&gt; trains a GPT from scratch to add numbers (inspired by the addition section in the GPT-3 paper)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;projects/chargpt&lt;/code&gt; trains a GPT to be a character-level language model on some input text file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;demo.ipynb&lt;/code&gt; shows a minimal usage of the &lt;code&gt;GPT&lt;/code&gt; and &lt;code&gt;Trainer&lt;/code&gt; in a notebook format on a simple sorting example&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;generate.ipynb&lt;/code&gt; shows how one can load a pretrained GPT2 and generate text given some prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Library Installation&lt;/h3&gt; &#xA;&lt;p&gt;If you want to &lt;code&gt;import mingpt&lt;/code&gt; into your project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/karpathy/minGPT.git&#xA;cd minGPT&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s how you&#39;d instantiate a GPT-2 (124M param version):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mingpt.model import GPT&#xA;model_config = GPT.get_default_config()&#xA;model_config.model_type = &#39;gpt2&#39;&#xA;model_config.vocab_size = 50257 # openai&#39;s model vocabulary&#xA;model_config.block_size = 1024  # openai&#39;s model block_size (i.e. input context length)&#xA;model = GPT(model_config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here&#39;s how you&#39;d train it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# your subclass of torch.utils.data.Dataset that emits example&#xA;# torch LongTensor of lengths up to 1024, with integers from [0,50257)&#xA;train_dataset = YourDataset()&#xA;&#xA;from mingpt.trainer import Trainer&#xA;train_config = Trainer.get_default_config()&#xA;train_config.learning_rate = 5e-4 # many possible options, see the file&#xA;train_config.max_iters = 1000&#xA;train_config.batch_size = 32&#xA;trainer = Trainer(train_config, model, train_dataset)&#xA;trainer.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;demo.ipynb&lt;/code&gt; for a more concrete example.&lt;/p&gt; &#xA;&lt;h3&gt;Unit tests&lt;/h3&gt; &#xA;&lt;p&gt;Coverage is not super amazing just yet but:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m unittest discover tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;todos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;add gpt-2 finetuning demo on arbitrary given text file&lt;/li&gt; &#xA; &lt;li&gt;add dialog agent demo&lt;/li&gt; &#xA; &lt;li&gt;better docs of outcomes for existing projects (adder, chargpt)&lt;/li&gt; &#xA; &lt;li&gt;add mixed precision and related training scaling goodies&lt;/li&gt; &#xA; &lt;li&gt;distributed training support&lt;/li&gt; &#xA; &lt;li&gt;reproduce some benchmarks in projects/, e.g. text8 or other language modeling&lt;/li&gt; &#xA; &lt;li&gt;proper logging instead of print statement amateur hour haha&lt;/li&gt; &#xA; &lt;li&gt;i probably should have a requirements.txt file...&lt;/li&gt; &#xA; &lt;li&gt;it should be possible to load in many other model weights other than just gpt2-*&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;References&lt;/h3&gt; &#xA;&lt;p&gt;Code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;openai/gpt-2&lt;/a&gt; has the model definition in TensorFlow, but not the training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/image-gpt&#34;&gt;openai/image-gpt&lt;/a&gt; has some more modern gpt-3 like modification in its code, good reference as well&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;huggingface/transformers&lt;/a&gt; has a &lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling&#34;&gt;language-modeling example&lt;/a&gt;. It is full-featured but as a result also somewhat challenging to trace. E.g. some large functions have as much as 90% unused code behind various branching statements that is unused in the default setting of simple language modeling&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Papers + some implementation notes:&lt;/p&gt; &#xA;&lt;h4&gt;Improving Language Understanding by Generative Pre-Training (GPT-1)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our model largely follows the original transformer work&lt;/li&gt; &#xA; &lt;li&gt;We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.&lt;/li&gt; &#xA; &lt;li&gt;Adam max learning rate of 2.5e-4. (later GPT-3 for this model size uses 6e-4)&lt;/li&gt; &#xA; &lt;li&gt;LR decay: increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule&lt;/li&gt; &#xA; &lt;li&gt;We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.&lt;/li&gt; &#xA; &lt;li&gt;Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient&lt;/li&gt; &#xA; &lt;li&gt;bytepair encoding (BPE) vocabulary with 40,000 merges&lt;/li&gt; &#xA; &lt;li&gt;residual, embedding, and attention dropouts with a rate of 0.1 for regularization.&lt;/li&gt; &#xA; &lt;li&gt;modified version of L2 regularization proposed in (37), with w = 0.01 on all non bias or gain weights&lt;/li&gt; &#xA; &lt;li&gt;For the activation function, we used the Gaussian Error Linear Unit (GELU).&lt;/li&gt; &#xA; &lt;li&gt;We used learned position embeddings instead of the sinusoidal version proposed in the original work&lt;/li&gt; &#xA; &lt;li&gt;For finetuning: We add dropout to the classifier with a rate of 0.1. learning rate of 6.25e-5 and a batchsize of 32. 3 epochs. We use a linear learning rate decay schedule with warmup over 0.2% of training. λ was set to 0.5.&lt;/li&gt; &#xA; &lt;li&gt;GPT-1 model is 12 layers and d_model 768, ~117M params&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Language Models are Unsupervised Multitask Learners (GPT-2)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LayerNorm was moved to the input of each sub-block, similar to a pre-activation residual network&lt;/li&gt; &#xA; &lt;li&gt;an additional layer normalization was added after the final self-attention block.&lt;/li&gt; &#xA; &lt;li&gt;modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/√N where N is the number of residual layers. (weird because in their released code i can only find a simple use of the old 0.02... in their release of image-gpt I found it used for c_proj, and even then only for attn, not for mlp. huh. &lt;a href=&#34;https://github.com/openai/image-gpt/raw/master/src/model.py&#34;&gt;https://github.com/openai/image-gpt/blob/master/src/model.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;the vocabulary is expanded to 50,257&lt;/li&gt; &#xA; &lt;li&gt;increase the context size from 512 to 1024 tokens&lt;/li&gt; &#xA; &lt;li&gt;larger batchsize of 512 is used&lt;/li&gt; &#xA; &lt;li&gt;GPT-2 used 48 layers and d_model 1600 (vs. original 12 layers and d_model 768). ~1.542B params&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Language Models are Few-Shot Learners (GPT-3)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT-3: 96 layers, 96 heads, with d_model of 12,288 (175B parameters).&lt;/li&gt; &#xA; &lt;li&gt;GPT-1-like: 12 layers, 12 heads, d_model 768 (125M)&lt;/li&gt; &#xA; &lt;li&gt;We use the same model and architecture as GPT-2, including the modified initialization, pre-normalization, and reversible tokenization described therein&lt;/li&gt; &#xA; &lt;li&gt;we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer&lt;/li&gt; &#xA; &lt;li&gt;we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 ∗ dmodel&lt;/li&gt; &#xA; &lt;li&gt;all models use a context window of nctx = 2048 tokens.&lt;/li&gt; &#xA; &lt;li&gt;Adam with β1 = 0.9, β2 = 0.95, and eps = 10−8&lt;/li&gt; &#xA; &lt;li&gt;All models use weight decay of 0.1 to provide a small amount of regularization. (NOTE: GPT-1 used 0.01 I believe, see above)&lt;/li&gt; &#xA; &lt;li&gt;clip the global norm of the gradient at 1.0&lt;/li&gt; &#xA; &lt;li&gt;Linear LR warmup over the first 375 million tokens. Then use cosine decay for learning rate down to 10% of its value, over 260 billion tokens.&lt;/li&gt; &#xA; &lt;li&gt;gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size.&lt;/li&gt; &#xA; &lt;li&gt;full 2048-sized time context window is always used, with a special END OF DOCUMENT token delimiter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Generative Pretraining from Pixels (Image GPT)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When working with images, we pick the identity permutation πi = i for 1 ≤ i ≤ n, also known as raster order.&lt;/li&gt; &#xA; &lt;li&gt;we create our own 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512.&lt;/li&gt; &#xA; &lt;li&gt;Our largest model, iGPT-XL, contains L = 60 layers and uses an embedding size of d = 3072 for a total of 6.8B parameters.&lt;/li&gt; &#xA; &lt;li&gt;Our next largest model, iGPT-L, is essentially identical to GPT-2 with L = 48 layers, but contains a slightly smaller embedding size of d = 1536 (vs 1600) for a total of 1.4B parameters.&lt;/li&gt; &#xA; &lt;li&gt;We use the same model code as GPT-2, except that we initialize weights in the layerdependent fashion as in Sparse Transformer (Child et al., 2019) and zero-initialize all projections producing logits.&lt;/li&gt; &#xA; &lt;li&gt;We also train iGPT-M, a 455M parameter model with L = 36 and d = 1024&lt;/li&gt; &#xA; &lt;li&gt;iGPT-S, a 76M parameter model with L = 24 and d = 512 (okay, and how many heads? looks like the Github code claims 8)&lt;/li&gt; &#xA; &lt;li&gt;When pre-training iGPT-XL, we use a batch size of 64 and train for 2M iterations, and for all other models we use a batch size of 128 and train for 1M iterations.&lt;/li&gt; &#xA; &lt;li&gt;Adam with β1 = 0.9 and β2 = 0.95&lt;/li&gt; &#xA; &lt;li&gt;The learning rate is warmed up for one epoch, and then decays to 0&lt;/li&gt; &#xA; &lt;li&gt;We did not use weight decay because applying a small weight decay of 0.01 did not change representation quality.&lt;/li&gt; &#xA; &lt;li&gt;iGPT-S lr 0.003&lt;/li&gt; &#xA; &lt;li&gt;No dropout is used.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>