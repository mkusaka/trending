<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-21T01:42:40Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>karfly/chatgpt_telegram_bot</title>
    <updated>2023-01-21T01:42:40Z</updated>
    <id>tag:github.com,2023-01-21:/karfly/chatgpt_telegram_bot</id>
    <link href="https://github.com/karfly/chatgpt_telegram_bot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT Telegram Bot: &lt;strong&gt;Fast. No daily limits. Special chat modes&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/karfly/chatgpt_telegram_bot/main/static/header.png&#34; align=&#34;center&#34; style=&#34;width: 100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We all love &lt;a href=&#34;https://chat.openai.com&#34;&gt;chat.openai.com&lt;/a&gt;, but...&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s TERRIBLY laggy, has daily limits, and is only accessible through an archaic web interface.&lt;/p&gt; &#xA;&lt;p&gt;This repo is ChatGPT re-created with GPT-3.5 LLM as Telegram Bot. &lt;strong&gt;And it works great.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Low latency replies (it usually takes about 3-5 seconds)&lt;/li&gt; &#xA; &lt;li&gt;No request limits&lt;/li&gt; &#xA; &lt;li&gt;Code highlighting&lt;/li&gt; &#xA; &lt;li&gt;Special chat modes: üë©üèº‚Äçüéì Assistant, üë©üèº‚Äçüíª Code Assistant, üé¨ Movie Expert. More soon&lt;/li&gt; &#xA; &lt;li&gt;List of allowed Telegram users&lt;/li&gt; &#xA; &lt;li&gt;Track $ balance spent on OpenAI API&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Bot commands&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/retry&lt;/code&gt; ‚Äì Regenerate last bot answer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/new&lt;/code&gt; ‚Äì Start new dialog&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/mode&lt;/code&gt; ‚Äì Select chat mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/balance&lt;/code&gt; ‚Äì Show balance&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/help&lt;/code&gt; ‚Äì Show help&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Get your &lt;a href=&#34;https://openai.com/api/&#34;&gt;OpenAI API&lt;/a&gt; key&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get your Telegram bot token from &lt;a href=&#34;https://t.me/BotFather&#34;&gt;@BotFather&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Edit &lt;code&gt;config/config.example.yml&lt;/code&gt; to set your tokens and run 2 commands below (&lt;em&gt;if you&#39;re advanced user, you can also edit&lt;/em&gt; &lt;code&gt;config/config.example.env&lt;/code&gt;):&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mv config/config.example.yml config/config.yml&#xA;mv config/config.example.env config/config.env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üî• And now &lt;strong&gt;run&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose --env-file config/config.env up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learnprompting.org/docs/applied_prompting/build_chatgpt&#34;&gt;&lt;em&gt;Build ChatGPT from GPT-3&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>espnet/espnet</title>
    <updated>2023-01-21T01:42:40Z</updated>
    <id>tag:github.com,2023-01-21:/espnet/espnet</id>
    <link href="https://github.com/espnet/espnet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;End-to-End Speech Processing Toolkit&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;left&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/espnet/espnet/master/doc/image/espnet_logo1.png&#34; width=&#34;550&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h1&gt;ESPnet: end-to-end speech processing toolkit&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;system/pytorch ver.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1.4.0&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1.5.1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1.6.0&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1.7.1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1.8.1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1.9.1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1.10.2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1.11.0&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1.12.1&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ubuntu20/python3.9/pip&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ubuntu20/python3.8/pip&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ubuntu18/python3.7/pip&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Github Actions&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;debian9/python3.7/conda&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions?query=workflow%3Adebian9&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/debian9/badge.svg?sanitize=true&#34; alt=&#34;debian9&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;centos7/python3.7/conda&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions?query=workflow%3Acentos7&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/centos7/badge.svg?sanitize=true&#34; alt=&#34;centos7&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;doc/python3.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/actions?query=workflow%3Adoc&#34;&gt;&lt;img src=&#34;https://github.com/espnet/espnet/workflows/doc/badge.svg?sanitize=true&#34; alt=&#34;doc&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/espnet&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/espnet.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/espnet/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/espnet.svg?sanitize=true&#34; alt=&#34;Python Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/espnet&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/espnet&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/espnet/espnet.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/espnet/espnet&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/espnet/espnet/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pycqa.github.io/isort/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&amp;amp;labelColor=ef8336&#34; alt=&#34;Imports: isort&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mergify.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://api.mergify.com/v1/badges/espnet/espnet&amp;amp;style=flat&#34; alt=&#34;Mergify Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/espnet-en/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/espnet-en/community.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://espnet.github.io/espnet/&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs&#34;&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2&#34;&gt;&lt;strong&gt;Example (ESPnet2)&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/espnet/espnet/tree/master/docker&#34;&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/espnet/notebook&#34;&gt;&lt;strong&gt;Notebook&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ESPnet is an end-to-end speech processing toolkit covering end-to-end speech recognition, text-to-speech, speech translation, speech enhancement, speaker diarization, spoken language understanding, and so on. ESPnet uses &lt;a href=&#34;http://pytorch.org/&#34;&gt;pytorch&lt;/a&gt; as a deep learning engine and also follows &lt;a href=&#34;http://kaldi-asr.org/&#34;&gt;Kaldi&lt;/a&gt; style data processing, feature extraction/format, and recipes to provide a complete setup for various speech processing experiments.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorial Series&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2019 Tutorial at Interspeech &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/espnet/interspeech2019-tutorial&#34;&gt;Material&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2021 Tutorial at CMU &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://youtu.be/2mRz3wH1vd0&#34;&gt;Online video&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tutorial_2021_CMU_11751_18781.ipynb&#34;&gt;Material&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2022 Tutorial at CMU &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Usage of ESPnet (ASR as an example) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://youtu.be/YDN8cVjxSik&#34;&gt;Online video&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.ipynb&#34;&gt;Material&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Add new models/tasks to ESPnet &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://youtu.be/Css3XAes7SU&#34;&gt;Online video&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.ipynb&#34;&gt;Material&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;Kaldi style complete recipe&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support numbers of &lt;code&gt;ASR&lt;/code&gt; recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Support numbers of &lt;code&gt;TTS&lt;/code&gt; recipes with a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Support numbers of &lt;code&gt;ST&lt;/code&gt; recipes (Fisher-CallHome Spanish, Libri-trans, IWSLT&#39;18, How2, Must-C, Mboshi-French, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Support numbers of &lt;code&gt;MT&lt;/code&gt; recipes (IWSLT&#39;14, IWSLT&#39;16, the above ST recipes etc.)&lt;/li&gt; &#xA; &lt;li&gt;Support numbers of &lt;code&gt;SLU&lt;/code&gt; recipes (CATSLU-MAPS, FSC, Grabo, IEMOCAP, JDCINAL, SNIPS, SLURP, SWBD-DA, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Support numbers of &lt;code&gt;SE/SS&lt;/code&gt; recipes (DNS-IS2020, LibriMix, SMS-WSJ, VCTK-noisyreverb, WHAM!, WHAMR!, WSJ-2mix, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Support voice conversion recipe (VCC2020 baseline)&lt;/li&gt; &#xA; &lt;li&gt;Support speaker diarization recipe (mini_librispeech, librimix)&lt;/li&gt; &#xA; &lt;li&gt;Support singing voice synthesis recipe (ofuton_p_utagoe_db)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ASR: Automatic Speech Recognition&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;State-of-the-art performance&lt;/strong&gt; in several ASR benchmarks (comparable/superior to hybrid DNN/HMM and CTC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hybrid CTC/attention&lt;/strong&gt; based end-to-end ASR &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fast/accurate training with CTC/attention multitask training&lt;/li&gt; &#xA;   &lt;li&gt;CTC/attention joint decoding to boost monotonic alignment decoding&lt;/li&gt; &#xA;   &lt;li&gt;Encoder: VGG-like CNN + BiRNN (LSTM/GRU), sub-sampling BiRNN (LSTM/GRU), Transformer, Conformer or &lt;a href=&#34;https://proceedings.mlr.press/v162/peng22a.html&#34;&gt;Branchformer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Decoder: RNN (LSTM/GRU), Transformer, or S4&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Attention: Dot product, location-aware attention, variants of multi-head&lt;/li&gt; &#xA; &lt;li&gt;Incorporate RNNLM/LSTMLM/TransformerLM/N-gram trained only with text data&lt;/li&gt; &#xA; &lt;li&gt;Batch GPU decoding&lt;/li&gt; &#xA; &lt;li&gt;Data augmentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transducer&lt;/strong&gt; based end-to-end ASR &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Architecture: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;RNN-based encoder and decoder.&lt;/li&gt; &#xA;     &lt;li&gt;Custom encoder and decoder supporting Transformer, Conformer (encoder), 1D Conv / TDNN (encoder) and causal 1D Conv (decoder) blocks.&lt;/li&gt; &#xA;     &lt;li&gt;VGG2L (RNN/custom encoder) and Conv2D (custom encoder) bottlenecks.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Search algorithms: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Greedy search constrained to one emission by timestep.&lt;/li&gt; &#xA;     &lt;li&gt;Default beam search algorithm &lt;a href=&#34;https://arxiv.org/abs/1211.3711&#34;&gt;[Graves, 2012]&lt;/a&gt; without prefix search.&lt;/li&gt; &#xA;     &lt;li&gt;Alignment-Length Synchronous decoding &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9053040&#34;&gt;[Saon et al., 2020]&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Time Synchronous Decoding &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9053040&#34;&gt;[Saon et al., 2020]&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;N-step Constrained beam search modified from &lt;a href=&#34;https://arxiv.org/abs/2002.03577&#34;&gt;[Kim et al., 2020]&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;modified Adaptive Expansion Search based on &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9250505&#34;&gt;[Kim et al., 2021]&lt;/a&gt; and NSC.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Features: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Multi-task learning with various auxiliary losses: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Encoder: CTC, auxiliary Transducer and symmetric KL divergence.&lt;/li&gt; &#xA;       &lt;li&gt;Decoder: cross-entropy w/ label smoothing.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Transfer learning with acoustic model and/or language model.&lt;/li&gt; &#xA;     &lt;li&gt;Training with FastEmit regularization method &lt;a href=&#34;https://arxiv.org/abs/2010.11148&#34;&gt;[Yu et al., 2021]&lt;/a&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Please refer to the &lt;a href=&#34;https://espnet.github.io/espnet/tutorial.html#transducer&#34;&gt;tutorial page&lt;/a&gt; for complete documentation.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;CTC segmentation&lt;/li&gt; &#xA; &lt;li&gt;Non-autoregressive model based on Mask-CTC&lt;/li&gt; &#xA; &lt;li&gt;ASR examples for supporting endangered language documentation (Please refer to egs/puebla_nahuatl and egs/yoloxochitl_mixtec for details)&lt;/li&gt; &#xA; &lt;li&gt;Wav2Vec2.0 pretrained model as Encoder, imported from &lt;a href=&#34;https://github.com/pytorch/fairseq/tree/master/fairseq&#34;&gt;FairSeq&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Self-supervised learning representations as features, using upstream models in &lt;a href=&#34;https://github.com/s3prl/s3prl&#34;&gt;S3PRL&lt;/a&gt; in frontend. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set &lt;code&gt;frontend&lt;/code&gt; to be &lt;code&gt;s3prl&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Select any upstream model by setting the &lt;code&gt;frontend_conf&lt;/code&gt; to the corresponding name.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Transfer Learning : &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;easy usage and transfers from models previously trained by your group, or models from &lt;a href=&#34;https://huggingface.co/espnet&#34;&gt;ESPnet Hugging Face repository&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/mini_an4/asr1/transfer_learning.md&#34;&gt;Documentation&lt;/a&gt; and &lt;a href=&#34;https://github.com/espnet/notebook/raw/master/espnet2_asr_transfer_learning_demo.ipynb&#34;&gt;toy example runnable on colab&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Streaming Transformer/Conformer ASR with blockwise synchronous beam search.&lt;/li&gt; &#xA; &lt;li&gt;Restricted Self-Attention based on &lt;a href=&#34;https://arxiv.org/abs/2004.05150&#34;&gt;Longformer&lt;/a&gt; as an encoder for long sequences&lt;/li&gt; &#xA; &lt;li&gt;OpenAI &lt;a href=&#34;https://openai.com/blog/whisper/&#34;&gt;Whisper&lt;/a&gt; model, robust ASR based on large-scale, weakly-supervised multitask learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Demonstration&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Real-time ASR demo with ESPnet2 &lt;a href=&#34;https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_asr_realtime_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt; Web Demo on &lt;a href=&#34;https://huggingface.co/docs/hub/spaces&#34;&gt;Hugging Face Spaces&lt;/a&gt;. Check out the &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/espnet2_asr&#34;&gt;Web Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Streaming Transformer ASR &lt;a href=&#34;https://github.com/espnet/notebook/raw/master/espnet2_streaming_asr_demo.ipynb&#34;&gt;Local Demo&lt;/a&gt; with ESPnet2.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TTS: Text-to-speech&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Architecture &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Tacotron2&lt;/li&gt; &#xA;   &lt;li&gt;Transformer-TTS&lt;/li&gt; &#xA;   &lt;li&gt;FastSpeech&lt;/li&gt; &#xA;   &lt;li&gt;FastSpeech2&lt;/li&gt; &#xA;   &lt;li&gt;Conformer FastSpeech &amp;amp; FastSpeech2&lt;/li&gt; &#xA;   &lt;li&gt;VITS&lt;/li&gt; &#xA;   &lt;li&gt;JETS&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Multi-speaker &amp;amp; multi-language extention &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pretrained speaker embedding (e.g., X-vector)&lt;/li&gt; &#xA;   &lt;li&gt;Speaker ID embedding&lt;/li&gt; &#xA;   &lt;li&gt;Language ID embedding&lt;/li&gt; &#xA;   &lt;li&gt;Global style token (GST) embedding&lt;/li&gt; &#xA;   &lt;li&gt;Mix of the above embeddings&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;End-to-end training &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;End-to-end text-to-wav model (e.g., VITS, JETS, etc.)&lt;/li&gt; &#xA;   &lt;li&gt;Joint training of text2mel and vocoder&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Various language support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;En / Jp / Zn / De / Ru / And more...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Integration with neural vocoders &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parallel WaveGAN&lt;/li&gt; &#xA;   &lt;li&gt;MelGAN&lt;/li&gt; &#xA;   &lt;li&gt;Multi-band MelGAN&lt;/li&gt; &#xA;   &lt;li&gt;HiFiGAN&lt;/li&gt; &#xA;   &lt;li&gt;StyleMelGAN&lt;/li&gt; &#xA;   &lt;li&gt;Mix of the above models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Demonstration&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Real-time TTS demo with ESPnet2 &lt;a href=&#34;https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Hugging Face Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See demo: &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/ESPnet2-TTS&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To train the neural vocoder, please check the following repositories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;kan-bayashi/ParallelWaveGAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34;&gt;r9y9/wavenet_vocoder&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;We are moving on ESPnet2-based development for TTS.&lt;/li&gt; &#xA;  &lt;li&gt;The use of ESPnet1-TTS is deprecated, please use &lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1&#34;&gt;ESPnet2-TTS&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;SE: Speech enhancement (and separation)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Single-speaker speech enhancement&lt;/li&gt; &#xA; &lt;li&gt;Multi-speaker speech separation&lt;/li&gt; &#xA; &lt;li&gt;Unified encoder-separator-decoder structure for time-domain and frequency-domain models &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Encoder/Decoder: STFT/iSTFT, Convolution/Transposed-Convolution&lt;/li&gt; &#xA;   &lt;li&gt;Separators: BLSTM, Transformer, Conformer, &lt;a href=&#34;https://arxiv.org/abs/1809.07454&#34;&gt;TasNet&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1910.06379&#34;&gt;DPRNN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2201.10800&#34;&gt;SkiM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2011.02329&#34;&gt;SVoice&lt;/a&gt;, &lt;a href=&#34;https://web.cse.ohio-state.edu/~wang.77/papers/TZW.taslp21.pdf&#34;&gt;DC-CRN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2008.00264&#34;&gt;DCCRN&lt;/a&gt;, &lt;a href=&#34;https://ieeexplore.ieee.org/document/7471631&#34;&gt;Deep Clustering&lt;/a&gt;, &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/29430212/&#34;&gt;Deep Attractor Network&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1909.13387&#34;&gt;FaSNet&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1910.14104&#34;&gt;iFaSNet&lt;/a&gt;, Neural Beamformers, etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Flexible ASR integration: working as an individual task or as the ASR frontend&lt;/li&gt; &#xA; &lt;li&gt;Easy to import pretrained models from &lt;a href=&#34;https://github.com/asteroid-team/asteroid&#34;&gt;Asteroid&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Both the pre-trained models from Asteroid and the specific configuration are supported.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Demonstration&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Interactive SE demo with ESPnet2 &lt;a href=&#34;https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ST: Speech Translation &amp;amp; MT: Machine Translation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;State-of-the-art performance&lt;/strong&gt; in several ST benchmarks (comparable/superior to cascaded ASR and MT)&lt;/li&gt; &#xA; &lt;li&gt;Transformer based end-to-end ST (new!)&lt;/li&gt; &#xA; &lt;li&gt;Transformer based end-to-end MT (new!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;VC: Voice conversion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Transformer and Tacotron2 based parallel VC using melspectrogram (new!)&lt;/li&gt; &#xA; &lt;li&gt;End-to-end VC based on cascaded ASR+TTS (Baseline system for Voice Conversion Challenge 2020!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SLU: Spoken Language Understanding&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Architecture &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Transformer based Encoder&lt;/li&gt; &#xA;   &lt;li&gt;Conformer based Encoder&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v162/peng22a.html&#34;&gt;Branchformer&lt;/a&gt; based Encoder&lt;/li&gt; &#xA;   &lt;li&gt;RNN based Decoder&lt;/li&gt; &#xA;   &lt;li&gt;Transformer based Decoder&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support Multitasking with ASR &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Predict both intent and ASR transcript&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support Multitasking with NLU &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Deliberation encoder based 2 pass model&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support using pretrained ASR models &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Hubert&lt;/li&gt; &#xA;   &lt;li&gt;Wav2vec2&lt;/li&gt; &#xA;   &lt;li&gt;VQ-APC&lt;/li&gt; &#xA;   &lt;li&gt;TERA and more ...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support using pretrained NLP models &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;BERT&lt;/li&gt; &#xA;   &lt;li&gt;MPNet And more...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Various language support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;En / Jp / Zn / Nl / And more...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Supports using context from previous utterances&lt;/li&gt; &#xA; &lt;li&gt;Supports using other tasks like SE in pipeline manner&lt;/li&gt; &#xA; &lt;li&gt;Supports Two Pass SLU that combines audio and ASR transcript Demonstration&lt;/li&gt; &#xA; &lt;li&gt;Performing noisy spoken language understanding using speech enhancement model followed by spoken language understanding model. &lt;a href=&#34;https://colab.research.google.com/drive/14nCrJ05vJcQX0cJuXjbMVFWUHJ3Wfb6N?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Performing two pass spoken language understanding where the second pass model attends on both acoustic and semantic information. &lt;a href=&#34;https://colab.research.google.com/drive/1p2cbGIPpIIcynuDl4ZVHDpmNPl8Nh_ci?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Hugging Face Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See SLU demo on multiple languages: &lt;a href=&#34;https://huggingface.co/spaces/Siddhant/ESPnet2-SLU&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SUM: Speech Summarization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;End to End Speech Summarization Recipe for Instructional Videos using Restricted Self-Attention &lt;a href=&#34;https://arxiv.org/abs/2110.06263&#34;&gt;[Sharma et al., 2022]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SVS: Singing Voice Synthesis&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Framework merge from &lt;a href=&#34;https://github.com/SJTMusicTeam/Muskits&#34;&gt;Muskits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Architecture &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RNN-based non-autoregressive model&lt;/li&gt; &#xA;   &lt;li&gt;Xiaoice&lt;/li&gt; &#xA;   &lt;li&gt;Sequence-to-sequence Transformer (with GLU-based encoder)&lt;/li&gt; &#xA;   &lt;li&gt;MLP singer (in progress)&lt;/li&gt; &#xA;   &lt;li&gt;Tacotron-singing (in progress)&lt;/li&gt; &#xA;   &lt;li&gt;DiffSinger (in progress)&lt;/li&gt; &#xA;   &lt;li&gt;VISinger&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support multi-speaker &amp;amp; multilingual singing synthesis &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Speaker ID embedding&lt;/li&gt; &#xA;   &lt;li&gt;Language ID embedding&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Various language support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jp / En / Kr / Zh&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Tight integration with neural vocoders (the same as TTS)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Self-supervised Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support HuBERT Pretraining: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example recipe: &lt;a href=&#34;https://raw.githubusercontent.com/espnet/espnet/master/egs2/LibriSpeech/ssl1&#34;&gt;egs2/LibriSpeech/ssl1&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;DNN Framework&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Flexible network architecture thanks to chainer and pytorch&lt;/li&gt; &#xA; &lt;li&gt;Flexible front-end processing thanks to &lt;a href=&#34;https://github.com/nttcslab-sp/kaldiio&#34;&gt;kaldiio&lt;/a&gt; and HDF5 support&lt;/li&gt; &#xA; &lt;li&gt;Tensorboard based monitoring&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ESPnet2&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://espnet.github.io/espnet/espnet2_tutorial.html&#34;&gt;ESPnet2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Independent from Kaldi/Chainer, unlike ESPnet1&lt;/li&gt; &#xA; &lt;li&gt;On the fly feature extraction and text processing when training&lt;/li&gt; &#xA; &lt;li&gt;Supporting DistributedDataParallel and DaraParallel both&lt;/li&gt; &#xA; &lt;li&gt;Supporting multiple nodes training and integrated with &lt;a href=&#34;https://slurm.schedmd.com/&#34;&gt;Slurm&lt;/a&gt; or MPI&lt;/li&gt; &#xA; &lt;li&gt;Supporting Sharded Training provided by &lt;a href=&#34;https://github.com/facebookresearch/fairscale&#34;&gt;fairscale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A template recipe which can be applied for all corpora&lt;/li&gt; &#xA; &lt;li&gt;Possible to train any size of corpus without CPU memory error&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/espnet/espnet_model_zoo&#34;&gt;ESPnet Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Integrated with &lt;a href=&#34;https://espnet.github.io/espnet/espnet2_training_option.html#weights-biases-integration&#34;&gt;wandb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you intend to do full experiments including DNN training, then see &lt;a href=&#34;https://espnet.github.io/espnet/installation.html&#34;&gt;Installation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you just need the Python module only:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# We recommend you installing pytorch before installing espnet following https://pytorch.org/get-started/locally/&#xA;pip install espnet&#xA;# To install latest&#xA;# pip install git+https://github.com/espnet/espnet&#xA;# To install additional packages&#xA;# pip install &#34;espnet[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you&#39;ll use ESPnet1, please install chainer and cupy.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install chainer==6.0.0 cupy==6.0.0    # [Option]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You might need to install some packages depending on each task. We prepared various installation scripts at &lt;a href=&#34;https://raw.githubusercontent.com/espnet/espnet/master/tools/installers&#34;&gt;tools/installers&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(ESPnet2) Once installed, run &lt;code&gt;wandb login&lt;/code&gt; and set &lt;code&gt;--use_wandb true&lt;/code&gt; to enable tracking runs using W&amp;amp;B.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://espnet.github.io/espnet/tutorial.html&#34;&gt;Usage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Docker Container&lt;/h2&gt; &#xA;&lt;p&gt;go to &lt;a href=&#34;https://raw.githubusercontent.com/espnet/espnet/master/docker/&#34;&gt;docker/&lt;/a&gt; and follow &lt;a href=&#34;https://espnet.github.io/espnet/docker.html&#34;&gt;instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for taking times for ESPnet! Any contributions to ESPnet are welcome and feel free to ask any questions or requests to &lt;a href=&#34;https://github.com/espnet/espnet/issues&#34;&gt;issues&lt;/a&gt;. If it&#39;s the first contribution to ESPnet for you, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/espnet/espnet/master/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Results and demo&lt;/h2&gt; &#xA;&lt;p&gt;You can find useful tutorials and demos in &lt;a href=&#34;https://github.com/espnet/interspeech2019-tutorial&#34;&gt;Interspeech 2019 Tutorial&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ASR results&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;expand&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;We list the character error rate (CER) and word error rate (WER) of major ASR tasks.&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;Task&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;CER (%)&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;WER (%)&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Pretrained model&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Aishell dev/test&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;4.6/5.1&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/aishell/asr1/RESULTS.md#conformer-kernel-size--15--specaugment--lm-weight--00-result&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;strong&gt;ESPnet2&lt;/strong&gt; Aishell dev/test&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;4.1/4.4&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/aishell/asr1#branchformer-initial&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Common Voice dev/test&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1.7/1.8&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;2.2/2.3&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/commonvoice/asr1/RESULTS.md#first-results-default-pytorch-transformer-setting-with-bpe-100-epochs-single-gpu&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;CSJ eval1/eval2/eval3&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;5.7/3.8/4.2&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/csj/asr1/RESULTS.md#pytorch-backend-transformer-without-any-hyperparameter-tuning&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;strong&gt;ESPnet2&lt;/strong&gt; CSJ eval1/eval2/eval3&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;4.5/3.3/3.6&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/csj/asr1#initial-conformer-results&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;HKUST dev&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;23.5&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/hkust/asr1/RESULTS.md#transformer-only-20-epochs&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;strong&gt;ESPnet2&lt;/strong&gt; HKUST dev&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;21.2&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/hkust/asr1#transformer-asr--transformer-lm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Librispeech dev_clean/dev_other/test_clean/test_other&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1.9/4.9/2.1/4.9&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/librispeech/asr1/RESULTS.md#pytorch-large-conformer-with-specaug--speed-perturbation-8-gpus--transformer-lm-4-gpus&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;strong&gt;ESPnet2&lt;/strong&gt; Librispeech dev_clean/dev_other/test_clean/test_other&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;0.6/1.5/0.6/1.4&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1.7/3.4/1.8/3.6&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1#self-supervised-learning-features-hubert_large_ll60k-conformer-utt_mvn-with-transformer-lm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Switchboard (eval2000) callhm/swbd&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;14.0/6.8&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/swbd/asr1/RESULTS.md#conformer-with-bpe-2000-specaug-speed-perturbation-transformer-lm-decoding&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;TEDLIUM2 dev/test&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;8.6/7.2&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/tedlium2/asr1/RESULTS.md#conformer-large-model--specaug--speed-perturbation--rnnlm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;strong&gt;ESPnet2&lt;/strong&gt; TEDLIUM2 dev/test&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;7.3/7.1&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs2/tedlium2/asr1/README.md#e-branchformer-12-encoder-layers&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;TEDLIUM3 dev/test&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;9.6/7.6&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/tedlium3/asr1/RESULTS.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;WSJ dev93/eval92&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.2/2.1&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;7.0/4.7&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;strong&gt;ESPnet2&lt;/strong&gt; WSJ dev93/eval92&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1.1/0.8&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;2.8/1.8&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/wsj/asr1#self-supervised-learning-features-wav2vec2_large_ll60k-conformer-utt_mvn-with-transformer-lm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;p&gt;Note that the performance of the CSJ, HKUST, and Librispeech tasks was significantly improved by using the wide network (#units = 1024) and large subword units if necessary reported by &lt;a href=&#34;https://arxiv.org/pdf/1805.03294.pdf&#34;&gt;RWTH&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;If you want to check the results of the other recipes, please check &lt;code&gt;egs/&amp;lt;name_of_recipe&amp;gt;/asr1/RESULTS.md&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;ASR demo&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;expand&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;You can recognize speech in a WAV file using pretrained models. Go to a recipe directory and run &lt;code&gt;utils/recog_wav.sh&lt;/code&gt; as follows:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# go to recipe directory and source path of espnet tools&#xA;cd egs/tedlium2/asr1 &amp;amp;&amp;amp; . ./path.sh&#xA;# let&#39;s recognize speech!&#xA;recog_wav.sh --models tedlium2.transformer.v1 example.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;where &lt;code&gt;example.wav&lt;/code&gt; is a WAV file to be recognized. The sampling rate must be consistent with that of data used in training.&lt;/p&gt; &#xA;  &lt;p&gt;Available pretrained models in the demo script are listed as below.&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Notes&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1UqIY6WJMZ4sxNxSugUqp3mrGb3j6h7xe&#34;&gt;tedlium2.rnn.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Streaming decoding based on CTC-based VAD&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1cac5Uc09lJrCYfWkLQsF8eapQcxZnYdf&#34;&gt;tedlium2.rnn.v2&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Streaming decoding based on CTC-based VAD (batch decoding)&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1cVeSOYY1twOfL9Gns7Z3ZDnkrJqNwPow&#34;&gt;tedlium2.transformer.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Joint-CTC attention Transformer trained on Tedlium 2&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1zcPglHAKILwVgfACoMWWERiyIquzSYuU&#34;&gt;tedlium3.transformer.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Joint-CTC attention Transformer trained on Tedlium 3&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1BtQvAnsFvVi-dp_qsaFP7n4A_5cwnlR6&#34;&gt;librispeech.transformer.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Joint-CTC attention Transformer trained on Librispeech&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1tWccl6aYU67kbtkm8jv5H6xayqg1rzjh&#34;&gt;commonvoice.transformer.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Joint-CTC attention Transformer trained on CommonVoice&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=120nUQcSsKeY5dpyMWw_kI33ooMRGT2uF&#34;&gt;csj.transformer.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Joint-CTC attention Transformer trained on CSJ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1ALvD4nHan9VDJlYJwNurVr7H7OV0j2X9&#34;&gt;csj.rnn.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Joint-CTC attention VGGBLSTM trained on CSJ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;SE results&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;expand&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;We list results from three different models on WSJ0-2mix, which is one the most widely used benchmark dataset for speech separation.&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;STOI&lt;/th&gt; &#xA;     &lt;th&gt;SAR&lt;/th&gt; &#xA;     &lt;th&gt;SDR&lt;/th&gt; &#xA;     &lt;th&gt;SIR&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;a href=&#34;https://zenodo.org/record/4498554&#34;&gt;TF Masking&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td&gt;0.89&lt;/td&gt; &#xA;     &lt;td&gt;11.40&lt;/td&gt; &#xA;     &lt;td&gt;10.24&lt;/td&gt; &#xA;     &lt;td&gt;18.04&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;a href=&#34;https://zenodo.org/record/4498562&#34;&gt;Conv-Tasnet&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td&gt;0.95&lt;/td&gt; &#xA;     &lt;td&gt;16.62&lt;/td&gt; &#xA;     &lt;td&gt;15.94&lt;/td&gt; &#xA;     &lt;td&gt;25.90&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;a href=&#34;https://zenodo.org/record/4688000&#34;&gt;DPRNN-Tasnet&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td&gt;0.96&lt;/td&gt; &#xA;     &lt;td&gt;18.82&lt;/td&gt; &#xA;     &lt;td&gt;18.29&lt;/td&gt; &#xA;     &lt;td&gt;28.92&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;SE demos&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;expand&lt;/summary&gt;&#xA; &lt;div&gt;&#xA;   You can try the interactive demo with Google Colab. Please click the following button to get access to the demos. &#xA;  &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;p&gt;It is based on ESPnet2. Pretrained models are available for both speech enhancement and speech separation tasks.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;ST results&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;expand&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;We list 4-gram BLEU of major ST tasks.&lt;/p&gt; &#xA;  &lt;h4&gt;end-to-end system&lt;/h4&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;Task&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;BLEU&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Pretrained model&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Fisher-CallHome Spanish fisher_test (Es-&amp;gt;En)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;51.03&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Fisher-CallHome Spanish callhome_evltest (Es-&amp;gt;En)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;20.44&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Libri-trans test (En-&amp;gt;Fr)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;16.70&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/libri_trans/st1/RESULTS.md#train_spfr_lc_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans-1&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;How2 dev5 (En-&amp;gt;Pt)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;45.68&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/how2/st1/RESULTS.md#trainpt_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans-1&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Must-C tst-COMMON (En-&amp;gt;De)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;22.91&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/must_c/st1/RESULTS.md#train_spen-dede_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Mboshi-French dev (Fr-&amp;gt;Mboshi)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;6.18&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;h4&gt;cascaded system&lt;/h4&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;Task&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;BLEU&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Pretrained model&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Fisher-CallHome Spanish fisher_test (Es-&amp;gt;En)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;42.16&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Fisher-CallHome Spanish callhome_evltest (Es-&amp;gt;En)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;19.82&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Libri-trans test (En-&amp;gt;Fr)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;16.96&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;How2 dev5 (En-&amp;gt;Pt)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;44.90&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Must-C tst-COMMON (En-&amp;gt;De)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;23.65&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;p&gt;If you want to check the results of the other recipes, please check &lt;code&gt;egs/&amp;lt;name_of_recipe&amp;gt;/st1/RESULTS.md&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;ST demo&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;expand&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;(&lt;strong&gt;New!&lt;/strong&gt;) We made a new real-time E2E-ST + TTS demonstration in Google Colab. Please access the notebook from the following button and enjoy the real-time speech-to-speech translation!&lt;/p&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;p&gt;You can translate speech in a WAV file using pretrained models. Go to a recipe directory and run &lt;code&gt;utils/translate_wav.sh&lt;/code&gt; as follows:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# go to recipe directory and source path of espnet tools&#xA;cd egs/fisher_callhome_spanish/st1 &amp;amp;&amp;amp; . ./path.sh&#xA;# download example wav file&#xA;wget -O - https://github.com/espnet/espnet/files/4100928/test.wav.tar.gz | tar zxvf -&#xA;# let&#39;s translate speech!&#xA;translate_wav.sh --models fisher_callhome_spanish.transformer.v1.es-en test.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;where &lt;code&gt;test.wav&lt;/code&gt; is a WAV file to be translated. The sampling rate must be consistent with that of data used in training.&lt;/p&gt; &#xA;  &lt;p&gt;Available pretrained models in the demo script are listed as below.&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Notes&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1hawp5ZLw4_SIHIT3edglxbKIIkPVe8n3&#34;&gt;fisher_callhome_spanish.transformer.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Transformer-ST trained on Fisher-CallHome Spanish Es-&amp;gt;En&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;MT results&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;expand&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;Task&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;BLEU&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Pretrained model&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Fisher-CallHome Spanish fisher_test (Es-&amp;gt;En)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;61.45&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Fisher-CallHome Spanish callhome_evltest (Es-&amp;gt;En)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;29.86&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Libri-trans test (En-&amp;gt;Fr)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;18.09&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/libri_trans/mt1/RESULTS.md#trainfr_lcrm_tc_pytorch_train_pytorch_transformer_bpe1000&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;How2 dev5 (En-&amp;gt;Pt)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;58.61&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/how2/mt1/RESULTS.md#trainpt_tc_tc_pytorch_train_pytorch_transformer_bpe8000&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Must-C tst-COMMON (En-&amp;gt;De)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;27.63&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/must_c/mt1/RESULTS.md#summary-4-gram-bleu&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;IWSLT&#39;14 test2014 (En-&amp;gt;De)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;24.70&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/iwslt16/mt1/RESULTS.md#result&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;IWSLT&#39;14 test2014 (De-&amp;gt;En)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;29.22&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/iwslt16/mt1/RESULTS.md#result&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;IWSLT&#39;14 test2014 (De-&amp;gt;En)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;32.2&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs2/iwslt14/mt1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;IWSLT&#39;16 test2014 (En-&amp;gt;De)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;24.05&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/iwslt16/mt1/RESULTS.md#result&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;IWSLT&#39;16 test2014 (De-&amp;gt;En)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;29.13&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/raw/master/egs/iwslt16/mt1/RESULTS.md#result&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;TTS results&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;ESPnet2&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;You can listen to the generated samples in the following URL.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1H3fnlBbWMEkQUfrHqosKN_ZX_WjO29ma?usp=sharing&#34;&gt;ESPnet2 TTS generated samples&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Note that in the generation we use Griffin-Lim (&lt;code&gt;wav/&lt;/code&gt;) and &lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;Parallel WaveGAN&lt;/a&gt; (&lt;code&gt;wav_pwg/&lt;/code&gt;).&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;p&gt;You can download pretrained models via &lt;code&gt;espnet_model_zoo&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/espnet/espnet_model_zoo&#34;&gt;ESPnet model zoo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/espnet/espnet_model_zoo/raw/master/espnet_model_zoo/table.csv&#34;&gt;Pretrained model list&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;You can download pretrained vocoders via &lt;code&gt;kan-bayashi/ParallelWaveGAN&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;kan-bayashi/ParallelWaveGAN&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN#results&#34;&gt;Pretrained vocoder list&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;ESPnet1&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest results in the above ESPnet2 results.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;p&gt;You can listen to our samples in demo HP &lt;a href=&#34;https://espnet.github.io/espnet-tts-sample/&#34;&gt;espnet-tts-sample&lt;/a&gt;. Here we list some notable ones:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=18JgsOCWiP_JkhONasTplnHS7yaF_konr&#34;&gt;Single English speaker Tacotron2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1fEgS4-K4dtgVxwI4Pr7uOA1h4PE-zN7f&#34;&gt;Single Japanese speaker Tacotron2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1q_66kyxVZGU99g8Xb5a0Q8yZ1YVm2tN0&#34;&gt;Single other language speaker Tacotron2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=18S_B8Ogogij34rIfJOeNF8D--uG7amz2&#34;&gt;Multi English speaker Tacotron2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=14EboYVsMVcAq__dFP1p6lyoZtdobIL1X&#34;&gt;Single English speaker Transformer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1PSxs1VauIndwi8d5hJmZlppGRVu2zuy5&#34;&gt;Single English speaker FastSpeech&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1_vrdqjM43DdN1Qz7HJkvMQ6lCMmWLeGp&#34;&gt;Multi English speaker Transformer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=13I5V2w7deYFX4DlVk1-0JfaXmUR2rNOv&#34;&gt;Single Italian speaker FastSpeech&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD&#34;&gt;Single Mandarin speaker Transformer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1Ol_048Tuy6BgvYm1RpjhOX4HfhUeBqdK&#34;&gt;Single Mandarin speaker FastSpeech&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1fFMQDF6NV5Ysz48QLFYE8fEvbAxCsMBw&#34;&gt;Multi Japanese speaker Transformer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1HvB0_LDf1PVinJdehiuCt5gWmXGguqtx&#34;&gt;Single English speaker models with Parallel WaveGAN&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1wG-Y0itVYalxuLAHdkAHO7w1CWFfRPF4&#34;&gt;Single English speaker knowledge distillation-based FastSpeech&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;You can download all of the pretrained models and generated samples:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF&#34;&gt;All of the pretrained E2E-TTS models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1bQGuqH92xuxOX__reWLP4-cif0cbpMLX&#34;&gt;All of the generated samples&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;Note that in the generated samples we use the following vocoders: Griffin-Lim (&lt;strong&gt;GL&lt;/strong&gt;), WaveNet vocoder (&lt;strong&gt;WaveNet&lt;/strong&gt;), Parallel WaveGAN (&lt;strong&gt;ParallelWaveGAN&lt;/strong&gt;), and MelGAN (&lt;strong&gt;MelGAN&lt;/strong&gt;). The neural vocoders are based on following repositories.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;kan-bayashi/ParallelWaveGAN&lt;/a&gt;: Parallel WaveGAN / MelGAN / Multi-band MelGAN&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34;&gt;r9y9/wavenet_vocoder&lt;/a&gt;: 16 bit mixture of Logistics WaveNet vocoder&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/kan-bayashi/PytorchWaveNetVocoder&#34;&gt;kan-bayashi/PytorchWaveNetVocoder&lt;/a&gt;: 8 bit Softmax WaveNet Vocoder with the noise shaping&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;If you want to build your own neural vocoder, please check the above repositories. &lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;kan-bayashi/ParallelWaveGAN&lt;/a&gt; provides &lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN#decoding-with-espnet-tts-models-features&#34;&gt;the manual&lt;/a&gt; about how to decode ESPnet-TTS model&#39;s features with neural vocoders. Please check it.&lt;/p&gt; &#xA;  &lt;p&gt;Here we list all of the pretrained neural vocoders. Please download and enjoy the generation of high quality speech!&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model link&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Lang&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Fs [Hz]&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Mel range [Hz]&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;FFT / Shift / Win [pt]&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model type&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1eA1VcRS9jzFa-DovyTgJLQ_jmwOLIi8L&#34;&gt;ljspeech.wavenet.softmax.ns.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;EN&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;22.05k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1024 / 256 / None&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/kan-bayashi/PytorchWaveNetVocoder&#34;&gt;Softmax WaveNet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1sY7gEUg39QaO1szuN62-Llst9TrFno2t&#34;&gt;ljspeech.wavenet.mol.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;EN&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;22.05k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1024 / 256 / None&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34;&gt;MoL WaveNet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7&#34;&gt;ljspeech.parallel_wavegan.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;EN&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;22.05k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1024 / 256 / None&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;Parallel WaveGAN&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1es2HuKUeKVtEdq6YDtAsLNpqCy4fhIXr&#34;&gt;ljspeech.wavenet.mol.v2&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;EN&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;22.05k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;80-7600&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1024 / 256 / None&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34;&gt;MoL WaveNet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1Grn7X9wD35UcDJ5F7chwdTqTa4U7DeVB&#34;&gt;ljspeech.parallel_wavegan.v2&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;EN&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;22.05k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;80-7600&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1024 / 256 / None&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;Parallel WaveGAN&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1ipPWYl8FBNRlBFaKj1-i23eQpW_W_YcR&#34;&gt;ljspeech.melgan.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;EN&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;22.05k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;80-7600&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1024 / 256 / None&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;MelGAN&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1_a8faVA5OGCzIcJNw4blQYjfG4oA9VEt&#34;&gt;ljspeech.melgan.v3&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;EN&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;22.05k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;80-7600&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1024 / 256 / None&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;MelGAN&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1jHUUmQFjWiQGyDd7ZeiCThSjjpbF_B4h&#34;&gt;libritts.wavenet.mol.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;EN&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;24k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1024 / 256 / None&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34;&gt;MoL WaveNet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=187xvyNbmJVZ0EZ1XHCdyjZHTXK9EcfkK&#34;&gt;jsut.wavenet.mol.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;JP&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;24k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;80-7600&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;2048 / 300 / 1200&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34;&gt;MoL WaveNet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM&#34;&gt;jsut.parallel_wavegan.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;JP&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;24k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;80-7600&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;2048 / 300 / 1200&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;Parallel WaveGAN&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=1PsjFRV5eUP0HHwBaRYya9smKy5ghXKzj&#34;&gt;csmsc.wavenet.mol.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;ZH&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;24k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;80-7600&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;2048 / 300 / 1200&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34;&gt;MoL WaveNet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy&#34;&gt;csmsc.parallel_wavegan.v1&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;ZH&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;24k&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;80-7600&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;2048 / 300 / 1200&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;Parallel WaveGAN&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;p&gt;If you want to use the above pretrained vocoders, please exactly match the feature setting with them.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;TTS demo&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;ESPnet2&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;You can try the real-time demo in Google Colab. Please access the notebook from the following button and enjoy the real-time synthesis!&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Real-time TTS demo with ESPnet2 &lt;a href=&#34;https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;English, Japanese, and Mandarin models are available in the demo.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;ESPnet1&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest demo in the above ESPnet2 demo.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;p&gt;You can try the real-time demo in Google Colab. Please access the notebook from the following button and enjoy the real-time synthesis.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Real-time TTS demo with ESPnet1 &lt;a href=&#34;https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;We also provide shell script to perform synthesize. Go to a recipe directory and run &lt;code&gt;utils/synth_wav.sh&lt;/code&gt; as follows:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# go to recipe directory and source path of espnet tools&#xA;cd egs/ljspeech/tts1 &amp;amp;&amp;amp; . ./path.sh&#xA;# we use upper-case char sequence for the default model.&#xA;echo &#34;THIS IS A DEMONSTRATION OF TEXT TO SPEECH.&#34; &amp;gt; example.txt&#xA;# let&#39;s synthesize speech!&#xA;synth_wav.sh example.txt&#xA;&#xA;# also you can use multiple sentences&#xA;echo &#34;THIS IS A DEMONSTRATION OF TEXT TO SPEECH.&#34; &amp;gt; example_multi.txt&#xA;echo &#34;TEXT TO SPEECH IS A TECHNIQUE TO CONVERT TEXT INTO SPEECH.&#34; &amp;gt;&amp;gt; example_multi.txt&#xA;synth_wav.sh example_multi.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;You can change the pretrained model as follows:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;synth_wav.sh --models ljspeech.fastspeech.v1 example.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Waveform synthesis is performed with Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN). You can change the pretrained vocoder model as follows:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;synth_wav.sh --vocoder_models ljspeech.wavenet.mol.v1 example.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;WaveNet vocoder provides very high quality speech but it takes time to generate.&lt;/p&gt; &#xA;  &lt;p&gt;See more details or available models via &lt;code&gt;--help&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;synth_wav.sh --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;VC results&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;expand&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Transformer and Tacotron2 based VC&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;You can listen to some samples on the &lt;a href=&#34;https://unilight.github.io/Publication-Demos/publications/transformer-vc/&#34;&gt;demo webpage&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Cascade ASR+TTS as one of the baseline systems of VCC2020&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;The &lt;a href=&#34;http://www.vc-challenge.org/&#34;&gt;Voice Conversion Challenge 2020&lt;/a&gt; (VCC2020) adopts ESPnet to build an end-to-end based baseline system. In VCC2020, the objective is intra/cross lingual nonparallel VC. You can download converted samples of the cascade ASR+TTS baseline system &lt;a href=&#34;https://drive.google.com/drive/folders/1oeZo83GrOgtqxGwF7KagzIrfjr8X59Ue?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;SLU results&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;expand&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;We list the performance on various SLU tasks and dataset using the metric reported in the original dataset paper&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;Task&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Metric&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Result&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Pretrained Model&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;SLURP&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;86.3&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/slurp/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;FSC&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;99.6&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/fsc/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;FSC Unseen Speaker Set&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;98.6&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;FSC Unseen Utterance Set&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;86.4&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;FSC Challenge Speaker Set&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;97.5&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;FSC Challenge Utterance Set&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;78.5&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;SNIPS&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;F1&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;91.7&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/snips/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Grabo (Nl)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;97.2&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/grabo/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;CAT SLU MAP (Zn)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;78.9&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/catslu/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intent Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Google Speech Commands&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;98.4&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/speechcommands/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Slot Filling&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;SLURP&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;SLU-F1&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;71.9&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/slurp_entity/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Dialogue Act Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Switchboard&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;67.5&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/swbd_da/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Dialogue Act Classification&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Jdcinal (Jp)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;67.4&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/jdcinal/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Emotion Recognition&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;IEMOCAP&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Acc&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;69.4&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/iemocap/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Emotion Recognition&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;swbd_sentiment&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Macro F1&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;61.4&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/swbd_sentiment/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Emotion Recognition&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;slue_voxceleb&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Macro F1&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;44.0&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/espnet/espnet/tree/master/egs2/slue-voxceleb/asr1/README.md&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;p&gt;If you want to check the results of the other recipes, please check &lt;code&gt;egs2/&amp;lt;name_of_recipe&amp;gt;/asr1/RESULTS.md&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;CTC Segmentation demo&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;ESPnet1&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.09127&#34;&gt;CTC segmentation&lt;/a&gt; determines utterance segments within audio files. Aligned utterance segments constitute the labels of speech datasets.&lt;/p&gt; &#xA;  &lt;p&gt;As demo, we align start and end of utterances within the audio file &lt;code&gt;ctc_align_test.wav&lt;/code&gt;, using the example script &lt;code&gt;utils/asr_align_wav.sh&lt;/code&gt;. For preparation, set up a data directory:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd egs/tedlium2/align1/&#xA;# data directory&#xA;align_dir=data/demo&#xA;mkdir -p ${align_dir}&#xA;# wav file&#xA;base=ctc_align_test&#xA;wav=../../../test_utils/${base}.wav&#xA;# recipe files&#xA;echo &#34;batchsize: 0&#34; &amp;gt; ${align_dir}/align.yaml&#xA;&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; ${align_dir}/utt_text&#xA;${base} THE SALE OF THE HOTELS&#xA;${base} IS PART OF HOLIDAY&#39;S STRATEGY&#xA;${base} TO SELL OFF ASSETS&#xA;${base} AND CONCENTRATE&#xA;${base} ON PROPERTY MANAGEMENT&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Here, &lt;code&gt;utt_text&lt;/code&gt; is the file containing the list of utterances. Choose a pre-trained ASR model that includes a CTC layer to find utterance segments:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# pre-trained ASR model&#xA;model=wsj.transformer_small.v1&#xA;mkdir ./conf &amp;amp;&amp;amp; cp ../../wsj/asr1/conf/no_preprocess.yaml ./conf&#xA;&#xA;../../../utils/asr_align_wav.sh \&#xA;    --models ${model} \&#xA;    --align_dir ${align_dir} \&#xA;    --align_config ${align_dir}/align.yaml \&#xA;    ${wav} ${align_dir}/utt_text&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Segments are written to &lt;code&gt;aligned_segments&lt;/code&gt; as a list of file/utterance name, utterance start and end times in seconds and a confidence score. The confidence score is a probability in log space that indicates how good the utterance was aligned. If needed, remove bad utterances:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;min_confidence_score=-5&#xA;awk -v ms=${min_confidence_score} &#39;{ if ($5 &amp;gt; ms) {print} }&#39; ${align_dir}/aligned_segments&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;The demo script &lt;code&gt;utils/ctc_align_wav.sh&lt;/code&gt; uses an already pretrained ASR model (see list above for more models). It is recommended to use models with RNN-based encoders (such as BLSTMP) for aligning large audio files; rather than using Transformer models that have a high memory consumption on longer audio data. The sample rate of the audio must be consistent with that of the data used in training; adjust with &lt;code&gt;sox&lt;/code&gt; if needed. A full example recipe is in &lt;code&gt;egs/tedlium2/align1/&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;ESPnet2&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.09127&#34;&gt;CTC segmentation&lt;/a&gt; determines utterance segments within audio files. Aligned utterance segments constitute the labels of speech datasets.&lt;/p&gt; &#xA;  &lt;p&gt;As demo, we align start and end of utterances within the audio file &lt;code&gt;ctc_align_test.wav&lt;/code&gt;. This can be done either directly from the Python command line or using the script &lt;code&gt;espnet2/bin/asr_align.py&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;From the Python command line interface:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load a model with character tokens&#xA;from espnet_model_zoo.downloader import ModelDownloader&#xA;d = ModelDownloader(cachedir=&#34;./modelcache&#34;)&#xA;wsjmodel = d.download_and_unpack(&#34;kamo-naoyuki/wsj&#34;)&#xA;# load the example file included in the ESPnet repository&#xA;import soundfile&#xA;speech, rate = soundfile.read(&#34;./test_utils/ctc_align_test.wav&#34;)&#xA;# CTC segmentation&#xA;from espnet2.bin.asr_align import CTCSegmentation&#xA;aligner = CTCSegmentation( **wsjmodel , fs=rate )&#xA;text = &#34;&#34;&#34;&#xA;utt1 THE SALE OF THE HOTELS&#xA;utt2 IS PART OF HOLIDAY&#39;S STRATEGY&#xA;utt3 TO SELL OFF ASSETS&#xA;utt4 AND CONCENTRATE ON PROPERTY MANAGEMENT&#xA;&#34;&#34;&#34;&#xA;segments = aligner(speech, text)&#xA;print(segments)&#xA;# utt1 utt 0.26 1.73 -0.0154 THE SALE OF THE HOTELS&#xA;# utt2 utt 1.73 3.19 -0.7674 IS PART OF HOLIDAY&#39;S STRATEGY&#xA;# utt3 utt 3.19 4.20 -0.7433 TO SELL OFF ASSETS&#xA;# utt4 utt 4.20 6.10 -0.4899 AND CONCENTRATE ON PROPERTY MANAGEMENT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Aligning also works with fragments of the text. For this, set the &lt;code&gt;gratis_blank&lt;/code&gt; option that allows skipping unrelated audio sections without penalty. It&#39;s also possible to omit the utterance names at the beginning of each line, by setting &lt;code&gt;kaldi_style_text&lt;/code&gt; to False.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;aligner.set_config( gratis_blank=True, kaldi_style_text=False )&#xA;text = [&#34;SALE OF THE HOTELS&#34;, &#34;PROPERTY MANAGEMENT&#34;]&#xA;segments = aligner(speech, text)&#xA;print(segments)&#xA;# utt_0000 utt 0.37 1.72 -2.0651 SALE OF THE HOTELS&#xA;# utt_0001 utt 4.70 6.10 -5.0566 PROPERTY MANAGEMENT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;The script &lt;code&gt;espnet2/bin/asr_align.py&lt;/code&gt; uses a similar interface. To align utterances:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# ASR model and config files from pretrained model (e.g. from cachedir):&#xA;asr_config=&amp;lt;path-to-model&amp;gt;/config.yaml&#xA;asr_model=&amp;lt;path-to-model&amp;gt;/valid.*best.pth&#xA;# prepare the text file&#xA;wav=&#34;test_utils/ctc_align_test.wav&#34;&#xA;text=&#34;test_utils/ctc_align_text.txt&#34;&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; ${text}&#xA;utt1 THE SALE OF THE HOTELS&#xA;utt2 IS PART OF HOLIDAY&#39;S STRATEGY&#xA;utt3 TO SELL OFF ASSETS&#xA;utt4 AND CONCENTRATE&#xA;utt5 ON PROPERTY MANAGEMENT&#xA;EOF&#xA;# obtain alignments:&#xA;python espnet2/bin/asr_align.py --asr_train_config ${asr_config} --asr_model_file ${asr_model} --audio ${wav} --text ${text}&#xA;# utt1 ctc_align_test 0.26 1.73 -0.0154 THE SALE OF THE HOTELS&#xA;# utt2 ctc_align_test 1.73 3.19 -0.7674 IS PART OF HOLIDAY&#39;S STRATEGY&#xA;# utt3 ctc_align_test 3.19 4.20 -0.7433 TO SELL OFF ASSETS&#xA;# utt4 ctc_align_test 4.20 4.97 -0.6017 AND CONCENTRATE&#xA;# utt5 ctc_align_test 4.97 6.10 -0.3477 ON PROPERTY MANAGEMENT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;The output of the script can be redirected to a &lt;code&gt;segments&lt;/code&gt; file by adding the argument &lt;code&gt;--output segments&lt;/code&gt;. Each line contains file/utterance name, utterance start and end times in seconds and a confidence score; optionally also the utterance text. The confidence score is a probability in log space that indicates how good the utterance was aligned. If needed, remove bad utterances:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;min_confidence_score=-7&#xA;# here, we assume that the output was written to the file `segments`&#xA;awk -v ms=${min_confidence_score} &#39;{ if ($5 &amp;gt; ms) {print} }&#39; segments&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;See the module documentation for more information. It is recommended to use models with RNN-based encoders (such as BLSTMP) for aligning large audio files; rather than using Transformer models that have a high memory consumption on longer audio data. The sample rate of the audio must be consistent with that of the data used in training; adjust with &lt;code&gt;sox&lt;/code&gt; if needed.&lt;/p&gt; &#xA;  &lt;p&gt;Also, we can use this tool to provide token-level segmentation information if we prepare a list of tokens instead of that of utterances in the &lt;code&gt;text&lt;/code&gt; file. See the discussion in &lt;a href=&#34;https://github.com/espnet/espnet/issues/4278#issuecomment-1100756463&#34;&gt;https://github.com/espnet/espnet/issues/4278#issuecomment-1100756463&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{watanabe2018espnet,&#xA;  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},&#xA;  title={{ESPnet}: End-to-End Speech Processing Toolkit},&#xA;  year={2018},&#xA;  booktitle={Proceedings of Interspeech},&#xA;  pages={2207--2211},&#xA;  doi={10.21437/Interspeech.2018-1456},&#xA;  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}&#xA;}&#xA;@inproceedings{hayashi2020espnet,&#xA;  title={{Espnet-TTS}: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit},&#xA;  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Inoue, Katsuki and Yoshimura, Takenori and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya and Zhang, Yu and Tan, Xu},&#xA;  booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},&#xA;  pages={7654--7658},&#xA;  year={2020},&#xA;  organization={IEEE}&#xA;}&#xA;@inproceedings{inaguma-etal-2020-espnet,&#xA;    title = &#34;{ESP}net-{ST}: All-in-One Speech Translation Toolkit&#34;,&#xA;    author = &#34;Inaguma, Hirofumi  and&#xA;      Kiyono, Shun  and&#xA;      Duh, Kevin  and&#xA;      Karita, Shigeki  and&#xA;      Yalta, Nelson  and&#xA;      Hayashi, Tomoki  and&#xA;      Watanabe, Shinji&#34;,&#xA;    booktitle = &#34;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations&#34;,&#xA;    month = jul,&#xA;    year = &#34;2020&#34;,&#xA;    address = &#34;Online&#34;,&#xA;    publisher = &#34;Association for Computational Linguistics&#34;,&#xA;    url = &#34;https://www.aclweb.org/anthology/2020.acl-demos.34&#34;,&#xA;    pages = &#34;302--311&#34;,&#xA;}&#xA;@inproceedings{li2020espnet,&#xA;  title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration},&#xA;  author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe},&#xA;  booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},&#xA;  pages={785--792},&#xA;  year={2021},&#xA;  organization={IEEE},&#xA;}&#xA;@article{arora2021espnet,&#xA;  title={ESPnet-SLU: Advancing Spoken Language Understanding through ESPnet},&#xA;  author={Arora, Siddhant and Dalmia, Siddharth and Denisov, Pavel and Chang, Xuankai and Ueda, Yushi and Peng, Yifan and Zhang, Yuekai and Kumar, Sujay and Ganesan, Karthik and Yan, Brian and others},&#xA;  journal={arXiv preprint arXiv:2111.14706},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>robusta-dev/robusta</title>
    <updated>2023-01-21T01:42:40Z</updated>
    <id>tag:github.com,2023-01-21:/robusta-dev/robusta</id>
    <link href="https://github.com/robusta-dev/robusta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kubernetes observability and automation, with an awesome Prometheus integration&lt;/p&gt;&lt;hr&gt;&lt;div id=&#34;top&#34;&gt;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a href=&#34;https://home.robusta.dev/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/robusta-dev/robusta/master/logos/Robusta_readme.png&#34; alt=&#34;Robusta.dev&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;h2&gt;Keep your Kubernetes microservices up and running&lt;/h2&gt; &#xA; &lt;h3&gt;Connect your existing Prometheus, gain 360¬∞ observability&lt;/h3&gt; &#xA; &lt;p&gt;(Prometheus recommended, but not required)&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://twitter.com/RobustaDev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/RobustaDev?logo=twitter&amp;amp;color=blue&amp;amp;label=@RobustaDev&amp;amp;style=flat-square&#34; alt=&#34;twitter robusta&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bit.ly/robusta-slack&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-Join-4A154B?style=flat-square&amp;amp;logo=slack&amp;amp;logoColor=white&#34; alt=&#34;slack robusta&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/company/robusta-dev/&#34;&gt;&lt;img alt=&#34;LinkedIn&#34; title=&#34;LinkedIn&#34; src=&#34;https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&amp;amp;logo=Linkedin&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCeLrAOI3anJAfO3BrYVB62Q&#34;&gt;&lt;img alt=&#34;Youtube&#34; title=&#34;Youtube&#34; src=&#34;https://img.shields.io/youtube/channel/subscribers/UCeLrAOI3anJAfO3BrYVB62Q?color=%23ff0000&amp;amp;label=Robusta%20Dev&amp;amp;logo=youtube&amp;amp;logoColor=%23ff0000&amp;amp;style=flat-square&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üíª About the project&lt;/h2&gt; &#xA;&lt;p&gt;Robusta is both an automations engine for Kubernetes, and a &lt;a href=&#34;https://home.robusta.dev/&#34;&gt;multi-cluster observability platform&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Robusta is commonly used alongside Prometheus, but other tools are supported too.&lt;/p&gt; &#xA;&lt;p&gt;By listening to all the events in your cluster, Robusta can tell you &lt;em&gt;why&lt;/em&gt; alerts fired, what happened at the same time, and what you can do about it.&lt;/p&gt; &#xA;&lt;p&gt;Robusta can either improve your existing alerts, or be used to define new alerts triggered by APIServer changes.&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è How it works&lt;/h2&gt; &#xA;&lt;p&gt;Robusta&#39;s behaviour is defined by rules like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;triggers:&#xA;  - on_prometheus_alert:&#xA;      alert_name: KubePodCrashLooping&#xA;actions:&#xA;  - logs_enricher: {}&#xA;sinks:&#xA;  - slack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the above example, whenever the &lt;code&gt;KubePodCrashLooping&lt;/code&gt; alert fires, Robusta will fetch logs from the right pod and attach them to the alert. The result looks like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/robusta-dev/robusta/master/docs/images/crash-report.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Robusta also supports alert-remediations:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/robusta-dev/robusta/master/docs/images/alert_on_hpa_reached_limit1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.robusta.dev/master/catalog/actions/index.html&#34;&gt;Over 50 types of automations and enrichments are built-in ¬ª&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/robusta-dev/robusta/master/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;üìí Installing Robusta&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install our python cli:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python3 -m pip install -U robusta-cli --no-cache&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Generate a values file for Helm:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;robusta gen-config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install Robusta with Helm:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;helm repo add robusta https://robusta-charts.storage.googleapis.com &amp;amp;&amp;amp; helm repo update&#xA;helm install robusta robusta/robusta -f ./generated_values.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.robusta.dev/master/installation.html&#34;&gt;Detailed instructions ¬ª&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- &lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; --&gt; &#xA;&lt;h2&gt;üìù Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Interested? Learn more about Robusta&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.robusta.dev/master/architecture.html&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.robusta.dev/master/upgrade.html&#34;&gt;Upgrade and Uninstall&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.robusta.dev/master/user-guide/configuration.html&#34;&gt;Configuration Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.robusta.dev/master/catalog/triggers/index.html&#34;&gt;Triggers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.robusta.dev/master/index.html&#34;&gt;Full documentation ¬ª&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/robusta-dev/robusta/master/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;‚úâÔ∏è Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Slack - &lt;a href=&#34;https://bit.ly/robusta-slack&#34;&gt;robustacommunity.slack.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Twitter - &lt;a href=&#34;https://twitter.com/RobustaDev&#34;&gt;@RobustaDev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LinkedIn - &lt;a href=&#34;https://www.linkedin.com/company/robusta-dev/&#34;&gt;robusta-dev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Jobs - &lt;a href=&#34;mailto:jobs@robusta.dev&#34;&gt;jobs@robusta.dev &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Email Support - &lt;a href=&#34;mailto:support@robusta.dev&#34;&gt;support@robusta.dev &lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/robusta-dev/robusta/master/#top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;üìë License&lt;/h2&gt; &#xA;&lt;p&gt;Robusta is distributed under the MIT License. See &lt;a href=&#34;https://github.com/robusta-dev/robusta/raw/master/LICENSE&#34;&gt;LICENSE.md&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;üïê Stay up to date&lt;/h2&gt; &#xA;&lt;p&gt;We add new features regularly. Stay up to date by watching us on GitHub.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/robusta-dev/robusta/master/docs/images/star-repo.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>