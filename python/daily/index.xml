<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-05T01:45:16Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/hiera</title>
    <updated>2023-06-05T01:45:16Z</updated>
    <id>tag:github.com,2023-06-05:/facebookresearch/hiera</id>
    <link href="https://github.com/facebookresearch/hiera" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hiera: A fast, powerful, and simple hierarchical vision transformer.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-CC%20BY--NC%204.0-lightgrey&#34; alt=&#34;CC BY-NC 4.0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation for our ICML 2023 Oral paper:&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00989/&#34;&gt;Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://scholar.google.com/citations?user=4LWx24UAAAAJ&#34;&gt;Chaitanya Ryali&lt;/a&gt;*, &lt;a href=&#34;https://scholar.google.com/citations?user=aMpbemkAAAAJ&#34;&gt;Yuan-Ting Hu&lt;/a&gt;*, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=K3ht_ZUAAAAJ&#34;&gt;Daniel Bolya&lt;/a&gt;*, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=LHQGpBUAAAAJ&#34;&gt;Chen Wei&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=76B8lrgAAAAJ&#34;&gt;Haoqi Fan&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=E8K25LIAAAAJ&#34;&gt;Po-Yao Huang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=Qwm6ZOYAAAAJ&#34;&gt;Vaibhav Aggarwal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=42v1i_YAAAAJ&#34;&gt;Arkabandhu Chowdhury&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=Ugw9DX0AAAAJ&#34;&gt;Omid Poursaeed&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=mqpjAt4AAAAJ&#34;&gt;Judy Hoffman&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=oY9R5YQAAAAJ&#34;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=-VgS8AIAAAAJ&#34;&gt;Yanghao Li&lt;/a&gt;*, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=UxuqG1EAAAAJ&#34;&gt;Christoph Feichtenhofer&lt;/a&gt;*&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://icml.cc/Conferences/2023&#34;&gt;ICML &#39;23 Oral&lt;/a&gt;&lt;/em&gt; | &lt;em&gt;&lt;a href=&#34;https://github.com/facebookresearch/hiera&#34;&gt;GitHub&lt;/a&gt;&lt;/em&gt; | &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00989/&#34;&gt;arXiv&lt;/a&gt;&lt;/em&gt; | &lt;em&gt;&lt;a href=&#34;https://github.com/facebookresearch/hiera#citation&#34;&gt;BibTeX&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;*: Equal contribution.&lt;/p&gt; &#xA;&lt;h2&gt;What is Hiera?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hiera&lt;/strong&gt; is a &lt;em&gt;hierarchical&lt;/em&gt; vision transformer that is fast, powerful, and, above all, &lt;em&gt;simple&lt;/em&gt;. It outperforms the state-of-the-art across a wide array of image and video tasks &lt;em&gt;while being much faster&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/hiera/raw/main/examples/img/inference_speed.png&#34; width=&#34;75%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;How does it work?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/hiera/raw/main/examples/img/hiera_arch.png&#34; alt=&#34;A diagram of Hiera&#39;s architecture.&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Vision transformers like &lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;ViT&lt;/a&gt; use the same spatial resolution and number of features throughout the whole network. But this is inefficient: the early layers don&#39;t need that many features, and the later layers don&#39;t need that much spatial resolution. Prior hierarchical models like &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34;&gt;ResNet&lt;/a&gt; accounted for this by using fewer features at the start and less spatial resolution at the end.&lt;/p&gt; &#xA;&lt;p&gt;Several domain specific vision transformers have been introduced that employ this hierarchical design, such as &lt;a href=&#34;https://arxiv.org/abs/2103.14030&#34;&gt;Swin&lt;/a&gt; or &lt;a href=&#34;https://arxiv.org/abs/2104.11227&#34;&gt;MViT&lt;/a&gt;. But in the pursuit of state-of-the-art results using fully supervised training on ImageNet-1K, these models have become more and more complicated as they add specialized modules to make up for spatial biases that ViTs lack. While these changes produce effective models with attractive FLOP counts, under the hood the added complexity makes these models &lt;em&gt;slower&lt;/em&gt; overall.&lt;/p&gt; &#xA;&lt;p&gt;We show that a lot of this bulk is actually &lt;em&gt;unnecessary&lt;/em&gt;. Instead of manually adding spatial bases through architectural changes, we opt to &lt;em&gt;teach&lt;/em&gt; the model these biases instead. By training with &lt;a href=&#34;https://arxiv.org/abs/2111.06377&#34;&gt;MAE&lt;/a&gt;, we can simplify or remove &lt;em&gt;all&lt;/em&gt; of these bulky modules in existing transformers and &lt;em&gt;increase accuracy&lt;/em&gt; in the process. The result is Hiera, an extremely efficient and simple architecture that outperforms the state-of-the-art in several image and video recognition tasks.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.06.01]&lt;/strong&gt; Initial release.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Hiera requires a reasonably recent version of &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;torch&lt;/a&gt;. After that, you can install hiera through &lt;a href=&#34;https://pypi.org/project/hiera-transformer/0.1.0/&#34;&gt;pip&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install hiera-transformer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This repo &lt;em&gt;should&lt;/em&gt; support the latest timm version, but timm is a constantly updating package. Create an issue if you have problems with a newer version of timm.&lt;/p&gt; &#xA;&lt;h3&gt;Installing from Source&lt;/h3&gt; &#xA;&lt;p&gt;If using &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#model-zoo&#34;&gt;torch hub&lt;/a&gt;, you don&#39;t need to install the &lt;code&gt;hiera&lt;/code&gt; package. But, if you&#39;d like to develop using hiera, it could be a good idea to install it from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/facebookresearch/hiera.git&#xA;cd hiera&#xA;python setup.by build develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;Here we provide model checkpoints for Hiera. Each model listed is accessible on &lt;a href=&#34;https://pytorch.org/docs/stable/hub.html&#34;&gt;torch hub&lt;/a&gt;, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;model = torch.hub.load(&#34;facebookresearch/hiera&#34;, model=&#34;hiera_base_224&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model name is the same as the checkpoint name.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the speeds listed here were benchmarked &lt;em&gt;without&lt;/em&gt; PyTorch&#39;s optimized &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html&#34;&gt;scaled dot product attention&lt;/a&gt;. If using PyTorch 2.0 or above, your inference speed will probably be faster than what&#39;s listed here.&lt;/p&gt; &#xA;&lt;h4&gt;Coming Soon&lt;/h4&gt; &#xA;&lt;p&gt;As of now, base finetuned models are available. The rest are coming soon.&lt;/p&gt; &#xA;&lt;h3&gt;Image Models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input Size&lt;/th&gt; &#xA;   &lt;th&gt;Pretrained Models&lt;br&gt;(IN-1K MAE)&lt;/th&gt; &#xA;   &lt;th&gt;Finetuned Models&lt;br&gt;(IN-1K Supervised)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;IN-1K&lt;br&gt;Top-1 (%)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A100 fp16&lt;br&gt;Speed (im/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;mae_hiera_tiny_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;hiera_tiny_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2758&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;mae_hiera_small_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;hiera_small_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2211&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;mae_hiera_base_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/hiera/hiera_base_224.pth&#34;&gt;hiera_base_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1556&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-B+&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;mae_hiera_base_plus_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;hiera_base_plus_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1247&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;mae_hiera_large_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;hiera_large_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;531&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-H&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;mae_hiera_huge_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;hiera_huge_224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;274&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Video Models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input Size&lt;/th&gt; &#xA;   &lt;th&gt;Pretrained Models&lt;br&gt;(K400 MAE)&lt;/th&gt; &#xA;   &lt;th&gt;Finetuned Models&lt;br&gt;(K400)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;K400 (3x5 views)&lt;br&gt;Top-1 (%)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A100 fp16&lt;br&gt;Speed (clip/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16x224x224&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/hiera/hiera_base_16x224.pth&#34;&gt;hiera_base_16x224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;133.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-B+&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16x224x224&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;hiera_base_plus_16x224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16x224x224&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;hiera_large_16x224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera-H&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16x224x224&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#coming-soon&#34;&gt;hiera_huge_16x224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;This repo implements the code to run Hiera models for inference. This repository is still in progress. Here&#39;s what we currently have available and what we have planned:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Image Inference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; MAE implementation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Video Inference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; MAE implementation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training scripts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Full Model Zoo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/facebookresearch/hiera/tree/main/examples&#34;&gt;examples&lt;/a&gt; for examples of how to use Hiera.&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/facebookresearch/hiera/raw/main/examples/inference.ipynb&#34;&gt;examples/inference&lt;/a&gt; for an example of how to prepare the data for inference.&lt;/p&gt; &#xA;&lt;p&gt;Instantiate a model with either &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#model-zoo&#34;&gt;torch hub&lt;/a&gt; or by &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/#installing-from-source&#34;&gt;installing hiera&lt;/a&gt; and running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import hiera&#xA;model = hiera.hiera_base_224(pretrained=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run inference like any other model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;output = model(x)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Video inference works the same way, just use a &lt;code&gt;16x224&lt;/code&gt; model instead.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: for efficiency, Hiera re-orders its tokens at the start of the network (see the &lt;code&gt;Roll&lt;/code&gt; and &lt;code&gt;Unroll&lt;/code&gt; modules in &lt;code&gt;hiera_utils.py&lt;/code&gt;). Thus, tokens &lt;em&gt;aren&#39;t in spatial order&lt;/em&gt; by default. If you&#39;d like to use intermediate feature maps for a downstream task, pass the &lt;code&gt;return_intermediates&lt;/code&gt; flag when running the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;output, intermediates = model(img_norm[None, ...], return_intermediates=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Benchmarking&lt;/h3&gt; &#xA;&lt;p&gt;We provide a script for easy benchmarking. See &lt;a href=&#34;https://github.com/facebookresearch/hiera/raw/main/examples/benchmark.ipynb&#34;&gt;examples/benchmark&lt;/a&gt; to see how to use it.&lt;/p&gt; &#xA;&lt;h4&gt;Scaled Dot Product Attention&lt;/h4&gt; &#xA;&lt;p&gt;PyTorch 2.0 introduced optimized &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html&#34;&gt;scaled dot product attention&lt;/a&gt;, which can speed up transformers quite a bit. We didn&#39;t use this in our original benchmarking, but since it&#39;s a free speed-up this repo will automatically use it if available. To get its benefits, make sure your torch version is 2.0 or above.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Hiera or this code in your work, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{ryali2023hiera,&#xA;  title={Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles},&#xA;  author={Ryali, Chaitanya and Hu, Yuan-Ting and Bolya, Daniel and Wei, Chen and Fan, Haoqi and Huang, Po-Yao and Aggarwal, Vaibhav and Chowdhury, Arkabandhu and Poursaeed, Omid and Hoffman, Judy and Malik, Jitendra and Li, Yanghao and Feichtenhofer, Christoph},&#xA;  journal={ICML},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;This work is licensed under a &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34;&gt;Creative Commons Attribution-NonCommercial 4.0 International License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34;&gt;&lt;img src=&#34;https://licensebuttons.net/l/by-nc/4.0/88x31.png&#34; alt=&#34;CC BY-NC 4.0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/hiera/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pittcsc/Summer2024-Internships</title>
    <updated>2023-06-05T01:45:16Z</updated>
    <id>tag:github.com,2023-06-05:/pittcsc/Summer2024-Internships</id>
    <link href="https://github.com/pittcsc/Summer2024-Internships" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Collection of Summer 2023 &amp; Summer 2024 tech internships!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Summer 2024 Tech Internships by Pitt CSC &amp;amp; Simplify&lt;/h1&gt; &#xA;&lt;p&gt;And we&#39;re back! Use this repo to share and keep track of software, tech, CS, PM, quant internships for &lt;strong&gt;Summer 2024&lt;/strong&gt;. List maintained by &lt;a href=&#34;https://pittcsc.org/&#34;&gt;the Pitt Computer Science Club&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; &lt;strong&gt;This repository is only for internships/co-ops in the United States, Canada or for Remote positions &lt;span&gt;üåé&lt;/span&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;üß† For tips on the internship process check out &lt;a href=&#34;https://www.pittcs.wiki/zero-to-offer&#34;&gt;Zero to Offer&lt;/a&gt; üß†&lt;/p&gt; &#xA;&lt;p&gt;üôè &lt;strong&gt;Contribute by submitting a &lt;a href=&#34;https://github.com/susam/gitpr#create-pull-request&#34;&gt;pull request&lt;/a&gt;! See the contribution guidelines &lt;a href=&#34;https://github.com/pittcsc/Summer2023-Internships/raw/dev/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;!&lt;/strong&gt; üôè&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a href=&#34;https://simplify.jobs/?utm_source=pittcsc&amp;amp;utm_medium=internships_repo&#34;&gt; &lt;b&gt;Applying to internships?&lt;/b&gt; &lt;br&gt; Autofill all your applications in a single click. &lt;br&gt; &lt;/a&gt;&lt;/p&gt;&#xA; &lt;div&gt;&#xA;  &lt;a href=&#34;https://simplify.jobs/?utm_source=pittcsc&amp;amp;utm_medium=internships_repo&#34;&gt; &lt;/a&gt;&#xA;  &lt;a href=&#34;https://simplify.jobs/?utm_source=pittcsc&amp;amp;utm_medium=internships_repo&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/dpeo4xcnc/image/upload/v1636594918/simplify_pittcsc.png&#34; width=&#34;450&#34; alt=&#34;Simplify&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt;  &#xA; &lt;sub&gt;&lt;i&gt;Stop manually re-entering your information. Simplify‚Äôs extension helps you autofill internship applications on millions of sites.&lt;/i&gt;&lt;/sub&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt; Thanks for a great three years üíñüíñ &lt;/h3&gt; &#xA; &lt;p&gt; &lt;img src=&#34;https://api.star-history.com/svg?repos=pittcsc/Summer2024-Internships&amp;amp;type=Date&#34; width=&#34;500&#34; alt=&#34;Star History&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;The List üö¥üèî&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This README file is for &lt;strong&gt;2024 internships only&lt;/strong&gt;. For 2023 internships, please &lt;a href=&#34;https://github.com/pittcsc/Summer2023-Internships/raw/dev/README-2023.md&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pittcsc/Summer2023-Internships#we-love-our-contributors-%EF%B8%8F%EF%B8%8F&#34;&gt;‚¨áÔ∏è Jump to bottom ‚¨áÔ∏è&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- Please leave a one line gap between this and the table --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Location&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.goldmansachs.com/careers/students/programs/americas/summer-analyst-program.html&#34;&gt;Goldman Sachs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Global&lt;/td&gt; &#xA;   &lt;td&gt;Summer 2024 Analyst&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KPMG&lt;/td&gt; &#xA;   &lt;td&gt;Multiple Locations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üîí Closed üîí&lt;/strong&gt; Summer 2024 Engineering &amp;amp; IT Internship (No sponsorship available)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://optiver.com/working-at-optiver/career-opportunities/&#34;&gt;Optiver&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chicago, IL &lt;br&gt; Austin, TX&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://optiver.com/working-at-optiver/career-opportunities/6497784002&#34;&gt;2024 Tech Graduate &amp;amp; Intern Expression of Interest&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://optiver.com/working-at-optiver/career-opportunities/6614387002&#34;&gt;2024 Trading Graduate &amp;amp; Intern Expression of Interest&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://boards.greenhouse.io/bridgewater89/jobs/6570837002&#34;&gt;Bridgewater Associates&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Westport, CT&lt;/td&gt; &#xA;   &lt;td&gt;Investment Engineer Intern&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://blackrock.tal.net/vx/lang-en-GB/mobile-0/brand-3/xf-232eb66ac89a/candidate/so/pm/1/pl/1/opp/7894-Summer-Internship-Program-Americas/en-GB&#34;&gt;BlackRock&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Americas&lt;/td&gt; &#xA;   &lt;td&gt;Summer 2024 Internship Program&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://careers.ansys.com/job/Vancouver-Spring-2024-Electronics-Intern-Software-Development-and-Testing-(BSMS)-Brit-V6E2M6/1026739100&#34;&gt;Ansys&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vancouver, BC, Canada &lt;br&gt; Montreal, QC, Canada &lt;br&gt; Waterloo, ON, Canada&lt;/td&gt; &#xA;   &lt;td&gt;Software Development and Testing (Spring 2024)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://careers.walmart.com/us/jobs/WD1391200-2024-summer-intern-software-engineer-ii-bentonville-ar&#34;&gt;Walmart&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Bentonville, AR&lt;/td&gt; &#xA;   &lt;td&gt;2024 Summer Intern: Software Engineer II (No sponsorship available)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://careers.aqr.com/jobs/university-open-positions/greenwich-ct/2024-summer-internship-express-interest/4478927&#34;&gt;AQR Capital Management, LLC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Greenwich, CT&lt;/td&gt; &#xA;   &lt;td&gt;2024 Summer Internship-Express Interest&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://haier.wd3.myworkdayjobs.com/en-US/GE_Appliances/job/USA-Louisville-KY/Summer-2024-Digital-Technology-Intern_REQ-16073&#34;&gt;GE Appliances&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Louisville, KY&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üîí Closed üîí&lt;/strong&gt; Digital Technology Intern&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://epic.avature.net/Careers/FolderDetail/Software-Developer-Intern---Summer-2024/23429&#34;&gt;Epic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Madison, WI&lt;/td&gt; &#xA;   &lt;td&gt;Software Developer Intern - Summer 2024 (No sponsorship available)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://boards.greenhouse.io/ramp&#34;&gt;Ramp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;New York, Miami, Remote&lt;/td&gt; &#xA;   &lt;td&gt;Software Engineer Intern--&lt;a href=&#34;https://boards.greenhouse.io/ramp/jobs/5083064002&#34;&gt;Backend&lt;/a&gt;, &lt;a href=&#34;https://boards.greenhouse.io/ramp/jobs/4820594002&#34;&gt;Frontend&lt;/a&gt;, &lt;a href=&#34;https://boards.greenhouse.io/ramp/jobs/6726450002&#34;&gt;Android&lt;/a&gt;, &lt;a href=&#34;https://boards.greenhouse.io/ramp/jobs/5649105002&#34;&gt;iOS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Apple&lt;/td&gt; &#xA;   &lt;td&gt;Multiple US Locations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://jobs.apple.com/en-us/details/200480063/software-engineering-internships&#34;&gt;Software Engineering Internships (Express Interest)&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://jobs.apple.com/en-us/details/200480066/machine-learning-ai-internships&#34;&gt;ML/AI Internships (Express Interest)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://boards.greenhouse.io/neuralink&#34;&gt;Neuralink&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fremont, CA &lt;br&gt; Austin, TX&lt;/td&gt; &#xA;   &lt;td&gt;Software Engineer Intern at &lt;a href=&#34;https://boards.greenhouse.io/neuralink/jobs/5285389003&#34;&gt;Fremont, CA&lt;/a&gt; and &lt;a href=&#34;https://boards.greenhouse.io/neuralink/jobs/5552197003&#34;&gt;Austin, TX&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://jobs.lever.co/certik&#34;&gt;Certik&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NYC, NY &lt;br&gt; Seattle, WA &lt;br&gt; SF Bay Area, CA &lt;br&gt; Remote&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://jobs.lever.co/certik/2e33570a-f495-44ef-9d7d-a0c5a7fd8190&#34;&gt;Development Intern&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://jobs.lever.co/certik/095fdcff-99e8-408d-bb8a-e638e44d0b40&#34;&gt;Platform Engineering Intern&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://jobs.lever.co/certik/ca67aab6-9b8b-4c2f-ad80-ff5855292f48&#34;&gt;Full Stack Intern - Matrix&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://jobs.lever.co/certik/c05535ca-0845-4248-9de5-da4455393c9d&#34;&gt;UX/UI Designer Intern&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://boards.greenhouse.io/mosaicml&#34;&gt;MosaicML&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SF and San Diego, CA &lt;br&gt; NYC, NY&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://boards.greenhouse.io/mosaicml/jobs/4133756004&#34;&gt;Software Engineer Intern&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://boards.greenhouse.io/mosaicml/jobs/4170454004&#34;&gt;Research Scientist Intern&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- Please leave a one line gap between this and the table --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pittcsc/Summer2023-Internships#the-list-&#34;&gt;‚¨ÜÔ∏è Back to Top ‚¨ÜÔ∏è&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;We love our contributors ‚ù§Ô∏è‚ù§Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;Make a &lt;a href=&#34;https://github.com/susam/gitpr#create-pull-request&#34;&gt;pull request&lt;/a&gt; to help contribute. &lt;a href=&#34;https://github.com/pittcsc/Summer2023-Internships/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=pittcsc/Summer2023-Internships&amp;amp;columns=24&amp;amp;max=480&#34;&gt; &lt;/a&gt; &lt;em&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>emarco177/ice_breaker</title>
    <updated>2023-06-05T01:45:16Z</updated>
    <id>tag:github.com,2023-06-05:/emarco177/ice_breaker</id>
    <link href="https://github.com/emarco177/ice_breaker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ice_breaker&lt;/h1&gt; &#xA;&lt;p&gt;A repository for learning LangChain by building a generative ai application.&lt;/p&gt; &#xA;&lt;p&gt;This is a web applicaiton crawling Linkedin &amp;amp; Twitter data about a person an customize an ice breaker with them.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/emarco177/ice_breaker/raw/main/static/banner.jpeg&#34; alt=&#34;Logo&#34;&gt; &lt;a href=&#34;https://www.udemy.com/course/langchain/?referralCode=D981B8213164A3EA91AC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LangChain%20Udemy%20Course-Coupon%20%2412.99-brightgreen&#34; alt=&#34;udemy&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Environment Variables&lt;/h2&gt; &#xA;&lt;p&gt;To run this project, you will need to add the following environment variables to your .env file&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;PYTHONPATH=/{YOUR_PATH_TO_PROJECT}/ice_breaker&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;PROXYCURL_API_KEY&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;SERPAPI_API_KEY&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;TWITTER_API_KEY&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;TWITTER_API_SECRET&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;TWITTER_ACCESS_TOKEN&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;TWITTER_ACCESS_SECRET&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Run Locally&lt;/h2&gt; &#xA;&lt;p&gt;Clone the project&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  git clone [https://github.com/emarco177/ice_breaker/blob/main/](https://github.com/emarco177/ice_breaker.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Go to the project directory&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  cd ice_breaker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  pipenv install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start the flask server&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  pipenv run app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run tests, run the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  pipenv run pytest .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîó Links&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.udemy.com/course/langchain/?referralCode=D981B8213164A3EA91AC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/my_portfolio-000?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white&#34; alt=&#34;portfolio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/in/eden-marco/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white&#34; alt=&#34;linkedin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.udemy.com/user/eden-marco/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/twitter-1DA1F2?style=for-the-badge&amp;amp;logo=twitter&amp;amp;logoColor=white&#34; alt=&#34;twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>