<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-29T01:33:31Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jackmpcollins/magentic</title>
    <updated>2023-09-29T01:33:31Z</updated>
    <id>tag:github.com,2023-09-29:/jackmpcollins/magentic</id>
    <link href="https://github.com/jackmpcollins/magentic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Seamlessly integrate LLMs as Python functions&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;magentic&lt;/h1&gt; &#xA;&lt;p&gt;Easily integrate Large Language Models into your Python code. Simply use the &lt;code&gt;@prompt&lt;/code&gt; decorator to create functions that return structured output from the LLM. Mix LLM queries and function calling with regular Python code to create complex logic.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;magentic&lt;/code&gt; is&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compact:&lt;/strong&gt; Query LLMs without duplicating boilerplate code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Atomic:&lt;/strong&gt; Prompts are functions that can be individually tested and reasoned about.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transparent:&lt;/strong&gt; Create &#34;chains&#34; using regular Python code. Define all of your own prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compatible:&lt;/strong&gt; Use &lt;code&gt;@prompt&lt;/code&gt; functions as normal functions, including with decorators like &lt;code&gt;@lru_cache&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Type Annotated:&lt;/strong&gt; Works with linters and IDEs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Continue reading for sample usage, or go straight to the &lt;a href=&#34;https://raw.githubusercontent.com/jackmpcollins/magentic/main/examples/&#34;&gt;examples directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install magentic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or using poetry&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;poetry add magentic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Configure your OpenAI API key by setting the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable or using &lt;code&gt;openai.api_key = &#34;sk-...&#34;&lt;/code&gt;. See the &lt;a href=&#34;https://github.com/openai/openai-python#usage&#34;&gt;OpenAI Python library documentation&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;@prompt&lt;/code&gt; decorator allows you to define a template for a Large Language Model (LLM) prompt as a Python function. When this function is called, the arguments are inserted into the template, then this prompt is sent to an LLM which generates the function output.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from magentic import prompt&#xA;&#xA;&#xA;@prompt(&#39;Add more &#34;dude&#34;ness to: {phrase}&#39;)&#xA;def dudeify(phrase: str) -&amp;gt; str:&#xA;    ...  # No function body as this is never executed&#xA;&#xA;&#xA;dudeify(&#34;Hello, how are you?&#34;)&#xA;# &#34;Hey, dude! What&#39;s up? How&#39;s it going, my man?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;@prompt&lt;/code&gt; decorator will respect the return type annotation of the decorated function. This can be &lt;a href=&#34;https://docs.pydantic.dev/latest/usage/types/types/&#34;&gt;any type supported by pydantic&lt;/a&gt; including a &lt;code&gt;pydantic&lt;/code&gt; model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from magentic import prompt&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class Superhero(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;    power: str&#xA;    enemies: list[str]&#xA;&#xA;&#xA;@prompt(&#34;Create a Superhero named {name}.&#34;)&#xA;def create_superhero(name: str) -&amp;gt; Superhero:&#xA;    ...&#xA;&#xA;&#xA;create_superhero(&#34;Garden Man&#34;)&#xA;# Superhero(name=&#39;Garden Man&#39;, age=30, power=&#39;Control over plants&#39;, enemies=[&#39;Pollution Man&#39;, &#39;Concrete Woman&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An LLM can also decide to call functions. In this case the &lt;code&gt;@prompt&lt;/code&gt;-decorated function returns a &lt;code&gt;FunctionCall&lt;/code&gt; object which can be called to execute the function using the arguments provided by the LLM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import Literal&#xA;&#xA;from magentic import prompt, FunctionCall&#xA;&#xA;&#xA;def activate_oven(temperature: int, mode: Literal[&#34;broil&#34;, &#34;bake&#34;, &#34;roast&#34;]) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Turn the oven on with the provided settings.&#34;&#34;&#34;&#xA;    return f&#34;Preheating to {temperature} F with mode {mode}&#34;&#xA;&#xA;&#xA;@prompt(&#xA;    &#34;Prepare the oven so I can make {food}&#34;,&#xA;    functions=[activate_oven],&#xA;)&#xA;def configure_oven(food: str) -&amp;gt; FunctionCall[str]:&#xA;    ...&#xA;&#xA;&#xA;output = configure_oven(&#34;cookies!&#34;)&#xA;# FunctionCall(&amp;lt;function activate_oven at 0x1105a6200&amp;gt;, temperature=350, mode=&#39;bake&#39;)&#xA;output()&#xA;# &#39;Preheating to 350 F with mode bake&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sometimes the LLM requires making one or more function calls to generate a final answer. The &lt;code&gt;@prompt_chain&lt;/code&gt; decorator will resolve &lt;code&gt;FunctionCall&lt;/code&gt; objects automatically and pass the output back to the LLM to continue until the final answer is reached.&lt;/p&gt; &#xA;&lt;p&gt;In the following example, when &lt;code&gt;describe_weather&lt;/code&gt; is called the LLM first calls the &lt;code&gt;get_current_weather&lt;/code&gt; function, then uses the result of this to formulate its final answer which gets returned.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from magentic import prompt_chain&#xA;&#xA;&#xA;def get_current_weather(location, unit=&#34;fahrenheit&#34;):&#xA;    &#34;&#34;&#34;Get the current weather in a given location&#34;&#34;&#34;&#xA;    # Pretend to query an API&#xA;    return {&#xA;        &#34;location&#34;: location,&#xA;        &#34;temperature&#34;: &#34;72&#34;,&#xA;        &#34;unit&#34;: unit,&#xA;        &#34;forecast&#34;: [&#34;sunny&#34;, &#34;windy&#34;],&#xA;    }&#xA;&#xA;&#xA;@prompt_chain(&#xA;    &#34;What&#39;s the weather like in {city}?&#34;,&#xA;    functions=[get_current_weather],&#xA;)&#xA;def describe_weather(city: str) -&amp;gt; str:&#xA;    ...&#xA;&#xA;&#xA;describe_weather(&#34;Boston&#34;)&#xA;# &#39;The current weather in Boston is 72Â°F and it is sunny and windy.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LLM-powered functions created using &lt;code&gt;@prompt&lt;/code&gt; and &lt;code&gt;@prompt_chain&lt;/code&gt; can be supplied as &lt;code&gt;functions&lt;/code&gt; to other &lt;code&gt;@prompt&lt;/code&gt;/&lt;code&gt;@prompt_chain&lt;/code&gt; decorators, just like regular python functions. This enables increasingly complex LLM-powered functionality, while allowing individual components to be tested and improved in isolation.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/jackmpcollins/magentic/main/examples/&#34;&gt;examples directory&lt;/a&gt; for more.&lt;/p&gt; &#xA;&lt;h3&gt;Streaming&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;StreamedStr&lt;/code&gt; (and &lt;code&gt;AsyncStreamedStr&lt;/code&gt;) class can be used to stream the output of the LLM. This allows you to process the text while it is being generated, rather than receiving the whole output at once.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from magentic import prompt, StreamedStr&#xA;&#xA;&#xA;@prompt(&#34;Tell me about {country}&#34;)&#xA;def describe_country(country: str) -&amp;gt; StreamedStr:&#xA;    ...&#xA;&#xA;&#xA;# Print the chunks while they are being received&#xA;for chunk in describe_country(&#34;Brazil&#34;):&#xA;    print(chunk, end=&#34;&#34;)&#xA;# &#39;Brazil, officially known as the Federative Republic of Brazil, is ...&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Multiple &lt;code&gt;StreamedStr&lt;/code&gt; can be created at the same time to stream LLM outputs concurrently. In the below example, generating the description for multiple countries takes approximately the same amount of time as for a single country.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from time import time&#xA;&#xA;countries = [&#34;Australia&#34;, &#34;Brazil&#34;, &#34;Chile&#34;]&#xA;&#xA;&#xA;# Generate the descriptions one at a time&#xA;start_time = time()&#xA;for country in countries:&#xA;    # Converting `StreamedStr` to `str` blocks until the LLM output is fully generated&#xA;    description = str(describe_country(country))&#xA;    print(f&#34;{time() - start_time:.2f}s : {country} - {len(description)} chars&#34;)&#xA;&#xA;# 22.72s : Australia - 2130 chars&#xA;# 41.63s : Brazil - 1884 chars&#xA;# 74.31s : Chile - 2968 chars&#xA;&#xA;&#xA;# Generate the descriptions concurrently by creating the StreamedStrs at the same time&#xA;start_time = time()&#xA;streamed_strs = [describe_country(country) for country in countries]&#xA;for country, streamed_str in zip(countries, streamed_strs):&#xA;    description = str(streamed_str)&#xA;    print(f&#34;{time() - start_time:.2f}s : {country} - {len(description)} chars&#34;)&#xA;&#xA;# 22.79s : Australia - 2147 chars&#xA;# 23.64s : Brazil - 2202 chars&#xA;# 24.67s : Chile - 2186 chars&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Object Streaming&lt;/h3&gt; &#xA;&lt;p&gt;Structured outputs can also be streamed from the LLM by using the return type annotation &lt;code&gt;Iterable&lt;/code&gt; (or &lt;code&gt;AsyncIterable&lt;/code&gt;). This allows each item to be processed while the next one is being generated. See the example in &lt;a href=&#34;https://raw.githubusercontent.com/jackmpcollins/magentic/main/examples/quiz/&#34;&gt;examples/quiz&lt;/a&gt; for how this can be used to improve user experience by quickly displaying/using the first item returned.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections.abc import Iterable&#xA;from time import time&#xA;&#xA;from magentic import prompt&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class Superhero(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;    power: str&#xA;    enemies: list[str]&#xA;&#xA;&#xA;@prompt(&#34;Create a Superhero team named {name}.&#34;)&#xA;def create_superhero_team(name: str) -&amp;gt; Iterable[Superhero]:&#xA;    ...&#xA;&#xA;&#xA;start_time = time()&#xA;for hero in create_superhero_team(&#34;The Food Dudes&#34;):&#xA;    print(f&#34;{time() - start_time:.2f}s : {hero}&#34;)&#xA;&#xA;# 2.23s : name=&#39;Pizza Man&#39; age=30 power=&#39;Can shoot pizza slices from his hands&#39; enemies=[&#39;The Hungry Horde&#39;, &#39;The Junk Food Gang&#39;]&#xA;# 4.03s : name=&#39;Captain Carrot&#39; age=35 power=&#39;Super strength and agility from eating carrots&#39; enemies=[&#39;The Sugar Squad&#39;, &#39;The Greasy Gang&#39;]&#xA;# 6.05s : name=&#39;Ice Cream Girl&#39; age=25 power=&#39;Can create ice cream out of thin air&#39; enemies=[&#39;The Hot Sauce Squad&#39;, &#39;The Healthy Eaters&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Asyncio&lt;/h3&gt; &#xA;&lt;p&gt;Asynchronous functions / coroutines can be used to concurrently query the LLM. This can greatly increase the overall speed of generation, and also allow other asynchronous code to run while waiting on LLM output. In the below example, the LLM generates a description for each US president while it is waiting on the next one in the list. Measuring the characters generated per second shows that this example achieves a 7x speedup over serial processing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from time import time&#xA;from typing import AsyncIterable&#xA;&#xA;from magentic import prompt&#xA;&#xA;&#xA;@prompt(&#34;List ten presidents of the United States&#34;)&#xA;async def iter_presidents() -&amp;gt; AsyncIterable[str]:&#xA;    ...&#xA;&#xA;&#xA;@prompt(&#34;Tell me more about {topic}&#34;)&#xA;async def tell_me_more_about(topic: str) -&amp;gt; str:&#xA;    ...&#xA;&#xA;&#xA;# For each president listed, generate a description concurrently&#xA;start_time = time()&#xA;tasks = []&#xA;async for president in await iter_presidents():&#xA;    # Use asyncio.create_task to schedule the coroutine for execution before awaiting it&#xA;    # This way descriptions will start being generated while the list of presidents is still being generated&#xA;    task = asyncio.create_task(tell_me_more_about(president))&#xA;    tasks.append(task)&#xA;&#xA;descriptions = await asyncio.gather(*tasks)&#xA;&#xA;# Measure the characters per second&#xA;total_chars = sum(len(desc) for desc in descriptions)&#xA;time_elapsed = time() - start_time&#xA;print(total_chars, time_elapsed, total_chars / time_elapsed)&#xA;# 24575 28.70 856.07&#xA;&#xA;&#xA;# Measure the characters per second to describe a single president&#xA;start_time = time()&#xA;out = await tell_me_more_about(&#34;George Washington&#34;)&#xA;time_elapsed = time() - start_time&#xA;print(len(out), time_elapsed, len(out) / time_elapsed)&#xA;# 2206 18.72 117.78&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Additional Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;functions&lt;/code&gt; argument to &lt;code&gt;@prompt&lt;/code&gt; can contain async/coroutine functions. When the corresponding &lt;code&gt;FunctionCall&lt;/code&gt; objects are called the result must be awaited.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;Annotated&lt;/code&gt; type annotation can be used to provide descriptions and other metadata for function parameters. See &lt;a href=&#34;https://docs.pydantic.dev/latest/usage/validation_decorator/#using-field-to-describe-function-arguments&#34;&gt;the pydantic documentation on using &lt;code&gt;Field&lt;/code&gt; to describe function arguments&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;@prompt&lt;/code&gt; and &lt;code&gt;@prompt_chain&lt;/code&gt; decorators also accept a &lt;code&gt;model&lt;/code&gt; argument. You can pass an instance of &lt;code&gt;OpenaiChatModel&lt;/code&gt; (from &lt;code&gt;magentic.chat_model.openai_chat_model&lt;/code&gt;) to use GPT4 or configure a different temperature.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The order of precedence of configuration is&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Arguments passed when initializing an instance in Python&lt;/li&gt; &#xA; &lt;li&gt;Environment variables&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The following environment variables can be set.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Environment Variable&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MAGENTIC_OPENAI_MODEL&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI model&lt;/td&gt; &#xA;   &lt;td&gt;gpt-4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MAGENTIC_OPENAI_TEMPERATURE&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI temperature&lt;/td&gt; &#xA;   &lt;td&gt;0.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Type Checking&lt;/h2&gt; &#xA;&lt;p&gt;Many type checkers will raise warnings or errors for functions with the &lt;code&gt;@prompt&lt;/code&gt; decorator due to the function having no body or return value. There are several ways to deal with these.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Disable the check globally for the type checker. For example in mypy by disabling error code &lt;code&gt;empty-body&lt;/code&gt;. &lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# pyproject.toml&#xA;[tool.mypy]&#xA;disable_error_code = [&#34;empty-body&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Make the function body &lt;code&gt;...&lt;/code&gt; (this does not satisfy mypy) or &lt;code&gt;raise&lt;/code&gt;. &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@prompt(&#34;Choose a color&#34;)&#xA;def random_color() -&amp;gt; str:&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Use comment &lt;code&gt;# type: ignore[empty-body]&lt;/code&gt; on each function. In this case you can add a docstring instead of &lt;code&gt;...&lt;/code&gt;. &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@prompt(&#34;Choose a color&#34;)&#xA;def random_color() -&amp;gt; str:  # type: ignore[empty-body]&#xA;    &#34;&#34;&#34;Returns a random color.&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>mlfoundations/open_lm</title>
    <updated>2023-09-29T01:33:31Z</updated>
    <id>tag:github.com,2023-09-29:/mlfoundations/open_lm</id>
    <link href="https://github.com/mlfoundations/open_lm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A repository for research on medium sized language models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenLM&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/plots/logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenLM is a minimal but performative language modeling (LM) repository, aimed to facilitate research on medium sized LMs. We have verified the performance of OpenLM up to 7B parameters and 256 GPUs, with larger scales planned.&lt;/p&gt; &#xA;&lt;h1&gt;Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#release-notes&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#process-training-data&#34;&gt;Process training data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#run-training&#34;&gt;Run training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#evaluate-model&#34;&gt;Evaluate Model&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#pretrained-models&#34;&gt;Pretrained Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#team-and-acknowledgements&#34;&gt;Team and Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Release Notes&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;08/18/23: Updated README.md&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;p&gt;Here we&#39;ll go over a basic example where we start from a fresh install, download and preprocess some training data, and train a model.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We require python &amp;gt;=3.9, and a current installation of pyTorch, as well as several other packages. The full list of requirements is contained in &lt;code&gt;requirements.txt&lt;/code&gt; and can be installed in your python enviornment via &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; pip install -r requirements.txt&lt;/code&gt; Some considerations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We like &lt;a href=&#34;https://wandb.ai/&#34;&gt;WandB&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/tensorboard&#34;&gt;tensorboard&lt;/a&gt; for logging. We specify how to use these during training below.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Process Training Data&lt;/h2&gt; &#xA;&lt;p&gt;Next you must specify a collection of tokenized data. For the purposes of this example, we will use a recent dump of english Wikipedia, available on HuggingFace. To download this locally, we&#39;ve included a script located at &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#datapreprocess/wiki_download.py&#34;&gt;datapreprocess/wiki_download.py&lt;/a&gt;. All you have to do is specify an output directory for where the raw data should be stored:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python datapreprocess/wiki_download.py --output-dir path/to/raw_data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next we process our training data by running it through a BPE tokenizer and chunk it into chunks of appropriate length. By default we use the tokenizer attached with &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX-20B&lt;/a&gt;. To do this, use the script &lt;code&gt;datapreprocess/make_2048.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; python datapreprocess/make_2048.py \&#xA;    --input-files path_to_raw_data/*.jsonl&#xA;    --output-dir preproc_data&#xA;    --num-workers 32&#xA;    --num-consumers 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where &lt;code&gt;input-files&lt;/code&gt; passes all of its (possibly many) arguments through the python &lt;code&gt;glob&lt;/code&gt; module, allowing for wildcards. Optionally, data can be stored in S3 by setting the environment variables: &lt;code&gt;S3_BASE&lt;/code&gt;, and passing the flag &lt;code&gt;--upload-to-s3&lt;/code&gt; to the script. This saves sharded data to the given bucket with prefix of &lt;code&gt;S3_BASE&lt;/code&gt;. E.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; export S3_BASE=preproc_data-v1/&#xA;&amp;gt;&amp;gt;&amp;gt; python datapreprocess/make2048.py --upload-to-s3 ... # same arguments as before&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run Training&lt;/h2&gt; &#xA;&lt;p&gt;Tokenized data can now be passed to the main training script, &lt;code&gt;open_lm/main.py&lt;/code&gt;. Distributed computatation is handled via &lt;code&gt;torchrun&lt;/code&gt;, and hyperparameters are specified by a variety of keyword arguments. We highlight several of the most important ones here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;train-data&lt;/code&gt;: location of the sharded tokenized training data. If locally generated and stored, this will point to a directory containing files like &lt;code&gt;preproc_data/2048-v1/0/XXXXXXX.tar&lt;/code&gt;. Data are processed using the &lt;a href=&#34;https://github.com/webdataset/webdataset&#34;&gt;webdataset&lt;/a&gt; package where wildcards are supported like &lt;code&gt;preproc_data/2048-v1/0/{0000000..0000099}.tar&lt;/code&gt; to select the first 100 .tar files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: Which model to use. See the table below to see valid options and parameter sizes for each.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;train-num-samples&lt;/code&gt;: how many samples to use from the specified training dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;name&lt;/code&gt;: name of this particular training run for logging purposes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;report-to&lt;/code&gt;: if present, can be &lt;code&gt;wandb&lt;/code&gt;, &lt;code&gt;tensorboard&lt;/code&gt;, or &lt;code&gt;all&lt;/code&gt; to stash logging information on WandB or Tensorboard.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Model choices are contained in the following table, where, for instance &lt;code&gt;11m&lt;/code&gt; indicates an 11 million parameter model and &lt;code&gt;1b&lt;/code&gt; indicates a 1 billion parameter model.&lt;/p&gt; &#xA;&lt;center&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;open_lm_11m&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;open_lm_25m&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;open_lm_87m&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;open_lm_160m&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;open_lm_411m&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;open_lm_830m&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;open_lm_1b&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;open_lm_3b&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;open_lm_7b&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/center&gt; &#xA;&lt;p&gt;An example training run can be called as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; export CUDA_VISIBLE_DEVICES=0,1,2,3&#xA;&amp;gt;&amp;gt;&amp;gt; torchrun --nproc-per-node 4 -m open_lm.main   \&#xA; --model open_lm_3b \&#xA; --train-data /preproc_data/shard-{0000000..0000099}.tar \&#xA; --train-num-samples 1000000000 \&#xA; --workers 8 \&#xA; --dataset-resampled \&#xA; --precision amp_bfloat16 \&#xA; --batch-size 8 \&#xA; --grad-checkpointing \&#xA; --log-every-n-steps 100 \&#xA; --grad-clip-norm 1 \&#xA; --data-key txt \&#xA; --lr 3e-4 \&#xA; --fsdp --fsdp-amp \&#xA; --warmup 2000 \&#xA; --wd 0.1 \&#xA; --beta2 0.95 \&#xA; --epochs 100 \&#xA; --report-to wandb \&#xA; --wandb-project-name open_lm_example \&#xA; --name open_lm_ex_$RANDOM \&#xA; --resume latest \&#xA; --logs path/to/logging/dir/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Checkpoints and final model weights will be saved to the specified logs directory.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluate Model&lt;/h2&gt; &#xA;&lt;p&gt;Once trained, we can evaluate the model. This requires &lt;a href=&#34;https://github.com/mosaicml/llm-foundry&#34;&gt;LLM Foundry&lt;/a&gt;, which can be installed via &lt;code&gt;pip install llm-foundry&lt;/code&gt;. Next some configurations are required to pass to the evaluator: a skeleton of these parameters is located at &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/eval/in_memory_hf_eval.yaml&#34;&gt;eval/in_memory_hf_eval.yaml&lt;/a&gt;. Then just run the following script, making sure to point it at the checkpoint of your trained model (and it&#39;s correspending config .json file):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd eval&#xA;python eval_openlm_ckpt.py \&#xA;--eval-yaml in_memory_hf_eval.yaml \&#xA;--model-config ../open_lm/model_configs/open_lm_3b.json  \ --checkpoint /path/to/openlm_checkpoint.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Pretrained Models&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://huggingface.co/mlfoundations/open_lm_1B&#34;&gt;OpenLM 1B&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;OpenLM 1B is a ~1Billion parameter model trained on a 1.6T token dataset which consists of a mix of RedPajama, Pile, S2ORC, The Pile of Law, Deepmind Math, and RealNews (the full mixture of training data is described in &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1YW-_1vGsSPmVtEt2oeeJOecH6dYX2SuEuhOwZyGwy4k/edit?usp=sharing&#34;&gt;more detail here&lt;/a&gt;). The model checkpoint can be downloaded from &lt;a href=&#34;https://huggingface.co/mlfoundations/open_lm_1B/tree/main&#34;&gt;HuggingFace here&lt;/a&gt;. The script used to train this model (for config-copying purposes) is &lt;a href=&#34;https://github.com/mlfoundations/open_lm/raw/main/scripts/train_example.sh&#34;&gt;located here&lt;/a&gt;. Once this checkpoint has been downloaded, you can evaluate it by following the directions in the &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/open_lm/main/#evaluate-model&#34;&gt;Evaluate Model&lt;/a&gt; section above with the &lt;code&gt;model_configs/m1b_neox_rotary_old.json&lt;/code&gt; model config.&lt;/p&gt; &#xA;&lt;p&gt;Note: We trained this model with rotary embeddings applied to the &lt;em&gt;head&lt;/em&gt; dimension, which is the default in xformers as of 09/01/2023. Since these models were trained, we have updated openlm to correctly apply the rotary embeddings to the sequence dimension (see &lt;a href=&#34;https://github.com/mlfoundations/open_lm/issues/4&#34;&gt;this issue&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/xformers/issues/841&#34;&gt;this issue&lt;/a&gt; for details). To evaluate these models, ensure you use the &lt;code&gt;m1b_neox_rotary_old.json&lt;/code&gt; config.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;OpenLM-1B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;250B Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;500B tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;750B tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;1T Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;1.25T Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;1.5T Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;1.6T Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_challenge&lt;/td&gt; &#xA;   &lt;td&gt;0.27&lt;/td&gt; &#xA;   &lt;td&gt;0.28&lt;/td&gt; &#xA;   &lt;td&gt;0.29&lt;/td&gt; &#xA;   &lt;td&gt;0.28&lt;/td&gt; &#xA;   &lt;td&gt;0.29&lt;/td&gt; &#xA;   &lt;td&gt;0.31&lt;/td&gt; &#xA;   &lt;td&gt;0.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_easy&lt;/td&gt; &#xA;   &lt;td&gt;0.49&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;0.51&lt;/td&gt; &#xA;   &lt;td&gt;0.53&lt;/td&gt; &#xA;   &lt;td&gt;0.54&lt;/td&gt; &#xA;   &lt;td&gt;0.56&lt;/td&gt; &#xA;   &lt;td&gt;0.56&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;boolq&lt;/td&gt; &#xA;   &lt;td&gt;0.60&lt;/td&gt; &#xA;   &lt;td&gt;0.61&lt;/td&gt; &#xA;   &lt;td&gt;0.62&lt;/td&gt; &#xA;   &lt;td&gt;0.62&lt;/td&gt; &#xA;   &lt;td&gt;0.65&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;0.65&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;copa&lt;/td&gt; &#xA;   &lt;td&gt;0.71&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;0.78&lt;/td&gt; &#xA;   &lt;td&gt;0.71&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hellaswag&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;0.54&lt;/td&gt; &#xA;   &lt;td&gt;0.54&lt;/td&gt; &#xA;   &lt;td&gt;0.57&lt;/td&gt; &#xA;   &lt;td&gt;0.59&lt;/td&gt; &#xA;   &lt;td&gt;0.61&lt;/td&gt; &#xA;   &lt;td&gt;0.61&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lambada_openai&lt;/td&gt; &#xA;   &lt;td&gt;0.56&lt;/td&gt; &#xA;   &lt;td&gt;0.57&lt;/td&gt; &#xA;   &lt;td&gt;0.61&lt;/td&gt; &#xA;   &lt;td&gt;0.61&lt;/td&gt; &#xA;   &lt;td&gt;0.65&lt;/td&gt; &#xA;   &lt;td&gt;0.65&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;piqa&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;0.71&lt;/td&gt; &#xA;   &lt;td&gt;0.72&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;0.74&lt;/td&gt; &#xA;   &lt;td&gt;0.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;triviaqa&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;winogrande&lt;/td&gt; &#xA;   &lt;td&gt;0.55&lt;/td&gt; &#xA;   &lt;td&gt;0.57&lt;/td&gt; &#xA;   &lt;td&gt;0.58&lt;/td&gt; &#xA;   &lt;td&gt;0.59&lt;/td&gt; &#xA;   &lt;td&gt;0.61&lt;/td&gt; &#xA;   &lt;td&gt;0.60&lt;/td&gt; &#xA;   &lt;td&gt;0.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU&lt;/td&gt; &#xA;   &lt;td&gt;0.24&lt;/td&gt; &#xA;   &lt;td&gt;0.24&lt;/td&gt; &#xA;   &lt;td&gt;0.24&lt;/td&gt; &#xA;   &lt;td&gt;0.23&lt;/td&gt; &#xA;   &lt;td&gt;0.26&lt;/td&gt; &#xA;   &lt;td&gt;0.24&lt;/td&gt; &#xA;   &lt;td&gt;0.25&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Jeopardy&lt;/td&gt; &#xA;   &lt;td&gt;0.01&lt;/td&gt; &#xA;   &lt;td&gt;0.02&lt;/td&gt; &#xA;   &lt;td&gt;0.01&lt;/td&gt; &#xA;   &lt;td&gt;0.01&lt;/td&gt; &#xA;   &lt;td&gt;0.04&lt;/td&gt; &#xA;   &lt;td&gt;0.09&lt;/td&gt; &#xA;   &lt;td&gt;0.10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Winograd&lt;/td&gt; &#xA;   &lt;td&gt;0.75&lt;/td&gt; &#xA;   &lt;td&gt;0.77&lt;/td&gt; &#xA;   &lt;td&gt;0.77&lt;/td&gt; &#xA;   &lt;td&gt;0.79&lt;/td&gt; &#xA;   &lt;td&gt;0.81&lt;/td&gt; &#xA;   &lt;td&gt;0.80&lt;/td&gt; &#xA;   &lt;td&gt;0.79&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.49&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.51&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.52&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.53&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.54&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.54&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;1B Baselines&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;&lt;strong&gt;OPT-1.3B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;&lt;strong&gt;Pythia-1B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;&lt;strong&gt;Neox-1.3B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;&lt;strong&gt;OPT-IML-1.3B&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_challenge&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.27&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.26&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.26&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_easy&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.49&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.51&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.47&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.58&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;boolq&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.58&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.62&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.72&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;copa&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.75&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.68&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.72&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hellaswag&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.54&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.49&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.48&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lambada_openai&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.59&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.58&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;piqa&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.72&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.70&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.72&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;triviaqa&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;winogrande&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.59&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.53&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.55&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.59&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.26&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.26&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.30&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Jeopardy&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.01&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Winograd&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.74&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.71&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.75&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.48&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.49&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;0.54&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://huggingface.co/mlfoundations/open_lm_7B_1.25T&#34;&gt;OpenLM 7B&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;OpenLM 7B is not yet done training, but we&#39;ve released a checkpoint at 1.25T tokens. Information is the same as for OpenLM-1B above, including the information pertaining to rotary embeddings.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;OpenLM-7B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;275B Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;500B tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;675B tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;775B tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;1T Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;1.25T Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;1.5T Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;1.6T Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;LLAMA-7B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;MPT-7B&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_challenge&lt;/td&gt; &#xA;   &lt;td&gt;0.35&lt;/td&gt; &#xA;   &lt;td&gt;0.35&lt;/td&gt; &#xA;   &lt;td&gt;0.36&lt;/td&gt; &#xA;   &lt;td&gt;0.37&lt;/td&gt; &#xA;   &lt;td&gt;0.39&lt;/td&gt; &#xA;   &lt;td&gt;0.39&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.41&lt;/td&gt; &#xA;   &lt;td&gt;0.39&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_easy&lt;/td&gt; &#xA;   &lt;td&gt;0.60&lt;/td&gt; &#xA;   &lt;td&gt;0.61&lt;/td&gt; &#xA;   &lt;td&gt;0.62&lt;/td&gt; &#xA;   &lt;td&gt;0.62&lt;/td&gt; &#xA;   &lt;td&gt;0.63&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.65&lt;/td&gt; &#xA;   &lt;td&gt;0.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;boolq&lt;/td&gt; &#xA;   &lt;td&gt;0.67&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.69&lt;/td&gt; &#xA;   &lt;td&gt;0.69&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.77&lt;/td&gt; &#xA;   &lt;td&gt;0.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;copa&lt;/td&gt; &#xA;   &lt;td&gt;0.75&lt;/td&gt; &#xA;   &lt;td&gt;0.79&lt;/td&gt; &#xA;   &lt;td&gt;0.75&lt;/td&gt; &#xA;   &lt;td&gt;0.80&lt;/td&gt; &#xA;   &lt;td&gt;0.80&lt;/td&gt; &#xA;   &lt;td&gt;0.78&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.78&lt;/td&gt; &#xA;   &lt;td&gt;0.81&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hellaswag&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;0.67&lt;/td&gt; &#xA;   &lt;td&gt;0.68&lt;/td&gt; &#xA;   &lt;td&gt;0.68&lt;/td&gt; &#xA;   &lt;td&gt;0.69&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.75&lt;/td&gt; &#xA;   &lt;td&gt;0.76&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lambada_openai&lt;/td&gt; &#xA;   &lt;td&gt;0.67&lt;/td&gt; &#xA;   &lt;td&gt;0.68&lt;/td&gt; &#xA;   &lt;td&gt;0.69&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.74&lt;/td&gt; &#xA;   &lt;td&gt;0.70&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;piqa&lt;/td&gt; &#xA;   &lt;td&gt;0.75&lt;/td&gt; &#xA;   &lt;td&gt;0.76&lt;/td&gt; &#xA;   &lt;td&gt;0.76&lt;/td&gt; &#xA;   &lt;td&gt;0.76&lt;/td&gt; &#xA;   &lt;td&gt;0.77&lt;/td&gt; &#xA;   &lt;td&gt;0.77&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.79&lt;/td&gt; &#xA;   &lt;td&gt;0.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;triviaqa&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;winogrande&lt;/td&gt; &#xA;   &lt;td&gt;0.62&lt;/td&gt; &#xA;   &lt;td&gt;0.65&lt;/td&gt; &#xA;   &lt;td&gt;0.65&lt;/td&gt; &#xA;   &lt;td&gt;0.65&lt;/td&gt; &#xA;   &lt;td&gt;0.67&lt;/td&gt; &#xA;   &lt;td&gt;0.67&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.68&lt;/td&gt; &#xA;   &lt;td&gt;0.68&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU-0 shot&lt;/td&gt; &#xA;   &lt;td&gt;0.25&lt;/td&gt; &#xA;   &lt;td&gt;0.25&lt;/td&gt; &#xA;   &lt;td&gt;0.27&lt;/td&gt; &#xA;   &lt;td&gt;0.27&lt;/td&gt; &#xA;   &lt;td&gt;0.28&lt;/td&gt; &#xA;   &lt;td&gt;0.30&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.30&lt;/td&gt; &#xA;   &lt;td&gt;0.30&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Jeopardy&lt;/td&gt; &#xA;   &lt;td&gt;0.15&lt;/td&gt; &#xA;   &lt;td&gt;0.18&lt;/td&gt; &#xA;   &lt;td&gt;0.23&lt;/td&gt; &#xA;   &lt;td&gt;0.22&lt;/td&gt; &#xA;   &lt;td&gt;0.16&lt;/td&gt; &#xA;   &lt;td&gt;0.21&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.33&lt;/td&gt; &#xA;   &lt;td&gt;0.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Winograd&lt;/td&gt; &#xA;   &lt;td&gt;0.82&lt;/td&gt; &#xA;   &lt;td&gt;0.81&lt;/td&gt; &#xA;   &lt;td&gt;0.84&lt;/td&gt; &#xA;   &lt;td&gt;0.84&lt;/td&gt; &#xA;   &lt;td&gt;0.85&lt;/td&gt; &#xA;   &lt;td&gt;0.86&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.81&lt;/td&gt; &#xA;   &lt;td&gt;0.88&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.57&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.58&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.61&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.64&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.64&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;MMLU-5 shot&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.34&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.34&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Team and acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Team (so-far, * = equal contrib): Suchin Gururangan*, Mitchell Wortsman*, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, Ludwig Schmidt.&lt;/p&gt; &#xA;&lt;p&gt;Code is based heavily on &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;open-clip&lt;/a&gt; developed by a team including Ross Wightman, Romain Beaumont, Cade Gordon, Mehdi Cherti, Jenia Jitsev, and &lt;a href=&#34;https://github.com/mlfoundations/open_flamingo&#34;&gt;open-flamingo&lt;/a&gt;, developed by a team including Anas Awadalla and Irena Gao. Additional inspiration is from &lt;a href=&#34;https://github.com/Lightning-AI/lit-llama&#34;&gt;lit-llama&lt;/a&gt;. We are greatful to stability.ai for resource support. OpenLM is developed by researchers from various affiliations including the &lt;a href=&#34;https://raivn.cs.washington.edu/&#34;&gt;RAIVN Lab&lt;/a&gt; at the University of Washington, &lt;a href=&#34;https://nlp.washington.edu/&#34;&gt;UWNLP&lt;/a&gt;, &lt;a href=&#34;https://www.tri.global/&#34;&gt;Toyota Research Institute&lt;/a&gt;, &lt;a href=&#34;https://www.columbia.edu/&#34;&gt;Columbia University&lt;/a&gt;, and more.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this model in your work, please use the following BibTeX citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{open_lm,&#xA;  author = {Gururangan, Suchin and Wortsman, Mitchell and Gadre, Samir Yitzhak and Dave, Achal and Kilian, Maciej and Shi, Weijia and Mercat, Jean and Smyrnis, Georgios and Ilharco, Gabriel and Jordan, Matt and Heckel, Reinhard and Dimakis, Alex and Farhadi, Ali and Shankar, Vaishaal and Schmidt, Ludwig},&#xA;  title = {{open_lm}:  a minimal but performative language modeling (LM) repository},&#xA;  year = {2023},&#xA;  note = {GitHub repository},&#xA;  url = {https://github.com/mlfoundations/open_lm/}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>