<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-25T01:34:51Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>daytonaio/daytona</title>
    <updated>2025-06-25T01:34:51Z</updated>
    <id>tag:github.com,2025-06-25:/daytonaio/daytona</id>
    <link href="https://github.com/daytonaio/daytona" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Daytona is a Secure and Elastic Infrastructure for Running AI-Generated Code&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.daytona.io/docs&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/daytonaio/docs?label=Docs&amp;amp;color=23cc71&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/License-AGPL--3-blue&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/daytonaio/daytona&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/daytonaio/daytona&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/daytonaio/daytona/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/daytonaio/daytona&#34; alt=&#34;Issues - daytona&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/daytonaio/daytona&#34; alt=&#34;GitHub Release&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-white.png&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-black.png&#34;&gt; &#xA;  &lt;img alt=&#34;Daytona logo&#34; src=&#34;https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-black.png&#34; width=&#34;50%&#34;&gt; &#xA; &lt;/picture&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Run AI Code. &lt;br&gt; Secure and Elastic Infrastructure for Running Your AI-Generated Code. &lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.daytona.io/docs&#34;&gt; Documentation &lt;/a&gt;路 &lt;a href=&#34;https://github.com/daytonaio/daytona/issues/new?assignees=&amp;amp;labels=bug&amp;amp;projects=&amp;amp;template=bug_report.md&amp;amp;title=%F0%9F%90%9B+Bug+Report%3A+&#34;&gt; Report Bug &lt;/a&gt;路 &lt;a href=&#34;https://github.com/daytonaio/daytona/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;template=feature_request.md&amp;amp;title=%F0%9F%9A%80+Feature%3A+&#34;&gt; Request Feature &lt;/a&gt;路 &lt;a href=&#34;https://go.daytona.io/slack&#34;&gt; Join our Slack &lt;/a&gt;路 &lt;a href=&#34;https://x.com/daytonaio&#34;&gt; Connect on X &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.producthunt.com/posts/daytona-2?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-daytona-2&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=957617&amp;amp;theme=neutral&amp;amp;period=daily&amp;amp;t=1746176740150&#34; alt=&#34;Daytona  - Secure and elastic infra for running your AI-generated code. | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.producthunt.com/posts/daytona-2?embed=true&amp;amp;utm_source=badge-top-post-topic-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-daytona-2&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=957617&amp;amp;theme=neutral&amp;amp;period=monthly&amp;amp;topic_id=237&amp;amp;t=1746176740150&#34; alt=&#34;Daytona  - Secure and elastic infra for running your AI-generated code. | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Python SDK&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install daytona&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;TypeScript SDK&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @daytonaio/sdk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lightning-Fast Infrastructure&lt;/strong&gt;: Sub-90ms Sandbox creation from code to execution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Separated &amp;amp; Isolated Runtime&lt;/strong&gt;: Execute AI-generated code with zero risk to your infrastructure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Massive Parallelization for Concurrent AI Workflows&lt;/strong&gt;: Fork Sandbox filesystem and memory state (Coming soon!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Programmatic Control&lt;/strong&gt;: File, Git, LSP, and Execute API&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unlimited Persistence&lt;/strong&gt;: Your Sandboxes can live forever&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OCI/Docker Compatibility&lt;/strong&gt;: Use any OCI/Docker image to create a Sandbox&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create an account at &lt;a href=&#34;https://app.daytona.io&#34;&gt;https://app.daytona.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Generate a &lt;a href=&#34;https://app.daytona.io/dashboard/keys&#34;&gt;new API key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow the &lt;a href=&#34;https://www.daytona.io/docs/getting-started/&#34;&gt;Getting Started docs&lt;/a&gt; to start using the Daytona SDK&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Creating your first Sandbox&lt;/h2&gt; &#xA;&lt;h3&gt;Python SDK&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from daytona import Daytona, DaytonaConfig, CreateSandboxParams&#xA;&#xA;# Initialize the Daytona client&#xA;daytona = Daytona(DaytonaConfig(api_key=&#34;YOUR_API_KEY&#34;))&#xA;&#xA;# Create the Sandbox instance&#xA;sandbox = daytona.create(CreateSandboxParams(language=&#34;python&#34;))&#xA;&#xA;# Run code securely inside the Sandbox&#xA;response = sandbox.process.code_run(&#39;print(&#34;Sum of 3 and 4 is &#34; + str(3 + 4))&#39;)&#xA;if response.exit_code != 0:&#xA;    print(f&#34;Error running code: {response.exit_code} {response.result}&#34;)&#xA;else:&#xA;    print(response.result)&#xA;&#xA;# Clean up the Sandbox&#xA;daytona.remove(sandbox)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Typescript SDK&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsx&#34;&gt;import { Daytona } from &#39;@daytonaio/sdk&#39;&#xA;&#xA;async function main() {&#xA;  // Initialize the Daytona client&#xA;  const daytona = new Daytona({&#xA;    apiKey: &#39;YOUR_API_KEY&#39;,&#xA;  })&#xA;&#xA;  let sandbox&#xA;  try {&#xA;    // Create the Sandbox instance&#xA;    sandbox = await daytona.create({&#xA;      language: &#39;python&#39;,&#xA;    })&#xA;    // Run code securely inside the Sandbox&#xA;    const response = await sandbox.process.codeRun(&#39;print(&#34;Sum of 3 and 4 is &#34; + str(3 + 4))&#39;)&#xA;    if (response.exitCode !== 0) {&#xA;      console.error(&#39;Error running code:&#39;, response.exitCode, response.result)&#xA;    } else {&#xA;      console.log(response.result)&#xA;    }&#xA;  } catch (error) {&#xA;    console.error(&#39;Sandbox flow error:&#39;, error)&#xA;  } finally {&#xA;    if (sandbox) await daytona.remove(sandbox)&#xA;  }&#xA;}&#xA;&#xA;main().catch(console.error)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Daytona is Open Source under the &lt;a href=&#34;https://raw.githubusercontent.com/daytonaio/daytona/main/LICENSE&#34;&gt;GNU AFFERO GENERAL PUBLIC LICENSE&lt;/a&gt;, and is the &lt;a href=&#34;https://raw.githubusercontent.com/daytonaio/daytona/main/NOTICE&#34;&gt;copyright of its contributors&lt;/a&gt;. If you would like to contribute to the software, read the Developer Certificate of Origin Version 1.1 (&lt;a href=&#34;https://developercertificate.org/&#34;&gt;https://developercertificate.org/&lt;/a&gt;). Afterwards, navigate to the &lt;a href=&#34;https://raw.githubusercontent.com/daytonaio/daytona/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LeCAR-Lab/ASAP</title>
    <updated>2025-06-25T01:34:51Z</updated>
    <id>tag:github.com,2025-06-25:/LeCAR-Lab/ASAP</id>
    <link href="https://github.com/LeCAR-Lab/ASAP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of &#34;ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; ASAP: Aligning Simulation and Real-World Physics for &lt;p&gt;Learning Agile Humanoid Whole-Body Skills &lt;/p&gt;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;Robotics: Science and Systems (RSS) 2025&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://agile.human2humanoid.com/&#34;&gt;[Website]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2502.01143&#34;&gt;[Arxiv]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU&#34;&gt;[Video]&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/CMU-NV-logo-crop-png.png&#34; height=&#34;50&amp;quot;&#34;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/isaac-gym&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IsaacGym-Preview4-b.svg?sanitize=true&#34; alt=&#34;IsaacGym&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IsaacSim-4.2.0-b.svg?sanitize=true&#34; alt=&#34;IsaacSim&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Genesis-0.2.1-b.svg?sanitize=true&#34; alt=&#34;IsaacSim&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ubuntu.com/blog/tag/22-04-lts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Platform-linux--64-orange.svg?sanitize=true&#34; alt=&#34;Linux platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img src=&#34;https://agile.human2humanoid.com/static/images/asap-preview-gif-480P.gif&#34; width=&#34;400px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release code backbone&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release phase-based motion tracking training pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release ASAP motion datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release motion retargeting pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release sim2sim in MuJoCo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release sim2real with UnitreeSDK&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release ASAP delta action model training pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;ASAP codebase is built on top of &lt;a href=&#34;https://github.com/LeCAR-Lab/HumanoidVerse&#34;&gt;HumanoidVerse&lt;/a&gt; (a multi-simulator framework for humanoid learning) and &lt;a href=&#34;https://github.com/LeCAR-Lab/human2humanoid&#34;&gt;Human2Humanoid&lt;/a&gt; (our prior work on humanoid whole-body tracking).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/LeCAR-Lab/HumanoidVerse&#34;&gt;HumanoidVerse&lt;/a&gt; allows you to train humanoid skills in multiple simulators, including IsaacGym, IsaacSim, and Genesis. Its key design logic is the separation and modularization of simulators, tasks, and algorithms, which enables smooth transfers between different simulators and the real world with minimum effort (just one line of code change). We leverage this framework to develop &lt;a href=&#34;https://agile.human2humanoid.com/&#34;&gt;ASAP&lt;/a&gt; and study how to best transfer policies across simulators and the real world.&lt;/p&gt; &#xA;&lt;h2&gt;IsaacGym Conda Env&lt;/h2&gt; &#xA;&lt;p&gt;Create mamba/conda environment, in the following we use conda for example, but you can use mamba as well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n hvgym python=3.8&#xA;conda activate hvgym&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install IsaacGym&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://developer.nvidia.com/isaac-gym/download&#34;&gt;IsaacGym&lt;/a&gt; and extract:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://developer.nvidia.com/isaac-gym-preview-4&#xA;tar -xvzf isaac-gym-preview-4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install IsaacGym Python API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e isaacgym/python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Test installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python 1080_balls_of_solitude.py  # or&#xA;python joint_monkey.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For libpython error:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check conda path: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda info -e&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set LD_LIBRARY_PATH: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LD_LIBRARY_PATH=&amp;lt;/path/to/conda/envs/your_env/lib&amp;gt;:$LD_LIBRARY_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install HumanoidVerse&lt;/h3&gt; &#xA;&lt;p&gt;Install dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;pip install -e isaac_utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Test with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \&#xA;+simulator=isaacgym \&#xA;+exp=locomotion \&#xA;+domain_rand=NO_domain_rand \&#xA;+rewards=loco/reward_g1_locomotion \&#xA;+robot=g1/g1_29dof_anneal_23dof \&#xA;+terrain=terrain_locomotion_plane \&#xA;+obs=loco/leggedloco_obs_singlestep_withlinvel \&#xA;num_envs=1 \&#xA;project_name=TestIsaacGymInstallation \&#xA;experiment_name=G123dof_loco \&#xA;headless=False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Note:&lt;/summary&gt; This is ONLY for testing, NOT how we train the locomotion policy in the ASAP paper. But still, you can train a locomotion policy by: &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \&#xA;+simulator=isaacgym \&#xA;+exp=locomotion \&#xA;+domain_rand=NO_domain_rand \&#xA;+rewards=loco/reward_g1_locomotion \&#xA;+robot=g1/g1_29dof_anneal_23dof \&#xA;+terrain=terrain_locomotion_plane \&#xA;+obs=loco/leggedloco_obs_singlestep_withlinvel \&#xA;num_envs=4096 \&#xA;project_name=TestIsaacGymInstallation \&#xA;experiment_name=G123dof_loco \&#xA;headless=True \&#xA;rewards.reward_penalty_curriculum=True \&#xA;rewards.reward_initial_penalty_scale=0.1 \&#xA;rewards.reward_penalty_degree=0.00003 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;IsaacLab Environment&lt;/h2&gt; &#xA;&lt;h3&gt;Install IsaacSim&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download Omniverse Launcher&lt;/li&gt; &#xA; &lt;li&gt;Install Isaac Sim through launcher&lt;/li&gt; &#xA; &lt;li&gt;Set environment variables:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export ISAACSIM_PATH=&#34;${HOME}/.local/share/ov/pkg/isaac-sim-4.2.0&#34;&#xA;export ISAACSIM_PYTHON_EXE=&#34;${ISAACSIM_PATH}/python.sh&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install IsaacLab&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/isaac-sim/IsaacLab.git&#xA;cd IsaacLab &amp;amp;&amp;amp; ./isaaclab.sh --conda hvlab&#xA;mamba activate hvlab&#xA;sudo apt install cmake build-essential&#xA;./isaaclab.sh --install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setup HumanoidVerse&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;pip install -e isaac_utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Genesis Environment&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mamba create -n hvgen python=3.10&#xA;mamba activate hvgen&#xA;pip install genesis-world torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;pip install -e isaac_utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Motion Tracking Training&lt;/h1&gt; &#xA;&lt;p&gt;Train a phase-based motion tracking policy to imitate Cristiano Ronaldo&#39;s signature Siuuu move&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python humanoidverse/train_agent.py \&#xA;+simulator=isaacgym \&#xA;+exp=motion_tracking \&#xA;+domain_rand=NO_domain_rand \&#xA;+rewards=motion_tracking/reward_motion_tracking_dm_2real \&#xA;+robot=g1/g1_29dof_anneal_23dof \&#xA;+terrain=terrain_locomotion_plane \&#xA;+obs=motion_tracking/deepmimic_a2c_nolinvel_LARGEnoise_history \&#xA;num_envs=4096 \&#xA;project_name=MotionTracking \&#xA;experiment_name=MotionTracking_CR7 \&#xA;robot.motion.motion_file=&#34;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-TairanTestbed_TairanTestbed_CR7_video_CR7_level1_filter_amass.pkl&#34; \&#xA;rewards.reward_penalty_curriculum=True \&#xA;rewards.reward_penalty_degree=0.00001 \&#xA;env.config.resample_motion_when_training=False \&#xA;env.config.termination.terminate_when_motion_far=True \&#xA;env.config.termination_curriculum.terminate_when_motion_far_curriculum=True \&#xA;env.config.termination_curriculum.terminate_when_motion_far_threshold_min=0.3 \&#xA;env.config.termination_curriculum.terminate_when_motion_far_curriculum_degree=0.000025 \&#xA;robot.asset.self_collisions=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After training, you can visualize the policy by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python humanoidverse/eval_agent.py \&#xA;+checkpoint=logs/MotionTracking/xxxxxxxx_xxxxxxx-MotionTracking_CR7-motion_tracking-g1_29dof_anneal_23dof/model_5800.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is the visualization of the policy after traning 5800 iters. The policy is able to imitate the motion of Cristiano Ronaldo&#39;s Siuuu move. With more training, the policy will be more accurate and smooth (see the video in the &lt;a href=&#34;https://arxiv.org/pdf/2502.01143&#34;&gt;paper&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/motion_tracking_5800.gif&#34; width=&#34;400px&#34;&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find our work useful, please consider citing us!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{he2025asap,&#xA;  title={ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills},&#xA;  author={He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbabu, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi &#34;Jim&#34; and Zhu, Yuke and Liu, Changliu and Shi, Guanya},&#xA;  journal={arXiv preprint arXiv:2502.01143},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dendenxu/fast-gaussian-rasterization</title>
    <updated>2025-06-25T01:34:51Z</updated>
    <id>tag:github.com,2025-06-25:/dendenxu/fast-gaussian-rasterization</id>
    <link href="https://github.com/dendenxu/fast-gaussian-rasterization" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A geometry-shader-based, global CUDA sorted high-performance 3D Gaussian Splatting rasterizer. Can achieve a 5-10x speedup in rendering compared to the vanialla diff-gaussian-rasterization.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Fast Gaussian Rasterization&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Can be 5-10x faster than the original software CUDA rasterizer (&lt;a href=&#34;https://github.com/graphdeco-inria/diff-gaussian-rasterization&#34;&gt;diff-gaussian-rasterization&lt;/a&gt;).&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Can be 2-3x faster if using offline rendering. (Bottleneck: copying rendered images around, thinking about improvements.)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speedup most visible with high pixel-to-point ratio (large Gaussians, small point count, high-res rendering).&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dendenxu/fast-gaussian-splatting/assets/43734697/f50afd6f-bbd5-4e18-aca6-a7356a5d3f75&#34;&gt;https://github.com/dendenxu/fast-gaussian-splatting/assets/43734697/f50afd6f-bbd5-4e18-aca6-a7356a5d3f75&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;No backward pass is supported yet. Will think of ways to add a backward. Depth-peeling (&lt;a href=&#34;https://zju3dv.github.io/4k4d&#34;&gt;4K4D&lt;/a&gt;) is too slow. Discussion welcomed.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install the latest release from PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install fast_gauss&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or the latest commit from GitHub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install git+https://github.com/dendenxu/fast-gaussian-rasterization&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;No CUDA compilation is required to build &lt;code&gt;fast_gauss&lt;/code&gt; since we&#39;re only shader-based for now.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Replace the original import of &lt;code&gt;diff_gaussian_rasterization&lt;/code&gt; with &lt;code&gt;fast_gauss&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, replace this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diff_gaussian_rasterization import GaussianRasterizationSettings, GaussianRasterizer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;with this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fast_gauss import GaussianRasterizationSettings, GaussianRasterizer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And you&#39;re good to go.&lt;/p&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: for the ultimate 5-10x performance increase, you&#39;ll need to let &lt;code&gt;fast_gauss&lt;/code&gt;&#39;s shader directly write to your desired framebuffer.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Currently, we are trying to automatically detect whether you&#39;re managing your own OpenGL context (i.e. opening up a GUI) by checking for the module &lt;code&gt;OpenGL&lt;/code&gt; during the import of &lt;code&gt;fast_gauss&lt;/code&gt;. If detected, all rendering commands will return &lt;code&gt;None&lt;/code&gt;s and we will directly write to the bound framebuffer at the time of the draw call. Thus if you&#39;re running in a GUI (OpenGL-based) environment, the output of our rasterizer will be &lt;code&gt;None&lt;/code&gt;s and does not require further processing.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Improve offline rendering performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Add a warning to the user if they&#39;re performing further processing on the returned values.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: the speedup is the most visible when the pixel-to-point ratio is high.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;That is, when there are large Gaussians and very high-resolution rendering, the speedup is more visible. The CUDA-based software implementation is more resolution sensitive and for some extremely dense point clouds (&amp;gt; 1 million points), the CUDA implementation might be faster. This is because the typical rasterization-based pipeline on modern graphics hardware is &lt;a href=&#34;https://www.youtube.com/watch?v=hf27qsQPRLQ&amp;amp;list=WL&#34;&gt;not well-optimized for small triangles&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: for best performance, cache the persistent results (for example, the 6 elements of the covariance matrix).&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is more of a general tip and not directly related to &lt;code&gt;fast_gauss&lt;/code&gt;. However, the impact is more observable here since we haven&#39;t implemented a fast 3D covariance computation (from scales and rotations) in the shader yet. Only PyTorch implementation is available for now.&lt;/p&gt; &#xA;&lt;p&gt;When the point count increases, even the smallest &lt;code&gt;precomputation&lt;/code&gt; can help. An example is the concatenation of the base 0-degree SH parameter and the rest, that small maneuver might cost us 10ms on a 3060 with 5 million points. Thus, store the concatenated tensors instead and avoid concatenating them in every frame.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Implement SH eval in the vertex shader.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Warn users if they&#39;re not properly precomputing the covariance matrix.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Implement a more optimized &lt;code&gt;OptimizedGaussians&lt;/code&gt; for precomputing things and apply a cache. Similar to that of the vertex shader (see &lt;a href=&#34;https://www.khronos.org/opengl/wiki/Vertex_Shader&#34;&gt;Invokation frequency&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: it&#39;s recommended to pass in a CPU tensor in the &lt;code&gt;GaussianRasterizationSettings&lt;/code&gt; to avoid explicit synchronizations for even better performance.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Add a warning to the user if GPU tensors are detected.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: the second output of the &lt;code&gt;GaussianRasterizer&lt;/code&gt; is not radii anymore (since we&#39;re not gonna use it for the backward pass), but the alpha values of the rendered image instead.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;And the alpha channel content seems to be bugged currently, will debug.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Debug alpha channel values&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Apply more of the optimization techniques used by similar shaders, including packing the data into a texture and bit reduction during computation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Thinks of ways for a backward pass. Welcome to discuss!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Compute covariance from scaling and rotation in the shader, currently it&#39;s on the CUDA (PyTorch) side.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Compute SH in the shader, currently it&#39;s on the CUDA (PyTorch) side.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Try to align the rendering results at the pixel level, small deviation exists currently.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Use indexed draw calls to minimize data passing and shuffling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Do incremental sorting based on viewport change, currently it&#39;s a full resort on with CUDA (PyTorch).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implementation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guidelines&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Let the professionals do the work. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Let GPU do the large-scale sorting.&lt;/li&gt; &#xA;   &lt;li&gt;Let the graphics pipeline do the rasterization for us, not the other way around.&lt;/li&gt; &#xA;   &lt;li&gt;Let OpenGL directly write to your framebuffer.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Minimize repeated work. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Compute the 3D to 2D covariance projection only once for each Gaussian, instead of 4 times for the quad, enabled by the geometry shader.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Minimize stalls (minimize explicit synchronizations between GPU and CPU). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enabled by using &lt;code&gt;non_blocking=True&lt;/code&gt; data passing and moving sync points to as early as possible.&lt;/li&gt; &#xA;   &lt;li&gt;Boosted by the fact that we&#39;re sorting on the GPU, thus no need to perform synchronized host-to-device copies.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why does a global sort work?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The OpenGL specification is somewhat vague but there&#39;s this reference: (in the 4th paragraph of section 2.1 of chapter 2 of this specification: &lt;a href=&#34;https://registry.khronos.org/OpenGL/specs/gl/glspec44.core.pdf&#34;&gt;https://registry.khronos.org/OpenGL/specs/gl/glspec44.core.pdf&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Commands are always processed in the order in which they are received, although there may be an indeterminate delay before the effects of a command are realized. This means, for example, that one primitive must be drawn completely before any subsequent one can affect the framebuffer.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Thus if the order of the data in the vertex buffer (or as specified by an index buffer) is back-to-front, and alpha blending is enabled, you can count on OpenGL to correctly update the framebuffer in the correct back to front order.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TODO: Expand implementation details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Environment&lt;/h2&gt; &#xA;&lt;p&gt;This project requires you to have an NVIDIA GPU with the ability to interop between CUDA and OpenGL. Thus, WSL is &lt;a href=&#34;https://docs.nvidia.com/cuda/wsl-user-guide/index.html#features-not-yet-supported&#34;&gt;not supported&lt;/a&gt; and OSX (MacOS) is not supported. Tested on Linux and Windows.&lt;/p&gt; &#xA;&lt;p&gt;For offline rendering (the drop-in replacement of the original CUDA rasterizer), we also need a valid EGL environment. It can sometimes be hard to set up for virtualized machines. &lt;a href=&#34;https://github.com/zju3dv/4K4D/issues/27#issuecomment-2026747401&#34;&gt;Potential fix&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Inspired by those insanely fast WebGL-based 3DGS viewers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mkkellogg/GaussianSplats3D&#34;&gt;GaussianSplats3D&lt;/a&gt; for inspiring our vertex-geometry-fragment shader pipeline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gsplat.tech/&#34;&gt;gsplat.tech&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/antimatter15/splat&#34;&gt;splat&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Using the algorithm and improvements from:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/graphdeco-inria/diff-gaussian-rasterization&#34;&gt;diff-gaussian-rasterization&lt;/a&gt; for the main Gaussian Splatting algorithm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dendenxu/diff-gaussian-rasterization&#34;&gt;diff_gauss&lt;/a&gt; for the fixed culling.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;CUDA-GL interop &amp;amp; EGL environment inspired by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/4k4d&#34;&gt;4K4D&lt;/a&gt; where they(I) used the interop for depth-peeling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zju3dv/EasyVolcap&#34;&gt;EasyVolcap&lt;/a&gt; for the collection of utilities, including EGL setup.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nvlabs.github.io/nvdiffrast&#34;&gt;nvdiffrast&lt;/a&gt; for their EGL context setup and CUDA-GL interop setup.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{fast_gauss,  &#xA;    title = {Fast Gaussian Rasterization},&#xA;    howpublished = {GitHub},  &#xA;    year = {2024},&#xA;    url = {https://github.com/dendenxu/fast-gaussian-rasterization}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>