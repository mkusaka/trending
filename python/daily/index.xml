<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-20T01:41:01Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>protectai/ai-exploits</title>
    <updated>2023-11-20T01:41:01Z</updated>
    <id>tag:github.com,2023-11-20:/protectai/ai-exploits</id>
    <link href="https://github.com/protectai/ai-exploits" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of real world AI/ML exploits for responsibly disclosed vulnerabilities&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;AI Exploits&lt;/h1&gt; &#xA; &lt;img width=&#34;250&#34; src=&#34;https://github.com/protectai/ai-exploits/assets/5151193/aef11c4a-d758-45fe-aab8-c9df714cdbe5&#34; alt=&#34;AI Exploits Logo&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The AI world has a security problem and it&#39;s not just in the inputs given to LLMs such as ChatGPT. Based on research done by &lt;a href=&#34;https://protectai.com&#34;&gt;Protect AI&lt;/a&gt; and independent security experts on the &lt;a href=&#34;https://huntr.com&#34;&gt;Huntr&lt;/a&gt; Bug Bounty Platform, there are far more impactful and practical attacks against the tools, libraries and frameworks used to build, train, and deploy machine learning models. Many of these attacks lead to complete system takeovers and/or loss of sensitive data, models, or credentials most often without the need for authentication.&lt;/p&gt; &#xA;&lt;p&gt;With the release of this repository, &lt;a href=&#34;https://protectai.com&#34;&gt;Protect AI&lt;/a&gt; hopes to demystify to the Information Security community what practical attacks against AI/Machine Learning infrastructure look like in the real world and raise awareness to the amount of vulnerable components that currently exist in the AI/ML ecosystem. More vulnerabilities can be found here: &lt;a href=&#34;https://protectai.com/threat-research/november-vulnerability-report&#34;&gt;November Vulnerability Report&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This repository, &lt;strong&gt;ai-exploits&lt;/strong&gt;, is a collection of exploits and scanning templates for responsibly disclosed vulnerabilities affecting machine learning tools.&lt;/p&gt; &#xA;&lt;p&gt;Each vulnerable tool has a number of subfolders containing three types of utilities: &lt;a href=&#34;https://github.com/rapid7/metasploit-framework&#34;&gt;Metasploit&lt;/a&gt; modules, &lt;a href=&#34;https://github.com/projectdiscovery/nuclei&#34;&gt;Nuclei&lt;/a&gt; templates and CSRF templates. Metasploit modules are for security professionals looking to exploit the vulnerabilities and Nuclei templates are for scanning a large number of remote servers to determine if they&#39;re vulnerable.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Video demonstrating running one of the Metasploit modules against Ray:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/5aSwPQKKhi4&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/5aSwPQKKhi4/0.jpg&#34; alt=&#34;Exploit Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Setup &amp;amp; Usage&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to use the modules and scanning templates is to build and run the Docker image provided by the &lt;code&gt;Dockerfile&lt;/code&gt; in this repository. The Docker image will have Metasploit and Nuclei already installed along with all the necessary configuration.&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the image:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t protectai/ai-exploits https://raw.githubusercontent.com/protectai/ai-exploits/main/Dockerfile&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the docker image:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm protectai/ai-exploits /bin/bash&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The latter command will drop you into a &lt;code&gt;bash&lt;/code&gt; session in the container with &lt;code&gt;msfconsole&lt;/code&gt; and &lt;code&gt;nuclei&lt;/code&gt; ready to go.&lt;/p&gt; &#xA;&lt;h3&gt;Using the Metasploit Modules&lt;/h3&gt; &#xA;&lt;h4&gt;With Docker&lt;/h4&gt; &#xA;&lt;p&gt;Start the Metasploit console (the new modules will be available under the &lt;code&gt;exploits/protectai&lt;/code&gt; category), load a module, set the options, and run the exploit.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;msfconsole&#xA;msf6 &amp;gt; use exploit/protectai/ray_job_rce&#xA;msf6 exploit(protectai/ray_job_rce) &amp;gt; set RHOSTS &amp;lt;target IP&amp;gt;&#xA;msf6 exploit(protectai/ray_job_rce) &amp;gt; run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;With Metasploit Installed Locally&lt;/h4&gt; &#xA;&lt;p&gt;Create a folder &lt;code&gt;~/.msf4/modules/exploits/protectai&lt;/code&gt; and copy the exploit modules into it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/.msf4/modules/exploits/protectai&#xA;cp ai-exploits/ray/msfmodules/* ~/.msf4/modules/exploits/protectai&#xA;msfconsole&#xA;msf6 &amp;gt; use exploit/protectai/&amp;lt;exploit_name.py&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Nuclei Templates&lt;/h3&gt; &#xA;&lt;p&gt;Nuclei is a vulnerability scanning engine which can be used to scan large numbers of servers for known vulnerabilities in web applications and networks.&lt;/p&gt; &#xA;&lt;p&gt;Navigate to nuclei templates folder such as &lt;code&gt;ai-exploits/mlflow/nuclei-templates&lt;/code&gt;. In the Docker container these are stored in the &lt;code&gt;/root/nuclei-templates&lt;/code&gt; folder. Then simply point to the template file and the target server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ai-exploits/mlflow/nuclei-templates&#xA;nuclei -t mlflow-lfi.yaml -u http://&amp;lt;target&amp;gt;:&amp;lt;port&amp;gt;`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using CSRF Templates&lt;/h3&gt; &#xA;&lt;p&gt;Cross-Site Request Forgery (CSRF) vulnerabilities enable attackers to stand up a web server hosting a malicious HTML page that will execute a request to the target server on behalf of the victim. This is a common attack vector for exploiting vulnerabilities in web applications, including web applications which are only exposed on the localhost interface and not to the broader network. Below is a simple demo example of how to use a CSRF template to exploit a vulnerability in a web application.&lt;/p&gt; &#xA;&lt;p&gt;Start a web server in the csrf-templates folder. Python allows one to stand up a simple web server in any directory. Navigate to the template folder and start the server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ai-exploits/ray/csrf-templates&#xA;python3 -m http.server 9999&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now visit the web server address you just stood up (&lt;a href=&#34;http://127.0.0.1:9999&#34;&gt;http://127.0.0.1:9999&lt;/a&gt;) and hit F12 to open the developer tools, then click the Network tab. Click the link to ray-cmd-injection-csrf.html. You should see that the browser sent a request to the vulnerable server on your behalf.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution Guidelines&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to this repository. Please read our &lt;a href=&#34;https://raw.githubusercontent.com/protectai/ai-exploits/main/CONTRIBUTING.md&#34;&gt;Contribution Guidelines&lt;/a&gt; for more information on how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/protectai/ai-exploits/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>spm81/Quansheng_UV-K5</title>
    <updated>2023-11-20T01:41:01Z</updated>
    <id>tag:github.com,2023-11-20:/spm81/Quansheng_UV-K5</id>
    <link href="https://github.com/spm81/Quansheng_UV-K5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Quansheng UV-K5&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://uvmod.meshtastic.pt/&#34;&gt;NEW UVMOD in Portuguese&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;QuanSheng_UV-K5 Firmwares for UV-K5, UV-K5(8), UV-K6 &amp;amp; UV-R5 Plus&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/spm81/Quansheng_UV-K5/tree/main/Firmware/UV-K5&#34;&gt;QuanSheng UV-K5 Firmwares&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/spm81/Quansheng_UV-K5/tree/main/Firmware/UV-5R%20Plus&#34;&gt;QuanSheng UV-R5 Plus Firmwares&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/spm81/Quansheng_UV-K5/tree/main/Firmware/UV-K6&#34;&gt;QuanSheng UV-K6 Firmwares&lt;/a&gt;&lt;br&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/spm81/Quansheng_UV-K5/tree/main/Firmware/UNIVERSAL%20FIRMWARE%20(%20All%20Radios%20-%20UV-K5%2C%20UV-K5(8)%2C%20UV-K6%2C%20UV-5R%20Plus%2C%20etc...)&#34;&gt;UNIVERSAL FIRMWARE ( All Radios - UV-K5, UV-K5(8), UV-K6, UV-5R Plus, etc...)&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;You have a QuanSheng UV-K6 or UV-R5 Plus and you get this error ???&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/spm81/Quansheng_UV-K5/main/photos/fwerror.jpg&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You are in the right place !!! :) &lt;br&gt; You can find more help on &lt;a href=&#34;https://t.me/quansheng_uvk5_en&#34;&gt;QuanSheng UV-K5 Telegram Group&lt;/a&gt; (English Only)&lt;br&gt;&lt;br&gt; Podes-me encontrar no &lt;a href=&#34;https://t.me/PMR446PT&#34;&gt;Telegram&lt;/a&gt; (Portuguese Only) !!!&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Explanation of differences between product types&lt;/h1&gt; &#xA;&lt;p&gt;FCC ID : XBPUV-K5&lt;br&gt; Model Number: UV-K5, UV-K5(2), UV-K5(6), UV-K5(8), UV-K5(9), UV-K5(11), UV-K5(22), UV-K5(66), UV-K5(88), UV-K5(99), UV-5R, UV-5R PLUS, UV-82&lt;br&gt; To Whom It May Concern:&lt;br&gt; We, The QUANSHENG ELECTRONICS CO., LTD. hereby declare that the models are identical except the model names &amp;amp; cover designs. &lt;a href=&#34;https://fcc.report/FCC-ID/XBPUV-K5/6401581.pdf&#34;&gt;https://fcc.report/FCC-ID/XBPUV-K5/6401581.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;#FCC Report &lt;br&gt; &lt;a href=&#34;https://fccid.io/XBPUV-K5&#34;&gt;https://fccid.io/XBPUV-K5&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Acly/krita-ai-diffusion</title>
    <updated>2023-11-20T01:41:01Z</updated>
    <id>tag:github.com,2023-11-20:/Acly/krita-ai-diffusion</id>
    <link href="https://github.com/Acly/krita-ai-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Streamlined interface for generating images with AI in Krita. Inpaint and outpaint with optional text prompt, no tweaking required.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img width=&#34;64px&#34; src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/ai_diffusion/icons/logo-128.png&#34;&gt; Generative AI &lt;i&gt;for Krita&lt;/i&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/#features&#34;&gt;Features&lt;/a&gt; | &lt;a href=&#34;https://github.com/Acly/krita-ai-diffusion/releases/latest&#34;&gt;Download&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/#installation&#34;&gt;Installation&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/Ly6USRwTHe0&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/#screenshots&#34;&gt;Screenshots&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generate images from within Krita with minimal fuss: Select an area, push a button, and new content that matches your image will be generated. Or expand your canvas and fill new areas with generated content that blends right in. Text prompts are optional. No tweaking required!&lt;/p&gt; &#xA;&lt;p&gt;This plugin seeks to provide what &#34;Generative Fill/Expand&#34; do in Photoshop - and go beyond. Adjust strength to refine existing content &lt;em&gt;(img2img)&lt;/em&gt; or generate images from scratch. Powerful customization is available for advanced users.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Local. Open source. Free.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/Ly6USRwTHe0&#34; title=&#34;Watch video demo&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/media/screenshot-1.png&#34; alt=&#34;Watch video demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;features&#34;&gt;&lt;/a&gt; Features&lt;/h2&gt; &#xA;&lt;p&gt;Features are designed to fit an interactive workflow where AI generation is used as just another tool while painting. They are meant to synergize with traditional tools and the layer stack.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inpaint&lt;/strong&gt;: Use Krita&#39;s selection tools to mark an area and remove or replace existing content in the image. Simple text prompts can be used to steer generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Outpaint&lt;/strong&gt;: Extend your canvas, select a blank area and automatically fill it with content that seamlessly blends into the existing image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: Create new images from scratch by decribing them with words or existing images. Supports SD1.5 and SDXL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Refine&lt;/strong&gt;: Use the strength slider to refine existing image content instead of replacing it entirely. This also works great for adding new things to an image by painting a (crude) approximation and refining at high strength!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;: Guide image creation directly with sketches or line art. Use depth or normal maps from existing images or 3D scenes. Transfer character pose from snapshots. Control composition with segmentation maps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resolutions&lt;/strong&gt;: Work efficiently at any resolution. The plugin will automatically use resolutions appropriate for the AI model, and scale them to fit your image region.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Upscaling&lt;/strong&gt;: Upscale and enrich images to 4k, 8k and beyond without running out of memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Job Queue&lt;/strong&gt;: Depending on hardware, image generation can take some time. The plugin allows you to queue and cancel jobs while working on your image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;History&lt;/strong&gt;: Not every image will turn out a masterpiece. Preview results and browse previous generations and prompts at any time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Strong Defaults&lt;/strong&gt;: Versatile default style presets allow for a simple UI which covers many scenarios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customization&lt;/strong&gt;: Create your own presets - select a Stable Diffusion checkpoint, add LoRA, tweak samplers and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;installation&#34;&gt;&lt;/a&gt; Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The plugin comes with an integrated installer for the Stable Diffusion backend.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows or Linux (MacOS is untested)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;On Linux:&lt;/em&gt; Python + venv must be installed (available via package manager, eg. &lt;code&gt;apt install python3-venv&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Hardware support&lt;/h4&gt; &#xA;&lt;p&gt;To run locally a powerful graphics card with at least 6 GB VRAM is recommended. Otherwise generating images will take very long!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;NVIDIA GPU&lt;/td&gt;&#xA;   &lt;td&gt;supported via CUDA&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;AMD GPU&lt;/td&gt;&#xA;   &lt;td&gt;supported via DirectML, Windows only&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;CPU&lt;/td&gt;&#xA;   &lt;td&gt;supported, but very slow&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;Cloud GPU&lt;/td&gt;&#xA;   &lt;td&gt;supported, rent a GPU on an hourly basis, see &lt;a href=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/#gpu-cloud&#34;&gt;below&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If you haven&#39;t yet, go and install &lt;a href=&#34;https://krita.org/&#34;&gt;Krita&lt;/a&gt;! &lt;em&gt;Recommended version: 5.2.1&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Acly/krita-ai-diffusion/releases/latest&#34;&gt;Download the plugin&lt;/a&gt;. Unpack the archive into your &lt;code&gt;pykrita&lt;/code&gt; folder. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Windows:&lt;/em&gt; Usually &lt;code&gt;C:\Users\&amp;lt;user&amp;gt;\AppData\Roaming\krita\pykrita&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Linux:&lt;/em&gt; Usually &lt;code&gt;~/.local/share/krita/pykrita&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Check &lt;a href=&#34;https://docs.krita.org/en/user_manual/python_scripting/install_custom_python_plugin.html&#34;&gt;Krita&#39;s official documentation&lt;/a&gt; if you have trouble locating it.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Enable the plugin in Krita (Settings ‣ Configure Krita ‣ Python Plugins Manager) and restart.&lt;/li&gt; &#xA; &lt;li&gt;To show the plugin docker: Settings ‣ Dockers ‣ AI Image Generation.&lt;/li&gt; &#xA; &lt;li&gt;In the plugin docker, click &#34;Configure&#34; to start server installation. &lt;em&gt;Requires ~10 GB free disk space.&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;GPU Cloud&lt;/h3&gt; &#xA;&lt;p&gt;You can also rent a GPU instead of running locally. In that case, step 5 is not needed. Instead use the plugin to connect to a remote server.&lt;/p&gt; &#xA;&lt;p&gt;There is a &lt;a href=&#34;https://github.com/Acly/krita-ai-diffusion/raw/main/doc/cloud-gpu.md&#34;&gt;step by step guide&lt;/a&gt; on how to setup cloud GPU on &lt;a href=&#34;https://www.runpod.io&#34;&gt;runpod.io&lt;/a&gt; or &lt;a href=&#34;https://vast.ai&#34;&gt;vast.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;em&gt;Optional:&lt;/em&gt; Custom ComfyUI Server&lt;/h3&gt; &#xA;&lt;p&gt;The plugin uses &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt; as backend. As an alternative to the automatic installation, you can install it manually or use an existing installation. If the server is already running locally before starting Krita, the plugin will automatically try to connect. Using a remote server is also possible this way.&lt;/p&gt; &#xA;&lt;p&gt;To use an external installation, &lt;a href=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/doc/comfy-requirements.md&#34;&gt;some extensions and models are required&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;screenshots&#34;&gt;&lt;/a&gt; Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Inpainting on a photo using a realistic model&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/media/screenshot-2.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Reworking and adding content to an AI generated image&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/media/screenshot-1.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Adding detail and iteratively refining small parts of the image&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/media/screenshot-3.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Using ControlNet to guide image generation with a crude scribble&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/media/screenshot-4.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Modifying the pose vector layer to control character stances (Click for video)&lt;/em&gt; &lt;a href=&#34;https://youtu.be/-QDPEcVmdLI&#34; title=&#34;Watch video demo&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/media/screenshot-5.png&#34; alt=&#34;Watch video demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Upscaling to improve image quality and add details&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/media/screenshot-6.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Server installation&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/media/screenshot-installation.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Style preset configuration&lt;/em&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Acly/krita-ai-diffusion/main/media/screenshot-style.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Technology&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image generation: &lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Diffusion backend: &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inpainting: &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IP-Adapter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>