<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-19T01:46:02Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Anil-matcha/ChatPDF</title>
    <updated>2023-06-19T01:46:02Z</updated>
    <id>tag:github.com,2023-06-19:/Anil-matcha/ChatPDF</id>
    <link href="https://github.com/Anil-matcha/ChatPDF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with any PDF. Easily upload the PDF documents you&#39;d like to chat with. Instant answers. Ask questions, extract information, and summarize documents with AI. Sources included.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatPDF&lt;/h1&gt; &#xA;&lt;p&gt;Chat with any PDF.&lt;/p&gt; &#xA;&lt;p&gt;Easily upload the PDF documents you&#39;d like to chat with. Instant answers. Ask questions, extract information, and summarize documents with AI. Sources included.&lt;/p&gt; &#xA;&lt;p&gt;Create app like &lt;a href=&#34;https://www.chatpdf.com/&#34;&gt;ChatPDF&lt;/a&gt; or &lt;a href=&#34;https://pdf.ai/&#34;&gt;PDF.ai&lt;/a&gt; in less than 10 lines of code&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;Code is up now, â­ (Star) the repo to receive updates&lt;/p&gt; &#xA;&lt;p&gt;Replit and streamlit version coming soon&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/matchaman11&#34;&gt;Anil Chandra Naidu Matcha&lt;/a&gt; on twitter for updates&lt;/p&gt; &#xA;&lt;p&gt;Subscribe to &lt;a href=&#34;https://www.youtube.com/@AnilChandraNaiduMatcha&#34;&gt;https://www.youtube.com/@AnilChandraNaiduMatcha&lt;/a&gt; for more such video tutorials&lt;/p&gt; &#xA;&lt;h3&gt;How to run ? (Things might change based on OS)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a virtual environment in python &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;https://docs.python.org/3/library/venv.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &#34;pip install -r requirements.txt&#34;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set OPENAI_API_KEY environment variable with your openai key&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &#34;python main.py&#34;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Change pdf file and query in code if you want to try with any other content&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To run streamlit app, follow the steps run &#34;streamlit run streamlitui.py&#34;&lt;/p&gt; &#xA;&lt;p&gt;Parts of the streamlit code is inspired from &lt;a href=&#34;https://github.com/viniciusarruda/chatpdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo link&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://heybot.thesamur.ai/&#34;&gt;https://heybot.thesamur.ai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Also check&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Website-to-Chatbot&#34;&gt;Chat with Website code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Chat-With-Excel&#34;&gt;Chat with CSV code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/Chat-Youtube&#34;&gt;Chat with Youtube code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Anil-matcha/DiscordGPT&#34;&gt;ChatGPT in Discord code&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OptimalScale/LMFlow</title>
    <updated>2023-06-19T01:46:02Z</updated>
    <id>tag:github.com,2023-06-19:/OptimalScale/LMFlow</id>
    <link href="https://github.com/OptimalScale/LMFlow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Model for All.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/logo.png&#34; alt=&#34;LMFlow&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto; background-color: transparent;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;LMFlow&lt;/h1&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;b&gt;English&lt;/b&gt; | &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/readme/README_zh-hans.md&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/readme/README_es.md&#34;&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/readme/README_jp.md&#34;&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/readme/README_ko.md&#34;&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/readme/README_hindi.md&#34;&gt;à¤¹à¤¿à¤‚à¤¦à¥€&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lmflow.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Website-Demo-20B2AA.svg?sanitize=true&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Website-Doc-ff69b4.svg?sanitize=true&#34; alt=&#34;Doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/u9VJNpzhvA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-LMFlow-%237289da.svg?logo=discord&#34; alt=&#34;Embark&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/lmflow/shared_invite/zt-1s6egx12s-THlwHuCjF6~JGKmx7JoJPA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-Join-blueviolet?logo=slack&amp;amp;amp&#34; alt=&#34;slack badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://i.imgloc.com/2023/06/14/VHJmza.jpeg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WeChat-Join-brightgreen?logo=wechat&amp;amp;amp&#34; alt=&#34;WeChat badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community.&lt;/p&gt; &#xA;&lt;p&gt;Large Model for All. See our &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#vision&#34;&gt;vision&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/features.png&#34; alt=&#34;LMFlow-features&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023-06-16] &lt;span&gt;ğŸš€&lt;/span&gt; Our finetuned Robin-33B-V2 scored an impressive 64.1 on the Huggingface LLM leaderboard in our offline evaluation, outperforming major open-source LLMs! All checkpoints (7B, 13B, 33B, and 65B) are &lt;a href=&#34;https://huggingface.co/OptimalScale&#34;&gt;released&lt;/a&gt;! Checkout the performance &lt;a href=&#34;https://medium.com/@hkust.ml/robin-v2-launches-achieves-unparalleled-performance-on-openllm-4f6886e822c1&#34;&gt;here&lt;/a&gt;. &lt;span&gt;ğŸš€&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-06-07] &lt;span&gt;ğŸš€&lt;/span&gt; LMFlow is now officially available on PyPI! Install it with &lt;code&gt;pip install lmflow-finetune&lt;/code&gt;! &lt;span&gt;ğŸš€&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-05-30] Release &lt;a href=&#34;https://huggingface.co/OptimalScale/robin-13b-v2-delta&#34;&gt;Robin-13B-v2&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/OptimalScale/robin-33b-v2-delta&#34;&gt;Robin-33B-v2&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[2023-05-15] Release &lt;a href=&#34;http://lmflow.org:5000/lmflow_data.tar.gz&#34;&gt;LMFlow-data&lt;/a&gt;, the training dataset of Robin-7B-v2. A new &lt;a href=&#34;http://lmflow.org:5000/lmflow_chat_en_dialog_multiturn_single_nll_text2text.tar.gz&#34;&gt;test data&lt;/a&gt; is also released.&lt;/li&gt; &#xA; &lt;li&gt;[2023-05-09] Release &lt;a href=&#34;http://lmflow.org:5000/robin-7b-v2-delta.tar.gz&#34;&gt;Robin-7B-v2&lt;/a&gt;, achieving competitive performance on chitchat, commonsense reasoning and instruction-following tasks. Refer to our &lt;a href=&#34;https://medium.com/@hkust.ml/lmflow-benchmark-an-automatic-evaluation-framework-for-open-source-llms-ef5c6f142418&#34;&gt;comprehensive study&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023-05-08] Release &lt;a href=&#34;https://medium.com/@hkust.ml/lmflow-benchmark-an-automatic-evaluation-framework-for-open-source-llms-ef5c6f142418&#34;&gt;LMFlow Benchmark&lt;/a&gt;, an automatic evaluation framework for open-source chat-style LLMs. &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1JYh4_pxNzmNA9I0YM2epgRA7VXBIeIGS64gPJBg5NHA/edit#gid=0&#34;&gt;Benchmark results&lt;/a&gt; on 31 popular models are reported. &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#33-lmflow-benchmark&#34;&gt;Participate in LMFlow Benchmark&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023-04-21] Release &lt;a href=&#34;http://lmflow.org:5000/robin-7b.tar.gz&#34;&gt;Robin-7B&lt;/a&gt; (based on LLaMA-7B), and two models for commercial use: Parakeets-2.7B (based on GPT-NEO-2.7B) and Cokatoo-7B (based on StableLM-7B) &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/tree/main#model-zoo&#34;&gt;Download here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-04-10] We propose a new alignment algorithm: &lt;a href=&#34;https://optimalscale.github.io/LMFlow/examples/raft.html&#34;&gt;Reward rAnked FineTuning (RAFT)&lt;/a&gt;, which is more efficient than conventional (PPO-based) RLHF. [&lt;a href=&#34;https://arxiv.org/abs/2304.06767&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;[2023-04-02] &lt;a href=&#34;https://lmflow.com/&#34;&gt;Web service&lt;/a&gt; is online!&lt;/li&gt; &#xA; &lt;li&gt;[2023-04-01] Release three instruction-tuned checkpoints and three medical checkpoints in &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#model-zoo&#34;&gt;model zoo&lt;/a&gt;: LLaMA-7B-tuned, LLaMA-13B-tuned, LLaMA-33B-tuned, LLaMA-7B-medical, LLaMA-13B-medical, and LLaMA-33B-medical.&lt;/li&gt; &#xA; &lt;li&gt;[2023-03-27] Release code and checkpoints - &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;version 0.0.1&lt;/a&gt;! &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#model-performance&#34;&gt;Our tasked-tuned model beats ChatGPT on medical domain&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- * [2023-03-27] Support full tuning and lora tuning for all decoder models.&#xA;* [2023-03-27] [Tasked tuned model beats ChatGPT on medical domain](https://github.com/OptimalScale/LMFlow#model-performance). --&gt; &#xA;&lt;!-- * [2023-04-15] Inference: Support streaming output and ChatGLM. --&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;We provide four kinds of demos which include&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Online Service: If you don&#39;t want to run any code and just want to try our models, we deploy our instruction-tuned LLaMA you to have a try.&lt;/li&gt; &#xA; &lt;li&gt;Colab Chatbot (shell): An interactive shell-based chatbot for you to easily deploy a chatbot on colab.&lt;/li&gt; &#xA; &lt;li&gt;Colab Chatbot (web): An interactive web-based chatbot for you to easily deploy your own chatbot on colab.&lt;/li&gt; &#xA; &lt;li&gt;Local Deploy: We also provide a way for you to deploy your model/chatbot locally, which means you can deploy much larger model than previous three methods if you have enough resource.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lmflow.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Online%20Service-Web-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1P9Hf6_mLE7WHH92pw73j9D5kz6GTdkow?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-(shell)%20%20chatbot:%20gpt--neo-orange?logo=google-colab&amp;amp;amp&#34; alt=&#34;colab badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1LLtiiQO-ZIIFsTKxYzGWYX9BDRc-v8dq?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-(web)%20%20chatbot:%20gpt--neo-blue?logo=google-colab&amp;amp;amp&#34; alt=&#34;colab badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1bQmlSiKnqFjrkijFUJ5ylbYW-zUwObqL#scrollTo=9U2P_PUN-5xX&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-(demo)%20%20RAFT:%20diffusion-blueviolet?logo=google-colab&amp;amp;amp&#34; alt=&#34;colab badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Online Service&lt;/h3&gt; &#xA;&lt;p&gt;Welcome to visit our &lt;a href=&#34;https://lmflow.com/&#34;&gt;web service&lt;/a&gt;. We deploy LLaMA-7B-tuned model online for preview. Due to the high website traffic, sometimes the website may fail to respond. You can also deploy the chatbot following &lt;code&gt;Local Deploy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Colab chatbot (shell)&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/colab-shell-chatbot-demo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We provide a simple shell demo of chatbot with Google Colab&#39;s T4/P100/V100 GPU. Notice that the provided gpt-neo-2.7b model is &lt;strong&gt;a rather weak model&lt;/strong&gt;, which only supports English and may sometimes generate unsatisfactory responses. To improve the performance, users can use their own dataset to finetune and obtain a better model with LMFlow. One can also try other available decoder-only models provided in ğŸ¤— &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;Hugging Face&lt;/a&gt;, by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_chatbot.sh {another-model-name}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Colab chatbot (web)&lt;/h3&gt; &#xA;&lt;p&gt;We provide a simple web demo of chatbot with Google Colab&#39;s T4/P100/V100 GPU. Notice that the provided gpt-neo-2.7b model is &lt;strong&gt;a rather weak model&lt;/strong&gt;, which only supports English and may sometimes generate unsatisfactory responses.&lt;/p&gt; &#xA;&lt;h3&gt;Local Deploy&lt;/h3&gt; &#xA;&lt;p&gt;If you have resources and want to deploy your own model locally. We provide you an easy way to run a flask server to launch a backend (to further provide services to other frontend) and an interactive web frontend (to let you communicate directly) by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_app.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide a gradio-based UI for building chatbots. Running the following command will launch the demo for robin-7b:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install gradio&#xA;python ./examples/chatbot_gradio.py --deepspeed configs/ds_config_chatbot.json --model_name_or_path YOUR-LLAMA  --lora_model_path ./robin-7b --prompt_structure &#34;A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human&#39;s questions.###Human: {input_text}###Assistant:&#34;       --end_string &#34;#&#34; --max_new_tokens 200&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also hosted it on Hugging Face &lt;a href=&#34;https://huggingface.co/spaces/OptimalScale/Robin-7b&#34;&gt;Space&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Colab RAFT-diffusion&lt;/h3&gt; &#xA;&lt;p&gt;We also provide a simple demo to display the effectiveness of RAFT algorithm on diffusion models. You can refer to either &lt;a href=&#34;https://colab.research.google.com/drive/1bQmlSiKnqFjrkijFUJ5ylbYW-zUwObqL#scrollTo=9U2P_PUN-5xX&#34;&gt;Colab link&lt;/a&gt; or &lt;code&gt;experimental/RAFT-diffusion/SD256-RAFT.ipynb&lt;/code&gt;. The support of multi-modal training of LMFlow is under development.&lt;/p&gt; &#xA;&lt;h2&gt;Medical Performance&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PubMedQA (ID)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MedQA-USMLE (OOD)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MedMCQA (ID)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Human (pass)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Human (expert)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;InstructGPT 175B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ChatGPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA 7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA 33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Task-tuned LLaMA 7B (Full)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;75.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Task-tuned LLaMA 33B (LoRA)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;50.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;58.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The LLaMA 33B (LoRA) performance is achieved with only &lt;strong&gt;~16h&lt;/strong&gt; finetuning on the training split of PubMedQA and MedMCQA with a single 8 * A100 server. For more performance, including instruction tuning results, please refer to our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;We open-sourced the trained checkpoints to everyone for further training and inference.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/robin-7b.tar.gz&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/robin7b.jpg&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama13b-lora-380k.tar.gz&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/robin13b.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama30b-lora-170k.tar.gz&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/robin33b.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/robin65b.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;Model&lt;br&gt;Base Model&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/robin-7b.tar.gz&#34;&gt;Robin-7B &lt;span&gt;â­&lt;/span&gt; &lt;/a&gt;&lt;br&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama13b-lora-380k.tar.gz&#34;&gt;Robin-13B&lt;/a&gt;&lt;br&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama30b-lora-170k.tar.gz&#34;&gt;Robin-33B&lt;/a&gt;&lt;br&gt;LLaMA-33B&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Robin-65B&lt;/a&gt;&lt;br&gt;LLaMA-65B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama7b-lora-medical.tar.gz&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/robin7b_.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama13b-lora-medical.tar.gz&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/robin13b_.jpg&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama30b-lora-medical.tar.gz&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/robin33b_.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/robin65b_.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;Model&lt;br&gt;Base Model&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama7b-lora-medical.tar.gz&#34;&gt;Robin-7B-medical&lt;/a&gt;&lt;br&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama13b-lora-medical.tar.gz&#34;&gt;Robin-13B-medical&lt;/a&gt;&lt;br&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/llama30b-lora-medical.tar.gz&#34;&gt;Robin-33B-medical&lt;/a&gt;&lt;br&gt;LLaMA-33B&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Robin-65B-medical&lt;/a&gt;&lt;br&gt;LLaMA-65B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/parakeets-2.7b.tar.gz&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/Parakeets.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/Cockatoo3b.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/cockatoo-7b.tar.gz&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/Cockatoo7b.png&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;Model&lt;br&gt;Base Model&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/parakeets-2.7b.tar.gz&#34;&gt;Parakeets-2.7B &lt;span&gt;â­&lt;/span&gt; &lt;/a&gt;&lt;br&gt;GPT-NEO-2.7B&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Cockatoo-3B&lt;/a&gt;&lt;br&gt;StableLM-3B&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://lmflow.org:10001/cockatoo-7b.tar.gz&#34;&gt;Cockatoo-7B &lt;span&gt;â­&lt;/span&gt; &lt;/a&gt;&lt;br&gt;StableLM-7B&lt;/td&gt; &#xA;   &lt;td width=&#34;160&#34; align=&#34;center&#34;&gt;&lt;br&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Pipelines&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Pipelines&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Task Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ…&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Instruction Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ…&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parameter-Efficient Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ…&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alignment Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ…&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Large Model Inference&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ…&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;Seamlessly supported all the &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;decoder models&lt;/a&gt; in ğŸ¤— Hugging Face. LLaMA, GPT2, GPT-Neo, Galactica, have been fully tested. We will support encoder models soon.&lt;/p&gt; &#xA;&lt;h2&gt;1.Setup&lt;/h2&gt; &#xA;&lt;p&gt;Our package has been fully tested on Linux OS (Ubuntu 20.04). Other OS platforms (MacOS, Windows) are not fully tested. You may encounter some unexpected errors. You may try it first on a Linux machine or use Google Colab to experience it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/OptimalScale/LMFlow.git&#xA;cd LMFlow&#xA;conda create -n lmflow python=3.9 -y&#xA;conda activate lmflow&#xA;conda install mpi4py&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2.Prepare Dataset&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/examples/DATASETS.html&#34;&gt;doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;3. Running Scripts&lt;/h2&gt; &#xA;&lt;h3&gt;3.1 Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;You can run &lt;code&gt;scripts/run_finetune.sh&lt;/code&gt; to finetune a GPT-2 base model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_finetune.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to provide arguments for deepspeed to reflect your machine settings, you may pass the corresponding deepspeed arguments to the script. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_finetune.sh &#34;--num_gpus=8 --master_port 10001&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable LoRA finetuning, you may refer to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_finetune_with_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which can be run in similar manner.&lt;/p&gt; &#xA;&lt;p&gt;For detailed configurations, one may modify these scripts directly. These scripts actually just call python script &lt;code&gt;examples/finetune.py&lt;/code&gt;, which can be run in following manner,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;deepspeed ${deepspeed_args} \&#xA;  examples/finetune.py \&#xA;    --deepspeed configs/ds_config_zero3.json \&#xA;    --bf16 \&#xA;    --run_name finetune_with_lora \&#xA;    --model_name_or_path facebook/galactica-1.3b \&#xA;    --num_train_epochs 0.01 \&#xA;    --learning_rate 2e-5 \&#xA;    --dataset_path ${dataset_path} \&#xA;    --per_device_train_batch_size 1 \&#xA;    --per_device_eval_batch_size 1 \&#xA;    --validation_split_percentage 0 \&#xA;    --logging_steps 20 \&#xA;    --block_size 512 \&#xA;    --do_train \&#xA;    --output_dir output_models/finetune \&#xA;    --overwrite_output_dir \&#xA;    --ddp_timeout 72000 \&#xA;    --save_steps 5000 \&#xA;    --dataloader_num_workers 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we set number of epochs &lt;code&gt;--num_train_epochs&lt;/code&gt; to &lt;code&gt;0.01&lt;/code&gt; so that the finetuning process can be finished quickly. If you wish to obtain a model with better performance, feel free to adjust those hyperparameters. You may run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python examples/finetune.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: In the case of a small training data set, the value of &lt;code&gt;block_size&lt;/code&gt; needs to be reduced, otherwise there will be no samples available in the Epoch iterator.&lt;/p&gt; &#xA;&lt;p&gt;to view all possible finetuning arguments. The finetuned model checkpoint will be saved in the argument specified by &lt;code&gt;--output_dir&lt;/code&gt;, which is &lt;code&gt;output_models/finetune&lt;/code&gt; in the above example. We follow &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt; in the model tuning process and serve the model in our web service.&lt;/p&gt; &#xA;&lt;h3&gt;3.2 Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;One can directly run evaluation with an existing Hugging Face model, e.g. to run GPT2 large, one may execute&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_evaluation.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or run the corresponding python script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;CUDA_VISIBLE_DEVICES=0 \&#xA;    deepspeed examples/evaluate.py \&#xA;    --answer_type medmcqa \&#xA;    --model_name_or_path gpt2-large \&#xA;    --dataset_path data/MedQA-USMLE/validation \&#xA;    --deepspeed examples/ds_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To load the finetuned model, specify &lt;code&gt;--model_name_or_path&lt;/code&gt; with the saved model checkpoint directory path.&lt;/p&gt; &#xA;&lt;p&gt;For LoRA finetuned models, one may refer to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_evaluation_with_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Those scripts invoke the examples &lt;code&gt;examples/*.py&lt;/code&gt; built based on our APIs. For more API-related examples, one may refer to the methods in the unittest &lt;code&gt;tests&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3.3 LMFlow Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;LMFlow Benchmark is an automatic evaluation framework for open-source large language models. We use negative log likelihood (NLL) as the metric to evaluate different aspects of a language model: chitchat, commonsense reasoning, and instruction following abilities.&lt;/p&gt; &#xA;&lt;p&gt;You can directly run the LMFlow benchmark evaluation to obtain the results to participate in the &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1JYh4_pxNzmNA9I0YM2epgRA7VXBIeIGS64gPJBg5NHA/edit?usp=sharing&#34;&gt;LLM comparision&lt;/a&gt;. For example, to run GPT2 XL, one may execute&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_benchmark.sh --model_name_or_path gpt2-xl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;--model_name_or_path&lt;/code&gt; is required, you may fill in huggingface model name or local model path here.&lt;/p&gt; &#xA;&lt;p&gt;To check the evaluation results, you may check &lt;code&gt;benchmark.log&lt;/code&gt; in &lt;code&gt;./output_dir/gpt2-xl_lmflow_chat_nll_eval&lt;/code&gt;, &lt;code&gt;./output_dir/gpt2-xl_all_nll_eval&lt;/code&gt; and &lt;code&gt;./output_dir/gpt2-xl_commonsense_qa_eval&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Additional Notes&lt;/h2&gt; &#xA;&lt;h3&gt;4.1 LLaMA Checkpoint&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/examples/checkpoints.html&#34;&gt;doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;4.2 DeepSpeed Config&lt;/h3&gt; &#xA;&lt;p&gt;You can config the deepspeed under configs. Details can be referred at &lt;a href=&#34;https://www.deepspeed.ai/docs/config-json/&#34;&gt;DeepSpeed Configuration&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;4.3 LLaMA Inference on CPU&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to the great efforts of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;. It is possible for everyone to run their LLaMA models on CPU by 4-bit quantization. We provide a script to convert LLaMA LoRA weights to &lt;code&gt;.pt&lt;/code&gt; files. You only need to use &lt;code&gt;convert-pth-to-ggml.py&lt;/code&gt; in llama.cpp to perform quantization.&lt;/p&gt; &#xA;&lt;h2&gt;5. Model Release&lt;/h2&gt; &#xA;&lt;h3&gt;5.1 Medical Model Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;You can run following script to download our medical model checkpoints :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd output_models&#xA;bash download.sh medical_ckpt&#xA;cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also directly download our model via google drive link : &lt;a href=&#34;https://drive.google.com/file/d/1bnsQGNGNYchsOfiNyRAmL2fNiowbmFNw/view?usp=share_link&#34;&gt;medical_ckpt.tar.gz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5.2 Instruction Model Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;Similarly, you can run following script to download our instruction model checkpoints :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd output_models&#xA;bash download.sh instruction_ckpt&#xA;cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also directly download our model via google drive link : &lt;a href=&#34;https://drive.google.com/file/d/1d_ioQ-ViVweeifbsFSO4pczc3UORFHZO/view?usp=share_link&#34;&gt;instruction_ckpt.tar.gz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5.3 Reproduce the result&lt;/h3&gt; &#xA;&lt;p&gt;After downloading the model checkpoints, you can merge the lora model into the base model via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/merge_lora.py \&#xA;    --model_name_or_path {huggingface-model-name-or-path-to-base-model} \&#xA;    --lora_model_path {path-to-lora-model} \&#xA;    --output_model_path {path-to-merged-model}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can replace the &lt;code&gt;--lora_model_path&lt;/code&gt; with &lt;code&gt;output_models/instruction_ckpt/llama7b-lora&lt;/code&gt; (example for llama-7b for instruction) and replace &lt;code&gt;--model_name_or_path&lt;/code&gt; with your converted llama model inside &lt;code&gt;LMFlow/scripts/run_evaluation_with_lora.sh&lt;/code&gt; and run this shell script to reproduce the result.&lt;/p&gt; &#xA;&lt;p&gt;For full model deltas, such as robin-7b-v2-delta, you may use the delta merge script to obtain the full model,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python utils/apply_delta.py \&#xA;    --base-model-path {huggingface-model-name-or-path-to-base-model} \&#xA;    --delta-path {path-to-delta-model} \&#xA;    --target-model-path {path-to-merged-model}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can check the model performance at our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Documentation&lt;/a&gt; for more API reference and experimental results.&lt;/p&gt; &#xA;&lt;h2&gt;Vision&lt;/h2&gt; &#xA;&lt;p&gt;Hello there! We are excited to announce the upcoming release of our code repository that includes a complete LLM training process, enabling users to quickly build their own language models and train them effectively.&lt;/p&gt; &#xA;&lt;p&gt;Our code repository is not just a simple model; it includes the complete training workflow, model optimization, and testing tools. You can use it to build various types of language models, including conversation models, question-answering models, and text generation models, among others.&lt;/p&gt; &#xA;&lt;p&gt;Moreover, we aim to create an open and democratic LLM sharing platform where people can share their checkpoints and experiences to collectively improve the skills of the community. We welcome anyone who is interested in LLM to participate and join us in building an open and friendly community!&lt;/p&gt; &#xA;&lt;p&gt;Whether you are a beginner or an expert, we believe that you can benefit from this platform. Let&#39;s work together to build a vibrant and innovative LLM community!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/u9VJNpzhvA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-LMFlow-%237289da.svg?logo=discord&#34; alt=&#34;Embark&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/lmflow/shared_invite/zt-1s6egx12s-THlwHuCjF6~JGKmx7JoJPA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp;amp;amp&#34; alt=&#34;slack badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://i.imgloc.com/2023/06/14/VHJmza.jpeg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WeChat-Join-brightgreen?logo=wechat&amp;amp;amp&#34; alt=&#34;WeChat badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;LMFlow draws inspiration from various studies, including but not limited to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Alpaca: &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vicuna: &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;https://github.com/lm-sys/FastChat&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you need any help, please submit a &lt;a href=&#34;https://github.com/OptimalScale/LMFlow&#34;&gt;Github&lt;/a&gt; issue.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code included in this project is licensed under the &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;. If you wish to use the codes and models included in this project for commercial purposes, please sign this &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSfJYcci6cbgpIvx_Fh1xDL6pNkzsjGDH1QIcm4cYk88K2tqkw/viewform?usp=pp_url&#34;&gt;document&lt;/a&gt; to obtain authorization.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/OptimalScale/LMFlow/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=OptimalScale/LMFlow&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please consider giving â­ and citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lmflow,&#xA;  author = {Shizhe Diao and Rui Pan and Hanze Dong and KaShun Shum and Jipeng Zhang and Wei Xiong and Tong Zhang},&#xA;  title = {LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://optimalscale.github.io/LMFlow/}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>shibing624/MedicalGPT</title>
    <updated>2023-06-19T01:46:02Z</updated>
    <id>tag:github.com,2023-06-19:/shibing624/MedicalGPT</id>
    <link href="https://github.com/shibing624/MedicalGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°åŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/README.md&#34;&gt;&lt;strong&gt;ğŸ‡¨ğŸ‡³ä¸­æ–‡&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/README_EN.md&#34;&gt;&lt;strong&gt;ğŸŒEnglish&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/shibing624/MedicalGPT/wiki&#34;&gt;&lt;strong&gt;ğŸ“–æ–‡æ¡£/Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/shibing624&#34;&gt;&lt;strong&gt;ğŸ¤–æ¨¡å‹/Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/shibing624/MedicalGPT&#34;&gt; &lt;img src=&#34;https://github.com/shibing624/MedicalGPT/raw/main/docs/logo.png&#34; height=&#34;100&#34; alt=&#34;Logo&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;MedicalGPT: Training Medical GPT Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/shibing624&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging%20Face-shibing624-green&#34; alt=&#34;HF Models&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#shibing624/MedicalGPT&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/shibing624/MedicalGPT?color=yellow&#34; alt=&#34;Github Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/CONTRIBUTING.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/contributions-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;Contributions welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License Apache 2.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/requirements.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.8%2B-green.svg?sanitize=true&#34; alt=&#34;python_version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shibing624/MedicalGPT/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/shibing624/MedicalGPT.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/#Contact&#34;&gt;&lt;img src=&#34;http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20&#34; alt=&#34;Wechat Group&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“– Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedicalGPT&lt;/strong&gt; training medical GPT model with ChatGPT training pipeline, implemantation of Pretraining, Supervised Finetuning, Reward Modeling and Reinforcement Learning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedicalGPT&lt;/strong&gt; è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°åŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/shibing624/MedicalGPT/raw/main/docs/GPT_Training.jpg&#34; width=&#34;860&#34;&gt; &#xA;&lt;p&gt;åˆ†å››é˜¶æ®µè®­ç»ƒGPTæ¨¡å‹ï¼Œæ¥è‡ªAndrej Karpathyçš„æ¼”è®²PDF &lt;a href=&#34;https://karpathy.ai/stateofgpt.pdf&#34;&gt;State of GPT&lt;/a&gt;ï¼Œè§†é¢‘ &lt;a href=&#34;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&#34;&gt;Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ”¥ News&lt;/h2&gt; &#xA;&lt;p&gt;[2023/06/15] v1.0ç‰ˆæœ¬: å‘å¸ƒä¸­æ–‡åŒ»ç–—LoRAæ¨¡å‹&lt;a href=&#34;https://huggingface.co/shibing624/ziya-llama-13b-medical-lora&#34;&gt;shibing624/ziya-llama-13b-medical-lora&lt;/a&gt;ï¼ŒåŸºäºZiya-LLaMA-13B-v1æ¨¡å‹ï¼ŒSFTå¾®è°ƒäº†ä¸€ç‰ˆåŒ»ç–—æ¨¡å‹ï¼ŒåŒ»ç–—é—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡ï¼Œè¯¦è§&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/releases/tag/1.0.0&#34;&gt;Release-v1.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2023/06/05] v0.2ç‰ˆæœ¬: ä»¥åŒ»ç–—ä¸ºä¾‹ï¼Œè®­ç»ƒé¢†åŸŸå¤§æ¨¡å‹ï¼Œå®ç°äº†å››é˜¶æ®µè®­ç»ƒï¼šåŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚è¯¦è§&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/releases/tag/0.2.0&#34;&gt;Release-v0.2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ˜Š Feature&lt;/h2&gt; &#xA;&lt;p&gt;åŸºäºChatGPT Training Pipelineï¼Œæœ¬é¡¹ç›®å®ç°äº†é¢†åŸŸæ¨¡å‹--åŒ»ç–—æ¨¡å‹çš„å››é˜¶æ®µè®­ç»ƒï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æ¡£æ•°æ®ä¸ŠäºŒæ¬¡é¢„è®­ç»ƒGPTæ¨¡å‹ï¼Œä»¥æ³¨å…¥é¢†åŸŸçŸ¥è¯†&lt;/li&gt; &#xA; &lt;li&gt;ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾&lt;/li&gt; &#xA; &lt;li&gt;ç¬¬ä¸‰é˜¶æ®µï¼šRM(Reward Model)å¥–åŠ±æ¨¡å‹å»ºæ¨¡ï¼Œæ„é€ äººç±»åå¥½æ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å¯¹é½äººç±»åå¥½ï¼Œä¸»è¦æ˜¯&#34;HHH&#34;åŸåˆ™ï¼Œå…·ä½“æ˜¯&#34;helpful, honest, harmless&#34;&lt;/li&gt; &#xA; &lt;li&gt;ç¬¬å››é˜¶æ®µï¼šRL(Reinforcement Learning)åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)ï¼Œç”¨å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒSFTæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Release Models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Base Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Introduction&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/shibing624/ziya-llama-13b-medical-lora&#34;&gt;shibing624/ziya-llama-13b-medical-lora&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1&#34;&gt;IDEA-CCNL/Ziya-LLaMA-13B-v1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;åœ¨240ä¸‡æ¡ä¸­è‹±æ–‡åŒ»ç–—æ•°æ®é›†&lt;a href=&#34;https://huggingface.co/datasets/shibing624/medical&#34;&gt;shibing624/medical&lt;/a&gt;ä¸ŠSFTå¾®è°ƒäº†ä¸€ç‰ˆZiya-LLaMA-13Bæ¨¡å‹ï¼ŒåŒ»ç–—é—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;â–¶ï¸ Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face Demo: doing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€æ´çš„åŸºäºgradioçš„äº¤äº’å¼webç•Œé¢ï¼Œå¯åŠ¨æœåŠ¡åï¼Œå¯é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼Œè¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹ä¼šè¿”å›ç­”æ¡ˆã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¯åŠ¨æœåŠ¡ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python gradio_demo.py --model_type base_model_type --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å‚æ•°è¯´æ˜ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type {base_model_type}&lt;/code&gt;ï¼šé¢„è®­ç»ƒæ¨¡å‹ç±»å‹ï¼Œå¦‚llamaã€bloomã€chatglmç­‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--base_model {base_model}&lt;/code&gt;ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--lora_model {lora_model}&lt;/code&gt;ï¼šLoRAæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚è‹¥loraæƒé‡å·²ç»åˆå¹¶åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™åˆ é™¤--lora_modelå‚æ•°&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--tokenizer_path {tokenizer_path}&lt;/code&gt;ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--use_cpu&lt;/code&gt;: ä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpus {gpu_ids}&lt;/code&gt;: æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚0,1,2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸš€ Training Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;Training Stage:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Stage&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Introduction&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Python script&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Shell script&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage 1: Continue Pretraining&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å¢é‡é¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/pretraining.py&#34;&gt;pretraining.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/run_pt.sh&#34;&gt;run_pt.sh&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage 2: Supervised Fine-tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æœ‰ç›‘ç£å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/supervised_finetuning.py&#34;&gt;supervised_finetuning.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/run_sft.sh&#34;&gt;run_sft.sh&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage 3: Reward Modeling&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å¥–åŠ±æ¨¡å‹å»ºæ¨¡&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/reward_modeling.py&#34;&gt;reward_modeling.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/run_rm.sh&#34;&gt;run_rm.sh&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage 4: Reinforcement Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å¼ºåŒ–å­¦ä¹ &lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/rl_training.py&#34;&gt;rl_training.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/run_rl.sh&#34;&gt;run_rl.sh&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æä¾›å®Œæ•´å››é˜¶æ®µä¸²èµ·æ¥è®­ç»ƒçš„pipelineï¼š&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/run_training_pipeline.ipynb&#34;&gt;run_training_pipeline.ipynb&lt;/a&gt; ï¼Œå…¶å¯¹åº”çš„colabï¼š &lt;a href=&#34;https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; ï¼Œè¿è¡Œå®Œå¤§æ¦‚éœ€è¦15åˆ†é’Ÿï¼Œæˆ‘è¿è¡ŒæˆåŠŸåçš„å‰¯æœ¬colabï¼š&lt;a href=&#34;https://colab.research.google.com/drive/1RGkbev8D85gR33HJYxqNdnEThODvGUsS?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82%E8%AF%B4%E6%98%8E&#34;&gt;è®­ç»ƒç»†èŠ‚è¯´æ˜wiki&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Supported Models&lt;/h4&gt; &#xA;&lt;p&gt;The following models are tested:&lt;/p&gt; &#xA;&lt;p&gt;bloom:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloomz-560m&#34;&gt;bigscience/bloomz-560m&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloomz-1b7&#34;&gt;bigscience/bloomz-1b7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloomz-7b1&#34;&gt;bigscience/bloomz-7b1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;llama:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/shibing624/chinese-alpaca-plus-7b-hf&#34;&gt;shibing624/chinese-alpaca-plus-7b-hf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf&#34;&gt;shibing624/chinese-alpaca-plus-13b-hf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/minlik/chinese-llama-plus-7b-merged&#34;&gt;minlik/chinese-llama-plus-7b-merged&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/shibing624/chinese-llama-plus-13b-hf&#34;&gt;shibing624/chinese-llama-plus-13b-hf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/decapoda-research/llama-7b-hf&#34;&gt;decapoda-research/llama-7b-hf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;chatglm:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;THUDM/chatglm-6b&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;baichuan:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B&#34;&gt;baichuan-inc/baichuan-7B&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ’» Inference&lt;/h2&gt; &#xA;&lt;p&gt;è®­ç»ƒå®Œæˆåï¼Œç°åœ¨æˆ‘ä»¬åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ŒéªŒè¯æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ•ˆæœã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --model_type base_model_type \&#xA;    --base_model path_to_llama_hf_dir \&#xA;    --lora_model path_to_lora \&#xA;    --with_prompt \&#xA;    --interactive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å‚æ•°è¯´æ˜ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type {base_model_type}&lt;/code&gt;ï¼šé¢„è®­ç»ƒæ¨¡å‹ç±»å‹ï¼Œå¦‚llamaã€bloomã€chatglmç­‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--base_model {base_model}&lt;/code&gt;ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--lora_model {lora_model}&lt;/code&gt;ï¼šLoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚å¦‚æœå·²ç»åˆå¹¶äº†LoRAæƒé‡åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™å¯ä»¥ä¸æä¾›æ­¤å‚æ•°&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--tokenizer_path {tokenizer_path}&lt;/code&gt;ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--with_prompt&lt;/code&gt;ï¼šæ˜¯å¦å°†è¾“å…¥ä¸promptæ¨¡ç‰ˆè¿›è¡Œåˆå¹¶ã€‚å¦‚æœåŠ è½½Alpacaæ¨¡å‹ï¼Œè¯·åŠ¡å¿…å¯ç”¨æ­¤é€‰é¡¹ï¼&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--interactive&lt;/code&gt;ï¼šä»¥äº¤äº’æ–¹å¼å¯åŠ¨ï¼Œä»¥ä¾¿è¿›è¡Œå¤šæ¬¡å•è½®é—®ç­”&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_file {file_name}&lt;/code&gt;ï¼šéäº¤äº’æ–¹å¼å¯åŠ¨ä¸‹ï¼ŒæŒ‰è¡Œè¯»å–file_nameä¸­çš„çš„å†…å®¹è¿›è¡Œé¢„æµ‹&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--predictions_file {file_name}&lt;/code&gt;ï¼šéäº¤äº’å¼æ–¹å¼ä¸‹ï¼Œå°†é¢„æµ‹çš„ç»“æœä»¥jsonæ ¼å¼å†™å…¥file_name&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--use_cpu&lt;/code&gt;: ä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpus {gpu_ids}&lt;/code&gt;: æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚0,1,2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Inference Examples&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/shibing624/ziya-llama-13b-medical-lora&#34;&gt;shibing624/ziya-llama-13b-medical-lora&lt;/a&gt; inference examples:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Human&lt;/th&gt; &#xA;   &lt;th&gt;Assitant&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å°å­©å‘çƒ§æ€ä¹ˆåŠ&lt;/td&gt; &#xA;   &lt;td&gt;å®å®çš„æŠµæŠ—åŠ›æ˜¯æ¯”è¾ƒå·®çš„ï¼Œæ‰€ä»¥å¾ˆå®¹æ˜“å°±ä¼šå‡ºç°æ„Ÿå†’æˆ–è€…å…¶ä»–ç–¾ç—…ã€‚å¦‚æœå®å®åœ¨å¹³æ—¶æ²¡æœ‰æ³¨æ„ä¿æš–ï¼Œé‚£ä¹ˆä¹Ÿå¯èƒ½å¯¼è‡´èº«ä½“å—å‡‰è€Œå¼•èµ·äº†æ„Ÿå†’ç­‰ç—‡çŠ¶ï¼Œè¿™æ ·ä¸ä»…å¯¹äºå¥åº·é€ æˆå½±å“ï¼Œè¿˜å¯èƒ½è®©å®å®å‡ºç°é«˜çƒ­çš„æƒ…å†µã€‚å› æ­¤å½“å®¶é•¿ä»¬å‘ç°è‡ªå·±çš„å®å®å‡ºç°äº†é«˜çƒ­çš„é—®é¢˜ä¹‹åä¸€å®šè¦åŠæ—©è¿›è¡Œæ²»ç–—å’ŒæŠ¤ç†æ‰å¥½ã€‚&lt;br&gt;1ã€é¦–å…ˆæˆ‘ä»¬åº”è¯¥ç»™å®å®æœç”¨é€€çƒ§è¯ç‰©æ¥å¸®åŠ©é™æ¸©ï¼Œä½†æ˜¯åƒä¸‡ä¸è¦ç›²ç›®ä½¿ç”¨é€€çƒ§è¯ç‰©ï¼Œå¦åˆ™åªä¼šåŠ é‡å®å®çš„ç—…æƒ…ï¼Œæœ€å¥½å¸¦ç€å®å®åˆ°åŒ»é™¢å»æ£€æŸ¥æ¸…æ¥šå…·ä½“åŸå› ä¹‹åå†é’ˆå¯¹æ€§åœ°é‡‡å–æªæ–½å¤„ç†ã€‚&lt;br&gt;2ã€å¦å¤–å¤§äººä»¬ä¹Ÿéœ€è¦åšå¥½é¢„é˜²å·¥ä½œï¼Œå°¤å…¶æ˜¯å¤©æ°”å˜åŒ–çš„æ—¶å€™æ›´åº”è¯¥å¤šå…³å¿ƒä¸‹å®å®çš„èº«ä½“çŠ¶æ€ï¼Œé¿å…å®å®å—å¯’å—å‡‰ä»è€Œè¯±å‘å„ç§ç–¾ç—…ï¼ŒåŒæ—¶ä¹Ÿè¦å°½é‡å‡å°‘å®å®æ¥è§¦ç»†èŒç—…æ¯’çš„æœºä¼šï¼Œå¢å¼ºå®å®çš„å…ç–«åŠ›ã€‚&lt;br&gt;3ã€é™¤äº†ä¸Šé¢ä¸¤ç‚¹ä¹‹å¤–ï¼Œå¤§äººä»¬è¿˜å¯ä»¥é€šè¿‡æŒ‰æ‘©æ¨æ‹¿ç©´ä½çš„æ–¹å¼æ¥ç¼“è§£å®å®çš„ç—…ç—›ï¼Œä¿ƒè¿›è¡€æ¶²å¾ªç¯ï¼Œæå‡æ–°é™ˆä»£è°¢é€Ÿåº¦ï¼Œè¾¾åˆ°å¿«é€Ÿæ¶ˆç‚æ­¢ç–¼çš„æ•ˆæœ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä¸€å²å®å®å‘çƒ§èƒ½åƒå•¥è¯ï¼Ÿ&lt;/td&gt; &#xA;   &lt;td&gt;å­©å­çš„èº«ä½“æ˜¯å¾ˆè„†å¼±çš„ï¼Œå¦‚æœä¸æ³¨æ„ä¿æŠ¤å¥½è‡ªå·±å°±ä¼šå‡ºç°å„ç§ç–¾ç—…ã€‚æœ‰äº›å®¶é•¿åœ¨çœ‹åˆ°å­©å­ç”Ÿç—…åéƒ½éå¸¸ç€æ€¥ï¼Œæƒ³è¦ç«‹åˆ»ç»™ä»–ä»¬ç”¨è¯æ²»ç–—ï¼Œä½†å…¶å®å¹¶æ²¡æœ‰é‚£ä¹ˆç®€å•ï¼Œå› ä¸ºæ¯ä¸ªäººå¯¹äºè¯ç‰©çš„æ•æ„Ÿåº¦å’Œè€å—æ€§éƒ½æ˜¯ä¸åŒçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ ¹æ®å…·ä½“æƒ…å†µæ¥é€‰æ‹©åˆé€‚çš„æ–¹æ³•è¿›è¡Œæ²»ç–—æ‰å¯ä»¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚&lt;br&gt;1ã€é¦–å…ˆå¤§å¤šæ•°å°å„¿é€€çƒ­è¯ä¸­å«æœ‰è§£çƒ­é•‡ç—›æˆåˆ†é˜¿å¸åŒ¹æ—æˆ–å¸ƒæ´›èŠ¬ç­‰ï¼Œè¿™ç±»è¯å“è™½ç„¶å‰¯ä½œç”¨è¾ƒå°‘ï¼Œä½†ä¹Ÿå­˜åœ¨è¿‡é‡ä½¿ç”¨å¼•èµ·èƒƒè‚ é“æŸä¼¤ç”šè‡³æ¶ˆåŒ–é“æºƒç–¡å‡ºè¡€çš„é£é™©ï¼›&lt;br&gt;2ã€è¿˜æœ‰éƒ¨åˆ†å°å„¿é€€çƒ­è¯ä¸­å«æœ‰æ‰‘å°”æ•ç­‰æŠ—ç»„èƒºè¯ï¼Œå®ƒä»¬æœ¬èº«å±äºå¤„æ–¹è¯ï¼Œåº”è¯¥ä¸¥æ ¼æŒ‰ç…§åŒ»å˜±æœç”¨ï¼Œåˆ‡å‹¿æ“…è‡ªåŠ å‡å‰‚é‡æˆ–è€…å»¶é•¿ç”¨è¯æ—¶é—´;&lt;br&gt;3ã€æ­¤å¤–ï¼ŒæŸäº›å°å„¿é€€çƒ­è¯ä¸­å«æœ‰ç»´ç”Ÿç´ CåŠè‘¡è„ç³–é…¸é’™ç­‰è¥å…»è¡¥å……å‰‚ï¼Œåªæœ‰å½“æ‚£å„¿ä¼´éšè„±æ°´ç—‡çŠ¶ï¼ˆå¦‚å£æ¸´ã€å°¿è‰²æ·±ï¼‰æ—¶æ‰è€ƒè™‘æ·»åŠ ï¼Œå¦åˆ™å®¹æ˜“é€ æˆç”µè§£è´¨ç´Šä¹±ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è‚›é—¨ç—…å˜å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…çš„ç—‡çŠ¶&lt;/td&gt; &#xA;   &lt;td&gt;ç—”ç–®ã€ç›´è‚ æ¯è‚‰ç­‰ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç”Ÿï¼Œæ‚¨å¥½ç™½å¸¦å¢å¤šæ³›é»„æ€ä¹ˆå›äº‹ï¼Ÿ&lt;/td&gt; &#xA;   &lt;td&gt;ä½ è¿™ç§æƒ…å†µè€ƒè™‘å¯èƒ½ä¼šæ˜¯é˜´é“ç‚ç—‡å¼•èµ·çš„å»ºè®®å¹³æ—¶ç•™æ„å«ç”Ÿæ¸…æ´ï¼Œå‹¤æ¢æ´—è¡£ç‰©å†…è£¤ï¼Œå¿…è¦æ—¶å»æ­£è§„å…¬ç«‹åŒ»é™¢å¦‡ç§‘å¤è¯Šåšåˆ†æ³Œç‰©å¸¸è§„ç­‰ç›¸å…³æ£€æŸ¥æ˜ç¡®ç—…å› åå¯¹ç—‡æ²»ç–—ï¼Œä¸åŒåŸå› æ‰€è‡´çš„ç–¾ç—…åœ¨ä¸´åºŠè¡¨ç°ä¸Šä¹Ÿå­˜åœ¨å·®å¼‚æ€§ï¼Œå¦‚æœæ‚£è€…å‡ºç°äº†ç±»ä¼¼äºçš„ç—‡çŠ¶åº”å½“åŠæ—©åˆ°ä¸“ä¸šã€æƒå¨çš„åŒ»é™¢è¿›è¡Œè¯¦ç»†çš„æ£€éªŒå’Œæ•‘æ²»&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—&lt;/td&gt; &#xA;   &lt;td&gt;åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬è¦æˆ–å¤šæˆ–å°‘äº†è§£ä¸€äº›å¸¸è§ç–¾ç—…çš„ç›¸å…³çŸ¥è¯†ï¼Œä¸ä»…èƒ½èµ·åˆ°å¢åŠ çŸ¥è¯†æ°´å¹³çš„ä½œç”¨ï¼Œæ›´èƒ½è®©è‡ªå·±æ›´å¥½åœ°é¢„é˜²æ­¤ç±»ç–¾ç—…çš„äº§ç”Ÿã€‚å¾ˆå¤šå¥³æ€§éƒ½ä¼šå‡ºç°æœˆç»ç´Šä¹±ã€è…¹ç—›ç­‰ç—‡çŠ¶ï¼Œè¿™æ—¶å€™å°±éœ€è¦è€ƒè™‘å¦‡ç§‘ç‚ç—‡å¯èƒ½ï¼Œæ¯”å¦‚è¯´å®«é¢ˆç³œçƒ‚å’Œç›†è…”ç§¯æ¶²ã€‚é‚£ä¹ˆä¸‹é¢å°ç¼–æ¥ç»™å¤§å®¶ä»‹ç»ä¸€ç§æƒ…å†µï¼šè¯ç‰©æµäº§åç™½å¸¦å‘ˆå·§å…‹åŠ›è‰²æ€ä¹ˆå›äº‹ï¼Ÿ&lt;br&gt;1ã€é¦–å…ˆï¼Œå¯¹äºè¯ç‰©æµäº§ä¹‹åå‘ç°é˜´é“åˆ†æ³Œç‰©å¼‚å¸¸ï¼Œåº”è¯¥åŠæ—¶å»åŒ»é™¢åšæ£€æŸ¥ç¡®è¯Šå…·ä½“åŸå› ï¼Œç„¶åå†æ ¹æ®ç»“æœè¿›è¡Œæ²»ç–—è°ƒç†ã€‚&lt;br&gt;2ã€å…¶æ¬¡ï¼Œç”±äºæ¯ä¸ªäººèº«ä½“ç´ è´¨ä»¥åŠæ‚£ç—…ç¨‹åº¦å­˜åœ¨å·®åˆ«ï¼Œæ‰€ä»¥é‡‡å–çš„æªæ–½ä¹Ÿä¸å°½ç›¸åŒï¼Œä½†æ— è®ºé€‰æ‹©å“ªç§æ–¹å¼å¤„ç†ï¼Œéƒ½å¿…é¡»ä¸¥æ ¼æŒ‰ç…§è§„å®šå‰‚é‡æœç”¨ï¼Œå¹¶ä¸”æ³¨æ„è§‚å¯Ÿå‰¯ä½œç”¨åæ˜ æƒ…å†µï¼Œè‹¥å‡ºç°æ˜æ˜¾ä¸é€‚æ„Ÿè§‰ï¼Œåˆ™åº”ç«‹å³åœæ­¢ä½¿ç”¨ã€‚&lt;br&gt;3ã€æœ€åï¼Œå»ºè®®ä½ ä¿æŒå¤–é˜´æ¸…æ´å«ç”Ÿï¼Œå‹¤æ¢å†…è£¤ï¼Œé¿å…åƒè¾›è¾£åˆºæ¿€é£Ÿå“ï¼Œè€Œä¸”è¿˜è¦ç¦å¿Œæˆ¿äº‹ï¼Œå¦åˆ™å®¹æ˜“å¼•èµ·äº¤å‰æ„ŸæŸ“å½±å“æ¢å¤å¥åº·ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç»´èƒºé…¯ç»´Eä¹³è†èƒ½æ²»ç†ä»€ä¹ˆç–¾ç—…&lt;/td&gt; &#xA;   &lt;td&gt;çš®è‚¤æ·€ç²‰æ ·å˜æ€§ï¼›è„‚æº¢æ€§è§’åŒ–ç—‡&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç—¤ç–®æ˜¯ä»€ä¹ˆåŸå› å¯¼è‡´çš„?&lt;/td&gt; &#xA;   &lt;td&gt;ç—¤ç–®ï¼Œåˆç§°é’æ˜¥ç—˜ã€ç²‰åˆºã€‚å®ƒä¸»è¦å‘ç”Ÿåœ¨é¢éƒ¨å’Œèƒ¸èƒŒéƒ¨ä½ï¼Œè¡¨ç°ä¸ºé»‘å¤´ç²‰åˆºã€ç™½å¤´ç²‰åˆºã€ä¸˜ç–¹ã€è„“ç–±ã€ç»“èŠ‚ç­‰å¤šç§å½¢å¼ã€‚å…¶ä¸­ä»¥é»‘å¤´ç²‰åˆºæœ€å¸¸è§ï¼Œçº¦å å…¨èº«çš®è‚¤ç—…æ‚£è€…çš„70%å·¦å³ã€‚é‚£ä¹ˆï¼Œç—¤ç–®æ˜¯ä»€ä¹ˆåŸå› å¼•èµ·çš„å‘¢ï¼Ÿä¸‹é¢å°±ç»™å¤§å®¶ä»‹ç»ä¸€äº›ç›¸å…³å†…å®¹ã€‚&lt;br&gt;1ã€é›„æ€§æ¿€ç´ åˆ†æ³Œè¿‡æ—ºï¼šç”·å¥³éƒ½ä¼šé•¿ç—˜ç—˜ï¼Œä½†æ˜¯å¥³å­©å­æ¯”è¾ƒå–œæ¬¢ç”¨åŒ–å¦†å“æ¥æ©ç›–è„¸ä¸Šçš„é—®é¢˜ï¼Œè€Œä¸”å¾ˆå¤šäººä¸çŸ¥é“è¿™æ ·åšåè€ŒåŠ é‡äº†è‡ªå·±çš„æ¯›å›Šç‚ç—‡çŠ¶ï¼Œæ‰€ä»¥è¯´å¹³æ—¶åº”è¯¥å°‘ä½¿ç”¨å«æœ‰é›Œæ¿€ç´ ç±»çš„äº§å“ã€‚&lt;br&gt;2ã€é¥®é£Ÿä¹ æƒ¯å·®ï¼šç»å¸¸åƒè¾›è¾£æ²¹è…»çš„ä¸œè¥¿æˆ–è€…ç”œé£Ÿä¹Ÿå¯èƒ½é€ æˆä½“å†…æ¯’ç´ å †ç§¯ï¼Œä»è€Œè¯±å‘ç—¤ç–®ã€‚&lt;br&gt;3ã€å‹åŠ›å¤ªå¤§ï¼šå¦‚æœä½ æ€»æ˜¯å¤„äºç´§å¼ ç„¦è™‘çš„æƒ…ç»ªä¹‹ä¸­ï¼ŒåŒæ—¶è¿˜æ²¡æœ‰åŠæ—¶è°ƒæ•´å¥½å¿ƒæ€ï¼Œé‚£ä¹ˆå¾ˆå®¹æ˜“å‡ºç°å†…åˆ†æ³Œå¤±è°ƒçš„æƒ…å†µï¼Œè¿›è€Œå½±å“åˆ°è‚Œè‚¤å¥åº·ã€‚&lt;br&gt;4ã€ç¡çœ è´¨é‡å·®ï¼šç†¬å¤œå¯¹æˆ‘ä»¬çš„èº«ä½“ä¼¤å®³éå¸¸ä¸¥é‡ï¼Œå°¤å…¶æ˜¯å¯¹äºå¹´è½»äººç¾¤æ›´ç”šï¼Œä»–ä»¬éœ€è¦ä¿è¯å……è¶³çš„ç¡çœ æ‰è¡Œï¼Œå¦åˆ™å¾ˆå®¹æ˜“è®©è‚Œè‚¤å˜å¾—æš—æ²‰æ— å…‰æ³½ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è„¸ä¸Šé•¿é’æ˜¥ç—˜æ€ä¹ˆåŠ,èƒ½åƒå•¥è¯,éœ€è¦æ³¨æ„å•¥&lt;/td&gt; &#xA;   &lt;td&gt;ä½ å¥½ï¼Œæ ¹æ®æ‚¨çš„æè¿°è€ƒè™‘æ˜¯ç—¤ç–®ã€‚å»ºè®®å¤–ç”¨ç»´èƒºé…¯ä¹³è†æˆ–è€…é˜¿è¾¾å¸•æ—å‡èƒ¶è¿›è¡Œæ²»ç–—çœ‹çœ‹ï¼Œå¹³æ—¶ç¦å¿Œè¾›è¾£å’Œåˆºæ¿€æ€§é£Ÿç‰©ï¼Œå¦‚æµ·é²œã€ç‹—è‚‰ã€ç¾Šè‚‰ç­‰â€œå‘â€åŠ¨å“ï¼Œä»¥åŠè‘±è’œéŸ­å§œç­‰åˆºæ¿€æ€§è°ƒæ–™ã€‚å¤šå–æ°´ï¼Œå°‘åƒæ²¹è…»é«˜çƒ­é‡é£Ÿç‰©ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç”±äºè¾“ç²¾ç®¡å µå¡ä¼šä¸ä¼šå¯¼è‡´ç”·æ€§ä¸è‚²ï¼Ÿï¼ŒåŒ»ç”Ÿæ‚¨å¥½ï¼Œæˆ‘åŒäº‹è¿™å‡ å¤©ä¸å¼€å¿ƒï¼Œæˆ‘é—®å¥¹æ€ä¹ˆäº†ï¼Œå¥¹è¯´å¥¹è€å…¬è¾“ç²¾ç®¡å µå¡ï¼Œæ‰€ä»¥ä¸€ç›´æ²¡å­©å­ï¼Œä¸æ˜¯ä¸æƒ³è¦ï¼Œè¯·é—®è¾“ç²¾ç®¡å µå¡ä¼šä¸ä¼šå¯¼è‡´ç”·æ€§ä¸è‚²ï¼Ÿ&lt;/td&gt; &#xA;   &lt;td&gt;ä½ å¥½ï¼æ ¹æ®ä½ çš„å™è¿°åŠç—‡çŠ¶æ¨æµ‹ï¼šå¦‚æœæœ‰è¾“ç²¾ç®¡é˜»å¡æˆ–è€…ç‹­çª„ç­‰æƒ…å†µéƒ½å¯èƒ½ä¼šå¼•æ¥æ— æ³•æ’å‡ºæ¥ç²¾æ¶²è€Œå½±å“æ€€ä¸Šã€‚å»ºè®®åˆ°æ­£è§„ä¸“ä¸šçš„å¿«å­•åŒ»é™¢å®æ–½è¿›è¡Œå’¨è¯¢å’Œæ²»ç–—ä¸ºå®œã€‚å¯¹äºè¾“ç²¾ç®¡æ‚£è€…æ¥è¯´ï¼ŒåŠæ—¶æ•‘æ²»æ˜¯å¯ä»¥å‡ä½ç—…å‘ï¼Œå¦‚æœä¸åŠæ—¶æ•‘æ²»ä¼šå½±å“åˆ°å—å­•æƒ…å†µã€‚å› æ­¤æ‚£è€…è¦åŠæ—¶è¯ç‰©æ•‘æ²»ï¼Œå¹¶ä¸”æ—¥å¸¸ç”Ÿæ´»ä¸­è¦å¤šçœ‹é‡è‡ªèº«æŠ¤ç†&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¿ƒè‚Œæ¢—å¡èƒ½æ²»ç–—å—ï¼Ÿ&lt;/td&gt; &#xA;   &lt;td&gt;å¿ƒè„ç—…æ˜¯ä¸€ç§å±å®³æ€§æå¤§çš„ç–¾ç—…ï¼Œæˆ‘ä»¬è¦å¯¹å®ƒæœ‰è¶³å¤Ÿå¤šçš„äº†è§£ã€‚åœ¨ç”Ÿæ´»ä¸­å¾ˆå¤šäººéƒ½ä¼šæ‚£ä¸Šè¿™ä¸ªç–¾ç—…ï¼Œè€Œä¸”è¿˜ä¸çŸ¥é“æ€ä¹ˆå»é¢„é˜²å’ŒæŠ¤ç†ï¼Œæ‰€ä»¥å°±å¯¼è‡´è‡ªå·±çš„èº«ä½“è¶Šæ¥è¶Šå·®ï¼Œé‚£ä¹ˆä¸‹é¢å°ç¼–ç»™ä½ ä»‹ç»å‡ ç‚¹å…³äºå¿ƒè„ç—…çš„å¸¸è¯†å§ï¼&lt;br&gt;1ã€å† çŠ¶åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–æ€§å¿ƒè„ç—…ï¼šåˆç§°ä¸ºå† å¿ƒç—…æˆ–ç¼ºè¡€å‹å¿ƒè„ç—…ï¼ˆIHDï¼‰,æ˜¯ä¸–ç•Œå„åœ°æœ€é‡è¦çš„æ­»äº¡åŸå› ä¹‹ä¸€ã€‚&lt;br&gt;2ã€é£æ¹¿çƒ­ï¼šä¸»è¦ä¾µçŠ¯å¿ƒè„ç“£è†œï¼Œå¯å¼•èµ·äºŒå°–ç“£ç‹­çª„åŠå…¶ä»–ç±»å‹çš„å¿ƒè„ç“£è†œç—…å˜ï¼›&lt;br&gt;3ã€å…ˆå¤©æ€§å¿ƒè„ç—…ï¼šåŒ…æ‹¬æˆ¿é—´éš”ç¼ºæŸã€å®¤é—´éš”ç¼ºæŸç­‰ï¼›&lt;br&gt;4ã€é«˜è„‚è›‹ç™½è¡€ç—‡ï¼šç”±äºèƒ†å›ºé†‡ä¸ä½å¯†åº¦è„‚è›‹ç™½å‡é«˜ï¼Œä½¿å¾—åŠ¨è„‰å£å¢åšï¼Œç®¡è…”ç¼©å°ï¼Œæ˜“å½¢æˆæ–‘å—å¹¶è„±è½é€ æˆæ€¥æ€§æ “å¡ï¼›&lt;br&gt;5ã€è‚¥èƒ–ï¼šè¿‡é‡æ‘„å…¥é£Ÿç‰©ï¼Œç‰¹åˆ«æ˜¯å«ç³–é¥®æ–™ï¼ŒåŠ é€Ÿè„‚è‚ªå †ç§¯ï¼Œä¿ƒè¿›åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–å‘å±•ï¼›&lt;br&gt;6ã€å¸çƒŸï¼šå°¼å¤ä¸åˆºæ¿€äº¤æ„Ÿç¥ç»å…´å¥‹ï¼Œä½¿å¤–å‘¨é˜»åŠ›å¢å¼ºï¼Œæ”¶ç¼©å‹å‡é«˜ï¼Œèˆ’å¼ æœŸå‡çŸ­ï¼Œä»è€Œé™ä½å·¦å³å¿ƒå®¤å°„è¡€åˆ†æ•°ï¼Œè¯±å‘å¿ƒå¾‹å¤±å¸¸ï¼›&lt;br&gt;7ã€é…—é…’ï¼šé•¿æœŸå—œé…’è€…ï¼Œè‚åŠŸå—åˆ°å½±å“ï¼Œä»£è°¢ç´Šä¹±ï¼Œå®¹æ˜“å‡ºç°è„‚è´¨ä»£è°¢å¼‚å¸¸ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ€ä¹ˆè®©è‡ªå·±ç²¾åŠ›å……æ²›ï¼Œåˆ—5ç‚¹å»ºè®®&lt;/td&gt; &#xA;   &lt;td&gt;1ã€ä¿è¯ç¡çœ è´¨é‡ï¼šæ¯å¤©æ™šä¸Š7-8å°æ—¶çš„é«˜è´¨é‡ç¡çœ æ˜¯æå‡èº«ä½“å’Œå¤§è„‘æ´»è·ƒåº¦æœ€æœ‰æ•ˆçš„æ–¹æ³•ã€‚&lt;br&gt;2ã€åˆç†é¥®é£Ÿï¼šå‡è¡¡è¥å…»æ‘„å…¥å¯¹äºç»´æŒäººä½“æ­£å¸¸è¿è½¬è‡³å…³é‡è¦ï¼Œå¤šåƒè”¬èœæ°´æœï¼Œå°‘æ²¹è…»è¾›è¾£åˆºæ¿€æ€§é£Ÿç‰©ã€‚&lt;br&gt;3ã€é€‚å½“é”»ç‚¼ï¼šåšæŒä¸€å®šå¼ºåº¦çš„é”»ç‚¼å¯ä»¥ä¿ƒè¿›è¡€æ¶²å¾ªç¯ï¼Œå¢åŠ è‚Œè‚‰è€åŠ›ï¼Œæ”¹å–„å¿ƒè‚ºåŠŸèƒ½ç­‰ã€‚&lt;br&gt;4ã€å­¦ä¹ æ–°çŸ¥è¯†ï¼šä¸æ–­æ¥è§¦æ–°äº‹ç‰©ï¼Œå¼€æ‹“è§†é‡ï¼Œæ€è€ƒé—®é¢˜ï¼Œéƒ½ä¼šä½¿å¾—æˆ‘ä»¬æ›´å…·åˆ›é€ åŠ›ä¸çµæ„Ÿã€‚&lt;br&gt;5ã€æ”¾æ¾å¿ƒæƒ…ï¼šé€šè¿‡å†¥æƒ³æˆ–è€…ç‘œä¼½æ¥ç¼“è§£å‹åŠ›ï¼Œä¹Ÿå¯ä»¥å¬éŸ³ä¹æˆ–çœ‹ç”µå½±æ¥è°ƒèŠ‚æƒ…ç»ªã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å†™ä¸ªæ˜¥å¤©çš„è¯—&lt;/td&gt; &#xA;   &lt;td&gt;æ˜¥é£å¹æ‹‚ï¼ŒèŠ±å„¿ç»½æ”¾ã€‚&lt;br&gt;é˜³å…‰æ˜åªšï¼Œé¸Ÿè¯­é¸£å«ã€‚&lt;br&gt;æ ‘æœ¨æŠ½èŠ½ï¼Œè‰åœ°å˜ç»¿ã€‚&lt;br&gt;ä¸‡ç‰©å¤è‹ï¼Œç”Ÿæœºå‹ƒå‘ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å—ææ´²ä¸ºå•¥æ²¡æœ‰åŒ—æç†Š?&lt;/td&gt; &#xA;   &lt;td&gt;å—ææ˜¯ä¸–ç•Œä¸Šæœ€å¤§çš„å†°ç›–ï¼Œè¦†ç›–ç€98ï¼…ä»¥ä¸Šçš„åœ°çƒè¡¨é¢ç§¯ã€‚å®ƒä½äºå—çº¬60åº¦è‡³74åº¦ä¹‹é—´ï¼Œç¯ç»•åœ¨æµ·æ´‹ä¸­å¿ƒå‘¨å›´ã€‚ç”±äºå…¶ç‹¬ç‰¹çš„æ°”å€™æ¡ä»¶å’Œç”Ÿæ€ç³»ç»Ÿï¼Œå—æè¢«ç§°ä¸ºâ€œçœŸæ­£çš„è’é‡â€ã€‚å—ææ´²æ‹¥æœ‰ä¸°å¯Œå¤šæ ·çš„åŠ¨æ¤ç‰©èµ„æºï¼Œä½†è¿™é‡Œå¹¶ä¸å­˜åœ¨åŒ—æç†Šã€‚ &lt;br&gt;é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆå—ææ´²æ²¡æœ‰åŒ—æç†Šå‘¢ï¼Ÿ&lt;br&gt;é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä¸€ä¸‹ä¸¤è€…æ –æ¯åœ°çš„å·®å¼‚ã€‚åŒ—æç†Šä¸»è¦åˆ†å¸ƒåœ¨åŒ—åŠçƒå¯’å†·çš„é«˜çº¬åº¦åœ°åŒºï¼Œå¦‚åŠ æ‹¿å¤§ã€æ ¼é™µå…°å²›ç­‰åœ°ã€‚è€Œå—ææ´²åˆ™å¤„äºå—åŠçƒï¼Œå±äºæ¸©å¸¦æˆ–å¯’å¸¦æ°”å€™ç±»å‹ã€‚è™½ç„¶å—ææ´²ä¹Ÿæœ‰å¾ˆä½çš„å¹³å‡æ°”æ¸©ï¼ˆ-25â„ƒå·¦å³ï¼‰ï¼Œä½†ä¸åŒ—æç›¸æ¯”è¿˜æ˜¯å¤ªçƒ­äº†ã€‚å› æ­¤ï¼Œå³ä½¿åŒ—æç†Šèƒ½å¤Ÿé€‚åº”æ›´ä¸¥é…·çš„æ°”å€™æ¡ä»¶ï¼Œå®ƒä»¬ä¹Ÿæ— æ³•åœ¨å—ææ‰¾åˆ°åˆé€‚çš„æ –æ¯åœ°ã€‚&lt;br&gt;å¦å¤–ï¼Œå—ææ´²ç¼ºä¹é™†åœ°å“ºä¹³åŠ¨ç‰©é£Ÿç‰©æ¥æºï¼ŒåŒ…æ‹¬é±¼ç±»ã€é²¸é±¼å’Œä¼é¹…ç­‰ã€‚å°½ç®¡å—ææ´²çš„æ°´åŸŸä¸­ä¹Ÿæœ‰å„ç§é±¼ç±»ï¼Œä½†æ•°é‡è¿œå°‘äºåŒ—æåœˆå†…ã€‚&lt;br&gt;åŒæ—¶ï¼Œå—ææ´²çš„åœŸè‘—å±…æ°‘â€”â€”ä¼é¹…ç¾¤ä½“ç¹æ®–å­£èŠ‚æœŸé—´ä¼šæ¶ˆè€—æ‰å¤§éƒ¨åˆ†å¯ç”¨çš„é£Ÿç‰©èµ„æºï¼Œå¯¼è‡´å½“åœ°çš„é±¼ç±»æ•°é‡å‡å°‘ç”šè‡³æ¯ç«­ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ğŸ“š Dataset&lt;/h2&gt; &#xA;&lt;h3&gt;åŒ»ç–—æ•°æ®é›†&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;240ä¸‡æ¡ä¸­æ–‡åŒ»ç–—æ•°æ®é›†(åŒ…æ‹¬é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’Œå¥–åŠ±æ•°æ®é›†)ï¼š&lt;a href=&#34;https://huggingface.co/datasets/shibing624/medical&#34;&gt;shibing624/medical&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;22ä¸‡æ¡ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›†(åä½—é¡¹ç›®)ï¼š&lt;a href=&#34;https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1&#34;&gt;FreedomIntelligence/HuatuoGPT-sft-data-v1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;é€šç”¨æ•°æ®é›†&lt;/h3&gt; &#xA;&lt;h4&gt;SFT datasets&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;50ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BelleGroup/train_0.5M_CN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BelleGroup/train_1M_CN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca#data-release&#34;&gt;50k English Stanford Alpaca dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;2ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/shibing624/alpaca-zh&#34;&gt;shibing624/alpaca-zh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)ï¼š&lt;a href=&#34;https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0&#34;&gt;Chinese-Vicuna/guanaco_belle_merge_v1.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;5ä¸‡æ¡è‹±æ–‡ChatGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/RyokoAI/ShareGPT52K&#34;&gt;RyokoAI/ShareGPT52K&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;80ä¸‡æ¡ä¸­æ–‡ChatGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BelleGroup/multiturn_chat_0.8M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;116ä¸‡æ¡ä¸­æ–‡ChatGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/fnlp/moss-002-sft-data&#34;&gt;fnlp/moss-002-sft-data&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Reward Model datasets&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åŸç‰ˆçš„oasst1æ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;OpenAssistant/oasst1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;2ä¸‡æ¡å¤šè¯­è¨€oasst1çš„rewardæ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/tasksource/oasst1_pairwise_rlhf_reward&#34;&gt;tasksource/oasst1_pairwise_rlhf_reward&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;11ä¸‡æ¡è‹±æ–‡hh-rlhfçš„rewardæ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/Dahoas/full-hh-rlhf&#34;&gt;Dahoas/full-hh-rlhf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;9ä¸‡æ¡è‹±æ–‡rewardæ•°æ®é›†(æ¥è‡ªAnthropic&#39;s Helpful Harmless dataset)ï¼š&lt;a href=&#34;https://huggingface.co/datasets/Dahoas/static-hh&#34;&gt;Dahoas/static-hh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;7ä¸‡æ¡è‹±æ–‡rewardæ•°æ®é›†ï¼ˆæ¥æºåŒä¸Šï¼‰ï¼š&lt;a href=&#34;https://huggingface.co/datasets/Dahoas/rm-static&#34;&gt;Dahoas/rm-static&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;7ä¸‡æ¡ç¹ä½“ä¸­æ–‡çš„rewardæ•°æ®é›†ï¼ˆç¿»è¯‘è‡ªrm-staticï¼‰&lt;a href=&#34;https://huggingface.co/datasets/liswei/rm-static-m2m100-zh&#34;&gt;liswei/rm-static-m2m100-zh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;7ä¸‡æ¡è‹±æ–‡Rewardæ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets&#34;&gt;yitingxie/rlhf-reward-datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;3åƒæ¡ä¸­æ–‡çŸ¥ä¹é—®ç­”åå¥½æ•°æ®é›†ï¼š&lt;a href=&#34;https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k&#34;&gt;liyucheng/zhihu_rlhf_3k&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;âœ… Todo&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add multi-round dialogue data fine-tuning method&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add reward model fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add rl fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add medical reward dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add llama in8/int4 training&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add all training and predict demo in colab&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;â˜ï¸ Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Issue(å»ºè®®) ï¼š&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/shibing624/MedicalGPT.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;é‚®ä»¶æˆ‘ï¼šxuming: &lt;a href=&#34;mailto:xuming624@qq.com&#34;&gt;xuming624@qq.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘&lt;em&gt;å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP&lt;/em&gt; è¿›NLPäº¤æµç¾¤ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://github.com/shibing624/MedicalGPT/raw/main/docs/wechat.jpeg&#34; width=&#34;200&#34;&gt; &#xA;&lt;h2&gt;âš ï¸ å±€é™æ€§ã€ä½¿ç”¨é™åˆ¶ä¸å…è´£å£°æ˜&lt;/h2&gt; &#xA;&lt;p&gt;åŸºäºå½“å‰æ•°æ®å’ŒåŸºç¡€æ¨¡å‹è®­ç»ƒå¾—åˆ°çš„SFTæ¨¡å‹ï¼Œåœ¨æ•ˆæœä¸Šä»å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;åœ¨æ¶‰åŠäº‹å®æ€§çš„æŒ‡ä»¤ä¸Šå¯èƒ½ä¼šäº§ç”Ÿè¿èƒŒäº‹å®çš„é”™è¯¯å›ç­”ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¯¹äºå…·å¤‡å±å®³æ€§çš„æŒ‡ä»¤æ— æ³•å¾ˆå¥½çš„é‰´åˆ«ï¼Œç”±æ­¤ä¼šäº§ç”Ÿå±å®³æ€§è¨€è®ºã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åœ¨ä¸€äº›æ¶‰åŠæ¨ç†ã€ä»£ç ã€å¤šè½®å¯¹è¯ç­‰åœºæ™¯ä¸‹æ¨¡å‹çš„èƒ½åŠ›ä»æœ‰å¾…æé«˜ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;åŸºäºä»¥ä¸Šæ¨¡å‹å±€é™æ€§ï¼Œæˆ‘ä»¬è¦æ±‚å¼€å‘è€…ä»…å°†æˆ‘ä»¬å¼€æºçš„æ¨¡å‹æƒé‡åŠåç»­ç”¨æ­¤é¡¹ç›®ç”Ÿæˆçš„è¡ç”Ÿç‰©ç”¨äºç ”ç©¶ç›®çš„ï¼Œä¸å¾—ç”¨äºå•†ä¸šï¼Œä»¥åŠå…¶ä»–ä¼šå¯¹ç¤¾ä¼šå¸¦æ¥å±å®³çš„ç”¨é€”ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ä»…å¯åº”ç”¨äºç ”ç©¶ç›®çš„ï¼Œé¡¹ç›®å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®ï¼ˆåŒ…å«ä½†ä¸é™äºæ•°æ®ã€æ¨¡å‹ã€ä»£ç ç­‰ï¼‰å¯¼è‡´çš„å±å®³æˆ–æŸå¤±ã€‚è¯¦ç»†è¯·å‚è€ƒ&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/DISCLAIMER&#34;&gt;å…è´£å£°æ˜&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;é¡¹ç›®ä»£ç çš„æˆæƒåè®®ä¸º &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/LICENSE&#34;&gt;The Apache License 2.0&lt;/a&gt;ï¼Œä»£ç å¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ï¼Œæ¨¡å‹æƒé‡å’Œæ•°æ®åªèƒ½ç”¨äºç ”ç©¶ç›®çš„ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ MedicalGPTçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ˜‡ Citation&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†MedicalGPTï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{MedicalGPT,&#xA;  title={MedicalGPT: Training Medical GPT Model},&#xA;  author={Ming Xu},&#xA;  year={2023},&#xA;  howpublished={\url{https://github.com/shibing624/MedicalGPT}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ˜ Contribute&lt;/h2&gt; &#xA;&lt;p&gt;é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åœ¨&lt;code&gt;tests&lt;/code&gt;æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•&lt;/li&gt; &#xA; &lt;li&gt;ä½¿ç”¨&lt;code&gt;python -m pytest&lt;/code&gt;æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ä¹‹åå³å¯æäº¤PRã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ’• Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tloen/alpaca-lora/raw/main/finetune.py&#34;&gt;tloen/alpaca-lora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks for their great work!&lt;/p&gt;</summary>
  </entry>
</feed>