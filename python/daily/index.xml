<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-20T01:34:34Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>arkohut/pensieve</title>
    <updated>2024-11-20T01:34:34Z</updated>
    <id>tag:github.com,2024-11-20:/arkohut/pensieve</id>
    <link href="https://github.com/arkohut/pensieve" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A passive recording project allows you to have complete control over your data. ‰∏Ä‰∏™ÂÆåÂÖ®Áî±‰Ω†ÊéåÊéßÊï∞ÊçÆÁöÑ„ÄåË¢´Âä®ËÆ∞ÂΩï„ÄçÈ°πÁõÆ„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/arkohut/pensieve/master/README_ZH.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/arkohut/pensieve/master/docs/images/memos-search-en.gif&#34; alt=&#34;memos-search&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;I changed the name to Pensieve because Memos was already taken.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Pensieve (previously named Memos)&lt;/h1&gt; &#xA;&lt;p&gt;Pensieve is a privacy-focused passive recording project. It can automatically record screen content, build intelligent indices, and provide a convenient web interface to retrieve historical records.&lt;/p&gt; &#xA;&lt;p&gt;This project draws heavily from two other projects: one called &lt;a href=&#34;https://www.rewind.ai/&#34;&gt;Rewind&lt;/a&gt; and another called &lt;a href=&#34;https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c&#34;&gt;Windows Recall&lt;/a&gt;. However, unlike both of them, Pensieve allows you to have complete control over your data, avoiding the transfer of data to untrusted data centers.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöÄ Simple installation: just install dependencies via pip to get started&lt;/li&gt; &#xA; &lt;li&gt;üîí Complete data control: all data is stored locally, allowing for full local operation and self-managed data processing&lt;/li&gt; &#xA; &lt;li&gt;üîç Full-text and vector search support&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ Integrates with Ollama, using it as the machine learning engine for Pensieve&lt;/li&gt; &#xA; &lt;li&gt;üåê Compatible with any OpenAI API models (e.g., OpenAI, Azure OpenAI, vLLM, etc.)&lt;/li&gt; &#xA; &lt;li&gt;üíª Supports Mac and Windows (Linux support is in development)&lt;/li&gt; &#xA; &lt;li&gt;üîå Extensible functionality through plugins&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/arkohut/pensieve/master/docs/images/memos-installation.gif&#34; alt=&#34;memos-installation&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. Install Pensieve&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install memos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Initialize&lt;/h3&gt; &#xA;&lt;p&gt;Initialize the pensieve configuration file and sqlite database:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memos init&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Data will be stored in the &lt;code&gt;~/.memos&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;3. Start the Service&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memos enable&#xA;memos start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Begin recording all screens&lt;/li&gt; &#xA; &lt;li&gt;Start the Web service&lt;/li&gt; &#xA; &lt;li&gt;Set the service to start on boot&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;4. Access the Web Interface&lt;/h3&gt; &#xA;&lt;p&gt;Open your browser and visit &lt;code&gt;http://localhost:8839&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/arkohut/pensieve/master/docs/images/init-page-en.png&#34; alt=&#34;init page&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Mac Permission Issues&lt;/h3&gt; &#xA;&lt;p&gt;On Mac, Pensieve needs screen recording permission. When the program starts, Mac will prompt for screen recording permission - please allow it to proceed.&lt;/p&gt; &#xA;&lt;h2&gt;User Guide&lt;/h2&gt; &#xA;&lt;h3&gt;Using the Appropriate Embedding Model&lt;/h3&gt; &#xA;&lt;h4&gt;1. Model Selection&lt;/h4&gt; &#xA;&lt;p&gt;Pensieve uses embedding models to extract semantic information and build vector indices. Therefore, choosing an appropriate embedding model is crucial. Depending on the user&#39;s primary language, different embedding models should be selected.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For Chinese scenarios, you can use the &lt;a href=&#34;https://huggingface.co/jinaai/jina-embeddings-v2-base-zh&#34;&gt;jinaai/jina-embeddings-v2-base-zh&lt;/a&gt; model.&lt;/li&gt; &#xA; &lt;li&gt;For English scenarios, you can use the &lt;a href=&#34;https://huggingface.co/jinaai/jina-embeddings-v2-base-en&#34;&gt;jinaai/jina-embeddings-v2-base-en&lt;/a&gt; model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Adjust Memos Configuration&lt;/h4&gt; &#xA;&lt;p&gt;Open the &lt;code&gt;~/.memos/config.yaml&lt;/code&gt; file with your preferred text editor and modify the &lt;code&gt;embedding&lt;/code&gt; configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;embedding:&#xA;  use_local: true&#xA;  model: jinaai/jina-embeddings-v2-base-en   # Model name used&#xA;  num_dim: 768                               # Model dimensions&#xA;  use_modelscope: false                      # Whether to use ModelScope&#39;s model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Restart Memos Service&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memos stop&#xA;memos start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first time you use the embedding model, Pensieve will automatically download and load the model.&lt;/p&gt; &#xA;&lt;h4&gt;4. Rebuild Index&lt;/h4&gt; &#xA;&lt;p&gt;If you switch the embedding model during use, meaning you have already indexed screenshots before, you need to rebuild the index:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memos reindex --force&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;--force&lt;/code&gt; parameter indicates rebuilding the index table and deleting previously indexed screenshot data.&lt;/p&gt; &#xA;&lt;h3&gt;Using Ollama for Visual Search&lt;/h3&gt; &#xA;&lt;p&gt;By default, Pensieve only enables the OCR plugin to extract text from screenshots and build indices. However, this method significantly limits search effectiveness for images without text.&lt;/p&gt; &#xA;&lt;p&gt;To achieve more comprehensive visual search capabilities, we need a multimodal image understanding service compatible with the OpenAI API. Ollama perfectly fits this role.&lt;/p&gt; &#xA;&lt;h4&gt;Important Notes Before Use&lt;/h4&gt; &#xA;&lt;p&gt;Before deciding to enable the VLM feature, please note the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hardware Requirements&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Recommended configuration: NVIDIA graphics card with at least 8GB VRAM or Mac with M series chip&lt;/li&gt; &#xA;   &lt;li&gt;The minicpm-v model will occupy about 5.5GB of storage space&lt;/li&gt; &#xA;   &lt;li&gt;CPU mode is not recommended as it will cause severe system lag&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance and Power Consumption Impact&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enabling VLM will significantly increase system power consumption&lt;/li&gt; &#xA;   &lt;li&gt;Consider using other devices to provide OpenAI API compatible model services&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;1. Install Ollama&lt;/h4&gt; &#xA;&lt;p&gt;Visit the &lt;a href=&#34;https://ollama.com&#34;&gt;Ollama official documentation&lt;/a&gt; for detailed installation and configuration instructions.&lt;/p&gt; &#xA;&lt;h4&gt;2. Prepare the Multimodal Model&lt;/h4&gt; &#xA;&lt;p&gt;Download and run the multimodal model &lt;code&gt;minicpm-v&lt;/code&gt; using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ollama run minicpm-v &#34;Describe what this service is&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will download and run the minicpm-v model. If the running speed is too slow, it is not recommended to use this feature.&lt;/p&gt; &#xA;&lt;h4&gt;3. Configure Pensieve to Use Ollama&lt;/h4&gt; &#xA;&lt;p&gt;Open the &lt;code&gt;~/.memos/config.yaml&lt;/code&gt; file with your preferred text editor and modify the &lt;code&gt;vlm&lt;/code&gt; configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;vlm:&#xA;  endpoint: http://localhost:11434  # Ollama service address&#xA;  modelname: minicpm-v              # Model name to use&#xA;  force_jpeg: true                  # Convert images to JPEG format to ensure compatibility&#xA;  prompt: Please describe the content of this image, including the layout and visual elements  # Prompt sent to the model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use the above configuration to overwrite the &lt;code&gt;vlm&lt;/code&gt; configuration in the &lt;code&gt;~/.memos/config.yaml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Also, modify the &lt;code&gt;default_plugins&lt;/code&gt; configuration in the &lt;code&gt;~/.memos/plugins/vlm/config.yaml&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;default_plugins:&#xA;- builtin_ocr&#xA;- builtin_vlm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This adds the &lt;code&gt;builtin_vlm&lt;/code&gt; plugin to the default plugin list.&lt;/p&gt; &#xA;&lt;h4&gt;4. Restart Pensieve Service&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memos stop&#xA;memos start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After restarting the Pensieve service, wait a moment to see the data extracted by VLM in the latest screenshots on the Pensieve web interface:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/arkohut/pensieve/master/docs/images/single-screenshot-view-with-minicpm-result.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you do not see the VLM results, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use the command &lt;code&gt;memos ps&lt;/code&gt; to check if the Pensieve process is running normally&lt;/li&gt; &#xA; &lt;li&gt;Check for error messages in &lt;code&gt;~/.memos/logs/memos.log&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Confirm whether the Ollama model is loaded correctly (&lt;code&gt;ollama ps&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Full Indexing&lt;/h3&gt; &#xA;&lt;p&gt;Pensieve is a compute-intensive application. The indexing process requires the collaboration of OCR, VLM, and embedding models. To minimize the impact on the user&#39;s computer, Pensieve calculates the average processing time for each screenshot and adjusts the indexing frequency accordingly. Therefore, not all screenshots are indexed immediately by default.&lt;/p&gt; &#xA;&lt;p&gt;If you want to index all screenshots, you can use the following command for full indexing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memos scan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will scan and index all recorded screenshots. Note that depending on the number of screenshots and system configuration, this process may take some time and consume significant system resources. The index construction is idempotent, and running this command multiple times will not re-index already indexed data.&lt;/p&gt; &#xA;&lt;h2&gt;Privacy and Security&lt;/h2&gt; &#xA;&lt;p&gt;During the development of Pensieve, I closely followed the progress of similar products, especially &lt;a href=&#34;https://www.rewind.ai/&#34;&gt;Rewind&lt;/a&gt; and &lt;a href=&#34;https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c&#34;&gt;Windows Recall&lt;/a&gt;. I greatly appreciate their product philosophy, but they do not do enough in terms of privacy protection, which is a concern for many users (or potential users). Recording the screen of a personal computer may expose extremely sensitive private data, such as bank accounts, passwords, chat records, etc. Therefore, ensuring that data storage and processing are completely controlled by the user to prevent data leakage is particularly important.&lt;/p&gt; &#xA;&lt;p&gt;The advantages of Pensieve are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The code is completely open-source and easy-to-understand Python code, allowing anyone to review the code to ensure there are no backdoors.&lt;/li&gt; &#xA; &lt;li&gt;Data is completely localized, all data is stored locally, and data processing is entirely controlled by the user. Data will be stored in the user&#39;s &lt;code&gt;~/.memos&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;Easy to uninstall. If you no longer use Pensieve, you can close the program with &lt;code&gt;memos stop &amp;amp;&amp;amp; memos disable&lt;/code&gt;, then uninstall it with &lt;code&gt;pip uninstall memos&lt;/code&gt;, and finally delete the &lt;code&gt;~/.memos&lt;/code&gt; directory to clean up all databases and screenshot data.&lt;/li&gt; &#xA; &lt;li&gt;Data processing is entirely controlled by the user. Pensieve is an independent project, and the machine learning models used (including VLM and embedding models) are chosen by the user. Due to Pensieve&#39; operating mode, using smaller models can also achieve good results.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Of course, there is still room for improvement in terms of privacy, and contributions are welcome to make Pensieve better.&lt;/p&gt; &#xA;&lt;h2&gt;Other Noteworthy Content&lt;/h2&gt; &#xA;&lt;h3&gt;About Storage Space&lt;/h3&gt; &#xA;&lt;p&gt;Pensieve records the screen every 5 seconds and saves the original screenshots in the &lt;code&gt;~/.memos/screenshots&lt;/code&gt; directory. Storage space usage mainly depends on the following factors:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Screenshot Data&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Single screenshot size: about 40-400KB (depending on screen resolution and display complexity)&lt;/li&gt; &#xA;   &lt;li&gt;Daily data volume: about 400MB (based on 10 hours of usage, single screen 2560x1440 resolution)&lt;/li&gt; &#xA;   &lt;li&gt;Multi-screen usage: data volume increases with the number of screens&lt;/li&gt; &#xA;   &lt;li&gt;Monthly estimate: about 8GB based on 20 working days&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Screenshots are deduplicated. If the content of consecutive screenshots does not change much, only one screenshot will be retained. The deduplication mechanism can significantly reduce storage usage in scenarios where content does not change frequently (such as reading, document editing, etc.).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Space&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;SQLite database size depends on the number of indexed screenshots&lt;/li&gt; &#xA;   &lt;li&gt;Reference value: about 2.2GB of storage space after indexing 100,000 screenshots&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;About Power Consumption&lt;/h3&gt; &#xA;&lt;p&gt;Pensieve requires two compute-intensive tasks by default:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;One is the OCR task, used to extract text from screenshots&lt;/li&gt; &#xA; &lt;li&gt;The other is the embedding task, used to extract semantic information and build vector indices&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Resource Usage&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OCR Task&lt;/strong&gt;: Executed using the CPU, and optimized to select the OCR engine based on different operating systems to minimize CPU usage&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Embedding Task&lt;/strong&gt;: Intelligently selects the computing device&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;NVIDIA GPU devices prioritize using the GPU&lt;/li&gt; &#xA;   &lt;li&gt;Mac devices prioritize using Metal GPU&lt;/li&gt; &#xA;   &lt;li&gt;Other devices use the CPU&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Performance Optimization Strategy&lt;/h4&gt; &#xA;&lt;p&gt;To avoid affecting users&#39; daily use, Pensieve has adopted the following optimization measures:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dynamically adjust the indexing frequency, adapting to system processing speed&lt;/li&gt; &#xA; &lt;li&gt;Automatically reduce processing frequency when on battery power to save power&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development Guide&lt;/h2&gt; &#xA;&lt;h3&gt;Peeling the First Layer of the Onion&lt;/h3&gt; &#xA;&lt;p&gt;In fact, after Pensieve starts, it runs three programs:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;memos serve&lt;/code&gt; starts the web service&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;memos record&lt;/code&gt; starts the screenshot recording program&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;memos watch&lt;/code&gt; listens to the image events generated by &lt;code&gt;memos record&lt;/code&gt; and dynamically submits indexing requests to the server based on actual processing speed&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Therefore, if you are a developer or want to see the logs of the entire project running more clearly, you can use these three commands to run each part in the foreground instead of the &lt;code&gt;memos enable &amp;amp;&amp;amp; memos start&lt;/code&gt; command.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DrewThomasson/ebook2audiobook</title>
    <updated>2024-11-20T01:34:34Z</updated>
    <id>tag:github.com,2024-11-20:/DrewThomasson/ebook2audiobook</id>
    <link href="https://github.com/DrewThomasson/ebook2audiobook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generates an audiobook with chapters and ebook metadata using Calibre and Xtts from Coqui tts, and with optional voice cloning, and supports multiple languages&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üìö ebook2audiobook&lt;/h1&gt; &#xA;&lt;p&gt;Convert eBooks to audiobooks with chapters and metadata using Calibre and Coqui XTTS. Supports optional voice cloning and multiple languages!&lt;/p&gt; &#xA;&lt;h4&gt;üñ•Ô∏è Web GUI Interface&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/85af88a7-05dd-4a29-91de-76a14cf5ef06&#34; alt=&#34;demo_web_gui&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;image&#34; src=&#34;https://github.com/user-attachments/assets/b36c71cf-8e06-484c-a252-934e6b1d0c2f&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;image&#34; src=&#34;https://github.com/user-attachments/assets/c0dab57a-d2d4-4658-bff9-3842ec90cb40&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;image&#34; src=&#34;https://github.com/user-attachments/assets/0a99eeac-c521-4b21-8656-e064c1adc528&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;README.md&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;en &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/README.md&#34;&gt;English&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;zh_CN &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/readme/README_CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ru &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/readme/README_RU.md&#34;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåü Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ Converts eBooks to text format with Calibre.&lt;/li&gt; &#xA; &lt;li&gt;üìö Splits eBook into chapters for organized audio.&lt;/li&gt; &#xA; &lt;li&gt;üéôÔ∏è High-quality text-to-speech with Coqui XTTS.&lt;/li&gt; &#xA; &lt;li&gt;üó£Ô∏è Optional voice cloning with your own voice file.&lt;/li&gt; &#xA; &lt;li&gt;üåç Supports multiple languages (English by default).&lt;/li&gt; &#xA; &lt;li&gt;üñ•Ô∏è Designed to run on 4GB RAM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ó &lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/ebook2audiobookXTTS&#34;&gt;Huggingface space demo&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Huggingface space is running on free cpu tier so expect very slow or timeout lol, just don&#39;t give it giant files is all&lt;/li&gt; &#xA; &lt;li&gt;Best to duplicate space or run locally.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Free Google Colab &lt;a href=&#34;https://colab.research.google.com/github/DrewThomasson/ebook2audiobookXTTS/blob/main/Notebooks/colab_ebook2audiobookxtts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Free Google Colab&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;coqui-tts&lt;/code&gt; Python package&lt;/li&gt; &#xA; &lt;li&gt;Calibre (for eBook conversion)&lt;/li&gt; &#xA; &lt;li&gt;FFmpeg (for audiobook creation)&lt;/li&gt; &#xA; &lt;li&gt;Optional: Custom voice file for voice cloning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üîß Installation Instructions&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Python 3.x&lt;/strong&gt; from &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python.org&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Calibre&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;: &lt;code&gt;sudo apt-get install -y calibre&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;brew install calibre&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; (Admin Powershell): &lt;code&gt;choco install calibre&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install FFmpeg&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;: &lt;code&gt;sudo apt-get install -y ffmpeg&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;brew install ffmpeg&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; (Admin Powershell): &lt;code&gt;choco install ffmpeg&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Optional: Install Mecab&lt;/strong&gt; (for non-Latin languages):&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;: &lt;code&gt;sudo apt-get install -y mecab libmecab-dev mecab-ipadic-utf8&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;brew install mecab&lt;/code&gt;, &lt;code&gt;brew install mecab-ipadic&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;a href=&#34;https://taku910.github.io/mecab/#download&#34;&gt;mecab-website-to-install-manually&lt;/a&gt; (Note: Japanese support is limited)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Python packages&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install coqui-tts==0.24.2 pydub nltk beautifulsoup4 ebooklib tqdm gradio==4.44.0&#xA;&#xA;python -m nltk.downloader punkt&#xA;python -m nltk.downloader punkt_tab&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;For non-Latin languages&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install mecab mecab-python3 unidic&#xA;&#xA;python -m unidic download&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üåê Supported Languages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chinese (zh-cn)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Specify the language code when running the script in headless mode.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Usage&lt;/h2&gt; &#xA;&lt;h3&gt;üñ•Ô∏è Launching Gradio Web Interface&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the Script&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: Add &lt;code&gt;--share True&lt;/code&gt; to the end of it like this: &lt;code&gt;python app.py --share True&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;-h&lt;/code&gt; parameter like this &lt;code&gt;python app.py -h&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üìù Basic Headless Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py --headless True --ebook &amp;lt;path_to_ebook_file&amp;gt; --voice [path_to_voice_file] --language [language_code]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt;path_to_ebook_file&amp;gt;&lt;/strong&gt;: Path to your eBook file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[path_to_voice_file]&lt;/strong&gt;: Optional for voice cloning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[language_code]&lt;/strong&gt;: Optional to specify language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;-h&lt;/code&gt; parameter like this &lt;code&gt;python app.py -h&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üß© Headless Custom XTTS Model Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py --headless True --use_custom_model True --ebook &amp;lt;ebook_file_path&amp;gt; --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt; --custom_config &amp;lt;custom_config_path&amp;gt; --custom_vocab &amp;lt;custom_vocab_path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt;ebook_file_path&amp;gt;&lt;/strong&gt;: Path to your eBook file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt;target_voice_file_path&amp;gt;&lt;/strong&gt;: Optional for voice cloning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#xA;   &lt;language&gt;&lt;/language&gt;&lt;/strong&gt;: Optional to specify language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model.pth&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt;custom_config_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;config.json&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt;custom_vocab_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;vocab.json&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;-h&lt;/code&gt; parameter like this &lt;code&gt;python app.py -h&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üß© Headless Custom XTTS Model Usage With Zip link to XTTS Fine-Tune Model üåê&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py --headless True --use_custom_model True --ebook &amp;lt;ebook_file_path&amp;gt; --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model_url &amp;lt;custom_model_URL_ZIP_path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt;ebook_file_path&amp;gt;&lt;/strong&gt;: Path to your eBook file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt;target_voice_file_path&amp;gt;&lt;/strong&gt;: Optional for voice cloning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#xA;   &lt;language&gt;&lt;/language&gt;&lt;/strong&gt;: Optional to specify language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_URL_ZIP_path&amp;gt;&lt;/strong&gt;: URL Path to zip of Model folder. For Example this for the &lt;a href=&#34;https://huggingface.co/drewThomasson/xtts_David_Attenborough_fine_tune/tree/main&#34;&gt;xtts_David_Attenborough_fine_tune&lt;/a&gt; &lt;code&gt;https://huggingface.co/drewThomasson/xtts_David_Attenborough_fine_tune/resolve/main/Finished_model_files.zip?download=true&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;For a custom model a ref audio clip of the voice will also be needed: &lt;a href=&#34;https://huggingface.co/drewThomasson/xtts_David_Attenborough_fine_tune/blob/main/ref.wav&#34;&gt;ref audio clip of David Attenborough&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;-h&lt;/code&gt; parameter like this &lt;code&gt;python app.py -h&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üîç For Detailed Guide with list of all Parameters to use&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This will output the following:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;usage: app.py [-h] [--share SHARE] [--headless HEADLESS] [--ebook EBOOK] [--voice VOICE]&#xA;              [--language LANGUAGE] [--use_custom_model USE_CUSTOM_MODEL]&#xA;              [--custom_model CUSTOM_MODEL] [--custom_config CUSTOM_CONFIG]&#xA;              [--custom_vocab CUSTOM_VOCAB] [--custom_model_url CUSTOM_MODEL_URL]&#xA;              [--temperature TEMPERATURE] [--length_penalty LENGTH_PENALTY]&#xA;              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]&#xA;              [--speed SPEED] [--enable_text_splitting ENABLE_TEXT_SPLITTING]&#xA;&#xA;Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the&#xA;Gradio interface or run the script in headless mode for direct conversion.&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --share SHARE         Set to True to enable a public shareable Gradio link. Defaults&#xA;                        to False.&#xA;  --headless HEADLESS   Set to True to run in headless mode without the Gradio&#xA;                        interface. Defaults to False.&#xA;  --ebook EBOOK         Path to the ebook file for conversion. Required in headless&#xA;                        mode.&#xA;  --voice VOICE         Path to the target voice file for TTS. Optional, uses a default&#xA;                        voice if not provided.&#xA;  --language LANGUAGE   Language for the audiobook conversion. Options: en, es, fr, de,&#xA;                        it, pt, pl, tr, ru, nl, cs, ar, zh-cn, ja, hu, ko. Defaults to&#xA;                        English (en).&#xA;  --use_custom_model USE_CUSTOM_MODEL&#xA;                        Set to True to use a custom TTS model. Defaults to False. Must&#xA;                        be True to use custom models, otherwise you&#39;ll get an error.&#xA;  --custom_model CUSTOM_MODEL&#xA;                        Path to the custom model file (.pth). Required if using a custom&#xA;                        model.&#xA;  --custom_config CUSTOM_CONFIG&#xA;                        Path to the custom config file (config.json). Required if using&#xA;                        a custom model.&#xA;  --custom_vocab CUSTOM_VOCAB&#xA;                        Path to the custom vocab file (vocab.json). Required if using a&#xA;                        custom model.&#xA;  --custom_model_url CUSTOM_MODEL_URL&#xA;                        URL to download the custom model as a zip file. Optional, but&#xA;                        will be used if provided. Examples include David Attenborough&#39;s&#xA;                        model: &#39;https://huggingface.co/drewThomasson/xtts_David_Attenbor&#xA;                        ough_fine_tune/resolve/main/Finished_model_files.zip?download=tr&#xA;                        ue&#39;. More XTTS fine-tunes can be found on my Hugging Face at&#xA;                        &#39;https://huggingface.co/drewThomasson&#39;.&#xA;  --temperature TEMPERATURE&#xA;                        Temperature for the model. Defaults to 0.65. Higher Tempatures&#xA;                        will lead to more creative outputs IE: more Hallucinations.&#xA;                        Lower Tempatures will be more monotone outputs IE: less&#xA;                        Hallucinations.&#xA;  --length_penalty LENGTH_PENALTY&#xA;                        A length penalty applied to the autoregressive decoder. Defaults&#xA;                        to 1.0. Not applied to custom models.&#xA;  --repetition_penalty REPETITION_PENALTY&#xA;                        A penalty that prevents the autoregressive decoder from&#xA;                        repeating itself. Defaults to 2.0.&#xA;  --top_k TOP_K         Top-k sampling. Lower values mean more likely outputs and&#xA;                        increased audio generation speed. Defaults to 50.&#xA;  --top_p TOP_P         Top-p sampling. Lower values mean more likely outputs and&#xA;                        increased audio generation speed. Defaults to 0.8.&#xA;  --speed SPEED         Speed factor for the speech generation. IE: How fast the&#xA;                        Narrerator will speak. Defaults to 1.0.&#xA;  --enable_text_splitting ENABLE_TEXT_SPLITTING&#xA;                        Enable splitting text into sentences. Defaults to True.&#xA;&#xA;Example: python script.py --headless --ebook path_to_ebook --voice path_to_voice&#xA;--language en --use_custom_model True --custom_model model.pth --custom_config&#xA;config.json --custom_vocab vocab.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚ö†Ô∏è Legacy-Depricated Old Use Instructions&lt;/summary&gt; &#xA; &lt;h2&gt;üöÄ Usage&lt;/h2&gt; &#xA; &lt;h2&gt;Legacy files have been moved to &lt;code&gt;ebook2audiobookXTTS/legacy/&lt;/code&gt;&lt;/h2&gt; &#xA; &lt;h3&gt;üñ•Ô∏è Gradio Web Interface&lt;/h3&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the Script&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python custom_model_ebook2audiobookXTTS_gradio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h3&gt;üìù Basic Usage&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ebook2audiobook.py &amp;lt;path_to_ebook_file&amp;gt; [path_to_voice_file] [language_code]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&amp;lt;path_to_ebook_file&amp;gt;&lt;/strong&gt;: Path to your eBook file.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[path_to_voice_file]&lt;/strong&gt;: Optional for voice cloning.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[language_code]&lt;/strong&gt;: Optional to specify language.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;üß© Custom XTTS Model&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python custom_model_ebook2audiobookXTTS.py &amp;lt;ebook_file_path&amp;gt; &amp;lt;target_voice_file_path&amp;gt; &amp;lt;language&amp;gt; &amp;lt;custom_model_path&amp;gt; &amp;lt;custom_config_path&amp;gt; &amp;lt;custom_vocab_path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&amp;lt;ebook_file_path&amp;gt;&lt;/strong&gt;: Path to your eBook file.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&amp;lt;target_voice_file_path&amp;gt;&lt;/strong&gt;: Optional for voice cloning.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&#xA;    &lt;language&gt;&lt;/language&gt;&lt;/strong&gt;: Optional to specify language.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model.pth&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&amp;lt;custom_config_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;config.json&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&amp;lt;custom_vocab_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;vocab.json&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;üê≥ Using Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can also use Docker to run the eBook to Audiobook converter. This method ensures consistency across different environments and simplifies setup.&lt;/p&gt; &#xA;&lt;h4&gt;üöÄ Running the Docker Container&lt;/h4&gt; &#xA;&lt;p&gt;To run the Docker container and start the Gradio interface, use the following command:&lt;/p&gt; &#xA;&lt;p&gt;-Run with CPU only&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;docker run -it --rm -p 7860:7860 --platform=linux/amd64 athomasson2/ebook2audiobookxtts:huggingface python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;-Run with GPU Speedup (Nvida graphics cards only)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;docker run -it --rm --gpus all -p 7860:7860 --platform=linux/amd64 athomasson2/ebook2audiobookxtts:huggingface python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For more options like running the docker in headless mode or making the gradio link public add the &lt;code&gt;-h&lt;/code&gt; parameter after the &lt;code&gt;app.py&lt;/code&gt; in the docker launch command&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Example of using docker in headless mode or modifying anything with the extra parameters + Full guide&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;h2&gt;Example of using docker in headless mode&lt;/h2&gt; &#xA; &lt;p&gt;first for a docker pull of the latest with&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull athomasson2/ebook2audiobookxtts:huggingface&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Before you do run this you need to create a dir named &#34;input-folder&#34; in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm \&#xA;    -v $(pwd)/input-folder:/home/user/app/input_folder \&#xA;    -v $(pwd)/Audiobooks:/home/user/app/Audiobooks \&#xA;    --platform linux/amd64 \&#xA;    athomasson2/ebook2audiobookxtts:huggingface \&#xA;    python app.py --headless True --ebook /home/user/app/input_folder/YOUR_INPUT_FILE.TXT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;And that should be it!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm \&#xA;    --platform linux/amd64 \&#xA;    athomasson2/ebook2audiobookxtts:huggingface \&#xA;    python app.py -h&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;and that will output this&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;user/app/ebook2audiobookXTTS/input-folder -v $(pwd)/Audiobooks:/home/user/app/ebook2audiobookXTTS/Audiobooks --memory=&#34;4g&#34; --network none --platform linux/amd64 athomasson2/ebook2audiobookxtts:huggingface python app.py -h&#xA;starting...&#xA;usage: app.py [-h] [--share SHARE] [--headless HEADLESS] [--ebook EBOOK] [--voice VOICE]&#xA;              [--language LANGUAGE] [--use_custom_model USE_CUSTOM_MODEL]&#xA;              [--custom_model CUSTOM_MODEL] [--custom_config CUSTOM_CONFIG]&#xA;              [--custom_vocab CUSTOM_VOCAB] [--custom_model_url CUSTOM_MODEL_URL]&#xA;              [--temperature TEMPERATURE] [--length_penalty LENGTH_PENALTY]&#xA;              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]&#xA;              [--speed SPEED] [--enable_text_splitting ENABLE_TEXT_SPLITTING]&#xA;&#xA;Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the&#xA;Gradio interface or run the script in headless mode for direct conversion.&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --share SHARE         Set to True to enable a public shareable Gradio link. Defaults&#xA;                        to False.&#xA;  --headless HEADLESS   Set to True to run in headless mode without the Gradio&#xA;                        interface. Defaults to False.&#xA;  --ebook EBOOK         Path to the ebook file for conversion. Required in headless&#xA;                        mode.&#xA;  --voice VOICE         Path to the target voice file for TTS. Optional, uses a default&#xA;                        voice if not provided.&#xA;  --language LANGUAGE   Language for the audiobook conversion. Options: en, es, fr, de,&#xA;                        it, pt, pl, tr, ru, nl, cs, ar, zh-cn, ja, hu, ko. Defaults to&#xA;                        English (en).&#xA;  --use_custom_model USE_CUSTOM_MODEL&#xA;                        Set to True to use a custom TTS model. Defaults to False. Must&#xA;                        be True to use custom models, otherwise you&#39;ll get an error.&#xA;  --custom_model CUSTOM_MODEL&#xA;                        Path to the custom model file (.pth). Required if using a custom&#xA;                        model.&#xA;  --custom_config CUSTOM_CONFIG&#xA;                        Path to the custom config file (config.json). Required if using&#xA;                        a custom model.&#xA;  --custom_vocab CUSTOM_VOCAB&#xA;                        Path to the custom vocab file (vocab.json). Required if using a&#xA;                        custom model.&#xA;  --custom_model_url CUSTOM_MODEL_URL&#xA;                        URL to download the custom model as a zip file. Optional, but&#xA;                        will be used if provided. Examples include David Attenborough&#39;s&#xA;                        model: &#39;https://huggingface.co/drewThomasson/xtts_David_Attenbor&#xA;                        ough_fine_tune/resolve/main/Finished_model_files.zip?download=tr&#xA;                        ue&#39;. More XTTS fine-tunes can be found on my Hugging Face at&#xA;                        &#39;https://huggingface.co/drewThomasson&#39;.&#xA;  --temperature TEMPERATURE&#xA;                        Temperature for the model. Defaults to 0.65. Higher Tempatures&#xA;                        will lead to more creative outputs IE: more Hallucinations.&#xA;                        Lower Tempatures will be more monotone outputs IE: less&#xA;                        Hallucinations.&#xA;  --length_penalty LENGTH_PENALTY&#xA;                        A length penalty applied to the autoregressive decoder. Defaults&#xA;                        to 1.0. Not applied to custom models.&#xA;  --repetition_penalty REPETITION_PENALTY&#xA;                        A penalty that prevents the autoregressive decoder from&#xA;                        repeating itself. Defaults to 2.0.&#xA;  --top_k TOP_K         Top-k sampling. Lower values mean more likely outputs and&#xA;                        increased audio generation speed. Defaults to 50.&#xA;  --top_p TOP_P         Top-p sampling. Lower values mean more likely outputs and&#xA;                        increased audio generation speed. Defaults to 0.8.&#xA;  --speed SPEED         Speed factor for the speech generation. IE: How fast the&#xA;                        Narrerator will speak. Defaults to 1.0.&#xA;  --enable_text_splitting ENABLE_TEXT_SPLITTING&#xA;                        Enable splitting text into sentences. Defaults to True.&#xA;&#xA;Example: python script.py --headless --ebook path_to_ebook --voice path_to_voice&#xA;--language en --use_custom_model True --custom_model model.pth --custom_config&#xA;config.json --custom_vocab vocab.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;üñ•Ô∏è Docker GUI&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/85af88a7-05dd-4a29-91de-76a14cf5ef06&#34; alt=&#34;demo_web_gui&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;image&#34; src=&#34;https://github.com/user-attachments/assets/b36c71cf-8e06-484c-a252-934e6b1d0c2f&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;image&#34; src=&#34;https://github.com/user-attachments/assets/c0dab57a-d2d4-4658-bff9-3842ec90cb40&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;image&#34; src=&#34;https://github.com/user-attachments/assets/0a99eeac-c521-4b21-8656-e064c1adc528&#34;&gt; &#xA;&lt;/details&gt; ### üõ†Ô∏è For Custom Xtts Models &#xA;&lt;p&gt;Models built to be better at a specific voice. Check out my Hugging Face page &lt;a href=&#34;https://huggingface.co/drewThomasson&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use a custom model, paste the link of the &lt;code&gt;Finished_model_files.zip&lt;/code&gt; file like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/drewThomasson/xtts_David_Attenborough_fine_tune/resolve/main/Finished_model_files.zip?download=true&#34;&gt;David Attenborough fine tuned Finished_model_files.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For a custom model a ref audio clip of the voice will also be needed: &lt;a href=&#34;https://huggingface.co/drewThomasson/xtts_David_Attenborough_fine_tune/blob/main/ref.wav&#34;&gt;ref audio clip of David Attenborough&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;More details can be found at the &lt;a href=&#34;%5Bhttps://github.com/DrewThomasson/ebook2audiobookXTTS%5D(https://hub.docker.com/repository/docker/athomasson2/ebook2audiobookxtts/general)&#34;&gt;Dockerfile Hub Page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üåê Fine Tuned Xtts models&lt;/h2&gt; &#xA;&lt;p&gt;To find already fine-tuned XTTS models, visit &lt;a href=&#34;https://huggingface.co/drewThomasson&#34;&gt;this Hugging Face link&lt;/a&gt; üåê. Search for models that include &#34;xtts fine tune&#34; in their names.&lt;/p&gt; &#xA;&lt;h2&gt;üé• Demos&lt;/h2&gt; &#xA;&lt;p&gt;Rainy day voice&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/8486603c-38b1-43ce-9639-73757dfb1031&#34;&gt;https://github.com/user-attachments/assets/8486603c-38b1-43ce-9639-73757dfb1031&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;David Attenborough voice&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/47c846a7-9e51-4eb9-844a-7460402a20a8&#34;&gt;https://github.com/user-attachments/assets/47c846a7-9e51-4eb9-844a-7460402a20a8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ó &lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/ebook2audiobookXTTS&#34;&gt;Huggingface space demo&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Huggingface space is running on free cpu tier so expect very slow or timeout lol, just don&#39;t give it giant files is all&lt;/li&gt; &#xA; &lt;li&gt;Best to duplicate space or run locally.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Free Google Colab &lt;a href=&#34;https://colab.research.google.com/github/DrewThomasson/ebook2audiobookXTTS/blob/main/Notebooks/colab_ebook2audiobookxtts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Free Google Colab&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;üìö Supported eBook Formats&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìÇ Output&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creates an &lt;code&gt;.m4b&lt;/code&gt; file with metadata and chapters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Example Output&lt;/strong&gt;: &lt;img src=&#34;https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg&#34; alt=&#34;Example&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Common Issues:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;It&#39;s slow!&#34; - On CPU only this is very slow, and you can only get speedups though a NVIDIA GPU. &lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobookXTTS/discussions/19#discussioncomment-10879846&#34;&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobookpiper-tts&#34;&gt;project that uses piper-tts&lt;/a&gt; instead(It doesn&#39;t have zero-shot voice cloning though, and is siri quality voices, but it is much faster on cpu.)&lt;/li&gt; &#xA; &lt;li&gt;&#34;I&#39;m having dependency issues&#34; - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;-h&lt;/code&gt; parameter after the &lt;code&gt;app.py&lt;/code&gt; in the docker run command for more information.&lt;/li&gt; &#xA; &lt;li&gt;&#34;Im getting a truncated audio issue!&#34; - PLEASE MAKE AN ISSUE OF THIS, I don&#39;t speak every language and I need advise from each person to fine tune my sentense splitting function on any other languages.üòä&lt;/li&gt; &#xA; &lt;li&gt;&#34;The loading bar is stuck at 30% in the web gui!&#34; - The web gui loading bar is extreamly basic as its just split between the three loading steps, refer to the terminal and what sentense it&#39;s on for a more accurate gauge on where is it progress wise.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What I need help with! üôå&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobookXTTS/issues/32&#34;&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Any help from people speaking any of the supported langues to help with proper sentence splitting methods&lt;/li&gt; &#xA; &lt;li&gt;Potentially creating readme Guides for Multiple languages(Becuase the only language I know is English üòî)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üôè Special Thanks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href=&#34;https://github.com/coqui-ai/TTS&#34;&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href=&#34;https://calibre-ebook.com&#34;&gt;Calibre Website&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobookXTTS/issues/8&#34;&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>PacktPublishing/LLM-Engineers-Handbook</title>
    <updated>2024-11-20T01:34:34Z</updated>
    <id>tag:github.com,2024-11-20:/PacktPublishing/LLM-Engineers-Handbook</id>
    <link href="https://github.com/PacktPublishing/LLM-Engineers-Handbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The LLM&#39;s practical guide: From the fundamentals to deploying advanced LLM and RAG apps to AWS using LLMOps best practices&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;üë∑ LLM Engineer&#39;s Handbook&lt;/h1&gt; &#xA; &lt;p class=&#34;tagline&#34;&gt;Official repository of the &lt;a href=&#34;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&#34;&gt;LLM Engineer&#39;s Handbook&lt;/a&gt; by &lt;a href=&#34;https://github.com/iusztinpaul&#34;&gt;Paul Iusztin&lt;/a&gt; and &lt;a href=&#34;https://github.com/mlabonne&#34;&gt;Maxime Labonne&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PacktPublishing/LLM-Engineers-Handbook/main/images/cover_plus.png&#34; alt=&#34;Book cover&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üåü Features&lt;/h2&gt; &#xA;&lt;p&gt;The goal of this book is to create your own end-to-end LLM-based system using best practices:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìù Data collection &amp;amp; generation&lt;/li&gt; &#xA; &lt;li&gt;üîÑ LLM training pipeline&lt;/li&gt; &#xA; &lt;li&gt;üìä Simple RAG system&lt;/li&gt; &#xA; &lt;li&gt;üöÄ Production-ready AWS deployment&lt;/li&gt; &#xA; &lt;li&gt;üîç Comprehensive monitoring&lt;/li&gt; &#xA; &lt;li&gt;üß™ Testing and evaluation framework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can download and use the final trained model on &lt;a href=&#34;https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üîó Dependencies&lt;/h2&gt; &#xA;&lt;h3&gt;Local dependencies&lt;/h3&gt; &#xA;&lt;p&gt;To install and run the project locally, you need the following dependencies.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Tool&lt;/th&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;   &lt;th&gt;Installation Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pyenv&lt;/td&gt; &#xA;   &lt;td&gt;‚â•2.3.36&lt;/td&gt; &#xA;   &lt;td&gt;Multiple Python versions (optional)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/pyenv/pyenv?tab=readme-ov-file#installation&#34;&gt;Install Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;3.11&lt;/td&gt; &#xA;   &lt;td&gt;Runtime environment&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Poetry&lt;/td&gt; &#xA;   &lt;td&gt;‚â•1.8.3&lt;/td&gt; &#xA;   &lt;td&gt;Package management&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://python-poetry.org/docs/#installation&#34;&gt;Install Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docker&lt;/td&gt; &#xA;   &lt;td&gt;‚â•27.1.1&lt;/td&gt; &#xA;   &lt;td&gt;Containerization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Install Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AWS CLI&lt;/td&gt; &#xA;   &lt;td&gt;‚â•2.15.42&lt;/td&gt; &#xA;   &lt;td&gt;Cloud management&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html&#34;&gt;Install Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Git&lt;/td&gt; &#xA;   &lt;td&gt;‚â•2.44.0&lt;/td&gt; &#xA;   &lt;td&gt;Version control&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Cloud services&lt;/h3&gt; &#xA;&lt;p&gt;The code also uses and depends on the following cloud services. For now, you don&#39;t have to do anything. We will guide you in the installation and deployment sections on how to use them:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Service&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.com/&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Model registry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/site/&#34;&gt;Comet ML&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Experiment tracker&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/site/products/opik/&#34;&gt;Opik&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Prompt monitoring&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.zenml.io/&#34;&gt;ZenML&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Orchestrator and artifacts layer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/&#34;&gt;AWS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Compute and storage&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.mongodb.com/&#34;&gt;MongoDB&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NoSQL database&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://qdrant.tech/&#34;&gt;Qdrant&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vector database&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/features/actions&#34;&gt;GitHub Actions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CI/CD pipeline&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;In the &lt;a href=&#34;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&#34;&gt;LLM Engineer&#39;s Handbook&lt;/a&gt;, Chapter 2 will walk you through each tool. Chapters 10 and 11 provide step-by-step guides on how to set up everything you need.&lt;/p&gt; &#xA;&lt;h2&gt;üóÇÔ∏è Project Structure&lt;/h2&gt; &#xA;&lt;p&gt;Here is the directory overview:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.&#xA;‚îú‚îÄ‚îÄ code_snippets/       # Standalone example code&#xA;‚îú‚îÄ‚îÄ configs/             # Pipeline configuration files&#xA;‚îú‚îÄ‚îÄ llm_engineering/     # Core project package&#xA;‚îÇ   ‚îú‚îÄ‚îÄ application/    &#xA;‚îÇ   ‚îú‚îÄ‚îÄ domain/         &#xA;‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/ &#xA;‚îÇ   ‚îú‚îÄ‚îÄ model/         &#xA;‚îú‚îÄ‚îÄ pipelines/           # ML pipeline definitions&#xA;‚îú‚îÄ‚îÄ steps/               # Pipeline components&#xA;‚îú‚îÄ‚îÄ tests/               # Test examples&#xA;‚îú‚îÄ‚îÄ tools/               # Utility scripts&#xA;‚îÇ   ‚îú‚îÄ‚îÄ run.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ml_service.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ rag.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ data_warehouse.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;llm_engineering/&lt;/code&gt; is the main Python package implementing LLM and RAG functionality. It follows Domain-Driven Design (DDD) principles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;domain/&lt;/code&gt;: Core business entities and structures&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;application/&lt;/code&gt;: Business logic, crawlers, and RAG implementation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model/&lt;/code&gt;: LLM training and inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;infrastructure/&lt;/code&gt;: External service integrations (AWS, Qdrant, MongoDB, FastAPI)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The code logic and imports flow as follows: &lt;code&gt;infrastructure&lt;/code&gt; ‚Üí &lt;code&gt;model&lt;/code&gt; ‚Üí &lt;code&gt;application&lt;/code&gt; ‚Üí &lt;code&gt;domain&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pipelines/&lt;/code&gt;: Contains the ZenML ML pipelines, which serve as the entry point for all the ML pipelines. Coordinates the data processing and model training stages of the ML lifecycle.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;steps/&lt;/code&gt;: Contains individual ZenML steps, which are reusable components for building and customizing ZenML pipelines. Steps perform specific tasks (e.g., data loading, preprocessing) and can be combined within the ML pipelines.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;tests/&lt;/code&gt;: Covers a few sample tests used as examples within the CI pipeline.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;tools/&lt;/code&gt;: Utility scripts used to call the ZenML pipelines and inference code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;run.py&lt;/code&gt;: Entry point script to run ZenML pipelines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ml_service.py&lt;/code&gt;: Starts the REST API inference server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rag.py&lt;/code&gt;: Demonstrates usage of the RAG retrieval module.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;data_warehouse.py&lt;/code&gt;: Used to export or import data from the MongoDB data warehouse through JSON files.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;configs/&lt;/code&gt;: ZenML YAML configuration files to control the execution of pipelines and steps.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;code_snippets/&lt;/code&gt;: Independent code examples that can be executed independently.&lt;/p&gt; &#xA;&lt;h2&gt;üíª Installation&lt;/h2&gt; &#xA;&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; &#xA;&lt;p&gt;Start by cloning the repository and navigating to the project directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git&#xA;cd LLM-Engineers-Handbook &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, we have to prepare your Python environment and its adjacent dependencies.&lt;/p&gt; &#xA;&lt;h3&gt;2. Set Up Python Environment&lt;/h3&gt; &#xA;&lt;p&gt;The project requires Python 3.11. You can either use your global Python installation or set up a project-specific version using pyenv.&lt;/p&gt; &#xA;&lt;h4&gt;Option A: Using Global Python (if version 3.11 is installed)&lt;/h4&gt; &#xA;&lt;p&gt;Verify your Python version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python --version  # Should show Python 3.11.x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Option B: Using pyenv (recommended)&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Verify pyenv installation:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pyenv --version   # Should show pyenv 2.3.36 or later&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Python 3.11.8:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pyenv install 3.11.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Verify the installation:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python --version  # Should show Python 3.11.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Confirm Python version in the project directory:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python --version&#xA;# Output: Python 3.11.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; The project includes a &lt;code&gt;.python-version&lt;/code&gt; file that automatically sets the correct Python version when you&#39;re in the project directory.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;3. Install Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;The project uses Poetry for dependency management.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Verify Poetry installation:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry --version  # Should show Poetry version 1.8.3 or later&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Set up the project environment and install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry env use 3.11&#xA;poetry install --without aws&#xA;poetry run pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Configure Poetry to use Python 3.11&lt;/li&gt; &#xA; &lt;li&gt;Install project dependencies (excluding AWS-specific packages)&lt;/li&gt; &#xA; &lt;li&gt;Set up pre-commit hooks for code verification&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;4. Activate the Environment&lt;/h3&gt; &#xA;&lt;p&gt;As our task manager, we run all the scripts using &lt;a href=&#34;https://poethepoet.natn.io/index.html&#34;&gt;Poe the Poet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start a Poetry shell:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run project commands using Poe the Poet:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üîß Troubleshooting Poe the Poet Installation&lt;/summary&gt; &#xA; &lt;h3&gt;Alternative Command Execution&lt;/h3&gt; &#xA; &lt;p&gt;If you&#39;re experiencing issues with &lt;code&gt;poethepoet&lt;/code&gt;, you can still run the project commands directly through Poetry. Here&#39;s how:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Look up the command definition in &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Use &lt;code&gt;poetry run&lt;/code&gt; with the underlying command&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h4&gt;Example:&lt;/h4&gt; &#xA; &lt;p&gt;Instead of:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe local-infrastructure-up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Use the direct command from pyproject.toml:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run &amp;lt;actual-command-from-pyproject-toml&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Note: All project commands are defined in the [tool.poe.tasks] section of pyproject.toml&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Now, let&#39;s configure our local project with all the necessary credentials and tokens to run the code locally.&lt;/p&gt; &#xA;&lt;h3&gt;5. Local Development Setup&lt;/h3&gt; &#xA;&lt;p&gt;After you have installed all the dependencies, you must create and fill a&amp;nbsp;&lt;code&gt;.env&lt;/code&gt; file with your credentials to appropriately interact with other services and run the project. Setting your sensitive credentials in a &lt;code&gt;.env&lt;/code&gt; file is a good security practice, as this file won&#39;t be committed to GitHub or shared with anyone else.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First, copy our example by running the following:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env # The file must be at your repository&#39;s root!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Now, let&#39;s understand how to fill in all the essential variables within the &lt;code&gt;.env&lt;/code&gt; file to get you started. The following are the mandatory settings we must complete when working locally:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;OpenAI&lt;/h4&gt; &#xA;&lt;p&gt;To authenticate to OpenAI&#39;s API, you must fill out the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; env var with an authentication token.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;OPENAI_API_KEY=your_api_key_here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‚Üí Check out this &lt;a href=&#34;https://platform.openai.com/docs/quickstart&#34;&gt;tutorial&lt;/a&gt; to learn how to provide one from OpenAI.&lt;/p&gt; &#xA;&lt;h4&gt;Hugging Face&lt;/h4&gt; &#xA;&lt;p&gt;To authenticate to Hugging Face, you must fill out the &lt;code&gt;HUGGINGFACE_ACCESS_TOKEN&lt;/code&gt; env var with an authentication token.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;HUGGINGFACE_ACCESS_TOKEN=your_token_here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‚Üí Check out this &lt;a href=&#34;https://huggingface.co/docs/hub/en/security-tokens&#34;&gt;tutorial&lt;/a&gt; to learn how to provide one from Hugging Face.&lt;/p&gt; &#xA;&lt;h4&gt;Comet ML &amp;amp; Opik&lt;/h4&gt; &#xA;&lt;p&gt;To authenticate to Comet ML (required only during training) and Opik, you must fill out the &lt;code&gt;COMET_API_KEY&lt;/code&gt; env var with your authentication token.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;COMET_API_KEY=your_api_key_here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‚Üí Check out this &lt;a href=&#34;https://www.comet.com/docs/v2/api-and-sdk/rest-api/overview/&#34;&gt;tutorial&lt;/a&gt; to learn how to get the Comet ML variables from above. You can also access Opik&#39;s dashboard using üîó&lt;a href=&#34;https://www.comet.com/opik&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;6. Deployment Setup&lt;/h3&gt; &#xA;&lt;p&gt;When deploying the project to the cloud, we must set additional settings for Mongo, Qdrant, and AWS. If you are just working locally, the default values of these env vars will work out of the box. Detailed deployment instructions are available in Chapter 11 of the &lt;a href=&#34;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&#34;&gt;LLM Engineer&#39;s Handbook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;MongoDB&lt;/h4&gt; &#xA;&lt;p&gt;We must change the &lt;code&gt;DATABASE_HOST&lt;/code&gt; env var with the URL pointing to your cloud MongoDB cluster.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;DATABASE_HOST=your_mongodb_url&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‚Üí Check out this &lt;a href=&#34;https://www.mongodb.com/resources/products/fundamentals/mongodb-cluster-setup&#34;&gt;tutorial&lt;/a&gt; to learn how to create and host a MongoDB cluster for free.&lt;/p&gt; &#xA;&lt;h4&gt;Qdrant&lt;/h4&gt; &#xA;&lt;p&gt;Change &lt;code&gt;USE_QDRANT_CLOUD&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;, &lt;code&gt;QDRANT_CLOUD_URL&lt;/code&gt; with the URL point to your cloud Qdrant cluster, and &lt;code&gt;QDRANT_APIKEY&lt;/code&gt; with its API key.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;USE_QDRANT_CLOUD=true&#xA;QDRANT_CLOUD_URL=your_qdrant_cloud_url&#xA;QDRANT_APIKEY=your_qdrant_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‚Üí Check out this &lt;a href=&#34;https://qdrant.tech/documentation/cloud/create-cluster/&#34;&gt;tutorial&lt;/a&gt; to learn how to create a Qdrant cluster for free&lt;/p&gt; &#xA;&lt;h4&gt;AWS&lt;/h4&gt; &#xA;&lt;p&gt;For your AWS set-up to work correctly, you need the AWS CLI installed on your local machine and properly configured with an admin user (or a user with enough permissions to create new SageMaker, ECR, and S3 resources; using an admin user will make everything more straightforward).&lt;/p&gt; &#xA;&lt;p&gt;Chapter 2 provides step-by-step instructions on how to install the AWS CLI, create an admin user on AWS, and get an access key to set up the &lt;code&gt;AWS_ACCESS_KEY&lt;/code&gt; and &lt;code&gt;AWS_SECRET_KEY&lt;/code&gt; environment variables. If you already have an AWS admin user in place, you have to configure the following env vars in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;AWS_REGION=eu-central-1 # Change it with your AWS region.&#xA;AWS_ACCESS_KEY=your_aws_access_key&#xA;AWS_SECRET_KEY=your_aws_secret_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;AWS credentials are typically stored in &lt;code&gt;~/.aws/credentials&lt;/code&gt;. You can view this file directly using &lt;code&gt;cat&lt;/code&gt; or similar commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat ~/.aws/credentials&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Additional configuration options are available in &lt;a href=&#34;https://github.com/PacktPublishing/LLM-Engineers-Handbook/raw/main/llm_engineering/settings.py&#34;&gt;settings.py&lt;/a&gt;. Any variable in the &lt;code&gt;Settings&lt;/code&gt; class can be configured through the &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üèóÔ∏è Infrastructure&lt;/h2&gt; &#xA;&lt;h3&gt;Local infrastructure (for testing and development)&lt;/h3&gt; &#xA;&lt;p&gt;When running the project locally, we host a MongoDB and Qdrant database using Docker. Also, a testing ZenML server is made available through their Python package.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] You need Docker installed (&amp;gt;= v27.1.1)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For ease of use, you can start the whole local development infrastructure with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe local-infrastructure-up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, you can stop the ZenML server and all the Docker containers using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe local-infrastructure-down&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING]&lt;br&gt; When running on MacOS, before starting the server, export the following environment variable: &lt;code&gt;export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES&lt;/code&gt; Otherwise, the connection between the local server and pipeline will break. üîó More details in &lt;a href=&#34;https://github.com/zenml-io/zenml/issues/2369&#34;&gt;this issue&lt;/a&gt;. This is done by default when using Poe the Poet.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Start the inference real-time RESTful API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-inference-ml-service&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The LLM microservice, called by the RESTful API, will work only after deploying the LLM to AWS SageMaker.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;ZenML&lt;/h4&gt; &#xA;&lt;p&gt;Dashboard URL: &lt;code&gt;localhost:8237&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Default credentials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;username&lt;/code&gt;: default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;password&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;‚Üí Find out more about using and setting up &lt;a href=&#34;https://docs.zenml.io/&#34;&gt;ZenML&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Qdrant&lt;/h4&gt; &#xA;&lt;p&gt;REST API URL: &lt;code&gt;localhost:6333&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dashboard URL: &lt;code&gt;localhost:6333/dashboard&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚Üí Find out more about using and setting up &lt;a href=&#34;https://qdrant.tech/documentation/quick-start/&#34;&gt;Qdrant with Docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;MongoDB&lt;/h4&gt; &#xA;&lt;p&gt;Database URI: &lt;code&gt;mongodb://llm_engineering:llm_engineering@127.0.0.1:27017&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Database name: &lt;code&gt;twin&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Default credentials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;username&lt;/code&gt;: llm_engineering&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;password&lt;/code&gt;: llm_engineering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;‚Üí Find out more about using and setting up &lt;a href=&#34;https://www.mongodb.com/docs/manual/tutorial/install-mongodb-community-with-docker&#34;&gt;MongoDB with Docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can search your MongoDB collections using your &lt;strong&gt;IDEs MongoDB plugin&lt;/strong&gt; (which you have to install separately), where you have to use the database URI to connect to the MongoDB database hosted within the Docker container: &lt;code&gt;mongodb://llm_engineering:llm_engineering@127.0.0.1:27017&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Everything related to training or running the LLMs (e.g., training, evaluation, inference) can only be run if you set up AWS SageMaker, as explained in the next section on cloud infrastructure.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Cloud infrastructure (for production)&lt;/h3&gt; &#xA;&lt;p&gt;Here we will quickly present how to deploy the project to AWS and other serverless services. We won&#39;t go into the details (as everything is presented in the book) but only point out the main steps you have to go through.&lt;/p&gt; &#xA;&lt;p&gt;First, reinstall your Python dependencies with the AWS group:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry install --with aws&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;AWS SageMaker&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Chapter 10 provides step-by-step instructions in the section &#34;Implementing the LLM microservice using AWS SageMaker&#34;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;By this point, we expect you to have AWS CLI installed and your AWS CLI and project&#39;s env vars (within the &lt;code&gt;.env&lt;/code&gt; file) properly configured with an AWS admin user.&lt;/p&gt; &#xA;&lt;p&gt;To ensure best practices, we must create a new AWS user restricted to creating and deleting only resources related to AWS SageMaker. Create it by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe create-sagemaker-role&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will create a &lt;code&gt;sagemaker_user_credentials.json&lt;/code&gt; file at the root of your repository with your new &lt;code&gt;AWS_ACCESS_KEY&lt;/code&gt; and &lt;code&gt;AWS_SECRET_KEY&lt;/code&gt; values. &lt;strong&gt;But before replacing your new AWS credentials, also run the following command to create the execution role (to create it using your admin credentials).&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To create the IAM execution role used by AWS SageMaker to access other AWS resources on our behalf, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe create-sagemaker-execution-role&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will create a &lt;code&gt;sagemaker_execution_role.json&lt;/code&gt; file at the root of your repository with your new &lt;code&gt;AWS_ARN_ROLE&lt;/code&gt; value. Add it to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve updated the &lt;code&gt;AWS_ACCESS_KEY&lt;/code&gt;, &lt;code&gt;AWS_SECRET_KEY&lt;/code&gt;, and &lt;code&gt;AWS_ARN_ROLE&lt;/code&gt; values in your &lt;code&gt;.env&lt;/code&gt; file, you can use AWS SageMaker. &lt;strong&gt;Note that this step is crucial to complete the AWS setup.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;p&gt;We start the training pipeline through ZenML by running the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-training-pipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will start the training code using the configs from &lt;code&gt;configs/training.yaml&lt;/code&gt; directly in SageMaker. You can visualize the results in Comet ML&#39;s dashboard.&lt;/p&gt; &#xA;&lt;p&gt;We start the evaluation pipeline through ZenML by running the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-evaluation-pipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will start the evaluation code using the configs from &lt;code&gt;configs/evaluating.yaml&lt;/code&gt; directly in SageMaker. You can visualize the results in &lt;code&gt;*-results&lt;/code&gt; datasets saved to your Hugging Face profile.&lt;/p&gt; &#xA;&lt;h4&gt;Inference&lt;/h4&gt; &#xA;&lt;p&gt;To create an AWS SageMaker Inference Endpoint, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe deploy-inference-endpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To test it out, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe test-sagemaker-endpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To delete it, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe delete-inference-endpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;AWS: ML pipelines, artifacts, and containers&lt;/h4&gt; &#xA;&lt;p&gt;The ML pipelines, artifacts, and containers are deployed to AWS by leveraging ZenML&#39;s deployment features. Thus, you must create an account with ZenML Cloud and follow their guide on deploying a ZenML stack to AWS. Otherwise, we provide step-by-step instructions in &lt;strong&gt;Chapter 11&lt;/strong&gt;, section &lt;strong&gt;Deploying the LLM Twin&#39;s pipelines to the cloud&lt;/strong&gt; on what you must do.&lt;/p&gt; &#xA;&lt;h4&gt;Qdrant &amp;amp; MongoDB&lt;/h4&gt; &#xA;&lt;p&gt;We leverage Qdrant&#39;s and MongoDB&#39;s serverless options when deploying the project. Thus, you can either follow &lt;a href=&#34;https://qdrant.tech/documentation/cloud/create-cluster/&#34;&gt;Qdrant&#39;s&lt;/a&gt; and &lt;a href=&#34;https://www.mongodb.com/resources/products/fundamentals/mongodb-cluster-setup&#34;&gt;MongoDB&#39;s&lt;/a&gt; tutorials on how to create a freemium cluster for each or go through &lt;strong&gt;Chapter 11&lt;/strong&gt;, section &lt;strong&gt;Deploying the LLM Twin&#39;s pipelines to the cloud&lt;/strong&gt; and follow our step-by-step instructions.&lt;/p&gt; &#xA;&lt;h4&gt;GitHub Actions&lt;/h4&gt; &#xA;&lt;p&gt;We use GitHub Actions to implement our CI/CD pipelines. To implement your own, you have to fork our repository and set the following env vars as Actions secrets in your forked repository:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AWS_ECR_NAME&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AWS_REGION&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also, we provide instructions on how to set everything up in &lt;strong&gt;Chapter 11&lt;/strong&gt;, section &lt;strong&gt;Adding LLMOps to the LLM Twin&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Comet ML &amp;amp; Opik&lt;/h4&gt; &#xA;&lt;p&gt;You can visualize the results on their self-hosted dashboards if you create a Comet account and correctly set the &lt;code&gt;COMET_API_KEY&lt;/code&gt; env var. As Opik is powered by Comet, you don&#39;t have to set up anything else along Comet:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.comet.com/&#34;&gt;Comet ML (for experiment tracking)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.comet.com/opik&#34;&gt;Opik (for prompt monitoring)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ö° Pipelines&lt;/h2&gt; &#xA;&lt;p&gt;All the ML pipelines will be orchestrated behind the scenes by &lt;a href=&#34;https://www.zenml.io/&#34;&gt;ZenML&lt;/a&gt;. A few exceptions exist when running utility scrips, such as exporting or importing from the data warehouse.&lt;/p&gt; &#xA;&lt;p&gt;The ZenML pipelines are the entry point for most processes throughout this project. They are under the &lt;code&gt;pipelines/&lt;/code&gt; folder. Thus, when you want to understand or debug a workflow, starting with the ZenML pipeline is the best approach.&lt;/p&gt; &#xA;&lt;p&gt;To see the pipelines running and their results:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;go to your ZenML dashboard&lt;/li&gt; &#xA; &lt;li&gt;go to the &lt;code&gt;Pipelines&lt;/code&gt; section&lt;/li&gt; &#xA; &lt;li&gt;click on a specific pipeline (e.g., &lt;code&gt;feature_engineering&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;click on a specific run (e.g., &lt;code&gt;feature_engineering_run_2024_06_20_18_40_24&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;click on a specific step or artifact of the DAG to find more details about it&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Now, let&#39;s explore all the pipelines you can run. From data collection to training, we will present them in their natural order to go through the LLM project end-to-end.&lt;/p&gt; &#xA;&lt;h3&gt;Data pipelines&lt;/h3&gt; &#xA;&lt;p&gt;Run the data collection ETL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-digital-data-etl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] You must have Chrome (or another Chromium-based browser) installed on your system for LinkedIn and Medium crawlers to work (which use Selenium under the hood). Based on your Chrome version, the Chromedriver will be automatically installed to enable Selenium support. Another option is to run everything using our Docker image if you don&#39;t want to install Chrome. For example, to run all the pipelines combined you can run &lt;code&gt;poetry poe run-docker-end-to-end-data-pipeline&lt;/code&gt;. Note that the command can be tweaked to support any other pipeline.&lt;/p&gt; &#xA; &lt;p&gt;If, for any other reason, you don&#39;t have a Chromium-based browser installed and don&#39;t want to use Docker, you have two other options to bypass this Selenium issue:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Comment out all the code related to Selenium, Chrome and all the links that use Selenium to crawl them (e.g., Medium), such as the &lt;code&gt;chromedriver_autoinstaller.install()&lt;/code&gt; command from &lt;a href=&#34;https://github.com/PacktPublishing/LLM-Engineers-Handbook/raw/main/llm_engineering/application/crawlers/base.py&#34;&gt;application.crawlers.base&lt;/a&gt; and other static calls that check for Chrome drivers and Selenium.&lt;/li&gt; &#xA;  &lt;li&gt;Install Google Chrome using your CLI in environments such as GitHub Codespaces or other cloud VMs using the same command as in our &lt;a href=&#34;https://github.com/PacktPublishing/LLM-Engineers-Handbook/raw/main/Dockerfile#L10&#34;&gt;Docker file&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To add additional links to collect from, go to &lt;code&gt;configs/digital_data_etl_[author_name].yaml&lt;/code&gt; and add them to the &lt;code&gt;links&lt;/code&gt; field. Also, you can create a completely new file and specify it at run time, like this: &lt;code&gt;python -m llm_engineering.interfaces.orchestrator.run --run-etl --etl-config-filename configs/digital_data_etl_[your_name].yaml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run the feature engineering pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-feature-engineering-pipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generate the instruct dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-generate-instruct-datasets-pipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generate the preference dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-generate-preference-datasets-pipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run all of the above compressed into a single pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-end-to-end-data-pipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Utility pipelines&lt;/h3&gt; &#xA;&lt;p&gt;Export the data from the data warehouse to JSON files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-export-data-warehouse-to-json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Import data to the data warehouse from JSON files (by default, it imports the data from the &lt;code&gt;data/data_warehouse_raw_data&lt;/code&gt; directory):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-import-data-warehouse-from-json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Export ZenML artifacts to JSON:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-export-artifact-to-json-pipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will export the following ZenML artifacts to the &lt;code&gt;output&lt;/code&gt; folder as JSON files (it will take their latest version):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cleaned_documents.json&lt;/li&gt; &#xA; &lt;li&gt;instruct_datasets.json&lt;/li&gt; &#xA; &lt;li&gt;preference_datasets.json&lt;/li&gt; &#xA; &lt;li&gt;raw_documents.json&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can configure what artifacts to export by tweaking the &lt;code&gt;configs/export_artifact_to_json.yaml&lt;/code&gt; configuration file.&lt;/p&gt; &#xA;&lt;h3&gt;Training pipelines&lt;/h3&gt; &#xA;&lt;p&gt;Run the training pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-training-pipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the evaluation pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-evaluation-pipeline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] For this to work, make sure you properly configured AWS SageMaker as described in &lt;a href=&#34;https://raw.githubusercontent.com/PacktPublishing/LLM-Engineers-Handbook/main/#set-up-cloud-infrastructure-for-production&#34;&gt;Set up cloud infrastructure (for production)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Inference pipelines&lt;/h3&gt; &#xA;&lt;p&gt;Call the RAG retrieval module with a test query:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe call-rag-retrieval-module&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start the inference real-time RESTful API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe run-inference-ml-service&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Call the inference real-time RESTful API with a test query:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe call-inference-ml-service&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remember that you can monitor the prompt traces on &lt;a href=&#34;https://www.comet.com/opik&#34;&gt;Opik&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] For the inference service to work, you must have the LLM microservice deployed to AWS SageMaker, as explained in the setup cloud infrastructure section.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Linting &amp;amp; formatting (QA)&lt;/h3&gt; &#xA;&lt;p&gt;Check or fix your linting issues:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe lint-check&#xA;poetry poe lint-fix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check or fix your formatting issues:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe format-check&#xA;poetry poe format-fix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check the code for leaked credentials:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe gitleaks-check&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tests&lt;/h3&gt; &#xA;&lt;p&gt;Run all the tests using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry poe test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üèÉ Run project&lt;/h2&gt; &#xA;&lt;p&gt;Based on the setup and usage steps described above, assuming the local and cloud infrastructure works and the &lt;code&gt;.env&lt;/code&gt; is filled as expected, follow the next steps to run the LLM system end-to-end:&lt;/p&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Collect data: &lt;code&gt;poetry poe run-digital-data-etl&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Compute features: &lt;code&gt;poetry poe run-feature-engineering-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Compute instruct dataset: &lt;code&gt;poetry poe run-generate-instruct-datasets-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Compute preference alignment dataset: &lt;code&gt;poetry poe run-generate-preference-datasets-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] From now on, for these steps to work, you need to properly set up AWS SageMaker, such as running &lt;code&gt;poetry install --with aws&lt;/code&gt; and filling in the AWS-related environment variables and configs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;SFT fine-tuning Llamma 3.1: &lt;code&gt;poetry poe run-training-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For DPO, go to &lt;code&gt;configs/training.yaml&lt;/code&gt;, change &lt;code&gt;finetuning_type&lt;/code&gt; to &lt;code&gt;dpo&lt;/code&gt;, and run &lt;code&gt;poetry poe run-training-pipeline&lt;/code&gt; again&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Evaluate fine-tuned models: &lt;code&gt;poetry poe run-evaluation-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] From now on, for these steps to work, you need to properly set up AWS SageMaker, such as running &lt;code&gt;poetry install --with aws&lt;/code&gt; and filling in the AWS-related environment variables and configs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Call only the RAG retrieval module: &lt;code&gt;poetry poe call-rag-retrieval-module&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy the LLM Twin microservice to SageMaker: &lt;code&gt;poetry poe deploy-inference-endpoint&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Test the LLM Twin microservice: &lt;code&gt;poetry poe test-sagemaker-endpoint&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start end-to-end RAG server: &lt;code&gt;poetry poe run-inference-ml-service&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Test RAG server: &lt;code&gt;poetry poe call-inference-ml-service&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üìÑ License&lt;/h2&gt; &#xA;&lt;p&gt;This course is an open-source project released under the MIT license. Thus, as long you distribute our LICENSE and acknowledge our work, you can safely clone or fork this project and use it as a source of inspiration for whatever you want (e.g., university projects, college degree projects, personal projects, etc.).&lt;/p&gt;</summary>
  </entry>
</feed>