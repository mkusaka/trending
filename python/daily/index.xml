<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-20T01:36:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>amrrs/chatgpt-clone</title>
    <updated>2022-12-20T01:36:47Z</updated>
    <id>tag:github.com,2022-12-20:/amrrs/chatgpt-clone</id>
    <link href="https://github.com/amrrs/chatgpt-clone" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build Yo&#39;own ChatGPT with OpenAI API &amp; Gradio&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;chatgpt-clone&lt;/h1&gt; &#xA;&lt;p&gt;Build Yo&#39;own ChatGPT with OpenAI API &amp;amp; Gradio&lt;/p&gt; &#xA;&lt;h3&gt;Instructions:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get your OpenAI API key here - &lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;https://beta.openai.com/account/api-keys&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Replace that key in the &lt;code&gt;app.py&lt;/code&gt; code&lt;/li&gt; &#xA; &lt;li&gt;Install the required libraries &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;python app.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Complete Tutorial: &lt;a href=&#34;https://youtu.be/n5nn3mQxrE8&#34;&gt;https://youtu.be/n5nn3mQxrE8&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/5347322/207718196-c5fccff3-1531-4402-99db-fe0fc6bf0e5a.mp4&#34;&gt;https://user-images.githubusercontent.com/5347322/207718196-c5fccff3-1531-4402-99db-fe0fc6bf0e5a.mp4&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/jukebox</title>
    <updated>2022-12-20T01:36:47Z</updated>
    <id>tag:github.com,2022-12-20:/openai/jukebox</id>
    <link href="https://github.com/openai/jukebox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the paper &#34;Jukebox: A Generative Model for Music&#34;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Archive (code is provided as-is, no updates expected)&lt;/p&gt; &#xA;&lt;h1&gt;Jukebox&lt;/h1&gt; &#xA;&lt;p&gt;Code for &#34;Jukebox: A Generative Model for Music&#34;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.00341&#34;&gt;Paper&lt;/a&gt; &lt;a href=&#34;https://openai.com/blog/jukebox&#34;&gt;Blog&lt;/a&gt; &lt;a href=&#34;http://jukebox.openai.com/&#34;&gt;Explorer&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb&#34;&gt;Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;p&gt;Install the conda package manager from &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Required: Sampling&#xA;conda create --name jukebox python=3.7.5&#xA;conda activate jukebox&#xA;conda install mpi4py=3.0.3 # if this fails, try: pip install mpi4py==3.0.3&#xA;conda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch&#xA;git clone https://github.com/openai/jukebox.git&#xA;cd jukebox&#xA;pip install -r requirements.txt&#xA;pip install -e .&#xA;&#xA;# Required: Training&#xA;conda install av=7.0.01 -c conda-forge &#xA;pip install ./tensorboardX&#xA; &#xA;# Optional: Apex for faster training with fused_adam&#xA;conda install pytorch=1.1 torchvision=0.3 cudatoolkit=10.0 -c pytorch&#xA;pip install -v --no-cache-dir --global-option=&#34;--cpp_ext&#34; --global-option=&#34;--cuda_ext&#34; ./apex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Sampling&lt;/h1&gt; &#xA;&lt;h2&gt;Sampling from scratch&lt;/h2&gt; &#xA;&lt;p&gt;To sample normally, run the following command. Model can be &lt;code&gt;5b&lt;/code&gt;, &lt;code&gt;5b_lyrics&lt;/code&gt;, &lt;code&gt;1b_lyrics&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --sample_length_in_seconds=20 \&#xA;--total_sample_length_in_seconds=180 --sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;python jukebox/sample.py --model=1b_lyrics --name=sample_1b --levels=3 --sample_length_in_seconds=20 \&#xA;--total_sample_length_in_seconds=180 --sr=44100 --n_samples=16 --hop_fraction=0.5,0.5,0.125&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above generates the first &lt;code&gt;sample_length_in_seconds&lt;/code&gt; seconds of audio from a song of total length &lt;code&gt;total_sample_length_in_seconds&lt;/code&gt;. To use multiple GPU&#39;s, launch the above scripts as &lt;code&gt;mpiexec -n {ngpus} python jukebox/sample.py ...&lt;/code&gt; so they use &lt;code&gt;{ngpus}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The samples decoded from each level are stored in &lt;code&gt;{name}/level_{level}&lt;/code&gt;. You can also view the samples as an html with the aligned lyrics under &lt;code&gt;{name}/level_{level}/index.html&lt;/code&gt;. Run &lt;code&gt;python -m http.server&lt;/code&gt; and open the html through the server to see the lyrics animate as the song plays.&lt;br&gt; A summary of all sampling data including zs, x, labels and sampling_kwargs is stored in &lt;code&gt;{name}/level_{level}/data.pth.tar&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The hps are for a V100 GPU with 16 GB GPU memory. The &lt;code&gt;1b_lyrics&lt;/code&gt;, &lt;code&gt;5b&lt;/code&gt;, and &lt;code&gt;5b_lyrics&lt;/code&gt; top-level priors take up 3.8 GB, 10.3 GB, and 11.5 GB, respectively. The peak memory usage to store transformer key, value cache is about 400 MB for &lt;code&gt;1b_lyrics&lt;/code&gt; and 1 GB for &lt;code&gt;5b_lyrics&lt;/code&gt; per sample. If you are having trouble with CUDA OOM issues, try &lt;code&gt;1b_lyrics&lt;/code&gt; or decrease &lt;code&gt;max_batch_size&lt;/code&gt; in sample.py, and &lt;code&gt;--n_samples&lt;/code&gt; in the script call.&lt;/p&gt; &#xA;&lt;p&gt;On a V100, it takes about 3 hrs to fully sample 20 seconds of music. Since this is a long time, it is recommended to use &lt;code&gt;n_samples &amp;gt; 1&lt;/code&gt; so you can generate as many samples as possible in parallel. The 1B lyrics and upsamplers can process 16 samples at a time, while 5B can fit only up to 3. Since the vast majority of time is spent on upsampling, we recommend using a multiple of 3 less than 16 like &lt;code&gt;--n_samples 15&lt;/code&gt; for &lt;code&gt;5b_lyrics&lt;/code&gt;. This will make the top-level generate samples in groups of three while upsampling is done in one pass.&lt;/p&gt; &#xA;&lt;p&gt;To continue sampling from already generated codes for a longer duration, you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --mode=continue \&#xA;--codes_file=sample_5b/level_0/data.pth.tar --sample_length_in_seconds=40 --total_sample_length_in_seconds=180 \&#xA;--sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, we take the 20 seconds samples saved from the first sampling run at &lt;code&gt;sample_5b/level_0/data.pth.tar&lt;/code&gt; and continue by adding 20 more seconds.&lt;/p&gt; &#xA;&lt;p&gt;You could also continue directly from the level 2 saved outputs, just pass &lt;code&gt;--codes_file=sample_5b/level_2/data.pth.tar&lt;/code&gt;. Note this will upsample the full 40 seconds song at the end.&lt;/p&gt; &#xA;&lt;p&gt;If you stopped sampling at only the first level and want to upsample the saved codes, you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --mode=upsample \&#xA;--codes_file=sample_5b/level_2/data.pth.tar --sample_length_in_seconds=20 --total_sample_length_in_seconds=180 \&#xA;--sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, we take the 20 seconds samples saved from the first sampling run at &lt;code&gt;sample_5b/level_2/data.pth.tar&lt;/code&gt; and upsample the lower two levels.&lt;/p&gt; &#xA;&lt;h2&gt;Prompt with your own music&lt;/h2&gt; &#xA;&lt;p&gt;If you want to prompt the model with your own creative piece or any other music, first save them as wave files and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python jukebox/sample.py --model=5b_lyrics --name=sample_5b_prompted --levels=3 --mode=primed \&#xA;--audio_file=path/to/recording.wav,awesome-mix.wav,fav-song.wav,etc.wav --prompt_length_in_seconds=12 \&#xA;--sample_length_in_seconds=20 --total_sample_length_in_seconds=180 --sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will load the four files, tile them to fill up to &lt;code&gt;n_samples&lt;/code&gt; batch size, and prime the model with the first &lt;code&gt;prompt_length_in_seconds&lt;/code&gt; seconds.&lt;/p&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;h2&gt;VQVAE&lt;/h2&gt; &#xA;&lt;p&gt;To train a small vqvae, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae --name=small_vqvae --sample_length=262144 --bs=4 \&#xA;--audio_files_dir={audio_files_dir} --labels=False --train --aug_shift --aug_blend&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, &lt;code&gt;{audio_files_dir}&lt;/code&gt; is the directory in which you can put the audio files for your dataset, and &lt;code&gt;{ngpus}&lt;/code&gt; is number of GPU&#39;s you want to use to train. The above trains a two-level VQ-VAE with &lt;code&gt;downs_t = (5,3)&lt;/code&gt;, and &lt;code&gt;strides_t = (2, 2)&lt;/code&gt; meaning we downsample the audio by &lt;code&gt;2**5 = 32&lt;/code&gt; to get the first level of codes, and &lt;code&gt;2**8 = 256&lt;/code&gt; to get the second level codes.&lt;br&gt; Checkpoints are stored in the &lt;code&gt;logs&lt;/code&gt; folder. You can monitor the training by running Tensorboard&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensorboard --logdir logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prior&lt;/h2&gt; &#xA;&lt;h3&gt;Train prior or upsamplers&lt;/h3&gt; &#xA;&lt;p&gt;Once the VQ-VAE is trained, we can restore it from its saved checkpoint and train priors on the learnt codes. To train the top-level prior, we can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_prior,all_fp16,cpu_ema --name=small_prior \&#xA;--sample_length=2097152 --bs=4 --audio_files_dir={audio_files_dir} --labels=False --train --test --aug_shift --aug_blend \&#xA;--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar --prior --levels=2 --level=1 --weight_decay=0.01 --save_iters=1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train the upsampler, we can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_upsampler,all_fp16,cpu_ema --name=small_upsampler \&#xA;--sample_length=262144 --bs=4 --audio_files_dir={audio_files_dir} --labels=False --train --test --aug_shift --aug_blend \&#xA;--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar --prior --levels=2 --level=0 --weight_decay=0.01 --save_iters=1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We pass &lt;code&gt;sample_length = n_ctx * downsample_of_level&lt;/code&gt; so that after downsampling the tokens match the n_ctx of the prior hps. Here, &lt;code&gt;n_ctx = 8192&lt;/code&gt; and &lt;code&gt;downsamples = (32, 256)&lt;/code&gt;, giving &lt;code&gt;sample_lengths = (8192 * 32, 8192 * 256) = (65536, 2097152)&lt;/code&gt; respectively for the bottom and top level.&lt;/p&gt; &#xA;&lt;h3&gt;Learning rate annealing&lt;/h3&gt; &#xA;&lt;p&gt;To get the best sample quality anneal the learning rate to 0 near the end of training. To do so, continue training from the latest checkpoint and run with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--restore_prior=&#34;path/to/checkpoint&#34; --lr_use_linear_decay --lr_start_linear_decay={already_trained_steps} --lr_decay={decay_steps_as_needed}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reuse pre-trained VQ-VAE and train top-level prior on new dataset from scratch.&lt;/h3&gt; &#xA;&lt;h4&gt;Train without labels&lt;/h4&gt; &#xA;&lt;p&gt;Our pre-trained VQ-VAE can produce compressed codes for a wide variety of genres of music, and the pre-trained upsamplers can upsample them back to audio that sound very similar to the original audio. To re-use these for a new dataset of your choice, you can retrain just the top-level&lt;/p&gt; &#xA;&lt;p&gt;To train top-level on a new dataset, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior \&#xA;--sample_length=1048576 --bs=4 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \&#xA;--labels=False --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training the &lt;code&gt;small_prior&lt;/code&gt; with a batch size of 2, 4, and 8 requires 6.7 GB, 9.3 GB, and 15.8 GB of GPU memory, respectively. A few days to a week of training typically yields reasonable samples when the dataset is homogeneous (e.g. all piano pieces, songs of the same style, etc).&lt;/p&gt; &#xA;&lt;p&gt;Near the end of training, follow &lt;a href=&#34;https://raw.githubusercontent.com/openai/jukebox/master/#learning-rate-annealing&#34;&gt;this&lt;/a&gt; to anneal the learning rate to 0&lt;/p&gt; &#xA;&lt;h4&gt;Sample from new model&lt;/h4&gt; &#xA;&lt;p&gt;You can then run sample.py with the top-level of our models replaced by your new model. To do so,&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add an entry &lt;code&gt;my_model=(&#34;vqvae&#34;, &#34;upsampler_level_0&#34;, &#34;upsampler_level_1&#34;, &#34;small_prior&#34;)&lt;/code&gt; in &lt;code&gt;MODELS&lt;/code&gt; in &lt;code&gt;make_models.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Update the &lt;code&gt;small_prior&lt;/code&gt; dictionary in &lt;code&gt;hparams.py&lt;/code&gt; to include &lt;code&gt;restore_prior=&#39;path/to/checkpoint&#39;&lt;/code&gt;. If you you changed any hps directly in the command line script (eg:&lt;code&gt;heads&lt;/code&gt;), make sure to update them in the dictionary too so that &lt;code&gt;make_models&lt;/code&gt; restores our checkpoint correctly.&lt;/li&gt; &#xA; &lt;li&gt;Run sample.py as outlined in the sampling section, but now with &lt;code&gt;--model=my_model&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example, let&#39;s say we trained &lt;code&gt;small_vqvae&lt;/code&gt;, &lt;code&gt;small_prior&lt;/code&gt;, and &lt;code&gt;small_upsampler&lt;/code&gt; under &lt;code&gt;/path/to/jukebox/logs&lt;/code&gt;. In &lt;code&gt;make_models.py&lt;/code&gt;, we are going to declare a tuple of the new models as &lt;code&gt;my_model&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MODELS = {&#xA;    &#39;5b&#39;: (&#34;vqvae&#34;, &#34;upsampler_level_0&#34;, &#34;upsampler_level_1&#34;, &#34;prior_5b&#34;),&#xA;    &#39;5b_lyrics&#39;: (&#34;vqvae&#34;, &#34;upsampler_level_0&#34;, &#34;upsampler_level_1&#34;, &#34;prior_5b_lyrics&#34;),&#xA;    &#39;1b_lyrics&#39;: (&#34;vqvae&#34;, &#34;upsampler_level_0&#34;, &#34;upsampler_level_1&#34;, &#34;prior_1b_lyrics&#34;),&#xA;    &#39;my_model&#39;: (&#34;my_small_vqvae&#34;, &#34;my_small_upsampler&#34;, &#34;my_small_prior&#34;),&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, in &lt;code&gt;hparams.py&lt;/code&gt;, we add them to the registry with the corresponding &lt;code&gt;restore_&lt;/code&gt;paths and any other command line options used during training. Another important note is that for top-level priors with lyric conditioning, we have to locate a self-attention layer that shows alignment between the lyric and music tokens. Look for layers where &lt;code&gt;prior.prior.transformer._attn_mods[layer].attn_func&lt;/code&gt; is either 6 or 7. If your model is starting to sing along lyrics, it means some layer, head pair has learned alignment. Congrats!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;my_small_vqvae = Hyperparams(&#xA;    restore_vqvae=&#39;/path/to/jukebox/logs/small_vqvae/checkpoint_some_step.pth.tar&#39;,&#xA;)&#xA;my_small_vqvae.update(small_vqvae)&#xA;HPARAMS_REGISTRY[&#34;my_small_vqvae&#34;] = my_small_vqvae&#xA;&#xA;my_small_prior = Hyperparams(&#xA;    restore_prior=&#39;/path/to/jukebox/logs/small_prior/checkpoint_latest.pth.tar&#39;,&#xA;    level=1,&#xA;    labels=False,&#xA;    # TODO For the two lines below, if `--labels` was used and the model is&#xA;    # trained with lyrics, find and enter the layer, head pair that has learned&#xA;    # alignment.&#xA;    alignment_layer=47,&#xA;    alignment_head=0,&#xA;)&#xA;my_small_prior.update(small_prior)&#xA;HPARAMS_REGISTRY[&#34;my_small_prior&#34;] = my_small_prior&#xA;&#xA;my_small_upsampler = Hyperparams(&#xA;    restore_prior=&#39;/path/to/jukebox/logs/small_upsampler/checkpoint_latest.pth.tar&#39;,&#xA;    level=0,&#xA;    labels=False,&#xA;)&#xA;my_small_upsampler.update(small_upsampler)&#xA;HPARAMS_REGISTRY[&#34;my_small_upsampler&#34;] = my_small_upsampler&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Train with labels&lt;/h4&gt; &#xA;&lt;p&gt;To train with you own metadata for your audio files, implement &lt;code&gt;get_metadata&lt;/code&gt; in &lt;code&gt;data/files_dataset.py&lt;/code&gt; to return the &lt;code&gt;artist&lt;/code&gt;, &lt;code&gt;genre&lt;/code&gt; and &lt;code&gt;lyrics&lt;/code&gt; for a given audio file. For now, you can pass &lt;code&gt;&#39;&#39;&lt;/code&gt; for lyrics to not use any lyrics.&lt;/p&gt; &#xA;&lt;p&gt;For training with labels, we&#39;ll use &lt;code&gt;small_labelled_prior&lt;/code&gt; in &lt;code&gt;hparams.py&lt;/code&gt;, and we set &lt;code&gt;labels=True,labels_v3=True&lt;/code&gt;. We use 2 kinds of labels information:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Artist/Genre: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For each file, we return an artist_id and a list of genre_ids. The reason we have a list and not a single genre_id is that in v2, we split genres like &lt;code&gt;blues_rock&lt;/code&gt; into a bag of words &lt;code&gt;[blues, rock]&lt;/code&gt;, and we pass atmost &lt;code&gt;max_bow_genre_size&lt;/code&gt; of those, in &lt;code&gt;v3&lt;/code&gt; we consider it as a single word and just set &lt;code&gt;max_bow_genre_size=1&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Update the &lt;code&gt;v3_artist_ids&lt;/code&gt; and &lt;code&gt;v3_genre_ids&lt;/code&gt; to use ids from your new dataset.&lt;/li&gt; &#xA;   &lt;li&gt;In &lt;code&gt;small_labelled_prior&lt;/code&gt;, set the hps &lt;code&gt;y_bins = (number_of_genres, number_of_artists)&lt;/code&gt; and &lt;code&gt;max_bow_genre_size=1&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Timing: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For each chunk of audio, we return the &lt;code&gt;total_length&lt;/code&gt; of the song, the &lt;code&gt;offset&lt;/code&gt; the current audio chunk is at and the &lt;code&gt;sample_length&lt;/code&gt; of the audio chunk. We have three timing embeddings: total_length, our current position, and our current position as a fraction of the total length, and we divide the range of these values into &lt;code&gt;t_bins&lt;/code&gt; discrete bins.&lt;/li&gt; &#xA;   &lt;li&gt;In &lt;code&gt;small_labelled_prior&lt;/code&gt;, set the hps &lt;code&gt;min_duration&lt;/code&gt; and &lt;code&gt;max_duration&lt;/code&gt; to be the shortest/longest duration of audio files you want for your dataset, and &lt;code&gt;t_bins&lt;/code&gt; for how many bins you want to discretize timing information into. Note &lt;code&gt;min_duration * sr&lt;/code&gt; needs to be at least &lt;code&gt;sample_length&lt;/code&gt; to have an audio chunk in it.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After these modifications, to train a top-level with labels, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_labelled_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior_labels \&#xA;--sample_length=1048576 --bs=4 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \&#xA;--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For sampling, follow same instructions as &lt;a href=&#34;https://raw.githubusercontent.com/openai/jukebox/master/#sample-from-new-model&#34;&gt;above&lt;/a&gt; but use &lt;code&gt;small_labelled_prior&lt;/code&gt; instead of &lt;code&gt;small_prior&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Train with lyrics&lt;/h4&gt; &#xA;&lt;p&gt;To train in addition with lyrics, update &lt;code&gt;get_metadata&lt;/code&gt; in &lt;code&gt;data/files_dataset.py&lt;/code&gt; to return &lt;code&gt;lyrics&lt;/code&gt; too. For training with lyrics, we&#39;ll use &lt;code&gt;small_single_enc_dec_prior&lt;/code&gt; in &lt;code&gt;hparams.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lyrics: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For each file, we linearly align the lyric characters to the audio, find the position in lyric that corresponds to the midpoint of our audio chunk, and pass a window of &lt;code&gt;n_tokens&lt;/code&gt; lyric characters centred around that.&lt;/li&gt; &#xA;   &lt;li&gt;In &lt;code&gt;small_single_enc_dec_prior&lt;/code&gt;, set the hps &lt;code&gt;use_tokens=True&lt;/code&gt; and &lt;code&gt;n_tokens&lt;/code&gt; to be the number of lyric characters to use for an audio chunk. Set it according to the &lt;code&gt;sample_length&lt;/code&gt; you&#39;re training on so that its large enough that the lyrics for an audio chunk are almost always found inside a window of that size.&lt;/li&gt; &#xA;   &lt;li&gt;If you use a non-English vocabulary, update &lt;code&gt;text_processor.py&lt;/code&gt; with your new vocab and set &lt;code&gt;n_vocab = number of characters in vocabulary&lt;/code&gt; accordingly in &lt;code&gt;small_single_enc_dec_prior&lt;/code&gt;. In v2, we had a &lt;code&gt;n_vocab=80&lt;/code&gt; and in v3 we missed &lt;code&gt;+&lt;/code&gt; and so &lt;code&gt;n_vocab=79&lt;/code&gt; of characters.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After these modifications, to train a top-level with labels and lyrics, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_single_enc_dec_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_single_enc_dec_prior_labels \&#xA;--sample_length=786432 --bs=4 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \&#xA;--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To simplify hps choices, here we used a &lt;code&gt;single_enc_dec&lt;/code&gt; model like the &lt;code&gt;1b_lyrics&lt;/code&gt; model that combines both encoder and decoder of the transformer into a single model. We do so by merging the lyric vocab and vq-vae vocab into a single larger vocab, and flattening the lyric tokens and the vq-vae codes into a single sequence of length &lt;code&gt;n_ctx + n_tokens&lt;/code&gt;. This uses &lt;code&gt;attn_order=12&lt;/code&gt; which includes &lt;code&gt;prime_attention&lt;/code&gt; layers with keys/values from lyrics and queries from audio. If you instead want to use a model with the usual encoder-decoder style transformer, use &lt;code&gt;small_sep_enc_dec_prior&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For sampling, follow same instructions as &lt;a href=&#34;https://raw.githubusercontent.com/openai/jukebox/master/#sample-from-new-model&#34;&gt;above&lt;/a&gt; but use &lt;code&gt;small_single_enc_dec_prior&lt;/code&gt; instead of &lt;code&gt;small_prior&lt;/code&gt;. To also get the alignment between lyrics and samples in the saved html, you&#39;ll need to set &lt;code&gt;alignment_layer&lt;/code&gt; and &lt;code&gt;alignment_head&lt;/code&gt; in &lt;code&gt;small_single_enc_dec_prior&lt;/code&gt;. To find which layer/head is best to use, run a forward pass on a training example, save the attention weight tensors for all prime_attention layers, and pick the (layer, head) which has the best linear alignment pattern between the lyrics keys and music queries.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tune pre-trained top-level prior to new style(s)&lt;/h3&gt; &#xA;&lt;p&gt;Previously, we showed how to train a small top-level prior from scratch. Assuming you have a GPU with at least 15 GB of memory and support for fp16, you could fine-tune from our pre-trained 1B top-level prior. Here are the steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support &lt;code&gt;--labels=True&lt;/code&gt; by implementing &lt;code&gt;get_metadata&lt;/code&gt; in &lt;code&gt;jukebox/data/files_dataset.py&lt;/code&gt; for your dataset.&lt;/li&gt; &#xA; &lt;li&gt;Add new entries in &lt;code&gt;jukebox/data/ids&lt;/code&gt;. We recommend replacing existing mappings (e.g. rename &lt;code&gt;&#34;unknown&#34;&lt;/code&gt;, etc with styles of your choice). This uses the pre-trained style vectors as initialization and could potentially save some compute.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After these modifications, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,prior_1b_lyrics,all_fp16,cpu_ema --name=finetuned \&#xA;--sample_length=1048576 --bs=1 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \&#xA;--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get the best sample quality, it is recommended to anneal the learning rate in the end. Training the 5B top-level requires GPipe which is not supported in this release.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;Please cite using the following bibtex entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{dhariwal2020jukebox,&#xA;  title={Jukebox: A Generative Model for Music},&#xA;  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},&#xA;  journal={arXiv preprint arXiv:2005.00341},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/jukebox/master/LICENSE&#34;&gt;Noncommercial Use License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;It covers both released code and weights.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zsyOAOA/DifFace</title>
    <updated>2022-12-20T01:36:47Z</updated>
    <id>tag:github.com,2022-12-20:/zsyOAOA/DifFace</id>
    <link href="https://github.com/zsyOAOA/DifFace" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DifFace: Blind Face Restoration with Diffused Error Contraction (PyTorch)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DifFace: Blind Face Restoration with Diffused Error Contraction&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zsyoaoa.github.io/&#34;&gt;Zongsheng Yue&lt;/a&gt;, &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34;&gt;Chen Change Loy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.06512&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1BNtoPPRuJwNDvqfwDOOmD9XJyF05Zh4m?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/OAOA/DifFace&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.laobi.icu/badge?page_id=zsyOAOA/DifFace&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/assets/DifFace_Framework.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;p&gt;&lt;span&gt;⭐&lt;/span&gt; If DifFace is helpful to your images or projects, please help star this repo. Thanks! &lt;span&gt;🤗&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.12.13&lt;/strong&gt;: Create this repo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;span&gt;👉&lt;/span&gt; Old Photo Enhancement&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MTM5NTgw&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/assets/Solvay_conference.png&#34; width=&#34;805px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MTM5NTc5&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/assets/Hepburn.png&#34; height=&#34;555px&#34; width=&#34;400px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTM5NTgy&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/assets/oldimg_05.png&#34; height=&#34;555px&#34; width=&#34;400px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;👉&lt;/span&gt; Face Restoration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/testdata/cropped_faces/0368.png&#34; height=&#34;200px&#34; width=&#34;200px&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/assets/0368.png&#34; height=&#34;200px&#34; width=&#34;200px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/testdata/cropped_faces/0885.png&#34; height=&#34;200px&#34; width=&#34;200px&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/assets/0885.png&#34; height=&#34;200px&#34; width=&#34;200px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/testdata/cropped_faces/0729.png&#34; height=&#34;200px&#34; width=&#34;200px&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/assets/0729.png&#34; height=&#34;200px&#34; width=&#34;200px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/testdata/cropped_faces/0934.png&#34; height=&#34;200px&#34; width=&#34;200px&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zsyOAOA/DifFace/master/assets/0934.png&#34; height=&#34;200px&#34; width=&#34;200px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;DifFace&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate taming&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;span&gt;👦&lt;/span&gt; Face image restoration (cropped and aligned)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_difface.py --aligned --in_path [image folder/image path] --out_path [result folder] --gpu_id [gpu index]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;span&gt;👫&lt;/span&gt; Whole image enhancement&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_difface.py --in_path [image folder/image path] --out_path [result folder] --gpu_id [gpu index]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;span&gt;🐢&lt;/span&gt; Prepare data&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;FFHQ&lt;/a&gt; dataset, and resize them into size 512x512.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python datapipe/prepare/face/big2small_face.py --face_dir [Face folder(1024x1024)] --save_dir [Saving folder] --pch_size 512 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Extract the image path into &#39;datapipe/files_txt/ffhq512.txt&#39;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python datapipe/prepare/face/split_train_val.py --face_dir [Face folder(512x512)] --save_dir [Saving folder] &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Making the testing dataset&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python datapipe/prepare/face/make_testing_data.py --files_txt datapipe/files_txt/ffhq512.txt --save_dir [Saving folder]  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;span&gt;🐬&lt;/span&gt; Train diffusion model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nproc_per_node=4 --nnodes=1 main_diffusion.py --cfg_path configs/training/diffsuion_ffhq512.yaml --save_dir [Logging Folder]  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;span&gt;🐳&lt;/span&gt; Train diffused estimator (SwinIR)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nproc_per_node=4 --nnodes=1 main_sr.py --cfg_path configs/training/swinir_ffhq512.yaml --save_dir [Logging Folder]  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under &lt;a rel=&#34;license&#34; href=&#34;https://github.com/sczhou/CodeFormer/raw/master/LICENSE&#34;&gt;NTU S-Lab License 1.0&lt;/a&gt;. Redistribution and use should follow this license.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on &lt;a href=&#34;https://github.com/openai/improved-diffusion&#34;&gt;Improved Diffusion Model&lt;/a&gt;. Some codes are brought from &lt;a href=&#34;https://github.com/XPixelGroup/BasicSR&#34;&gt;BasicSR&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;YOLOv5-face&lt;/a&gt;, and &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;FaceXLib&lt;/a&gt;. We also adopt &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt; to support background image enhancement. Thanks for their awesome works.&lt;/p&gt; &#xA;&lt;h3&gt;Contact&lt;/h3&gt; &#xA;&lt;p&gt;If you have any questions, please feel free to contact me via &lt;code&gt;zsyzam@gmail.com&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>