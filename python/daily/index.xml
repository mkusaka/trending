<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-10T01:42:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>JJTech0130/pypush</title>
    <updated>2023-12-10T01:42:42Z</updated>
    <id>tag:github.com,2023-12-10:/JJTech0130/pypush</id>
    <link href="https://github.com/JJTech0130/pypush" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cross-platform iMessage POC&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pypush&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;pypush&lt;/code&gt; is a POC demo of my recent iMessage reverse-engineering. It can currently register as a new device on an Apple ID, set up encryption keys, and &lt;em&gt;&lt;strong&gt;send and receive iMessages&lt;/strong&gt;&lt;/em&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pypush&lt;/code&gt; is completely platform-independent, and does not require a Mac or other Apple device to use!&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s pretty self explanatory:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/JJTech0130/pypush&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python3 ./demo.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;If you have any issues, please join &lt;a href=&#34;https://discord.gg/BVvNukmfTC&#34;&gt;the Discord&lt;/a&gt; and ask for help.&lt;/p&gt; &#xA;&lt;h2&gt;Operation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pypush&lt;/code&gt; will generate a &lt;code&gt;config.json&lt;/code&gt; in the repository when you run demo.py. DO NOT SHARE THIS FILE. It contains all the encryption keys necessary to log into you Apple ID and send iMessages as you.&lt;/p&gt; &#xA;&lt;p&gt;Once it loads, it should prompt you with &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt;. Type &lt;code&gt;help&lt;/code&gt; and press enter for a list of supported commands.&lt;/p&gt; &#xA;&lt;h2&gt;Special Notes&lt;/h2&gt; &#xA;&lt;h3&gt;Unicorn dependency&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;pypush&lt;/code&gt; currently uses the Unicorn CPU emulator and a custom MachO loader to load a framework from an old version of macOS, in order to call some obfuscated functions.&lt;/p&gt; &#xA;&lt;p&gt;This is only necessary during initial registration, so theoretically you can register on one device, and then copy the &lt;code&gt;config.json&lt;/code&gt; to another device that doesn&#39;t support the Unicorn emulator. Or you could switch out the emulator for another x86 emulator if you really wanted to.&lt;/p&gt; &#xA;&lt;h2&gt;&#34;data.plist&#34; and Mac serial numbers&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains a sample &lt;a href=&#34;https://github.com/JJTech0130/pypush/raw/main/emulated/data.plist&#34;&gt;&lt;code&gt;data.plist&lt;/code&gt;&lt;/a&gt;, which contains the serial number and several other identifiers from a real Mac device. If you run into issues related to rate-limiting or messages failing to deliver, you may regenerate this file by cloning &lt;a href=&#34;https://github.com/JJTech0130/nacserver&#34;&gt;nacserver&lt;/a&gt; and running &lt;code&gt;build.sh&lt;/code&gt; on a non-M1 Mac. It should place the generated file in the current directory, which you can then copy to the emulated/ folder in pypush.&lt;/p&gt; &#xA;&lt;h2&gt;Licensing&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the terms of the &lt;a href=&#34;https://www.mongodb.com/licensing/server-side-public-license&#34;&gt;SSPL&lt;/a&gt;. Portions of this project are based on &lt;a href=&#34;https://github.com/aaronst/macholibre/raw/master/LICENSE&#34;&gt;macholibre by Aaron Stephens&lt;/a&gt; under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;p&gt;This project has been purchased by &lt;a href=&#34;https://github.com/beeper&#34;&gt;Beeper&lt;/a&gt;, please contact them with any questions about licensing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>luciddreamer-cvlab/LucidDreamer</title>
    <updated>2023-12-10T01:42:42Z</updated>
    <id>tag:github.com,2023-12-10:/luciddreamer-cvlab/LucidDreamer</id>
    <link href="https://github.com/luciddreamer-cvlab/LucidDreamer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official code for the paper &#34;LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes&#34;.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/luciddreamer-cvlab/LucidDreamer/main/assets/logo_color.png&#34; height=&#34;180&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;üò¥ LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes üò¥&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://luciddreamer-cvlab.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-LucidDreamer-green&#34; alt=&#34;Project&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.13384&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2311.13384-red&#34; alt=&#34;ArXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/luciddreamer-cvlab/LucidDreamer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/luciddreamer-cvlab/LucidDreamer&#34; alt=&#34;Github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/_ironjr_&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?label=_ironjr_&amp;amp;url=https%3A%2F%2Ftwitter.com%2F_ironjr_&#34; alt=&#34;X&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/luciddreamer-cvlab/LucidDreamer/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-CC--BY--NC--SA--4.0-lightgrey&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/camenduru/LucidDreamer-Gaussian-colab/blob/main/LucidDreamer_Gaussian_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/ironjr/LucidDreamer-mini&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg?sanitize=true&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/luciddreamer-cvlab/LucidDreamer/assets/12259041/35004aaa-dffc-4133-b15a-05224e68b91e&#34;&gt;https://github.com/luciddreamer-cvlab/LucidDreamer/assets/12259041/35004aaa-dffc-4133-b15a-05224e68b91e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.13384&#34;&gt;LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;h5&gt;*&lt;a href=&#34;https://robot0321.github.io/&#34;&gt;Jaeyoung Chung&lt;/a&gt;, *&lt;a href=&#34;https://esw0116.github.io/&#34;&gt;Suyoung Lee&lt;/a&gt;, &lt;a href=&#34;https://hygenie1228.github.io/&#34;&gt;Hyeongjin Nam&lt;/a&gt;, &lt;a href=&#34;http://jaerinlee.com/&#34;&gt;Jaerin Lee&lt;/a&gt;, &lt;a href=&#34;https://cv.snu.ac.kr/index.php/~kmlee/&#34;&gt;Kyoung Mu Lee&lt;/a&gt;&lt;/h5&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h6&gt;*Denotes equal contribution.&lt;/h6&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/luciddreamer-cvlab/LucidDreamer/main/assets/logo_cvlab.png&#34; height=&#34;60&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;‚ö° Usage&lt;/h2&gt; &#xA;&lt;p&gt;We offer several ways to interact with LucidDreamer:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A demo is available on &lt;a href=&#34;https://huggingface.co/spaces/ironjr/LucidDreamer&#34;&gt;&lt;code&gt;ironjr/LucidDreamer&lt;/code&gt; HuggingFace Space&lt;/a&gt; (including custom SD ckpt) and &lt;a href=&#34;https://huggingface.co/spaces/ironjr/LucidDreamer-mini&#34;&gt;&lt;code&gt;ironjr/LucidDreamer-mini&lt;/code&gt; HuggingFace Space&lt;/a&gt; (minimal features / try at here in case of the former is down) (We appreciate all the HF / Gradio team for their support).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/luciddreamer-cvlab/LucidDreamer/assets/12259041/745bfc46-8215-4db2-80d5-4825e91316bc&#34;&gt;https://github.com/luciddreamer-cvlab/LucidDreamer/assets/12259041/745bfc46-8215-4db2-80d5-4825e91316bc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Another demo is available on a &lt;a href=&#34;https://colab.research.google.com/github/camenduru/LucidDreamer-Gaussian-colab/blob/main/LucidDreamer_Gaussian_colab.ipynb&#34;&gt;Colab&lt;/a&gt;, implemented by &lt;a href=&#34;https://github.com/camenduru&#34;&gt;@camenduru&lt;/a&gt; (We greatly thank &lt;a href=&#34;https://github.com/camenduru&#34;&gt;@camenduru&lt;/a&gt; for the contribution).&lt;/li&gt; &#xA; &lt;li&gt;You can use the gradio demo locally by running &lt;a href=&#34;https://raw.githubusercontent.com/luciddreamer-cvlab/LucidDreamer/main/app.py&#34;&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python app.py&lt;/code&gt;&lt;/a&gt; (full feature including huggingface model download, requires ~15GB) or &lt;a href=&#34;https://raw.githubusercontent.com/luciddreamer-cvlab/LucidDreamer/main/app_mini.py&#34;&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python app_mini.py&lt;/code&gt;&lt;/a&gt; (minimum viable demo, uses only SD1.5).&lt;/li&gt; &#xA; &lt;li&gt;You can also run this with command line interface as described below.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Prerequisite&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux: Ubuntu&amp;gt;=18.04.&lt;/li&gt; &#xA; &lt;li&gt;CUDA&amp;gt;=11.4 (higher version is OK).&lt;/li&gt; &#xA; &lt;li&gt;Python==3.9 (cannot use 3.10 due to open3d compatibility)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n lucid python=3.9&#xA;conda activate lucid&#xA;pip install peft diffusers scipy numpy imageio[ffmpeg] opencv-python Pillow open3d torchvision gradio&#xA;pip install torch==2.0.1 timm==0.6.7 # ZoeDepth&#xA;pip install plyfile==0.8.1 # Gaussian splatting&#xA;&#xA;cd submodules/depth-diff-gaussian-rasterization-min&#xA;# sudo apt-get install libglm-dev # may be required for the compilation.&#xA;python setup.py install&#xA;cd ../simple-knn&#xA;python setup.py install&#xA;cd ../..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run with your own samples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Default Example&#xA;python run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run with your own inputs and prompts, attach following arguments after &lt;code&gt;run.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-img&lt;/code&gt; : path of input image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-t&lt;/code&gt; : text prompt. Can be either path to txt file or the text itself.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-nt&lt;/code&gt; : negative text prompt. Can be either path to txt file or the text itself.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-cg&lt;/code&gt; : camera extrinsic path for generating scenes. Can be one of &#34;Rotate_360&#34;, &#34;LookAround&#34;, or &#34;LookDown&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-cr&lt;/code&gt; : camera extrinsic path for rendering videos. Can be one of &#34;Back_and_forth&#34;, &#34;LLFF&#34;, or &#34;Headbanging&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--seed&lt;/code&gt; : manual seed for Stable Diffusion inpainting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--diff_steps&lt;/code&gt; : number of denoising steps for Stable Diffusion inpainting. Default is 50.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-s&lt;/code&gt; : path to save results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Visualize &lt;code&gt;.ply&lt;/code&gt; files&lt;/h3&gt; &#xA;&lt;p&gt;There are multiple available viewers / editors for Gaussian splatting &lt;code&gt;.ply&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/playcanvas&#34;&gt;@playcanvas&lt;/a&gt;&#39;s &lt;a href=&#34;https://github.com/playcanvas/super-splat&#34;&gt;Super-Splat&lt;/a&gt; project (&lt;a href=&#34;https://playcanvas.com/super-splat&#34;&gt;Live demo&lt;/a&gt;). This is the viewer we have used for our debugging along with MeshLab.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/luciddreamer-cvlab/LucidDreamer/assets/12259041/89c4b5dd-c66f-4ad2-b1be-e5f951273049&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/antimatter15&#34;&gt;@antimatter15&lt;/a&gt;&#39;s &lt;a href=&#34;https://github.com/antimatter15/splat&#34;&gt;WebGL viewer&lt;/a&gt; for Gaussian splatting (&lt;a href=&#34;https://antimatter15.com/splat/&#34;&gt;Live demo&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/splinetool&#34;&gt;@splinetool&lt;/a&gt;&#39;s &lt;a href=&#34;https://spline.design/&#34;&gt;web-based viewer&lt;/a&gt; for Gaussian splatting. This is the version we have used in our project page&#39;s demo.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üö© &lt;strong&gt;Updates&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ December 8, 2023: &lt;a href=&#34;https://huggingface.co/spaces/ironjr/LucidDreamer&#34;&gt;HuggingFace Space demo&lt;/a&gt; is out. We deeply thank all the HF team for their support!&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ December 7, 2023: &lt;a href=&#34;https://colab.research.google.com/github/camenduru/LucidDreamer-Gaussian-colab/blob/main/LucidDreamer_Gaussian_colab.ipynb&#34;&gt;Colab&lt;/a&gt; implementation is now available thanks to &lt;a href=&#34;https://github.com/camenduru&#34;&gt;@camenduru&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ December 6, 2023: Code release!&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ November 22, 2023: We have released our paper, LucidDreamer on &lt;a href=&#34;https://arxiv.org/abs/2311.13384&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåè Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite us if you find our project useful!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@article{chung2023luciddreamer,&#xA;    title={LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes},&#xA;    author={Chung, Jaeyoung and Lee, Suyoung and Nam, Hyeongjin and Lee, Jaerin and Lee, Kyoung Mu},&#xA;    journal={arXiv preprint arXiv:2311.13384},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§ó Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We deeply appreciate &lt;a href=&#34;https://github.com/isl-org/ZoeDepth&#34;&gt;ZoeDepth&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Stability AI&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Runway&lt;/a&gt; for their models.&lt;/p&gt; &#xA;&lt;h2&gt;üìß Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please email &lt;code&gt;robot0321@snu.ac.kr&lt;/code&gt;, &lt;code&gt;esw0116@snu.ac.kr&lt;/code&gt;, &lt;code&gt;jarin.lee@gmail.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; &#xA;&lt;a href=&#34;https://star-history.com/#luciddreamer-cvlab/LucidDreamer&amp;amp;Date&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=luciddreamer-cvlab/LucidDreamer&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=luciddreamer-cvlab/LucidDreamer&amp;amp;type=Date&#34;&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=luciddreamer-cvlab/LucidDreamer&amp;amp;type=Date&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/PurpleLlama</title>
    <updated>2023-12-10T01:42:42Z</updated>
    <id>tag:github.com,2023-12-10:/facebookresearch/PurpleLlama</id>
    <link href="https://github.com/facebookresearch/PurpleLlama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Set of tools to assess and improve LLM security.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/PurpleLlama/raw/main/logo.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/meta-Llama&#34;&gt; Models on Hugging Face&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai&#34;&gt; Blog&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/llama/purple-llama&#34;&gt;Website&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/&#34;&gt;CyberSec Eval Paper&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/&#34;&gt;Llama Guard Paper&lt;/a&gt;&amp;nbsp; &lt;br&gt; &lt;/p&gt;&#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Purple Llama&lt;/h1&gt; &#xA;&lt;p&gt;Purple Llama is a an umbrella project that over time will bring together tools and evals to help the community build responsibly with open generative AI models. The initial release will include tools and evals for Cyber Security and Input/Output safeguards but we plan to contribute more in the near future.&lt;/p&gt; &#xA;&lt;h2&gt;Why purple?&lt;/h2&gt; &#xA;&lt;p&gt;Borrowing a &lt;a href=&#34;https://www.youtube.com/watch?v=ab_Fdp6FVDI&#34;&gt;concept&lt;/a&gt; from the cybersecurity world, we believe that to truly mitigate the challenges which generative AI presents, we need to take both attack (red team) and defensive (blue team) postures. Purple teaming, composed of both red and blue team responsibilities, is a collaborative approach to evaluating and mitigating potential risks and the same ethos applies to generative AI and hence our investment in Purple Llama will be comprehensive.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Components within the Purple Llama project will be licensed permissively enabling both research and commercial usage. We believe this is a major step towards enabling community collaboration and standardizing the development and usage of trust and safety tools for generative AI development. More concretely evals and benchmarks are licensed under the MIT license while any models use the Llama 2 Community license. See the table below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Component Type&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Components&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Evals/Benchmarks&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cyber Security Eval (others to come)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama Guard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/PurpleLlama/raw/main/LICENSE&#34;&gt;Llama 2 Community License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Evals &amp;amp; Benchmarks&lt;/h2&gt; &#xA;&lt;h3&gt;Cybersecurity&lt;/h3&gt; &#xA;&lt;p&gt;We are sharing what we believe is the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&amp;amp;CK) and built in collaboration with our security subject matter experts. With this initial release, we aim to provide tools that will help address some risks outlined in the &lt;a href=&#34;https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/&#34;&gt;White House commitments on developing responsible AI&lt;/a&gt;, including:&lt;/p&gt; &#xA;&lt;p&gt;Metrics for quantifying LLM cybersecurity risks. Tools to evaluate the frequency of insecure code suggestions. Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks. We believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our &lt;a href=&#34;https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/&#34;&gt;Cybersec Eval paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;You can also check out the ü§ó leaderboard &lt;a href=&#34;https://huggingface.co/spaces/facebook/CyberSecEval&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Input/Output Safeguards&lt;/h2&gt; &#xA;&lt;p&gt;As we outlined in Llama 2‚Äôs &lt;a href=&#34;https://ai.meta.com/llama/responsible-use-guide/&#34;&gt;Responsible Use Guide&lt;/a&gt;, we recommend that all inputs and outputs to the LLM be checked and filtered in accordance with content guidelines appropriate to the application.&lt;/p&gt; &#xA;&lt;h3&gt;Llama Guard&lt;/h3&gt; &#xA;&lt;p&gt;To support this, and empower the community, we are releasing Llama Guard, an openly-available model that performs competitively on common open benchmarks and provides developers with a pretrained model to help defend against generating potentially risky outputs.&lt;/p&gt; &#xA;&lt;p&gt;As part of our ongoing commitment to open and transparent science, we are releasing our methodology and an extended discussion of model performance in our &lt;a href=&#34;https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/&#34;&gt;Llama Guard paper&lt;/a&gt;. This model has been trained on a mix of publicly-available datasets to enable detection of common types of potentially risky or violating content that may be relevant to a number of developer use cases. Ultimately, our vision is to enable developers to customize this model to support relevant use cases and to make it easier to adopt best practices and improve the open ecosystem.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started and learn how to use Purple Llama components with Llama models, see the getting started guide &lt;a href=&#34;https://ai.meta.com/llama/get-started/&#34;&gt;here&lt;/a&gt;. The guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while responsibly building with Llama. The guide will be updated as more Purple Llama components get released.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;For a running list of frequently asked questions, for not only Purple Llama components but also generally for Llama models, see the FAQ &lt;a href=&#34;https://ai.meta.com/llama/faq/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Join the Purple Llama community&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PurpleLlama/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; file for how to help out.&lt;/p&gt;</summary>
  </entry>
</feed>