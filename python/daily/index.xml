<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-17T01:41:57Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>THUDM/ChatGLM-6B</title>
    <updated>2023-03-17T01:41:57Z</updated>
    <id>tag:github.com,2023-03-17:/THUDM/ChatGLM-6B</id>
    <link href="https://github.com/THUDM/ChatGLM-6B" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatGLM-6B：开源双语对话语言模型 | An open bilingual dialogue language model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGLM-6B&lt;/h1&gt; &#xA;&lt;h2&gt;介绍&lt;/h2&gt; &#xA;&lt;p&gt;ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 &lt;a href=&#34;https://github.com/THUDM/GLM&#34;&gt;General Language Model (GLM)&lt;/a&gt; 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。更多信息请参考我们的&lt;a href=&#34;https://chatglm.cn/blog&#34;&gt;博客&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;不过，由于ChatGLM-6B的规模较小，目前已知其具有相当多的&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/#%E5%B1%80%E9%99%90%E6%80%A7&#34;&gt;&lt;strong&gt;局限性&lt;/strong&gt;&lt;/a&gt;，如事实性/数学逻辑错误，可能生成有害/有偏见内容，较弱的上下文能力，自我认知混乱，以及对英文指示生成与中文指示完全矛盾的内容。请大家在使用前了解这些问题，以免产生误解。&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Read this in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/README_en.md&#34;&gt;English&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;硬件需求&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;量化等级&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;最低 GPU 显存&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP16（无量化）&lt;/td&gt; &#xA;   &lt;td&gt;13 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;10 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;6 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;使用方式&lt;/h2&gt; &#xA;&lt;h3&gt;环境安装&lt;/h3&gt; &#xA;&lt;p&gt;使用 pip 安装依赖：&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;，其中 &lt;code&gt;transformers&lt;/code&gt; 库版本推荐为 &lt;code&gt;4.26.1&lt;/code&gt;，但理论上不低于 &lt;code&gt;4.23.1&lt;/code&gt; 即可。&lt;/p&gt; &#xA;&lt;h3&gt;代码调用&lt;/h3&gt; &#xA;&lt;p&gt;可以通过如下代码调用 ChatGLM-6B 模型来生成对话：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoTokenizer, AutoModel&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model = AutoModel.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True).half().cuda()&#xA;&amp;gt;&amp;gt;&amp;gt; response, history = model.chat(tokenizer, &#34;你好&#34;, history=[])&#xA;&amp;gt;&amp;gt;&amp;gt; print(response)&#xA;你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。&#xA;&amp;gt;&amp;gt;&amp;gt; response, history = model.chat(tokenizer, &#34;晚上睡不着应该怎么办&#34;, history=history)&#xA;&amp;gt;&amp;gt;&amp;gt; print(response)&#xA;晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:&#xA;&#xA;1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。&#xA;2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。&#xA;3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。&#xA;4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。&#xA;5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。&#xA;6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。&#xA;&#xA;如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;完整的模型实现可以在 &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;Hugging Face Hub&lt;/a&gt; 上查看。如果你从Hugging Face Hub上下载checkpoint的速度较慢，也可以从&lt;a href=&#34;https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/&#34;&gt;这里&lt;/a&gt;手动下载。&lt;/p&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;我们提供了一个基于 &lt;a href=&#34;https://gradio.app&#34;&gt;Gradio&lt;/a&gt; 的网页版 Demo 和一个命令行 Demo。使用时首先需要下载本仓库：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/THUDM/ChatGLM-6B&#xA;cd ChatGLM-6B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;网页版 Demo&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/resources/web-demo.png&#34; alt=&#34;web-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;首先安装 Gradio：&lt;code&gt;pip install gradio&lt;/code&gt;，然后运行仓库中的 &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/web_demo.py&#34;&gt;web_demo.py&lt;/a&gt;：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;程序会运行一个 Web Server，并输出地址。在浏览器中打开输出的地址即可使用。&lt;/p&gt; &#xA;&lt;h4&gt;命令行 Demo&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/resources/cli-demo.png&#34; alt=&#34;cli-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;运行仓库中 &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/cli_demo.py&#34;&gt;cli_demo.py&lt;/a&gt;：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python cli_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;程序会在命令行中进行交互式的对话，在命令行中输入指示并回车即可生成回复，输入&lt;code&gt;clear&lt;/code&gt;可以清空对话历史，输入&lt;code&gt;stop&lt;/code&gt;终止程序。&lt;/p&gt; &#xA;&lt;h2&gt;低成本部署&lt;/h2&gt; &#xA;&lt;h3&gt;模型量化&lt;/h3&gt; &#xA;&lt;p&gt;默认情况下，模型以 FP16 精度加载，运行上述代码需要大概 13GB 显存。如果你的 GPU 显存有限，可以尝试以量化方式加载模型，使用方法如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 按需修改，目前只支持 4/8 bit 量化&#xA;model = AutoModel.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True).half().quantize(4).cuda()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;进行 2 至 3 轮对话后，8-bit 量化下 GPU 显存占用约为 10GB，4-bit 量化下仅需 6GB 占用。随着对话轮数的增多，对应消耗显存也随之增长，由于采用了相对位置编码，理论上 ChatGLM-6B 支持无限长的 context-length，但总长度超过 2048（训练长度）后性能会逐渐下降。&lt;/p&gt; &#xA;&lt;p&gt;模型量化会带来一定的性能损失，经过测试，ChatGLM-6B 在 4-bit 量化下仍然能够进行自然流畅的生成。使用 &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPT-Q&lt;/a&gt; 等量化方案可以进一步压缩量化精度/提升相同量化精度下的模型性能，欢迎大家提出对应的 Pull Request。&lt;/p&gt; &#xA;&lt;h3&gt;CPU部署&lt;/h3&gt; &#xA;&lt;p&gt;如果你没有GPU硬件的话，也可以在CPU上进行推理。使用方法如下&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModel.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True).float()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CPU上推理速度可能会比较慢。&lt;/p&gt; &#xA;&lt;p&gt;以上方法需要32G内存。如果你只有16G内存，可以尝试&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModel.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True).bfloat16()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;需保证空闲内存接近16G，并且推理速度会很慢。&lt;/p&gt; &#xA;&lt;p&gt;MacOS 如果报错&lt;code&gt;RuntimeError: Unknown platform: darwin&lt;/code&gt;的话请参考这个&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/issues/6#issuecomment-1470060041&#34;&gt;Issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ChatGLM-6B示例&lt;/h2&gt; &#xA;&lt;p&gt;以下是一些使用&lt;code&gt;web_demo.py&lt;/code&gt;得到的示例截图。更多ChatGLM-6B的可能，等待你来探索发现！&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;自我认知&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/self-introduction.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;提纲写作&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/blog-outline.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;文案写作&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/ad-writing-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/comments-writing.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;邮件写作助手&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/email-writing-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/email-writing-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;信息抽取&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/information-extraction.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;角色扮演&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/role-play.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;评论比较&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/sport.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;旅游向导&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/examples/tour-guide.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;局限性&lt;/h2&gt; &#xA;&lt;p&gt;由于ChatGLM-6B的小规模，其能力仍然有许多局限性。以下是我们目前发现的一些问题：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;模型容量较小：6B的小容量，决定了其相对较弱的模型记忆和语言能力。在面对许多事实性知识任务时，ChatGLM-6B可能会生成不正确的信息；它也不擅长逻辑类问题（如数学、编程）的解答。&lt;/p&gt; &#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;&lt;b&gt;点击查看例子&lt;/b&gt;&lt;/summary&gt; &#xA;   &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/limitations/factual_error.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;   &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/limitations/math_error.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;产生有害说明或有偏见的内容：ChatGLM-6B只是一个初步与人类意图对齐的语言模型，可能会生成有害、有偏见的内容。（内容可能具有冒犯性，此处不展示）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;英文能力不足：ChatGLM-6B 训练时使用的指示/回答大部分都是中文的，仅有极小一部分英文内容。因此，如果输入英文指示，回复的质量远不如中文，甚至与中文指示下的内容矛盾，并且出现中英夹杂的情况。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;易被误导，对话能力较弱：ChatGLM-6B 对话能力还比较弱，而且 “自我认知” 存在问题，并很容易被误导并产生错误的言论。例如当前版本的模型在被误导的情况下，会在自我认知上发生偏差。&lt;/p&gt; &#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;&lt;b&gt;点击查看例子&lt;/b&gt;&lt;/summary&gt; &#xA;   &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/limitations/self-confusion_google.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;   &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/limitations/self-confusion_openai.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;   &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/limitations/self-confusion_tencent.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;协议&lt;/h2&gt; &#xA;&lt;p&gt;本仓库的代码依照 &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt; 协议开源，ChatGLM-6B 模型的权重的使用则需要遵循 &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;引用&lt;/h2&gt; &#xA;&lt;p&gt;如果你觉得我们的工作有帮助的话，请考虑引用下列论文&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{&#xA;  zeng2023glm-130b,&#xA;  title={{GLM}-130B: An Open Bilingual Pre-trained Model},&#xA;  author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Zhiyuan Liu and Peng Zhang and Yuxiao Dong and Jie Tang},&#xA;  booktitle={The Eleventh International Conference on Learning Representations (ICLR)},&#xA;  year={2023},&#xA;  url={https://openreview.net/forum?id=-Aw0rrrPUF}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{du2022glm,&#xA;  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},&#xA;  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},&#xA;  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},&#xA;  pages={320--335},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>openai/evals</title>
    <updated>2023-03-17T01:41:57Z</updated>
    <id>tag:github.com,2023-03-17:/openai/evals</id>
    <link href="https://github.com/openai/evals" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Evals&lt;/h1&gt; &#xA;&lt;p&gt;Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.&lt;/p&gt; &#xA;&lt;p&gt;You can use Evals to create and run evaluations that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;use datasets to generate prompts,&lt;/li&gt; &#xA; &lt;li&gt;measure the quality of completions provided by an OpenAI model, and&lt;/li&gt; &#xA; &lt;li&gt;compare performance across different datasets and models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With Evals, we aim to make it as simple as possible to build an eval while writing as little code as possible. To get started, we recommend that you follow these steps &lt;strong&gt;in order&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Read through this doc and follow the &lt;a href=&#34;https://raw.githubusercontent.com/openai/evals/main/README.md#Setup&#34;&gt;setup instructions below&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Learn how to run existing evals: &lt;a href=&#34;https://raw.githubusercontent.com/openai/evals/main/docs/run-evals.md&#34;&gt;run-evals.md&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Familiarize yourself with the existing eval templates: &lt;a href=&#34;https://raw.githubusercontent.com/openai/evals/main/docs/eval-templates.md&#34;&gt;eval-templates.md&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Walk through the process for building an eval: &lt;a href=&#34;https://raw.githubusercontent.com/openai/evals/main/docs/build-eval.md&#34;&gt;build-eval.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;See an example of implementing custom eval logic: &lt;a href=&#34;https://raw.githubusercontent.com/openai/evals/main/docs/custom-eval.md&#34;&gt;custom-eval.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you think you have an interesting eval, please open a PR with your contribution. OpenAI staff actively review these evals when considering improvements to upcoming models.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;🚨 For a limited time, we will be granting GPT-4 access to those who contribute high quality evals. Please follow the instructions mentioned above and note that spam or low quality submissions will be ignored❗️&lt;/p&gt; &#xA;&lt;p&gt;Access will be granted to the email address associated with an accepted Eval. Due to high volume, we are unable to grant access to any email other than the one used for the pull request.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;To run evals, you will need to set up and specify your OpenAI API key. You can generate one at &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;https://platform.openai.com/account/api-keys&lt;/a&gt;. After you obtain an API key, specify it using the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable. &lt;strong&gt;Please be aware of the &lt;a href=&#34;https://openai.com/pricing&#34;&gt;costs&lt;/a&gt; associated with using the API when running evals.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Downloading evals&lt;/h3&gt; &#xA;&lt;p&gt;Our Evals registry is stored using &lt;a href=&#34;https://git-lfs.com/&#34;&gt;Git-LFS&lt;/a&gt;. Once you have downloaded and installed LFS, you can fetch the evals with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git lfs fetch --all&#xA;git lfs pull&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may just want to fetch data for a select eval. You can achieve this via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git lfs fetch --include=evals/registry/data/${your eval}&#xA;git lfs pull&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Making evals&lt;/h3&gt; &#xA;&lt;p&gt;If you are going to be creating evals, we suggest cloning this repo directly from GitHub and installing the requirements using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using &lt;code&gt;-e&lt;/code&gt;, changes you make to your eval will be reflected immediately without having to reinstall.&lt;/p&gt; &#xA;&lt;h3&gt;Running evals&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t want to contribute new evals, but simply want to run them locally, you can install the evals package via pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install evals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide the option for you to log your eval results to a Snowflake database, if you have one or wish to set one up. For this option, you will further have to specify the &lt;code&gt;SNOWFLAKE_ACCOUNT&lt;/code&gt;, &lt;code&gt;SNOWFLAKE_DATABASE&lt;/code&gt;, &lt;code&gt;SNOWFLAKE_USERNAME&lt;/code&gt;, and &lt;code&gt;SNOWFLAKE_PASSWORD&lt;/code&gt; environment variables.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;Do you have any examples of how to build an eval from start to finish?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Yes! These are in the &lt;code&gt;examples&lt;/code&gt; folder. We recommend that you also read through &lt;a href=&#34;https://raw.githubusercontent.com/openai/evals/main/docs/build-eval.md&#34;&gt;build-eval.md&lt;/a&gt; in order to gain a deeper understanding of what is happening in these examples.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Do you have any examples of evals implemented in multiple different ways?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Yes! In particular, see &lt;code&gt;evals/registry/evals/coqa.yaml&lt;/code&gt;. We have implemented small subsets of the &lt;a href=&#34;https://stanfordnlp.github.io/coqa/&#34;&gt;CoQA&lt;/a&gt; dataset for various eval templates to help illustrate the differences.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I changed my data but this isn&#39;t reflected when running my eval, what&#39;s going on?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Your data may have been cached to &lt;code&gt;/tmp/filecache&lt;/code&gt;. Try removing this cache and rerunning your eval.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There&#39;s a lot of code, and I just want to spin up a quick eval. Help? OR,&lt;/p&gt; &#xA;&lt;p&gt;I am a world-class prompt engineer. I choose not to code. How can I contribute my wisdom?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you follow an existing &lt;a href=&#34;https://raw.githubusercontent.com/openai/evals/main/docs/eval-templates.md&#34;&gt;eval template&lt;/a&gt; to build a basic or model-graded eval, you don&#39;t need to write any evaluation code at all! Just provide your data in JSON format and specify your eval parameters in YAML. &lt;a href=&#34;https://raw.githubusercontent.com/openai/evals/main/docs/build-eval.md&#34;&gt;build-eval.md&lt;/a&gt; walks you through these steps, and you can supplement these instructions with the Jupyter notebooks in the &lt;code&gt;examples&lt;/code&gt; folder to help you get started quickly. Keep in mind, though, that a good eval will inevitably require careful thought and rigorous experimentation!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;By contributing to Evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an Eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI Evals will be subject to our usual Usage Policies: &lt;a href=&#34;https://platform.openai.com/docs/usage-policies&#34;&gt;https://platform.openai.com/docs/usage-policies&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hnmr293/sd-webui-cutoff</title>
    <updated>2023-03-17T01:41:57Z</updated>
    <id>tag:github.com,2023-03-17:/hnmr293/sd-webui-cutoff</id>
    <link href="https://github.com/hnmr293/sd-webui-cutoff" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cutoff - Cutting Off Prompt Effect&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cutoff - Cutting Off Prompt Effect&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hnmr293/sd-webui-cutoff/main/images/cover.jpg&#34; alt=&#34;cover&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Update Info&lt;/summary&gt; &#xA; &lt;p&gt;Upper is newer.&lt;/p&gt; &#xA; &lt;dl&gt; &#xA;  &lt;dt&gt;&#xA;   20e87ce264338b824296b7559679ed1bb0bdacd7&#xA;  &lt;/dt&gt; &#xA;  &lt;dd&gt;&#xA;   Skip empty targets.&#xA;  &lt;/dd&gt; &#xA;  &lt;dt&gt;&#xA;   03bfe60162ba418e18dbaf8f1b9711fd62195ef3&#xA;  &lt;/dt&gt; &#xA;  &lt;dd&gt;&#xA;   Add &#xA;   &lt;code&gt;Disable for Negative prompt&lt;/code&gt; option. Default is &#xA;   &lt;code&gt;True&lt;/code&gt;.&#xA;  &lt;/dd&gt; &#xA;  &lt;dt&gt;&#xA;   f0990088fed0f5013a659cacedb194313a398860&#xA;  &lt;/dt&gt; &#xA;  &lt;dd&gt;&#xA;   Accept an empty prompt.&#xA;  &lt;/dd&gt; &#xA; &lt;/dl&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;What is this?&lt;/h2&gt; &#xA;&lt;p&gt;This is an extension for &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;stable-diffusion-webui&lt;/a&gt; which limits the tokens&#39; influence scope.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Select &lt;code&gt;Enabled&lt;/code&gt; checkbox.&lt;/li&gt; &#xA; &lt;li&gt;Input words which you want to limit scope in &lt;code&gt;Target tokens&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Generate images.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Note&lt;/h2&gt; &#xA;&lt;p&gt;If the generated image was corrupted or something like that, try to change the &lt;code&gt;Weight&lt;/code&gt; value or change the interpolation method to &lt;code&gt;SLerp&lt;/code&gt;. Interpolation method can be found in &lt;code&gt;Details&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;7th_anime_v3_A-fp16 / kl-f8-anime2 / DPM++ 2M Karras / 15 steps / 512x768&#xA;Prompt: a cute girl, white shirt with green tie, red shoes, blue hair, yellow eyes, pink skirt&#xA;Negative Prompt: (low quality, worst quality:1.4), nsfw&#xA;Target tokens: white, green, red, blue, yellow, pink&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sample 1.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hnmr293/sd-webui-cutoff/main/images/sample-1.png&#34; alt=&#34;sample 1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sample 2. (use &lt;code&gt;SLerp&lt;/code&gt; for interpolation)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hnmr293/sd-webui-cutoff/main/images/sample-2.png&#34; alt=&#34;sample 2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sample 3.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hnmr293/sd-webui-cutoff/main/images/sample-3.png&#34; alt=&#34;sample 3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hnmr293/sd-webui-cutoff/main/images/idea.png&#34; alt=&#34;idea&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Japanese&lt;/h3&gt; &#xA;&lt;p&gt;プロンプトをCLIPに通して得られる (77, 768) 次元の埋め込み表現（？正式な用語は分かりません）について、 ごく単純には、77個の行ベクトルはプロンプト中の75個のトークン（＋開始トークン＋終了トークン）に対応していると考えられる。&lt;/p&gt; &#xA;&lt;p&gt;※上図は作図上、この説明とは行と列を入れ替えて描いている。&lt;/p&gt; &#xA;&lt;p&gt;このベクトルには単語単体の意味だけではなく、文章全体の、例えば係り結びなどの情報を集約したものが入っているはずである。&lt;/p&gt; &#xA;&lt;p&gt;ここで &lt;code&gt;a cute girl, pink hair, red shoes&lt;/code&gt; というプロンプトを考える。 普通、こういったプロンプトの意図は&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;pink&lt;/code&gt; は &lt;code&gt;hair&lt;/code&gt; だけに係っており &lt;code&gt;shoes&lt;/code&gt; には係っていない。&lt;/li&gt; &#xA; &lt;li&gt;同様に &lt;code&gt;red&lt;/code&gt; も &lt;code&gt;hair&lt;/code&gt; には係っていない。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a cute girl&lt;/code&gt; は全体に係っていて欲しい。&lt;code&gt;hair&lt;/code&gt; や &lt;code&gt;shoes&lt;/code&gt; は女の子に合うものが出て欲しい。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;……というもののはずである。&lt;/p&gt; &#xA;&lt;p&gt;しかしながら、&lt;a href=&#34;https://github.com/hnmr293/sd-webui-evviz2&#34;&gt;EvViz2&lt;/a&gt; などでトークン間の関係を見ると、そううまくはいっていないことが多い。 つまり、&lt;code&gt;shoes&lt;/code&gt; の位置のベクトルに &lt;code&gt;pink&lt;/code&gt; の影響が出てしまっていたりする。&lt;/p&gt; &#xA;&lt;p&gt;一方で上述の通り &lt;code&gt;a cute girl&lt;/code&gt; の影響は乗っていて欲しいわけで、どうにかして、特定のトークンの影響を取り除けるようにしたい。&lt;/p&gt; &#xA;&lt;p&gt;この拡張では、指定されたトークンを &lt;em&gt;padding token&lt;/em&gt; に書き換えることでそれを実現している。&lt;/p&gt; &#xA;&lt;p&gt;たとえば &lt;code&gt;red shoes&lt;/code&gt; の部分に対応して &lt;code&gt;a cute girl, _ hair, red shoes&lt;/code&gt; というプロンプトを生成する。&lt;code&gt;red&lt;/code&gt; と &lt;code&gt;shoes&lt;/code&gt; に対応する位置のベクトルをここから生成したもので上書きしてやることで、&lt;code&gt;pink&lt;/code&gt; の影響を除外している。&lt;/p&gt; &#xA;&lt;p&gt;これを &lt;code&gt;pink&lt;/code&gt; の側から見ると、自分の影響が &lt;code&gt;pink hair&lt;/code&gt; の範囲内に制限されているように見える。What is this? の &#34;limits the tokens&#39; influence scope&#34; はそういう意味。&lt;/p&gt; &#xA;&lt;p&gt;ところで &lt;code&gt;a cute girl&lt;/code&gt; の方は、&lt;code&gt;pink hair, red shoes&lt;/code&gt; の影響を受けていてもいいし受けなくてもいいような気がする。 そこでこの拡張では、こういうどちらでもいいプロンプトに対して&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;a cute girl, pink hair, red shoes&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a cute girl, _ hair, _ shoes&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;のどちらを適用するか選べるようにしている。&lt;code&gt;Details&lt;/code&gt; の &lt;code&gt;Cutoff strongly&lt;/code&gt; がそれで、オフのとき1.を、オンのとき2.を、それぞれ選ぶようになっている。 元絵に近いのが出るのはオフのとき。デフォルトもこちらにしてある。&lt;/p&gt;</summary>
  </entry>
</feed>