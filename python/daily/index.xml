<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-07T01:36:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jzhang38/TinyLlama</title>
    <updated>2023-09-07T01:36:12Z</updated>
    <id>tag:github.com,2023-09-07:/jzhang38/TinyLlama</id>
    <link href="https://github.com/jzhang38/TinyLlama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The TinyLlama project is an open endeavor to pretrain a 1.1B Llama model on 3 trillion tokens.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;TinyLlama-1.1B&lt;/h1&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/jzhang38/TinyLlama/main/README_zh-CN.md&#34;&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The TinyLlama project aims to &lt;strong&gt;pretrain&lt;/strong&gt; a &lt;strong&gt;1.1B Llama model on 3 trillion tokens&lt;/strong&gt;. With some proper optimization, we can achieve this within a span of &#34;just&#34; 90 days using 16 A100-40G GPUs ðŸš€ðŸš€. The training has started on 2023-09-01.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/jzhang38/TinyLlama/main/.github/TinyLlama_logo.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.&lt;/p&gt; &#xA;&lt;h4&gt;Releases Schedule&lt;/h4&gt; &#xA;&lt;p&gt;We will be rolling out intermediate checkpoints following the below schedule. We also include some baseline models for comparison.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;HF Checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;Tokens&lt;/th&gt; &#xA;   &lt;th&gt;Step&lt;/th&gt; &#xA;   &lt;th&gt;HellaSwag Acc_norm&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baseline&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-base-alpha-3b&#34;&gt;StableLM-Alpha-3B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;800B&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;38.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baseline&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1b/tree/step50000&#34;&gt;Pythia-1B-intermediate-step-50k-105b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;105B&lt;/td&gt; &#xA;   &lt;td&gt;50k&lt;/td&gt; &#xA;   &lt;td&gt;42.04&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baseline&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1b&#34;&gt;Pythia-1B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;300B&lt;/td&gt; &#xA;   &lt;td&gt;143k&lt;/td&gt; &#xA;   &lt;td&gt;47.16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023-09-04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b&#34;&gt;TinyLlama-1.1B-intermediate-step-50k-105b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;105B&lt;/td&gt; &#xA;   &lt;td&gt;50k&lt;/td&gt; &#xA;   &lt;td&gt;43.50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023-09-16&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;500B&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023-10-01&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;1T&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023-10-16&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;1.5T&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023-10-31&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;2T&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023-11-15&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;2.5T&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2023-12-01&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;3T&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- | Baseline   | [Pythia-1B-intermediate-52b](https://huggingface.co/EleutherAI/pythia-1b/tree/step25000)             | 52B   | 25k   |  38.81            | --&gt; &#xA;&lt;!-- | Baseline   | [Pythia-1.4B-intermediate-52b](https://huggingface.co/EleutherAI/pythia-1.4b/tree/step25000)             | 52B   | 25k   |  42.49            | --&gt; &#xA;&lt;!-- | Baseline   | [Pythia-1.4B-intermediate-105b](https://huggingface.co/EleutherAI/pythia-1.4b/tree/step50000)             | 105B   | 50k   |  46.14            | --&gt; &#xA;&lt;!-- | 2023-09-04 | [TinyLlama-1.1B-intermediate-52b](https://huggingface.co/PY007/TinyLlama-1.1B-52b)   | 52B    | 25k  |  40.85            |&#xA;| 2023-09-04 | [TinyLlama-1.1B-intermediate-84b](https://huggingface.co/PY007/TinyLlama-1.1B-84b)   | 84B    | 40k  |  42.65            |  --&gt; &#xA;&lt;p&gt;It can be observed that TinyLlama has so far progressed well ðŸŽ‰ðŸŽ‰.&lt;/p&gt; &#xA;&lt;p&gt;Meanwhile, you can track the live cross entropy loss &lt;a href=&#34;https://wandb.ai/lance777/lightning_logs/reports/metric-train_loss-23-09-04-23-38-15---Vmlldzo1MzA4MzIw?accessToken=5eu2sndit2mo6eqls8h38sklcgfwt660ek1f2czlgtqjv2c6tida47qm1oty8ik9&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Potential Usecase&lt;/h2&gt; &#xA;&lt;p&gt;Tiny but strong language models are useful for many applications. Here are some potential usecases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Assisting speculative decoding of larger models. (See this &lt;a href=&#34;https://twitter.com/karpathy/status/1697318534555336961&#34;&gt;tutorial&lt;/a&gt; by Andrej Karpathy)&lt;/li&gt; &#xA; &lt;li&gt;Deployment on edge devices with restricted memory and computational capacities, for functionalities like real-time machine translation without an internet connection (the 4bit-quantized TinyLlama-1.1B&#39;s weight only takes up 550MB RAM).&lt;/li&gt; &#xA; &lt;li&gt;Enabling real-time dialogue generation in video games.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Moreover, our code can be a &lt;strong&gt;reference for enthusiasts keen on pretraining language models under 5 billion parameters&lt;/strong&gt; without diving too early into &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training Details&lt;/h2&gt; &#xA;&lt;p&gt;Below are some details of our training setup:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Setting&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parameters&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Attention Variant&lt;/td&gt; &#xA;   &lt;td&gt;Grouped Query Attention&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Model Size&lt;/td&gt; &#xA;   &lt;td&gt;Layers: 22, Heads: 32, Query Groups: 4, Embedding Size: 2048, Intermediate Size (Swiglu): 5632&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sequence Length&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Batch Size&lt;/td&gt; &#xA;   &lt;td&gt;2 million tokens (2048 * 1024)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Learning Rate&lt;/td&gt; &#xA;   &lt;td&gt;4e-4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Learning Rate Schedule&lt;/td&gt; &#xA;   &lt;td&gt;Cosine with 2000 warmup steps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training Data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/cerebras/slimpajama-627b&#34;&gt;Slimpajama&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/datasets/bigcode/starcoderdata&#34;&gt;Starcoderdata&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Data Preprocessing&lt;/td&gt; &#xA;   &lt;td&gt;Excluded GitHub subset of Slimpajama; Sampled all code from Starcoderdata&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Combined Dataset Size&lt;/td&gt; &#xA;   &lt;td&gt;Around 950B tokens&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Total Tokens During Training&lt;/td&gt; &#xA;   &lt;td&gt;3 trillion (slightly more than 3 epochs/1430k steps)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Natural Language to Code Ratio&lt;/td&gt; &#xA;   &lt;td&gt;7:3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hardware&lt;/td&gt; &#xA;   &lt;td&gt;16 A100-40G GPUs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Blazingly Fast&lt;/h2&gt; &#xA;&lt;p&gt;Our codebase supports the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;multi-gpu and multi-node distributed training with FSDP.&lt;/li&gt; &#xA; &lt;li&gt;flash attention 2.&lt;/li&gt; &#xA; &lt;li&gt;fused layernorm.&lt;/li&gt; &#xA; &lt;li&gt;fused swiglu.&lt;/li&gt; &#xA; &lt;li&gt;fused cross entropy loss .&lt;/li&gt; &#xA; &lt;li&gt;fused rotary positional embedding.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to those optimizations, we achieve a throughput of &lt;strong&gt;24k&lt;/strong&gt; tokens per second per A100-40G GPU, which translates to &lt;strong&gt;56% model flops utilization&lt;/strong&gt; without activation checkpointing (We expect the MFU to be even higher on A100-80G). It means you can train a chinchilla-optimal TinyLlama (1.1B param, 22B tokens) in &lt;strong&gt;32 hours with 8 A100&lt;/strong&gt;. Those optimizations also greatly reduce the memory footprint, allowing us to stuff our 1.1B model into 40GB GPU RAM and train with a per-gpu batch size of 16k tokens. &lt;strong&gt;You can also pretrain TinyLlama on 3090/4090 GPUs with a smaller per-gpu batch size&lt;/strong&gt;. Below is a comparison of the training speed of our codebase with that of Pythia and MPT.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;A100 GPU hours taken on 300B tokens&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TinyLlama-1.1B&lt;/td&gt; &#xA;   &lt;td&gt;3456&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-1b&#34;&gt;Pythia-1.0B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4830&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-1b-redpajama-200b&#34;&gt;MPT-1.3B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7920&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;small&gt; The Pythia number comes from their &lt;a href=&#34;https://arxiv.org/abs/2304.01373&#34;&gt;paper&lt;/a&gt;. The MPT number comes from &lt;a href=&#34;https://huggingface.co/mosaicml/mpt-1b-redpajama-200b&#34;&gt;here&lt;/a&gt;, in which they say MPT-1.3B &#34; was trained on 440 A100-40GBs for about half a day&#34; on 200B tokens. &lt;/small&gt;&lt;/p&gt; &#xA;&lt;p&gt;The fact that TinyLlama is a relatively small model with grouped query attention means it is also fast during inference. Below are some throughputs that we measure:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Device&lt;/th&gt; &#xA;   &lt;th&gt;Settings&lt;/th&gt; &#xA;   &lt;th&gt;Throughput (tokens/sec)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;Llama.cpp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mac M2 16GB RAM&lt;/td&gt; &#xA;   &lt;td&gt;batch_size=1; 4-bit inference&lt;/td&gt; &#xA;   &lt;td&gt;71.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A40 GPU&lt;/td&gt; &#xA;   &lt;td&gt;batch_size=100, n=10&lt;/td&gt; &#xA;   &lt;td&gt;7094.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/jzhang38/TinyLlama/main/PRETRAIN.md&#34;&gt;PRETRAIN.md&lt;/a&gt; for instructions on how to pretrain TinyLlama.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;p&gt;This project is still under active development. We are a really small team. Community feedback and contributions are highly appreciated. Here are some things we plan to work on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add scripts for pretraining on other datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Sequence length extrapolation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Test out speculative decoding for Llama-2-7B.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Test the throughput on RTX 3090/4090.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add fine-tuning scripts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Properly evaluate the model on downstream tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; A demo running on mobile phones.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Explore retrieval-augmentation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This repository is built upon &lt;a href=&#34;https://github.com/Lightning-AI/lit-gpt&#34;&gt;lit-gpt&lt;/a&gt; and &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;flash-attention&lt;/a&gt;. Be sure to explore this fantastic open-source project if it&#39;s new to you!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@online{lit-gpt,&#xA;  author    = {Lightning AI},&#xA;  title     = {Lit-GPT},&#xA;  url       = {https://github.com/Lightning-AI/lit-gpt},&#xA;  year      = {2023},&#xA;}&#xA;@article{dao2023flashattention2,&#xA;  title     ={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},&#xA;  author    ={Dao, Tri},&#xA;  year      ={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;This project is currently contributed by &lt;a href=&#34;https://github.com/jzhang38&#34;&gt;Peiyuan Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/ChaosCodes&#34;&gt;Guangtao Zeng&lt;/a&gt;, &lt;a href=&#34;https://github.com/TianduoWang&#34;&gt;Tianduo Wang&lt;/a&gt; and &lt;a href=&#34;https://istd.sutd.edu.sg/people/faculty/lu-wei/&#34;&gt;Wei Lu&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you find our work valuable, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@online{tinyllama,&#xA;  author    = {Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu},&#xA;  title     = {TinyLlama},&#xA;  url       = {https://github.com/jzhang38/TinyLlama},&#xA;  year      = {2023},&#xA;  month     = {Sep},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;h4&gt;1. Why would pretraining a 1.1B model for so long make sense? Doesn&#39;t it contradict the Chinchilla Scaling Law?&lt;/h4&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/jzhang38/TinyLlama/main/.github/llama2-training.png&#34; alt=&#34;The training loss curve of Llama 2&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;Above is the training loss curve taken from the Llama 2 paper. Here I quote from that paper: &#34;We observe that after pretraining on 2T Tokens, the models still did not show any sign of saturation&#34;. That is why we believe pretraining a 1.1B model for 3T tokens is a reasonable thing to do. Even if the loss curve does not go down eventually, we can still study the phenomenon of saturation and learn something from it.&lt;/p&gt; &#xA;&lt;h4&gt;2. What does &#34;saturation&#34; mean?&lt;/h4&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/jzhang38/TinyLlama/main/.github/Pythia_saturation.png&#34; alt=&#34;Figure 10 of the Pythia paper&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;The figure from the Pythia paper displays the LAMBADA accuracy plotted against the total training tokens (300B). The term &#34;saturation&#34; pertains specifically to the 70M and 160M models. Notably, even the 410M model does not saturate with 300B tokens, as it continues to show an increasing trend, similar to the trend of larger models.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#jzhang38/TinyLlama&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=jzhang38/TinyLlama&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aigc-apps/sd-webui-EasyPhoto</title>
    <updated>2023-09-07T01:36:12Z</updated>
    <id>tag:github.com,2023-09-07:/aigc-apps/sd-webui-EasyPhoto</id>
    <link href="https://github.com/aigc-apps/sd-webui-EasyPhoto" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ðŸ“· EasyPhoto | Your Smart AI Photo Generator.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸ“· EasyPhoto | Your Smart AI Photo Generator.&lt;/h1&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/aigc-apps/sd-webui-EasyPhoto/main/README_zh-CN.md&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;EasyPhoto is a Webui UI plugin for generating AI portraits that can be used to train digital doppelgangers relevant to you. Training is recommended to be done with 5 to 20 portrait images, preferably half-body photos and do not wear glasses (It doesn&#39;t matter if the characters in a few pictures wear glasses). After the training is done, we can generate it in the Inference section. We support using preset template images or uploading your own images for Inference.&lt;/p&gt; &#xA;&lt;p&gt;These are our generated results: &lt;img src=&#34;https://raw.githubusercontent.com/aigc-apps/sd-webui-EasyPhoto/main/images/results_1.jpg&#34; alt=&#34;results_1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/aigc-apps/sd-webui-EasyPhoto/main/images/results_2.jpg&#34; alt=&#34;results_2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our ui interface is as follows:&lt;br&gt; &lt;strong&gt;train part:&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/aigc-apps/sd-webui-EasyPhoto/main/images/train_ui.jpg&#34; alt=&#34;train_ui&#34;&gt; &lt;strong&gt;inference part:&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/aigc-apps/sd-webui-EasyPhoto/main/images/infer_ui.jpg&#34; alt=&#34;infer_ui&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What&#39;s New&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create Code! Support for Windows and Linux Now. [ðŸ”¥ 2023.09.02]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;TODO List&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support chinese ui.&lt;/li&gt; &#xA; &lt;li&gt;Support change in template&#39;s background.&lt;/li&gt; &#xA; &lt;li&gt;Support high resolution.&lt;/li&gt; &#xA; &lt;li&gt;Support multi-person templates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h3&gt;1. Environment Check&lt;/h3&gt; &#xA;&lt;p&gt;We have verified EasyPhoto execution on the following environment:&lt;/p&gt; &#xA;&lt;p&gt;The detailed of Windows 10:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OS: Windows10&lt;/li&gt; &#xA; &lt;li&gt;python: py3.10&lt;/li&gt; &#xA; &lt;li&gt;pytorch: torch2.0.1&lt;/li&gt; &#xA; &lt;li&gt;tensorflow-cpu: 2.13.0&lt;/li&gt; &#xA; &lt;li&gt;CUDA: 11.7&lt;/li&gt; &#xA; &lt;li&gt;CUDNN: 8+&lt;/li&gt; &#xA; &lt;li&gt;GPU: Nvidia-3060 12G&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The detailed of Linux:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OS: Ubuntu 20.04, CentOS&lt;/li&gt; &#xA; &lt;li&gt;python: py3.10 &amp;amp; py3.11&lt;/li&gt; &#xA; &lt;li&gt;pytorch: torch2.0.1&lt;/li&gt; &#xA; &lt;li&gt;tensorflow-cpu: 2.13.0&lt;/li&gt; &#xA; &lt;li&gt;CUDA: 11.7&lt;/li&gt; &#xA; &lt;li&gt;CUDNN: 8+&lt;/li&gt; &#xA; &lt;li&gt;GPU: Nvidia-A10 24G &amp;amp; Nvidia-V100 16G &amp;amp; Nvidia-A100 40G&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We need about 60GB available on disk (for saving weights and datasets process), please check!&lt;/p&gt; &#xA;&lt;h3&gt;2. Relevant Repositories &amp;amp; Weights Downloading&lt;/h3&gt; &#xA;&lt;h4&gt;a. Controlnet&lt;/h4&gt; &#xA;&lt;p&gt;We need to use Controlnet for inference. The related repo is &lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet&#34;&gt;Mikubill/sd-webui-controlnet&lt;/a&gt;. You need install this repo before using EasyPhoto.&lt;/p&gt; &#xA;&lt;p&gt;In addition, we need at least three Controlnets for inference. So you need to set the &lt;strong&gt;Multi ControlNet: Max models amount (requires restart)&lt;/strong&gt; in Setting. &lt;img src=&#34;https://raw.githubusercontent.com/aigc-apps/sd-webui-EasyPhoto/main/images/controlnet_num.png&#34; alt=&#34;controlnet_num&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;b. Other Dependencies.&lt;/h4&gt; &#xA;&lt;p&gt;We are mutually compatible with the existing stable-diffusion-webui environment, and the relevant repositories are installed when starting stable-diffusion-webui.&lt;/p&gt; &#xA;&lt;p&gt;The weights we need will be downloaded automatically when you start training first time.&lt;/p&gt; &#xA;&lt;h3&gt;3. Plug-in Installation&lt;/h3&gt; &#xA;&lt;p&gt;Now we support installing EasyPhoto from git. The url of our Repository is &lt;a href=&#34;https://github.com/aigc-apps/sd-webui-EasyPhoto&#34;&gt;https://github.com/aigc-apps/sd-webui-EasyPhoto&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We will support installing EasyPhoto from &lt;strong&gt;Available&lt;/strong&gt; in the future.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aigc-apps/sd-webui-EasyPhoto/main/images/install.jpg&#34; alt=&#34;install&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Algorithm Detailed&lt;/h1&gt; &#xA;&lt;h3&gt;1.Architectural Overview&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aigc-apps/sd-webui-EasyPhoto/main/images/overview.jpg&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the field of AI portraits, we expect model-generated images to be realistic and resemble the user, and traditional approaches introduce unrealistic lighting (such as face fusion or roop). To address this unrealism, we introduce the image-to-image capability of the stable diffusion model. Generating a perfect personal portrait takes into account the desired generation scenario and the user&#39;s digital doppelgÃ¤nger. We use a pre-prepared template as the desired generation scene and an online trained face LoRA model as the user&#39;s digital doppelganger, which is a popular stable diffusion fine-tuning model. We use a small number of user images to train a stable digital doppelgÃ¤nger of the user, and generate a personal portrait image based on the face LoRA model and the expected generative scene during inference.&lt;/p&gt; &#xA;&lt;h3&gt;2.Training Detailed&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aigc-apps/sd-webui-EasyPhoto/main/images/train_detail.jpg&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;First, we perform face detection on the input user image, and after determining the face location, we intercept the input image according to a certain ratio. Then, we use the saliency detection model and the skin beautification model to obtain a clean face training image, which basically consists of only faces. Then, we label each image with a fixed label. There is no need to use a labeler here, and the results are good. Finally, we fine-tune the stabilizing diffusion model to get the user&#39;s digital doppelganger.&lt;/p&gt; &#xA;&lt;p&gt;During training, we utilize the template image for verification in real time, and at the end of training, we calculate the face id gap between the verification image and the user&#39;s image to achieve Lora fusion, which ensures that our Lora is a perfect digital doppelganger of the user.&lt;/p&gt; &#xA;&lt;p&gt;In addition, we will choose the image that is most similar to the user in the validation as the face_id image, which will be used in Inference.&lt;/p&gt; &#xA;&lt;h3&gt;3.Inference Detailed&lt;/h3&gt; &#xA;&lt;h4&gt;a.First Diffusion:&lt;/h4&gt; &#xA;&lt;p&gt;First, we will perform face detection on our incoming template image to determine the mask that needs to be inpainted for stable diffusion. then we will use the template image to perform face fusion with the optimal user image. After the face fusion is completed, we use the above mask to inpaint (fusion_image) with the face fused image. In addition, we will affix the optimal face_id image obtained during training to the template image by affine transformation (replaced_image). Then we will apply Controlnets on it, we use canny with color to extract features for fusion_image and openpose for replaced_image to ensure the similarity and stability of the images. Then we will use Stable Diffusion combined with the user&#39;s digital split for generation.&lt;/p&gt; &#xA;&lt;h4&gt;b.Second Diffusion:&lt;/h4&gt; &#xA;&lt;p&gt;After getting the result of First Diffusion, we will fuse the result with the optimal user image for face fusion, and then we will use Stable Diffusion again with the user&#39;s digital doppelganger for generation. The second generation will use higher resolution.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;insightfaceï¼š&lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;https://github.com/deepinsight/insightface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cv_resnet50_faceï¼š&lt;a href=&#34;https://www.modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface/summary&#34;&gt;https://www.modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface/summary&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cv_u2net_salientï¼š&lt;a href=&#34;https://www.modelscope.cn/models/damo/cv_u2net_salient-detection/summary&#34;&gt;https://www.modelscope.cn/models/damo/cv_u2net_salient-detection/summary&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cv_unet_skin_retouching_torchï¼š&lt;a href=&#34;https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch/summary&#34;&gt;https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch/summary&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cv_unet-image-face-fusionï¼š&lt;a href=&#34;https://www.modelscope.cn/models/damo/cv_unet-image-face-fusion_damo/summary&#34;&gt;https://www.modelscope.cn/models/damo/cv_unet-image-face-fusion_damo/summary&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;kohyaï¼š&lt;a href=&#34;https://github.com/bmaltais/kohya_ss&#34;&gt;https://github.com/bmaltais/kohya_ss&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;controlnet-webuiï¼š&lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet&#34;&gt;https://github.com/Mikubill/sd-webui-controlnet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/LICENSE&#34;&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>omerbt/TokenFlow</title>
    <updated>2023-09-07T01:36:12Z</updated>
    <id>tag:github.com,2023-09-07:/omerbt/TokenFlow</id>
    <link href="https://github.com/omerbt/TokenFlow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Pytorch Implementation for &#34;TokenFlow: Consistent Diffusion Features for Consistent Video Editing&#34; presenting &#34;TokenFlow&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TokenFlow: Consistent Diffusion Features for Consistent Video Editing&lt;/h1&gt; &#xA;&lt;h2&gt;[&lt;a href=&#34;https://diffusion-tokenflow.github.io/&#34; target=&#34;_blank&#34;&gt;Project Page&lt;/a&gt;]&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.10373&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-TokenFlow-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/weizmannscience/tokenflow&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/PyTorch-%3E=1.10.0-Red?logo=pytorch&#34; alt=&#34;Pytorch&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/omerbt/TokenFlow/assets/52277000/93dccd63-7e9a-4540-a941-31962361b0bb&#34;&gt;https://github.com/omerbt/TokenFlow/assets/52277000/93dccd63-7e9a-4540-a941-31962361b0bb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TokenFlow&lt;/strong&gt; is a framework that enables consistent video editing, using a pre-trained text-to-image diffusion model, without any further training or finetuning.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The generative AI revolution has been recently expanded to videos. Nevertheless, current state-of-the-art video mod- els are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial lay- out and dynamics of the input video. Our method is based on our key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in con- junction with any off-the-shelf text-to-image editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more see the &lt;a href=&#34;https://diffusion-tokenflow.github.io&#34;&gt;project webpage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Sample results&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/omerbt/TokenFlow/master/assets/videos.gif&#34;&gt; &#xA;&lt;h2&gt;Environment&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n tokenflow python=3.9&#xA;conda activate tokenflow&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Preprocess&lt;/h2&gt; &#xA;&lt;p&gt;Preprocess you video by running using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python preprocess.py --data_path &amp;lt;data/myvideo.mp4&amp;gt; \&#xA;                     --inversion_prompt &amp;lt;&#39;&#39; or a string describing the video content&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;                     --save_dir &amp;lt;latents&amp;gt;&#xA;                     --H &amp;lt;video height&amp;gt;&#xA;                     --W &amp;lt;video width&amp;gt;&#xA;                     --sd_version &amp;lt;Stable-Diffusion version&amp;gt;&#xA;                     --steps &amp;lt;number of inversion steps&amp;gt;&#xA;                     --save_steps &amp;lt;number of sampling steps that will be used later for editing&amp;gt;&#xA;                     --n_frames &amp;lt;number of frames&amp;gt;&#xA;                     &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;more information on the arguments can be found here.&lt;/p&gt; &#xA;&lt;h3&gt;Note:&lt;/h3&gt; &#xA;&lt;p&gt;The video reconstruction will be saved as inverted.mp4. A good reconstruction is required for successfull editing with our method.&lt;/p&gt; &#xA;&lt;h2&gt;Editing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TokenFlow is designed for video for structure-preserving edits.&lt;/li&gt; &#xA; &lt;li&gt;Our method is built on top of an image editing technique (e.g., Plug-and-Play, ControlNet, etc.) - therefore, it is important to ensure that the edit works with the chosen base technique.&lt;/li&gt; &#xA; &lt;li&gt;The LDM decoder may introduce some jitterness, depending on the original video.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To edit your video, first create a yaml config as in &lt;code&gt;configs/config_pnp.yaml&lt;/code&gt;. Then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_tokenflow_pnp.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly, if you want to use ControlNet or SDEedit, create a yaml config as in &lt;code&gt;config/config_controlnet.yaml&lt;/code&gt; or &lt;code&gt;configs/config_SDEdit.yaml&lt;/code&gt; and run &lt;code&gt;python run_tokenflow_controlnet.py&lt;/code&gt; or &lt;code&gt;python run_tokenflow_SDEdit.py&lt;/code&gt; respectivly.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{tokenflow2023,&#xA;        title = {TokenFlow: Consistent Diffusion Features for Consistent Video Editing},&#xA;        author = {Geyer, Michal and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},&#xA;        journal={arXiv preprint arxiv:2307.10373},&#xA;        year={2023}&#xA;        }&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>