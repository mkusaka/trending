<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-07T01:25:32Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PrometheusStealer/Prometheus</title>
    <updated>2024-03-07T01:25:32Z</updated>
    <id>tag:github.com,2024-03-07:/PrometheusStealer/Prometheus</id>
    <link href="https://github.com/PrometheusStealer/Prometheus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Very powerful stealer + miner + rat + keylogger + clipper&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>hpcaitech/Open-Sora</title>
    <updated>2024-03-07T01:25:32Z</updated>
    <id>tag:github.com,2024-03-07:/hpcaitech/Open-Sora</id>
    <link href="https://github.com/hpcaitech/Open-Sora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Building your own video generation model like OpenAI&#39;s Sora&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üé• Open-Sora&lt;/h1&gt; &#xA;&lt;div id=&#34;top&#34; align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hpcaitech/public_assets/tree/main/colossalai/contact/slack&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp;amp;amp&#34; alt=&#34;slack badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1-%E5%8A%A0%E5%85%A5-green?logo=wechat&amp;amp;amp&#34; alt=&#34;WeChat badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üìé Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#-open-sora&#34;&gt;üé• Open-Sora&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#-table-of-contents&#34;&gt;üìé Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#-overview&#34;&gt;üìç Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#-dataset-preparation&#34;&gt;üìÇ Dataset Preparation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#use-msr-vtt&#34;&gt;Use MSR-VTT&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#use-customized-datasets&#34;&gt;Use Customized Datasets&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#-get-started&#34;&gt;üöÄ Get Started&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#-acknowledgement&#34;&gt;ü™Ñ Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/03] &lt;a href=&#34;https://hpc-ai.com/blog/open-sora&#34;&gt;Open-SoraÔºöSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìç Overview&lt;/h2&gt; &#xA;&lt;p&gt;Open-Sora is an open-source project that provides a high-performance implementation of the development pipeline that Sora might use powered by &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;Colossal-AI&lt;/a&gt;, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provides &lt;strong&gt;a complete Sora reproduction architecture solution&lt;/strong&gt;, including the whole process from data processing to training and deployment.&lt;/li&gt; &#xA; &lt;li&gt;Supports &lt;strong&gt;dynamic resolution&lt;/strong&gt;, training can directly train any resolution of the video, without scaling.&lt;/li&gt; &#xA; &lt;li&gt;Supports &lt;strong&gt;multiple model structures&lt;/strong&gt;. Since the actual model structure of Sora is unknown, we implement three common multimodal model structures such as adaLN-zero, cross attention, and in-context conditioning (token concat).&lt;/li&gt; &#xA; &lt;li&gt;Supports &lt;strong&gt;multiple video compression methods&lt;/strong&gt;. Users can choose to use original video, VQVAE (video native model), or SD-VAE (image native model) for training.&lt;/li&gt; &#xA; &lt;li&gt;Supports &lt;strong&gt;multiple parallel training optimizations&lt;/strong&gt;. Including the AI large model system optimization capability compatible with Colossal-AI, and hybrid sequence parallelism with Ulysses and FastSeq.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p id=&#34;diffusion_demo&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-1.png&#34; width=&#34;800/&#34;&gt; &lt;/p&gt; &#xA;&lt;p id=&#34;diffusion_demo&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-2.png&#34; width=&#34;800/&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üìÇ Dataset Preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Use MSR-VTT&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://cove.thecvf.com/datasets/839&#34;&gt;MSR-VTT&lt;/a&gt; dataset, which is a large-scale video description dataset. Users should preprocess the raw videos before training the model. You can use the following scripts to perform data processing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Step 1: download the dataset to ./dataset/MSRVTT&#xA;bash scripts/data/download_msr_vtt_dataset.sh&#xA;&#xA;# Step 2: collate the video and annotations&#xA;python scripts/data/collate_msr_vtt_dataset.py -d ./dataset/MSRVTT/ -o ./dataset/MSRVTT-collated&#xA;&#xA;# Step 3: perform data processing&#xA;# NOTE: each script could several minutes so we apply the script to the dataset split individually&#xA;python scripts/data/preprocess_data.py -c ./dataset/MSRVTT-collated/train/annotations.json -v ./dataset/MSRVTT-collated/train/videos -o ./dataset/MSRVTT-processed/train&#xA;python scripts/data/preprocess_data.py -c ./dataset/MSRVTT-collated/val/annotations.json -v ./dataset/MSRVTT-collated/val/videos -o ./dataset/MSRVTT-processed/val&#xA;python scripts/data/preprocess_data.py -c ./dataset/MSRVTT-collated/test/annotations.json -v ./dataset/MSRVTT-collated/test/videos -o ./dataset/MSRVTT-processed/test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After completing these steps, you should have a processed MSR-VTT dataset in &lt;code&gt;./dataset/MSRVTT-processed&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Use Customized Datasets&lt;/h3&gt; &#xA;&lt;p&gt;You can also use other datasets and transform the dataset to the required format. You should prepare a captions file and a video directory. The captions file should be a JSON file or a JSONL file. The video directory should contain all the videos.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of the captions file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#xA;        &#34;file&#34;: &#34;video0.mp4&#34;,&#xA;        &#34;captions&#34;: [&#34;a girl is throwing away folded clothes&#34;, &#34;a girl throwing cloths around&#34;]&#xA;    },&#xA;    {&#xA;        &#34;file&#34;: &#34;video1.mp4&#34;,&#xA;        &#34;captions&#34;: [&#34;a  comparison of two opposing team football athletes&#34;]&#xA;    }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is an example of the video directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;‚îú‚îÄ‚îÄ video0.mp4&#xA;‚îú‚îÄ‚îÄ video1.mp4&#xA;‚îî‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each video may have multiple captions. So the outputs are video-caption pairs. E.g., If the first video has two captions, then the output will be two video-caption pairs.&lt;/p&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/wilson1yan/VideoGPT/&#34;&gt;VQ-VAE&lt;/a&gt; to quantize the video frames. And we use &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/clip#clip&#34;&gt;CLIP&lt;/a&gt; to extract the text features.&lt;/p&gt; &#xA;&lt;p&gt;The output is an arrow dataset, which contains the following columns: &#34;video_file&#34;, &#34;video_latent_states&#34;, &#34;text_latent_states&#34;. The dimension of &#34;video_latent_states&#34; is (T, H, W), and the dimension of &#34;text_latent_states&#34; is (S, D).&lt;/p&gt; &#xA;&lt;p&gt;Then you can run the data processing script with the command below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python preprocess_data.py -c /path/to/captions.json -v /path/to/video_dir -o /path/to/output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that this script needs to be run on a machine with a GPU. To avoid CUDA OOM, we filter out the videos that are too long.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Get Started&lt;/h2&gt; &#xA;&lt;p&gt;In this section, we will guide how to run training and inference. Before that, make sure you installed the dependencies with the command below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;You can invoke the training via the command below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also modify the arguments in &lt;code&gt;train.sh&lt;/code&gt; for your own need.&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve provided a script to perform inference, allowing you to generate videos from the trained model. You can invoke the inference via the command below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sample.py -m &#34;DiT/XL-2&#34; --text &#34;a person is walking on the street&#34; --ckpt /path/to/checkpoint --height 256 --width 256 --fps 10 --sec 5 --disable-cfg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate a &#34;sample.mp4&#34; file in the current directory.&lt;/p&gt; &#xA;&lt;p&gt;For more command line options, you can use the following command to check the help message.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sample.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü™Ñ Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;During the development of the project, we learned a lot from the following information:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/research/video-generation-models-as-world-simulators&#34;&gt;OpenAI Sora Technical Report&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wilson1yan/VideoGPT&#34;&gt;VideoGPT Project&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;Diffusion Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.14509&#34;&gt;Deepspeed Ulysses&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/OpenDiT&#34;&gt;OpenDiT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Ryujinx/release-channel-master</title>
    <updated>2024-03-07T01:25:32Z</updated>
    <id>tag:github.com,2024-03-07:/Ryujinx/release-channel-master</id>
    <link href="https://github.com/Ryujinx/release-channel-master" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ryujinx&#39;s Release channel: master&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;What is this?&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains the &lt;a href=&#34;https://github.com/Ryujinx/release-channel-master/releases&#34;&gt;releases&lt;/a&gt; of the master build channel of &lt;a href=&#34;https://github.com/Ryujinx/Ryujinx&#34;&gt;Ryujinx&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>