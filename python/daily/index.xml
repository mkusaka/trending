<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-09T01:43:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>acikkaynak/deprem-yardim-backend</title>
    <updated>2023-02-09T01:43:07Z</updated>
    <id>tag:github.com,2023-02-09:/acikkaynak/deprem-yardim-backend</id>
    <link href="https://github.com/acikkaynak/deprem-yardim-backend" rel="alternate"></link>
    <summary type="html">&lt;p&gt;afetharita.com backend projesi&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Afet Harita Backend&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acikkaynak/deprem-yardim-backend/main/README.en.md&#34;&gt;ENGLISH VERSION&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://afetharita.com&#34;&gt;https://afetharita.com&lt;/a&gt; için back-end projesi. &lt;a href=&#34;https://api.afetharita.com&#34;&gt;https://api.afetharita.com&lt;/a&gt; adresinden erişilebilir.&lt;/p&gt; &#xA;&lt;p&gt;Diğer projeler: &lt;a href=&#34;https://github.com/acikkaynak/deprem-yardim-projesi&#34;&gt;https://github.com/acikkaynak/deprem-yardim-projesi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Mimari&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/acikkaynak/deprem-yardim-backend/main/docs/afetharita-backend.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Kullanılan teknolojiler&lt;/h2&gt; &#xA;&lt;p&gt;Python (Django), Postgres (PostgreSQL), Redis, AWS (Elastic Load Balancer, ECS, AWS Fargate), OpenAI (Görsellerin metine çevirilmesi)&lt;/p&gt; &#xA;&lt;h1&gt;Projeyi çalıştırmak&lt;/h1&gt; &#xA;&lt;h2&gt;Bağımlılıklar:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.docker.com&#34;&gt;Docker&lt;/a&gt; (opsiyonel): Geliştirmek için şart değilse de gereksinimleri yüklemeyi ve geliştirme yapmayı kolaylaştıracaktır. Canlıda proje docker üzerinde AWS ECS&#39;lerde çalışmaktadır.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.postgresql.org&#34;&gt;PostgreSql&lt;/a&gt;: Veritabanı olarak kullanılmaktadır. Adresten doğrudan bilgisayarınıza indirebilir ya da &lt;a href=&#34;https://hub.docker.com/_/postgres&#34;&gt;docker imajını&lt;/a&gt; kullanabilirsiniz.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://redis.io&#34;&gt;Redis&lt;/a&gt;: Asenkron Celery görevleri için kuyruk ve cache olarak kullanılmaktadır. Doğrudan bilgisayarınıza indirebilir (Linux için) ya da &lt;a href=&#34;https://hub.docker.com/_/redis&#34;&gt;docker imajını&lt;/a&gt; kullanabilirsiniz. Windows&#39;ta son versiyon doğrudan çalışmadığı için WSL ile docker&#39;da çalıştırmak en iyi seçenek.&lt;/p&gt; &#xA;&lt;h2&gt;Geliştirme ortamının hazırlanması&lt;/h2&gt; &#xA;&lt;p&gt;Docker yükledikten sonra tüm projeyi docker-compose ile çalıştırmak için&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker-compose up --build -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Geliştirme için sadece postgres ve redisi ayağa kaldırmak için:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker-compose up -d postgres redis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;p&gt;Python bağımlılık yönetimi poetry ile sağlanmaktadır.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install poetry&#xA;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ile gerekli paketleri yükleyebilirsiniz. Poetry kendi ortamını oluşturup paketleri oraya yükleyecektir.&lt;/p&gt; &#xA;&lt;p&gt;Daha sonra ortam değişkenlerini ayarlayın. .env.template dosyasını .env adıyla kopyalayıp gerekli ayarları yapın. Compose&#39;dan gelen örnek ayarlarla aşağıdaki gibi olacaktır:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;DJANGO_SECRET_KEY= # django için secret-key&#xA;POSTGRES_PASSWORD=debug&#xA;POSTGRES_USER=debug&#xA;POSTGRES_DB=debug&#xA;POSTGRES_HOST=trquake-database&#xA;POSTGRES_PORT=5432&#xA;CELERY_BROKER_URL=trquake-redis&#xA;ZEKAI_USERNAME= # zekai.co kullanıcı adı&#xA;ZEKAI_PASSWORD= # zekai.co şifre&#xA;DEFAULT_ADMIN_PASSWORD= # ilk oluşturulan admin kullanıcısı için şifre&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Django Secret key oluşturmak için:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python&#xA;&amp;gt;&amp;gt;&amp;gt; from django.core.management.utils import get_random_secret_key&#xA;&amp;gt;&amp;gt;&amp;gt; print(get_random_secret_key())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Projeyi development modunda açmak için:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;django-admin migrate&#xA;django-admin createsuperuser&#xA;django-admin collectstatic --no-input&#xA;django-admin runserver&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Celery için geliştirilen taskları çalıştırmak için:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;celery -A trquake.celery.app worker -B -l DEBUG&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>BlinkDL/RWKV-LM</title>
    <updated>2023-02-09T01:43:07Z</updated>
    <id>tag:github.com,2023-02-09:/BlinkDL/RWKV-LM</id>
    <link href="https://github.com/BlinkDL/RWKV-LM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RWKV is a RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it&#39;s combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, &#34;infinite&#34; ctx_len, and free sentence embedding.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The RWKV Language Model (and my LM tricks)&lt;/h1&gt; &#xA;&lt;h2&gt;RWKV: RNN with Transformer-level LLM Performance&lt;/h2&gt; &#xA;&lt;p&gt;RWKV is a RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable). And it&#39;s 100% attention-free. You only need the hidden state at position t to compute the state at position t+1. You can use the &#34;GPT&#34; mode to quickly compute the hidden state for the &#34;RNN&#34; mode.&lt;/p&gt; &#xA;&lt;p&gt;So it&#39;s combining the best of RNN and transformer - &lt;strong&gt;great performance, fast inference, saves VRAM, fast training, &#34;infinite&#34; ctx_len, and free sentence embedding&lt;/strong&gt; (using the final hidden state).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Download RWKV-4 0.1/0.4/1.5/3/7/14B weights&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/BlinkDL&#34;&gt;https://huggingface.co/BlinkDL&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV chatbot&lt;/strong&gt;: &lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV&#34;&gt;https://github.com/BlinkDL/ChatRWKV&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-chat.png&#34; alt=&#34;RWKV-chat&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can run RWKV on low VRAM GPUs with this fork (choose pytorch-stream):&lt;/strong&gt; &lt;a href=&#34;https://github.com/harrisonvanderbyl/rwkv_chatbot&#34;&gt;https://github.com/harrisonvanderbyl/rwkv_chatbot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;I am training RWKV-4 14B on the Pile (final release around Feb-15-2023): &lt;a href=&#34;https://wandb.ai/blinkdl/RWKV-v4-Pile&#34;&gt;https://wandb.ai/blinkdl/RWKV-v4-Pile&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-eval2.png&#34; alt=&#34;RWKV-eval2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;RWKV-3 1.5B on A40 (tf32) = always 0.015 sec/token, tested using simple pytorch code (no CUDA), GPU utilization 45%, VRAM 7823M&lt;/p&gt; &#xA;&lt;p&gt;GPT2-XL 1.3B on A40 (tf32) = 0.032 sec/token (for ctxlen 1000), tested using HF, GPU utilization 45% too (interesting), VRAM 9655M&lt;/p&gt; &#xA;&lt;p&gt;Training speed: RWKV-4 1.5B BF16 ctxlen1024 = 106K tokens/s on 8xA100 40G.&lt;/p&gt; &#xA;&lt;p&gt;I am doing image experiments too (For example: &lt;a href=&#34;https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder&#34;&gt;https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder&lt;/a&gt;) and RWKV will be able to do txt2img diffusion :) My idea: 256x256 rgb image -&amp;gt; 32x32x13bit latents -&amp;gt; apply RWKV to compute transition probability for each of the 32x32 grid -&amp;gt; pretend the grids are independent and &#34;diffuse&#34; using these probabilities.&lt;/p&gt; &#xA;&lt;p&gt;Smooth training - no loss spikes! (lr &amp;amp; bsz change around 15G tokens) &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-loss.png&#34; alt=&#34;RWKV-loss&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Join our Discord: &lt;a href=&#34;https://discord.gg/bDSBUMeFpc&#34;&gt;https://discord.gg/bDSBUMeFpc&lt;/a&gt; :)&lt;/h2&gt; &#xA;&lt;p&gt;You are welcome to join the RWKV discord &lt;a href=&#34;https://discord.gg/bDSBUMeFpc&#34;&gt;https://discord.gg/bDSBUMeFpc&lt;/a&gt; to build upon it. We have plenty of potential compute (A100 40Gs) now (thanks to Stability and EleutherAI), so if you have interesting ideas I can run them.&lt;/p&gt; &#xA;&lt;p&gt;Twitter: &lt;a href=&#34;https://twitter.com/BlinkDL_AI&#34;&gt;https://twitter.com/BlinkDL_AI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-eval.png&#34; alt=&#34;RWKV-eval&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All of the trained models will be open-source. Inference is very fast (only matrix-vector multiplications, no matrix-matrix multiplications) even on CPUs, so you can even run a LLM on your phone.&lt;/p&gt; &#xA;&lt;p&gt;How it works: RWKV gathers information to a number of channels, which are also decaying with different speeds as you move to the next token. It&#39;s very simple once you understand it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV is parallelizable because the time-decay of each channel is data-independent (and trainable)&lt;/strong&gt;. For example, in usual RNN you can adjust the time-decay of a channel from say 0.8 to 0.5 (these are called &#34;gates&#34;), while in RWKV you simply move the information from a W-0.8-channel to a W-0.5-channel to achieve the same effect. Moreover, you can fine-tune RWKV into a non-parallelizable RNN (then you can use outputs of later layers of the previous token) if you want extra performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-formula.png&#34; alt=&#34;RWKV-formula&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here are some of my TODOs. Let&#39;s work together :)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;HuggingFace integration (check &lt;a href=&#34;https://github.com/huggingface/transformers/issues/17230&#34;&gt;https://github.com/huggingface/transformers/issues/17230&lt;/a&gt; ), and optimized CPU &amp;amp; iOS &amp;amp; Android &amp;amp; WASM &amp;amp; WebGL inference. RWKV is a RNN and very friendly for edge devices. Let&#39;s make it possible to run a LLM on your phone.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Test it on bidirectional &amp;amp; MLM tasks, and image &amp;amp; audio &amp;amp; video tokens. I think RWKV can support Encoder-Decoder via this: for each decoder token, use a learned mixture of [decoder previous hidden state] &amp;amp; [encoder final hidden state]. Hence all decoder tokens will have access to the encoder output.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now training RWKV-4a with one single tiny extra attention (just a few extra lines comparing with RWKV-4) to further improve some difficult zeroshot tasks (such as LAMBADA) for smaller models. See &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/commit/a268cd2e40351ee31c30c5f8a5d1266d35b41829&#34;&gt;https://github.com/BlinkDL/RWKV-LM/commit/a268cd2e40351ee31c30c5f8a5d1266d35b41829&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;User feedback:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;I&#39;ve so far toyed around the character-based model on our relatively small pre-training dataset (around 10GB of text), and the results are extremely good - similar ppl to models taking much, much longer to train.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;dear god rwkv is fast. i switched to another tab after starting training it from scratch &amp;amp; when i returned it was emitting plausible english &amp;amp; maori words, i left to go microwave some coffee &amp;amp; when i came back it was producing fully grammatically correct sentences.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Tweet from Sepp Hochreiter (thank you!): &lt;a href=&#34;https://twitter.com/HochreiterSepp/status/1524270961314484227&#34;&gt;https://twitter.com/HochreiterSepp/status/1524270961314484227&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can find me (BlinkDL) in the EleutherAI Discord too: &lt;a href=&#34;https://www.eleuther.ai/get-involved/&#34;&gt;https://www.eleuther.ai/get-involved/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-demo.png&#34; alt=&#34;RWKV-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;New ideas (just to record some new ideas)&lt;/h2&gt; &#xA;&lt;p&gt;I have an idea to improve tokenization. We can hardcode some channels to have meanings. Example:&lt;/p&gt; &#xA;&lt;p&gt;Channel 0 = &#34;space&#34;&lt;/p&gt; &#xA;&lt;p&gt;Channel 1 = &#34;capitalize first letter&#34;&lt;/p&gt; &#xA;&lt;p&gt;Channel 2 = &#34;capitalize all letters&#34;&lt;/p&gt; &#xA;&lt;p&gt;Therefore:&lt;/p&gt; &#xA;&lt;p&gt;Embedding of &#34;abc&#34;: [0, 0, 0, x0, x1, x2 , ..]&lt;/p&gt; &#xA;&lt;p&gt;Embedding of &#34; abc&#34;: [1, 0, 0, x0, x1, x2, ..]&lt;/p&gt; &#xA;&lt;p&gt;Embedding of &#34; Abc&#34;: [1, 1, 0, x0, x1, x2, ..]&lt;/p&gt; &#xA;&lt;p&gt;Embedding of &#34;ABC&#34;: [0, 0, 1, x0, x1, x2, ...]&lt;/p&gt; &#xA;&lt;p&gt;......&lt;/p&gt; &#xA;&lt;p&gt;so they will share most of the embedding. And we can rapidly compute the output probability of all variations of &#34;abc&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Note: the above method is assuming that p(&#34; xyz&#34;) / p(&#34;xyz&#34;) is the same for any &#34;xyz&#34;, which can be wrong.&lt;/p&gt; &#xA;&lt;p&gt;Better: define emb_space emb_capitalize_first emb_capitalize_all to be a function of emb.&lt;/p&gt; &#xA;&lt;p&gt;Maybe the Best: let &#39;abc&#39; &#39; abc&#39; etc. to share the last 90% of their embeddings.&lt;/p&gt; &#xA;&lt;p&gt;At this moment, all our tokenizers spend too many items to represent all variations of &#39;abc&#39; &#39; abc&#39; &#39; Abc&#39; etc. Moreover the model cannot discover that these are actually similar if some of these variations are rare in the dataset. My method can solve this. I plan to test this in a new version of RWKV.&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo&#34;&gt;https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo&lt;/a&gt; (latest code, compatible with v4).&lt;/p&gt; &#xA;&lt;p&gt;Here is a great prompt for testing Q&amp;amp;A of LLMs. Works for any model: (found by minimizing ChatGPT ppls for RWKV 1.5B)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = f&#39;\nQ &amp;amp; A\n\nQuestion:\n{qq}\n\nDetailed Expert Answer:\n&#39; # let the model generate after this&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cool Community RWKV Projects (check them!)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/rwkvstic/&#34;&gt;https://pypi.org/project/rwkvstic/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harrisonvanderbyl/rwkv_chatbot&#34;&gt;https://github.com/harrisonvanderbyl/rwkv_chatbot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mrsteyk/RWKV-LM-deepspeed&#34;&gt;https://github.com/mrsteyk/RWKV-LM-deepspeed&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/issues/17230&#34;&gt;https://github.com/huggingface/transformers/issues/17230&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ArEnSc/Production-RWKV&#34;&gt;https://github.com/ArEnSc/Production-RWKV&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nlpodyssey/verbaflow&#34;&gt;https://github.com/nlpodyssey/verbaflow&lt;/a&gt; (in Go)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nlpodyssey/rwkv&#34;&gt;https://github.com/nlpodyssey/rwkv&lt;/a&gt; (in Go)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/resloved/RWKV-notebooks&#34;&gt;https://github.com/resloved/RWKV-notebooks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Pathos14489/RWKVDistributedInference&#34;&gt;https://github.com/Pathos14489/RWKVDistributedInference&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AXKuhta/rwkv-onnx-dml&#34;&gt;https://github.com/AXKuhta/rwkv-onnx-dml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/josephrocca/rwkv-v4-web&#34;&gt;https://github.com/josephrocca/rwkv-v4-web&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run RWKV-4 Pile models:&lt;/strong&gt; Download models from &lt;a href=&#34;https://huggingface.co/BlinkDL&#34;&gt;https://huggingface.co/BlinkDL&lt;/a&gt;. Set TOKEN_MODE = &#39;pile&#39; in run.py and run it. It&#39;s fast even on CPU (the default mode).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Colab for RWKV-4 Pile 1.5B&lt;/strong&gt;: &lt;a href=&#34;https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM&#34;&gt;https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run RWKV-4 Pile models in your browser (and onnx version): see this issue &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/issues/7&#34;&gt;https://github.com/BlinkDL/RWKV-LM/issues/7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;RWKV-4 Web Demo: &lt;a href=&#34;https://josephrocca.github.io/rwkv-v4-web/demo/&#34;&gt;https://josephrocca.github.io/rwkv-v4-web/demo/&lt;/a&gt; (note: only greedy sampling for now)&lt;/p&gt; &#xA;&lt;p&gt;For the old RWKV-2: see the release here for a 27M params model on enwik8 with 0.72 BPC(dev). Run run.py in &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v2-RNN&#34;&gt;https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v2-RNN&lt;/a&gt;. You can even run it in your browser: &lt;a href=&#34;https://github.com/BlinkDL/AI-Writer/tree/main/docs/eng&#34;&gt;https://github.com/BlinkDL/AI-Writer/tree/main/docs/eng&lt;/a&gt; &lt;a href=&#34;https://blinkdl.github.io/AI-Writer/eng/&#34;&gt;https://blinkdl.github.io/AI-Writer/eng/&lt;/a&gt; (this is using tf.js WASM single-thread mode).&lt;/p&gt; &#xA;&lt;p&gt;I&#39;d like to build an almost-INT8 version of RWKV. A simple method to quantize a matrix with outliers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as npA&#xA;&#xA;# the original M, with outliers&#xA;M = np.array([[1,   2,   1,  2],[2,  100,    2, 10],[1,   2,   1, 2],[2,   1, 20, 1]])&#xA;&#xA;# the scaled M, without outliers&#xA;Q = np.array([[1, 0.2, 0.1,  2],[0.4,  2, 0.04, 2], [1, 0.2, 0.1, 2],[2, 0.1,  2, 1]])&#xA;# we can find optimal a &amp;amp; b to minimize inference error after quantization&#xA;a = np.array([1, 10, 10, 1])&#xA;b = np.array([1, 5, 1, 1])&#xA;&#xA;# test M.v with random v - the results will be the same&#xA;v = np.array([1.23, 5.44, 9.75, 2.98])&#xA;print(M.dot(v))&#xA;print(Q.dot(v * a) * b)&#xA;&#xA;# even better: decompose M.dot(v) as Q.dot(v * a + aa) * b + bb where aa &amp;amp; bb are vectors too&#xA;# and can apply more scaling to achieve W8A8 (example: https://arxiv.org/pdf/2211.10438.pdf)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training / Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training RWKV-4 from scratch:&lt;/strong&gt; run train.py, which by default is using the enwik8 dataset (unzip &lt;a href=&#34;https://data.deepai.org/enwik8.zip&#34;&gt;https://data.deepai.org/enwik8.zip&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You will be training the &#34;GPT&#34; version because it&#39;s paralleziable and faster to train. RWKV-4 can extrapolate, so training with ctxLen 1024 can work for ctxLen of 2500+. You can fine-tune the model with longer ctxLen and it can quickly adapt to longer ctxLens.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fine-tuning RWKV-4 Pile models:&lt;/strong&gt; use &#39;prepare-data.py&#39; in &lt;a href=&#34;https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3&#34;&gt;https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3&lt;/a&gt; to tokenize .txt into train.npy data. Then set EXPRESS_PILE_MODE to True in train.py, and run it.&lt;/p&gt; &#xA;&lt;p&gt;Read the inference code in src/model.py and try using the final hidden state（.xx .aa .bb) as a faithful sentence embedding for other tasks. Probably you should begin with .xx and .aa/.bb (.aa divided by .bb).&lt;/p&gt; &#xA;&lt;p&gt;Colab for fine-tuning RWKV-4 Pile models: &lt;a href=&#34;https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_v4_RNN_Pile_Fine_Tuning.ipynb&#34;&gt;https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_v4_RNN_Pile_Fine_Tuning.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Large corpus:&lt;/strong&gt; Use &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;https://github.com/EleutherAI/gpt-neox&lt;/a&gt; to convert .jsonl into .bin and .idx&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python tools/preprocess_data.py --input ./my_data.jsonl --output-prefix ./data/my_data --vocab ./20B_tokenizer.json --dataset-impl mmap --tokenizer-type HFTokenizer --append-eod&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The jsonl format sample (one line for each document):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#34;meta&#34;: {&#34;ID&#34;: 101}, &#34;text&#34;: &#34;This is the first document.&#34;}&#xA;{&#34;meta&#34;: {&#34;ID&#34;: 102}, &#34;text&#34;: &#34;Hello\nWorld&#34;}&#xA;{&#34;meta&#34;: {&#34;ID&#34;: 103}, &#34;text&#34;: &#34;1+1=2\n1+2=3\n2+2=4&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;generated by code like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ss = json.dumps({&#34;meta&#34;: meta, &#34;text&#34;: text}, ensure_ascii=False)&#xA;out.write(ss + &#34;\n&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;RWKV is inspired by Apple&#39;s AFT (&lt;a href=&#34;https://arxiv.org/abs/2105.14103&#34;&gt;https://arxiv.org/abs/2105.14103&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Moreover it&#39;s using a number of my tricks, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;SmallInitEmb: &lt;a href=&#34;https://github.com/BlinkDL/SmallInitEmb&#34;&gt;https://github.com/BlinkDL/SmallInitEmb&lt;/a&gt; (applicable to all transformers) which helps the embedding quality, and stabilizes Post-LN (which is what I am using).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Token-shift: &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing&#34;&gt;https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing&lt;/a&gt; (applicable to all transformers), especially helpful for char-level models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Head-QK: &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens&#34;&gt;https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens&lt;/a&gt; (applicable to all transformers). Note: it&#39;s helpful, but I disabled it in the Pile model to keep it 100% RNN.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Extra R-gate in the FFN (applicable to all transformers). I am also using reluSquared from Primer.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Better initilization: I init most of the matrices to ZERO (see RWKV_Init in &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-v2-RNN/src/model.py&#34;&gt;https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can transfer some parameters from a small model to a large model (note: I sort &amp;amp; smooth them too), for faster and better convergence (see &lt;a href=&#34;https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/&#34;&gt;https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;My CUDA kernel: &lt;a href=&#34;https://github.com/BlinkDL/RWKV-CUDA&#34;&gt;https://github.com/BlinkDL/RWKV-CUDA&lt;/a&gt; to speedup training.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The pseudocode (execution from top to bottom):&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-v2-RNN.png&#34; alt=&#34;RWKV-v2-RNN&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The a b c d factors work together to build a time-decay curve: [X, 1, W, W^2, W^3, ...].&lt;/p&gt; &#xA;&lt;p&gt;Write out the formulas for &#34;token at pos 2&#34; and &#34;token at pos 3&#34; and you will get the idea:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a and b: EMAs of kv and k.&lt;/li&gt; &#xA; &lt;li&gt;c and d: these are a and b combined with &#34;self-attention&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;kv / k is the memory mechanism. The token with high k can be remembered for a long duration, if W is close to 1 in the channel.&lt;/p&gt; &#xA;&lt;p&gt;The R-gate is important for performance. k = info strength of this token (to be passed to future tokens). r = whether to apply the info to this token.&lt;/p&gt; &#xA;&lt;h2&gt;RWKV-3 improvements&lt;/h2&gt; &#xA;&lt;p&gt;Use different trainable TimeMix factors for R / K / V in SA and FF layers. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xx = self.time_shift(x)&#xA;xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)&#xA;xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)&#xA;xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use preLN instead of postLN (more stable &amp;amp; faster convergence):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if self.layer_id == 0:&#xA;&#x9;x = self.ln0(x)&#xA;x = x + self.att(self.ln1(x))&#xA;x = x + self.ffn(self.ln2(x))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Explaining the code for RWKV-3 GPT mode&lt;/h2&gt; &#xA;&lt;h3&gt;The GPT mode - overview&lt;/h3&gt; &#xA;&lt;p&gt;The building blocks of RWKV-3 GPT mode are similar to that of a usual preLN GPT.&lt;/p&gt; &#xA;&lt;p&gt;The only difference is an extra LN after embedding. Note you can absorb this LN into the embedding after finishing the training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = self.emb(idx)  # input: idx = token indices&#xA;x = self.ln_emb(x) # extra LN after embedding&#xA;x = x + self.att_0(self.ln_att_0(x)) # preLN&#xA;x = x + self.ffn_0(self.ln_ffn_0(x))&#xA;...&#xA;x = x + self.att_n(self.ln_att_n(x))&#xA;x = x + self.ffn_n(self.ln_ffn_n(x))&#xA;x = self.ln_head(x) # final LN before projection&#xA;x = self.head(x)    # output: x = logits&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is important to initialize emb to tiny values, such as nn.init.uniform_(a=-1e-4, b=1e-4), to utilize my trick &lt;a href=&#34;https://github.com/BlinkDL/SmallInitEmb&#34;&gt;https://github.com/BlinkDL/SmallInitEmb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the 1.5B RWKV-3, I use Adam (no wd, no dropout) optimizer on 8 * A100 40G.&lt;/p&gt; &#xA;&lt;p&gt;batchSz = 32 * 896, ctxLen = 896. I am using tf32 so the batchSz is a bit small.&lt;/p&gt; &#xA;&lt;p&gt;For the first 15B tokens, LR is fixed at 3e-4, and beta=(0.9, 0.99).&lt;/p&gt; &#xA;&lt;p&gt;Then I set beta=(0.9, 0.999), and do an exponential decay of LR, reaching 1e-5 at 332B tokens.&lt;/p&gt; &#xA;&lt;h3&gt;The GPT mode - ATT block&lt;/h3&gt; &#xA;&lt;p&gt;The RWKV-3 does not have any attention in the usual sense, but we will call this block ATT anyway.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;B, T, C = x.size() # x = (Batch,Time,Channel)&#xA;&#xA;# Mix x with the previous timestep to produce xk, xv, xr&#xA;xx = self.time_shift(x) # self.time_shift = nn.ZeroPad2d((0,0,1,-1))&#xA;xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)&#xA;xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)&#xA;xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)&#xA;&#xA;# Use xk, xv, xr to produce k, v, r&#xA;k = self.key(xk).transpose(-1, -2)&#xA;v = self.value(xv).transpose(-1, -2)&#xA;r = self.receptance(xr)&#xA;k = torch.clamp(k, max=60) # clamp k to avoid overflow&#xA;k = torch.exp(k)&#xA;kv = k * v&#xA;&#xA;# Compute the W-curve = [e^(-n * e^time_decay), e^(-(n-1) * e^time_decay), ..., 1, e^(time_first)]&#xA;self.time_w = torch.cat([torch.exp(self.time_decay) * self.time_curve.to(x.device), self.time_first], dim=-1)&#xA;w = torch.exp(self.time_w)&#xA;&#xA;# Use W to mix kv and k respectively. Add K_EPS to wk to avoid divide-by-zero&#xA;if RUN_DEVICE == &#39;cuda&#39;:&#xA;    wkv = TimeX.apply(w, kv, B,C,T, 0)&#xA;    wk = TimeX.apply(w, k, B,C,T, K_EPS)&#xA;else:&#xA;    w = w[:,-T:].unsqueeze(1)&#xA;    wkv = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(kv), w, groups=C)&#xA;    wk = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(k), w, groups=C) + K_EPS&#xA;&#xA;# The RWKV formula&#xA;rwkv = torch.sigmoid(r) * (wkv / wk).transpose(-1, -2)&#xA;rwkv = self.output(rwkv) # final output projection&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The self.key, self.receptance, self.output matrices are all initialized to zero.&lt;/p&gt; &#xA;&lt;p&gt;The time_mix, time_decay, time_first vectors are transferred from a smaller trained model (note: I sort &amp;amp; smooth them too).&lt;/p&gt; &#xA;&lt;h3&gt;The GPT mode - FFN block&lt;/h3&gt; &#xA;&lt;p&gt;The FFN block has three tricks comparing with the usual GPT:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;My time_mix trick.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The sqReLU from the Primer paper.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;An extra receptance-gate (similar to the receptance-gate in ATT block).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Mix x with the previous timestep to produce xk, xr&#xA;xx = self.time_shift(x)&#xA;xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)&#xA;xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)&#xA;&#xA;# The usual FFN operation&#xA;k = self.key(xk)&#xA;k = torch.square(torch.relu(k)) # from the Primer paper&#xA;kv = self.value(k)&#xA;&#xA;# Apply an extra receptance-gate to kv&#xA;rkv = torch.sigmoid(self.receptance(xr)) * kv&#xA;return rkv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The self.value, self.receptance matrices are all initialized to zero.&lt;/p&gt; &#xA;&lt;h2&gt;RWKV-4 improvements&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-v3-plan.png&#34; alt=&#34;RWKV-v3-plan&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;From GPT to RWKV (the formulas)&lt;/h2&gt; &#xA;&lt;p&gt;Let F[t] be the system state at t.&lt;/p&gt; &#xA;&lt;p&gt;Let x[t] be the new external input at t.&lt;/p&gt; &#xA;&lt;p&gt;In GPT, predicting F[t+1] requires considering F[0], F[1], .. F[t]. So it takes O(T^2) to generate a length T sequence.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;simplified formula&lt;/strong&gt; for GPT:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D&#34; alt=&#34;F[\mathrm{t}+1]=\frac{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}])}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s very capable in theory, however that &lt;strong&gt;does not mean we can fully utilize its capability with usual optimizers&lt;/strong&gt;. I suspect the loss landscape is too difficult for our current methods.&lt;/p&gt; &#xA;&lt;p&gt;Compare with the &lt;strong&gt;simplified formula&lt;/strong&gt; for RWKV (the parallel mode, looks similar to Apple&#39;s AFT):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D&#34; alt=&#34;F[\mathrm{t}+1]=\sigma(\mathbf{R}x[\mathrm{t}]) \cdot \frac{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K }F[\mathrm{i}])}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The R, K, V are trainable matrices, and W is a trainable vector (time-decay factor for each channel).&lt;/p&gt; &#xA;&lt;p&gt;In GPT, the contribution of F[i] to F[t+1] is weighted by &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle++%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+&#34; alt=&#34; \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) &#34;&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In RWKV-2, the contribution of F[i] to F[t+1] is weighted by &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+&#34; alt=&#34;\sigma(\mathbf{R}x[\mathrm{t}]) \cdot \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}]) &#34;&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma&#34; alt=&#34;\sigma&#34;&gt; is a non-linearity and we can use sigmoid.&lt;/li&gt; &#xA; &lt;li&gt;Note &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29&#34; alt=&#34;\sigma(\mathbf{R}x[\mathrm{t}])&#34;&gt; is not in the denominator, and I call R the &#34;receptance&#34;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29&#34; alt=&#34;\exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i}))&#34;&gt; is the time-decay factor. I proposed the same idea (scaling the attention by distance) in Aug 2020 and called it the &#34;time-weighting&#34; (check the commit history of &lt;a href=&#34;https://github.com/BlinkDL/minGPT-tuned&#34;&gt;https://github.com/BlinkDL/minGPT-tuned&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here comes the punchline: we can rewrite it into a RNN (recursive formula). Note:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B0%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D&#34; alt=&#34;F[1]=\sigma(\mathbf{R }x[0]) \cdot \frac{ \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{\exp (\mathbf{K }F[0])}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B2%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B1%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D&#34; alt=&#34;F[2]=\sigma(\mathbf{R }x[1]) \cdot \frac{ \exp (\mathbf{K }F[1]) \cdot(\mathbf{V }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{ \exp (\mathbf{K }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0])}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Therefore it&#39;s straightforward to verify:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5Bt%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5Bt%5D%29+%5Ccdot+%5Cfrac%7B%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+A%5B%5Cmathrm%7Bt%7D%5D%7D%7B+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+B%5B%5Cmathrm%7Bt%7D%5D%7D&#34; alt=&#34;F[t+1]=\sigma(\mathbf{R }x[t]) \cdot \frac{\exp (\mathbf{K}F[\mathrm{t}]) \cdot(\mathbf{V}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot A[\mathrm{t}]}{ \exp (\mathbf{K}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot B[\mathrm{t}]}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;where A[t] and B[t] are the numerator and denominator of the previous step, respectively.&lt;/p&gt; &#xA;&lt;p&gt;I believe RWKV is performant because W is like repeatedly applying a diagonal matrix. Note (P^{-1} D P)^n = P^{-1} D^n P, so it is similar to repeatedly applying a general diagonalizable matrix.&lt;/p&gt; &#xA;&lt;p&gt;Moreover it&#39;s possible to turn it into a continuous ODE (a bit similar to State Space Models). I will write about it later.&lt;/p&gt; &#xA;&lt;h2&gt;Multimodal ideas&lt;/h2&gt; &#xA;&lt;p&gt;I have an idea for [text --&amp;gt; 32x32 RGB image] using a LM (transformer, RWKV, etc.). Will test it soon.&lt;/p&gt; &#xA;&lt;p&gt;Firstly, LM loss (instead of L2 loss), so the image will not be blurry.&lt;/p&gt; &#xA;&lt;p&gt;Secondly, color quantization. For example, only allowing 8 levels for R/G/B. Then the image vocab size is 8x8x8 = 512 (for each pixel), instead of 2^24. Therefore, a 32x32 RGB image = a len1024 sequence of vocab512 (image tokens), which is a typical input for usual LMs. (Later we can use diffusion models to upsample and generate RGB888 images. We might be able to use a LM for this too.)&lt;/p&gt; &#xA;&lt;p&gt;Thirdly, 2D positional embeddings that are easy for the model to understand. For example, add one-hot X &amp;amp; Y coords to the first 64(=32+32) channels. Say if the pixel is at x=8, y=20, then we will add 1 to channel 8 and channel 52 (=32+20). Moreover probably we can add the float X &amp;amp; Y coords (normalized to 0~1 range) to another 2 channels. And other periodic pos. encoding might help too (will test).&lt;/p&gt; &#xA;&lt;p&gt;Finally, RandRound when doing the color quantization in the DataLoader. For example, if the float level is 4.578, then there is a 57.8% chance to use 5, and (1-57.8%) chance to use 4. And we can allow both 4 and 5 in the prediction, but the loss will be higher if the prediction is 4.&lt;/p&gt; &#xA;&lt;p&gt;Multi-task training might help too. I will try this dataset format: [TxtFirst] [Desc of Img (txt tokens)] [Img] [img tokens] and sometimes [ImgFirst] [img tokens] [Txt] [Desc of Img (txt tokens)] ... the order of the imgs should be randomized in the DataLoader, and [TxtFirst] [ImgFirst] [Img] [Txt] are special tokens and do random sampling of the full dataset. So sometimes the model will see the img tokens first and then the corresponding txt tokens, which is a [img -&amp;gt; txt] task. And the model will see some partial imgs and partial txts. I think a char-level LM might help the model to write correct text on images.&lt;/p&gt; &#xA;&lt;h2&gt;How to sample a large dataset (for training)&lt;/h2&gt; &#xA;&lt;p&gt;I am using a trick to sample the Pile deterministically yet randomly enough.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s say the pile has x chunks (a chunk = ctx_len tokens).&lt;/p&gt; &#xA;&lt;p&gt;pick a prime number p just less than x, and make sure p = 2 (mod 3).&lt;/p&gt; &#xA;&lt;p&gt;Use (step * step * step) mod p to sample it. Add some bias to step for extra randomness.&lt;/p&gt; &#xA;&lt;h2&gt;The top-p-x sampling method (for inference)&lt;/h2&gt; &#xA;&lt;p&gt;We propose a new sampling method called top-p-x:&lt;/p&gt; &#xA;&lt;p&gt;it&#39;s like top-p, and the only difference is you also keep all tokens whose prob &amp;gt; x.&lt;/p&gt; &#xA;&lt;p&gt;Try x = 0.01 first.&lt;/p&gt; &#xA;&lt;h2&gt;Better Learning Rate Schedule via Variantional Method of Loss Curve&lt;/h2&gt; &#xA;&lt;p&gt;I propose a simple new method to find better LR schedules. The method is cost-efficient and practical for large LMs. The takeaway is we can model the loss curve dynamics (phenomenology) w.r.t. the LR, and a nice closed-form LR curve can be directly computed from it using variantional method. Moreover we can predict the final loss with reasonable accuracy.&lt;/p&gt; &#xA;&lt;p&gt;UPDATE: In &#34;Conclusion 1.&#34;, use the best-fitting regime (ignore the initial steps where our approximations break down) to fit the parameters.&lt;/p&gt; &#xA;&lt;p&gt;Try this: fixed lr for 1 hr, then exponential decay to 0.2 * lr in 12 hrs, and choose the t=[1hr, 13hr] segment.&lt;/p&gt; &#xA;&lt;p&gt;In the last three plots, black = predicted loss curve of the new LR schedule, blue = original (unoptimized) real loss curve, orange = new LR schedule.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/Research/better_lr_schedule.png&#34; alt=&#34;better_lr_schedule&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;RWKV v1&lt;/h1&gt; &#xA;&lt;p&gt;We propose the RWKV language model, with alternating time-mix and channel-mix layers:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Balign%2A%7D%0A%5Ctext%7BTime-mix+%3A%7D+%26%26+%5Ctext%7BTM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_%7Bu%7D+%26%26%5Ctextbf%7BW%7D_%7Bt%2Cu%2Cc%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bu%2Cc%7D%5C%5C%0A%5Ctext%7BChannel-mix+%3A%7D+%26%26+%5Ctext%7BCM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_d+%26%26%5Ctextbf%7BW%7D_%7Bc%2Cd%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bgelu%7D%28%5Ctext%7BK%7D_%7Bt%2Cd%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bt%2Cd%7D%0A%5Cend%7Balign%2A%7D%0A&#34; alt=&#34;\begin{align*}&#xA;\text{Time-mix :} &amp;amp;&amp;amp; \text{TM}_{t,c} &amp;amp;&amp;amp;=&amp;amp;&amp;amp;\text{sigmoid}(\text{R}_{t,c}) &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; &amp;amp;&amp;amp;\textstyle\sum_{u} &amp;amp;&amp;amp;\textbf{W}_{t,u,c} &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; \text{softmax}_t(\text{K}_{u,c}) &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; \text{V}_{u,c}\\&#xA;\text{Channel-mix :} &amp;amp;&amp;amp; \text{CM}_{t,c} &amp;amp;&amp;amp;=&amp;amp;&amp;amp;\text{sigmoid}(\text{R}_{t,c}) &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; &amp;amp;&amp;amp;\textstyle\sum_d &amp;amp;&amp;amp;\textbf{W}_{c,d} &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; \text{gelu}(\text{K}_{t,d}) &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; \text{V}_{t,d}&#xA;\end{align*}&#xA;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The R, K, V are generated by linear transforms of input, and W is parameter. The idea of RWKV is to decompose attention into R(target) * W(src, target) * K(src). So we can call R &#34;receptance&#34;, and sigmoid means it&#39;s in 0~1 range.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The Time-mix is similar to AFT (&lt;a href=&#34;https://arxiv.org/abs/2105.14103&#34;&gt;https://arxiv.org/abs/2105.14103&lt;/a&gt;). There are two differences.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(1) We changed the normalization (denominator). For masked language models, we define:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%3D+%5Cfrac%7B%5Cexp%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29%7D%7B%5Csum_%7Bv+%5Cleq+t%7D%5Cexp%28%5Ctext%7BK%7D_%7Bv%2Cc%7D%29%7D&#34; alt=&#34;\text{softmax}_t(\text{K}_{u,c}) = \frac{\exp(\text{K}_{u,c})}{\sum_{v \leq t}\exp(\text{K}_{v,c})}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(UPDATE: We are using the original AFT normalization in v2)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Initialize K and R matrices (and the output projection matrix) to ZERO for fast &amp;amp; stable convergence.&lt;/p&gt; &#xA;&lt;p&gt;(2) We decompose W_{t,u,c} and introduce multi-head W (here h is the corresponding head of c):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+W_%7Bt%2Cu%2Cc%7D%3Df_h%28t-u%29%5Ccdot+%5Calpha_h%28u%29+%5Ccdot+%5Cbeta_h%28t%29&#34; alt=&#34;W_{t,u,c}=f_h(t-u)\cdot \alpha_h(u) \cdot \beta_h(t)&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Moreover we multiply the final output of Time-mix layer by γ(t). The reason for the α β γ factors, is because the context size is smaller when t is small, and this can be compensated using the α β γ factors.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(UPDATE: We remove α β γ factors in v2-RNN and restrict W to be of a simple form and hence able to rewrite it as RNN)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The Channel-mix is similar to GeGLU (&lt;a href=&#34;https://arxiv.org/abs/2002.05202&#34;&gt;https://arxiv.org/abs/2002.05202&lt;/a&gt;) with an extra R factor. Initialize R and W matrices to ZERO for fast &amp;amp; stable convergence.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Finally, we add extra token-shift (time-shift mixing) as in (&lt;a href=&#34;https://github.com/BlinkDL/minGPT-tuned&#34;&gt;https://github.com/BlinkDL/minGPT-tuned&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Token-shift (time-shift mixing)&lt;/h1&gt; &#xA;&lt;p&gt;The token-shift explicitly uses (half the channels of this token) &amp;amp; (half the channels of prev token) to generate all vectors (QKV, RWKV, ...).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;self.time_shift = nn.ZeroPad2d((0,0,1,-1))&#xA;&#xA;x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Dividing channels by 2 and shift-1 works great for char-level English and char-level Chinese LM.&lt;/p&gt; &#xA;&lt;p&gt;However for BPE-level English LM, it&#39;s only effective if your embedding is large enough (at least 1024 - so the usual small L12-D768 model is not enough).&lt;/p&gt; &#xA;&lt;p&gt;My theory on the effectiveness of token-shift:&lt;/p&gt; &#xA;&lt;p&gt;When we train a GPT, the hidden representation of a token has to accomplish two different objects:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Predict the next token. Sometimes this is easy (obvious next token).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Collect all previous context info, so later tokens can use it. This is always hard.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The shifted channels can focus on (2), so we have good propagation of info. It&#39;s like some kind of residual connection, or a small RNN inside the transformer.&lt;/p&gt; &#xA;&lt;p&gt;You can use token-shift in usual QKV self-attention too. I looked at the weights, and found V really likes the shifted channels, less so for Q. Makes sense if you think about it. I also found you may want to use less mixing in higher layers.&lt;/p&gt; &#xA;&lt;p&gt;p.s. There is a MHA_pro model in this repo with strong performance. Give it a try :)&lt;/p&gt; &#xA;&lt;h1&gt;The Head-QK Trick: learning to copy and avoid tokens&lt;/h1&gt; &#xA;&lt;p&gt;In usual transformer, a small model has difficulty copying tokens (such as person names) in the context. We add extra Q &amp;amp; K to the final output such that the model can directly copy (or avoid) tokens in the context. Afterwards the model will teach itself NER (named entity recognition) if you look at the learned weights.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;q = self.head_q(x)[:,:T,:] # projecting to 256-d&#xA;k = self.head_k(x)[:,:T,:] # projecting to 256-d&#xA;c = (q @ k.transpose(-2, -1)) * (1.0 / 256)&#xA;c = c.masked_fill(self.copy_mask[:T,:T] == 0, 0)&#xA;c = c @ F.one_hot(idx, num_classes = self.config.vocab_size).float()       &#xA;x = self.head(x) + c&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: when a token occurs multiple times in the context, it might be better to use max(prob) instead of sum(prob).&lt;/p&gt; &#xA;&lt;h1&gt;The top-a sampling method&lt;/h1&gt; &#xA;&lt;p&gt;We also propose a new sampling method called top-a (as in src/utils.py):&lt;/p&gt; &#xA;&lt;p&gt;(1) Find the max probability p_max after softmax.&lt;/p&gt; &#xA;&lt;p&gt;(2) Remove all entries whose probability is lower than 0.2 * pow(p_max, 2). So it&#39;s adaptive, hence &#34;top-a&#34;.&lt;/p&gt; &#xA;&lt;p&gt;(3) Feel free to tune the 0.2 and 2 factor. Tune 0.2 first.&lt;/p&gt; &#xA;&lt;p&gt;The idea of top-a:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If max_prob=0.9, then remove all tokens with prob &amp;lt; 0.162 (so, removing all alternatives)&lt;/li&gt; &#xA; &lt;li&gt;If max_prob=0.5, then remove all tokens with prob &amp;lt; 0.05 (so, allowing more choices)&lt;/li&gt; &#xA; &lt;li&gt;If max_prob=0.1, then remove all tokens with prob &amp;lt; 0.002 (so, allowing lots of possibilities)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;probs = F.softmax(logits, dim=-1)&#xA;&#xA;limit = torch.pow(torch.max(probs), 2) * 0.02&#xA;logits[probs &amp;lt; limit] = -float(&#39;Inf&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Performance&lt;/h1&gt; &#xA;&lt;p&gt;Character-level loss on simplebooks-92 dataset &lt;a href=&#34;https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip&#34;&gt;https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-vs-MHA.png&#34; alt=&#34;RWKV-vs-MHA&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gray: usual MHA+Rotary+GeGLU - performance not as good. 17.2M params.&lt;/p&gt; &#xA;&lt;p&gt;Red: RWKV (&#34;linear&#34; attention) - VRAM friendly - quite faster when ctx window is long - good performance. 16.6M params.&lt;/p&gt; &#xA;&lt;p&gt;Green: MHA+Rotary+GeGLU+Token_shift. 17.2M params.&lt;/p&gt; &#xA;&lt;p&gt;Blue: MHA_pro (MHA with various tweaks &amp;amp; RWKV-type-FFN) - slow - needs more VRAM - good performance. 16.6M params.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{peng_bo_2021_5196578,&#xA;  author       = {PENG Bo},&#xA;  title        = {BlinkDL/RWKV-LM: 0.01},&#xA;  month        = aug,&#xA;  year         = 2021,&#xA;  publisher    = {Zenodo},&#xA;  version      = {0.01},&#xA;  doi          = {10.5281/zenodo.5196577},&#xA;  url          = {https://doi.org/10.5281/zenodo.5196577}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Initialization&lt;/h1&gt; &#xA;&lt;p&gt;We use careful initialization for RWKV to get fast convergence - orthogonal matrices with proper scaling, and special time_w curves. Check model.py for details.&lt;/p&gt; &#xA;&lt;p&gt;Some learned time_w examples:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-time-w.png&#34; alt=&#34;RWKV-time-w&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PaddlePaddle/ERNIE</title>
    <updated>2023-02-09T01:43:07Z</updated>
    <id>tag:github.com,2023-02-09:/PaddlePaddle/ERNIE</id>
    <link href="https://github.com/PaddlePaddle/ERNIE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementations for various pre-training models of ERNIE-family, covering topics of Language Understanding &amp; Generation, Multimodal Understanding &amp; Generation, and beyond.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/.metas/ERNIE.png&#34; alt=&#34;ERNIE_milestone_20210519_zh&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;文心大模型ERNIE是百度发布的产业级知识增强大模型，涵盖了NLP大模型和跨模态大模型。2019年3月，开源了国内首个开源预训练模型文心ERNIE 1.0，此后在语言与跨模态的理解和生成等领域取得一系列技术突破，并对外开源与开放了系列模型，助力大模型研究与产业化应用发展。提醒: ERNIE老版本代码已经迁移至repro分支，欢迎使用我们全新升级的基于动静结合的新版ERNIE套件进行开发。另外，也欢迎上&lt;a href=&#34;https://ai.baidu.com/easydl/pro&#34;&gt;EasyDL&lt;/a&gt;、&lt;a href=&#34;https://ai.baidu.com/bml/app/overview&#34;&gt;BML&lt;/a&gt;体验更丰富的功能。 &lt;a href=&#34;https://wenxin.baidu.com/&#34;&gt;【了解更多】&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;开源Roadmap&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2022.8.18: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;图文跨模态预训练模型&lt;code&gt;ERNIE-ViL 2.0 (base)&lt;/code&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/tree/ernie-kit-open-v1.0/Research/ERNIE-ViL2&#34;&gt;正式开源&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2022.5.20: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;最新开源ERNIE 3.0系列预训练模型: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;110M参数通用模型ERNIE 3.0 Base&lt;/li&gt; &#xA;     &lt;li&gt;280M参数重量级通用模型ERNIE 3.0 XBase&lt;/li&gt; &#xA;     &lt;li&gt;74M轻量级通用模型ERNIE 3.0 Medium&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;新增语音-语言跨模态模型ERNIE-SAT &lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-sat&#34;&gt;正式开源&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;新增ERNIE-Gen（中文）预训练模型，支持多类主流生成任务：主要包括摘要、问题生成、对话、问答&lt;/li&gt; &#xA;   &lt;li&gt;动静结合的文心ERNIE开发套件：基于飞桨动态图功能，支持文心ERNIE模型动态图训练。您仅需要在模型训练开启前，修改一个参数配置，即可实现模型训练的动静切换。&lt;/li&gt; &#xA;   &lt;li&gt;将文本预处理、预训练模型、网络搭建、模型评估、上线部署等NLP开发流程规范封装。&lt;/li&gt; &#xA;   &lt;li&gt;支持NLP常用任务：文本分类、文本匹配、序列标注、信息抽取、文本生成、数据蒸馏等。&lt;/li&gt; &#xA;   &lt;li&gt;提供数据清洗、数据增强、分词、格式转换、大小写转换等数据预处理工具。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2021.12.3: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;多语言预训练模型&lt;code&gt;ERNIE-M&lt;/code&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-m&#34;&gt;正式开源&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2021.5.20: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ERNIE 最新开源四大预训练模型: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;多粒度语言知识模型&lt;code&gt;ERNIE-Gram&lt;/code&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/raw/develop/ernie-gram&#34;&gt;正式开源&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;超长文本双向建模预训练模型&lt;code&gt;ERNIE-Doc&lt;/code&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-doc&#34;&gt;正式开源&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;融合场景图知识的跨模态预训练模型教程&lt;code&gt;ERNIE-ViL&lt;/code&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-vil&#34;&gt;正式开源&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;语言与视觉一体的预训练模型&lt;code&gt;ERNIE-UNIMO&lt;/code&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-unimo&#34;&gt;正式开源&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2020.9.24: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;ERNIE-ViL&lt;/code&gt; 技术发布! (&lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-vil&#34;&gt;点击进入&lt;/a&gt;) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;面向视觉-语言知识增强的预训练框架，首次在视觉-语言预训练引入结构化的知识。 &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;利用场景图中的知识，构建了物体、属性和关系预测任务，精细刻画模态间细粒度语义对齐。&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;五项视觉-语言下游任务取得最好效果，&lt;a href=&#34;https://visualcommonsense.com/&#34;&gt;视觉常识推理榜单&lt;/a&gt;取得第一。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2020.5.20: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;ERNIE-GEN&lt;/code&gt; 模型正式开源! (&lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-gen&#34;&gt;点击进入&lt;/a&gt;) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;最强文本生成预训练模型正式开源，相关工作已被 &lt;code&gt;IJCAI-2020&lt;/code&gt; 收录。 &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;首次把 ERNIE 预训练技术能力扩展至文本生成领域，在多个典型任务上取得最佳。&lt;/li&gt; &#xA;       &lt;li&gt;您现在即可下载论文报告的所有模型（包含 &lt;a href=&#34;https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-gen/README.zh.md#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B&#34;&gt;base/large/large-430G&lt;/a&gt;）。&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;首次在预训练阶段加入span-by-span 生成任务，让模型每次能够生成一个语义完整的片段。&lt;/li&gt; &#xA;     &lt;li&gt;提出填充式生成机制和噪声感知机制来缓解曝光偏差问题。&lt;/li&gt; &#xA;     &lt;li&gt;精巧的 Mulit-Flow Attention 实现框架。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2020.4.30 发布&lt;a href=&#34;https://github.com/PaddlePaddle/PGL/tree/master/examples/erniesage&#34;&gt;ERNIESage&lt;/a&gt;， 一种新型图神经网络模型，采用ERNIE做为aggreagtor. 由&lt;a href=&#34;https://github.com/PaddlePaddle/PGL&#34;&gt;PGL&lt;/a&gt;实现。&lt;/li&gt; &#xA; &lt;li&gt;2020.3.27 &lt;a href=&#34;https://www.jiqizhixin.com/articles/2020-03-27-8&#34;&gt;在SemEval2020五项子任务上夺冠&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;2019.12.26 &lt;a href=&#34;https://www.technologyreview.com/2019/12/26/131372/ai-baidu-ernie-google-bert-natural-language-glue/&#34;&gt;GLUE榜第一名&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;2019.11.6 发布&lt;a href=&#34;https://www.jiqizhixin.com/articles/2019-11-06-9&#34;&gt;ERNIE Tiny&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;2019.7.7 发布&lt;a href=&#34;https://www.jiqizhixin.com/articles/2019-07-31-10&#34;&gt;ERNIE 2.0&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;2019.3.16 发布&lt;a href=&#34;https://www.jiqizhixin.com/articles/2019-03-16-3&#34;&gt;ERNIE 1.0&lt;/a&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;环境安装&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;安装环境依赖：&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/README_ENV.md&#34;&gt;环境安装&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;安装Ernie套件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;git clone https://github.com/PaddlePaddle/ERNIE.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;快速上手：使用文心ERNIE大模型进行训练&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;使用ERNIE3.0作为预训练模型，准备工作包括： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;下载模型&lt;/li&gt; &#xA;   &lt;li&gt;准备数据&lt;/li&gt; &#xA;   &lt;li&gt;配置训练json文件&lt;/li&gt; &#xA;   &lt;li&gt;启动训练模型&lt;/li&gt; &#xA;   &lt;li&gt;配置预测json文件&lt;/li&gt; &#xA;   &lt;li&gt;启动预测&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;我们以文本分类任务为例，来快速上手ERNIE大模型的使用&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;下载模型&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;使用ERNIE3.0预训练模型进行文本分类任务&lt;/li&gt; &#xA; &lt;li&gt;ERNNIE3.0预训练模型的下载与配置&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;# ernie_3.0 模型下载&#xA;# 进入models_hub目录&#xA;cd ./applications/models_hub&#xA;# 运行下载脚本&#xA;sh download_ernie_3.0_base_ch.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;准备数据&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;文心各个任务的data目录下自带一些示例数据，能够实现直接使用，方便快速熟悉文心的使用。&lt;/li&gt; &#xA; &lt;li&gt;文本分类任务的数据&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#进入文本分类任务文件夹&#xA;cd ./applications/tasks/text_classification/&#xA;#查看文本分类任务自带数据集&#xA;ls ./data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;注：示例数据仅作为格式演示使用，在真正训练模型时请替换为真实数据。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;配置训练json文件&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;其预置json文件在./examples/目录下，使用ERNIE3.0预训练模型进行训练的配置文件为的./examples/cls_ernie_fc_ch.json，在该json文件中对数据、模型、训练方式等逻辑进行了配置。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#查看 ERNIE3.0预训练模型 训练文本分类任务的配置文件&#xA;cat ./examples/cls_ernie_fc_ch.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;启动训练&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;将数据集存放妥当，并配置好cls_ernie_fc_ch.json，我们就可以运行模型训练的命令。&lt;/li&gt; &#xA; &lt;li&gt;其中，单卡指令为&lt;code&gt;python run_trainer.py&lt;/code&gt;，如下所示，使用基于ernie的中文文本分类模型在训练集上进行本地模型训练。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ernie 中文文本分类模型&#xA;# 基于json实现预置网络训练。其调用了配置文件./examples/cls_ernie_fc_ch.json&#xA;python run_trainer.py --param_path ./examples/cls_ernie_fc_ch.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;多卡指令为:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;fleetrun --gpus=x,y run_trainer.py./examples/cls_ernie_fc_ch.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;训练运行的日志会自动保存在**./log/test.log**文件中。&lt;/li&gt; &#xA; &lt;li&gt;训练中以及结束后产生的模型文件会默认保存在./output/&lt;strong&gt;目录下，其中&lt;/strong&gt;save_inference_model/文件夹会保存用于预测的模型文件，&lt;strong&gt;save_checkpoint/&lt;/strong&gt; 文件夹会保存用于热启动的模型文件。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;配置预测json文件&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;其预置json文件在./examples/目录下，使用ERNIE2.0预训练模型训练的模型进行预测的配置文件为的./examples/cls_ernie_fc_ch_infer.json&lt;/li&gt; &#xA; &lt;li&gt;主要修改./examples/cls_ernie_fc_ch_infer.json文件的预测模型的输入路径、预测文件的输入路径、预测结果的输出路径，对应修改配置如下：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;&#34;dataset_reader&#34;:{&#34;train_reader&#34;:{&#34;config&#34;:{&#34;data_path&#34;:&#34;./data/predict_data&#34;}}},&#xA;&#34;inference&#34;:{&#34;inference_model_path&#34;:&#34;./output/cls_ernie_fc_ch/save_inference_model/inference_step_251&#34;,&#xA;                        &#34;output_path&#34;: &#34;./output/predict_result.txt&#34;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;启动预测&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;运行run_infer.py ，选择对应的参数配置文件即可。如下所示：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;python run_infer.py --param_path ./examples/cls_ernie_fc_ch_infer.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;预测过程中的日志自动保存在./output/predict_result.txt文件中。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;预训练模型介绍&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;参考预训练模型原理介绍:&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/applications/models_hub&#34;&gt;模型介绍&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;预训练模型下载：进入./applications/models_hub目录下,下载示例：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;#进入预训练模型下载目录&#xA;cd ./applications/models_hub&#xA;#下载ERNIE3.0 base模型&#xA;sh downlaod_ernie_3.0_base_ch.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更多开源模型，见&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/Research/&#34;&gt;Research&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;数据集下载&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.cluebenchmarks.com/&#34;&gt;CLUE数据集&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.luge.ai/#/luge/dataDetail?id=5&#34;&gt;DuIE2.0数据集&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ernie-github.cdn.bcebos.com/data-msra_ner.tar.gz&#34;&gt;MSRA_NER数据集&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;模型效果评估&lt;/h1&gt; &#xA;&lt;h2&gt;评估数据集&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;分类和匹配采用&lt;a href=&#34;https://www.cluebenchmarks.com/&#34;&gt;CLUE数据集&lt;/a&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;CLUE 评测结果:&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;配置&lt;/th&gt; &#xA;   &lt;th&gt;模型&lt;/th&gt; &#xA;   &lt;th&gt;CLUEWSC2020&lt;/th&gt; &#xA;   &lt;th&gt;IFLYTEK&lt;/th&gt; &#xA;   &lt;th&gt;TNEWS&lt;/th&gt; &#xA;   &lt;th&gt;AFQMC&lt;/th&gt; &#xA;   &lt;th&gt;CMNLI&lt;/th&gt; &#xA;   &lt;th&gt;CSL&lt;/th&gt; &#xA;   &lt;th&gt;OCNLI&lt;/th&gt; &#xA;   &lt;th&gt;平均值&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;24L1024H&lt;/td&gt; &#xA;   &lt;td&gt;RoBERTa-wwm-ext-large&lt;/td&gt; &#xA;   &lt;td&gt;90.79&lt;/td&gt; &#xA;   &lt;td&gt;62.02&lt;/td&gt; &#xA;   &lt;td&gt;59.33&lt;/td&gt; &#xA;   &lt;td&gt;76.00&lt;/td&gt; &#xA;   &lt;td&gt;83.88&lt;/td&gt; &#xA;   &lt;td&gt;83.67&lt;/td&gt; &#xA;   &lt;td&gt;78.81&lt;/td&gt; &#xA;   &lt;td&gt;76.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;20L1024H&lt;/td&gt; &#xA;   &lt;td&gt;ERNIE 3.0-XBase&lt;/td&gt; &#xA;   &lt;td&gt;91.12&lt;/td&gt; &#xA;   &lt;td&gt;62.22&lt;/td&gt; &#xA;   &lt;td&gt;60.34&lt;/td&gt; &#xA;   &lt;td&gt;76.95&lt;/td&gt; &#xA;   &lt;td&gt;84.98&lt;/td&gt; &#xA;   &lt;td&gt;84.27&lt;/td&gt; &#xA;   &lt;td&gt;82.07&lt;/td&gt; &#xA;   &lt;td&gt;77.42&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12L768H&lt;/td&gt; &#xA;   &lt;td&gt;RoBERTa-wwm-ext-base&lt;/td&gt; &#xA;   &lt;td&gt;88.55&lt;/td&gt; &#xA;   &lt;td&gt;61.22&lt;/td&gt; &#xA;   &lt;td&gt;58.08&lt;/td&gt; &#xA;   &lt;td&gt;74.75&lt;/td&gt; &#xA;   &lt;td&gt;81.66&lt;/td&gt; &#xA;   &lt;td&gt;81.63&lt;/td&gt; &#xA;   &lt;td&gt;77.25&lt;/td&gt; &#xA;   &lt;td&gt;74.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12L768H&lt;/td&gt; &#xA;   &lt;td&gt;ERNIE 3.0-Base&lt;/td&gt; &#xA;   &lt;td&gt;88.18&lt;/td&gt; &#xA;   &lt;td&gt;60.72&lt;/td&gt; &#xA;   &lt;td&gt;58.73&lt;/td&gt; &#xA;   &lt;td&gt;76.53&lt;/td&gt; &#xA;   &lt;td&gt;83.65&lt;/td&gt; &#xA;   &lt;td&gt;83.30&lt;/td&gt; &#xA;   &lt;td&gt;80.31&lt;/td&gt; &#xA;   &lt;td&gt;75.63&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6L768H&lt;/td&gt; &#xA;   &lt;td&gt;RBT6, Chinese&lt;/td&gt; &#xA;   &lt;td&gt;75.00&lt;/td&gt; &#xA;   &lt;td&gt;59.68&lt;/td&gt; &#xA;   &lt;td&gt;56.62&lt;/td&gt; &#xA;   &lt;td&gt;73.15&lt;/td&gt; &#xA;   &lt;td&gt;79.26&lt;/td&gt; &#xA;   &lt;td&gt;80.04&lt;/td&gt; &#xA;   &lt;td&gt;73.15&lt;/td&gt; &#xA;   &lt;td&gt;70.99&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6L768H&lt;/td&gt; &#xA;   &lt;td&gt;ERNIE 3.0-Medium&lt;/td&gt; &#xA;   &lt;td&gt;79.93&lt;/td&gt; &#xA;   &lt;td&gt;60.14&lt;/td&gt; &#xA;   &lt;td&gt;57.16&lt;/td&gt; &#xA;   &lt;td&gt;74.56&lt;/td&gt; &#xA;   &lt;td&gt;80.87&lt;/td&gt; &#xA;   &lt;td&gt;81.23&lt;/td&gt; &#xA;   &lt;td&gt;77.02&lt;/td&gt; &#xA;   &lt;td&gt;72.99&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;strong&gt;具体评测方式&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;以上所有任务均基于 Grid Search 方式进行超参寻优。分类任务训练每间隔 100 steps 评估验证集效果，取验证集最优效果作为表格中的汇报指标。&lt;/li&gt; &#xA; &lt;li&gt;分类任务 Grid Search 超参范围: batch_size: 16, 32, 64; learning rates: 1e-5, 2e-5, 3e-5, 5e-5；因为 CLUEWSC2020 数据集较小，所以模型在该数据集上的效果对 batch_size 较敏感，所以对 CLUEWSC2020 评测时额外增加了 batch_size = 8 的超参搜索； 因为CLUEWSC2020 和 IFLYTEK 数据集对 dropout 概率值较为敏感，所以对 CLUEWSC2020 和 IFLYTEK 数据集评测时增加dropout_prob = 0.0 的超参搜索。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;下游任务的固定超参配置&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;分类和匹配任务:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;TASK&lt;/th&gt; &#xA;   &lt;th&gt;AFQMC&lt;/th&gt; &#xA;   &lt;th&gt;TNEWS&lt;/th&gt; &#xA;   &lt;th&gt;IFLYTEK&lt;/th&gt; &#xA;   &lt;th&gt;CMNLI&lt;/th&gt; &#xA;   &lt;th&gt;OCNLI&lt;/th&gt; &#xA;   &lt;th&gt;CLUEWSC2020&lt;/th&gt; &#xA;   &lt;th&gt;CSL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;epoch&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;max_seq_length&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;warmup_proportion&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ERNIE模型Grid Search 最优超参&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;AFQMC&lt;/th&gt; &#xA;   &lt;th&gt;TNEWS&lt;/th&gt; &#xA;   &lt;th&gt;IFLYTEK&lt;/th&gt; &#xA;   &lt;th&gt;CMNLI&lt;/th&gt; &#xA;   &lt;th&gt;OCNLI&lt;/th&gt; &#xA;   &lt;th&gt;CLUEWSC2020&lt;/th&gt; &#xA;   &lt;th&gt;CSL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ERNIE 3.0-Medium&lt;/td&gt; &#xA;   &lt;td&gt;bsz_32_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_3e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_5e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_1e-05/bsz_64_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_64_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_8_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_32_lr_1e-05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ERNIE 3.0-Base&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_64_lr_3e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_5e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_8_lr_2e-05(drop_out _0.1)&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_3e-05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ERNIE 3.0-XBase&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_1e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_3e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_16_lr_1e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_32_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_8_lr_2e-05&lt;/td&gt; &#xA;   &lt;td&gt;bsz_64_lr_1e-05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;应用场景&lt;/h1&gt; &#xA;&lt;p&gt;文本分类（&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/applications/tasks/text_classification&#34;&gt;文本分类&lt;/a&gt;）&lt;/p&gt; &#xA;&lt;p&gt;文本匹配（&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/applications/tasks/text_matching&#34;&gt;文本匹配&lt;/a&gt;）&lt;/p&gt; &#xA;&lt;p&gt;序列标注（&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/applications/tasks/sequence_labeling&#34;&gt;序列标注&lt;/a&gt;）&lt;/p&gt; &#xA;&lt;p&gt;信息抽取（&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/applications/tasks/information_extraction_many_to_many&#34;&gt;信息抽取&lt;/a&gt;）&lt;/p&gt; &#xA;&lt;p&gt;文本生成（&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/applications/tasks/text_generation&#34;&gt;文本生成&lt;/a&gt;）&lt;/p&gt; &#xA;&lt;p&gt;图文匹配（&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/Research/ERNIE-ViL2&#34;&gt;图文匹配&lt;/a&gt;）&lt;/p&gt; &#xA;&lt;p&gt;数据蒸馏（&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/applications/tasks/data_distillation&#34;&gt;数据蒸馏&lt;/a&gt;）&lt;/p&gt; &#xA;&lt;p&gt;工具使用（&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/ernie-kit-open-v1.0/applications/tools&#34;&gt;工具使用&lt;/a&gt;）&lt;/p&gt; &#xA;&lt;h1&gt;文献引用&lt;/h1&gt; &#xA;&lt;h3&gt;ERNIE 1.0&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{sun2019ernie,&#xA;  title={Ernie: Enhanced representation through knowledge integration},&#xA;  author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},&#xA;  journal={arXiv preprint arXiv:1904.09223},&#xA;  year={2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ERNIE 2.0&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{sun2020ernie,&#xA;  title={Ernie 2.0: A continual pre-training framework for language understanding},&#xA;  author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},&#xA;  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},&#xA;  volume={34},&#xA;  number={05},&#xA;  pages={8968--8975},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ERNIE-GEN&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{xiao2020ernie,&#xA;  title={Ernie-gen: An enhanced multi-flow pre-training and fine-tuning framework for natural language generation},&#xA;  author={Xiao, Dongling and Zhang, Han and Li, Yukun and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},&#xA;  journal={arXiv preprint arXiv:2001.11314},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ERNIE-ViL&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{yu2020ernie,&#xA;  title={Ernie-vil: Knowledge enhanced vision-language representations through scene graph},&#xA;  author={Yu, Fei and Tang, Jiji and Yin, Weichong and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},&#xA;  journal={arXiv preprint arXiv:2006.16934},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ERNIE-Gram&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{xiao2020ernie,&#xA;  title={ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding},&#xA;  author={Xiao, Dongling and Li, Yu-Kun and Zhang, Han and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},&#xA;  journal={arXiv preprint arXiv:2010.12148},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ERNIE-Doc&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{ding2020ernie,&#xA;  title={ERNIE-Doc: A retrospective long-document modeling transformer},&#xA;  author={Ding, Siyu and Shang, Junyuan and Wang, Shuohuan and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},&#xA;  journal={arXiv preprint arXiv:2012.15688},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ERNIE-UNIMO&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{li2020unimo,&#xA;  title={Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning},&#xA;  author={Li, Wei and Gao, Can and Niu, Guocheng and Xiao, Xinyan and Liu, Hao and Liu, Jiachen and Wu, Hua and Wang, Haifeng},&#xA;  journal={arXiv preprint arXiv:2012.15409},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ERNIE-M&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{ouyang2020ernie,&#xA;  title={Ernie-m: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora},&#xA;  author={Ouyang, Xuan and Wang, Shuohuan and Pang, Chao and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},&#xA;  journal={arXiv preprint arXiv:2012.15674},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>