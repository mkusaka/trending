<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-18T01:41:18Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Winfredy/SadTalker</title>
    <updated>2023-03-18T01:41:18Z</updated>
    <id>tag:github.com,2023-03-18:/Winfredy/SadTalker</id>
    <link href="https://github.com/Winfredy/SadTalker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ï¼ˆCVPR 2023ï¼‰SadTalkerï¼šLearning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt; ğŸ˜­ SadTalkerï¼š &lt;span style=&#34;font-size:12px&#34;&gt;Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation &lt;/span&gt; &lt;/h2&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12194&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ArXiv-2211.14758-red&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://sadtalker.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;div&gt; &#xA;  &lt;a target=&#34;_blank&#34;&gt;Wenxuan Zhang &lt;sup&gt;*,1,2&lt;/sup&gt; &lt;/a&gt;â€ƒ &#xA;  &lt;a href=&#34;https://vinthony.github.io/&#34; target=&#34;_blank&#34;&gt;Xiaodong Cun &lt;sup&gt;*,2&lt;/sup&gt;&lt;/a&gt;â€ƒ &#xA;  &lt;a href=&#34;https://xuanwangvc.github.io/&#34; target=&#34;_blank&#34;&gt;Xuan Wang &lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;â€ƒ &#xA;  &lt;a href=&#34;https://yzhang2016.github.io/&#34; target=&#34;_blank&#34;&gt;Yong Zhang &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;â€ƒ &#xA;  &lt;a href=&#34;https://xishen0220.github.io/&#34; target=&#34;_blank&#34;&gt;Xi Shen &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;â€ƒ &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://yuguo-xjtu.github.io/&#34; target=&#34;_blank&#34;&gt;Yu Guo&lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;â€ƒ &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=4oXBp9UAAAAJ&#34; target=&#34;_blank&#34;&gt;Ying Shan &lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;â€ƒ &#xA;  &lt;a target=&#34;_blank&#34;&gt;Fei Wang &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;â€ƒ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;sup&gt;1&lt;/sup&gt; Xi&#39;an Jiaotong University â€ƒ &#xA;  &lt;sup&gt;2&lt;/sup&gt; Tencent AI Lab â€ƒ &#xA;  &lt;sup&gt;3&lt;/sup&gt; Ant Group â€ƒ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;i&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12194&#34; target=&#34;_blank&#34;&gt;CVPR 2023&lt;/a&gt;&lt;/strong&gt;&lt;/i&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4397546/222490039-b1f6156b-bf00-405b-9fda-0c9a9156f991.gif&#34; alt=&#34;sadtalker&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;TL;DR: A realistic and stylized talking head video generation method from a single image and audio&lt;/p&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ğŸ“‹ Changelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.03.14 Specify the version of package &lt;code&gt;joblib&lt;/code&gt; to remove the errors in using &lt;code&gt;librosa&lt;/code&gt;, &lt;a href=&#34;https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; is online!&lt;/li&gt; &#xA; &lt;li&gt;2023.03.06 Solve some bugs in code and errors in installation&lt;/li&gt; &#xA; &lt;li&gt;2023.03.03 Release the test code for audio-driven single image animation!&lt;/li&gt; &#xA; &lt;li&gt;2023.02.28 SadTalker has been accepted by CVPR 2023!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ¼ Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4397546/222490596-4c8a2115-49a7-42ad-a2c3-3bb3288a5f36.png&#34; alt=&#34;main_of_sadtalker&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸš§ TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generating 2D face from a single Image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generating 3D face from Audio.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generating 4D free-view talking examples from audio and a single image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio/Colab Demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; training code of each componments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Audio-driven Anime Avatar.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; interpolate ChatGPT for a conversation demo ğŸ¤”&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; integrade with stable-diffusion-web-ui. (stay tunning!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/4397546/222513483-89161f58-83d0-40e4-8e41-96c32b47bd4e.mp4&#34;&gt;https://user-images.githubusercontent.com/4397546/222513483-89161f58-83d0-40e4-8e41-96c32b47bd4e.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ”® Inference Demo!&lt;/h2&gt; &#xA;&lt;h4&gt;Requirements&lt;/h4&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Python 3.8&lt;/li&gt; &#xA;  &lt;li&gt;PyTorch&lt;/li&gt; &#xA;  &lt;li&gt;ffmpeg&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Dependence Installation&lt;/h4&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;git clone https://github.com/Winfredy/SadTalker.git&#xA;cd SadTalker &#xA;conda create -n sadtalker python=3.8&#xA;source activate sadtalker&#xA;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;conda config --add channels conda-forge&#xA;conda install ffmpeg&#xA;pip install ffmpy&#xA;pip install Cmake&#xA;pip install boost&#xA;conda install dlib&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Trained Models&lt;/h4&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;p&gt;Please download our pre-trained model from &lt;a href=&#34;https://drive.google.com/drive/folders/1Wd88VDoLhVzYsQ30_qDVluQr_Xm46yHT?usp=sharing&#34;&gt;google drive&lt;/a&gt; or our &lt;a href=&#34;https://github.com/Winfredy/SadTalker/releases/tag/v0.0.1&#34;&gt;github release page&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;and then, put it in ./checkpoints.&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/auido2exp_00300-model.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained ExpNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/auido2pose_00140-model.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained PoseVAE in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00229-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/facevid2vid_00189-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained face-vid2vid model from &lt;a href=&#34;https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis&#34;&gt;the reappearance of face-vid2vid&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/epoch_20.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained 3DMM extractor in &lt;a href=&#34;https://github.com/microsoft/Deep3DFaceReconstruction&#34;&gt;Deep3DFaceReconstruction&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/wav2lip.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Highly accurate lip-sync model in &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2lip&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/shape_predictor_68_face_landmarks.dat&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face landmark model used in &lt;a href=&#34;http://dlib.net/&#34;&gt;dilb&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/BFM&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;3DMM library file.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/hub&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face detection models used in &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face alignment&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Generating 2D face from a single Image&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --driven_audio &amp;lt;audio.wav&amp;gt; \&#xA;                    --source_image &amp;lt;video.mp4 or picture.png&amp;gt; \&#xA;                    --result_dir &amp;lt;a file to store results&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Generating 3D face from Audio&lt;/h4&gt; &#xA;&lt;p&gt;To do ...&lt;/p&gt; &#xA;&lt;h4&gt;Generating 4D free-view talking examples from audio and a single image&lt;/h4&gt; &#xA;&lt;p&gt;We use &lt;code&gt;camera_yaw&lt;/code&gt;, &lt;code&gt;camera_pitch&lt;/code&gt;, &lt;code&gt;camera_roll&lt;/code&gt; to control camera pose. For example, &lt;code&gt;--camera_yaw -20 30 10&lt;/code&gt; means the camera yaw degree changes from -20 to 30 and then changes from 30 to 10.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --driven_audio &amp;lt;audio.wav&amp;gt; \&#xA;                    --source_image &amp;lt;video.mp4 or picture.png&amp;gt; \&#xA;                    --result_dir &amp;lt;a file to store results&amp;gt; \&#xA;                    --camera_yaw -20 30 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Winfredy/SadTalker/raw/main/free_view_result.gif&#34; alt=&#34;free_view&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ› Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhang2022sadtalker,&#xA;  title={SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation},&#xA;  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},&#xA;  journal={arXiv preprint arXiv:2211.12194},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ’— Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Facerender code borrows heavily from &lt;a href=&#34;https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis&#34;&gt;zhanglonghao&#39;s reproduction of face-vid2vid&lt;/a&gt; and &lt;a href=&#34;https://github.com/RenYurui/PIRender&#34;&gt;PIRender&lt;/a&gt;. We thank the authors for sharing their wonderful code. In training process, We also use the model from &lt;a href=&#34;https://github.com/microsoft/Deep3DFaceReconstruction&#34;&gt;Deep3DFaceReconstruction&lt;/a&gt; and &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2lip&lt;/a&gt;. We thank for their wonderful work.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ¥‚ Related Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/StyleHEAT&#34;&gt;StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN (ECCV 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Doubiiu/CodeTalker&#34;&gt;CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vinthony/video-retalking&#34;&gt;VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild (SIGGRAPH Asia 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.06281&#34;&gt;DPE: Disentanglement of Pose and Expression for General Video Portrait Editing (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/SPI/&#34;&gt;3D GAN Inversion with Facial Symmetry Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mael-zys/T2M-GPT&#34;&gt;T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“¢ Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an official product of Tencent. This repository can only be used for personal/research/non-commercial purposes.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ssbuild/chatglm_finetuning</title>
    <updated>2023-03-18T01:41:18Z</updated>
    <id>tag:github.com,2023-03-18:/ssbuild/chatglm_finetuning</id>
    <link href="https://github.com/ssbuild/chatglm_finetuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;chatglm 6b å¤§æ¨¡å‹å¾®è°ƒ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;çŠ¶æ€ - æµ‹è¯•ä¸­&lt;/h1&gt; &#xA;&lt;h2&gt;å®‰è£…&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pip install -U deep_training &amp;gt;= 0.0.18 cpm_kernels icetk transformers&amp;gt;=4.26.1 deepspeed&lt;/li&gt; &#xA; &lt;li&gt;æœ€å°ç‰ˆæœ¬è¦æ±‚ deep_training&amp;gt;=0.0.18.post5&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ›´æ–°è¯¦æƒ…&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ssbuild/deep_training&#34;&gt;deep_training&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ·±åº¦å­¦ä¹ å¸¸è§„ä»»åŠ¡ä¾‹å­&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ssbuild/pytorch-task-example&#34;&gt;pytorch-task-example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ssbuild/tf-task-example&#34;&gt;tf-task-example&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ChatGLM é¢„è®­ç»ƒæƒé‡&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://huggingface.co/THUDM/chatglm-6b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;æ•°æ®ç¤ºä¾‹&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;ç»™å‡ºä¸€ç§qa æ•°æ®ç¤ºä¾‹ , å¦‚æœæƒ³è·Ÿæ¯æ¨¡å‹ä¿æŒä¸€è‡´ï¼Œå¯ä»¥å‚è€ƒhttps://huggingface.co/THUDM/chatglm-6b æ•°æ®ç»„ç»‡ç»“æ„.&#xA;å•æ¡æ•°æ®ç¤ºä¾‹1&#xA;{&#xA;    &#34;id&#34;: 0, &#34;paragraph&#34;: [&#xA;        #ä¸€è½®ä¼šè¯&#xA;        {&#xA;            &#34;q&#34;: &#34;ä»å—äº¬åˆ°ä¸Šæµ·çš„è·¯çº¿&#34;,&#xA;            &#34;a&#34;: [&#xA;                &#34;ä½ å¥½ï¼Œå—äº¬åˆ°ä¸Šæµ·çš„è·¯çº¿å¦‚ä¸‹ï¼š&#34;,&#xA;                &#34;1. å—äº¬åˆ°ä¸Šæµ·ï¼Œå¯ä»¥ä¹˜åå—äº¬åœ°é“1å·çº¿ï¼Œåœ¨å—äº¬ç«™ä¹˜åè½¨é“äº¤é€š1å·çº¿ã€‚&#34;,&#xA;                &#34;2. å—äº¬åˆ°æµ¦ä¸œæœºåœºï¼Œå¯ä»¥æ­ä¹˜ä¸Šæµ·åœ°é“1å·ï¼Œåœ¨é™†å®¶å˜´ç«™ä¹˜ååœ°é“1çº¿ï¼Œåœ¨æµ¦ä¸œå›½é™…æœºåœºç«™ä¹˜åæœºåœºå¿«çº¿ï¼Œå‰å¾€ä¸Šæµ·æµ¦ä¸œå›½é™…æœºåœºã€‚&#34;,&#xA;                &#34;3. ä¸Šæµ·åˆ°å—äº¬ï¼Œå¯ä»¥æ¢ä¹˜ä¸Šæµ·åœ°é“2å·çº¿ï¼Œä»å—äº¬ç«™æ¢ä¹˜åœ°é“2çº¿ï¼Œå†ä»å—äº¬å—ç«™æ¢ä¹˜åœ°é“1è·¯ï¼Œç„¶ååˆ°è¾¾ä¸Šæµ·ç«™&#34;&#xA;            ]&#xA;        }&#xA;        #äºŒè½®....&#xA;    ]&#xA;}&#xA;å•æ¡æ•°æ®ç¤ºä¾‹2&#xA;{&#34;id&#34;: 0, &#34;paragraph&#34;: [&#xA;&#xA;    {&#xA;        &#34;q&#34;: &#34;å†™ä¸€ä¸ªè¯—æ­Œï¼Œå…³äºå†¬å¤©&#34;,&#xA;        &#34;a&#34;: [&#xA;            &#34;å†¬å¤œå¯‚é™å†·ï¼Œ&#34;,&#xA;             &#34;äº‘åœ¨å¤©è¾¹é£˜ï¼Œ&#34;, &#34;å†°å°ç™½é›ªä¸Šï¼Œ &#34;, &#34;å¯’å†·åƒä¸€åœºé›ªã€‚&#34;,&#xA;             &#34; &#34;,&#xA;             &#34;é›ªèŠ±èåŒ–æˆå†°ï¼Œ&#34;,&#xA;             &#34;åƒé‚£é›ªèŠ±é£˜æ´’ï¼Œ&#34;,&#xA;             &#34;åœ¨å¯’å†·çš„å†¬å¤©ï¼Œ&#34;,&#xA;             &#34;æ„Ÿå—æ˜¥å¤©çš„å–œæ‚¦ã€‚&#34;,&#xA;             &#34; å†¬æ—¥é‡Œï¼Œ&#34;,&#xA;             &#34;é£é›ªæ¸æ¶ˆï¼Œ&#34;,&#xA;             &#34;ä¸€ç‰‡å¯‚é™ï¼Œ&#34;,&#xA;             &#34;æŠŠå¿«ä¹å’Œæ¸©æš–å¸¦å›å®¶ã€‚&#34;&#xA;        ]&#xA;    }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ç”Ÿæˆè®­ç»ƒrecord&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python data_utils.py&#xA;&#xA;æ³¨:&#xA;num_process_worker ä¸ºå¤šè¿›ç¨‹åˆ¶ä½œæ•°æ® ï¼Œ å¦‚æœæ•°æ®é‡è¾ƒå¤§ ï¼Œ é€‚å½“è°ƒå¤§è‡³cpuæ•°é‡&#xA;dataHelper.make_dataset_with_args(data_args.train_file,mixed_data=False, shuffle=True,mode=&#39;train&#39;,num_process_worker=0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;æ¨ç†&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt; python infer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ç¡¬ä»¶éœ€æ±‚&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;é‡åŒ–ç­‰çº§&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;æœ€ä½ GPU æ˜¾å­˜&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP16ï¼ˆæ— é‡åŒ–ï¼‰&lt;/td&gt; &#xA;   &lt;td&gt;13 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;10 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;6 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ssbuild/chatglm_finetuning/main/1.png&#34; alt=&#34;inference&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;è®­ç»ƒ&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;å®Œæ•´å‚æ•°ä¸º config.json&#xA;è‹¥æ˜¾å­˜ä¸è¶³ ï¼Œ å¯ä»¥ä¿®æ”¹ config_small.json num_layers å±‚æ•°&#xA;è®­ç»ƒç²¾åº¦ å¯ä»¥ä¿®æ”¹ config_small.json precision 16 32&#xA;python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;æ˜¯å¦å¼€å¯deepspeed&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;å¯åŠ¨åˆ™å°†train.py  æ³¨é‡Šæ‰ deepspeed_config = None&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/THUDM/ChatGLM-6B&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/vision</title>
    <updated>2023-03-18T01:41:18Z</updated>
    <id>tag:github.com,2023-03-18:/pytorch/vision</id>
    <link href="https://github.com/pytorch/vision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Datasets, Transforms and Models specific to Computer Vision&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;torchvision&lt;/h1&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://pepy.tech/badge/torchvision&#34;&gt;https://pepy.tech/badge/torchvision&lt;/a&gt; :target: &lt;a href=&#34;https://pepy.tech/project/torchvision&#34;&gt;https://pepy.tech/project/torchvision&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/badge/dynamic/json.svg?label=docs&amp;amp;url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&amp;amp;query=%24.info.version&amp;amp;colorB=brightgreen&amp;amp;prefix=v&#34;&gt;https://img.shields.io/badge/dynamic/json.svg?label=docs&amp;amp;url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&amp;amp;query=%24.info.version&amp;amp;colorB=brightgreen&amp;amp;prefix=v&lt;/a&gt; :target: &lt;a href=&#34;https://pytorch.org/vision/stable/index.html&#34;&gt;https://pytorch.org/vision/stable/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;We recommend Anaconda as Python package management system. Please refer to &lt;code&gt;pytorch.org &amp;lt;https://pytorch.org/&amp;gt;&lt;/code&gt;_ for the detail of PyTorch (&lt;code&gt;torch&lt;/code&gt;) installation. The following is the corresponding &lt;code&gt;torchvision&lt;/code&gt; versions and supported Python versions.&lt;/p&gt; &#xA;&lt;p&gt;+--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;torch&lt;/code&gt; | &lt;code&gt;torchvision&lt;/code&gt; | &lt;code&gt;python&lt;/code&gt; | +==========================+==========================+=================================+ | &lt;code&gt;main&lt;/code&gt; / &lt;code&gt;nightly&lt;/code&gt; | &lt;code&gt;main&lt;/code&gt; / &lt;code&gt;nightly&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.11&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;2.0.0&lt;/code&gt; | &lt;code&gt;0.15.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.11&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.13.0&lt;/code&gt; | &lt;code&gt;0.14.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.7.2&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.10&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.12.0&lt;/code&gt; | &lt;code&gt;0.13.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.7&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.10&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.11.0&lt;/code&gt; | &lt;code&gt;0.12.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.7&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.10&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.10.2&lt;/code&gt; | &lt;code&gt;0.11.3&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.10.1&lt;/code&gt; | &lt;code&gt;0.11.2&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.10.0&lt;/code&gt; | &lt;code&gt;0.11.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.9.1&lt;/code&gt; | &lt;code&gt;0.10.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.9.0&lt;/code&gt; | &lt;code&gt;0.10.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.8.2&lt;/code&gt; | &lt;code&gt;0.9.2&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.8.1&lt;/code&gt; | &lt;code&gt;0.9.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.8.0&lt;/code&gt; | &lt;code&gt;0.9.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.7.1&lt;/code&gt; | &lt;code&gt;0.8.2&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.7.0&lt;/code&gt; | &lt;code&gt;0.8.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.7.0&lt;/code&gt; | &lt;code&gt;0.8.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.6.0&lt;/code&gt; | &lt;code&gt;0.7.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.5.1&lt;/code&gt; | &lt;code&gt;0.6.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.5.0&lt;/code&gt; | &lt;code&gt;0.6.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.4.0&lt;/code&gt; | &lt;code&gt;0.5.0&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.3.1&lt;/code&gt; | &lt;code&gt;0.4.2&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.3.0&lt;/code&gt; | &lt;code&gt;0.4.1&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.2.0&lt;/code&gt; | &lt;code&gt;0.4.0&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.1.0&lt;/code&gt; | &lt;code&gt;0.3.0&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;&amp;lt;=1.0.1&lt;/code&gt; | &lt;code&gt;0.2.2&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+&lt;/p&gt; &#xA;&lt;p&gt;Anaconda:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install torchvision -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;pip:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install torchvision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From source:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py install&#xA;# or, for OSX&#xA;# MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We don&#39;t officially support building from source using &lt;code&gt;pip&lt;/code&gt;, but &lt;em&gt;if&lt;/em&gt; you do, you&#39;ll need to use the &lt;code&gt;--no-build-isolation&lt;/code&gt; flag. In case building TorchVision from source fails, install the nightly version of PyTorch following the linked guide on the &lt;code&gt;contributing page &amp;lt;https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md#development-installation&amp;gt;&lt;/code&gt;_ and retry the install.&lt;/p&gt; &#xA;&lt;p&gt;By default, GPU support is built if CUDA is found and &lt;code&gt;torch.cuda.is_available()&lt;/code&gt; is true. It&#39;s possible to force building GPU support by setting &lt;code&gt;FORCE_CUDA=1&lt;/code&gt; environment variable, which is useful when building a docker image.&lt;/p&gt; &#xA;&lt;h1&gt;Image Backend&lt;/h1&gt; &#xA;&lt;p&gt;Torchvision currently supports the following image backends:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Pillow&lt;/code&gt;_ (default)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Pillow-SIMD&lt;/code&gt;_ - a &lt;strong&gt;much faster&lt;/strong&gt; drop-in replacement for Pillow with SIMD. If installed will be used as the default.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;accimage&lt;/code&gt;_ - if installed can be activated by calling :code:&lt;code&gt;torchvision.set_image_backend(&#39;accimage&#39;)&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;libpng&lt;/code&gt;_ - can be installed via conda :code:&lt;code&gt;conda install libpng&lt;/code&gt; or any of the package managers for debian-based and RHEL-based Linux distributions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;libjpeg&lt;/code&gt;_ - can be installed via conda :code:&lt;code&gt;conda install jpeg&lt;/code&gt; or any of the package managers for debian-based and RHEL-based Linux distributions. &lt;code&gt;libjpeg-turbo&lt;/code&gt;_ can be used as well.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt; &lt;code&gt;libpng&lt;/code&gt; and &lt;code&gt;libjpeg&lt;/code&gt; must be available at compilation time in order to be available. Make sure that it is available on the standard library locations, otherwise, add the include and library paths in the environment variables &lt;code&gt;TORCHVISION_INCLUDE&lt;/code&gt; and &lt;code&gt;TORCHVISION_LIBRARY&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;p&gt;.. _libpng : &lt;a href=&#34;http://www.libpng.org/pub/png/libpng.html&#34;&gt;http://www.libpng.org/pub/png/libpng.html&lt;/a&gt; .. _Pillow : &lt;a href=&#34;https://python-pillow.org/&#34;&gt;https://python-pillow.org/&lt;/a&gt; .. _Pillow-SIMD : &lt;a href=&#34;https://github.com/uploadcare/pillow-simd&#34;&gt;https://github.com/uploadcare/pillow-simd&lt;/a&gt; .. _accimage: &lt;a href=&#34;https://github.com/pytorch/accimage&#34;&gt;https://github.com/pytorch/accimage&lt;/a&gt; .. _libjpeg: &lt;a href=&#34;http://ijg.org/&#34;&gt;http://ijg.org/&lt;/a&gt; .. _libjpeg-turbo: &lt;a href=&#34;https://libjpeg-turbo.org/&#34;&gt;https://libjpeg-turbo.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Video Backend&lt;/h1&gt; &#xA;&lt;p&gt;Torchvision currently supports the following video backends:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pyav&lt;/code&gt;_ (default) - Pythonic binding for ffmpeg libraries.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;.. _pyav : &lt;a href=&#34;https://github.com/PyAV-Org/PyAV&#34;&gt;https://github.com/PyAV-Org/PyAV&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;video_reader - This needs ffmpeg to be installed and torchvision to be built from source. There shouldn&#39;t be any conflicting version of ffmpeg installed. Currently, this is only supported on Linux.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; conda install -c conda-forge ffmpeg&#xA; python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Using the models on C++&lt;/h1&gt; &#xA;&lt;p&gt;TorchVision provides an example project for how to use the models on C++ using JIT Script.&lt;/p&gt; &#xA;&lt;p&gt;Installation From source:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir build&#xA;cd build&#xA;# Add -DWITH_CUDA=on support for the CUDA if needed&#xA;cmake ..&#xA;make&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once installed, the library can be accessed in cmake (after properly configuring &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt;) via the :code:&lt;code&gt;TorchVision::TorchVision&lt;/code&gt; target:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: rest&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;find_package(TorchVision REQUIRED)&#xA;target_link_libraries(my-target PUBLIC TorchVision::TorchVision)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;TorchVision&lt;/code&gt; package will also automatically look for the &lt;code&gt;Torch&lt;/code&gt; package and add it as a dependency to &lt;code&gt;my-target&lt;/code&gt;, so make sure that it is also available to cmake via the &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For an example setup, take a look at &lt;code&gt;examples/cpp/hello_world&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Python linking is disabled by default when compiling TorchVision with CMake, this allows you to run models without any Python dependency. In some special cases where TorchVision&#39;s operators are used from Python code, you may need to link to Python. This can be done by passing &lt;code&gt;-DUSE_PYTHON=on&lt;/code&gt; to CMake.&lt;/p&gt; &#xA;&lt;h2&gt;TorchVision Operators&lt;/h2&gt; &#xA;&lt;p&gt;In order to get the torchvision operators registered with torch (eg. for the JIT), all you need to do is to ensure that you :code:&lt;code&gt;#include &amp;lt;torchvision/vision.h&amp;gt;&lt;/code&gt; in your project.&lt;/p&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;You can find the API documentation on the pytorch website: &lt;a href=&#34;https://pytorch.org/vision/stable/index.html&#34;&gt;https://pytorch.org/vision/stable/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;See the &lt;code&gt;CONTRIBUTING &amp;lt;CONTRIBUTING.md&amp;gt;&lt;/code&gt;_ file for how to help out.&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer on Datasets&lt;/h1&gt; &#xA;&lt;p&gt;This is a utility library that downloads and prepares public datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have license to use the dataset. It is your responsibility to determine whether you have permission to use the dataset under the dataset&#39;s license.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re a dataset owner and wish to update any part of it (description, citation, etc.), or do not want your dataset to be included in this library, please get in touch through a GitHub issue. Thanks for your contribution to the ML community!&lt;/p&gt; &#xA;&lt;h1&gt;Pre-trained Model License&lt;/h1&gt; &#xA;&lt;p&gt;The pre-trained models provided in this library may have their own licenses or terms and conditions derived from the dataset used for training. It is your responsibility to determine whether you have permission to use the models for your use case.&lt;/p&gt; &#xA;&lt;p&gt;More specifically, SWAG models are released under the CC-BY-NC 4.0 license. See &lt;code&gt;SWAG LICENSE &amp;lt;https://github.com/facebookresearch/SWAG/blob/main/LICENSE&amp;gt;&lt;/code&gt;_ for additional details.&lt;/p&gt; &#xA;&lt;h1&gt;Citing TorchVision&lt;/h1&gt; &#xA;&lt;p&gt;If you find TorchVision useful in your work, please consider citing the following BibTeX entry:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bibtex&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{torchvision2016,&#xA;    title        = {TorchVision: PyTorch&#39;s Computer Vision library},&#xA;    author       = {TorchVision maintainers and contributors},&#xA;    year         = 2016,&#xA;    journal      = {GitHub repository},&#xA;    publisher    = {GitHub},&#xA;    howpublished = {\url{https://github.com/pytorch/vision}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>