<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-10T01:41:30Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ECTO-1A/AppleJuice</title>
    <updated>2023-09-10T01:41:30Z</updated>
    <id>tag:github.com,2023-09-10:/ECTO-1A/AppleJuice</id>
    <link href="https://github.com/ECTO-1A/AppleJuice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apple BLE proximity pairing message spoofing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AppleJuice&lt;/h1&gt; &#xA;&lt;h4&gt;Apple BLE Proximity Pairing Message Spoofing&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;&lt;span&gt;üî¥&lt;/span&gt; Disclaimer&lt;/h3&gt; &#xA; &lt;p&gt;These scripts are an experimental PoC that uses Bluetooth Low Energy (BLE) to send proximity pairing messages to Apple devices.&lt;br&gt; This project is created for educational purposes and cannot be used for law violation or personal gain. The author of this project is not responsible for any possible harm caused by the materials of this project.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;h3&gt;Flipper Zero&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to the amazing work of &lt;a href=&#34;https://github.com/Willy-JL/Willy-JL&#34;&gt;Willy-JL&lt;/a&gt; this has been added to the &lt;a href=&#34;https://github.com/Flipper-XFW/Xtreme-Firmware&#34;&gt;Flipper Zero Xtreme Firmware&lt;/a&gt;. It&#39;s currently in dev and will be officially released in the next update.&lt;/p&gt; &#xA;&lt;h4&gt;To install it now, follow the guide below from the Xtreme Firmware page to clone and compile the current dev build that contains the Apple BLE Spam app.&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; &lt;strong&gt;Warning!&lt;/strong&gt; &lt;br&gt; We will not give basic support for compiling in our server. This is intended for people that already &lt;em&gt;know&lt;/em&gt; what they are doing!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;To download the needed tools:&#xA;$ git clone --recursive --jobs 8 https://github.com/Flipper-XFW/Xtreme-Firmware.git&#xA;$ cd Xtreme-Firmware/&#xA;&#xA;To flash directly to the Flipper (Needs to be connected via USB, qFlipper closed)&#xA;$ ./fbt flash_usb_full&#xA;&#xA;To compile a TGZ package&#xA;$ ./fbt updater_package&#xA;&#xA;To build and launch a single app:&#xA;$ ./fbt launch APPSRC=some_appid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ESP-32&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/ronaldstoner&#34;&gt;ronaldstoner&lt;/a&gt; for porting this over to the ESP-32&lt;/p&gt; &#xA;&lt;h2&gt;About This Project&lt;/h2&gt; &#xA;&lt;p&gt;This was created in response to the various AppleTV spoof messages being sent out during &lt;a href=&#34;https://techcrunch.com/2023/08/14/researcher-says-they-were-behind-iphone-popups-at-def-con/&#34;&gt;DEF CON 31&lt;/a&gt;. After experiencing it first hand, I had to figure out what was happening. The existing research projects I could find (see &lt;em&gt;credits&lt;/em&gt;) had great info but were both a couple years out of date with broken package dependencies, so I decided to take what I could from them and start building from there.&lt;/p&gt; &#xA;&lt;h2&gt;Hardware Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To run these scripts you need a Linux machine with an internal Bluetooth card or a USB Bluetooth adapter.&lt;/p&gt; &#xA;&lt;p&gt;All original testing was done on a Lenovo T480 with a built-in Bluetooth adapter.&lt;br&gt; Later tested on Raspberry Pi 3B+ and Raspberry Pi Zero W running Kali Linux with a &lt;a href=&#34;https://zexmte.com/collections/bluetooth-adapter/products/plug-play-long-range-bluetooth-5-1-usb-adapter&#34;&gt;Zexmte Long Range USB Bluetooth 5.1 Adapter with Dual Antenna&lt;/a&gt;.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ECTO-1A/AppleJuice/assets/112792126/a6f2b9fa-ca26-45c1-a440-681beb55c76e&#34; width=&#34;300&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Range&lt;/strong&gt; &lt;br&gt; Range of messages by device type&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Device&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Range&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Lenovo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Couple feet from machine&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Raspberry Pi and long range adapter&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20+ feet indoors in heavy BLE traffic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Please follow in this exact order or you might run into issues with bluetooth dependencies.&lt;/p&gt; &#xA;&lt;h3&gt;Clone the Main Repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ECTO-1A/AppleJuice.git &amp;amp;&amp;amp; cd ./AppleJuice&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update &amp;amp;&amp;amp; sudo apt install -y bluez libpcap-dev libev-dev libnl-3-dev libnl-genl-3-dev libnl-route-3-dev cmake libbluetooth-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dependencies requiring manual installation&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; &lt;strong&gt;Warning&lt;/strong&gt; &lt;br&gt; The &lt;code&gt;pybluez&lt;/code&gt; library is broken on GitHub and needs to be installed manually&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Download the latest version &#xA;pip install git+https://github.com/pybluez/pybluez.git#egg=pybluez&#xA;&#xA;pycrypto is not maintained, be sure to install pycryptodome instead &#xA;pip install pycryptodome&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install requirements&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Execute scripts without &lt;code&gt;sudo&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To be able to run without sudo, you need to set the capabilities of the python binary to allow it to access raw sockets. This is done with the following command&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo setcap cap_net_raw,cap_net_admin+eip $(eval readlink -f $(which python))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reboot Machine&lt;/h3&gt; &#xA;&lt;p&gt;Several users have reported the need for a reboot after installing the bluetooth packages in order for everything to work properly.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h4&gt;Before running the script, check that your Bluetooth adapter is connected and showing as &lt;code&gt;hci0&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Run &lt;code&gt;hcitool dev&lt;/code&gt; to get a list of connected adapters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hcitool dev&#xA;Devices:&#xA;    hci0    00:00:7C:00:3A:13&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üìù&lt;/span&gt; &lt;strong&gt;Note&lt;/strong&gt; &lt;br&gt; If the adapter is showing as &lt;code&gt;hci1&lt;/code&gt; you will need to edit the &lt;code&gt;dev_id&lt;/code&gt; variable in the scripts to match&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Available options&lt;/h3&gt; &#xA;&lt;p&gt;All messages have been combined into a single app. You can now run &lt;code&gt;app.py&lt;/code&gt; to get a list of available options.&lt;br&gt; To run the script use &lt;code&gt;-d (number of message)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; &lt;br&gt; &lt;code&gt;app.py -d 13&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python3 app.py&#xA;Please select a message option using -d.&#xA;Available message options:&#xA;1: Airpods&#xA;2: Airpods Pro&#xA;3: Airpods Max&#xA;4: Airpods Gen 2&#xA;5: Airpods Gen 3&#xA;6: Airpods Pro Gen 2&#xA;7: PowerBeats&#xA;8: PowerBeats Pro&#xA;9: Beats Solo Pro&#xA;10: Beats Studio Buds&#xA;11: Beats Flex&#xA;12: BeatsX&#xA;13: Beats Solo3&#xA;14: Beats Studio3&#xA;15: Beats Studio Pro&#xA;16: Beats Fit Pro&#xA;17: Beats Studio Buds+&#xA;18: AppleTV Setup&#xA;19: AppleTV Pair&#xA;20: AppleTV New User&#xA;21: AppleTV AppleID Setup&#xA;22: AppleTV Wireless Audio Sync&#xA;23: AppleTV Homekit Setup&#xA;24: AppleTV Keyboard&#xA;25: AppleTV &#39;Connecting to Network&#39;&#xA;26: Homepod Setup&#xA;27: Setup New Phone&#xA;28: Transfer Number to New Phone&#xA;29: TV Color Balance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;beatssolopro.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: Beats Solo Pro&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;img src=&#34;https://github.com/ECTO-1A/AppleJuice/assets/112792126/c3218a09-7aef-483b-957d-f3c19a55fc08&#34; width=&#34;300&#34;&gt; &#xA;&lt;p&gt;&lt;code&gt;airpods_max.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: Airpods Max&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;img src=&#34;https://github.com/ECTO-1A/AppleJuice/assets/112792126/5eea40e8-d7c1-4324-9f3d-1425228d0458&#34; width=&#34;300&#34;&gt; &#xA;&lt;h3&gt;Credit&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/furiousMAC/continuity&#34;&gt;FuriousMAC&lt;/a&gt; and &lt;a href=&#34;https://github.com/hexway/apple_bleee&#34;&gt;Hexway&lt;/a&gt; for all the prior research on Apple BLE, Continuity, and building the Wireshark disector.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://infosec.exchange/@jb0x168/110879394826675242&#34;&gt;Jae Bochs&lt;/a&gt; for &lt;a href=&#34;https://techcrunch.com/2023/08/14/researcher-says-they-were-behind-iphone-popups-at-def-con/&#34;&gt;exposing this to me at DEF CON 31&lt;/a&gt; which made me jump into learning about BLE.&lt;/li&gt; &#xA; &lt;li&gt;Guillaume Celosia and Mathieu Cunche for reverse engineering &lt;a href=&#34;https://petsymposium.org/2020/files/papers/issue1/popets-2020-0003.pdf%22&#34;&gt;Proximity Pairing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>XPixelGroup/DiffBIR</title>
    <updated>2023-09-10T01:41:30Z</updated>
    <id>tag:github.com,2023-09-10:/XPixelGroup/DiffBIR</id>
    <link href="https://github.com/XPixelGroup/DiffBIR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/logo.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.15070&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://0x3f3f3f3fun.github.io/projects/diffbir/&#34;&gt;Project Page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://visitor-badge.laobi.icu/badge?page_id=XPixelGroup/DiffBIR&#34; alt=&#34;visitors&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/camenduru/DiffBIR-colab/blob/main/DiffBIR_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://0x3f3f3f3fun.github.io/&#34;&gt;Xinqi Lin&lt;/a&gt;&lt;sup&gt;1,*&lt;/sup&gt;, &lt;a href=&#34;https://github.com/hejingwenhejingwen&#34;&gt;Jingwen He&lt;/a&gt;&lt;sup&gt;2,*&lt;/sup&gt;, &lt;a href=&#34;https://orcid.org/0000-0001-6277-5635&#34;&gt;Ziyan Chen&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com.tw/citations?user=gkXFhbwAAAAJ&amp;amp;hl=en&#34;&gt;Zhaoyang Lyu&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=skQROj8AAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Ben Fei&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;http://daibo.info/&#34;&gt;Bo Dai&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://wlouyang.github.io/&#34;&gt;Wanli Ouyang&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;http://mmlab.siat.ac.cn/yuqiao&#34;&gt;Yu Qiao&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;http://xpixel.group/2010/01/20/chaodong.html&#34;&gt;Chao Dong&lt;/a&gt;&lt;sup&gt;1,2&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences&lt;br&gt;&lt;sup&gt;2&lt;/sup&gt;Shanghai AI Laboratory&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/architecture.png&#34; style=&#34;border-radius: 15px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt;If DiffBIR is helpful for you, please help star this repo. Thanks!&lt;span&gt;ü§ó&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt;Table Of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/#visual_results&#34;&gt;Visual Results On Real-world Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/#pretrained_models&#34;&gt;Pretrained Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/#quick_start&#34;&gt;Quick Start (gradio demo)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/#train&#34;&gt;Train&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/#update&#34;&gt;Update&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/#todo&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;visual_results&#34;&gt;&lt;/a&gt;&lt;span&gt;üëÄ&lt;/span&gt;Visual Results On Real-world Images&lt;/h2&gt; &#xA;&lt;!-- &lt;details close&gt;&#xA;&lt;summary&gt;General Image Restoration&lt;/summary&gt; --&gt; &#xA;&lt;h3&gt;General Image Restoration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MTk5ODI3&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/general6.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTk5ODI4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/general7.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTk5ODI1&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/general4.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MTk5ODIy&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/general1.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTk5ODIz&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/general2.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MTk5ODI0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/general3.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MjAxMjM0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/general5.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- &lt;/details&gt; --&gt; &#xA;&lt;!-- &lt;details close&gt; --&gt; &#xA;&lt;!-- &lt;summary&gt;Face Image Restoration&lt;/summary&gt; --&gt; &#xA;&lt;h3&gt;Face Image Restoration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MTk5ODI5&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/face1.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTk5ODMw&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/face2.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTk5ODMy&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/face3.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MTk5ODM0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/face4.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTk5ODM1&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/face5.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTk5ODM2&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/face6.png&#34; height=&#34;223px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MjA0MzQw&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/visual_results/whole_image1.png&#34; height=&#34;410px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- &lt;/details&gt; --&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;installation&#34;&gt;&lt;/a&gt;&lt;span&gt;‚öô&lt;/span&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt; &amp;gt;= 3.9&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CUDA&lt;/strong&gt; &amp;gt;= 11.3&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt; &amp;gt;= 1.12.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;xformers&lt;/strong&gt; == 0.0.16&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# clone this repo&#xA;git clone https://github.com/XPixelGroup/DiffBIR.git&#xA;cd DiffBIR&#xA;&#xA;# create a conda environment with python &amp;gt;= 3.9&#xA;conda create -n diffbir python=3.9&#xA;conda activate diffbir&#xA;&#xA;conda install pytorch==1.12.1 torchvision==0.13.1 cudatoolkit=11.3 -c pytorch&#xA;conda install xformers==0.0.16 -c xformers&#xA;&#xA;# other dependencies&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;pretrained_models&#34;&gt;&lt;/a&gt;&lt;span&gt;üß¨&lt;/span&gt;Pretrained Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/general_swinir_v1.ckpt&#34;&gt;general_swinir_v1.ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage1 model (SwinIR) for general image restoration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/general_full_v1.ckpt&#34;&gt;general_full_v1.ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Full model for general image restoration. &#34;Full&#34; means it contains both the stage1 and stage2 model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/face_swinir_v1.ckpt&#34;&gt;face_swinir_v1.ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage1 model (SwinIR) for face restoration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/face_full_v1.ckpt&#34;&gt;face_full_v1.ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Full model for face restoration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;quick_start&#34;&gt;&lt;/a&gt;&lt;span&gt;üõ´&lt;/span&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/general_full_v1.ckpt&#34;&gt;general_full_v1.ckpt&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/general_swinir_v1.ckpt&#34;&gt;general_swinir_v1.ckpt&lt;/a&gt; to &lt;code&gt;weights/&lt;/code&gt;, then run the following command to interact with the gradio website.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_diffbir.py \&#xA;--ckpt weights/general_full_v1.ckpt \&#xA;--config configs/model/cldm.yaml \&#xA;--reload_swinir \&#xA;--swinir_ckpt weights/general_swinir_v1.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;kbd&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/gradio.png&#34;&gt;&lt;/kbd&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;inference&#34;&gt;&lt;/a&gt;&lt;span&gt;‚öî&lt;/span&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Full Pipeline (Remove Degradations &amp;amp; Refine Details)&lt;/h3&gt; &#xA;&lt;h4&gt;General Image&lt;/h4&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/general_full_v1.ckpt&#34;&gt;general_full_v1.ckpt&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/general_swinir_v1.ckpt&#34;&gt;general_swinir_v1.ckpt&lt;/a&gt; to &lt;code&gt;weights/&lt;/code&gt; and run the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;--input inputs/general \&#xA;--config configs/model/cldm.yaml \&#xA;--ckpt weights/general_full_v1.ckpt \&#xA;--reload_swinir --swinir_ckpt weights/general_swinir_v1.ckpt \&#xA;--steps 50 \&#xA;--sr_scale 4 \&#xA;--image_size 512 \&#xA;--color_fix_type wavelet --resize_back \&#xA;--output results/general&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are confused about where the &lt;code&gt;reload_swinir&lt;/code&gt; option came from, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/#degradation-details&#34;&gt;degradation details&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Face Image&lt;/h4&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/face_full_v1.ckpt&#34;&gt;face_full_v1.ckpt&lt;/a&gt; to &lt;code&gt;weights/&lt;/code&gt; and run the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# for aligned face inputs&#xA;python inference_face.py \&#xA;--config configs/model/cldm.yaml \&#xA;--ckpt weights/face_full_v1.ckpt \&#xA;--input inputs/face/aligned \&#xA;--steps 50 \&#xA;--sr_scale 1 \&#xA;--image_size 512 \&#xA;--color_fix_type wavelet \&#xA;--output results/face/aligned --resize_back \&#xA;--has_aligned&#xA;&#xA;# for unaligned face inputs&#xA;python inference_face.py \&#xA;--config configs/model/cldm.yaml \&#xA;--ckpt weights/face_full_v1.ckpt \&#xA;--input inputs/face/whole_img \&#xA;--steps 50 \&#xA;--sr_scale 1 \&#xA;--image_size 512 \&#xA;--color_fix_type wavelet \&#xA;--output results/face/whole_img --resize_back&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Only Stage1 Model (Remove Degradations)&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/general_swinir_v1.ckpt&#34;&gt;general_swinir_v1.ckpt&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/lxq007/DiffBIR/resolve/main/face_swinir_v1.ckpt&#34;&gt;face_swinir_v1.ckpt&lt;/a&gt; for general, face image respectively, and run the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/inference_stage1.py \&#xA;--config configs/model/swinir.yaml \&#xA;--ckpt [swinir_ckpt_path] \&#xA;--input [lq_dir] \&#xA;--sr_scale 1 --image_size 512 \&#xA;--output [output_dir_path]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Only Stage2 Model (Refine Details)&lt;/h3&gt; &#xA;&lt;p&gt;Since the proposed two-stage pipeline is very flexible, you can utilize other awesome models to remove degradations instead of SwinIR and then leverage the Stable Diffusion to refine details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# step1: Use other models to remove degradations and save results in [img_dir_path].&#xA;&#xA;# step2: Refine details of step1 outputs.&#xA;python inference.py \&#xA;--config configs/model/cldm.yaml \&#xA;--ckpt [full_ckpt_path] \&#xA;--steps 50 --sr_scale 1 --image_size 512 \&#xA;--input [img_dir_path] \&#xA;--color_fix_type wavelet --resize_back \&#xA;--output [output_dir_path] \&#xA;--disable_preprocess_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;train&#34;&gt;&lt;/a&gt;&lt;span&gt;üå†&lt;/span&gt;Train&lt;/h2&gt; &#xA;&lt;h3&gt;Degradation Details&lt;/h3&gt; &#xA;&lt;p&gt;For general image restoration, we first train both the stage1 and stage2 model under codeformer degradation to enhance the generative capacity of the stage2 model. In order to improve the ability for degradation removal, we train another stage1 model under Real-ESRGAN degradation and utilize it during inference.&lt;/p&gt; &#xA;&lt;p&gt;For face image restoration, we adopt the degradation model used in &lt;a href=&#34;https://github.com/zsyOAOA/DifFace/raw/master/configs/training/swinir_ffhq512.yaml&#34;&gt;DifFace&lt;/a&gt; for training and directly utilize the SwinIR model released by them as our stage1 model.&lt;/p&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate file list of training set and validation set.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/make_file_list.py \&#xA;--img_folder [hq_dir_path] \&#xA;--val_size [validation_set_size] \&#xA;--save_folder [save_dir_path] \&#xA;--follow_links&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This script will collect all image files in &lt;code&gt;img_folder&lt;/code&gt; and split them into training set and validation set automatically. You will get two file lists in &lt;code&gt;save_folder&lt;/code&gt;, each line in a file list contains an absolute path of an image file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;save_folder&#xA;‚îú‚îÄ‚îÄ train.list # training file list&#xA;‚îî‚îÄ‚îÄ val.list   # validation file list&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Configure training set and validation set.&lt;/p&gt; &lt;p&gt;For general image restoration, fill in the following configuration files with appropriate values.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/configs/dataset/general_deg_codeformer_train.yaml&#34;&gt;training set&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/configs/dataset/general_deg_codeformer_val.yaml&#34;&gt;validation set&lt;/a&gt; for &lt;strong&gt;CodeFormer&lt;/strong&gt; degradation.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/configs/dataset/general_deg_realesrgan_train.yaml&#34;&gt;training set&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/configs/dataset/general_deg_realesrgan_val.yaml&#34;&gt;validation set&lt;/a&gt; for &lt;strong&gt;Real-ESRGAN&lt;/strong&gt; degradation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;For face image restoration, fill in the face &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/configs/dataset/face_train.yaml&#34;&gt;training set&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/configs/dataset/face_val.yaml&#34;&gt;validation set&lt;/a&gt; configuration files with appropriate values.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Train Stage1 Model&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Configure training-related information.&lt;/p&gt; &lt;p&gt;Fill in the configuration file of &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/configs/train_swinir.yaml&#34;&gt;training&lt;/a&gt; with appropriate values.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start training.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --config [training_config_path]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;span&gt;üí°&lt;/span&gt;:Checkpoints of SwinIR will be used in training stage2 model.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Train Stage2 Model&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pretrained &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1-base&#34;&gt;Stable Diffusion v2.1&lt;/a&gt; to provide generative capabilities.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt --no-check-certificate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create the initial model weights.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/make_stage2_init_weight.py \&#xA;--cldm_config configs/model/cldm.yaml \&#xA;--sd_weight [sd_v2.1_ckpt_path] \&#xA;--swinir_weight [swinir_ckpt_path] \&#xA;--output [init_weight_output_path]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You will see some &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/assets/init_weight_outputs.txt&#34;&gt;outputs&lt;/a&gt; which show the weight initialization.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Configure training-related information.&lt;/p&gt; &lt;p&gt;Fill in the configuration file of &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/configs/train_cldm.yaml&#34;&gt;training&lt;/a&gt; with appropriate values.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start training.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --config [training_config_path]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;update&#34;&gt;&lt;/a&gt;&lt;span&gt;üÜï&lt;/span&gt;Update&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.08.30&lt;/strong&gt;: Repo is released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.09.06&lt;/strong&gt;: Update &lt;a href=&#34;https://colab.research.google.com/github/camenduru/DiffBIR-colab/blob/main/DiffBIR_colab.ipynb&#34;&gt;colab demo&lt;/a&gt;. Thanks to &lt;a href=&#34;https://github.com/camenduru&#34;&gt;camenduru&lt;/a&gt;!&lt;span&gt;ü§ó&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.09.08&lt;/strong&gt;: Add support for restoring unaligned faces.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;todo&#34;&gt;&lt;/a&gt;&lt;span&gt;üßó&lt;/span&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release code and pretrained models&lt;span&gt;üíª&lt;/span&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update links to paper and project page&lt;span&gt;üîó&lt;/span&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release real47 testset&lt;span&gt;üíΩ&lt;/span&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reduce the memory usage of DiffBIR&lt;span&gt;üò∫&lt;/span&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Provide HuggingFace demo&lt;span&gt;üìì&lt;/span&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Upload inference code of latent image guidance&lt;span&gt;üìÑ&lt;/span&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve the performance&lt;span&gt;ü¶∏&lt;/span&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add a patch-based sampling schedule&lt;span&gt;üîç&lt;/span&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite us if our work is useful for your research.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{2023diffbir,&#xA;  author    = {Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, Chao Dong},&#xA;  title     = {DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior},&#xA;  journal   = {arxiv},&#xA;  year      = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/DiffBIR/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt; and &lt;a href=&#34;https://github.com/XPixelGroup/BasicSR&#34;&gt;BasicSR&lt;/a&gt;. Thanks for their awesome work.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please feel free to contact with me at &lt;a href=&#34;mailto:linxinqi@tju.edu.cn&#34;&gt;linxinqi@tju.edu.cn&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenGVLab/LLaMA-Adapter</title>
    <updated>2023-09-10T01:41:30Z</updated>
    <id>tag:github.com,2023-09-10:/OpenGVLab/LLaMA-Adapter</id>
    <link href="https://github.com/OpenGVLab/LLaMA-Adapter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaMA-Adapter: Efficient Fine-tuning of LLaMA üöÄ&lt;/h1&gt; &#xA;&lt;h2&gt;Announcement: We release &lt;strong&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/LLaMA2-Accessory&#34;&gt;LLaMA2-Accessory&lt;/a&gt;&lt;/strong&gt;, an open-source toolkit for &lt;strong&gt;pre-training&lt;/strong&gt;, &lt;strong&gt;fine-tuning&lt;/strong&gt; and &lt;strong&gt;deployment&lt;/strong&gt; of &lt;strong&gt;LLMs&lt;/strong&gt; and &lt;strong&gt;mutlimodal LLMs&lt;/strong&gt;.üî•&lt;/h2&gt; &#xA;&lt;p&gt;Official implementation of &lt;a href=&#34;https://arxiv.org/pdf/2303.16199.pdf&#34;&gt;&#39;LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention&#39;&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2304.15010.pdf&#34;&gt;&#39;LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model&#39;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/docs/logo_v4.png&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;This repo proposes &lt;strong&gt;LLaMA-Adapter (V2)&lt;/strong&gt;, a lightweight adaption method for fine-tuning &lt;strong&gt;Instruction-following&lt;/strong&gt; and &lt;strong&gt;Multi-modal&lt;/strong&gt; &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; models üî•.&lt;/p&gt; &#xA;&lt;p&gt;Try out the web demo ü§ó of LLaMA-Adapter: &lt;a href=&#34;https://huggingface.co/spaces/csuhan/LLaMA-Adapter&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;, &lt;a href=&#34;http://llama-adapter.opengvlab.com/&#34;&gt;LLaMA-Adapter V2&lt;/a&gt; and &lt;a href=&#34;http://imagebind-llm.opengvlab.com/&#34;&gt;ImageBind-LLM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.08.28]&lt;/strong&gt; We release quantized LLM with &lt;a href=&#34;https://github.com/OpenGVLab/OmniQuant&#34;&gt;OmniQuant&lt;/a&gt;, which is an efficient, accurate, and omnibearing (even extremely low bit) quantization algorithm. Multimodal version is coming soon.üî•üî•üî•&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.07.24]&lt;/strong&gt; We release &lt;strong&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/LLaMA2-Accessory&#34;&gt;LLaMA2-Accessory&lt;/a&gt;&lt;/strong&gt;, an open-source toolkit for &lt;strong&gt;pre-training&lt;/strong&gt;, &lt;strong&gt;fine-tuning&lt;/strong&gt; and &lt;strong&gt;deployment&lt;/strong&gt; of &lt;strong&gt;Large Language Models (LLMs)&lt;/strong&gt; and &lt;strong&gt;mutlimodal LLMs&lt;/strong&gt;. Please check &lt;a href=&#34;https://github.com/Alpha-VLLM/LLaMA2-Accessory&#34;&gt;Alpha-VLLM/LLaMA2-Accessory&lt;/a&gt; for more details!üî•üî•üî•&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.07.05]&lt;/strong&gt; We release the pretrain/finetune code of &lt;a href=&#34;https://github.com/OpenGVLab/LLaMA-Adapter/tree/main/llama_adapter_v2_multimodal7b&#34;&gt;llama_adapter_v2_multimodal7b&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.07.04]&lt;/strong&gt; We release the code for reproducing &lt;a href=&#34;https://github.com/ShishirPatil/gorilla&#34;&gt;Gorilla&lt;/a&gt; by both full finetune and LLaMA-Adapter, please see &lt;a href=&#34;https://github.com/OpenGVLab/LLaMA-Adapter/raw/main/gorilla/README.md&#34;&gt;gorilla/README.md&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.06.08]&lt;/strong&gt; We release the &lt;a href=&#34;http://imagebind-llm.opengvlab.com/&#34;&gt;demo&lt;/a&gt; of ImageBind-LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.06.06]&lt;/strong&gt; We release &lt;a href=&#34;https://github.com/ZrrSkywalker/Point-Bind&#34;&gt;Point-Bind&lt;/a&gt; to extend ImageBind with 3D point clouds, which achieves 3D instruction-following capacity for &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/imagebind_LLM&#34;&gt;imagebind_LLM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.06.05]&lt;/strong&gt; We support the integration of LLaMA-Adapter (both V1 and V2) and &lt;a href=&#34;https://python.langchain.com/en/latest/index.html&#34;&gt;LangChain&lt;/a&gt;. Check out the &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/docs/langchain_LLaMA_AdapterV2_demo.ipynb&#34;&gt;Notebook&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.05.29]&lt;/strong&gt; We release the code of ImageBind-LLM at &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/imagebind_LLM&#34;&gt;imagebind_LLM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.05.23]&lt;/strong&gt; We release the &lt;a href=&#34;http://llama-adapter.opengvlab.com/&#34;&gt;demos&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/llama_adapter_v2_multimodal7b&#34;&gt;multi-modal code&lt;/a&gt; of LLaMA-Adapter V2!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.05.05]&lt;/strong&gt; We release the paper and code of our new work &lt;a href=&#34;https://github.com/ZrrSkywalker/Personalize-SAM&#34;&gt;Personalize Segment Anything&lt;/a&gt;, which efficiently fine-tunes Segment Anything with &lt;strong&gt;10 seconds&lt;/strong&gt;, and improves DreamBooth for better &lt;strong&gt;text-to-image generation&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.04.30]&lt;/strong&gt; We noticed that GPT-4 evaluation has a strong positional bias in favor of the first response. We will soon update the paper to reveal the position bias. Great thanks to &lt;a href=&#34;https://scholar.google.com/citations?user=oopKCDMAAAAJ&amp;amp;hl=en&#34;&gt;Canwen Xu&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.04.28]&lt;/strong&gt; We release &lt;strong&gt;LLaMA-Adapter V2&lt;/strong&gt;, a multi-modal instruction model. Check out our &lt;a href=&#34;https://arxiv.org/abs/2304.15010&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/#demos&#34;&gt;demos&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/llama_adapter_v2_chat65b&#34;&gt;code&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.03.28]&lt;/strong&gt; The &lt;a href=&#34;https://arxiv.org/pdf/2303.16199.pdf&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/alpaca_finetuning_v1&#34;&gt;training code&lt;/a&gt; for &lt;strong&gt;LLaMA-Adapter V1&lt;/strong&gt; are released. üìå&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Released Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Approach&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;Modality&lt;/th&gt; &#xA;   &lt;th&gt;Visual&lt;/th&gt; &#xA;   &lt;th&gt;Text&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/alpaca_finetuning_v1&#34;&gt;LLaMA-Adapter V1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;prefix, gate&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;√ó&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/llama_adapter_v2_chat65b&#34;&gt;LLaMA-Adapter V2 dialog&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;scale, bias, norm&lt;/td&gt; &#xA;   &lt;td&gt;ShareGPT&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;√ó&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-65B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/llama_adapter_v2_multimodal7b&#34;&gt;LLaMA-Adapter V2 multimodal&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;[P] prefix, projection, gate &lt;br&gt;[F] bias, norm&lt;/td&gt; &#xA;   &lt;td&gt;[P] Image-Text-V1&lt;br&gt;[F] GPT4LLM, LLaVA&lt;/td&gt; &#xA;   &lt;td&gt;Image&amp;amp;Text&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-ViT-L/14&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/imagebind_LLM&#34;&gt;ImageBind-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;[P] prefix, projection, gate&lt;br&gt;[F] bias, norm, lora&lt;/td&gt; &#xA;   &lt;td&gt;[P] Image-Text-V1&lt;br&gt;[F] &lt;a href=&#34;https://github.com/OpenGVLab/LLaMA-Adapter/raw/main/imagebind_LLM/docs/train.md#data-1&#34;&gt;Instruction Following&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ImageBind Modalities + Point Cloud&lt;/td&gt; &#xA;   &lt;td&gt;imagebind_huge&lt;/td&gt; &#xA;   &lt;td&gt;Open-Chinese-LLaMA-7B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ImageBind-dialog&lt;/td&gt; &#xA;   &lt;td&gt;[P] prefix, projection, gate&lt;br&gt;[F] bias, norm, lora&lt;/td&gt; &#xA;   &lt;td&gt;[P] Image-Text-V1&lt;br&gt;[F] LLaVA, ShareGPT&lt;/td&gt; &#xA;   &lt;td&gt;ImageBind Modalities + Point Cloud&lt;/td&gt; &#xA;   &lt;td&gt;imagebind_huge&lt;/td&gt; &#xA;   &lt;td&gt;Open-Chinese-LLaMA-7B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[P] means &lt;strong&gt;P&lt;/strong&gt;re-train and [F] means &lt;strong&gt;F&lt;/strong&gt;ine-tune&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Image-Text-V1&lt;/strong&gt; is a concatenation of LAION400M, COYO, MMC4, SBU, Conceptual Captions, and COCO&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ImageBind Modalities&lt;/strong&gt; include image, video, text, audio, depth, thermal, IMU&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ImageBind-dialog&lt;/strong&gt; will be release soon&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Efficiency Comparison:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Storage Space&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Training Time&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3 Hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA-Adapter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.2M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.7M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1 Hour&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;By inserting adapters into LLaMA&#39;s transformer, our method only introduces &lt;strong&gt;1.2M&lt;/strong&gt; learnable parameters, and turns a LLaMA into an instruction-following model within &lt;strong&gt;1 hour&lt;/strong&gt;. For stablizing training at early stages, we propose a novel &lt;strong&gt;Zero-init Attention&lt;/strong&gt; with zero gating mechanism to adaptively incorporate the instructional signals. After fine-tuning, LLaMA-Adapter can generate high-quality instruction-following sentences, comparable to the fully fine-tuned &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca-Lora&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/docs/pipeline.png&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Our approach can be simply extended to &lt;strong&gt;Multi-modal Input Instructions&lt;/strong&gt;. The reasoning framework of image-conditioned LLaMA-Adapter for &lt;a href=&#34;https://scienceqa.github.io/&#34;&gt;ScienceQA&lt;/a&gt; is as follows, which is also shared by other modalities, such as audio and video.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/docs/multimodal.png&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Here is a from-scratch script for &lt;strong&gt;LLaMA-Adapter V1&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n llama_adapter -y python=3.8&#xA;conda activate llama_adapter&#xA;&#xA;# install pytorch&#xA;conda install pytorch cudatoolkit -c pytorch -y&#xA;&#xA;# install dependency and llama-adapter&#xA;pip install -r requirements.txt&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;strong&gt;To setup other models&lt;/strong&gt;, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/llama_adapter_v2_chat65b&#34;&gt;llama_adapter_v2_chat65b&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/llama_adapter_v2_multimodal7b&#34;&gt;llama_adapter_v2_multimodal7b&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/imagebind_LLM&#34;&gt;imagebind_LLM&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Please request access to the pre-trained LLaMA from &lt;a href=&#34;https://forms.gle/jk851eBVbX1m5TAv5&#34;&gt;this form&lt;/a&gt; (official) or download the LLaMA-7B from &lt;a href=&#34;https://huggingface.co/nyanko7/LLaMA-7B/tree/main&#34;&gt;Hugging Face&lt;/a&gt; (unofficial). Then, obtain the weights of our LLaMA-Adapter from &lt;a href=&#34;https://github.com/OpenGVLab/LLaMA-Adapter/releases/download/v.1.0.0/llama_adapter_len10_layer30_release.pth&#34;&gt;here&lt;/a&gt;. We denote the path to the downloaded weights of LLaMA and adapters as &lt;code&gt;TARGET_FOLDER&lt;/code&gt; and &lt;code&gt;ADAPTER_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example to generate instruction-following sentences with 7B LLaMA model and our LLaMA-Adapter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node 1 example.py \&#xA;         --ckpt_dir $TARGET_FOLDER/model_size\&#xA;         --tokenizer_path $TARGET_FOLDER/tokenizer.model \&#xA;         --adapter_path $ADAPTER_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We release the simple fine-tuning code of LLaMA-Adapter on LLaMA-7B model at &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/LLaMA-Adapter/main/alpaca_finetuning_v1&#34;&gt;here&lt;/a&gt;, which is for effortless reproduction with minimal dependencies. We will soon release the fine-tuning code for LLaMA-65B and multi-model LLaMA-Adapter.&lt;/p&gt; &#xA;&lt;p&gt;Please download the 52K instruction-following training &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/alpaca_data.json&#34;&gt;data&lt;/a&gt; from Standford Alpaca, and put it under &lt;code&gt;DATA_PATH&lt;/code&gt;. Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd alpaca_finetuning_v1&#xA;&#xA;torchrun --nproc_per_node 8 finetuning.py \&#xA;         --model Llama7B_adapter \&#xA;         --llama_model_path $TARGET_FOLDER/ \&#xA;         --data_path $DATA_PATH/alpaca_data.json \&#xA;         --adapter_layer 30 \&#xA;         --adapter_len 10 \&#xA;         --max_seq_len 512 \&#xA;         --batch_size 4 \&#xA;         --epochs 5 \&#xA;         --warmup_epochs 2 \&#xA;         --blr 9e-3 \&#xA;         --weight_decay 0.02 \&#xA;         --output_dir ./checkpoint/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Comparison with Other Methods&lt;/h2&gt; &#xA;&lt;h3&gt;Instruction 1:&lt;/h3&gt; &#xA;&lt;p&gt;How can I develop my critical thinking skills? Give me three suggestions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter V2:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Read widely: Reading books, articles, and other sources of information can help you develop your critical thinking skills. By exposing yourself to different perspectives and ideas, you can learn to analyze and evaluate information more effectively.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Practice mindfulness: Mindfulness is the practice of being present in the moment and paying attention to your thoughts and feelings. By being more aware of your thoughts and emotions, you can better understand and evaluate them. This can help you develop a more objective and rational approach to thinking.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Engage in open-minded discussions: Join a discussion group or debate club where you can engage in open-minded discussions with others. This can help you develop your critical thinking skills by challenging your assumptions and encouraging you to consider different perspectives.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter V1:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Read widely and critically.&lt;/li&gt; &#xA; &lt;li&gt;Engage in discussions with people who have different perspectives.&lt;/li&gt; &#xA; &lt;li&gt;Ask questions and seek out answers.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Instruction 2:&lt;/h3&gt; &#xA;&lt;p&gt;As a superhero, how would you explain your origin story to a curious child?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter V2:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;As a superhero, I have a very interesting and exciting origin story to share with you. Once upon a time, there was a young boy named John who lived in a small town. John was always curious and wanted to help others, so he spent his days exploring the world around him and learning about different cultures and people. One day, while he was out exploring, he stumbled upon a mysterious cave. Inside the cave, John found a strange device that had the power to give him incredible abilities. With these abilities, John could fly, shoot lasers from his eyes, and even turn invisible.&lt;/p&gt; &#xA;&lt;p&gt;John knew that he had to use these powers for good, so he decided to become a superhero and use his abilities to protect his town and the people who lived there. He started to train every day, learning how to use his powers and how to fight against evil. Eventually, John became the superhero known as &#34;The Invisible Man&#34; and he used his powers to help others and make the world a better place.&lt;/p&gt; &#xA;&lt;p&gt;That&#39;s my origin story, and I&#39;m sure you&#39;ll love it!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter V1:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once upon a time, there was a young boy who had a special power. He used his power to help people in need and protect the innocent. He became known as the superhero, and he used his powers to fight evil and protect the world.&lt;/p&gt; &#xA;&lt;h3&gt;Instruction 3:&lt;/h3&gt; &#xA;&lt;p&gt;Tell me about alpacas.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: Alpacas are members of the camelid family and are native to the Andes Mountains of South America. They are typically found in herds of 10-20 animals and are known for their gentle nature and their ability to thrive in harsh climates. Alpacas are typically white or grey in color and have long, silky coats. They are also known for their ability to produce high-quality wool, which is used in the production of clothing and other products.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Alpacas are members of the camelid family and are native to the Andes Mountains of South America. They are known for their soft, luxurious fleece, which is used to make clothing, blankets, and other items. Alpacas are herbivores and graze on grasses and other plants. They are social animals and live in herds of up to 20 individuals.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Alpacas are small, fluffy animals related to camels and llamas. They are native to Peru and Bolivia, and were first domesticated around 5,000 years ago. They are kept mainly for their fine, soft fleece, which is used to make knitwear and other garments. Alpacas are herd animals and live in small family groups, led by an older male. They are highly social animals and can form strong bonds with their owners.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: Alpacas are a domesticated species of South American camelid, similar to llamas. They are native to the Andes Mountains in South America and are kept as livestock for their soft, luxurious wool. Alpacas are social animals and live in herds of up to 20 individuals. They are typically kept in small herds of two to five animals, and are relatively easy to care for. Alpacas are herbivores and feed on grass, hay, and other vegetation. They are also known for their gentle and friendly nature, making them popular as pets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 4:&lt;/h3&gt; &#xA;&lt;p&gt;Tell me about the president of Mexico in 2019.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: The president of Mexico in 2019 was Andr√©s Manuel L√≥pez Obrador, who took office on December 1st, 2018. He is a member of the left-wing party Morena and is known for his populist policies. He has focused on tackling corruption, reducing poverty, and improving the country&#39;s infrastructure. He has also been a vocal critic of the United States&#39; immigration policies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: The president of Mexico in 2019 was Andr√©s Manuel L√≥pez Obrador, who took office on December 1, 2018. He is a member of the National Regeneration Movement (MORENA) political party and is the first left-wing president of Mexico since 1946. He is known for his anti-corruption and anti-neoliberal policies, as well as his commitment to improving the living conditions of the Mexican people.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: In 2019, Andres Manuel Lopez Obrador was sworn in as President of Mexico. He is the country‚Äôs 58th president and the first from the left-wing Morena party. He was elected in 2018 on a promise to combat corruption and reduce poverty and has faced criticism for his handling of the economy, crime and the country‚Äôs relationship with the United States. He is also known for his ambitious social welfare programs and has been praised for raising the minimum wage and providing aid to low-income families. He is seen as a champion of democracy and human rights in Mexico.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: The president of Mexico in 2019 is Andr√©s Manuel L√≥pez Obrador, commonly known as AMLO. He is a Mexican politician who has been the President of Mexico since December 1, 2018. He is a member of the National Regeneration Movement (MORENA) party and previously served as the Head of Government of Mexico City from 2000 to 2005. He is a left-wing populist and has promised to reduce corruption and inequality in Mexico. He has also promised to increase public spending on social programs and infrastructure projects.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 5:&lt;/h3&gt; &#xA;&lt;p&gt;Write a Python program that prints the first 10 Fibonacci numbers.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fibonacci():&#xA;    a = 0&#xA;    b = 1&#xA;    for i in range(10):&#xA;        a, b = b, a + b&#xA;        print(a, b)&#xA;    print(&#34;Fibonacci numbers: &#34;, a, b)&#xA;&#xA;fibonacci()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fibonacci(n):&#xA;    if n == 0:&#xA;        return 0&#xA;    elif n == 1:&#xA;        return 1&#xA;    else:&#xA;        return fibonacci(n-1) + fibonacci(n-2)&#xA;&#xA;print(fibonacci(10))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;numbers = [0, 1]&#xA;&#xA;for i in range(2, 11):&#xA;    numbers.append(numbers[i-2] + numbers[i-1])&#xA;&#xA;print(numbers)&#xA;# Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fibonacci numbers are the numbers in the following integer sequence, called the Fibonacci sequence, and characterized by the fact that every number after the first two is the sum of the two preceding ones.&#xA;&#xA;# Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...&#xA;&#xA;# Program:&#xA;&#xA;# Initialize the first two numbers of the Fibonacci sequence&#xA;a = 0&#xA;b = 1&#xA;&#xA;# Print the first 10 numbers of the Fibonacci sequence&#xA;print(a)&#xA;print(b)&#xA;&#xA;for i in range(2,10):&#xA;    c = a + b&#xA;    a = b&#xA;    b = c&#xA;    print(c)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 6:&lt;/h3&gt; &#xA;&lt;p&gt;Write a program that prints the numbers from 1 to 100. But for multiples of three print &#39;Fizz&#39; instead of the number and for the multiples of five print &#39;Buzz&#39;. For numbers which are multiples of both three and five print &#39;FizzBuzz&#39;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def print_numbers(n):&#xA;    for i in range(1, 101):&#xA;        if i % 3 == 0 and i % 5 == 0:&#xA;            print(&#34;FizzBuzz&#34;)&#xA;        elif i % 3 == 0:&#xA;            print(&#34;Fizz&#34;)&#xA;        elif i % 5 == 0:&#xA;            print(&#34;Buzz&#34;)&#xA;        else:&#xA;            print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#34;FizzBuzz&#34;)&#xA;    elif i % 3 == 0:&#xA;        print(&#34;Fizz&#34;)&#xA;    elif i % 5 == 0:&#xA;        print(&#34;Buzz&#34;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#39;FizzBuzz&#39;)&#xA;    elif i % 3 == 0:&#xA;        print(&#39;Fizz&#39;)&#xA;    elif i % 5 == 0:&#xA;        print(&#39;Buzz&#39;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;for (let i = 1; i &amp;lt;= 100; i++) {&#xA;  if (i % 3 === 0 &amp;amp;&amp;amp; i % 5 === 0) {&#xA;    console.log(&#34;FizzBuzz&#34;);&#xA;  } else if (i % 3 === 0) {&#xA;    console.log(&#34;Fizz&#34;);&#xA;  } else if (i % 5 === 0) {&#xA;    console.log(&#34;Buzz&#34;);&#xA;  } else {&#xA;    console.log(i);&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=_go6DPsAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&#34;https://csuhan.com&#34;&gt;Jiaming Han&lt;/a&gt;, &lt;a href=&#34;https://github.com/ZrrSkywalker&#34;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/aojunzz&#34;&gt;Aojun Zhou&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Hiring Announcement&lt;/h2&gt; &#xA;&lt;p&gt;üî• &lt;strong&gt;We are hiring&lt;/strong&gt; interns, postdocs, and full-time researchers at the &lt;strong&gt;General Vision Group, Shanghai AI Lab&lt;/strong&gt;, with a focus on multi-modality and vision foundation models. If you are interested, please contact &lt;a href=&#34;mailto:gaopengcuhk@gmail.com&#34;&gt;gaopengcuhk@gmail.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our LLaMA-Adapter code and paper useful, please kindly cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{zhang2023llamaadapter,&#xA;  title = {LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention},&#xA;  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},&#xA;  journal={arXiv preprint arXiv:2303.16199},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you find our LLaMA-Adapter V2 code and paper useful, please kindly cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{gao2023llamaadapterv2,&#xA;  title = {LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model},&#xA;  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and Li, Hongsheng and Qiao, Yu},&#xA;  journal={arXiv preprint arXiv:2304.15010},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;, and &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca-Lora&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt;</summary>
  </entry>
</feed>