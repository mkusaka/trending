<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-12T01:35:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RapidAI/RapidOCR</title>
    <updated>2025-07-12T01:35:07Z</updated>
    <id>tag:github.com,2025-07-12:/RapidAI/RapidOCR</id>
    <link href="https://github.com/RapidAI/RapidOCR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üìÑ Awesome OCR multiple programing languages toolkits based on ONNXRuntime, OpenVINO, PaddlePaddle and PyTorch.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/RapidAI/RapidOCR/releases/download/v1.1.0/RapidOCR_LOGO_white.png&#34; width=&#34;55%&#34; height=&#34;55%&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://raw.githubusercontent.com/RapidAI/RapidOCR/main/assets/RapidOCR_LOGO.png&#34; width=&#34;55%&#34; height=&#34;55%&#34;&gt; &#xA;  &lt;img alt=&#34;Shows an illustrated sun in light mode and a moon with stars in dark mode.&#34; src=&#34;https://raw.githubusercontent.com/RapidAI/RapidOCR/main/assets/RapidOCR_LOGO.png&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;4&#34;&gt;&lt;i&gt;Open source OCR for the security of the digital world&lt;/i&gt;&lt;/font&gt;&lt;/b&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/RapidAI/RapidOCRv3&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%25F0%259F%25A4%2597-Hugging%20Face%20Demo-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.modelscope.cn/studios/RapidAI/RapidOCRv3.0.0/summary&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E9%AD%94%E6%90%AD-Demo-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/RapidAI/RapidOCR/blob/main/assets/RapidOCRDemo.ipynb&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RapidAI/RapidOCR/main/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-%3E=3.6-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OS-Linux%2C%20Win%2C%20Mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RapidAI/RapidOCR/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/RapidAI/RapidOCR?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/rapidocr&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/rapidocr?period=total&amp;amp;units=abbreviation&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=Downloads%20rapidocr&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/rapidocr_onnxruntime&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/rapidocr_onnxruntime?period=total&amp;amp;units=abbreviation&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=Downloads%20Ort&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/rapidocr_openvino&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/rapidocr_openvino?period=total&amp;amp;units=abbreviation&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=Downloads%20Vino&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/rapidocr_paddle&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/rapidocr_paddle?period=total&amp;amp;units=abbreviation&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=Downloads%20Paddle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/rapidocr/&#34;&gt;&lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/rapidocr&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RapidAI/RapidOCR/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RapidAI/RapidOCR?color=ccf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://semver.org/&#34;&gt;&lt;img alt=&#34;SemVer2.0&#34; src=&#34;https://img.shields.io/badge/SemVer-2.0-brightgreen&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RapidAI/RapidOCR/main/docs/README_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | English&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Introduction&lt;/h3&gt; &#xA;&lt;p&gt;üíñ Introducing the foremost multi-platform, multi-lingual OCR tool that boasts unparalleled speed, expansive support, and complete openness. This exceptional software is entirely free and renowned for facilitating swift offline deployments.&lt;/p&gt; &#xA;&lt;p&gt;ü¶ú &lt;strong&gt;Supported Languages&lt;/strong&gt;: It inherently supports Chinese and English, with self-service conversion required for additional languages. Please refer &lt;a href=&#34;https://rapidai.github.io/RapidOCRDocs/main/blog/2022/09/28/%E6%94%AF%E6%8C%81%E8%AF%86%E5%88%AB%E8%AF%AD%E8%A8%80/&#34;&gt;here&lt;/a&gt; for specific language support details.&lt;/p&gt; &#xA;&lt;p&gt;üîé &lt;strong&gt;Rationale&lt;/strong&gt;: Acknowledging the limitations in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34;&gt;PaddleOCR&lt;/a&gt;&#39;s architecture, we embarked on a mission to simplify OCR inference across diverse platforms. This endeavor culminated in converting PaddleOCR&#39;s model to the versatile ONNX format and seamlessly integrating it into Python, C++, Java, and C# environments.&lt;/p&gt; &#xA;&lt;p&gt;üéì &lt;strong&gt;Etymology&lt;/strong&gt;: Derived from its essence, RapidOCR embodies lightness, velocity, affordability, and intelligence. Rooted in deep learning, this OCR technology underscores AI&#39;s prowess and emphasizes compact models, prioritizing swiftness without compromising efficacy.&lt;/p&gt; &#xA;&lt;p&gt;üòâ &lt;strong&gt;Usage Scenarios&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Instant Deployment&lt;/strong&gt;: If the pre-existing models within our repository suffice, simply leverage RapidOCR for swift deployment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customization&lt;/strong&gt;: In case of specific requirements, refine PaddleOCR with your data and proceed with RapidOCR deployment, ensuring tailored results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If our repository proves beneficial to your endeavors, kindly consider leaving a star ‚≠ê on GitHub to show your appreciation. It means the world to us!&lt;/p&gt; &#xA;&lt;h3&gt;Visualization&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/RapidAI/RapidOCR/releases/download/v1.1.0/demo.gif&#34; alt=&#34;Demo&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install onnxruntime&#xA;pip install rapidocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rapidocr import RapidOCR&#xA;&#xA;engine = RapidOCR()&#xA;&#xA;img_url = &#34;https://github.com/RapidAI/RapidOCR/blob/main/python/tests/test_files/ch_en_num.jpg?raw=true&#34;&#xA;result = engine(img_url)&#xA;print(result)&#xA;&#xA;result.vis(&#34;vis_result.jpg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;Full documentation can be found on &lt;a href=&#34;https://rapidai.github.io/RapidOCRDocs/&#34;&gt;docs&lt;/a&gt;, in Chinese.&lt;/p&gt; &#xA;&lt;h3&gt;Who use?&lt;/h3&gt; &#xA;&lt;p&gt;Used by &lt;a href=&#34;https://github.com/RapidAI/RapidOCR/discussions/286&#34;&gt;link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/DeliciaLaniD&#34;&gt;DeliciaLaniD&lt;/a&gt; for fixing the misplaced start position of scan animation in ocrweb.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/zhsunlight&#34;&gt;zhsunlight&lt;/a&gt; for the suggestion about parameterized call GPU reasoning and the careful and thoughtful testing.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/lzh111222334&#34;&gt;lzh111222334&lt;/a&gt; for fixing some bugs of rec preprocessing under python version.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/AutumnSun1996&#34;&gt;AutumnSun1996&lt;/a&gt; for the suggestion in the &lt;a href=&#34;https://github.com/RapidAI/RapidOCR/issues/42&#34;&gt;#42&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/DeadWood8&#34;&gt;DeadWood8&lt;/a&gt; for providing the &lt;a href=&#34;https://rapidai.github.io/RapidOCRDocs/main/install_usage/rapidocr_web/nuitka_package/&#34;&gt;document&lt;/a&gt; which packages rapidocr_web to exe by Nuitka.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/Loovelj&#34;&gt;Loovelj&lt;/a&gt; for fixing the bug of sorting the text boxes. For details see &lt;a href=&#34;https://github.com/RapidAI/RapidOCR/issues/75&#34;&gt;issue 75&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üéñ Code Contributors&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://github.com/RapidAI/RapidOCR/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=RapidAI/RapidOCR&amp;amp;max=400&amp;amp;columns=20&#34; width=&#34;70%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://rapidai.github.io/RapidOCRDocs/main/sponsor/&#34;&gt;Sponsor&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;p&gt;If you want to sponsor the project, you can directly click the &lt;strong&gt;Buy me a coffee&lt;/strong&gt; image, please write a note (e.g. your github account name) to facilitate adding to the sponsorship list below.&lt;/p&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;a href=&#34;https://www.buymeacoffee.com/SWHL&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RapidAI/.github/main/assets/buymeacoffe.png&#34; width=&#34;30%&#34; height=&#34;30%&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sponsor&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Applied Products&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cuiliang&#34; title=&#34;cuiliang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1972649?v=4&#34; width=&#34;65&#34; height=&#34;65&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://getquicker.net/&#34; title=&#34;Quicker&#34;&gt;&lt;img src=&#34;https://github.com/RapidAI/RapidOCR/releases/download/v1.1.0/Quicker.jpg&#34; width=&#34;65&#34; height=&#34;65&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Eunsolfs&#34; title=&#34;Eunsolfs&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/53815751?v=4&#34; width=&#34;65&#34; height=&#34;65&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{RapidOCR 2021,&#xA;    title={{Rapid OCR}: OCR Toolbox},&#xA;    author={RapidAI Team},&#xA;    howpublished = {\url{https://github.com/RapidAI/RapidOCR}},&#xA;    year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;‚≠êÔ∏è Stargazers over time&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/RapidAI/RapidOCR&#34;&gt;&lt;img src=&#34;https://starchart.cc/RapidAI/RapidOCR.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;The copyright of the OCR model is held by Baidu, while the copyrights of all other engineering scripts are retained by the repository&#39;s owner.&lt;/p&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/RapidAI/RapidOCR/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>browser-use/macOS-use</title>
    <updated>2025-07-12T01:35:07Z</updated>
    <id>tag:github.com,2025-07-12:/browser-use/macOS-use</id>
    <link href="https://github.com/browser-use/macOS-use" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make Mac apps accessible for AI agents&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; &#xA; &lt;img alt=&#34;Shows a black Browser Use Logo in light color mode and a white one in dark color mode.&#34; src=&#34;https://raw.githubusercontent.com/browser-use/macOS-use/main/static/macos-use.png&#34; width=&#34;full&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/macOS-use/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/browser-use/macOS-use?style=social&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/OfirOzeri&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/OfirOzeri?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/gregpr07&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Gregor?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/mamagnus00&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Magnus?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;Tell your MacBook what to do, and it&#39;s done‚Äîacross ANY app.&lt;/h2&gt; Created by &#xA; &lt;a href=&#34;https://x.com/OfirOzeri&#34;&gt;Ofir Ozeri &lt;/a&gt;‚ô•Ô∏è migrated in collaboration with &#xA; &lt;a href=&#34;https://x.com/mamagnus00&#34;&gt;Magnus&lt;/a&gt; and &#xA; &lt;a href=&#34;https://x.com/gregpr07&#34;&gt;Gregor&lt;/a&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;macOS-use enables AI agents to interact with your Macbook &lt;a href=&#34;https://raw.githubusercontent.com/browser-use/macOS-use/main/#demos&#34;&gt;see it in action!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;p&gt;‚ö†Ô∏è Important: Review the &lt;a href=&#34;https://raw.githubusercontent.com/browser-use/macOS-use/main/#warning&#34;&gt;Warning&lt;/a&gt; section before proceeding. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;With pip:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install mlx-use&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;From github&lt;/h3&gt; &#xA;&lt;p&gt;Clone first &lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/browser-use/macOS-use.git &amp;amp;&amp;amp; cd macOS-use&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Don&#39;t forget API key &lt;br&gt;Supported providers: &lt;a href=&#34;https://platform.openai.com/docs/quickstart&#34;&gt;OAI&lt;/a&gt;, &lt;a href=&#34;https://docs.anthropic.com/en/api/admin-api/apikeys/get-api-key&#34;&gt;Anthropic&lt;/a&gt; or &lt;a href=&#34;https://ai.google.dev/gemini-api/docs/api-key&#34;&gt;Gemini&lt;/a&gt; (deepseek R1 coming soon!)&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt; At the moment, macOS-use works best with OAI or Anthropic API, although Gemini is free. While Gemini works great too, it is not as reliable. &lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;open ./.env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend using macOS-use with uv environment &lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install uv &amp;amp;&amp;amp; uv venv &amp;amp;&amp;amp; source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install locally and you&#39;re good to go! try the first exmaple! &lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv pip install --editable . &amp;amp;&amp;amp; python examples/try.py&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Try prompting it with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;open the calculator app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Demos&lt;/h1&gt; &#xA;&lt;h3&gt; Click the GIF for the full video! &lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/macOS-use/raw/main/examples/calculate.py&#34;&gt;prompt&lt;/a&gt;: Calculate how much is 5 X 4 and return the result, then call done.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/calculate.py&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://x.com/OfirOzeri/status/1883110905665433681&#34;&gt;&lt;img src=&#34;https://github.com/browser-use/macOS-use/raw/main/static/calc-5-X-4.gif&#34; alt=&#34;calc-5-times-4&#34; title=&#34;Click the GIF for full video!&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/macOS-use/raw/main/examples/login_to_auth0.py&#34;&gt;prompt&lt;/a&gt;: Go to auth0.com, sign in with google auth, choose ofiroz91 gmail account, login to the website and call done when you finish.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/login_to_auth0.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://x.com/OfirOzeri/status/1883455599423434966&#34;&gt;&lt;img src=&#34;https://github.com/browser-use/macOS-use/raw/main/static/login-to-auth0.gif&#34; alt=&#34;login-to-auth0&#34; title=&#34;Click for full video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/macOS-use/raw/main/examples/check_time_online.py&#34;&gt;prompt&lt;/a&gt;: Can you check what hour is Shabbat in israel today? call done when you finish.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/check_time_online.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://x.com/OfirOzeri/status/1883109604416278552&#34;&gt;&lt;img src=&#34;https://github.com/browser-use/macOS-use/raw/main/static/check-time-online.gif&#34; alt=&#34;check-time-online&#34; title=&#34;Click for full video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Our Vision:&lt;/h1&gt; &#xA;&lt;p&gt;TLDR: Tell every Apple device what to do, and see it done. on EVERY APP. &lt;br&gt;&lt;br&gt; This project aimes to build the AI agent for the MLX by Apple framework that would allow the agent to perform any action on any Apple device. Our final goal is a open source that anyone can clone, powered by the &lt;a href=&#34;https://github.com/ml-explore/mlx&#34;&gt;mlx&lt;/a&gt; and &lt;a href=&#34;https://github.com/Blaizzy/mlx-vlm&#34;&gt;mlx-vlm&lt;/a&gt; to run local private infrence at zero cost.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap goals:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Support MacBooks at SOTA reliability&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Refine the Agent prompting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release the first working version to pypi.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve self-correction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Adding ability to check which apps the machine has installed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add feature to allow the agent to check existing apps if failing, e.g. calendar app actual name is iCal.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add action for the agent to ask input from the user.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Test Test Test! and let us know what and how to improve!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make task cheaper and more efficient.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Support local inference with small fine tuned model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add support for inference with local models using mlx and mlx-vlm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine tune a small model that every device can run inference with.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; SOTA reliability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Support iPhone/iPad&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;WARNING&lt;/h1&gt; &#xA;&lt;p&gt;This project is still under development and user discretion is advised! macOS-use can and will use your do &lt;a href=&#34;https://raw.githubusercontent.com/browser-use/macOS-use/main/#demos&#34;&gt;login&lt;/a&gt;, use private credentials, &lt;a href=&#34;https://github.com/browser-use/macOS-use/raw/main/examples/login_to_auth0.py&#34;&gt;auth services&lt;/a&gt; or stored passwords to complete its task, launch and interact WITH EVERY APP and UI component in your MacBook and restrictions to the model are still under active development! It is not recommended to operate it unsupervised YET macOS-use WILL NOT STOP at captcha or any other forms of bot identifications, so once again, user discretion is advised.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer:&lt;/h2&gt; &#xA;&lt;p&gt;As this is an early stage release, You might experience varying success rates depending on task prompt, we&#39;re actively working on improvements. &lt;br&gt; talk me on &lt;a href=&#34;https://x.com/OfirOzeri&#34;&gt;X/Twitter&lt;/a&gt; or contact me on &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;Discord&lt;/a&gt;, your input is crucial and highly valuable!&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We are a new project and would love contributors! Feel free to PR, open issues for bugs or feature requests.&lt;/p&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;p&gt;I would like to extend my heartfelt thanks to &lt;a href=&#34;https://x.com/gregpr07&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Gregor?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;https://x.com/mamagnus00&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Magnus?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; for their incredible work in developing Browser Use. Their dedication and expertise have been invaluable, especially in helping with the migration process and I couldn&#39;t have done it without them!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>landing-ai/agentic-doc</title>
    <updated>2025-07-12T01:35:07Z</updated>
    <id>tag:github.com,2025-07-12:/landing-ai/agentic-doc</id>
    <link href="https://github.com/landing-ai/agentic-doc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python library for Agentic Document Extraction from LandingAI&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Agentic&amp;nbsp;Document&amp;nbsp;Extraction ‚Äì Python&amp;nbsp;Library&lt;/h1&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/landing-ai/agentic-doc/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;ci_status&#34;&gt; &lt;a href=&#34;https://discord.gg/RVcW3j9RgR&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/wPdN8RCYew?compact=true&amp;amp;style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/agentic-doc&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/agentic-doc.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://va.landing.ai/demo/doc-extraction&#34;&gt;Web App&lt;/a&gt;&amp;nbsp;¬∑ &lt;a href=&#34;https://discord.com/invite/RVcW3j9RgR&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;¬∑ &lt;a href=&#34;https://landing.ai/blog/going-beyond-ocrllm-introducing-agentic-document-extraction&#34;&gt;Blog&lt;/a&gt;&amp;nbsp;¬∑ &lt;a href=&#34;https://support.landing.ai/docs/document-extraction&#34;&gt;Docs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The LandingAI &lt;strong&gt;Agentic&amp;nbsp;Document&amp;nbsp;Extraction&lt;/strong&gt; API pulls structured data out of visually complex documents‚Äîthink tables, pictures, and charts‚Äîand returns a hierarchical JSON with exact element locations.&lt;/p&gt; &#xA;&lt;p&gt;This Python library wraps that API to provide:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Long‚Äëdocument support&lt;/strong&gt; ‚Äì process 100+&amp;nbsp;page PDFs in a single call&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto‚Äëretry / paging&lt;/strong&gt; ‚Äì handles concurrency, time‚Äëouts, and rate limits&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Helper utilities&lt;/strong&gt; ‚Äì bounding‚Äëbox snippets, visual debuggers, and more&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üì¶ &lt;strong&gt;Batteries‚Äëincluded install:&lt;/strong&gt; &lt;code&gt;pip install agentic-doc&lt;/code&gt; ‚Äì nothing else needed ‚Üí see&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üóÇÔ∏è &lt;strong&gt;All file types:&lt;/strong&gt; parse PDFs of &lt;em&gt;any&lt;/em&gt; length, single images, or URLs ‚Üí see&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#supported-files&#34;&gt;Supported&amp;nbsp;Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;strong&gt;Long‚Äëdoc ready:&lt;/strong&gt; auto‚Äësplit&amp;nbsp;&amp;amp;&amp;nbsp;parallel‚Äëprocess 1000+&amp;nbsp;page PDFs, then stitch results ‚Üí see&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#parse-large-pdf-files&#34;&gt;Parse&amp;nbsp;Large&amp;nbsp;PDF&amp;nbsp;Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üß© &lt;strong&gt;Structured output:&lt;/strong&gt; returns hierarchical JSON plus ready‚Äëto‚Äërender Markdown ‚Üí see&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#result-schema&#34;&gt;Result&amp;nbsp;Schema&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üëÅÔ∏è &lt;strong&gt;Ground‚Äëtruth visuals:&lt;/strong&gt; optional bounding‚Äëbox snippets and full‚Äëpage visualizations ‚Üí see&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#save-groundings-as-images&#34;&gt;Save&amp;nbsp;Groundings&amp;nbsp;as&amp;nbsp;Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üèÉ &lt;strong&gt;Batch&amp;nbsp;&amp;amp;&amp;nbsp;parallel:&lt;/strong&gt; feed a list; library manages threads&amp;nbsp;&amp;amp;&amp;nbsp;rate limits (&lt;code&gt;BATCH_SIZE&lt;/code&gt;, &lt;code&gt;MAX_WORKERS&lt;/code&gt;) ‚Üí see&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#parse-multiple-files-in-a-batch&#34;&gt;Parse&amp;nbsp;Multiple&amp;nbsp;Files&amp;nbsp;in&amp;nbsp;a&amp;nbsp;Batch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîÑ &lt;strong&gt;Resilient:&lt;/strong&gt; exponential‚Äëbackoff retries for 408/429/502/503/504 and rate‚Äëlimit hits ‚Üí see&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#automatically-handle-api-errors-and-rate-limits-with-retries&#34;&gt;Automatically&amp;nbsp;Handle&amp;nbsp;API&amp;nbsp;Errors&amp;nbsp;and&amp;nbsp;Rate&amp;nbsp;Limits&amp;nbsp;with&amp;nbsp;Retries&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Drop‚Äëin helpers:&lt;/strong&gt; &lt;code&gt;parse_documents&lt;/code&gt;, &lt;code&gt;parse_and_save_documents&lt;/code&gt;, &lt;code&gt;parse_and_save_document&lt;/code&gt; ‚Üí see&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#main-functions&#34;&gt;Main&amp;nbsp;Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Config via env / .env:&lt;/strong&gt; tweak parallelism, logging style, retry caps‚Äîno code changes ‚Üí see&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#configuration-options&#34;&gt;Configuration&amp;nbsp;Options&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üåê &lt;strong&gt;Raw API ready:&lt;/strong&gt; advanced users can still hit the REST endpoint directly ‚Üí see&amp;nbsp;the&amp;nbsp;&lt;a href=&#34;https://support.landing.ai/docs/document-extraction&#34;&gt;API&amp;nbsp;Docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install agentic-doc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python version 3.9, 3.10, 3.11 or 3.12&lt;/li&gt; &#xA; &lt;li&gt;LandingAI agentic AI API key (get the key &lt;a href=&#34;https://va.landing.ai/settings/api-key&#34;&gt;here&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Set the API Key as an Environment Variable&lt;/h3&gt; &#xA;&lt;p&gt;After you get the LandingAI agentic AI API key, set the key as an environment variable (or put it in a &lt;code&gt;.env&lt;/code&gt; file):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export VISION_AGENT_API_KEY=&amp;lt;your-api-key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Supported Files&lt;/h3&gt; &#xA;&lt;p&gt;The library can extract data from:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PDFs (any length)&lt;/li&gt; &#xA; &lt;li&gt;Images that are supported by OpenCV-Python (i.e. the &lt;code&gt;cv2&lt;/code&gt; library)&lt;/li&gt; &#xA; &lt;li&gt;URLs pointing to PDF or image files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Basic Usage&lt;/h3&gt; &#xA;&lt;h4&gt;Extract Data from One Document&lt;/h4&gt; &#xA;&lt;p&gt;Run the following script to extract data from one document and return the results in both markdown and structured chunks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse&#xA;&#xA;# Parse a local file&#xA;result = parse(&#34;path/to/image.png&#34;)&#xA;print(result[0].markdown)  # Get the extracted data as markdown&#xA;print(result[0].chunks)  # Get the extracted data as structured chunks of content&#xA;&#xA;# Parse a document from a URL&#xA;result = parse(&#34;https://example.com/document.pdf&#34;)&#xA;print(result[0].markdown)&#xA;&#xA;# Legacy approach (still supported)&#xA;from agentic_doc.parse import parse_documents&#xA;results = parse_documents([&#34;path/to/image.png&#34;])&#xA;parsed_doc = results[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Extract Data from Multiple Documents&lt;/h4&gt; &#xA;&lt;p&gt;Run the following script to extract data from multiple documents.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse&#xA;&#xA;# Parse multiple local files&#xA;file_paths = [&#34;path/to/your/document1.pdf&#34;, &#34;path/to/another/document2.pdf&#34;]&#xA;results = parse(file_paths)&#xA;for result in results:&#xA;    print(result.markdown)&#xA;&#xA;# Parse and save results to a directory&#xA;results = parse(file_paths, result_save_dir=&#34;path/to/save/results&#34;)&#xA;result_paths = []&#xA;for result in results:&#xA;    result_paths.append(result.result_path)&#xA;# result_paths: [&#34;path/to/save/results/document1_20250313_070305.json&#34;, ...]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using field extraction&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pydantic import BaseModel, Field&#xA;from agentic_doc.parse import parse&#xA;&#xA;class ExtractedFields(BaseModel):&#xA;    employee_name: str = Field(description=&#34;the full name of the employee&#34;)&#xA;    employee_ssn: str = Field(description=&#34;the social security number of the employee&#34;)&#xA;    gross_pay: float = Field(description=&#34;the gross pay of the employee&#34;)&#xA;    employee_address: str = Field(description=&#34;the address of the employee&#34;)&#xA;&#xA;results = parse(&#34;mydoc.pdf&#34;, extraction_model=ExtractedFields)&#xA;fields = results[0].extraction&#xA;metadata = results[0].extraction_metadata&#xA;print(f&#34;Field value: {fields.employee_name}, confidence: {metadata.employee_name.experimental_confidence}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Extract Data Using Connectors&lt;/h4&gt; &#xA;&lt;p&gt;The library now supports various connectors to easily access documents from different sources:&lt;/p&gt; &#xA;&lt;h5&gt;Google Drive Connector&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisites: Follow the &lt;a href=&#34;https://developers.google.com/workspace/drive/api/quickstart/python&#34;&gt;Google Drive API Python Quickstart&lt;/a&gt; tutorial first to set up your credentials.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Google Drive API quickstart will guide you through:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Creating a Google Cloud project&lt;/li&gt; &#xA; &lt;li&gt;Enabling the Google Drive API&lt;/li&gt; &#xA; &lt;li&gt;Setting up OAuth 2.0 credentials&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;After completing the quickstart tutorial, you can use the Google Drive connector as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse&#xA;from agentic_doc.connectors import GoogleDriveConnectorConfig&#xA;&#xA;# Using OAuth credentials file (from quickstart tutorial)&#xA;config = GoogleDriveConnectorConfig(&#xA;    client_secret_file=&#34;path/to/credentials.json&#34;,&#xA;    folder_id=&#34;your-google-drive-folder-id&#34;  # Optional&#xA;)&#xA;&#xA;# Parse all documents in the folder&#xA;results = parse(config)&#xA;&#xA;# Parse with filtering&#xA;results = parse(config, connector_pattern=&#34;*.pdf&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Amazon S3 Connector&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse&#xA;from agentic_doc.connectors import S3ConnectorConfig&#xA;&#xA;config = S3ConnectorConfig(&#xA;    bucket_name=&#34;your-bucket-name&#34;,&#xA;    aws_access_key_id=&#34;your-access-key&#34;,  # Optional if using IAM roles&#xA;    aws_secret_access_key=&#34;your-secret-key&#34;,  # Optional if using IAM roles&#xA;    region_name=&#34;us-east-1&#34;&#xA;)&#xA;&#xA;# Parse all documents in the bucket&#xA;results = parse(config)&#xA;&#xA;# Parse documents in a specific prefix/folder&#xA;results = parse(config, connector_path=&#34;documents/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Local Directory Connector&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse&#xA;from agentic_doc.connectors import LocalConnectorConfig&#xA;&#xA;config = LocalConnectorConfig()&#xA;&#xA;# Parse all supported documents in a directory&#xA;results = parse(config, connector_path=&#34;/path/to/documents&#34;)&#xA;&#xA;# Parse with pattern filtering&#xA;results = parse(config, connector_path=&#34;/path/to/documents&#34;, connector_pattern=&#34;*.pdf&#34;)&#xA;&#xA;# Parse all supported documents in a directory recursively (search subdirectories as well)&#xA;config = LocalConnectorConfig(recursive=True)&#xA;results = parse(config, connector_path=&#34;/path/to/documents&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;URL Connector&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse&#xA;from agentic_doc.connectors import URLConnectorConfig&#xA;&#xA;config = URLConnectorConfig(&#xA;    headers={&#34;Authorization&#34;: &#34;Bearer your-token&#34;},  # Optional&#xA;    timeout=60  # Optional&#xA;)&#xA;&#xA;# Parse document from URL&#xA;results = parse(config, connector_path=&#34;https://example.com/document.pdf&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Raw Bytes Input&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse&#xA;&#xA;# Load a PDF or image file as bytes&#xA;with open(&#34;document.pdf&#34;, &#34;rb&#34;) as f:&#xA;    raw_bytes = f.read()&#xA;&#xA;# Parse the document from bytes&#xA;results = parse(raw_bytes)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also parse image bytes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with open(&#34;image.png&#34;, &#34;rb&#34;) as f:&#xA;    image_bytes = f.read()&#xA;&#xA;results = parse(image_bytes)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is useful when documents are already loaded into memory (e.g., from an API response or uploaded via a web interface). The parser will auto-detect the file type from the bytes.&lt;/p&gt; &#xA;&lt;h2&gt;Why Use It?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simplified Setup:&lt;/strong&gt; No need to manage API keys or handle low-level REST calls.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic Large File Processing:&lt;/strong&gt; Splits large PDFs into manageable parts and processes them in parallel.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Built-In Error Handling:&lt;/strong&gt; Automatically retries requests with exponential backoff and jitter for common HTTP errors.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parallel Processing:&lt;/strong&gt; Efficiently parse multiple documents at once with configurable parallelism.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Main Features&lt;/h2&gt; &#xA;&lt;p&gt;With this library, you can do things that are otherwise hard to do with the Agentic Document Extraction API alone. This section describes some of the key features this library offers.&lt;/p&gt; &#xA;&lt;h3&gt;Parse Large PDF Files&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;A single REST API call can only handle up to certain amount of pages at a time&lt;/strong&gt; (see &lt;a href=&#34;https://docs.landing.ai/ade/ade-rate-limits#maximum-pages-per-document&#34;&gt;rate limits&lt;/a&gt;). This library automatically splits a large PDF into multiple calls, uses a thread pool to process the calls in parallel, and stitches the results back together as a single result.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve used this library to successfully parse PDFs that are 1000+ pages long.&lt;/p&gt; &#xA;&lt;h3&gt;Parse Multiple Files in a Batch&lt;/h3&gt; &#xA;&lt;p&gt;You can parse multiple files in a single function call with this library. The library processes files in parallel.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You can change the parallelism by setting the &lt;code&gt;batch_size&lt;/code&gt; setting.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Save Groundings as Images&lt;/h3&gt; &#xA;&lt;p&gt;The library can extract and save the visual regions (groundings) of the document where each chunk of content was found. This is useful for visualizing exactly what parts of the document were extracted and for debugging extraction issues.&lt;/p&gt; &#xA;&lt;p&gt;Each grounding represents a bounding box in the original document, and the library can save these regions as individual PNG images. The images are organized by page number and chunk ID.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s how to use this feature:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse_documents&#xA;&#xA;# Save groundings when parsing a document&#xA;results = parse_documents(&#xA;    [&#34;path/to/document.pdf&#34;],&#xA;    grounding_save_dir=&#34;path/to/save/groundings&#34;&#xA;)&#xA;&#xA;# The grounding images will be saved to:&#xA;# path/to/save/groundings/document_TIMESTAMP/page_X/CHUNK_TYPE_CHUNK_ID_Y.png&#xA;# Where X is the page number, CHUNK_ID is the unique ID of each chunk,&#xA;# and Y is the index of the grounding within the chunk&#xA;&#xA;# Each chunk&#39;s grounding in the result will have the image_path set&#xA;for chunk in results[0].chunks:&#xA;    for grounding in chunk.grounding:&#xA;        if grounding.image_path:&#xA;            print(f&#34;Grounding saved to: {grounding.image_path}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This feature works with all parsing functions: &lt;code&gt;parse_documents&lt;/code&gt;, &lt;code&gt;parse_and_save_documents&lt;/code&gt;, and &lt;code&gt;parse_and_save_document&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Visualize Parsing Result&lt;/h3&gt; &#xA;&lt;p&gt;The library provides a visualization utility that creates annotated images showing where each chunk of content was extracted from the document. This is useful for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Verifying the accuracy of the extraction&lt;/li&gt; &#xA; &lt;li&gt;Debugging extraction issues&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s how to use the visualization feature:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse&#xA;from agentic_doc.utils import viz_parsed_document&#xA;from agentic_doc.config import VisualizationConfig&#xA;&#xA;# Parse a document&#xA;results = parse(&#34;path/to/document.pdf&#34;)&#xA;parsed_doc = results[0]&#xA;&#xA;# Create visualizations with default settings&#xA;# The output images have a PIL.Image.Image type&#xA;images = viz_parsed_document(&#xA;    &#34;path/to/document.pdf&#34;,&#xA;    parsed_doc,&#xA;    output_dir=&#34;path/to/save/visualizations&#34;&#xA;)&#xA;&#xA;# Or customize the visualization appearance&#xA;viz_config = VisualizationConfig(&#xA;    thickness=2,  # Thicker bounding boxes&#xA;    text_bg_opacity=0.8,  # More opaque text background&#xA;    font_scale=0.7,  # Larger text&#xA;    # Custom colors for different chunk types&#xA;    color_map={&#xA;        ChunkType.TITLE: (0, 0, 255),  # Red for titles&#xA;        ChunkType.TEXT: (255, 0, 0),  # Blue for regular text&#xA;        # ... other chunk types ...&#xA;    }&#xA;)&#xA;&#xA;images = viz_parsed_document(&#xA;    &#34;path/to/document.pdf&#34;,&#xA;    parsed_doc,&#xA;    output_dir=&#34;path/to/save/visualizations&#34;,&#xA;    viz_config=viz_config&#xA;)&#xA;&#xA;# The visualization images will be saved as:&#xA;# path/to/save/visualizations/document_viz_page_X.png&#xA;# Where X is the page number&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The visualization shows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bounding boxes around each extracted chunk&lt;/li&gt; &#xA; &lt;li&gt;Chunk type and index labels&lt;/li&gt; &#xA; &lt;li&gt;Different colors for different types of content (titles, text, tables, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Semi-transparent text backgrounds for better readability&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Automatically Handle API Errors and Rate Limits with Retries&lt;/h3&gt; &#xA;&lt;p&gt;The REST API endpoint imposes rate limits per API key. This library automatically handles the rate limit error or other intermittent HTTP errors with retries.&lt;/p&gt; &#xA;&lt;p&gt;For more information, see &lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#error-handling&#34;&gt;Error Handling&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#configuration-options&#34;&gt;Configuration Options&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Error Handling&lt;/h3&gt; &#xA;&lt;p&gt;This library implements a retry mechanism for handling API failures:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Retries are performed for these HTTP status codes: 408, 429, 502, 503, 504.&lt;/li&gt; &#xA; &lt;li&gt;Exponential backoff with jitter is used for retry wait time.&lt;/li&gt; &#xA; &lt;li&gt;The initial retry wait time is 1 second, which increases exponentially.&lt;/li&gt; &#xA; &lt;li&gt;Retry will stop after &lt;code&gt;max_retries&lt;/code&gt; attempts. Exceeding the limit raises an exception and results in a failure for this request.&lt;/li&gt; &#xA; &lt;li&gt;Retry wait time is capped at &lt;code&gt;max_retry_wait_time&lt;/code&gt; seconds.&lt;/li&gt; &#xA; &lt;li&gt;Retries include a random jitter of up to 10 seconds to distribute requests and prevent the thundering herd problem.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Parsing Errors&lt;/h3&gt; &#xA;&lt;p&gt;If the REST API request encounters an unrecoverable error during parsing (either from client-side or server-side), the library includes an &lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/agentic_doc/common.py#L75&#34;&gt;errors&lt;/a&gt; field in the final result for the affected page(s). Each error contains the error message, error_code and corresponding page number.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration Options&lt;/h2&gt; &#xA;&lt;p&gt;The library uses a &lt;a href=&#34;https://raw.githubusercontent.com/landing-ai/agentic-doc/main/agentic_doc/config.py&#34;&gt;&lt;code&gt;Settings&lt;/code&gt;&lt;/a&gt; object to manage configuration. You can customize these settings either through environment variables or a &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;p&gt;Below is an example &lt;code&gt;.env&lt;/code&gt; file that customizes the configurations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Number of files to process in parallel, defaults to 4&#xA;BATCH_SIZE=4&#xA;# Number of threads used to process parts of each file in parallel, defaults to 5.&#xA;MAX_WORKERS=2&#xA;# Maximum number of retry attempts for failed intermittent requests, defaults to 100&#xA;MAX_RETRIES=80&#xA;# Maximum wait time in seconds for each retry, defaults to 60&#xA;MAX_RETRY_WAIT_TIME=30&#xA;# Logging style for retry, defaults to log_msg&#xA;RETRY_LOGGING_STYLE=log_msg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Max Parallelism&lt;/h3&gt; &#xA;&lt;p&gt;The maximum number of parallel requests is determined by multiplying &lt;code&gt;BATCH_SIZE&lt;/code&gt; √ó &lt;code&gt;MAX_WORKERS&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; The maximum parallelism allowed by this library is 100.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Specifically, increasing &lt;code&gt;MAX_WORKERS&lt;/code&gt; can speed up the processing of large individual files, while increasing &lt;code&gt;BATCH_SIZE&lt;/code&gt; improves throughput when processing multiple files.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Your job&#39;s maximum processing throughput may be limited by your API rate limit. If your rate limit isn&#39;t high enough, you may encounter rate limit errors, which the library will automatically handle through retries.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The optimal values for &lt;code&gt;MAX_WORKERS&lt;/code&gt; and &lt;code&gt;BATCH_SIZE&lt;/code&gt; depend on your API rate limit and the latency of each REST API call. For example, if your account has a rate limit of 5 requests per minute, and each REST API call takes approximately 60 seconds to complete, and you&#39;re processing a single large file, then &lt;code&gt;MAX_WORKERS&lt;/code&gt; should be set to 5 and &lt;code&gt;BATCH_SIZE&lt;/code&gt; to 1.&lt;/p&gt; &#xA;&lt;p&gt;You can find your REST API latency in the logs. If you want to increase your rate limit, schedule a time to meet with us &lt;a href=&#34;https://scheduler.zoom.us/d/56i81uc2/landingai-document-extraction&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Set &lt;code&gt;RETRY_LOGGING_STYLE&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;RETRY_LOGGING_STYLE&lt;/code&gt; setting controls how the library logs the retry attempts.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;log_msg&lt;/code&gt;: Log the retry attempts as a log messages. Each attempt is logged as a separate message. This is the default setting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inline_block&lt;/code&gt;: Print a yellow progress block (&#39;‚ñà&#39;) on the same line. Each block represents one retry attempt. Choose this if you don&#39;t want to see the verbose retry logging message and still want to track the number of retries that have been made.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;none&lt;/code&gt;: Do not log the retry attempts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Troubleshooting &amp;amp; FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Common Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Key Errors:&lt;/strong&gt;&lt;br&gt; Ensure your API key is correctly set as an environment variable.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rate Limits:&lt;/strong&gt;&lt;br&gt; The library automatically retries requests if you hit the API rate limit. Adjust &lt;code&gt;BATCH_SIZE&lt;/code&gt; or &lt;code&gt;MAX_WORKERS&lt;/code&gt; if you encounter frequent rate limit errors.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parsing Failures:&lt;/strong&gt;&lt;br&gt; If a document fails to parse, an error chunk will be included in the result, detailing the error message and page index.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;URL Access Issues:&lt;/strong&gt; If you&#39;re having trouble accessing documents from URLs, check that the URLs are publicly accessible and point to supported file types (PDF or images).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Note on &lt;code&gt;include_marginalia&lt;/code&gt; and &lt;code&gt;include_metadata_in_markdown&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;include_marginalia&lt;/code&gt;: If True, the parser will attempt to extract and include marginalia (footer notes, page number, etc.) from the document in the output.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;include_metadata_in_markdown&lt;/code&gt;: If True, the output markdown will include metadata.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Both parameters default to True. You can set them to False to exclude these elements from the output.&lt;/p&gt; &#xA;&lt;h4&gt;Example: Using the new parameters&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agentic_doc.parse import parse&#xA;&#xA;results = parse(&#xA;    &#34;path/to/document.pdf&#34;,&#xA;    include_marginalia=False,  # Exclude marginalia from output&#xA;    include_metadata_in_markdown=False  # Exclude metadata from markdown&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>