<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-22T01:37:45Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA-NeMo/RL</title>
    <updated>2025-08-22T01:37:45Z</updated>
    <id>tag:github.com,2025-08-22:/NVIDIA-NeMo/RL</id>
    <link href="https://github.com/NVIDIA-NeMo/RL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scalable toolkit for efficient model reinforcement&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Nemo RL: A Scalable and Efficient Post-Training Library&lt;/h1&gt; &#xA;&lt;h2&gt;üì£ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[7/25/2025] &lt;a href=&#34;https://github.com/NVIDIA-NeMo/RL/releases/tag/v0.3.0&#34;&gt;Release v0.3.0!&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üìù &lt;a href=&#34;https://nvidia-nemo.github.io/blog/2025/07/21/nemo-rl-v0.3/&#34;&gt;v0.3.0 Blog Post&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;üìä View the release run metrics on &lt;a href=&#34;https://colab.research.google.com/drive/15kpesCV1m_C5UQFStssTEjaN2RsBMeZ0?usp=sharing&#34;&gt;Google Colab&lt;/a&gt; to get a head start on your experimentation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[5/14/2025] &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/grpo-deepscaler.md&#34;&gt;Reproduce DeepscaleR with NeMo RL!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[5/14/2025] &lt;a href=&#34;https://github.com/NVIDIA-NeMo/RL/releases/tag/v0.2.1&#34;&gt;Release v0.2.1!&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üìä View the release run metrics on &lt;a href=&#34;https://colab.research.google.com/drive/1o14sO0gj_Tl_ZXGsoYip3C0r5ofkU1Ey?usp=sharing&#34;&gt;Google Colab&lt;/a&gt; to get a head start on your experimentation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;!-- markdown all in one --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#nemo-rl-a-scalable-and-efficient-post-training-library&#34;&gt;Nemo RL: A Scalable and Efficient Post-Training Library&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#-news&#34;&gt;üì£ News&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#training-backends&#34;&gt;Training Backends&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo&#34;&gt;GRPO&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-single-node&#34;&gt;GRPO Single Node&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-multi-node&#34;&gt;GRPO Multi-node&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-qwen25-32b&#34;&gt;GRPO Qwen2.5-32B&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-multi-turn&#34;&gt;GRPO Multi-Turn&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#supervised-fine-tuning-sft&#34;&gt;Supervised Fine-Tuning (SFT)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#sft-single-node&#34;&gt;SFT Single Node&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#sft-multi-node&#34;&gt;SFT Multi-node&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo&#34;&gt;DPO&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo-single-node&#34;&gt;DPO Single Node&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo-multi-node&#34;&gt;DPO Multi-node&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm&#34;&gt;RM&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm-single-node&#34;&gt;RM Single Node&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm-multi-node&#34;&gt;RM Multi-node&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#convert-model-format-optional&#34;&gt;Convert Model Format (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#run-evaluation&#34;&gt;Run Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#set-up-clusters&#34;&gt;Set Up Clusters&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#tips-and-tricks&#34;&gt;Tips and Tricks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#licenses&#34;&gt;Licenses&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nemo RL&lt;/strong&gt; is a scalable and efficient post-training library designed for models ranging from 1 GPU to thousands, and from tiny to over 100 billion parameters.&lt;/p&gt; &#xA;&lt;p&gt;What you can expect:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Seamless integration with Hugging Face&lt;/strong&gt; for ease of use, allowing users to leverage a wide range of pre-trained models and tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance implementation with Megatron Core&lt;/strong&gt;, supporting various parallelism techniques for large models (&amp;gt;100B) and large context lengths.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient resource management using Ray&lt;/strong&gt;, enabling scalable and flexible deployment across different hardware configurations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; with a modular design that allows easy integration and customization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive documentation&lt;/strong&gt; that is both detailed and user-friendly, with practical examples.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;‚úÖ &lt;em&gt;Available now&lt;/em&gt; | üîú &lt;em&gt;Coming in v0.4&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Fast Generation&lt;/strong&gt; - vLLM backend for optimized inference.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;HuggingFace Integration&lt;/strong&gt; - Works with 1-70B models (Qwen, Llama).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Distributed Training&lt;/strong&gt; - Fully Sharded Data Parallel (FSDP2) support and Ray-based infrastructure.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Environment Support&lt;/strong&gt; - Support for multi-environment training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Learning Algorithms&lt;/strong&gt; - GRPO (Group Relative Policy Optimization), SFT (Supervised Fine-Tuning), and DPO (Direct Preference Optimization).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Multi-Turn RL&lt;/strong&gt; - Multi-turn generation and training for RL with tool use, games, etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Large Model Support&lt;/strong&gt; - Native PyTorch support for models up to 70B parameters.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Advanced Parallelism&lt;/strong&gt; - PyTorch native FSDP2, TP, CP, and SP for efficient training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;(even) Larger Model Support with Long(er) Sequences&lt;/strong&gt; - Advanced parallelisms with Megatron Core (TP/PP/CP/SP/EP).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Worker Isolation&lt;/strong&gt; - Process isolation between RL Actors (no worries about global state).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Environment Isolation&lt;/strong&gt; - Dependency isolation between components.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Megatron Inference&lt;/strong&gt; - (static) Megatron Inference for day-0 support for new megatron models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;MoE Models&lt;/strong&gt; - Support for DeepseekV3 and Qwen-3 MoE models&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Sequence Packing&lt;/strong&gt; - Sequence packing in both DTensor and MCore for huge training perf gains&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîú &lt;strong&gt;Improved Native Performance&lt;/strong&gt; - Improve training time for Native Pytorch Models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîú &lt;strong&gt;Megatron Inference&lt;/strong&gt; - (dynamic) Megatron Inference for fast day-0 support for new megatron models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Clone &lt;strong&gt;NeMo RL&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone git@github.com:NVIDIA-NeMo/RL.git nemo-rl&#xA;cd nemo-rl&#xA;&#xA;# If you are using the Megatron backend, download the pinned versions of Megatron-LM and NeMo submodules &#xA;# by running (This is not necessary if you are using the pure Pytorch/DTensor path):&#xA;git submodule update --init --recursive&#xA;&#xA;# Different branches of the repo can have different pinned versions of these third-party submodules. Ensure&#xA;# submodules are automatically updated after switching branches or pulling updates by configuring git with:&#xA;# git config submodule.recurse true&#xA;&#xA;# **NOTE**: this setting will not download **new** or remove **old** submodules with the branch&#39;s changes.&#xA;# You will have to run the full `git submodule update --init --recursive` command in these situations.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using the Megatron backend on bare-metal (outside of a container), you may need to install the cudnn headers as well. Here is how you can check as well as install them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Check if you have libcudnn installed&#xA;dpkg -l | grep cudnn.*cuda&#xA;&#xA;# Find the version you need here: https://developer.nvidia.com/cudnn-downloads?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;Distribution=Ubuntu&amp;amp;target_version=20.04&amp;amp;target_type=deb_network&#xA;# As an example, these are the &#34;Linux Ubuntu 20.04 x86_64&#34; instructions&#xA;wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb&#xA;sudo dpkg -i cuda-keyring_1.1-1_all.deb&#xA;sudo apt-get update&#xA;sudo apt-get install cudnn-cuda-12&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For faster setup and environment isolation, we use &lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;uv&lt;/a&gt;. Follow &lt;a href=&#34;https://docs.astral.sh/uv/getting-started/installation/&#34;&gt;these instructions&lt;/a&gt; to install uv.&lt;/p&gt; &#xA;&lt;p&gt;Then, initialize NeMo RL project virtual environment via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Please do not use &lt;code&gt;-p/--python&lt;/code&gt; and instead allow &lt;code&gt;uv venv&lt;/code&gt; to read it from &lt;code&gt;.python-version&lt;/code&gt;. This ensures that the version of python used is always what we prescribe.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If working outside a container, it can help to build &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;flash-attn&lt;/a&gt; and warm the uv cache before your first run.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash tools/build-flash-attn-in-uv-cache.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] On the first install, &lt;code&gt;flash-attn&lt;/code&gt; can take a while to install (~45min with 48 CPU hyperthreads). After it is built once, it is cached in your uv&#39;s cache dir making subsequent installs much quicker.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] The NeMo RL Dockerfile will warm the uv cache with flash-attn. See &lt;a href=&#34;https://docs.nvidia.com/nemo/rl/latest/docker.html&#34;&gt;https://docs.nvidia.com/nemo/rl/latest/docker.html&lt;/a&gt; for instructions if you are looking for the NeMo RL container.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If sucessful, you should see &lt;code&gt;‚úÖ flash-attn successfully added to uv cache&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;code&gt;uv run&lt;/code&gt; to launch all commands. It handles pip installing implicitly and ensures your environment is up to date with our lock file.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;It is not recommended to activate the &lt;code&gt;venv&lt;/code&gt;, and you should use &lt;code&gt;uv run &amp;lt;command&amp;gt;&lt;/code&gt; instead to execute scripts within the managed environment. This ensures consistent environment usage across different shells and sessions. Example: &lt;code&gt;uv run python examples/run_grpo_math.py&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ensure you have the necessary CUDA drivers and PyTorch installed compatible with your hardware.&lt;/li&gt; &#xA;  &lt;li&gt;If you update your environment in &lt;code&gt;pyproject.toml&lt;/code&gt;, it is necessary to force a rebuild of the virtual environments by setting &lt;code&gt;NRL_FORCE_REBUILD_VENVS=true&lt;/code&gt; next time you launch a run.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Reminder&lt;/strong&gt;: Don&#39;t forget to set your &lt;code&gt;HF_HOME&lt;/code&gt;, &lt;code&gt;WANDB_API_KEY&lt;/code&gt;, and &lt;code&gt;HF_DATASETS_CACHE&lt;/code&gt; (if needed). You&#39;ll need to do a &lt;code&gt;huggingface-cli login&lt;/code&gt; as well for Llama models.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Training Backends&lt;/h2&gt; &#xA;&lt;p&gt;NeMo RL supports multiple training backends to accommodate different model sizes and hardware configurations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;DTensor (FSDP2)&lt;/strong&gt; - PyTorch&#39;s next-generation distributed training with improved memory efficiency&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Megatron&lt;/strong&gt; - NVIDIA&#39;s high-performance training framework for scaling to large models (&amp;gt;100B parameters)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The training backend is automatically determined based on your YAML configuration settings. For detailed information on backend selection, configuration, and examples, see the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/training-backends.md&#34;&gt;Training Backends documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;GRPO&lt;/h2&gt; &#xA;&lt;p&gt;We have a reference GRPO experiment config set up trained for math benchmarks using the &lt;a href=&#34;https://huggingface.co/datasets/nvidia/OpenMathInstruct-2&#34;&gt;OpenInstructMath2&lt;/a&gt; dataset.&lt;/p&gt; &#xA;&lt;p&gt;You can read about the details of the GRPO implementation &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/grpo.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;GRPO Single Node&lt;/h3&gt; &#xA;&lt;p&gt;To run GRPO on a single GPU for &lt;code&gt;Qwen/Qwen2.5-1.5B&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run the GRPO math example using a 1B parameter model&#xA;uv run python examples/run_grpo_math.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this uses the configuration in &lt;code&gt;examples/configs/grpo_math_1B.yaml&lt;/code&gt;. You can customize parameters with command-line overrides. For example, to run on 8 GPUs,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run the GRPO math example using a 1B parameter model using 8 GPUs&#xA;uv run python examples/run_grpo_math.py \&#xA;  cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can override any of the parameters listed in the yaml configuration file. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_grpo_math.py \&#xA;  policy.model_name=&#34;meta-llama/Llama-3.2-1B-Instruct&#34; \&#xA;  checkpointing.checkpoint_dir=&#34;results/llama1b_math&#34; \&#xA;  logger.wandb_enabled=True \&#xA;  logger.wandb.name=&#34;grpo-llama1b_math&#34; \&#xA;  logger.num_val_samples_to_print=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default configuration uses the DTensor training backend. We also provide a config &lt;code&gt;examples/configs/grpo_math_1B_megatron.yaml&lt;/code&gt; which is set up to use the Megatron backend out of the box.&lt;/p&gt; &#xA;&lt;p&gt;To train using this config on a single GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run a GRPO math example on 1 GPU using the Megatron backend&#xA;uv run python examples/run_grpo_math.py \&#xA;  --config examples/configs/grpo_math_1B_megatron.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional details on supported backends and how to configure the training backend to suit your setup, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/training-backends.md&#34;&gt;Training Backends documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;GRPO Multi-node&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;NUM_ACTOR_NODES=2&#xA;&#xA;# grpo_math_8b uses Llama-3.1-8B-Instruct model&#xA;COMMAND=&#34;uv run ./examples/run_grpo_math.py --config examples/configs/grpo_math_8B.yaml cluster.num_nodes=2 checkpointing.checkpoint_dir=&#39;results/llama8b_2nodes&#39; logger.wandb_enabled=True logger.wandb.name=&#39;grpo-llama8b_math&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The required &lt;code&gt;CONTAINER&lt;/code&gt; can be built by following the instructions in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/docker.md&#34;&gt;Docker documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;GRPO Qwen2.5-32B&lt;/h4&gt; &#xA;&lt;p&gt;This section outlines how to run GRPO for Qwen2.5-32B with a 16k sequence length.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;NUM_ACTOR_NODES=32&#xA;&#xA;# Download Qwen before the job starts to avoid spending time downloading during the training loop&#xA;HF_HOME=/path/to/hf_home huggingface-cli download Qwen/Qwen2.5-32B&#xA;&#xA;# Ensure HF_HOME is included in your MOUNTS&#xA;HF_HOME=/path/to/hf_home \&#xA;COMMAND=&#34;uv run ./examples/run_grpo_math.py --config examples/configs/grpo_math_8B.yaml policy.model_name=&#39;Qwen/Qwen2.5-32B&#39; policy.generation.vllm_cfg.tensor_parallel_size=4 policy.max_total_sequence_length=16384 cluster.num_nodes=${NUM_ACTOR_NODES} policy.dtensor_cfg.enabled=True policy.dtensor_cfg.tensor_parallel_size=8 policy.dtensor_cfg.sequence_parallel=True policy.dtensor_cfg.activation_checkpointing=True checkpointing.checkpoint_dir=&#39;results/qwen2.5-32b&#39; logger.wandb_enabled=True logger.wandb.name=&#39;qwen2.5-32b&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;GRPO Multi-Turn&lt;/h4&gt; &#xA;&lt;p&gt;We also support multi-turn generation and training (tool use, games, etc.). Reference example for training to play a Sliding Puzzle Game:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_grpo_sliding_puzzle.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Supervised Fine-Tuning (SFT)&lt;/h2&gt; &#xA;&lt;p&gt;We provide an example SFT experiment using the &lt;a href=&#34;https://rajpurkar.github.io/SQuAD-explorer/&#34;&gt;SQuAD dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SFT Single Node&lt;/h3&gt; &#xA;&lt;p&gt;The default SFT configuration is set to run on a single GPU. To start the experiment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_sft.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This fine-tunes the &lt;code&gt;Llama3.2-1B&lt;/code&gt; model on the SQuAD dataset using a 1 GPU.&lt;/p&gt; &#xA;&lt;p&gt;To use multiple GPUs on a single node, you can modify the cluster configuration. This adjustment will also let you potentially increase the model and batch size:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_sft.py \&#xA;  policy.model_name=&#34;meta-llama/Meta-Llama-3-8B&#34; \&#xA;  policy.train_global_batch_size=128 \&#xA;  sft.val_global_batch_size=128 \&#xA;  cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;code&gt;examples/configs/sft.yaml&lt;/code&gt; for a full list of parameters that can be overridden.&lt;/p&gt; &#xA;&lt;h3&gt;SFT Multi-node&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;NUM_ACTOR_NODES=2&#xA;&#xA;COMMAND=&#34;uv run ./examples/run_sft.py --config examples/configs/sft.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 checkpointing.checkpoint_dir=&#39;results/sft_llama8b_2nodes&#39; logger.wandb_enabled=True logger.wandb.name=&#39;sft-llama8b&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;DPO&lt;/h2&gt; &#xA;&lt;p&gt;We provide a sample DPO experiment that uses the &lt;a href=&#34;https://huggingface.co/datasets/nvidia/HelpSteer3&#34;&gt;HelpSteer3 dataset&lt;/a&gt; for preference-based training.&lt;/p&gt; &#xA;&lt;h3&gt;DPO Single Node&lt;/h3&gt; &#xA;&lt;p&gt;The default DPO experiment is configured to run on a single GPU. To launch the experiment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_dpo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This trains &lt;code&gt;Llama3.2-1B-Instruct&lt;/code&gt; on one GPU.&lt;/p&gt; &#xA;&lt;p&gt;If you have access to more GPUs, you can update the experiment accordingly. To run on 8 GPUs, we update the cluster configuration and switch to an 8B Llama3.1 Instruct model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_dpo.py \&#xA;  policy.model_name=&#34;meta-llama/Llama-3.1-8B-Instruct&#34; \&#xA;  policy.train_global_batch_size=256 \&#xA;  cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Any of the DPO parameters can be customized from the command line. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_dpo.py \&#xA;  dpo.sft_loss_weight=0.1 \&#xA;  dpo.preference_average_log_probs=True \&#xA;  checkpointing.checkpoint_dir=&#34;results/llama_dpo_sft&#34; \&#xA;  logger.wandb_enabled=True \&#xA;  logger.wandb.name=&#34;llama-dpo-sft&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;code&gt;examples/configs/dpo.yaml&lt;/code&gt; for a full list of parameters that can be overridden. For an in-depth explanation of how to add your own DPO dataset, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/dpo.md&#34;&gt;DPO documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;DPO Multi-node&lt;/h3&gt; &#xA;&lt;p&gt;For distributed DPO training across multiple nodes, modify the following script for your use case:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;## number of nodes to use for your job&#xA;NUM_ACTOR_NODES=2&#xA;&#xA;COMMAND=&#34;uv run ./examples/run_dpo.py --config examples/configs/dpo.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 dpo.val_global_batch_size=32 checkpointing.checkpoint_dir=&#39;results/dpo_llama81_2nodes&#39; logger.wandb_enabled=True logger.wandb.name=&#39;dpo-llama1b&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;RM&lt;/h2&gt; &#xA;&lt;p&gt;We provide a sample RM experiment that uses the &lt;a href=&#34;https://huggingface.co/datasets/nvidia/HelpSteer3&#34;&gt;HelpSteer3 dataset&lt;/a&gt; for preference-based training.&lt;/p&gt; &#xA;&lt;h3&gt;RM Single Node&lt;/h3&gt; &#xA;&lt;p&gt;The default RM experiment is configured to run on a single GPU. To launch the experiment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_rm.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This trains a RM based on &lt;code&gt;meta-llama/Llama-3.2-1B-Instruct&lt;/code&gt; on one GPU.&lt;/p&gt; &#xA;&lt;p&gt;If you have access to more GPUs, you can update the experiment accordingly. To run on 8 GPUs, we update the cluster configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_rm.py cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/rm.md&#34;&gt;RM documentation&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h3&gt;RM Multi-node&lt;/h3&gt; &#xA;&lt;p&gt;For distributed RM training across multiple nodes, modify the following script for your use case:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;## number of nodes to use for your job&#xA;NUM_ACTOR_NODES=2&#xA;&#xA;COMMAND=&#34;uv run ./examples/run_rm.py --config examples/configs/rm.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 checkpointing.checkpoint_dir=&#39;results/rm_llama1b_2nodes&#39; logger.wandb_enabled=True logger.wandb.name=&#39;rm-llama1b-2nodes&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We provide evaluation tools to assess model capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;Convert Model Format (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;If you have trained a model and saved the checkpoint in the Pytorch DCP format, you first need to convert it to the Hugging Face format before running evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Example for a GRPO checkpoint at step 170&#xA;uv run python examples/converters/convert_dcp_to_hf.py \&#xA;    --config results/grpo/step_170/config.yaml \&#xA;    --dcp-ckpt-path results/grpo/step_170/policy/weights/ \&#xA;    --hf-ckpt-path results/grpo/hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have a model saved in Megatron format, you can use the following command to convert it to Hugging Face format prior to running evaluation. This script requires mcore, so make sure to launch with the mcore extra:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Example for a GRPO checkpoint at step 170&#xA;uv run --extra mcore python examples/converters/convert_megatron_to_hf.py \&#xA;    --config results/grpo/step_170/config.yaml \&#xA;    --megatron-ckpt-path results/grpo/step_170/policy/weights/iter_0000000 \&#xA;    --hf-ckpt-path results/grpo/hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Adjust the paths according to your training output directory structure.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For an in-depth explanation of checkpointing, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/checkpointing.md&#34;&gt;Checkpointing documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Run evaluation script with converted model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_eval.py generation.model_name=$PWD/results/grpo/hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run evaluation script with custom settings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Example: Evaluation of DeepScaleR-1.5B-Preview on MATH-500 using 8 GPUs&#xA;#          Pass@1 accuracy averaged over 16 samples for each problem&#xA;uv run python examples/run_eval.py \&#xA;    --config examples/configs/evals/math_eval.yaml \&#xA;    generation.model_name=agentica-org/DeepScaleR-1.5B-Preview \&#xA;    generation.temperature=0.6 \&#xA;    generation.top_p=0.95 \&#xA;    generation.vllm_cfg.max_model_len=32768 \&#xA;    data.dataset_name=math500 \&#xA;    eval.num_tests_per_prompt=16 \&#xA;    cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Evaluation results may vary slightly due to various factors, such as sampling parameters, random seed, inference engine version, and inference engine settings.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Refer to &lt;code&gt;examples/configs/evals/eval.yaml&lt;/code&gt; for a full list of parameters that can be overridden. For an in-depth explanation of evaluation, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/eval.md&#34;&gt;Evaluation documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Set Up Clusters&lt;/h2&gt; &#xA;&lt;p&gt;For detailed instructions on how to set up and launch NeMo RL on Slurm or Kubernetes clusters, please refer to the dedicated &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/cluster.md&#34;&gt;Cluster Start&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Tips and Tricks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you forget to initialize the NeMo and Megatron submodules when cloning the NeMo-RL repository, you may run into an error like this:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ModuleNotFoundError: No module named &#39;megatron&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you see this error, there is likely an issue with your virtual environments. To fix this, first intialize the submodules:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then force a rebuild of the virtual environments by setting &lt;code&gt;NRL_FORCE_REBUILD_VENVS=true&lt;/code&gt; next time you launch a run:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;NRL_FORCE_REBUILD_VENVS=true uv run examples/run_grpo.py ...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use NeMo RL in your research, please cite it using the following BibTeX entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{nemo-rl,&#xA;title = {NeMo RL: A Scalable and Efficient Post-Training Library},&#xA;howpublished = {\url{https://github.com/NVIDIA-NeMo/RL}},&#xA;year = {2025},&#xA;note = {GitHub repository},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to NeMo RL! Please see our &lt;a href=&#34;https://github.com/NVIDIA-NeMo/RL/raw/main/CONTRIBUTING.md&#34;&gt;Contributing Guidelines&lt;/a&gt; for more information on how to get involved.&lt;/p&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA NeMo RL is licensed under the &lt;a href=&#34;https://github.com/NVIDIA-NeMo/RL/raw/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>