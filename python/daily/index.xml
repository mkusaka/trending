<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-03T01:36:52Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>HITsz-TMG/FilmAgent</title>
    <updated>2025-02-03T01:36:52Z</updated>
    <id>tag:github.com,2025-02-03:/HITsz-TMG/FilmAgent</id>
    <link href="https://github.com/HITsz-TMG/FilmAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Resources of our paper &#34;FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces&#34;. New versions in the making!&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HITsz-TMG/FilmAgent/main/pics/cover_page.jpg&#34; width=&#34;300&#34; style=&#34;margin-bottom: 0.2;&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/HITsz-TMG/FilmAgent&#34;&gt;FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;!-- &lt;h5 align=&#34;center&#34;&gt; If you like our project, please consider giving us a star ‚≠ê on GitHub to stay updated with the latest developments.  &lt;/h2&gt; --&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://img.shields.io/badge/Version-1.0.0-blue.svg?sanitize=true&#34; alt=&#34;Version&#34;&gt; &#xA;  &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt; &#xA;  &lt;img src=&#34;https://img.shields.io/github/stars/HITsz-TMG/FilmAgent?color=yellow&#34; alt=&#34;Stars&#34;&gt; &#xA;  &lt;img src=&#34;https://img.shields.io/github/issues/HITsz-TMG/FilmAgent?color=red&#34; alt=&#34;Issues&#34;&gt; &#xA;  &lt;img src=&#34;https://img.shields.io/badge/python-3.8-purple.svg?sanitize=true&#34; alt=&#34;Python&#34;&gt; &#xA;  &lt;!-- &lt;img src=&#34;https://img.shields.io/github/stars/AIDC-AI/Marco-o1?color=yellow&#34; alt=&#34;Stars&#34;&gt; --&gt; &#xA;  &lt;!-- [![Project Page](https://img.shields.io/badge/Project_Page-FilmAgent-blue)](https://filmagent.github.io/)&#xA;[![Project Page](https://img.shields.io/badge/Paper-Arxiv-yellow)](https://arxiv.org/abs/2501.12909)&#xA;[![Project Page](https://img.shields.io/badge/Video-Youtube-red)](https://www.youtube.com/watch?v=hTI-0777iHU)&#xA;![Gitea Stars](https://img.shields.io/gitea/stars/HITsz-TMG/FilmAgent) --&gt; &#xA; &lt;/div&gt;&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- **Affiliations:** --&gt; &#xA; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Zhenran Xu, Longyue Wang, Jifang Wang, Zhouyi Li, Senbao Shi, Xue Yang, Yiyu Wang, Baotian Hu, Jun Yu, Min Zhang&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;üéØ &lt;a href=&#34;https://filmagent.github.io&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; &lt;img alt=&#34;octocat&#34; src=&#34;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&#34;&gt;) &lt;a href=&#34;https://github.com/HITsz-TMG/FilmAgent&#34;&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/a&gt; üìù &lt;a href=&#34;https://arxiv.org/abs/2501.12909&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; üßë‚Äçüíª &lt;a href=&#34;https://github.com/filmagent/filmagent.github.io/raw/main/static/SA24_FilmAgent.pdf&#34;&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/a&gt; üìΩÔ∏è &lt;a href=&#34;https://www.youtube.com/watch?v=hTI-0777iHU&#34;&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;FilmAgent&lt;/strong&gt; is a multi-agent collaborative system for end-to-end film automation in 3D virtual spaces. FilmAgent simulates key crew roles‚Äîdirectors, screenwriters, actors, and cinematographers, and integrates efficient human workflows within a sandbox environment.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/HITsz-TMG/FilmAgent/raw/main/pics/intro.png&#34; height=&#34;100%&#34; width=&#34;85%&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üöÄ Framework&lt;/h2&gt; &#xA;&lt;p&gt;Following the traditional film studio workflow, we divide the whole film automation process into three sequential stages: idea development, scriptwriting and cinematography, and apply the &lt;strong&gt;Critique-Correct-Verify&lt;/strong&gt;, &lt;strong&gt;Debate-Judge&lt;/strong&gt; collaboration strategies. After these stages, each line in the script is specified with the positions of the actors, their actions, their dialogue, and the chosen camera shots.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/HITsz-TMG/FilmAgent/raw/main/pics/framework.png&#34; height=&#34;100%&#34; width=&#34;85%&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üåü Build Your own Film with FilmAgent&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n filmagent python==3.9.18&#xA;conda activate filmagent&#xA;pip install -r env.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create &lt;code&gt;Script&lt;/code&gt; and &lt;code&gt;Logs&lt;/code&gt; folders in the Filmagent directory, then replace the absolute pathname &#39;/path/to/&#39; with your specific path and modify the &lt;code&gt;topic&lt;/code&gt; in the &lt;code&gt;main.py&lt;/code&gt;. Modify the api_key and organization number in &lt;code&gt;LLMCaller.py&lt;/code&gt;. Run the following commands to get the movie script created by the agents collaboratively:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /path/to/FilmAgent&#xA;conda activate filmagent&#xA;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;We use &lt;a href=&#34;https://github.com/2noise/ChatTTS&#34;&gt;ChatTTS&lt;/a&gt; to provide voice acting for the characters in the script. You need to download the &lt;a href=&#34;https://github.com/2noise/ChatTTS&#34;&gt;ChatTTS&lt;/a&gt; repository to the &lt;code&gt;TTS&lt;/code&gt; directory. Then replace the absolute pathname &#39;/path/to/&#39; with your specific path in the &lt;code&gt;tts_main.py&lt;/code&gt;. Run the following commands to deploy the text-to-speech service:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /path/to/TTS&#xA;conda create -n tts python==3.9.18&#xA;conda activate tts&#xA;pip install -r tts_env.txt&#xA;python tts_main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Modify the &lt;code&gt;Script_path&lt;/code&gt;, &lt;code&gt;actos_path&lt;/code&gt;, &lt;code&gt;Audio_path&lt;/code&gt; and &lt;code&gt;url&lt;/code&gt; in the &lt;code&gt;GenerateAudio.py&lt;/code&gt;. Run the following commands to get the audio files:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /path/to/FilmAgent&#xA;conda activate filmagent&#xA;python GenerateAudio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;We now have the &lt;code&gt;script.json&lt;/code&gt;, &lt;code&gt;actors_profile.json&lt;/code&gt;, and a series of &lt;code&gt;.wav&lt;/code&gt; audio files. Next, we need to execute the script in Unity. The recommended version of the Unity editor is &lt;strong&gt;Unity 2022.3.14f1c1&lt;/strong&gt;. You need to download the &lt;a href=&#34;https://huggingface.co/datasets/wjfhit/filmagent_unity/tree/main&#34;&gt;Unity project file&lt;/a&gt; we provide. After decompression, open &lt;code&gt;TheBigBang\Assets\TheBigBang\Manyrooms.unity&lt;/code&gt; with Unity. Then replace all the absolute pathnames &#39;/path/to/&#39; with your specific path in &lt;code&gt;TheBigBang\Assets\Scirpts\StartVideo.cs&lt;/code&gt; and &lt;code&gt;TheBigBang\Assets\Scirpts\ScriptExecute.cs&lt;/code&gt;. Press &lt;strong&gt;&#39;ctrl+R&#39;&lt;/strong&gt; in the unity interface to recompile, click &lt;strong&gt;&#39;Play&#39;&lt;/strong&gt; to enter Game mode, then press &lt;strong&gt;&#39;E&#39;&lt;/strong&gt; to start executing the script (sometimes the audio files load slowly, so you may need to play it 2 or 3 times before it can run normally).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/HITsz-TMG/FilmAgent/raw/main/pics/unity_1.png&#34; height=&#34;100%&#34; width=&#34;50%&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/HITsz-TMG/FilmAgent/raw/main/pics/unity_2.png&#34; height=&#34;100%&#34; width=&#34;50%&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;For the tests on 15 topics in our experimental section, we provide three .py files: &lt;code&gt;test_full.py&lt;/code&gt; (The full FilmAgent framework, utilizing multi-agent collaboration.), &lt;code&gt;test_no_interation.py&lt;/code&gt; (A single agent is responsible for planning, scriptwriting, and cinematography, representing our FilmAgent framework without multi-agent collaboration algorithms.) and &lt;code&gt;test_cot.py&lt;/code&gt; (A single agent generates the chain-of-thought rationale and the complete script).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üåà Case Study&lt;/h2&gt; &#xA;&lt;h3&gt;What does Multi-Agent Collaboration do?&lt;/h3&gt; &#xA;&lt;p&gt;The following table records some comparisons of the scripts and camera settings before (left) and after (right) multi-agent collaboration, with excerpts from their discussion process.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/HITsz-TMG/FilmAgent/raw/main/pics/cases.png&#34; height=&#34;100%&#34; width=&#34;70%&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;p&gt;Case #1 and #2 are from the Critique-Correct-Verify method in Scriptwriting #2 and #3 stages respectively. Case #3 and #4 are from the Debate-Judge method in Cinematography.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Case #1&lt;/strong&gt; shows that Director-Screenwriter discussion reduces hallucinations in non-existent actions (e.g., standing suggest), enhances plot coherence, and ensures consistency across scenes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Case #2&lt;/strong&gt; shows that Actor-Director-Screenwriter discussion improves the alignment of dialogue with character profiles.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Case #3&lt;/strong&gt;, in the Debate-Judge method in cinematography, demonstrates the correction of an inappropriate dynamic shot, which is replaced with a medium shot to better convey body language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Case #4&lt;/strong&gt; replaces a series of identical static shots with a mix of dynamic and static shots, resulting in a more diverse camera setup.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Comparison with Sora&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/HITsz-TMG/FilmAgent/raw/main/pics/sora.png&#34; height=&#34;100%&#34; width=&#34;70%&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/65bb4c12-cba0-4ee9-a673-63ea5103fd76&#34;&gt;https://github.com/user-attachments/assets/65bb4c12-cba0-4ee9-a673-63ea5103fd76&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;While Sora shows &lt;strong&gt;great adaptability&lt;/strong&gt; to diverse locations, characters and shots, it struggles with consistency and narrative delivery. In contrast, FilmAgent requires pre-built 3D spaces, but it produces &lt;strong&gt;coherent, physics-compliant&lt;/strong&gt; videos with strong &lt;strong&gt;storytelling capabilities&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìö Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find FilmAgent useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{xu2025filmagent,&#xA;      title={FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces}, &#xA;      author={Zhenran Xu and Longyue Wang and Jifang Wang and Zhouyi Li and Senbao Shi and Xue Yang and Yiyu Wang and Baotian Hu and Jun Yu and Min Zhang},&#xA;      year={2025},&#xA;      eprint={2501.12909},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2501.12909}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>lllyasviel/Paints-UNDO</title>
    <updated>2025-02-03T01:36:52Z</updated>
    <id>tag:github.com,2025-02-03:/lllyasviel/Paints-UNDO</id>
    <link href="https://github.com/lllyasviel/Paints-UNDO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Understand Human Behavior to Align True Needs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Paints-Undo&lt;/h1&gt; &#xA;&lt;p&gt;PaintsUndo: A Base Model of Drawing Behaviors in Digital Paintings&lt;/p&gt; &#xA;&lt;p&gt;Paints-Undo is a project aimed at providing base models of human drawing behaviors with a hope that future AI models can better align with the real needs of human artists.&lt;/p&gt; &#xA;&lt;p&gt;The name &#34;Paints-Undo&#34; is inspired by the similarity that, the model&#39;s outputs look like pressing the &#34;undo&#34; button (usually Ctrl+Z) many times in digital painting software.&lt;/p&gt; &#xA;&lt;p&gt;Paints-Undo presents a family of models that take an image as input and then output the drawing sequence of that image. The model displays all kinds of human behaviors, including but not limited to sketching, inking, coloring, shading, transforming, left-right flipping, color curve tuning, changing the visibility of layers, and even changing the overall idea during the drawing process.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This page does not contain any examples. All examples are in the below Git page:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lllyasviel.github.io/pages/paints_undo/&#34;&gt;&amp;gt;&amp;gt;&amp;gt; Click Here to See the Example Page &amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This GitHub repo is the only official page of PaintsUndo. We do not have any other websites.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do note that many fake websites of PaintsUndo are on Google and social media recently.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Get Started&lt;/h1&gt; &#xA;&lt;p&gt;You can deploy PaintsUndo locally via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Paints-UNDO.git&#xA;cd Paints-UNDO&#xA;conda create -n paints_undo python=3.10&#xA;conda activate paints_undo&#xA;pip install xformers&#xA;pip install -r requirements.txt&#xA;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(If you do not know how to use these commands, you can paste those commands to ChatGPT and ask ChatGPT to explain and give more detailed instructions.)&lt;/p&gt; &#xA;&lt;p&gt;The inference is tested with 24GB VRAM on Nvidia 4090 and 3090TI. It may also work with 16GB VRAM, but does not work with 8GB. My estimation is that, under extreme optimization (including weight offloading and sliced attention), the theoretical minimal VRAM requirement is about 10~12.5 GB.&lt;/p&gt; &#xA;&lt;p&gt;You can expect to process one image in about 5 to 10 minutes, depending on your settings. As a typical result, you will get a video of 25 seconds at FPS 4, with resolution 320x512, or 512x320, or 384x448, or 448x384.&lt;/p&gt; &#xA;&lt;p&gt;Because the processing time, in most cases, is significantly longer than most tasks/quota in HuggingFace Space, I personally do not highly recommend to deploy this to HuggingFace Space, to avoid placing an unnecessary burden on the HF servers.&lt;/p&gt; &#xA;&lt;p&gt;If you do not have required computation devices and still wants an online solution, one option is to wait us to release a Colab notebook (but I am not sure if Colab free tier will work).&lt;/p&gt; &#xA;&lt;h1&gt;Model Notes&lt;/h1&gt; &#xA;&lt;p&gt;We currently release two models &lt;code&gt;paints_undo_single_frame&lt;/code&gt; and &lt;code&gt;paints_undo_multi_frame&lt;/code&gt;. Let&#39;s call them single-frame model and multi-frame model.&lt;/p&gt; &#xA;&lt;p&gt;The single-frame model takes one image and an &lt;code&gt;operation step&lt;/code&gt; as input, and outputs one single image. Assuming that an artwork can always be created with 1000 human operations (for example, one brush stroke is one operation), and the &lt;code&gt;operation step&lt;/code&gt; is an int number from 0 to 999. The number 0 is the finished final artwork, and the number 999 is the first brush stroke drawn on the pure white canvas. You can understand this model as an &#34;undo&#34; (or called Ctrl+Z) model. You input the final image, and indicate how many times you want to &#34;Ctrl+Z&#34;, and the model will give you a &#34;simulated&#34; screenshot after those &#34;Ctrl+Z&#34;s are pressed. If your &lt;code&gt;operation step&lt;/code&gt; is 100, then it means you want to simulate &#34;Ctrl+Z&#34; 100 times on this image to get the appearance after the 100-th &#34;Ctrl+Z&#34;.&lt;/p&gt; &#xA;&lt;p&gt;The multi-frame model takes two images as inputs and output 16 intermediate frames between the two input images. The result is much more consistent than the single-frame model, but also much slower, less &#34;creative&#34;, and limited in 16 frames.&lt;/p&gt; &#xA;&lt;p&gt;In this repo, the default method is to use them together. We will first infer the single-frame model about 5-7 times to get 5-7 &#34;keyframes&#34;, and then we use the multi-frame model to &#34;interpolate&#34; those keyframes to actually generate a relatively long video.&lt;/p&gt; &#xA;&lt;p&gt;In theory this system can be used in many ways and even give infinitely long video, but in practice results are good when the final frame count is about 100-500.&lt;/p&gt; &#xA;&lt;h3&gt;Model Architecture (paints_undo_single_frame)&lt;/h3&gt; &#xA;&lt;p&gt;The model is a modified architecture of SD1.5 trained on different betas scheduler, clip skip, and the aforementioned &lt;code&gt;operation step&lt;/code&gt; condition. To be specific, the model is trained with the betas of:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;betas = torch.linspace(0.00085, 0.020, 1000, dtype=torch.float64)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For comparison, the original SD1.5 is trained with the betas of:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;betas = torch.linspace(0.00085 ** 0.5, 0.012 ** 0.5, 1000, dtype=torch.float64) ** 2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can notice the difference in the ending betas and the removed square. The choice of this scheduler is based on our internal user study.&lt;/p&gt; &#xA;&lt;p&gt;The last layer of the text encoder CLIP ViT-L/14 is permanently removed. It is now mathematically consistent to always set CLIP Skip to 2 (if you use diffusers).&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;operation step&lt;/code&gt; condition is added to layer embeddings in a way similar to SDXL&#39;s extra embeddings.&lt;/p&gt; &#xA;&lt;p&gt;Also, since the solo purpose of this model is to process existing images, the model is strictly aligned with WD14 tagger without any other augmentations. You should always use WD14 tagger (the one in this repo) to process the input image to get the prompt. Otherwise, the results may be defective. Human-written prompts are not tested.&lt;/p&gt; &#xA;&lt;h3&gt;Model Architecture (paints_undo_multi_frame)&lt;/h3&gt; &#xA;&lt;p&gt;This model is trained by resuming from &lt;a href=&#34;https://github.com/AILab-CVC/VideoCrafter&#34;&gt;VideoCrafter&lt;/a&gt; family, but the original Crafter&#39;s &lt;code&gt;lvdm&lt;/code&gt; is not used and all training/inference codes are completely implemented from scratch. (BTW, now the codes are based on modern Diffusers.) Although the initial weights are resumed from VideoCrafter, the topology of neural network is modified a lot, and the network behavior is now largely different from original Crafter after extensive training.&lt;/p&gt; &#xA;&lt;p&gt;The overall architecture is like Crafter with 5 components, 3D-UNet, VAE, CLIP, CLIP-Vision, Image Projection.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VAE&lt;/strong&gt;: The VAE is the exactly same anime VAE extracted from &lt;a href=&#34;https://github.com/ToonCrafter/ToonCrafter&#34;&gt;ToonCrafter&lt;/a&gt;. Thanks ToonCrafter a lot for providing the excellent anime temporal VAE for Crafters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-UNet&lt;/strong&gt;: The 3D-UNet is modified from Crafters&#39;s &lt;code&gt;lvdm&lt;/code&gt; with revisions to attention modules. Other than some minor changes in codes, the major change is that now the UNet are trained and supports temporal windows in Spatial Self Attention layers. You can change the codes in &lt;code&gt;diffusers_vdm.attention.CrossAttention.temporal_window_for_spatial_self_attention&lt;/code&gt; and &lt;code&gt;temporal_window_type&lt;/code&gt; to activate three types of attention windows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&#34;prv&#34; mode: Each frame&#39;s Spatial Self-Attention also attend to full spatial contexts of its previous frame. The first frame only attend itself.&lt;/li&gt; &#xA; &lt;li&gt;&#34;first&#34;: Each frame&#39;s Spatial Self-Attention also attend to full spatial contexts of the first frame of the entire sequence. The first frame only attend its self.&lt;/li&gt; &#xA; &lt;li&gt;&#34;roll&#34;: Each frame&#39;s Spatial Self-Attention also attend to full spatial contexts of its previous and next frames, based on the ordering of &lt;code&gt;torch.roll&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note that this is by default disabled in inference to save GPU memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIP&lt;/strong&gt;: The CLIP of SD2.1.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIP-Vision&lt;/strong&gt;: Our implementation of Clip Vision (ViT/H) that supports arbitrary aspect ratios by interpolating the positional embedding. After experimenting with linear interpolation, nearest neighbor, and Rotary Positional Encoding (RoPE), our final choice is nearest neighbor. Note that this is different from Crafter methods that resize or center-crop images to 224x224.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Projection&lt;/strong&gt;: Our implementation of a tiny transformer that takes two frames as inputs and outputs 16 image embeddings for each frame. Note that this is different from Crafter methods that only use one image.&lt;/p&gt; &#xA;&lt;h1&gt;Tutorial&lt;/h1&gt; &#xA;&lt;p&gt;After you get into the Gradio interface:&lt;/p&gt; &#xA;&lt;p&gt;Step 0: Upload an image or just click an Example image on the bottom of the page.&lt;/p&gt; &#xA;&lt;p&gt;Step 1: In the UI titled &#34;step 1&#34;, click generate prompts to get the global prompt.&lt;/p&gt; &#xA;&lt;p&gt;Step 2: In the UI titled &#34;step 2&#34;, click &#34;Generate Key Frames&#34;. You can change seeds or other parameters on the left.&lt;/p&gt; &#xA;&lt;p&gt;Step 3: In the UI titled &#34;step 3&#34;, click &#34;Generate Video&#34;. You can change seeds or other parameters on the left.&lt;/p&gt; &#xA;&lt;h1&gt;Cite&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Misc{paintsundo,&#xA;  author = {Paints-Undo Team},&#xA;  title  = {Paints-Undo GitHub Page},&#xA;  year   = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Applications&lt;/h1&gt; &#xA;&lt;p&gt;Typical use cases of PaintsUndo:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Use PaintsUndo as a base model to analyze human behavior to build AI tools that align with human behavior and human demands, for seamless collaboration between AI and humans in a perfectly controlled workflow.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Combine PaintsUndo with sketch-guided image generators to achieve ‚ÄúPaintsRedo‚Äù, so as to move forward or backward arbitrarily in any of your finished/unfinished artworks to enhance human creativity power. *&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use PaintsUndo to view different possible procedures of your own artworks for artistic inspirations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the outputs of PaintsUndo as a kind of video/movie After Effects to achieve specific creative purposes.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;and much more ...&lt;/p&gt; &#xA;&lt;p&gt;* &lt;em&gt;this is already possible - if you use PaintsUndo to Undo 500 steps, and want to Redo 100 steps with different possibilities, you can use ControlNet to finish it (so that it becomes step 0) and then undo 400 steps. More integrated solution is still under experiments.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Related Works&lt;/h1&gt; &#xA;&lt;p&gt;Also read ...&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jiupinjia.github.io/neuralpainter/&#34;&gt;Stylized Neural Painting&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hzwer/ICCV2019-LearningToPaint&#34;&gt;Learning to Paint With Model-based Deep Reinforcement Learning&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Huage001/PaintTransformer&#34;&gt;Paint Transformer: Feed Forward Neural Painting with Stroke Prediction&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nicolaus-huang/ProcessPainter&#34;&gt;ProcessPainter: Learn Painting Process from Sequence Data&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cragl.cs.gmu.edu/timemap/&#34;&gt;Decomposing Time-Lapse Paintings into Layers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This project aims to develop base models of human drawing behaviors, facilitating future AI systems to better meet the real needs of human artists. Users are granted the freedom to create content using this tool, but they are expected to comply with local laws and use it responsibly. Users must not employ the tool to generate false information or incite confrontation. The developers do not assume any responsibility for potential misuse by users.&lt;/p&gt;</summary>
  </entry>
</feed>