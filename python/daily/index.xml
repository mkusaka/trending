<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-20T01:44:18Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>3b1b/videos</title>
    <updated>2023-02-20T01:44:18Z</updated>
    <id>tag:github.com,2023-02-20:/3b1b/videos</id>
    <link href="https://github.com/3b1b/videos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the manim-generated scenes used in 3blue1brown videos&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This project contains the code used to generate the explanatory math videos found on &lt;a href=&#34;https://www.3blue1brown.com/&#34;&gt;3Blue1Brown&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This almost entirely consists of scenes generated using the library &lt;a href=&#34;https://github.com/3b1b/manim&#34;&gt;Manim&lt;/a&gt;. See also the community maintained version at &lt;a href=&#34;https://github.com/ManimCommunity/manim/&#34;&gt;ManimCommunity&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note, while the library Manim itself is open source and under the MIT license, the contents of this project are intended only to be used for 3Blue1Brown videos themselves.&lt;/p&gt; &#xA;&lt;p&gt;Copyright © 2022 3Blue1Brown&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kohya-ss/sd-scripts</title>
    <updated>2023-02-20T01:44:18Z</updated>
    <id>tag:github.com,2023-02-20:/kohya-ss/sd-scripts</id>
    <link href="https://github.com/kohya-ss/sd-scripts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This repository contains training, generation and utility scripts for Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kohya-ss/sd-scripts/main/#change-history&#34;&gt;&lt;strong&gt;Change History&lt;/strong&gt;&lt;/a&gt; is moved to the bottom of the page. 更新履歴は&lt;a href=&#34;https://raw.githubusercontent.com/kohya-ss/sd-scripts/main/#change-history&#34;&gt;ページ末尾&lt;/a&gt;に移しました。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kohya-ss/sd-scripts/main/README-ja.md&#34;&gt;日本語版README&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For easier use (GUI and PowerShell scripts etc...), please visit &lt;a href=&#34;https://github.com/bmaltais/kohya_ss&#34;&gt;the repository maintained by bmaltais&lt;/a&gt;. Thanks to @bmaltais!&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the scripts for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DreamBooth training, including U-Net and Text Encoder&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuning (native training), including U-Net and Text Encoder&lt;/li&gt; &#xA; &lt;li&gt;LoRA training&lt;/li&gt; &#xA; &lt;li&gt;Texutl Inversion training&lt;/li&gt; &#xA; &lt;li&gt;Image generation&lt;/li&gt; &#xA; &lt;li&gt;Model conversion (supports 1.x and 2.x, Stable Diffision ckpt/safetensors and Diffusers)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stable Diffusion web UI now seems to support LoRA trained by &lt;code&gt;sd-scripts&lt;/code&gt;.&lt;/strong&gt; (SD 1.x based only) Thank you for great work!!!&lt;/p&gt; &#xA;&lt;h2&gt;About requirements.txt&lt;/h2&gt; &#xA;&lt;p&gt;These files do not contain requirements for PyTorch. Because the versions of them depend on your environment. Please install PyTorch at first (see installation guide below.)&lt;/p&gt; &#xA;&lt;p&gt;The scripts are tested with PyTorch 1.12.1 and 1.13.0, Diffusers 0.10.2.&lt;/p&gt; &#xA;&lt;h2&gt;Links to how-to-use documents&lt;/h2&gt; &#xA;&lt;p&gt;All documents are in Japanese currently, and CUI based.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kohya-ss/sd-scripts/main/train_db_README-ja.md&#34;&gt;DreamBooth training guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kohya-ss/sd-scripts/main/fine_tune_README_ja.md&#34;&gt;Step by Step fine-tuning guide&lt;/a&gt;: Including BLIP captioning and tagging by DeepDanbooru or WD14 tagger&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kohya-ss/sd-scripts/main/train_network_README-ja.md&#34;&gt;training LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kohya-ss/sd-scripts/main/train_ti_README-ja.md&#34;&gt;training Textual Inversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;note.com &lt;a href=&#34;https://note.com/kohya_ss/n/n2693183a798e&#34;&gt;Image generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;note.com &lt;a href=&#34;https://note.com/kohya_ss/n/n374f316fe4ad&#34;&gt;Model conversion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Windows Required Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Python 3.10.6 and Git:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10.6: &lt;a href=&#34;https://www.python.org/ftp/python/3.10.6/python-3.10.6-amd64.exe&#34;&gt;https://www.python.org/ftp/python/3.10.6/python-3.10.6-amd64.exe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;git: &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;https://git-scm.com/download/win&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Give unrestricted script access to powershell so venv can work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open an administrator powershell window&lt;/li&gt; &#xA; &lt;li&gt;Type &lt;code&gt;Set-ExecutionPolicy Unrestricted&lt;/code&gt; and answer A&lt;/li&gt; &#xA; &lt;li&gt;Close admin powershell window&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Windows Installation&lt;/h2&gt; &#xA;&lt;p&gt;Open a regular Powershell terminal and type the following inside:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;git clone https://github.com/kohya-ss/sd-scripts.git&#xA;cd sd-scripts&#xA;&#xA;python -m venv venv&#xA;.\venv\Scripts\activate&#xA;&#xA;pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116&#xA;pip install --upgrade -r requirements.txt&#xA;pip install -U -I --no-deps https://github.com/C43H66N12O12S2/stable-diffusion-webui/releases/download/f/xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl&#xA;&#xA;cp .\bitsandbytes_windows\*.dll .\venv\Lib\site-packages\bitsandbytes\&#xA;cp .\bitsandbytes_windows\cextension.py .\venv\Lib\site-packages\bitsandbytes\cextension.py&#xA;cp .\bitsandbytes_windows\main.py .\venv\Lib\site-packages\bitsandbytes\cuda_setup\main.py&#xA;&#xA;accelerate config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;update: &lt;code&gt;python -m venv venv&lt;/code&gt; is seemed to be safer than &lt;code&gt;python -m venv --system-site-packages venv&lt;/code&gt; (some user have packages in global python).&lt;/p&gt; &#xA;&lt;p&gt;Answers to accelerate config:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;- This machine&#xA;- No distributed training&#xA;- NO&#xA;- NO&#xA;- NO&#xA;- all&#xA;- fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;note: Some user reports &lt;code&gt;ValueError: fp16 mixed precision requires a GPU&lt;/code&gt; is occurred in training. In this case, answer &lt;code&gt;0&lt;/code&gt; for the 6th question: &lt;code&gt;What GPU(s) (by id) should be used for training on this machine as a comma-separated list? [all]:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Single GPU with id &lt;code&gt;0&lt;/code&gt; will be used.)&lt;/p&gt; &#xA;&lt;h3&gt;about PyTorch and xformers&lt;/h3&gt; &#xA;&lt;p&gt;Other versions of PyTorch and xformers seem to have problems with training. If there is no other reason, please install the specified version.&lt;/p&gt; &#xA;&lt;h2&gt;Upgrade&lt;/h2&gt; &#xA;&lt;p&gt;When a new release comes out you can upgrade your repo with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;cd sd-scripts&#xA;git pull&#xA;.\venv\Scripts\activate&#xA;pip install --use-pep517 --upgrade -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the commands have completed successfully you should be ready to use the new version.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;The implementation for LoRA is based on &lt;a href=&#34;https://github.com/cloneofsimo/lora&#34;&gt;cloneofsimo&#39;s repo&lt;/a&gt;. Thank you for great work!!!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The majority of scripts is licensed under ASL 2.0 (including codes from Diffusers, cloneofsimo&#39;s), however portions of the project are available under separate license terms:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lucidrains/memory-efficient-attention-pytorch&#34;&gt;Memory Efficient Attention Pytorch&lt;/a&gt;: MIT&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;: MIT&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/salesforce/BLIP&#34;&gt;BLIP&lt;/a&gt;: BSD-3-Clause&lt;/p&gt; &#xA;&lt;h2&gt;Change History&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;19 Feb. 2023, 2023/2/19:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Add &lt;code&gt;--use_lion_optimizer&lt;/code&gt; to each training script to use &lt;a href=&#34;https://github.com/lucidrains/lion-pytorch&#34;&gt;Lion optimizer&lt;/a&gt;.&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Please install Lion optimizer with &lt;code&gt;pip install lion-pytorch&lt;/code&gt; (it is not in &lt;code&gt;requirements.txt&lt;/code&gt; currently.)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Add &lt;code&gt;--lowram&lt;/code&gt; option to &lt;code&gt;train_network.py&lt;/code&gt;. Load models to VRAM instead of VRAM (for machines which have bigger VRAM than RAM such as Colab and Kaggle). Thanks to Isotr0py!&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Default behavior (without lowram) has reverted to the same as before 14 Feb.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Fixed git commit hash to be set correctly regardless of the working directory. Thanks to vladmandic!&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;--use_lion_optimizer&lt;/code&gt; オプションを各学習スクリプトに追加しました。 &lt;a href=&#34;https://github.com/lucidrains/lion-pytorch&#34;&gt;Lion optimizer&lt;/a&gt; を使用できます。&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;あらかじめ &lt;code&gt;pip install lion-pytorch&lt;/code&gt; でインストールしてください（現在は &lt;code&gt;requirements.txt&lt;/code&gt; に含まれていません）。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;--lowram&lt;/code&gt; オプションを &lt;code&gt;train_network.py&lt;/code&gt; に追加しました。モデルをRAMではなくVRAMに読み込みます（ColabやKaggleなど、VRAMがRAMに比べて多い環境で有効です）。 Isotr0py 氏に感謝します。&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;lowram オプションなしのデフォルト動作は2/14より前と同じに戻しました。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;git commit hash を現在のフォルダ位置に関わらず正しく取得するように修正しました。vladmandic 氏に感謝します。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;16 Feb. 2023, 2023/2/16:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Noise offset is recorded to the metadata. Thanks to space-nuko!&lt;/li&gt; &#xA;   &lt;li&gt;Show the moving average loss to prevent loss jumping in &lt;code&gt;train_network.py&lt;/code&gt; and &lt;code&gt;train_db.py&lt;/code&gt;. Thanks to shirayu!&lt;/li&gt; &#xA;   &lt;li&gt;Noise offsetがメタデータに記録されるようになりました。space-nuko氏に感謝します。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;train_network.py&lt;/code&gt;と&lt;code&gt;train_db.py&lt;/code&gt;で学習中に表示されるlossの値が移動平均になりました。epochの先頭で表示されるlossが大きく変動する事象を解決します。shirayu氏に感謝します。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;14 Feb. 2023, 2023/2/14:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add support with multi-gpu trainining for &lt;code&gt;train_network.py&lt;/code&gt;. Thanks to Isotr0py!&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;--verbose&lt;/code&gt; option for &lt;code&gt;resize_lora.py&lt;/code&gt;. For details, see &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/pull/179&#34;&gt;this PR&lt;/a&gt;. Thanks to mgz-dev!&lt;/li&gt; &#xA;   &lt;li&gt;Git commit hash is added to the metadata for LoRA. Thanks to space-nuko!&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;--noise_offset&lt;/code&gt; option for each training scripts. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Implementation of &lt;a href=&#34;https://www.crosslabs.org//blog/diffusion-with-offset-noise&#34;&gt;https://www.crosslabs.org//blog/diffusion-with-offset-noise&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;This option may improve ability to generate darker/lighter images. May work with LoRA.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;train_network.py&lt;/code&gt;でマルチGPU学習をサポートしました。Isotr0py氏に感謝します。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--verbose&lt;/code&gt;オプションを &lt;code&gt;resize_lora.py&lt;/code&gt; に追加しました。表示される情報の詳細は &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/pull/179&#34;&gt;こちらのPR&lt;/a&gt; をご参照ください。mgz-dev氏に感謝します。&lt;/li&gt; &#xA;   &lt;li&gt;LoRAのメタデータにgitのcommit hashを追加しました。space-nuko氏に感謝します。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--noise_offset&lt;/code&gt; オプションを各学習スクリプトに追加しました。 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;こちらの記事の実装になります: &lt;a href=&#34;https://www.crosslabs.org//blog/diffusion-with-offset-noise&#34;&gt;https://www.crosslabs.org//blog/diffusion-with-offset-noise&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;全体的に暗い、明るい画像の生成結果が良くなる可能性があるようです。LoRA学習でも有効なようです。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;11 Feb. 2023, 2023/2/11:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;lora_interrogator.py&lt;/code&gt; is added in &lt;code&gt;networks&lt;/code&gt; folder. See &lt;code&gt;python networks\lora_interrogator.py -h&lt;/code&gt; for usage.&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;For LoRAs where the activation word is unknown, this script compares the output of Text Encoder after applying LoRA to that of unapplied to find out which token is affected by LoRA. Hopefully you can figure out the activation word. LoRA trained with captions does not seem to be able to interrogate.&lt;/li&gt; &#xA;     &lt;li&gt;Batch size can be large (like 64 or 128).&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;train_textual_inversion.py&lt;/code&gt; now supports multiple init words.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Following feature is reverted to be the same as before. Sorry for confusion:&lt;/p&gt; &#xA;    &lt;blockquote&gt; &#xA;     &lt;p&gt;Now the number of data in each batch is limited to the number of actual images (not duplicated). Because a certain bucket may contain smaller number of actual images, so the batch may contain same (duplicated) images.&lt;/p&gt; &#xA;    &lt;/blockquote&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;lora_interrogator.py&lt;/code&gt; を &lt;code&gt;network&lt;/code&gt;フォルダに追加しました。使用法は &lt;code&gt;python networks\lora_interrogator.py -h&lt;/code&gt; でご確認ください。&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;このスクリプトは、起動promptがわからないLoRAについて、LoRA適用前後のText Encoderの出力を比較することで、どのtokenの出力が変化しているかを調べます。運が良ければ起動用の単語が分かります。キャプション付きで学習されたLoRAは影響が広範囲に及ぶため、調査は難しいようです。&lt;/li&gt; &#xA;     &lt;li&gt;バッチサイズはわりと大きくできます（64や128など）。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;train_textual_inversion.py&lt;/code&gt; で複数のinit_word指定が可能になりました。&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;次の機能を削除し元に戻しました。混乱を招き申し訳ありません。&lt;/p&gt; &#xA;    &lt;blockquote&gt; &#xA;     &lt;p&gt;これらのオプションによりbucketが細分化され、ひとつのバッチ内に同一画像が重複して存在することが増えたため、バッチサイズを&lt;code&gt;そのbucketの画像種類数&lt;/code&gt;までに制限する機能を追加しました。&lt;/p&gt; &#xA;    &lt;/blockquote&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;10 Feb. 2023, 2023/2/10:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Updated &lt;code&gt;requirements.txt&lt;/code&gt; to prevent upgrading with pip taking a long time or failure to upgrade.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;resize_lora.py&lt;/code&gt; keeps the metadata of the model. &lt;code&gt;dimension is resized from ...&lt;/code&gt; is added to the top of &lt;code&gt;ss_training_comment&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;merge_lora.py&lt;/code&gt; supports models with different &lt;code&gt;alpha&lt;/code&gt;s. If there is a problem, old version is &lt;code&gt;merge_lora_old.py&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;svd_merge_lora.py&lt;/code&gt; is added. This script merges LoRA models with any rank (dim) and alpha, and approximate a new LoRA with svd for a specified rank (dim).&lt;/li&gt; &#xA;   &lt;li&gt;Note: merging scripts erase the metadata currently.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;resize_images_to_resolution.py&lt;/code&gt; supports multibyte characters in filenames.&lt;/li&gt; &#xA;   &lt;li&gt;pipでの更新が長時間掛かったり、更新に失敗したりするのを防ぐため、&lt;code&gt;requirements.txt&lt;/code&gt;を更新しました。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;resize_lora.py&lt;/code&gt;がメタデータを保持するようになりました。 &lt;code&gt;dimension is resized from ...&lt;/code&gt; という文字列が &lt;code&gt;ss_training_comment&lt;/code&gt; の先頭に追加されます。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;merge_lora.py&lt;/code&gt;がalphaが異なるモデルをサポートしました。 何か問題がありましたら旧バージョン &lt;code&gt;merge_lora_old.py&lt;/code&gt; をお使いください。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;svd_merge_lora.py&lt;/code&gt; を追加しました。 複数の任意のdim (rank)、alphaのLoRAモデルをマージし、svdで任意dim(rank)のLoRAで近似します。&lt;/li&gt; &#xA;   &lt;li&gt;注：マージ系のスクリプトは現時点ではメタデータを消去しますのでご注意ください。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;resize_images_to_resolution.py&lt;/code&gt;が日本語ファイル名をサポートしました。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;9 Feb. 2023, 2023/2/9:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Caption dropout is supported in &lt;code&gt;train_db.py&lt;/code&gt;, &lt;code&gt;fine_tune.py&lt;/code&gt; and &lt;code&gt;train_network.py&lt;/code&gt;. Thanks to forestsource! &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;--caption_dropout_rate&lt;/code&gt; option specifies the dropout rate for captions (0~1.0, 0.1 means 10% chance for dropout). If dropout occurs, the image is trained with the empty caption. Default is 0 (no dropout).&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;--caption_dropout_every_n_epochs&lt;/code&gt; option specifies how many epochs to drop captions. If &lt;code&gt;3&lt;/code&gt; is specified, in epoch 3, 6, 9 ..., images are trained with all captions empty. Default is None (no dropout).&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;--caption_tag_dropout_rate&lt;/code&gt; option specified the dropout rate for tags (comma separated tokens) (0~1.0, 0.1 means 10% chance for dropout). If dropout occurs, the tag is removed from the caption. If &lt;code&gt;--keep_tokens&lt;/code&gt; option is set, these tokens (tags) are not dropped. Default is 0 (no droupout).&lt;/li&gt; &#xA;     &lt;li&gt;The bulk image downsampling script is added. Documentation is &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/raw/main/train_network_README-ja.md#%E7%94%BB%E5%83%8F%E3%83%AA%E3%82%B5%E3%82%A4%E3%82%BA%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%97%E3%83%88&#34;&gt;here&lt;/a&gt; (in Jpanaese). Thanks to bmaltais!&lt;/li&gt; &#xA;     &lt;li&gt;Typo check is added. Thanks to shirayu!&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;キャプションのドロップアウトを&lt;code&gt;train_db.py&lt;/code&gt;、&lt;code&gt;fine_tune.py&lt;/code&gt;、&lt;code&gt;train_network.py&lt;/code&gt;の各スクリプトに追加しました。forestsource氏に感謝します。 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;--caption_dropout_rate&lt;/code&gt;オプションでキャプションのドロップアウト率を指定します（0~1.0、 0.1を指定すると10%の確率でドロップアウト）。ドロップアウトされた場合、画像は空のキャプションで学習されます。デフォルトは 0 （ドロップアウトなし）です。&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;--caption_dropout_every_n_epochs&lt;/code&gt; オプションで何エポックごとにキャプションを完全にドロップアウトするか指定します。たとえば&lt;code&gt;3&lt;/code&gt;を指定すると、エポック3、6、9……で、すべての画像がキャプションなしで学習されます。デフォルトは None （ドロップアウトなし）です。&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;--caption_tag_dropout_rate&lt;/code&gt; オプションで各タグ（カンマ区切りの各部分）のドロップアウト率を指定します（0~1.0、 0.1を指定すると10%の確率でドロップアウト）。ドロップアウトが起きるとそのタグはそのときだけキャプションから取り除かれて学習されます。&lt;code&gt;--keep_tokens&lt;/code&gt; オプションを指定していると、シャッフルされない部分のタグはドロップアウトされません。デフォルトは 0 （ドロップアウトなし）です。&lt;/li&gt; &#xA;     &lt;li&gt;画像の一括縮小スクリプトを追加しました。ドキュメントは &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/raw/main/train_network_README-ja.md#%E7%94%BB%E5%83%8F%E3%83%AA%E3%82%B5%E3%82%A4%E3%82%BA%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%97%E3%83%88&#34;&gt;こちら&lt;/a&gt; です。bmaltais氏に感謝します。&lt;/li&gt; &#xA;     &lt;li&gt;誤字チェッカが追加されました。shirayu氏に感謝します。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;6 Feb. 2023, 2023/2/6：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;--bucket_reso_steps&lt;/code&gt; and &lt;code&gt;--bucket_no_upscale&lt;/code&gt; options are added to training scripts (fine tuning, DreamBooth, LoRA and Textual Inversion) and &lt;code&gt;prepare_buckets_latents.py&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;--bucket_reso_steps&lt;/code&gt; takes the steps for buckets in aspect ratio bucketing. Default is 64, same as before.&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Any value greater than or equal to 1 can be specified; 64 is highly recommended and a value divisible by 8 is recommended.&lt;/li&gt; &#xA;     &lt;li&gt;If less than 64 is specified, padding will occur within U-Net. The result is unknown.&lt;/li&gt; &#xA;     &lt;li&gt;If you specify a value that is not divisible by 8, it will be truncated to divisible by 8 inside VAE, because the size of the latent is 1/8 of the image size.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;If &lt;code&gt;--bucket_no_upscale&lt;/code&gt; option is specified, images smaller than the bucket size will be processed without upscaling.&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Internally, a bucket smaller than the image size is created (for example, if the image is 300x300 and &lt;code&gt;bucket_reso_steps=64&lt;/code&gt;, the bucket is 256x256). The image will be trimmed.&lt;/li&gt; &#xA;     &lt;li&gt;Implementation of &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/issues/130&#34;&gt;#130&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Images with an area larger than the maximum size specified by &lt;code&gt;--resolution&lt;/code&gt; are downsampled to the max bucket size.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Now the number of data in each batch is limited to the number of actual images (not duplicated). Because a certain bucket may contain smaller number of actual images, so the batch may contain same (duplicated) images.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;--random_crop&lt;/code&gt; now also works with buckets enabled.&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Instead of always cropping the center of the image, the image is shifted left, right, up, and down to be used as the training data. This is expected to train to the edges of the image.&lt;/li&gt; &#xA;     &lt;li&gt;Implementation of discussion &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/discussions/34&#34;&gt;#34&lt;/a&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;--bucket_reso_steps&lt;/code&gt;および&lt;code&gt;--bucket_no_upscale&lt;/code&gt;オプションを、学習スクリプトおよび&lt;code&gt;prepare_buckets_latents.py&lt;/code&gt;に追加しました。&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;--bucket_reso_steps&lt;/code&gt;オプションでは、bucketの解像度の単位を指定できます。デフォルトは64で、今までと同じ動作です。&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;1以上の任意の値を指定できます。基本的には64を推奨します。64以外の値では、8で割り切れる値を推奨します。&lt;/li&gt; &#xA;     &lt;li&gt;64未満を指定するとU-Netの内部でpaddingが発生します。どのような結果になるかは未知数です。&lt;/li&gt; &#xA;     &lt;li&gt;8で割り切れない値を指定すると余りはVAE内部で切り捨てられます。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;--bucket_no_upscale&lt;/code&gt;オプションを指定すると、bucketサイズよりも小さい画像は拡大せずそのまま処理します。&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;内部的には画像サイズ以下のサイズのbucketを作成します（たとえば画像が300x300で&lt;code&gt;bucket_reso_steps=64&lt;/code&gt;の場合、256x256のbucket）。余りは都度trimmingされます。&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/issues/130&#34;&gt;#130&lt;/a&gt; を実装したものです。&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;--resolution&lt;/code&gt;で指定した最大サイズよりも面積が大きい画像は、最大サイズと同じ面積になるようアスペクト比を維持したまま縮小され、そのサイズを元にbucketが作られます。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;これらのオプションによりbucketが細分化され、ひとつのバッチ内に同一画像が重複して存在することが増えたため、バッチサイズを&lt;code&gt;そのbucketの画像種類数&lt;/code&gt;までに制限する機能を追加しました。&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;たとえば繰り返し回数10で、あるbucketに1枚しか画像がなく、バッチサイズが10以上のとき、今まではepoch内で、同一画像を10枚含むバッチが1回だけ使用されていました。&lt;/li&gt; &#xA;     &lt;li&gt;機能追加後はepoch内にサイズ1のバッチが10回、使用されます。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;--random_crop&lt;/code&gt;がbucketを有効にした場合にも機能するようになりました。&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;常に画像の中央を切り取るのではなく、左右、上下にずらして教師データにします。これにより画像端まで学習されることが期待されます。&lt;/li&gt; &#xA;     &lt;li&gt;discussionの&lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/discussions/34&#34;&gt;#34&lt;/a&gt;を実装したものです。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/releases&#34;&gt;Releases&lt;/a&gt; for recent updates. 最近の更新情報は &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/releases&#34;&gt;Release&lt;/a&gt; をご覧ください。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hpcaitech/EnergonAI</title>
    <updated>2023-02-20T01:44:18Z</updated>
    <id>tag:github.com,2023-02-20:/hpcaitech/EnergonAI</id>
    <link href="https://github.com/hpcaitech/EnergonAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large-scale model inference.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Energon-AI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Made%20with-ColossalAI-blueviolet?style=flat&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI-Inference/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/hpcaitech/FastFold&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A service framework for large-scale model inference, Energon-AI has the following characteristics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parallelism for Large-scale Models:&lt;/strong&gt; With tensor parallel operations, pipeline parallel wrapper, distributed checkpoint loading, and customized CUDA kernel, EnergonAI can enable efficient parallel inference for larges-scale models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-built large models:&lt;/strong&gt; There are pre-built implementation for popular models, such as OPT. It supports the cache technique for the generation task and distributed parameter loading.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Engine encapsulation：&lt;/strong&gt; There has an abstraction layer called engine. It encapsulates the single instance multiple devices (SIMD) execution with the remote procedure call, making it acts as the single instance single device (SISD) execution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An online service system:&lt;/strong&gt; Based on FastAPI, users can launch a web service of the distributed infernce quickly. The online service makes special optimizations for the generation task. It adopts both left padding and bucket batching techniques for improving the efficiency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For models trained by &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;Colossal-AI&lt;/a&gt;, they can be easily transferred to Energon-AI. For single-device models, they require manual coding works to introduce tensor parallelism and pipeline parallelism.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install from source&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone git@github.com:hpcaitech/EnergonAI.git&#xA;$ pip install -r requirements.txt&#xA;$ pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Use docker&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker pull hpcaitech/energon-ai:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build an online OPT service in 5 minutes&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Download OPT model:&lt;/strong&gt; To launch the distributed inference service quickly, you can download the checkpoint of OPT-125M &lt;a href=&#34;https://huggingface.co/patrickvonplaten/opt_metaseq_125m/blob/main/model/restored.pt&#34;&gt;here&lt;/a&gt;. You can get details for loading other sizes of models &lt;a href=&#34;https://github.com/hpcaitech/EnergonAI/tree/main/examples/opt/script&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Launch an HTTP service:&lt;/strong&gt; To launch a service, we need to provide python scripts to describe the model type and related configurations, and start an http service. An OPT example is &lt;a href=&#34;https://github.com/hpcaitech/EnergonAI/tree/main/examples/opt&#34;&gt;EnergonAI/examples/opt&lt;/a&gt;.&lt;br&gt; The entrance of the service is a bash script &lt;em&gt;&lt;strong&gt;server.sh&lt;/strong&gt;&lt;/em&gt;. The config of the service is at &lt;em&gt;&lt;strong&gt;opt_config.py&lt;/strong&gt;&lt;/em&gt;, which defines the model type, the checkpoint file path, the parallel strategy, and http settings. You can adapt it for your own case. For example, set the model class as opt_125M and set the correct checkpoint path as follows. Set the tensor parallelism degree the same as your gpu number.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    model_class = opt_125M&#xA;    checkpoint = &#39;your_file_path&#39;&#xA;    tp_init_size = #gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, we can launch a service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    bash server.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then open &lt;em&gt;&lt;strong&gt;https://[ip]:[port]/docs&lt;/strong&gt;&lt;/em&gt; in your browser and try out!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Publication&lt;/h3&gt; &#xA;&lt;p&gt;You can find technical details in our blog and manuscript:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.colossalai.org/docs/advanced_tutorials/opt_service/&#34;&gt;Build an online OPT service using Colossal-AI in 5 minutes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.02341.pdf&#34;&gt;EnergonAI: An Inference System for 10-100 Billion Parameter Transformer Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{du2022energonai, &#xA;      title={EnergonAI: An Inference System for 10-100 Billion Parameter Transformer Models}, &#xA;      author={Jiangsu Du and Ziming Liu and Jiarui Fang and Shenggui Li and Yongbin Li and Yutong Lu and Yang You},&#xA;      year={2022},&#xA;      eprint={2209.02341},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;If interested in making your own contribution to the project, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/EnergonAI/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt; for guidance.&lt;/p&gt; &#xA;&lt;p&gt;Thanks so much!&lt;/p&gt;</summary>
  </entry>
</feed>