<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-26T01:43:24Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bentoml/BentoML</title>
    <updated>2023-02-26T01:43:24Z</updated>
    <id>tag:github.com,2023-02-26:/bentoml/BentoML</id>
    <link href="https://github.com/bentoml/BentoML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unified Model Serving Framework üç±&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/bentoml/BentoML&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bentoml/BentoML/main/docs/source/_static/img/bentoml-readme-header.jpeg&#34; width=&#34;600px&#34; margin-left=&#34;-5px&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;The Unified Model Serving Framework &lt;a href=&#34;https://twitter.com/intent/tweet?text=BentoML:%20The%20Unified%20Model%20Serving%20Framework%20&amp;amp;url=https://github.com/bentoml&amp;amp;via=bentomlai&amp;amp;hashtags=mlops,bentoml&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&#34; alt=&#34;Tweet&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/BentoML&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/bentoml.svg?sanitize=true&#34; alt=&#34;pypi_status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/bentoml&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/bentoml&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/bentoml/bentoml/actions&#34;&gt;&lt;img src=&#34;https://github.com/bentoml/bentoml/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;actions_status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.bentoml.org/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/bentoml/badge/?version=latest&#34; alt=&#34;documentation_status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.bentoml.org&#34;&gt;&lt;img src=&#34;https://badgen.net/badge/Join/BentoML%20Slack/cyan?icon=slack&#34; alt=&#34;join_slack&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;BentoML makes it easy to create Machine Learning services that are ready to deploy and scale.&lt;/p&gt; &#xA;&lt;p&gt;üëâ &lt;a href=&#34;https://l.bentoml.com/join-slack&#34;&gt;Join our Slack community today!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ú® Looking deploy your ML service quickly? Checkout &lt;a href=&#34;https://l.bentoml.com/bento-cloud&#34;&gt;BentoML Cloud&lt;/a&gt; for the easiest and fastest way to deploy your bento.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bentoml.org/&#34;&gt;Documentation&lt;/a&gt; - Overview of the BentoML docs and related resources&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bentoml.org/en/latest/tutorial.html&#34;&gt;Tutorial: Intro to BentoML&lt;/a&gt; - Learn by doing! In under 10 minutes, you&#39;ll serve a model via REST API and generate a docker image for deployment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bentoml.org/en/latest/concepts/index.html&#34;&gt;Main Concepts&lt;/a&gt; - A step-by-step tour for learning main concepts in BentoML&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bentoml/BentoML/tree/main/examples&#34;&gt;Examples&lt;/a&gt; - Gallery of sample projects using BentoML&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bentoml.org/en/latest/frameworks/index.html&#34;&gt;ML Framework Guides&lt;/a&gt; - Best practices and example usages by the ML framework of your choice&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bentoml.org/en/latest/guides/index.html&#34;&gt;Advanced Guides&lt;/a&gt; - Learn about BentoML&#39;s internals, architecture and advanced features&lt;/li&gt; &#xA; &lt;li&gt;Need help? &lt;a href=&#34;https://l.linklyhq.com/l/ktOh&#34;&gt;Join BentoML Community Slack üí¨&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;p&gt;üç≠ Unified Model Serving API&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Framework-agnostic model packaging for Tensorflow, PyTorch, XGBoost, Scikit-Learn, ONNX, and &lt;a href=&#34;https://docs.bentoml.org/en/latest/frameworks/index.html&#34;&gt;many more&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;Write &lt;strong&gt;custom Python code&lt;/strong&gt; alongside model inference for pre/post-processing and business logic&lt;/li&gt; &#xA; &lt;li&gt;Apply the &lt;strong&gt;same code&lt;/strong&gt; for online(REST API or gRPC), offline batch, and streaming inference&lt;/li&gt; &#xA; &lt;li&gt;Simple abstractions for building &lt;strong&gt;multi-model inference&lt;/strong&gt; pipelines or graphs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üöÇ &lt;strong&gt;Standardized process&lt;/strong&gt; for a frictionless transition to production&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build &lt;a href=&#34;https://docs.bentoml.org/en/latest/concepts/bento.html&#34;&gt;Bento&lt;/a&gt; as the standard deployable artifact for ML services&lt;/li&gt; &#xA; &lt;li&gt;Automatically &lt;strong&gt;generate docker images&lt;/strong&gt; with the desired dependencies&lt;/li&gt; &#xA; &lt;li&gt;Easy CUDA setup for inference with GPU&lt;/li&gt; &#xA; &lt;li&gt;Rich integration with the MLOps ecosystem, including Kubeflow, Airflow, MLFlow, Triton&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üèπ &lt;strong&gt;&lt;em&gt;Scalable&lt;/em&gt;&lt;/strong&gt; with powerful performance optimizations&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bentoml.org/en/latest/guides/batching.html&#34;&gt;Adaptive batching&lt;/a&gt; dynamically groups inference requests on server-side optimal performance&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bentoml.org/en/latest/concepts/runner.html&#34;&gt;Runner&lt;/a&gt; abstraction scales model inference separately from your custom code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bentoml.org/en/latest/guides/gpu.html&#34;&gt;Maximize your GPU&lt;/a&gt; and multi-core CPU utilization with automatic provisioning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üéØ Deploy anywhere in a &lt;strong&gt;DevOps-friendly&lt;/strong&gt; way&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streamline production deployment workflow via: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://l.bentoml.com/bento-cloud&#34;&gt;‚òÅÔ∏è BentoML Cloud&lt;/a&gt;: the fastest way to deploy your bento, simple and at scale&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/bentoml/yatai&#34;&gt;ü¶ÑÔ∏è Yatai&lt;/a&gt;: Model Deployment at scale on Kubernetes&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/bentoml/bentoctl&#34;&gt;üöÄ bentoctl&lt;/a&gt;: Fast model deployment on AWS SageMaker, Lambda, ECE, GCP, Azure, Heroku, and more!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run offline batch inference jobs with Spark or Dask&lt;/li&gt; &#xA; &lt;li&gt;Built-in support for Prometheus metrics and OpenTelemetry&lt;/li&gt; &#xA; &lt;li&gt;Flexible APIs for advanced CI/CD workflows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;Save your trained model with BentoML:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import bentoml&#xA;&#xA;saved_model = bentoml.pytorch.save_model(&#xA;    &#34;demo_mnist&#34;, # model name in the local model store&#xA;    model, # model instance being saved&#xA;)&#xA;&#xA;print(f&#34;Model saved: {saved_model}&#34;)&#xA;# Model saved: Model(tag=&#34;demo_mnist:3qee3zd7lc4avuqj&#34;, path=&#34;~/bentoml/models/demo_mnist/3qee3zd7lc4avuqj/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Define a prediction service in a &lt;code&gt;service.py&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import bentoml&#xA;from bentoml.io import NumpyNdarray, Image&#xA;from PIL.Image import Image as PILImage&#xA;&#xA;mnist_runner = bentoml.pytorch.get(&#34;demo_mnist:latest&#34;).to_runner()&#xA;&#xA;svc = bentoml.Service(&#34;pytorch_mnist&#34;, runners=[mnist_runner])&#xA;&#xA;@svc.api(input=Image(), output=NumpyNdarray(dtype=&#34;int64&#34;))&#xA;def predict(input_img: PILImage):&#xA;    img_arr = np.array(input_img)/255.0&#xA;    input_arr = np.expand_dims(img_arr, 0).astype(&#34;float32&#34;)&#xA;    output_tensor = mnist_runner.predict.run(input_arr)&#xA;    return output_tensor.numpy()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;bentofile.yaml&lt;/code&gt; build file for your ML service:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;service: &#34;service:svc&#34;&#xA;include:&#xA;  - &#34;*.py&#34;&#xA;python:&#xA;  packages:&#xA;    - numpy&#xA;    - torch&#xA;    - Pillow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, run the prediction service:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bentoml serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sent a prediction request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -F &#39;image=@samples/1.png&#39; http://127.0.0.1:3000/predict_image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build a Bento and generate a docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ bentoml build&#xA;Successfully built Bento(tag=&#34;pytorch_mnist:4mymorgurocxjuqj&#34;) at &#34;~/bentoml/bentos/pytorch_mnist/4mymorgurocxjuqj/&#34;&#xA;&#xA;$ bentoml containerize pytorch_mnist:4mymorgurocxjuqj&#xA;Successfully built docker image &#34;pytorch_mnist:4mymorgurocxjuqj&#34;&#xA;&#xA;$ docker run -p 3000:3000 pytorch_mnist:4mymorgurocxjuqj&#xA;Starting production BentoServer from &#34;pytorch_mnist:4mymorgurocxjuqj&#34; running on http://0.0.0.0:3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a more detailed user guide, check out the &lt;a href=&#34;https://docs.bentoml.org/en/latest/tutorial.html&#34;&gt;BentoML Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For general questions and support, join the &lt;a href=&#34;https://l.linklyhq.com/l/ktOh&#34;&gt;community slack&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To receive release notification, star &amp;amp; watch the BentoML project on &lt;a href=&#34;https://github.com/bentoml/BentoML&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To report a bug or suggest a feature request, use &lt;a href=&#34;https://github.com/bentoml/BentoML/issues/new/choose&#34;&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To stay informed with community updates, follow the &lt;a href=&#34;http://modelserving.com&#34;&gt;BentoML Blog&lt;/a&gt; and &lt;a href=&#34;http://twitter.com/bentomlai&#34;&gt;@bentomlai&lt;/a&gt; on Twitter.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;There are many ways to contribute to the project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have any feedback on the project, share it under the &lt;code&gt;#bentoml-contributors&lt;/code&gt; channel in the &lt;a href=&#34;https://l.linklyhq.com/l/ktOh&#34;&gt;community slack&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Report issues you&#39;re facing and &#34;Thumbs up&#34; on issues and feature requests that are relevant to you.&lt;/li&gt; &#xA; &lt;li&gt;Investigate bugs and reviewing other developer&#39;s pull requests.&lt;/li&gt; &#xA; &lt;li&gt;Contributing code or documentation to the project by submitting a GitHub pull request. Check out the &lt;a href=&#34;https://github.com/bentoml/BentoML/raw/main/DEVELOPMENT.md&#34;&gt;Development Guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Learn more in the &lt;a href=&#34;https://github.com/bentoml/BentoML/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to all of our amazing contributors!&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/bentoml/BentoML/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=bentoml/BentoML&#34;&gt; &lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Usage Reporting&lt;/h3&gt; &#xA;&lt;p&gt;BentoML collects usage data that helps our team to improve the product. Only BentoML&#39;s internal API calls are being reported. We strip out as much potentially sensitive information as possible, and we will never collect user code, model data, model names, or stack traces. Here&#39;s the &lt;a href=&#34;https://raw.githubusercontent.com/bentoml/BentoML/main/src/bentoml/_internal/utils/analytics/usage_stats.py&#34;&gt;code&lt;/a&gt; for usage tracking. You can opt-out of usage tracking by the &lt;code&gt;--do-not-track&lt;/code&gt; CLI option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bentoml [command] --do-not-track&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or by setting environment variable &lt;code&gt;BENTOML_DO_NOT_TRACK=True&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export BENTOML_DO_NOT_TRACK=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bentoml/BentoML/raw/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2Fbentoml%2FBentoML?ref=badge_small&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2Fbentoml%2FBentoML.svg?type=small&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jerry-git/learn-python3</title>
    <updated>2023-02-26T01:43:24Z</updated>
    <id>tag:github.com,2023-02-26:/jerry-git/learn-python3</id>
    <link href="https://github.com/jerry-git/learn-python3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Jupyter notebooks for teaching/learning Python 3&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/jerry-git/learn-python3/master/logo.png&#34; alt=&#34;logo&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Learn Python 3&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/jerry-git/learn-python3&#34;&gt;&lt;img src=&#34;https://travis-ci.org/jerry-git/learn-python3.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains a collection of materials for teaching/learning Python 3 (3.5+).&lt;/p&gt; &#xA;&lt;h4&gt;Requirements&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Have Python 3.5 or newer installed. You can check the version by typing &lt;code&gt;python3 --version&lt;/code&gt; in your command line. You can download the latest Python version from &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Have &lt;a href=&#34;http://jupyter.readthedocs.io/en/latest/install.html&#34;&gt;Jupyter Notebook installed&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you can not access Python and/or Jupyter Notebook on your machine, you can still follow the web based materials. However, you should be able to use Jupyter Notebook in order to complete the exercises.&lt;/p&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone or download this repository.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;jupyter notebook&lt;/code&gt; command in your command line in the repository directory.&lt;/li&gt; &#xA; &lt;li&gt;Jupyter Notebook session will open in the browser and you can start navigating through the materials.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Contributing&lt;/h4&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/jerry-git/learn-python3/raw/master/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;Beginner&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/strings.html&#34;&gt;Strings&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/strings.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/strings_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/numbers.html&#34;&gt;Numbers&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/numbers.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/numbers_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/conditionals.html&#34;&gt;Conditionals&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/conditionals.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/conditionals_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/lists.html&#34;&gt;Lists&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/lists.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/lists_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/dictionaries.html&#34;&gt;Dictionaries&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/dictionaries.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/dictionaries_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/for_loops.html&#34;&gt;For loops&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/for_loops.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/for_loops_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/functions.html&#34;&gt;Functions&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/functions.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/functions_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/testing1.html&#34;&gt;Testing with pytest - part 1&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/testing1.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/testing1_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Recap exercise 1 &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/recap1_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/file_io.html&#34;&gt;File I\O&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/file_io.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/file_io_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/classes.html&#34;&gt;Classes&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/classes.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/classes_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/exceptions.html&#34;&gt;Exceptions&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/exceptions.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/exceptions_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/modules_and_packages.html&#34;&gt;Modules and packages&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/modules_and_packages.ipynb&#34;&gt;[notebook]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/debugging.html&#34;&gt;Debugging&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/debugging.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/debugging_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/std_lib.html&#34;&gt;Goodies of the Standard Library - part 1&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/std_lib.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/std_lib1_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/testing2.html&#34;&gt;Testing with pytest - part 2&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/testing2.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/testing2_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/venv.html&#34;&gt;Virtual environment&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/venv.ipynb&#34;&gt;[notebook]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/beginner/html/project_structure.html&#34;&gt;Project structure&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/notebooks/project_structure.ipynb&#34;&gt;[notebook]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Recap exercise 2 &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/beginner/exercises/recap2_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Intermediate&lt;/h2&gt; &#xA;&lt;h4&gt;Idiomatic Python&lt;/h4&gt; &#xA;&lt;p&gt;Python is a powerful language which contains many features not presented in most other programming languages. Idiomatic section will cover some of these Pythonic features in detail. These materials are especially useful for people with background in other programming languages.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/intermediate/html/idiomatic_loops.html&#34;&gt;Idiomatic loops&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/intermediate/notebooks/idiomatic_loops.ipynb&#34;&gt;[notebook]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/intermediate/html/idiomatic_dicts.html&#34;&gt;Idiomatic dictionaries&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/intermediate/notebooks/idiomatic_dicts.ipynb&#34;&gt;[notebook]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/intermediate/html/idiomatic_misc1.html&#34;&gt;Idiomatic Python - miscellaneous part 1&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/intermediate/notebooks/idiomatic_misc1.ipynb&#34;&gt;[notebook]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/intermediate/html/idiomatic_misc2.html&#34;&gt;Idiomatic Python - miscellaneous part 2&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/intermediate/notebooks/idiomatic_misc2.ipynb&#34;&gt;[notebook]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Idiomatic Python exercise &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/intermediate/exercises/idiomatic_python_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Step up your &lt;code&gt;pytest&lt;/code&gt; game&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/intermediate/html/pytest_fixtures.html&#34;&gt;Efficient use of fixtures&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/intermediate/notebooks/pytest_fixtures.ipynb&#34;&gt;[notebook]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Other tips and tricks&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Best practices&lt;/h4&gt; &#xA;&lt;p&gt;A list of best development practices for Python projects. Most of the practices listed here are also applicable for other languages, however the presented tooling focuses mainly on Python.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/intermediate/html/best_practices.html&#34;&gt;Best practices&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/intermediate/notebooks/best_practices.ipynb&#34;&gt;[notebook]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;General topics&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jerry-git.github.io/learn-python3/notebooks/intermediate/html/std_lib2.html&#34;&gt;Goodies of the Standard Library - part 2&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/intermediate/notebooks/std_lib2.ipynb&#34;&gt;[notebook]&lt;/a&gt; &lt;a href=&#34;http://nbviewer.jupyter.org/github/jerry-git/learn-python3/blob/master/notebooks/intermediate/exercises/std_lib2_exercise.ipynb&#34;&gt;[exercise]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Backlog&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sets&lt;/li&gt; &#xA; &lt;li&gt;Generators&lt;/li&gt; &#xA; &lt;li&gt;Decorators&lt;/li&gt; &#xA; &lt;li&gt;Context managers&lt;/li&gt; &#xA; &lt;li&gt;Playing with attributes&lt;/li&gt; &#xA; &lt;li&gt;*, *args, **kwargs&lt;/li&gt; &#xA; &lt;li&gt;Command line arguments with click&lt;/li&gt; &#xA; &lt;li&gt;OOP - inheritance&lt;/li&gt; &#xA; &lt;li&gt;OOP - Abstract Base Classes&lt;/li&gt; &#xA; &lt;li&gt;OOP - attrs&lt;/li&gt; &#xA; &lt;li&gt;Testing with mocks&lt;/li&gt; &#xA; &lt;li&gt;Structuring your tests&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Use cases&lt;/h2&gt; &#xA;&lt;h4&gt;Playing with the web&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;requests&lt;/li&gt; &#xA; &lt;li&gt;testing requests with responses&lt;/li&gt; &#xA; &lt;li&gt;beautifulsoup4&lt;/li&gt; &#xA; &lt;li&gt;selenium&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Communicating with databases&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SQLAlchemy&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Working with documents&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;excel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;openpyxl&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;pdf &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;pdfrw / PyPDF2&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Logo: Abdur-Rahmaan Janhangeer, &lt;a href=&#34;https://github.com/Abdur-rahmaanJ&#34;&gt;@Abdur-rahmaanJ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>jacobgil/pytorch-grad-cam</title>
    <updated>2023-02-26T01:43:24Z</updated>
    <id>tag:github.com,2023-02-26:/jacobgil/pytorch-grad-cam</id>
    <link href="https://github.com/jacobgil/pytorch-grad-cam" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Advanced AI Explainability for computer vision. Support for CNNs, Vision Transformers, Classification, Object detection, Segmentation, Image similarity and more.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/jacobgil/pytorch-grad-cam/workflows/Tests/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt; &lt;a href=&#34;https://pepy.tech/project/grad-cam&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/grad-cam?period=month&amp;amp;units=international_system&amp;amp;left_color=black&amp;amp;right_color=brightgreen&amp;amp;left_text=Monthly%20Downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/grad-cam&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/grad-cam?period=total&amp;amp;units=international_system&amp;amp;left_color=black&amp;amp;right_color=blue&amp;amp;left_text=Total%20Downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Advanced AI explainability for PyTorch&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install grad-cam&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Documentation with advanced tutorials: &lt;a href=&#34;https://jacobgil.github.io/pytorch-gradcam-book&#34;&gt;https://jacobgil.github.io/pytorch-gradcam-book&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a package with state of the art methods for Explainable AI for computer vision. This can be used for diagnosing model predictions, either in production or while developing models. The aim is also to serve as a benchmark of algorithms and metrics for research of new explainability methods.&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Comprehensive collection of Pixel Attribution methods for Computer Vision.&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Tested on many Common CNN Networks and Vision Transformers.&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Advanced use cases: Works with Classification, Object Detection, Semantic Segmentation, Embedding-similarity and more.&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Includes smoothing methods to make the CAMs look nice.&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê High performance: full support for batches of images in all methods.&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Includes metrics for checking if you can trust the explanations, and tuning them for best performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/jacobgil/jacobgil.github.io/raw/master/assets/cam_dog.gif?raw=true&#34; alt=&#34;visualization&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;What it does&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GradCAM&lt;/td&gt; &#xA;   &lt;td&gt;Weight the 2D activations by the average gradient&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HiResCAM&lt;/td&gt; &#xA;   &lt;td&gt;Like GradCAM but element-wise multiply the activations with the gradients; provably guaranteed faithfulness for certain models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GradCAMElementWise&lt;/td&gt; &#xA;   &lt;td&gt;Like GradCAM but element-wise multiply the activations with the gradients then apply a ReLU operation before summing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GradCAM++&lt;/td&gt; &#xA;   &lt;td&gt;Like GradCAM but uses second order gradients&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;XGradCAM&lt;/td&gt; &#xA;   &lt;td&gt;Like GradCAM but scale the gradients by the normalized activations&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AblationCAM&lt;/td&gt; &#xA;   &lt;td&gt;Zero out activations and measure how the output drops (this repository includes a fast batched implementation)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ScoreCAM&lt;/td&gt; &#xA;   &lt;td&gt;Perbutate the image by the scaled activations and measure how the output drops&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EigenCAM&lt;/td&gt; &#xA;   &lt;td&gt;Takes the first principle component of the 2D Activations (no class discrimination, but seems to give great results)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EigenGradCAM&lt;/td&gt; &#xA;   &lt;td&gt;Like EigenCAM but with class discrimination: First principle component of Activations*Grad. Looks like GradCAM, but cleaner&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LayerCAM&lt;/td&gt; &#xA;   &lt;td&gt;Spatially weight the activations by positive gradients. Works better especially in lower layers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FullGrad&lt;/td&gt; &#xA;   &lt;td&gt;Computes the gradients of the biases from all over the network, and then sums them&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deep Feature Factorizations&lt;/td&gt; &#xA;   &lt;td&gt;Non Negative Matrix Factorization on the 2D activations&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Visual Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;What makes the network think the image label is &#39;pug, pug-dog&#39;&lt;/th&gt; &#xA;   &lt;th&gt;What makes the network think the image label is &#39;tabby, tabby cat&#39;&lt;/th&gt; &#xA;   &lt;th&gt;Combining Grad-CAM with Guided Backpropagation for the &#39;pug, pug-dog&#39; class&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/jacobgil/pytorch-grad-cam/raw/master/examples/dog.jpg?raw=true&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/jacobgil/pytorch-grad-cam/raw/master/examples/cat.jpg?raw=true&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/jacobgil/pytorch-grad-cam/raw/master/examples/cam_gb_dog.jpg?raw=true&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Object Detection and Semantic Segmentation&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Object Detection&lt;/th&gt; &#xA;   &lt;th&gt;Semantic Segmentation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/both_detection.png&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cars_segmentation.png&#34; width=&#34;256&#34; height=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Explaining similarity to other images / embeddings&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/embeddings.png&#34;&gt; &#xA;&lt;h2&gt;Deep Feature Factorization&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dff1.png&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dff2.png&#34;&gt; &#xA;&lt;h2&gt;Classification&lt;/h2&gt; &#xA;&lt;h4&gt;Resnet50:&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Category&lt;/th&gt; &#xA;   &lt;th&gt;Image&lt;/th&gt; &#xA;   &lt;th&gt;GradCAM&lt;/th&gt; &#xA;   &lt;th&gt;AblationCAM&lt;/th&gt; &#xA;   &lt;th&gt;ScoreCAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dog&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog_cat.jfif&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/resnet50_dog_gradcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/resnet50_dog_ablationcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/resnet50_dog_scorecam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cat&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog_cat.jfif?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/resnet50_cat_gradcam_cam.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/resnet50_cat_ablationcam_cam.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/resnet50_cat_scorecam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Vision Transfomer (Deit Tiny):&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Category&lt;/th&gt; &#xA;   &lt;th&gt;Image&lt;/th&gt; &#xA;   &lt;th&gt;GradCAM&lt;/th&gt; &#xA;   &lt;th&gt;AblationCAM&lt;/th&gt; &#xA;   &lt;th&gt;ScoreCAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dog&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog_cat.jfif&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/vit_dog_gradcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/vit_dog_ablationcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/vit_dog_scorecam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cat&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog_cat.jfif&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/vit_cat_gradcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/vit_cat_ablationcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/vit_cat_scorecam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Swin Transfomer (Tiny window:7 patch:4 input-size:224):&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Category&lt;/th&gt; &#xA;   &lt;th&gt;Image&lt;/th&gt; &#xA;   &lt;th&gt;GradCAM&lt;/th&gt; &#xA;   &lt;th&gt;AblationCAM&lt;/th&gt; &#xA;   &lt;th&gt;ScoreCAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dog&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog_cat.jfif&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/swinT_dog_gradcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/swinT_dog_ablationcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/swinT_dog_scorecam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cat&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog_cat.jfif&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/swinT_cat_gradcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/swinT_cat_ablationcam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/swinT_cat_scorecam_cam.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Metrics and Evaluation for XAI&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/metrics.png&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/road.png&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Chosing the Target Layer&lt;/h1&gt; &#xA;&lt;p&gt;You need to choose the target layer to compute CAM for. Some common choices are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FasterRCNN: model.backbone&lt;/li&gt; &#xA; &lt;li&gt;Resnet18 and 50: model.layer4[-1]&lt;/li&gt; &#xA; &lt;li&gt;VGG and densenet161: model.features[-1]&lt;/li&gt; &#xA; &lt;li&gt;mnasnet1_0: model.layers[-1]&lt;/li&gt; &#xA; &lt;li&gt;ViT: model.blocks[-1].norm1&lt;/li&gt; &#xA; &lt;li&gt;SwinT: model.layers[-1].blocks[-1].norm1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you pass a list with several layers, the CAM will be averaged accross them. This can be useful if you&#39;re not sure what layer will perform best.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Using from code as a library&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad&#xA;from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget&#xA;from pytorch_grad_cam.utils.image import show_cam_on_image&#xA;from torchvision.models import resnet50&#xA;&#xA;model = resnet50(pretrained=True)&#xA;target_layers = [model.layer4[-1]]&#xA;input_tensor = # Create an input tensor image for your model..&#xA;# Note: input_tensor can be a batch tensor with several images!&#xA;&#xA;# Construct the CAM object once, and then re-use it on many images:&#xA;cam = GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda)&#xA;&#xA;# You can also use it within a with statement, to make sure it is freed,&#xA;# In case you need to re-create it inside an outer loop:&#xA;# with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam:&#xA;#   ...&#xA;&#xA;# We have to specify the target we want to generate&#xA;# the Class Activation Maps for.&#xA;# If targets is None, the highest scoring category&#xA;# will be used for every image in the batch.&#xA;# Here we use ClassifierOutputTarget, but you can define your own custom targets&#xA;# That are, for example, combinations of categories, or specific outputs in a non standard model.&#xA;&#xA;targets = [ClassifierOutputTarget(281)]&#xA;&#xA;# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.&#xA;grayscale_cam = cam(input_tensor=input_tensor, targets=targets)&#xA;&#xA;# In this example grayscale_cam has only one image in the batch:&#xA;grayscale_cam = grayscale_cam[0, :]&#xA;visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Metrics and evaluating the explanations&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pytorch_grad_cam.utils.model_targets import ClassifierOutputSoftmaxTarget&#xA;from pytorch_grad_cam.metrics.cam_mult_image import CamMultImageConfidenceChange&#xA;# Create the metric target, often the confidence drop in a score of some category&#xA;metric_target = ClassifierOutputSoftmaxTarget(281)&#xA;scores, batch_visualizations = CamMultImageConfidenceChange()(input_tensor, &#xA;  inverse_cams, targets, model, return_visualization=True)&#xA;visualization = deprocess_image(batch_visualizations[0, :])&#xA;&#xA;# State of the art metric: Remove and Debias&#xA;from pytorch_grad_cam.metrics.road import ROADMostRelevantFirst, ROADLeastRelevantFirst&#xA;cam_metric = ROADMostRelevantFirst(percentile=75)&#xA;scores, perturbation_visualizations = cam_metric(input_tensor, &#xA;  grayscale_cams, targets, model, return_visualization=True)&#xA;&#xA;# You can also average accross different percentiles, and combine&#xA;# (LeastRelevantFirst - MostRelevantFirst) / 2&#xA;from pytorch_grad_cam.metrics.road import ROADMostRelevantFirstAverage,&#xA;                                          ROADLeastRelevantFirstAverage,&#xA;                                          ROADCombined&#xA;cam_metric = ROADCombined(percentiles=[20, 40, 60, 80])&#xA;scores = cam_metric(input_tensor, grayscale_cams, targets, model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Advanced use cases and tutorials:&lt;/h1&gt; &#xA;&lt;p&gt;You can use this package for &#34;custom&#34; deep learning models, for example Object Detection or Semantic Segmentation.&lt;/p&gt; &#xA;&lt;p&gt;You will have to define objects that you can then pass to the CAM algorithms:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A reshape_transform, that aggregates the layer outputs into 2D tensors that will be displayed.&lt;/li&gt; &#xA; &lt;li&gt;Model Targets, that define what target do you want to compute the visualizations for, for example a specific category, or a list of bounding boxes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here you can find detailed examples of how to use this for various custom use cases like object detection:&lt;/p&gt; &#xA;&lt;p&gt;These point to the new documentation jupter-book for fast rendering. The jupyter notebooks themselves can be found under the tutorials folder in the git repository.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://jacobgil.github.io/pytorch-gradcam-book/HuggingFace.html&#34;&gt;Notebook tutorial: XAI Recipes for the HuggingFace ü§ó Image Classification Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://jacobgil.github.io/pytorch-gradcam-book/Deep%20Feature%20Factorizations.html&#34;&gt;Notebook tutorial: Deep Feature Factorizations for better model explainability&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Object%20Detection%20With%20Faster%20RCNN.html&#34;&gt;Notebook tutorial: Class Activation Maps for Object Detection with Faster-RCNN&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html&#34;&gt;Notebook tutorial: Class Activation Maps for YOLO5&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Semantic%20Segmentation.html&#34;&gt;Notebook tutorial: Class Activation Maps for Semantic Segmentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://jacobgil.github.io/pytorch-gradcam-book/Pixel%20Attribution%20for%20embeddings.html&#34;&gt;Notebook tutorial: Adapting pixel attribution methods for embedding outputs from models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://jacobgil.github.io/pytorch-gradcam-book/CAM%20Metrics%20And%20Tuning%20Tutorial.html&#34;&gt;Notebook tutorial: May the best explanation win. CAM Metrics and Tuning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/tutorials/vision_transformers.md&#34;&gt;How it works with Vision/SwinT transformers&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Smoothing to get nice looking CAMs&lt;/h1&gt; &#xA;&lt;p&gt;To reduce noise in the CAMs, and make it fit better on the objects, two smoothing methods are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;aug_smooth=True&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Test time augmentation: increases the run time by x6.&lt;/p&gt; &lt;p&gt;Applies a combination of horizontal flips, and mutiplying the image by [1.0, 1.1, 0.9].&lt;/p&gt; &lt;p&gt;This has the effect of better centering the CAM around the objects.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;eigen_smooth=True&lt;/code&gt;&lt;/p&gt; &lt;p&gt;First principle component of &lt;code&gt;activations*weights&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This has the effect of removing a lot of noise.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;AblationCAM&lt;/th&gt; &#xA;   &lt;th&gt;aug smooth&lt;/th&gt; &#xA;   &lt;th&gt;eigen smooth&lt;/th&gt; &#xA;   &lt;th&gt;aug+eigen smooth&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/nosmooth.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/augsmooth.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/eigensmooth.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/eigenaug.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Running the example script:&lt;/h1&gt; &#xA;&lt;p&gt;Usage: &lt;code&gt;python cam.py --image-path &amp;lt;path_to_image&amp;gt; --method &amp;lt;method&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To use with CUDA: &lt;code&gt;python cam.py --image-path &amp;lt;path_to_image&amp;gt; --use-cuda&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;You can choose between:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;GradCAM&lt;/code&gt; , &lt;code&gt;HiResCAM&lt;/code&gt;, &lt;code&gt;ScoreCAM&lt;/code&gt;, &lt;code&gt;GradCAMPlusPlus&lt;/code&gt;, &lt;code&gt;AblationCAM&lt;/code&gt;, &lt;code&gt;XGradCAM&lt;/code&gt; , &lt;code&gt;LayerCAM&lt;/code&gt;, &lt;code&gt;FullGrad&lt;/code&gt; and &lt;code&gt;EigenCAM&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some methods like ScoreCAM and AblationCAM require a large number of forward passes, and have a batched implementation.&lt;/p&gt; &#xA;&lt;p&gt;You can control the batch size with &lt;code&gt;cam.batch_size = &lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this for research, please cite. Here is an example BibTeX entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{jacobgilpytorchcam,&#xA;  title={PyTorch library for CAM methods},&#xA;  author={Jacob Gildenblat and contributors},&#xA;  year={2021},&#xA;  publisher={GitHub},&#xA;  howpublished={\url{https://github.com/jacobgil/pytorch-grad-cam}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1610.02391&#34;&gt;https://arxiv.org/abs/1610.02391&lt;/a&gt; &lt;br&gt; &lt;code&gt;Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.08891&#34;&gt;https://arxiv.org/abs/2011.08891&lt;/a&gt; &lt;br&gt; &lt;code&gt;Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks Rachel L. Draelos, Lawrence Carin&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.11063&#34;&gt;https://arxiv.org/abs/1710.11063&lt;/a&gt; &lt;br&gt; &lt;code&gt;Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks Aditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, Vineeth N Balasubramanian&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.01279&#34;&gt;https://arxiv.org/abs/1910.01279&lt;/a&gt; &lt;br&gt; &lt;code&gt;Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9093360/&#34;&gt;https://ieeexplore.ieee.org/abstract/document/9093360/&lt;/a&gt; &lt;br&gt; &lt;code&gt;Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization. Saurabh Desai and Harish G Ramaswamy. In WACV, pages 972‚Äì980, 2020&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.02312&#34;&gt;https://arxiv.org/abs/2008.02312&lt;/a&gt; &lt;br&gt; &lt;code&gt;Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.00299&#34;&gt;https://arxiv.org/abs/2008.00299&lt;/a&gt; &lt;br&gt; &lt;code&gt;Eigen-CAM: Class Activation Map using Principal Components Mohammed Bany Muhammad, Mohammed Yeasin&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf&#34;&gt;http://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf&lt;/a&gt; &lt;br&gt; &lt;code&gt;LayerCAM: Exploring Hierarchical Class Activation Maps for Localization Peng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.00780&#34;&gt;https://arxiv.org/abs/1905.00780&lt;/a&gt; &lt;br&gt; &lt;code&gt;Full-Gradient Representation for Neural Network Visualization Suraj Srinivas, Francois Fleuret&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.10206&#34;&gt;https://arxiv.org/abs/1806.10206&lt;/a&gt; &lt;br&gt; &lt;code&gt;Deep Feature Factorization For Concept Discovery Edo Collins, Radhakrishna Achanta, Sabine S√ºsstrunk&lt;/code&gt;&lt;/p&gt;</summary>
  </entry>
</feed>