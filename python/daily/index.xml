<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-20T01:40:44Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>houseofsecrets/SdPaint</title>
    <updated>2023-04-20T01:40:44Z</updated>
    <id>tag:github.com,2023-04-20:/houseofsecrets/SdPaint</id>
    <link href="https://github.com/houseofsecrets/SdPaint" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion Painting&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SdPaint&lt;/h1&gt; &#xA;&lt;p&gt;A simple python script that lets you paint on a canvas and sends that image every stroke to the automatic1111 API and updates the canvas when the image is generated&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;added the possibility to save the image created by pressing the &lt;code&gt;s&lt;/code&gt; key&lt;/li&gt; &#xA; &lt;li&gt;You can use the scrollmouse key to change the brush size&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Run the Start.bat file and it will create a venv and install a few packages&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you have the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;automatic1111 webui&lt;/a&gt; in API mode running in the background and that you have the controlnet extension installed and activated To start the webui with the API enabled modify the webui-user.bat file by adding &lt;code&gt;--api&lt;/code&gt; after &lt;code&gt;set COMMANDLINE_ARGS=&lt;/code&gt; You also need to make sure the &#34;Allow other script to control this extension&#34; option is enabled in the settings of control net&lt;/p&gt; &#xA;&lt;p&gt;You can modify the payload.json file for a different prompt, seed or different controlnet model. When you save the json file the program will use it after the next brush stroke. in the extra folder there are the names of different controlnet models you may have. replace this part &lt;code&gt;&#34;control_sd15_scribble [fef5e48e]&#34;,&lt;/code&gt; in the Payload.json with a different one from the modelnames.txt left mouse to draw and middlemouse to erase press backspace to erase the image. the program is bound to 512x512 images right now and doesn&#39;t have the ability to save the image right now. I may add more features at a later time.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.&lt;/p&gt; &#xA;&lt;p&gt;Please make sure to update tests as appropriate.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://choosealicense.com/licenses/mit/&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Vision-CAIR/MiniGPT-4</title>
    <updated>2023-04-20T01:40:44Z</updated>
    <id>tag:github.com,2023-04-20:/Vision-CAIR/MiniGPT-4</id>
    <link href="https://github.com/Vision-CAIR/MiniGPT-4" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tsutikgiau.github.io/&#34;&gt;Deyao Zhu&lt;/a&gt;* (On Job Market!), &lt;a href=&#34;https://junchen14.github.io/&#34;&gt;Jun Chen&lt;/a&gt;* (On Job Market!), &lt;a href=&#34;https://xiaoqian-shen.github.io&#34;&gt;Xiaoqian Shen&lt;/a&gt;, &lt;a href=&#34;https://xiangli.ac.cn&#34;&gt;Xiang Li&lt;/a&gt;, and &lt;a href=&#34;https://www.mohamed-elhoseiny.com/&#34;&gt;Mohamed Elhoseiny&lt;/a&gt;. *Equal Contribution&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;King Abdullah University of Science and Technology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://minigpt-4.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/MiniGPT_4.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-red&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Demo&lt;/h2&gt; &#xA;&lt;p&gt;Click the image to chat with MiniGPT-4 around your images &lt;a href=&#34;https://minigpt-4.github.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/online_demo.png&#34; alt=&#34;demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/examples/wop_2.png&#34; alt=&#34;find wild&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/examples/ad_2.png&#34; alt=&#34;write story&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/examples/fix_1.png&#34; alt=&#34;solve problem&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/examples/rhyme_1.png&#34; alt=&#34;write Poem&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;More examples can be found in the &lt;a href=&#34;https://minigpt-4.github.io&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MiniGPT-4 aligns a frozen visual encoder from BLIP-2 with a frozen LLM, Vicuna, using just one projection layer.&lt;/li&gt; &#xA; &lt;li&gt;We train MiniGPT-4 with two stages. The first traditional pretraining stage is trained using roughly 5 million aligned image-text pairs in 10 hours using 4 A100s. After the first stage, Vicuna is able to understand the image. But the generation ability of Vicuna is heavilly impacted.&lt;/li&gt; &#xA; &lt;li&gt;To address this issue and improve usability, we propose a novel way to create high-quality image-text pairs by the model itself and ChatGPT together. Based on this, we then create a small (3500 pairs in total) yet high-quality dataset.&lt;/li&gt; &#xA; &lt;li&gt;The second finetuning stage is trained on this dataset in a conversation template to significantly improve its generation reliability and overall usability. To our surprise, this stage is computationally efficient and takes only around 7 minutes with a single A100.&lt;/li&gt; &#xA; &lt;li&gt;MiniGPT-4 yields many emerging vision-language capabilities similar to those demonstrated in GPT-4.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/overview.png&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Prepare the code and the environment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Git clone our repository, creating a python environment and ativate it via the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Vision-CAIR/MiniGPT-4.git&#xA;cd MiniGPT-4&#xA;conda env create -f environment.yml&#xA;conda activate minigpt4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Prepare the pretrained Vicuna weights&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The current version of MiniGPT-4 is built on the v0 versoin of Vicuna-13B. Please refer to our instruction &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/PrepareVicuna.md&#34;&gt;here&lt;/a&gt; to prepare the Vicuna weights. The final weights would be in a single folder with the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;vicuna_weights&#xA;├── config.json&#xA;├── generation_config.json&#xA;├── pytorch_model.bin.index.json&#xA;├── pytorch_model-00001-of-00003.bin&#xA;...   &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, set the path to the vicuna weight in the model config file &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/minigpt4/configs/models/minigpt4.yaml#L16&#34;&gt;here&lt;/a&gt; at Line 16.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Prepare the pretrained MiniGPT-4 checkpoint&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To play with our pretrained model, download the pretrained checkpoint &lt;a href=&#34;https://drive.google.com/file/d/1a4zLvaiDBr-36pasffmgpvH5P7CKmpze/view?usp=share_link&#34;&gt;here&lt;/a&gt;. Then, set the path to the pretrained checkpoint in the evaluation config file in &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/eval_configs/minigpt4_eval.yaml#L10&#34;&gt;eval_configs/minigpt4_eval.yaml&lt;/a&gt; at Line 11.&lt;/p&gt; &#xA;&lt;h3&gt;Launching Demo Locally&lt;/h3&gt; &#xA;&lt;p&gt;Try out our demo &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/demo.py&#34;&gt;demo.py&lt;/a&gt; on your local machine by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py --cfg-path eval_configs/minigpt4_eval.yaml  --gpu-id 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, we load Vicuna as 8 bit by default to save some GPU memory usage. Besides, the default beam search width is 1. Under this setting, the demo cost about 23G GPU memory. If you have a more powerful GPU with larger GPU memory, you can run the model in 16 bit by setting low_resource to False in the config file &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/eval_configs/minigpt4_eval.yaml&#34;&gt;minigpt4_eval.yaml&lt;/a&gt; and use a larger beam search width.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;The training of MiniGPT-4 contains two alignment stages.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. First pretraining stage&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the first pretrained stage, the model is trained using image-text pairs from Laion and CC datasets to align the vision and language model. To download and prepare the datasets, please check our &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/dataset/README_1_STAGE.md&#34;&gt;first stage dataset preparation instruction&lt;/a&gt;. After the first stage, the visual features are mapped and can be understood by the language model. To launch the first stage training, run the following command. In our experiments, we use 4 A100. You can change the save path in the config file &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/train_configs/minigpt4_stage1_pretrain.yaml&#34;&gt;train_configs/minigpt4_stage1_pretrain.yaml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage1_pretrain.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A MiniGPT-4 checkpoint with only stage one training can be downloaded &lt;a href=&#34;https://drive.google.com/file/d/1u9FRRBB3VovP1HxCAlpD9Lw4t4P6-Yq8/view?usp=share_link&#34;&gt;here&lt;/a&gt;. Compared to the model after stage two, this checkpoint generate incomplete and repeated sentences frequently.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Second finetuning stage&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the second stage, we use a small high quality image-text pair dataset created by ourselves and convert it to a conversation format to further align MiniGPT-4. To download and prepare our second stage dataset, please check our &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/dataset/README_2_STAGE.md&#34;&gt;second stage dataset preparation instruction&lt;/a&gt;. To launch the second stage alignment, first specify the path to the checkpoint file trained in stage 1 in &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/train_configs/minigpt4_stage2_finetune.yaml&#34;&gt;train_configs/minigpt4_stage1_pretrain.yaml&lt;/a&gt;. You can also specify the output path there. Then, run the following command. In our experiments, we use 1 A100.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage2_finetune.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the second stage alignment, MiniGPT-4 is able to talk about the image coherently and user-friendly.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/blip-2&#34;&gt;BLIP2&lt;/a&gt; The model architecture of MiniGPT-4 follows BLIP-2. Don&#39;t forget to check this great open-source work if you don&#39;t know it before!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;Lavis&lt;/a&gt; This repository is built upon Lavis!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt; The fantastic language ability of Vicuna with only 13B parameters is just amazing. And it is open-source!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re using MiniGPT-4 in your research or applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhu2022minigpt4,&#xA;      title={MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models}, &#xA;      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and xiang Li and Mohamed Elhoseiny},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is under &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/LICENSE.md&#34;&gt;BSD 3-Clause License&lt;/a&gt;. Many codes are based on &lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;Lavis&lt;/a&gt; with BSD 3-Clause License &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/LICENSE_Lavis.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zilliztech/GPTCache</title>
    <updated>2023-04-20T01:40:44Z</updated>
    <id>tag:github.com,2023-04-20:/zilliztech/GPTCache</id>
    <link href="https://github.com/zilliztech/GPTCache" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GPTCache is a library for creating semantic cache to store responses from LLM queries.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPTCache : A Library for Creating Semantic Cache for LLM Queries&lt;/h1&gt; &#xA;&lt;p&gt;Slash Your LLM API Costs by 10x 💰, Boost Speed by 100x ⚡&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/gptcache/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/gptcache?label=Release&amp;amp;color&amp;amp;logo=Python&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/gptcache/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/gptcache.svg?color=bright-green&amp;amp;logo=Pypi&#34; alt=&#34;pip download&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/zilliztech/GPTCache&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/zilliztech/GPTCache/dev?label=Codecov&amp;amp;logo=codecov&amp;amp;token=E30WxqBeJJ&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/license/mit/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/zilliz_universe&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/zilliz_universe.svg?style=social&amp;amp;label=Follow%20%40Zilliz&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/Q8C6WEjSWV&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1092648432495251507?label=Discord&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project is undergoing swift development, and as such, the API may be subject to change at any time.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re looking for the &lt;strong&gt;most recent&lt;/strong&gt; update on GPTCache, please refer to: &lt;a href=&#34;https://github.com/zilliztech/GPTCache/raw/main/docs/release_note.md&#34;&gt;release note&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install gptcache&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🚀 What is GPTCache?&lt;/h2&gt; &#xA;&lt;p&gt;ChatGPT and various large language models (LLMs) boast incredible versatility, enabling the development of a wide range of applications. However, as your application grows in popularity and encounters higher traffic levels, the expenses related to LLM API calls can become substantial. Additionally, LLM services might exhibit slow response times, especially when dealing with a significant number of requests.&lt;/p&gt; &#xA;&lt;p&gt;To tackle this challenge, we have created GPTCache, a project dedicated to building a semantic cache for storing LLM responses.&lt;/p&gt; &#xA;&lt;h2&gt;😊 Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can quickly try GPTCache and put it into a production environment without heavy development. However, please note that the repository is still under heavy development.&lt;/li&gt; &#xA; &lt;li&gt;By default, only a limited number of libraries are installed to support the basic cache functionalities. When you need to use additional features, the related libraries will be &lt;strong&gt;automatically installed&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Make sure that the Python version is &lt;strong&gt;3.8.1 or higher&lt;/strong&gt;, check: &lt;code&gt;python --version&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you encounter issues installing a library due to a low pip version, run: &lt;code&gt;python -m pip install --upgrade pip&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;dev install&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# clone GPTCache repo&#xA;git clone -b dev https://github.com/zilliztech/GPTCache.git&#xA;cd GPTCache&#xA;&#xA;# install the repo&#xA;pip install -r requirements.txt&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;example usage&lt;/h3&gt; &#xA;&lt;p&gt;These examples will help you understand how to use exact and similar matching with caching.&lt;/p&gt; &#xA;&lt;p&gt;Before running the example, &lt;strong&gt;make sure&lt;/strong&gt; the OPENAI_API_KEY environment variable is set by executing &lt;code&gt;echo $OPENAI_API_KEY&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If it is not already set, it can be set by using &lt;code&gt;export OPENAI_API_KEY=YOUR_API_KEY&lt;/code&gt; on Unix/Linux/MacOS systems or &lt;code&gt;set OPENAI_API_KEY=YOUR_API_KEY&lt;/code&gt; on Windows systems.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;It is important to note that this method is only effective temporarily, so if you want a permanent effect, you&#39;ll need to modify the environment variable configuration file. For instance, on a Mac, you can modify the file located at &lt;code&gt;/etc/profile&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to &lt;strong&gt;SHOW&lt;/strong&gt; example code &lt;/summary&gt; &#xA; &lt;h4&gt;OpenAI API original usage&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import time&#xA;&#xA;import openai&#xA;&#xA;&#xA;def response_text(openai_resp):&#xA;    return openai_resp[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]&#xA;&#xA;&#xA;question = &#39;what‘s chatgpt&#39;&#xA;&#xA;# OpenAI API original usage&#xA;openai.api_key = os.getenv(&#34;OPENAI_API_KEY&#34;)&#xA;start_time = time.time()&#xA;response = openai.ChatCompletion.create(&#xA;  model=&#39;gpt-3.5-turbo&#39;,&#xA;  messages=[&#xA;    {&#xA;        &#39;role&#39;: &#39;user&#39;,&#xA;        &#39;content&#39;: question&#xA;    }&#xA;  ],&#xA;)&#xA;print(f&#39;Question: {question}&#39;)&#xA;print(&#34;Time consuming: {:.2f}s&#34;.format(time.time() - start_time))&#xA;print(f&#39;Answer: {response_text(response)}\n&#39;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;OpenAI API + GPTCache, exact match cache&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;If you ask ChatGPT the exact same two questions, the answer to the second question will be obtained from the cache without requesting ChatGPT again.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time&#xA;&#xA;&#xA;def response_text(openai_resp):&#xA;    return openai_resp[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]&#xA;&#xA;print(&#34;Cache loading.....&#34;)&#xA;&#xA;# To use GPTCache, that&#39;s all you need&#xA;# -------------------------------------------------&#xA;from gptcache import cache&#xA;from gptcache.adapter import openai&#xA;&#xA;cache.init()&#xA;cache.set_openai_key()&#xA;# -------------------------------------------------&#xA;&#xA;question = &#34;what&#39;s github&#34;&#xA;for _ in range(2):&#xA;    start_time = time.time()&#xA;    response = openai.ChatCompletion.create(&#xA;      model=&#39;gpt-3.5-turbo&#39;,&#xA;      messages=[&#xA;        {&#xA;            &#39;role&#39;: &#39;user&#39;,&#xA;            &#39;content&#39;: question&#xA;        }&#xA;      ],&#xA;    )&#xA;    print(f&#39;Question: {question}&#39;)&#xA;    print(&#34;Time consuming: {:.2f}s&#34;.format(time.time() - start_time))&#xA;    print(f&#39;Answer: {response_text(response)}\n&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;OpenAI API + GPTCache, similar search cache&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;After obtaining an answer from ChatGPT in response to several similar questions, the answers to subsequent questions can be retrieved from the cache without the need to request ChatGPT again.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time&#xA;&#xA;&#xA;def response_text(openai_resp):&#xA;    return openai_resp[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]&#xA;&#xA;from gptcache import cache&#xA;from gptcache.adapter import openai&#xA;from gptcache.embedding import Onnx&#xA;from gptcache.manager import CacheBase, VectorBase, get_data_manager&#xA;from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation&#xA;&#xA;print(&#34;Cache loading.....&#34;)&#xA;&#xA;onnx = Onnx()&#xA;data_manager = get_data_manager(CacheBase(&#34;sqlite&#34;), VectorBase(&#34;faiss&#34;, dimension=onnx.dimension))&#xA;cache.init(&#xA;    embedding_func=onnx.to_embeddings,&#xA;    data_manager=data_manager,&#xA;    similarity_evaluation=SearchDistanceEvaluation(),&#xA;    )&#xA;cache.set_openai_key()&#xA;&#xA;questions = [&#xA;    &#34;what&#39;s github&#34;,&#xA;    &#34;can you explain what GitHub is&#34;,&#xA;    &#34;can you tell me more about GitHub&#34;&#xA;    &#34;what is the purpose of GitHub&#34;&#xA;]&#xA;&#xA;for question in questions:&#xA;    start_time = time.time()&#xA;    response = openai.ChatCompletion.create(&#xA;        model=&#39;gpt-3.5-turbo&#39;,&#xA;        messages=[&#xA;            {&#xA;                &#39;role&#39;: &#39;user&#39;,&#xA;                &#39;content&#39;: question&#xA;            }&#xA;        ],&#xA;    )&#xA;    print(f&#39;Question: {question}&#39;)&#xA;    print(&#34;Time consuming: {:.2f}s&#34;.format(time.time() - start_time))&#xA;    print(f&#39;Answer: {response_text(response)}\n&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;To use GPTCache exclusively, only the following lines of code are required, and there is no need to modify any existing code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gptcache import cache&#xA;from gptcache.adapter import openai&#xA;&#xA;cache.init()&#xA;cache.set_openai_key()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More Docs：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zilliztech/GPTCache/main/docs/usage.md&#34;&gt;Usage, how to use GPTCache better&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zilliztech/GPTCache/main/docs/feature.md&#34;&gt;Features, all features currently supported by the cache&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zilliztech/GPTCache/main/examples/README.md&#34;&gt;Examples, learn better custom caching&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;😎 What can this help with?&lt;/h2&gt; &#xA;&lt;p&gt;GPTCache offers the following primary benefits:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Decreased expenses&lt;/strong&gt;: Most LLM services charge fees based on a combination of number of requests and &lt;a href=&#34;https://openai.com/pricing&#34;&gt;token count&lt;/a&gt;. GPTCache effectively minimizes your expenses by caching query results, which in turn reduces the number of requests and tokens sent to the LLM service. As a result, you can enjoy a more cost-efficient experience when using the service.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhanced performance&lt;/strong&gt;: LLMs employ generative AI algorithms to generate responses in real-time, a process that can sometimes be time-consuming. However, when a similar query is cached, the response time significantly improves, as the result is fetched directly from the cache, eliminating the need to interact with the LLM service. In most situations, GPTCache can also provide superior query throughput compared to standard LLM services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adaptable development and testing environment&lt;/strong&gt;: As a developer working on LLM applications, you&#39;re aware that connecting to LLM APIs is generally necessary, and comprehensive testing of your application is crucial before moving it to a production environment. GPTCache provides an interface that mirrors LLM APIs and accommodates storage of both LLM-generated and mocked data. This feature enables you to effortlessly develop and test your application, eliminating the need to connect to the LLM service.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improved scalability and availability&lt;/strong&gt;: LLM services frequently enforce &lt;a href=&#34;https://platform.openai.com/docs/guides/rate-limits&#34;&gt;rate limits&lt;/a&gt;, which are constraints that APIs place on the number of times a user or client can access the server within a given timeframe. Hitting a rate limit means that additional requests will be blocked until a certain period has elapsed, leading to a service outage. With GPTCache, you can easily scale to accommodate an increasing volume of of queries, ensuring consistent performance as your application&#39;s user base expands.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🤔 How does it work?&lt;/h2&gt; &#xA;&lt;p&gt;Online services often exhibit data locality, with users frequently accessing popular or trending content. Cache systems take advantage of this behavior by storing commonly accessed data, which in turn reduces data retrieval time, improves response times, and eases the burden on backend servers. Traditional cache systems typically utilize an exact match between a new query and a cached query to determine if the requested content is available in the cache before fetching the data.&lt;/p&gt; &#xA;&lt;p&gt;However, using an exact match approach for LLM caches is less effective due to the complexity and variability of LLM queries, resulting in a low cache hit rate. To address this issue, GPTCache adopt alternative strategies like semantic caching. Semantic caching identifies and stores similar or related queries, thereby increasing cache hit probability and enhancing overall caching efficiency.&lt;/p&gt; &#xA;&lt;p&gt;GPTCache employs embedding algorithms to convert queries into embeddings and uses a vector store for similarity search on these embeddings. This process allows GPTCache to identify and retrieve similar or related queries from the cache storage, as illustrated in the &lt;a href=&#34;https://github.com/zilliztech/GPTCache#-modules&#34;&gt;Modules section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Featuring a modular design, GPTCache makes it easy for users to customize their own semantic cache. The system offers various implementations for each module, and users can even develop their own implementations to suit their specific needs.&lt;/p&gt; &#xA;&lt;p&gt;In a semantic cache, you may encounter false positives during cache hits and false negatives during cache misses. GPTCache offers three metrics to gauge its performance, which are helpful for developers to optimize their caching systems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hit Ratio&lt;/strong&gt;: This metric quantifies the cache&#39;s ability to fulfill content requests successfully, compared to the total number of requests it receives. A higher hit ratio indicates a more effective cache.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: This metric measures the time it takes for a query to be processed and the corresponding data to be retrieved from the cache. Lower latency signifies a more efficient and responsive caching system.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt;: This metric represents the proportion of queries served by the cache out of the total number of queries that should have been served by the cache. Higher recall percentages indicate that the cache is effectively serving the appropriate content.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://github.com/zilliztech/gpt-cache/raw/main/examples/benchmark/benchmark_sqlite_faiss_onnx.py&#34;&gt;sample benchmark&lt;/a&gt; is included for users to start with assessing the performance of their semantic cache.&lt;/p&gt; &#xA;&lt;h2&gt;🤗 Modules&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zilliztech/GPTCache/main/docs/GPTCacheStructure.png&#34; alt=&#34;GPTCache Struct&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Adapter&lt;/strong&gt;: The LLM Adapter is designed to integrate different LLM models by unifying their APIs and request protocols. GPTCache offers a standardized interface for this purpose, with current support for ChatGPT integration.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support OpenAI ChatGPT API.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support langchain.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support other LLMs, such as Hugging Face Hub, Bard, Anthropic, and self-hosted models like LLaMA.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multimodal Adapter (experimental)&lt;/strong&gt;: The Multimodal Adapter is designed to integrate different large multimodal models by unifying their APIs and request protocols. GPTCache offers a standardized interface for this purpose, with current support for integrations of image generation, audio transcription.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support OpenAI Image Create.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support OpenAI Audio Transcribe.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support Hugging Face Stable Diffusion Pipeline (local inference).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support BLIP model (local inference).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support other multimodal services or self-hosted large multimodal models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Embedding Generator&lt;/strong&gt;: This module is created to extract embeddings from requests for similarity search. GPTCache offers a generic interface that supports multiple embedding APIs, and presents a range of solutions to choose from.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Disable embedding. This will turn GPTCache into a keyword-matching cache.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support OpenAI embedding API.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://onnx.ai/&#34;&gt;ONNX&lt;/a&gt; with the GPTCache/paraphrase-albert-onnx model.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; embedding with transformers, Data2VecAudio.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://docs.cohere.ai/reference/embed&#34;&gt;Cohere&lt;/a&gt; embedding API.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://fasttext.cc&#34;&gt;fastText&lt;/a&gt; embedding.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://www.sbert.net&#34;&gt;SentenceTransformers&lt;/a&gt; embedding.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support other embedding APIs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cache Storage&lt;/strong&gt;: &lt;strong&gt;Cache Storage&lt;/strong&gt; is where the response from LLMs, such as ChatGPT, is stored. Cached responses are retrieved to assist in evaluating similarity and are returned to the requester if there is a good semantic match. At present, GPTCache supports SQLite and offers a universally accessible interface for extension of this module.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://sqlite.org/docs.html&#34;&gt;SQLite&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://www.mysql.com/&#34;&gt;MySQL&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://mariadb.org/&#34;&gt;MariaDB&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://www.microsoft.com/en-us/sql-server/&#34;&gt;SQL Server&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://www.oracle.com/&#34;&gt;Oracle&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;a href=&#34;https://www.mongodb.com/&#34;&gt;MongoDB&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;a href=&#34;https://redis.io/&#34;&gt;Redis&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;a href=&#34;https://min.io/&#34;&gt;Minio&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;a href=&#34;https://hbase.apache.org/&#34;&gt;HBase&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;a href=&#34;https://www.elastic.co/&#34;&gt;ElasticSearch&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;a href=&#34;https://github.com/duckdb/duckdb&#34;&gt;Duckdb&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support other storages.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector Store&lt;/strong&gt;: The &lt;strong&gt;Vector Store&lt;/strong&gt; module helps find the K most similar requests from the input request&#39;s extracted embedding. The results can help assess similarity. GPTCache provides a user-friendly interface that supports various vector stores, including Milvus, Zilliz Cloud, and FAISS. More options will be available in the future.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://milvus.io/&#34;&gt;Milvus&lt;/a&gt;, an open-source vector database for production-ready AI/LLM applicaionts.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://cloud.zilliz.com/&#34;&gt;Zilliz Cloud&lt;/a&gt;, a fully-managed cloud vector database based on Milvus.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://github.com/milvus-io/milvus-lite&#34;&gt;Milvus Lite&lt;/a&gt;, a lightweight version of Milvus that can be embedded into your Python application.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://faiss.ai/&#34;&gt;FAISS&lt;/a&gt;, a library for efficient similarity search and clustering of dense vectors.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support &lt;a href=&#34;https://github.com/nmslib/hnswlib&#34;&gt;Hnswlib&lt;/a&gt;, header-only C++/python library for fast approximate nearest neighbors.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support qdrant&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support weaviate&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support chroma&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support other vector databases.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cache Manager&lt;/strong&gt;: The &lt;strong&gt;Cache Manager&lt;/strong&gt; is responsible for controlling the operation of both the &lt;strong&gt;Cache Storage&lt;/strong&gt; and &lt;strong&gt;Vector Store&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Eviction Policy&lt;/strong&gt;: Currently, GPTCache makes decisions about evictions based solely on the number of lines. This approach can result in inaccurate resource evaluation and may cause out-of-memory (OOM) errors. We are actively investigating and developing a more sophisticated strategy. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support LRU eviction policy.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support FIFO eviction policy.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support more complicated eviction policies.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Similarity Evaluator&lt;/strong&gt;: This module collects data from both the &lt;strong&gt;Cache Storage&lt;/strong&gt; and &lt;strong&gt;Vector Store&lt;/strong&gt;, and uses various strategies to determine the similarity between the input request and the requests from the &lt;strong&gt;Vector Store&lt;/strong&gt;. Based on this similarity, it determines whether a request matches the cache. GPTCache provides a standardized interface for integrating various strategies, along with a collection of implementations to use. The following similarity definitions are currently supported or will be supported in the future:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; The distance we obtain from the &lt;strong&gt;Vector Store&lt;/strong&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A model-based similarity determined using the GPTCache/albert-duplicate-onnx model from &lt;a href=&#34;https://onnx.ai/&#34;&gt;ONNX&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Exact matches between the input request and the requests obtained from the &lt;strong&gt;Vector Store&lt;/strong&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Distance represented by applying linalg.norm from numpy to the embeddings.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; BM25 and other similarity measurements.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support other model serving framework such as PyTorch.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:Not all combinations of different modules may be compatible with each other. For instance, if we disable the &lt;strong&gt;Embedding Extractor&lt;/strong&gt;, the &lt;strong&gt;Vector Store&lt;/strong&gt; may not function as intended. We are currently working on implementing a combination sanity check for &lt;strong&gt;GPTCache&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;😇 Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Coming soon! &lt;a href=&#34;https://twitter.com/zilliz_universe&#34;&gt;Stay tuned!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;😍 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We are extremely open to contributions, be it through new features, enhanced infrastructure, or improved documentation.&lt;/p&gt; &#xA;&lt;p&gt;For comprehensive instructions on how to contribute, please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/zilliztech/GPTCache/main/docs/contributing.md&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>