<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-15T02:57:51Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chengaopan/AutoMergePublicNodes</title>
    <updated>2024-04-15T02:57:51Z</updated>
    <id>tag:github.com,2024-04-15:/chengaopan/AutoMergePublicNodes</id>
    <link href="https://github.com/chengaopan/AutoMergePublicNodes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;自动抓取合并互联网上的公开节点。 🚀 免费节点,🚀免费节点订阅,🚀v2ray免费节点,ssr免费节点订阅,clash免费节点订阅,免费梯子,免费翻墙,免费科学上网,免费ss/v2ray/trojan/clash节点,谷歌商店,翻墙梯子&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoMergePublicNodes&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chengaopan/AutoMergePublicNodes/actions/workflows/fetch.yml&#34;&gt;&lt;img src=&#34;https://github.com/chengaopan/AutoMergePublicNodes/actions/workflows/fetch.yml/badge.svg?sanitize=true&#34; alt=&#34;Fetch Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chengaopan/AutoMergePublicNodes/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/chengaopan/AutoMergePublicNodes&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chengaopan/AutoMergePublicNodes/watchers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/chengaopan/AutoMergePublicNodes&#34; alt=&#34;Watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chengaopan/AutoMergePublicNodes/forks&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/chengaopan/AutoMergePublicNodes&#34; alt=&#34;Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://996.icu&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/link-996.icu-red.svg?sanitize=true&#34; alt=&#34;996.icu&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chengaopan/AutoMergePublicNodes/raw/master/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Anti%20996-blue.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://proj3ctaurora.tilda.ws/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/link-%E6%9A%AE%E5%85%89%E8%AE%A1%E5%88%92--%E5%90%91%E6%88%92%E7%BD%91%E7%98%BE%E5%AD%A6%E6%A0%A1%E5%AE%A3%E6%88%98-red.svg?sanitize=true&#34; alt=&#34;暮光计划&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;自动抓取合并互联网上的公开节点。 🚀 免费节点,🚀免费节点订阅,🚀v2ray免费节点,ssr免费节点订阅,clash免费节点订阅,免费梯子,免费翻墙,免费科学上网,免费ss/v2ray/trojan/clash节点,谷歌商店,翻墙梯子&lt;/p&gt; &#xA;&lt;h2&gt;公告&lt;/h2&gt; &#xA;&lt;p&gt;本项目fork &lt;a href=&#34;https://github.com/peasoft/NoMoreWalls&#34;&gt;NoMoreWalls&lt;/a&gt; 为防止失联，&lt;a href=&#34;https://github.com/peasoft&#34;&gt;peasoft&lt;/a&gt;建立了镜像：&lt;a href=&#34;https://peasoft.github.io/NWalls.html&#34;&gt;https://peasoft.github.io/NWalls.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;关于 Google Play 等服务在国内无法使用的解决方法&lt;/h2&gt; &#xA;&lt;p&gt;由于 Google 调整了服务器安排，将原有的国外服务器的&lt;strong&gt;域名&lt;/strong&gt;调整到了国内专版，但是&lt;strong&gt;服务器&lt;/strong&gt;还没跟上，导致 Google Play 等服务在国内连上的是&lt;strong&gt;空域名&lt;/strong&gt;，直接不能用了。当前的解决办法有：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;正常更新本项目的 Clash 订阅&lt;/strong&gt;，我们将 &lt;code&gt;googleapis.cn&lt;/code&gt; 强制走了代理，让 Google Play 继续使用国外服务器，部分网络架构（如本机运行 Clash For Android）下服务能够恢复正常。如果您使用的是本项目提供的规则片段，请在 &lt;code&gt;rules&lt;/code&gt; 开头加上：&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  - DOMAIN-SUFFIX,googleapis.cn,🚀 选择代理&#xA;  - DOMAIN-SUFFIX,xn--ngstr-lra8j.com,DIRECT # Google Play 国外/国内 服务器&#xA;  - DOMAIN-SUFFIX,xn--ngstr-cn-8za9o.com,DIRECT # Google Play 纯国内 服务器，尚未完成部署&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;如果方案 1 无效，且你的手机已 ROOT，请解除 GMS 锁区&lt;/strong&gt;，安装 Magisk 模块 &lt;a href=&#34;https://github.com/fei-ke/unlock-cn-gms&#34;&gt;Unlock-cn-gms&lt;/a&gt;（&lt;a href=&#34;https://github.com/fei-ke/unlock-cn-gms/releases/download/v3.4/unlock-cn-gms-v3.4.zip&#34;&gt;zip 下载&lt;/a&gt;），这不一定适合所有手机，请先关注您手机中相关锁区文件的位置。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;如果你的手机未 ROOT，请使用 Clash For Android 试一试&lt;/strong&gt;，有概率正常。项目没了，找备份！&lt;/li&gt; &#xA; &lt;li&gt;实在不行就等等吧，但愿 Google 能尽快修复此问题。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;如果此问题有进展，我们会在此更新，请及时关注。&lt;/p&gt; &#xA;&lt;p&gt;注意：最近加速链接出现大量失效（在我所在的网络下），如果无法更新订阅，请把所有链接从上到下每个试一遍！&lt;/p&gt; &#xA;&lt;p&gt;我们新增了 &lt;code&gt;snippets&lt;/code&gt; 文件夹来存放从 &lt;code&gt;list.yml&lt;/code&gt; 中拆分出的配置片段，用于将本项目提供的一些配置整合到你自己的配置中。&lt;/p&gt; &#xA;&lt;p&gt;此项目现已添加“反 996 许可证”，请各位使用者&lt;strong&gt;不要违法违规要求别人加班，自觉遵守《中华人民共和国劳动法》及其它法律法规&lt;/strong&gt;！&lt;/p&gt; &#xA;&lt;h2&gt;使用方法&lt;/h2&gt; &#xA;&lt;p&gt;添加 Base64 订阅：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chengaopan/AutoMergePublicNodes/master/list.txt&#34;&gt;原始链接&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.jsdelivr.us/gh/chengaopan/AutoMergePublicNodes@master/list.txt&#34;&gt;JsDelivr 反代（zzko.cn）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fastly.jsdelivr.net/gh/chengaopan/AutoMergePublicNodes@master/list.txt&#34;&gt;JsDelivr Fastly CDN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://testingcf.jsdelivr.net/gh/chengaopan/AutoMergePublicNodes@master/list.txt&#34;&gt;JsDelivr Cloudflare CDN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gcore.jsdelivr.net/gh/chengaopan/AutoMergePublicNodes@master/list.txt&#34;&gt;JsDelivr GCore CDN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.kkgithub.com/chengaopan/AutoMergePublicNodes/master/list.txt&#34;&gt;KKGithub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.fgit.cf/chengaopan/AutoMergePublicNodes/master/list.txt&#34;&gt;FastGit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;以下加速链接可能无效：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.kgithub.com/chengaopan/AutoMergePublicNodes/master/list.txt&#34;&gt;KGithub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;或添加 Clash 订阅：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chengaopan/AutoMergePublicNodes/master/list.yml&#34;&gt;原始链接&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.jsdelivr.us/gh/chengaopan/AutoMergePublicNodes@master/list.yml&#34;&gt;JsDelivr 反代（zzko.cn）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fastly.jsdelivr.net/gh/chengaopan/AutoMergePublicNodes@master/list.yml&#34;&gt;JsDelivr Fastly CDN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://testingcf.jsdelivr.net/gh/chengaopan/AutoMergePublicNodes@master/list.yml&#34;&gt;JsDelivr Cloudflare CDN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gcore.jsdelivr.net/gh/chengaopan/AutoMergePublicNodes@master/list.yml&#34;&gt;JsDelivr GCore CDN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.kkgithub.com/chengaopan/AutoMergePublicNodes/master/list.yml&#34;&gt;KKGithub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.fgit.cf/chengaopan/AutoMergePublicNodes/master/list.yml&#34;&gt;FastGit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;以下加速链接可能无效：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.kgithub.com/chengaopan/AutoMergePublicNodes/master/list.yml&#34;&gt;KGithub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;订阅节点仅作学习交流使用，用于查找资料，学习知识，不做任何违法行为。所有资源均来自互联网，仅供大家交流学习使用，出现违法问题概不负责。&lt;strong&gt;做出违法行为需要承担法律责任，侥幸逃脱是不可能的&lt;/strong&gt;！为阻止违法行为，本项目随时可以停止运行！！！&lt;/p&gt; &#xA;&lt;h2&gt;一些题外话&lt;/h2&gt; &#xA;&lt;p&gt;各位看一看：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://peasoft.github.io/2023/08/26/cnedu.html&#34;&gt;【独家恢复】我们的教育弄虚作假，到底是为了什么&lt;/a&gt;：如此视频，为何惨遭删除？我们恢复了这段视频，只为让更多人可以看清现实。&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.zhihu.com/question/29129310&#34;&gt;最流氓的软件可以流氓到什么程度？&lt;/a&gt;我翻开其他网页一查，歪歪斜斜的每页上都写着“危险网页”几个字。我横竖睡不着，仔细看了半夜，才从字缝里看出字来，满本都写着两个字是“霸权”！&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://user.guancha.cn/main/content?id=100552&#34;&gt;百度？百毒！&lt;/a&gt;魏则西去世3周年：害死他的百度广告和莆田系医院&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV11v4y1t7Gw/&#34;&gt;《满江红》的行为艺术&lt;/a&gt;：秦桧竟是我自己？&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/P_EYQxOEupqdU0BJMRqWsw&#34;&gt;「 深蓝洞察 」2022 年度最“不可赦”漏洞&lt;/a&gt;：知名互联网厂商(TMD并夕夕)持续挖掘新的安卓 OEM 相关漏洞，在其公开发布的 App 中实现对目前市场主流手机系统的漏洞攻击&lt;/strong&gt;（&lt;a href=&#34;https://mp.weixin.qq.com/s/kiLvnJSDZpYRHI_XiUx9gg&#34;&gt;具体分析&lt;/a&gt;）现已被工信部提名！&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Vk4y1K79B&#34;&gt;暑假学校敢补课？举报！&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Mg4y1A7bE&#34;&gt;逃离戒网瘾学校？我们找到办法了&lt;/a&gt;：希望你永远用不到。&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1a14y1S7n6&#34;&gt;一学校扔掉学生百余份外卖&lt;/a&gt;：涉嫌违法！&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;未完待续……&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;a href=&#34;https://star-history.com/#chengaopan/AutoMergePublicNodes&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=chengaopan/AutoMergePublicNodes&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=chengaopan/AutoMergePublicNodes&#34;&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=chengaopan/AutoMergePublicNodes&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>openai/simple-evals</title>
    <updated>2024-04-15T02:57:51Z</updated>
    <id>tag:github.com,2024-04-15:/openai/simple-evals</id>
    <link href="https://github.com/openai/simple-evals" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains a lightweight library for evaluating language models. We are open sourcing it so we can be transparent about the accuracy numbers we&#39;re publishing alongside our latest models (starting with &lt;code&gt;gpt-4-turbo-2024-04-09&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Evals are sensitive to prompting, and there&#39;s significant variation in the formulations used in recent publications and libraries. Some use few-shot prompts or role playing prompts (&#34;You are an expert software programmer...&#34;). These approaches are carryovers from evaluating &lt;em&gt;base models&lt;/em&gt; (rather than instruction/chat-tuned models) and from models that were worse at following instructions.&lt;/p&gt; &#xA;&lt;p&gt;For this library, we are emphasizing the &lt;em&gt;zero-shot, chain-of-thought&lt;/em&gt; setting, with simple instructions like &#34;Solve the following multiple choice problem&#34;. We believe that this prompting technique is a better reflection of the models&#39; performance in realistic usage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We will not be actively maintaining this repository and monitoring PRs and Issues.&lt;/strong&gt; In particular, we&#39;re not accepting new evals. Here are the changes we might accept.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bug fixes (hopefully not needed!)&lt;/li&gt; &#xA; &lt;li&gt;Adding adapters for new models&lt;/li&gt; &#xA; &lt;li&gt;Adding new rows to the table below with eval results, given new models and new system prompts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repository is NOT intended as a replacement for &lt;a href=&#34;https://github.com/openai/evals&#34;&gt;https://github.com/openai/evals&lt;/a&gt;, which is designed to be a comprehensive collection of a large number of evals.&lt;/p&gt; &#xA;&lt;h2&gt;Evals&lt;/h2&gt; &#xA;&lt;p&gt;This repository currently contains the following evals:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MMLU: Measuring Massive Multitask Language Understanding, reference: &lt;a href=&#34;https://arxiv.org/abs/2009.03300&#34;&gt;https://arxiv.org/abs/2009.03300&lt;/a&gt;, &lt;a href=&#34;https://github.com/hendrycks/test&#34;&gt;https://github.com/hendrycks/test&lt;/a&gt;, &lt;a href=&#34;https://github.com/hendrycks/test/raw/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MATH: Measuring Mathematical Problem Solving With the MATH Dataset, reference: &lt;a href=&#34;https://arxiv.org/abs/2103.03874&#34;&gt;https://arxiv.org/abs/2103.03874&lt;/a&gt;, &lt;a href=&#34;https://github.com/hendrycks/math&#34;&gt;https://github.com/hendrycks/math&lt;/a&gt;, &lt;a href=&#34;https://github.com/idavidrein/gpqa/raw/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GPQA: A Graduate-Level Google-Proof Q&amp;amp;A Benchmark, reference: &lt;a href=&#34;https://arxiv.org/abs/2311.12022&#34;&gt;https://arxiv.org/abs/2311.12022&lt;/a&gt;, &lt;a href=&#34;https://github.com/idavidrein/gpqa/&#34;&gt;https://github.com/idavidrein/gpqa/&lt;/a&gt;, &lt;a href=&#34;https://github.com/idavidrein/gpqa/raw/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs, reference: &lt;a href=&#34;https://arxiv.org/abs/1903.00161&#34;&gt;https://arxiv.org/abs/1903.00161&lt;/a&gt;, &lt;a href=&#34;https://allenai.org/data/drop&#34;&gt;https://allenai.org/data/drop&lt;/a&gt;, &lt;a href=&#34;https://github.com/allenai/allennlp-models/raw/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MGSM: Multilingual Grade School Math Benchmark (MGSM), Language Models are Multilingual Chain-of-Thought Reasoners, reference: &lt;a href=&#34;https://arxiv.org/abs/2210.03057&#34;&gt;https://arxiv.org/abs/2210.03057&lt;/a&gt;, &lt;a href=&#34;https://github.com/google-research/url-nlp&#34;&gt;https://github.com/google-research/url-nlp&lt;/a&gt;, &lt;a href=&#34;https://github.com/google-research/url-nlp/raw/main/LICENSE&#34;&gt;Creative Commons Attribution 4.0 International Public License (CC-BY)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;HumanEval: Evaluating Large Language Models Trained on Code, reference &lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;https://arxiv.org/abs/2107.03374&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/human-eval&#34;&gt;https://github.com/openai/human-eval&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/human-eval/raw/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Samplers&lt;/h2&gt; &#xA;&lt;p&gt;We have implemented sampling interfaces for the following language model APIs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI: &lt;a href=&#34;https://platform.openai.com/docs/overview&#34;&gt;https://platform.openai.com/docs/overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Claude: &lt;a href=&#34;https://www.anthropic.com/api&#34;&gt;https://www.anthropic.com/api&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure to set the &lt;code&gt;*_API_KEY&lt;/code&gt; environment variables before using these APIs.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Due to the optional dependencies, we&#39;re not providing a unified setup mechanism. Instead, we&#39;re providing instructions for each eval and sampler.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;a href=&#34;https://github.com/openai/human-eval/&#34;&gt;HumanEval&lt;/a&gt; (python programming)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/openai/human-eval&#xA;pip install -e human-eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the &lt;a href=&#34;https://pypi.org/project/openai/&#34;&gt;OpenAI API&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the &lt;a href=&#34;https://docs.anthropic.com/claude/docs/quickstart-guide&#34;&gt;Anthropic API&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install anthropic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m simple-evals.demo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will launch evaluations through the OpenAI API.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark Results&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Prompt&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DROP(f1)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPQA%&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MATH%&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MGSM%&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MMLU%&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;HumanEval%&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPT4s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;gpt-4-turbo-2024-04-09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;chatgpt[^1]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;gpt-4-turbo-2024-04-09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;assistant[^2]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;gpt-4-1106(-vision)-preview&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;chatgpt&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;gpt-4-1106(-vision)-preview&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;assistant&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;gpt-4-0125-preview&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;chatgpt&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;gpt-4-0125-preview&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;assistant&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;REFERENCE&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Claude-3-Opus (rerun w/ api)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;empty[^3]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Claude-3-Opus (rerun w/ api)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;lmsys[^4]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Claude-3-Opus (report[^5])&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;unknown&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Gemini-Ultra-1.0 (report[^6])&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;unknown&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;n/a&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Gemini-Pro-1.5 (report[^6])&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;unknown&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;n/a&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;[^1]:chatgpt system message: &#34;You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\nKnowledge cutoff: 2023-12\nCurrent date: 2024-04-01&#34; [^2]:assistant system message in &lt;a href=&#34;https://platform.openai.com/docs/api-reference/introduction&#34;&gt;OpenAI API doc&lt;/a&gt;: &#34;You are a helpful assistant.&#34; . [^3]:claude-3 empty system message: suggested by Anthropic API doc, and we have done limited experiments due to &lt;a href=&#34;https://docs.anthropic.com/claude/reference/rate-limits&#34;&gt;rate limit&lt;/a&gt; issues, but we welcome PRs with alternative choices. [^4]:claude-3 lmsys system message: system message in LMSYS &lt;a href=&#34;https://github.com/lm-sys/FastChat/raw/7899355ebe32117fdae83985cf8ee476d2f4243f/fastchat/conversation.py#L894&#34;&gt;Fast-chat open source code&lt;/a&gt;: &#34;The assistant is Claude, created by Anthropic. The current date is {{currentDateTime}}. Claude&#39;s knowledge base was last updated ... &#34;. We have done limited experiments due to &lt;a href=&#34;https://docs.anthropic.com/claude/reference/rate-limits&#34;&gt;rate limit&lt;/a&gt; issues, but we welcome PRs with alternative choices. [^5]:claude-3 reports: &lt;a href=&#34;https://www.anthropic.com/news/claude-3-family&#34;&gt;https://www.anthropic.com/news/claude-3-family&lt;/a&gt;. [^6]:gemini-1.5 reports: &lt;a href=&#34;https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/&#34;&gt;https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/&lt;/a&gt;, we dont have rerun results due to &lt;a href=&#34;https://ai.google.dev/pricing&#34;&gt;rate_limit&lt;/a&gt; issues and paid-as-you-go version are still &#34;coming soon&#34; by the time of this study on 04/02.&lt;/p&gt; &#xA;&lt;h2&gt;Legal Stuff&lt;/h2&gt; &#xA;&lt;p&gt;By contributing to evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI evals will be subject to our usual Usage Policies: &lt;a href=&#34;https://platform.openai.com/docs/usage-policies&#34;&gt;https://platform.openai.com/docs/usage-policies&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenBMB/MiniCPM-V</title>
    <updated>2024-04-15T02:57:51Z</updated>
    <id>tag:github.com,2024-04-15:/OpenBMB/MiniCPM-V</id>
    <link href="https://github.com/OpenBMB/MiniCPM-V" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MiniCPM-V 2.0: An Efficient End-side MLLM with Strong OCR and Understanding Capabilities&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;!-- &lt;h1 style=&#34;color: #33A6B8; font-family: Helvetica&#34;&gt; OmniLMM &lt;/h1&gt; --&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv-omnilmm.png&#34; width=&#34;400em&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;性能领先且部署高效的多模态大模型&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;中文 | &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README_en.md&#34;&gt;English&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; MiniCPM-V 2.0 &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2/&#34;&gt;🤗&lt;/a&gt; &lt;a href=&#34;http://120.92.209.146:80/&#34;&gt;🤖&lt;/a&gt; | OmniLMM-12B &lt;a href=&#34;https://huggingface.co/openbmb/OmniLMM-12B/&#34;&gt;🤗&lt;/a&gt; &lt;a href=&#34;http://120.92.209.146:8081&#34;&gt;🤖&lt;/a&gt; | &lt;a href=&#34;https://openbmb.vercel.app/minicpm-v-2&#34;&gt;MiniCPM-V 2.0 技术博客&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-V&lt;/strong&gt;和&lt;strong&gt;OmniLMM&lt;/strong&gt; 是面向图文理解的开源多模态大模型系列。该系列模型接受图像和文本输入，并提供高质量的文本输出。我们发布了两个版本的模型，旨在实现&lt;strong&gt;领先的性能和高效的部署&lt;/strong&gt;：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 2.8B&lt;/strong&gt;：可在终端设备上部署的先进多模态大模型。最新发布的 MiniCPM-V 2.0 可以接受 180 万像素的任意长宽比图像输入，实现了和 Gemini Pro 相近的场景文字识别能力以及和 GPT-4V 相匹的低幻觉率。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OmniLMM-12B&lt;/strong&gt;：相比同规模其他模型在多个基准测试中具有领先性能，实现了相比 GPT-4V 更低的幻觉率。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;更新日志 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024.04.12] 我们开源了 MiniCPM-V 2.0，该模型刷新了 OCRBench 开源模型最佳成绩，在场景文字识别能力上比肩 Gemini Pro，同时还在综合了 11 个主流多模态大模型评测基准的 &lt;a href=&#34;https://rank.opencompass.org.cn/leaderboard-multimodal&#34;&gt;OpenCompass&lt;/a&gt; 榜单上超过了 Qwen-VL-Chat 10B、CogVLM-Chat 17B 和 Yi-VL 34B 等更大参数规模的模型！点击&lt;a href=&#34;https://openbmb.vercel.app/minicpm-v-2&#34;&gt;这里&lt;/a&gt;查看 MiniCPM-V 2.0 技术博客。&lt;/li&gt; &#xA; &lt;li&gt;[2024.03.14] MiniCPM-V 现在支持 SWIFT 框架下的&lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;微调&lt;/a&gt;了，感谢 &lt;a href=&#34;https://github.com/Jintao-Huang&#34;&gt;Jintao&lt;/a&gt; 的贡献！&lt;/li&gt; &#xA; &lt;li&gt;[2024.03.01] MiniCPM-V 现在支持在 Mac 电脑上进行部署！&lt;/li&gt; &#xA; &lt;li&gt;[2024.02.01] 我们开源了 MiniCPM-V 和 OmniLMM-12B，分别可以支持高效的端侧部署和同规模领先的多模态能力！&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;目录 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v-28b&#34;&gt;MiniCPM-V 2.8B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#omnilmm-12b&#34;&gt;OmniLMM-12B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E5%AE%89%E8%A3%85&#34;&gt;安装&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%8E%A8%E7%90%86&#34;&gt;推理&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%A8%A1%E5%9E%8B%E5%BA%93&#34;&gt;模型库&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D&#34;&gt;多轮对话&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#mac-%E6%8E%A8%E7%90%86&#34;&gt;Mac 推理&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%89%8B%E6%9C%BA%E7%AB%AF%E9%83%A8%E7%BD%B2&#34;&gt;手机端部署&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%9C%AA%E6%9D%A5%E8%AE%A1%E5%88%92&#34;&gt;未来计划&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;h2&gt;MiniCPM-V 2.8B&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-V 2.8B&lt;/strong&gt;可以高效部署到终端设备。该模型基于 SigLip-400M 和 &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/&#34;&gt;MiniCPM-2.4B&lt;/a&gt;构建，通过perceiver resampler连接。最新发布的 MiniCPM-V 2.0 的特点包括：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;优秀的性能。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 在多个测试基准（如 OCRBench, TextVQA, MME, MMB, MathVista 等）中实现了 7B 以下模型的&lt;strong&gt;最佳性能&lt;/strong&gt;。&lt;strong&gt;在综合了 11 个主流多模态大模型评测基准的 OpenCompass 榜单上超过了 Qwen-VL-Chat 9.6B、CogVLM-Chat 17.4B 和 Yi-VL 34B 等更大参数规模的模型&lt;/strong&gt;。MiniCPM-V 2.0 还展现出&lt;strong&gt;领先的 OCR 能力&lt;/strong&gt;，在场景文字识别能力上&lt;strong&gt;接近 Gemini Pro&lt;/strong&gt;，OCRBench 得分达到&lt;strong&gt;开源模型第一&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🏆 &lt;strong&gt;可信行为。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;多模态大模型深受幻觉问题困扰，模型经常生成和图像中的事实不符的文本。MiniCPM-V 2.0 是 &lt;strong&gt;第一个通过多模态 RLHF 对齐的端侧多模态大模型&lt;/strong&gt;（借助 &lt;a href=&#34;https://rlhf-v.github.io/&#34;&gt;RLHF-V&lt;/a&gt; [CVPR&#39;24] 系列技术）。该模型在 &lt;a href=&#34;https://arxiv.org/abs/2312.00849&#34;&gt;Object HalBench&lt;/a&gt; 达到&lt;strong&gt;和 GPT-4V 相仿&lt;/strong&gt;的性能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🌟 &lt;strong&gt;高清图像高效编码。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 可以接受 &lt;strong&gt;180 万像素的任意长宽比图像输入&lt;/strong&gt;（基于最新的&lt;a href=&#34;https://arxiv.org/pdf/2403.11703.pdf&#34;&gt;LLaVA-UHD&lt;/a&gt; 技术），这使得模型可以感知到小物体、密集文字等更加细粒度的视觉信息。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;⚡️ &lt;strong&gt;高效部署。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 可以&lt;strong&gt;高效部署在大多数消费级显卡和个人电脑上&lt;/strong&gt;，包括&lt;strong&gt;移动手机等终端设备&lt;/strong&gt;。在视觉编码方面，我们通过perceiver resampler将图像表示压缩为更少的 token。这使得 MiniCPM-V 2.0 即便是&lt;strong&gt;面对高分辨率图像，也能占用较低的存储并展现优秀的推理速度&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🙌 &lt;strong&gt;双语支持。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 &lt;strong&gt;提供领先的中英双语多模态能力支持&lt;/strong&gt;。 该能力通过 &lt;a href=&#34;https://arxiv.org/abs/2308.12038&#34;&gt;VisCPM&lt;/a&gt; [ICLR&#39;24] 论文中提出的多模态能力的跨语言泛化技术实现。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;性能评估 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv-2-peformance.png&#34; width=&#34;66%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TextVQA, DocVQA, OCRBench, OpenCompass, MME, MMBench, MMMU, MathVista, LLaVA Bench, Object HalBench 上的详细评测结果。 &lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;Size&lt;/th&gt; &#xA;     &lt;th&gt;TextVQA val&lt;/th&gt; &#xA;     &lt;th&gt;DocVQA test&lt;/th&gt; &#xA;     &lt;th&gt;OCRBench&lt;/th&gt; &#xA;     &lt;th&gt;OpenCompass&lt;/th&gt; &#xA;     &lt;th nowrap&gt;MME&lt;/th&gt; &#xA;     &lt;th&gt;MMB dev(en)&lt;/th&gt; &#xA;     &lt;th&gt;MMB dev(zh)&lt;/th&gt; &#xA;     &lt;th&gt;MMMU val&lt;/th&gt; &#xA;     &lt;th&gt;MathVista&lt;/th&gt; &#xA;     &lt;th&gt;LLaVA Bench&lt;/th&gt; &#xA;     &lt;th nowrap&gt;Object HalBench&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody align=&#34;center&#34;&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;12&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary models&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Gemini Pro Vision&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;74.6&lt;/td&gt; &#xA;     &lt;td&gt;88.1&lt;/td&gt; &#xA;     &lt;td&gt;680&lt;/td&gt; &#xA;     &lt;td&gt;63.8&lt;/td&gt; &#xA;     &lt;td&gt;2148.9&lt;/td&gt; &#xA;     &lt;td&gt;75.2&lt;/td&gt; &#xA;     &lt;td&gt;74.0&lt;/td&gt; &#xA;     &lt;td&gt;48.9&lt;/td&gt; &#xA;     &lt;td&gt;45.8&lt;/td&gt; &#xA;     &lt;td&gt;79.9&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT-4V&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;78.0&lt;/td&gt; &#xA;     &lt;td&gt;88.4&lt;/td&gt; &#xA;     &lt;td&gt;645&lt;/td&gt; &#xA;     &lt;td&gt;63.2&lt;/td&gt; &#xA;     &lt;td&gt;1771.5&lt;/td&gt; &#xA;     &lt;td&gt;75.1&lt;/td&gt; &#xA;     &lt;td&gt;75.0&lt;/td&gt; &#xA;     &lt;td&gt;53.8&lt;/td&gt; &#xA;     &lt;td&gt;47.8&lt;/td&gt; &#xA;     &lt;td&gt;93.1&lt;/td&gt; &#xA;     &lt;td&gt;86.4 / 92.7&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;12&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source models 6B~34B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Yi-VL-6B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;6.7B&lt;/td&gt; &#xA;     &lt;td&gt;45.5*&lt;/td&gt; &#xA;     &lt;td&gt;17.1*&lt;/td&gt; &#xA;     &lt;td&gt;290&lt;/td&gt; &#xA;     &lt;td&gt;49.3&lt;/td&gt; &#xA;     &lt;td&gt;1915.1 &lt;/td&gt; &#xA;     &lt;td&gt;68.6 &lt;/td&gt; &#xA;     &lt;td&gt;68.3 &lt;/td&gt; &#xA;     &lt;td&gt;40.3 &lt;/td&gt; &#xA;     &lt;td&gt;28.8 &lt;/td&gt; &#xA;     &lt;td&gt;51.9 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;9.6B&lt;/td&gt; &#xA;     &lt;td&gt;61.5&lt;/td&gt; &#xA;     &lt;td&gt;62.6&lt;/td&gt; &#xA;     &lt;td&gt;488 &lt;/td&gt; &#xA;     &lt;td&gt;52.1 &lt;/td&gt; &#xA;     &lt;td&gt;1860.0 &lt;/td&gt; &#xA;     &lt;td&gt;60.6 &lt;/td&gt; &#xA;     &lt;td&gt;56.7 &lt;/td&gt; &#xA;     &lt;td&gt;37.0 &lt;/td&gt; &#xA;     &lt;td&gt;33.8 &lt;/td&gt; &#xA;     &lt;td&gt;67.7 &lt;/td&gt; &#xA;     &lt;td&gt;56.2 / 80.0&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Yi-VL-34B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;34B&lt;/td&gt; &#xA;     &lt;td&gt;43.4*&lt;/td&gt; &#xA;     &lt;td&gt;16.9*&lt;/td&gt; &#xA;     &lt;td&gt;290&lt;/td&gt; &#xA;     &lt;td&gt;52.6 &lt;/td&gt; &#xA;     &lt;td&gt;2050.2&lt;/td&gt; &#xA;     &lt;td&gt;71.1&lt;/td&gt; &#xA;     &lt;td&gt;71.4&lt;/td&gt; &#xA;     &lt;td&gt;45.1&lt;/td&gt; &#xA;     &lt;td&gt;30.7&lt;/td&gt; &#xA;     &lt;td&gt;62.3&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;DeepSeek-VL-7B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;7.3B&lt;/td&gt; &#xA;     &lt;td&gt;64.7*&lt;/td&gt; &#xA;     &lt;td&gt;47.0* &lt;/td&gt; &#xA;     &lt;td&gt;435&lt;/td&gt; &#xA;     &lt;td&gt;55.6 &lt;/td&gt; &#xA;     &lt;td&gt;1765.4 &lt;/td&gt; &#xA;     &lt;td&gt;74.1 &lt;/td&gt; &#xA;     &lt;td&gt;72.8 &lt;/td&gt; &#xA;     &lt;td&gt;38.3 &lt;/td&gt; &#xA;     &lt;td&gt;36.8&lt;/td&gt; &#xA;     &lt;td&gt;77.8 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;TextMonkey&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;9.7B&lt;/td&gt; &#xA;     &lt;td&gt;64.3&lt;/td&gt; &#xA;     &lt;td&gt;66.7 &lt;/td&gt; &#xA;     &lt;td&gt;558&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;CogVLM-Chat&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;17.4B&lt;/td&gt; &#xA;     &lt;td&gt;70.4&lt;/td&gt; &#xA;     &lt;td&gt;33.3*&lt;/td&gt; &#xA;     &lt;td&gt;590 &lt;/td&gt; &#xA;     &lt;td&gt;52.5 &lt;/td&gt; &#xA;     &lt;td&gt;1736.6 &lt;/td&gt; &#xA;     &lt;td&gt;63.7 &lt;/td&gt; &#xA;     &lt;td&gt;53.8 &lt;/td&gt; &#xA;     &lt;td&gt;37.3 &lt;/td&gt; &#xA;     &lt;td&gt;34.7 &lt;/td&gt; &#xA;     &lt;td&gt;73.9 &lt;/td&gt; &#xA;     &lt;td&gt;73.6 / 87.4 &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;12&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source models 1B~3B &lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;DeepSeek-VL-1.3B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;1.7B&lt;/td&gt; &#xA;     &lt;td&gt;58.4*&lt;/td&gt; &#xA;     &lt;td&gt;37.9*&lt;/td&gt; &#xA;     &lt;td&gt;413&lt;/td&gt; &#xA;     &lt;td&gt;46.0 &lt;/td&gt; &#xA;     &lt;td&gt;1531.6 &lt;/td&gt; &#xA;     &lt;td&gt;64.0 &lt;/td&gt; &#xA;     &lt;td&gt;61.2 &lt;/td&gt; &#xA;     &lt;td&gt;33.8 &lt;/td&gt; &#xA;     &lt;td&gt;29.4 &lt;/td&gt; &#xA;     &lt;td&gt;51.1 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MobileVLM V2&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;3.1B&lt;/td&gt; &#xA;     &lt;td&gt;57.5&lt;/td&gt; &#xA;     &lt;td&gt;19.4*&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1440.5(P) &lt;/td&gt; &#xA;     &lt;td&gt;63.2 &lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Mini-Gemini&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;2.2B&lt;/td&gt; &#xA;     &lt;td&gt;56.2&lt;/td&gt; &#xA;     &lt;td&gt;34.2*&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1653.0 &lt;/td&gt; &#xA;     &lt;td&gt;59.8 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;31.7 &lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;2.8B &lt;/td&gt; &#xA;     &lt;td&gt;60.6&lt;/td&gt; &#xA;     &lt;td&gt;38.2 &lt;/td&gt; &#xA;     &lt;td&gt;366&lt;/td&gt; &#xA;     &lt;td&gt;47.6&lt;/td&gt; &#xA;     &lt;td&gt;1650.2 &lt;/td&gt; &#xA;     &lt;td&gt;67.9 &lt;/td&gt; &#xA;     &lt;td&gt;65.3 &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;38.3&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;28.9&lt;/td&gt; &#xA;     &lt;td&gt;51.3 &lt;/td&gt; &#xA;     &lt;td&gt;78.4 / 88.5 &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;&lt;strong&gt;MiniCPM-V 2.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;2.8B &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;74.1&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;71.9&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;605&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;1808.6&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;69.6&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;68.1&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;38.2 &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;38.7&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;69.2&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;85.5 / 92.2 &lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt; * 我们自己评测了正式开源的模型权重。 &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;典型示例 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv2-cases_2.png&#34; width=&#34;95%/&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;p&gt;我们将 MiniCPM-V 2.0 部署在小米 14 Pro 上，并录制了以下演示视频，未经任何视频剪辑。&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/station.gif&#34; width=&#34;36%/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/london_car.gif&#34; width=&#34;36%/&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;MiniCPM-V 1.0 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;请参考&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/minicpm_v1.md&#34;&gt;这里&lt;/a&gt;了解 MiniCPM-V 1.0 的信息和使用教程。&lt;/p&gt; &#xA;&lt;h2&gt;OmniLMM-12B&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;OmniLMM-12B&lt;/strong&gt; 是当前系列中性能最佳的版本。该模型基于EVA02-5B和Zephyr-7B-β初始化构建，并使用perceiver resampler连接，采用了课程学习的方法在多模态数据上进行训练。该模型具有三个特点：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;性能领先。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;OmniLMM-12B 相比其他同规模模型在多个基准测试中取得&lt;strong&gt;领先的性能&lt;/strong&gt;（包括 MME、MMBench、SEED-Bench 等），模型掌握了较为丰富的多模态世界知识。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🏆 &lt;strong&gt;行为可信。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;多模态大模型的幻觉问题备受关注，模型经常生成和图像中的事实不符的文本（例如，确信地描述图片中并不存在的物体）。OmniLMM-12B是 &lt;strong&gt;第一个通过多模态 RLHF 对齐的综合能力优秀的开源多模态大模型&lt;/strong&gt;（借助 &lt;a href=&#34;https://rlhf-v.github.io/&#34;&gt;RLHF-V&lt;/a&gt; [CVPR&#39;24] 系列技术）。该模型在 &lt;a href=&#34;https://huggingface.co/datasets/Shengcao1006/MMHal-Bench&#34;&gt;MMHal-Bench&lt;/a&gt; 幻觉评测基准上达到&lt;strong&gt;开源模型最佳水平&lt;/strong&gt;，并在 &lt;a href=&#34;https://arxiv.org/abs/2312.00849&#34;&gt;Object HalBench&lt;/a&gt; 中&lt;strong&gt;优于GPT-4V&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🕹 &lt;strong&gt;实时多模态交互。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;我们尝试结合OmniLMM-12B和GPT-3.5 (纯文本模型) ，实现&lt;strong&gt;实时多模态交互助手&lt;/strong&gt;。该模型接受来自摄像头的视频流，并借助工具处理语音输入输出。虽然还很初步，我们发现该模型无需视频编辑可以&lt;strong&gt;复现Gemini演示视频中的一些有趣例子&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;评测结果 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar_omnilmm12b.png&#34; width=&#34;66%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; MME, MMBench, MMMU, MMBench, MMHal-Bench, Object HalBench, SeedBench, LLaVA Bench W, MathVista 上的详细评测结果。 &lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;MME&lt;/th&gt; &#xA;    &lt;th nowrap&gt;MMB dev (en)&lt;/th&gt; &#xA;    &lt;th nowrap&gt;MMMU val&lt;/th&gt; &#xA;    &lt;th nowrap&gt;MMHal-Bench&lt;/th&gt; &#xA;    &lt;th nowrap&gt;Object HalBench&lt;/th&gt; &#xA;    &lt;th nowrap&gt;SeedBench-I&lt;/th&gt; &#xA;    &lt;th&gt;MathVista&lt;/th&gt; &#xA;    &lt;th nowrap&gt;LLaVA Bench&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody align=&#34;center&#34;&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;GPT-4V†&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;1771.5&lt;/td&gt; &#xA;    &lt;td&gt;75.1 &lt;/td&gt; &#xA;    &lt;td&gt;56.8&lt;/td&gt; &#xA;    &lt;td&gt;3.53 / 70.8&lt;/td&gt; &#xA;    &lt;td&gt;86.4 / 92.7&lt;/td&gt; &#xA;    &lt;td&gt;71.6 &lt;/td&gt; &#xA;    &lt;td&gt;47.8 &lt;/td&gt; &#xA;    &lt;td&gt;93.1 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Plus†&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;2183.4&lt;/td&gt; &#xA;    &lt;td&gt;66.2 &lt;/td&gt; &#xA;    &lt;td&gt;45.2&lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;65.7 &lt;/td&gt; &#xA;    &lt;td&gt;36.0 &lt;/td&gt; &#xA;    &lt;td&gt;73.7 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Yi-VL 6B&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;6.7B &lt;/td&gt; &#xA;    &lt;td&gt;1915.1 &lt;/td&gt; &#xA;    &lt;td&gt;68.6 &lt;/td&gt; &#xA;    &lt;td&gt;40.3 &lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;67.5 &lt;/td&gt; &#xA;    &lt;td&gt;28.8 &lt;/td&gt; &#xA;    &lt;td&gt;51.9 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;9.6B&lt;/td&gt; &#xA;    &lt;td&gt;1860.0&lt;/td&gt; &#xA;    &lt;td&gt;60.6 &lt;/td&gt; &#xA;    &lt;td&gt;35.9&lt;/td&gt; &#xA;    &lt;td&gt;2.93 / 59.4&lt;/td&gt; &#xA;    &lt;td&gt;56.2 / 80.0&lt;/td&gt; &#xA;    &lt;td&gt;64.8 &lt;/td&gt; &#xA;    &lt;td&gt;33.8 &lt;/td&gt; &#xA;    &lt;td&gt;67.7 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;CogVLM-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;17.4B&lt;/td&gt; &#xA;    &lt;td&gt;1736.6&lt;/td&gt; &#xA;    &lt;td&gt;63.7 &lt;/td&gt; &#xA;    &lt;td&gt;32.1 &lt;/td&gt; &#xA;    &lt;td&gt;2.68 / 52.1 &lt;/td&gt; &#xA;    &lt;td&gt;73.6 / 87.4 &lt;/td&gt; &#xA;    &lt;td&gt;68.8 &lt;/td&gt; &#xA;    &lt;td&gt;34.7 &lt;/td&gt; &#xA;    &lt;td&gt;73.9 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;LLaVA 1.5&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;13.6B &lt;/td&gt; &#xA;    &lt;td&gt;1808.4 &lt;/td&gt; &#xA;    &lt;td&gt;68.2 &lt;/td&gt; &#xA;    &lt;td&gt;36.4 &lt;/td&gt; &#xA;    &lt;td&gt;2.71 / 51.0 &lt;/td&gt; &#xA;    &lt;td&gt;53.7 / 77.4 &lt;/td&gt; &#xA;    &lt;td&gt;68.1 &lt;/td&gt; &#xA;    &lt;td&gt;26.4 &lt;/td&gt; &#xA;    &lt;td&gt;64.6 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;&lt;b&gt;OmniLMM-12B&lt;/b&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;11.6B &lt;/td&gt; &#xA;    &lt;td&gt;1935.8 &lt;/td&gt; &#xA;    &lt;td&gt;71.6 &lt;/td&gt; &#xA;    &lt;td&gt;40.7 &lt;/td&gt; &#xA;    &lt;td&gt;3.45 / 68.8 &lt;/td&gt; &#xA;    &lt;td&gt;90.3 / 95.5 &lt;/td&gt; &#xA;    &lt;td&gt;71.1 &lt;/td&gt; &#xA;    &lt;td&gt;34.9 &lt;/td&gt; &#xA;    &lt;td&gt;72.0 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;small&gt;†: 闭源模型&lt;/small&gt; &#xA; &lt;br&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;典型示例 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/omnilmm-12b-examples_2.png&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;p&gt;我们结合 OmniLMM-12B 和 ChatGPT-3.5 (纯文本模型) 尝试构建 &lt;strong&gt;实时多模态交互助手&lt;/strong&gt;. OmniLMM-12B 将视频帧转为对应的图像描述并输入给ChatGPT-3.5来生成对用户指令的响应。演示视频未经编辑。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video controls src=&#34;https://github.com/OpenBMB/OmniLMM/assets/157115220/8fec13bf-bb47-4bf8-8f8c-d0b716a964ec&#34; type=&#34;video/mp4&#34; width=&#34;80%/&#34;&gt; &#xA; &lt;/video&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;欢迎通过以下链接使用我们的网页端推理服务： &lt;a href=&#34;http://120.92.209.146:8081&#34;&gt;OmniLMM-12B&lt;/a&gt; ｜ &lt;a href=&#34;http://120.92.209.146:80&#34;&gt;MiniCPM-V 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;克隆我们的仓库并跳转到相应目录&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/OpenBMB/MiniCPM-V.git&#xA;cd MiniCPM-V&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;创建 conda 环境&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n MiniCPMV python=3.10 -y&#xA;conda activate MiniCPMV&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;安装依赖&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;推理&lt;/h2&gt; &#xA;&lt;h3&gt;模型库&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;模型&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;简介&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;下载链接&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;最新版本，提供高效而领先的端侧双语多模态理解能力。&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V-2/files&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;第一版 MiniCPM-V&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V/files&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OmniLMM-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;性能最强的版本&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/OmniLMM-12B&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/OmniLMM-12B/files&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;多轮对话&lt;/h3&gt; &#xA;&lt;p&gt;请参考以下代码使用 &lt;code&gt;MiniCPM-V&lt;/code&gt; 和 &lt;code&gt;OmniLMM&lt;/code&gt; 进行推理。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/hk_OCR.jpg&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from chat import OmniLMMChat, img2base64&#xA;&#xA;chat_model = OmniLMMChat(&#39;openbmb/MiniCPM-V-2&#39;) # or &#39;openbmb/OmniLMM-12B&#39;&#xA;&#xA;im_64 = img2base64(&#39;./assets/hk_OCR.jpg&#39;)&#xA;&#xA;# First round chat &#xA;msgs = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Where should I go to buy a camera?&#34;}]&#xA;&#xA;inputs = {&#34;image&#34;: im_64, &#34;question&#34;: json.dumps(msgs)}&#xA;answer = chat_model.chat(inputs)&#xA;print(answer)&#xA;&#xA;# Second round chat &#xA;# pass history context of multi-turn conversation&#xA;msgs.append({&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: answer})&#xA;msgs.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Where is this store in the image?&#34;})&#xA;&#xA;inputs = {&#34;image&#34;: im_64, &#34;question&#34;: json.dumps(msgs)}&#xA;answer = chat_model.chat(inputs)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;可以得到以下输出:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;You should go to the Canon store for a camera.&#34;&#xA;&#xA;&#34;The Canon store is located on the right side of the image.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mac 推理&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;点击查看 MiniCPM-V 2.0 基于Mac MPS运行 (Apple silicon or AMD GPUs)的示例。 &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# test.py&#xA;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-V-2&#39;, trust_remote_code=True, torch_dtype=torch.bfloat16)&#xA;model = model.to(device=&#39;mps&#39;, dtype=torch.float16)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-V-2&#39;, trust_remote_code=True)&#xA;model.eval()&#xA;&#xA;image = Image.open(&#39;./assets/hk_OCR.jpg&#39;).convert(&#39;RGB&#39;)&#xA;question = &#39;Where is this photo taken?&#39;&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: question}]&#xA;&#xA;answer, context, _ = model.chat(&#xA;    image=image,&#xA;    msgs=msgs,&#xA;    context=None,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;运行:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;手机端部署&lt;/h3&gt; &#xA;&lt;p&gt;MiniCPM-V 2.0 目前可以部署在Android和Harmony操作系统的手机上。 🚀 点击&lt;a href=&#34;https://github.com/OpenBMB/mlc-MiniCPM&#34;&gt;这里&lt;/a&gt;开始手机端部署。&lt;/p&gt; &#xA;&lt;h2&gt;未来计划&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 支持模型微调&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 本地用户图形界面部署&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 实时多模态交互代码开源&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;模型协议 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;本仓库中代码依照 Apache-2.0 协议开源&lt;/p&gt; &#xA;&lt;p&gt;OmniLMM 模型权重的使用遵循 “&lt;a href=&#34;https://github.com/OpenBMB/General-Model-License/raw/main/%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE-%E6%9D%A5%E6%BA%90%E8%AF%B4%E6%98%8E-%E5%AE%A3%E4%BC%A0%E9%99%90%E5%88%B6-%E5%95%86%E4%B8%9A%E6%8E%88%E6%9D%83.md&#34;&gt;通用模型许可协议-来源说明-宣传限制-商业授权&lt;/a&gt;”。&lt;/p&gt; &#xA;&lt;p&gt;OmniLMM 模型权重对学术研究完全开放。&lt;/p&gt; &#xA;&lt;p&gt;如需将模型用于商业用途，请联系 &lt;a href=&#34;mailto:cpm@modelbest.cn&#34;&gt;cpm@modelbest.cn&lt;/a&gt; 来获取书面授权，登记后可以免费商业使用。&lt;/p&gt; &#xA;&lt;h2&gt;声明 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;作为多模态大模型，MiniCPM-V 和 OmniLMM 通过学习大量的多模态数据来生成内容，但它无法理解、表达个人观点或价值判断，它所输出的任何内容都不代表模型开发者的观点和立场。&lt;/p&gt; &#xA;&lt;p&gt;因此用户在使用 MiniCPM-V 和 OmniLMM 生成的内容时，应自行负责对其进行评估和验证。如果由于使用 OmniLMM 开源模型而导致的任何问题，包括但不限于数据安全问题、公共舆论风险，或模型被误导、滥用、传播或不当利用所带来的任何风险和问题，我们将不承担任何责任。&lt;/p&gt; &#xA;&lt;h2&gt;机构 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;本项目由以下机构共同开发：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://nlp.csai.tsinghua.edu.cn/&#34;&gt;清华大学自然语言处理实验室&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://modelbest.cn/&#34;&gt;面壁智能&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/zhihu.webp&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://www.zhihu.com/&#34;&gt;知乎&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>