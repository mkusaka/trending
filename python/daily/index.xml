<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-03T01:40:03Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pytorch-labs/gpt-fast</title>
    <updated>2023-12-03T01:40:03Z</updated>
    <id>tag:github.com,2023-12-03:/pytorch-labs/gpt-fast</id>
    <link href="https://github.com/pytorch-labs/gpt-fast" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple and efficient pytorch-native transformer text generation in &lt;1000 LOC of python.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-fast&lt;/h1&gt; &#xA;&lt;p&gt;Simple and efficient pytorch-native transformer text generation.&lt;/p&gt; &#xA;&lt;p&gt;Featuring:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Very low latency&lt;/li&gt; &#xA; &lt;li&gt;&amp;lt;1000 lines of python&lt;/li&gt; &#xA; &lt;li&gt;No dependencies other than PyTorch and sentencepiece&lt;/li&gt; &#xA; &lt;li&gt;int8/int4 quantization&lt;/li&gt; &#xA; &lt;li&gt;Speculative decoding&lt;/li&gt; &#xA; &lt;li&gt;Tensor parallelism&lt;/li&gt; &#xA; &lt;li&gt;Supports Nvidia and AMD GPUs&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This is &lt;em&gt;NOT&lt;/em&gt; intended to be a &#34;framework&#34; or &#34;library&#34; - it is intended to show off what kind of performance you can get with native PyTorch :) Please copy-paste and fork as you desire.&lt;/p&gt; &#xA;&lt;p&gt;For an in-depth walkthrough of what&#39;s in this codebase, see this &lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai-2/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;Download PyTorch nightly&lt;/a&gt; Install sentencepiece and huggingface_hub&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install sentencepiece huggingface_hub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download llama models, go to &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b&#34;&gt;https://huggingface.co/meta-llama/Llama-2-7b&lt;/a&gt; and go through steps to obtain access. Then login with &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Downloading Weights&lt;/h2&gt; &#xA;&lt;p&gt;Models tested/supported&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;openlm-research/open_llama_7b&#xA;meta-llama/Llama-2-7b-chat-hf&#xA;meta-llama/Llama-2-13b-chat-hf&#xA;meta-llama/Llama-2-70b-chat-hf&#xA;codellama/CodeLlama-7b-Python-hf&#xA;codellama/CodeLlama-34b-Python-hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, to convert Llama-2-7b-chat-hf&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export MODEL_REPO=meta-llama/Llama-2-7b-chat-hf&#xA;./scripts/prepare.sh $MODEL_REPO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;Benchmarks run on an A100-80GB, power limited to 330W.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Technique&lt;/th&gt; &#xA;   &lt;th&gt;Tokens/Second&lt;/th&gt; &#xA;   &lt;th&gt;Memory Bandwidth (GB/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;104.9&lt;/td&gt; &#xA;   &lt;td&gt;1397.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8-bit&lt;/td&gt; &#xA;   &lt;td&gt;155.58&lt;/td&gt; &#xA;   &lt;td&gt;1069.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4-bit (G=32)&lt;/td&gt; &#xA;   &lt;td&gt;196.80&lt;/td&gt; &#xA;   &lt;td&gt;862.69&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-70B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8-bit&lt;/td&gt; &#xA;   &lt;td&gt;19.13&lt;/td&gt; &#xA;   &lt;td&gt;1322.58&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4-bit (G=32)&lt;/td&gt; &#xA;   &lt;td&gt;25.25&lt;/td&gt; &#xA;   &lt;td&gt;1097.66&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Speculative Sampling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch-labs/gpt-fast/main/scripts/speculate_70B_int4.sh&#34;&gt;Verifier: Llama-70B (int4), Draft: Llama-7B (int4)&lt;/a&gt;: 48.4 tok/s&lt;/p&gt; &#xA;&lt;h3&gt;Tensor Parallelism&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Number of GPUs&lt;/th&gt; &#xA;   &lt;th&gt;Tokens/Second&lt;/th&gt; &#xA;   &lt;th&gt;Memory Bandwidth (GB/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;104.9&lt;/td&gt; &#xA;   &lt;td&gt;1397.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;136.27&lt;/td&gt; &#xA;   &lt;td&gt;954.01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;168.78&lt;/td&gt; &#xA;   &lt;td&gt;635.09&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;179.27&lt;/td&gt; &#xA;   &lt;td&gt;395.85&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-70B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;20.53&lt;/td&gt; &#xA;   &lt;td&gt;1426.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;34.15&lt;/td&gt; &#xA;   &lt;td&gt;1204.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;47.25&lt;/td&gt; &#xA;   &lt;td&gt;858.28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;AMD&lt;/h3&gt; &#xA;&lt;p&gt;Benchmarks run on one GCD of a MI-250x.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Technique&lt;/th&gt; &#xA;   &lt;th&gt;Tokens/Second&lt;/th&gt; &#xA;   &lt;th&gt;Memory Bandwidth (GB/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;76.33&lt;/td&gt; &#xA;   &lt;td&gt;1028.70&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8-bit&lt;/td&gt; &#xA;   &lt;td&gt;101.86&lt;/td&gt; &#xA;   &lt;td&gt;700.06&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Generate Text&lt;/h2&gt; &#xA;&lt;p&gt;Model definition in &lt;code&gt;model.py&lt;/code&gt;, generation code in &lt;code&gt;generate.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model.pth --prompt &#34;Hello, my name is&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To squeeze out a little bit more performance, you can also compile the prefill with &lt;code&gt;--compile_prefill&lt;/code&gt;. This will increase compilation times though.&lt;/p&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;h3&gt;Int8 Weight-Only Quantization&lt;/h3&gt; &#xA;&lt;p&gt;To generate this version of the model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Spits out model at checkpoints/$MODEL_REPO/model_int8.pth&#xA;python quantize.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --mode int8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run with int8, just pass the int8 checkpoint to generate.py.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model_int8.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Int4 Weight-Only Quantization&lt;/h3&gt; &#xA;&lt;p&gt;To generate int4 version of model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Spits out model at checkpoints/$MODEL_REPO/model_int4.g32.pth&#xA;python quantize.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --mode int4 --groupsize 32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run with int4, just pass the int4 checkpoint to generate.py.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --checkpoint_path checkpoints/$MODEL_REPO/model_int4.g32.pth --compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Speculative Sampling&lt;/h2&gt; &#xA;&lt;p&gt;To generate with speculative sampling (DRAFT_MODEL_REPO should point to a smaller model compared with MODEL_REPO).&lt;/p&gt; &#xA;&lt;p&gt;In this example, the &#34;smaller&#34; model is just the int8 quantized version of the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export DRAFT_MODEL_REPO=meta-llama/Llama-2-7b-chat-hf&#xA;python generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model.pth --draft_checkpoint_path checkpoints/$DRAFT_MODEL_REPO/model_int8.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Running on an A100 80GB, albeit power-limited to 330 watts. Empirically, seems like peak bandwidth is about 1700 GB/s.&lt;/p&gt; &#xA;&lt;h2&gt;Tensor Parallelism&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --standalone --nproc_per_node=2 generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Experimental&lt;/h2&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;We use the EleutherAI evaluation harness to evaluate our model accuracy. To evaluate the accuracy, make sure the evaluation harness is installed and pass your model checkpoint and desired tasks to eval.py.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python eval.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --compile --tasks hellaswag winogrande&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Generative tasks are currently not supported for gpt-fast&lt;/p&gt; &#xA;&lt;p&gt;Installation Instructions for the evaluation harness: &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness/tree/master#install&#34;&gt;https://github.com/EleutherAI/lm-evaluation-harness/tree/master#install&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;GPTQ&lt;/h3&gt; &#xA;&lt;p&gt;We have a pure pytorch implementation of GPTQ that utilizes torch._dynamo.export to access the model structure. You can generate a GPTQ quantized version of int4 quantization by using the same command to quantize it but adding &#39;gptq&#39; to the quantization mode i.e.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Spits out model at checkpoints/$MODEL_REPO/model_int4-gptq.g32.pth&#xA;python quantize.py --mode int4-gptq --calibration_tasks wikitext --calibration_seq_length 2048&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then eval or generate text with this model in the same way as above.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;gpt-fast&lt;/code&gt; is released under the &lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast/main/LICENSE&#34;&gt;BSD 3&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lightning AI for supporting pytorch and work in flash attention, int8 quantization, and LoRA fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;GGML for driving forward fast, on device inference of LLMs&lt;/li&gt; &#xA; &lt;li&gt;Karpathy for spearheading simple, interpretable and fast LLM implementations&lt;/li&gt; &#xA; &lt;li&gt;MLC-LLM for pushing 4-bit quantization performance on heterogenous hardware&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>dangeng/visual_anagrams</title>
    <updated>2023-12-03T01:40:03Z</updated>
    <id>tag:github.com,2023-12-03:/dangeng/visual_anagrams</id>
    <link href="https://github.com/dangeng/visual_anagrams" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the paper &#34;Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dangeng.github.io/&#34;&gt;Daniel Geng&lt;/a&gt;, &lt;a href=&#34;https://inbumpark.github.io/&#34;&gt;Aaron Park&lt;/a&gt;, &lt;a href=&#34;https://andrewowens.com/&#34;&gt;Andrew Owens&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;[&lt;a href=&#34;https://arxiv.org/abs/2311.17919&#34;&gt;Arxiv&lt;/a&gt;] [&lt;a href=&#34;https://dangeng.github.io/visual_anagrams/&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/drive/1hCvJR5GsQrhH1ceDjdbzLG8y6m2UdJ6l?usp=sharing&#34;&gt;Colab&lt;/a&gt;]&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1hCvJR5GsQrhH1ceDjdbzLG8y6m2UdJ6l?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dangeng/visual_anagrams/main/assets/teaser.small.gif&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains code to generate visual anagrams and other multi-view optical illusions. These are images that change appearance or identity when transformed, such as by a rotation, a color inversion, or a jigsaw rearrangement. Please read our paper or visit our website for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Conda Environment&lt;/h3&gt; &#xA;&lt;p&gt;Create a conda env by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then activate it by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate visual_anagrams&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;DeepFloyd&lt;/h3&gt; &#xA;&lt;p&gt;Our method uses &lt;a href=&#34;https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if&#34;&gt;DeepFloyd IF&lt;/a&gt;, a pixel-based diffusion model. We do not use Stable Diffusion because latent diffusion models cause artifacts in illusions (see our paper for more details).&lt;/p&gt; &#xA;&lt;p&gt;Before using DeepFloyd IF, you must accept its usage conditions. To do so:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure to have a &lt;a href=&#34;https://huggingface.co/join&#34;&gt;Hugging Face account&lt;/a&gt; and be logged in.&lt;/li&gt; &#xA; &lt;li&gt;Accept the license on the model card of &lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-I-XL-v1.0&#34;&gt;DeepFloyd/IF-I-XL-v1.0&lt;/a&gt;. Accepting the license on the stage I model card will auto accept for the other IF models.&lt;/li&gt; &#xA; &lt;li&gt;Log in locally by running&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python huggingface_login.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and entering your &lt;a href=&#34;https://huggingface.co/docs/hub/security-tokens#what-are-user-access-tokens&#34;&gt;Hugging Face Hub access token&lt;/a&gt; when prompted. If asked &lt;code&gt;Add token as git credential? (Y/n)&lt;/code&gt;, you can respond with &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To generate 90 degree rotation illusions, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --name rotate_cw.village.horse --prompts &#34;a snowy mountain village&#34; &#34;a horse&#34; --style &#34;an oil painting of&#34; --views identity rotate_cw --num_samples 10 --num_inference_steps 30 --guidance_scale 10.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is a description of useful arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;name&lt;/code&gt;: Name for the illusion. Will save samples to &lt;code&gt;./results/{name}&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompts&lt;/code&gt;: A list of prompts for illusions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;style&lt;/code&gt;: Optional style prompt to prepend to each of the prompts. For example, could be &lt;code&gt;&#34;an oil painting of&#34;&lt;/code&gt;. Saves some writing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;views&lt;/code&gt;: A list of views to use. Must match the number of prompts. For a list of views see the &lt;code&gt;get_views&lt;/code&gt; function in &lt;code&gt;visual_anagrams/views/__init__.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;num_samples&lt;/code&gt;: Number of illusions to sample&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;num_inference_steps&lt;/code&gt;: Number of diffusion denoising steps to take.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;guidance_scale&lt;/code&gt;: Guidance scale for classifier free guidance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Animating&lt;/h3&gt; &#xA;&lt;p&gt;To animate the above two view illusion, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python animate.py --im_path results/rotate_cw.village.horse/0000/sample_256.png --metadata_path results/rotate_cw.village.horse/metadata.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is a description of useful arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;im_path&lt;/code&gt;: The path to your illusion&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;metadata_path&lt;/code&gt;: The path to metadata about your illusion, which is saved by &lt;code&gt;generate.py&lt;/code&gt;. Overrides the options below.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;view&lt;/code&gt;: Name of the view. For a list of views see the &lt;code&gt;get_views&lt;/code&gt; function in &lt;code&gt;visual_anagrams/views/__init__.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt_1&lt;/code&gt;: Prompt for the original image. You can add &lt;code&gt;\n&lt;/code&gt; characters here for line breaks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt_2&lt;/code&gt;: Same as &lt;code&gt;prompt_1&lt;/code&gt;, but for the transformed image.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The Art of Choosing Prompts&lt;/h2&gt; &#xA;&lt;p&gt;Choosing prompts for illusions can be fairly tricky and unintuitive. Here are some tips:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intuition and reasoning works less often than you would expect. Prompts that you think would work great often work poorly, and vice versa. So exploration is key.&lt;/li&gt; &#xA; &lt;li&gt;Styles such as &lt;code&gt;&#34;a photo of&#34;&lt;/code&gt; tend to be harder as the constraint of realism is fairly difficult (but this doesn&#39;t mean they can&#39;t work!).&lt;/li&gt; &#xA; &lt;li&gt;Conversely, styles such as &lt;code&gt;&#34;an oil painting of&#34;&lt;/code&gt; seem to do better because there&#39;s more freedom to how it can be depicted and interpreted.&lt;/li&gt; &#xA; &lt;li&gt;In a similar vein, subjects that allow for high degrees of flexibility in depiction tend to be good. For example, prompts such as &lt;code&gt;&#34;houseplants&#34;&lt;/code&gt; or &lt;code&gt;&#34;wine and cheese&#34;&lt;/code&gt; or &lt;code&gt;&#34;a kitchen&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;But be careful the subject is still easily recognizable. Illusions are much better when they are instantly understandable.&lt;/li&gt; &#xA; &lt;li&gt;Faces often make for very good &#34;hidden&#34; subjects. This is probably because the human visual system is particularly adept at picking out faces. For example, &lt;code&gt;&#34;an old man&#34;&lt;/code&gt; or &lt;code&gt;&#34;marilyn monroe&#34;&lt;/code&gt; tend to be good subjects.&lt;/li&gt; &#xA; &lt;li&gt;Perhaps a bit evident, but 3 view and 4 view illusions are considerably more difficult to get to work.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;More Examples&lt;/h2&gt; &#xA;&lt;p&gt;Flipping illusion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --name flip.campfire.man --prompts &#34;an oil painting of people around a campfire&#34; &#34;an oil painting of an old man&#34; --views identity flip --num_samples 10 --num_inference_steps 30 --guidance_scale 10.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Jigsaw illusions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --name jigsaw.houseplants.marilyn --prompts &#34;houseplants&#34; &#34;marilyn monroe&#34; --style &#34;an oil painting of&#34; --views identity jigsaw --num_samples 10 --num_inference_steps 30 --guidance_scale 10.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inner circle illusions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --name inner.einstein.marilyn --prompts &#34;albert einstein&#34; &#34;marilyn monroe&#34; --style &#34;an oil painting of&#34; --views identity inner_circle --num_samples 10 --num_inference_steps 30 --guidance_scale 10.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Color inversion illusions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --name negate.landscape.houseplants --prompts &#34;a landscape&#34; &#34;houseplants&#34; --style &#34;a lithograph of&#34; --views identity negate --num_samples 10 --num_inference_steps 30 --guidance_scale 10.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Patch permutation illusions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --name patch.lemur.kangaroo --prompts &#34;a lemur&#34; &#34;a kangaroo&#34; --style &#34;a pencil sketch of&#34; --views identity patch_permute --num_samples 10 --num_inference_steps 30 --guidance_scale 10.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pixel permutation illusions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --name pixel.duck.rabbit --prompts &#34;a duck&#34; &#34;a rabbit&#34; --style &#34;a mosaic of&#34; --views identity pixel_permute --num_samples 10 --num_inference_steps 30 --guidance_scale 10.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Skew illusions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --name skew.tudor.skull --prompts &#34;a tudor portrait&#34; &#34;a skull&#34; --style &#34;an oil painting of&#34; --views identity skew --num_samples 10 --num_inference_steps 30 --guidance_scale 10.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Three view illusions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate.py --name threeview.waterfall.teddy.rabbit --prompts &#34;a waterfall&#34; &#34;a teddy bear&#34; &#34;a rabbit&#34; --style &#34;an oil painting of&#34; --views identity rotate_cw rotate_ccw --num_samples 10 --num_inference_steps 30 --guidance_scale 10.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Custom Views&lt;/h2&gt; &#xA;&lt;p&gt;Views are derived from the base class &lt;code&gt;BaseView&lt;/code&gt;. You can see many examples of these transformations in &lt;code&gt;views.py&lt;/code&gt;, if you want to write your own view.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, if your view can be implemented as a permutation of pixels, you can probably get away with just saving a permutation array to disk and pasing it to the &lt;code&gt;PermuteView&lt;/code&gt; class. See &lt;code&gt;permutations/make_inner_rotation_perm.py&lt;/code&gt; and &lt;code&gt;get_view()&lt;/code&gt; in &lt;code&gt;views.py&lt;/code&gt; for an example of this.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-deepmind/materials_discovery</title>
    <updated>2023-12-03T01:40:03Z</updated>
    <id>tag:github.com,2023-12-03:/google-deepmind/materials_discovery</id>
    <link href="https://github.com/google-deepmind/materials_discovery" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Materials Discovery: GNoME&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#dataset&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#models&#34;&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#colabs&#34;&gt;&lt;strong&gt;Colabs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#license&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#disclaimer&#34;&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#upcoming&#34;&gt;&lt;strong&gt;Upcoming&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#citing&#34;&gt;&lt;strong&gt;Citing&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What is Materials Discovery: GNoME?&lt;/h3&gt; &#xA;&lt;p&gt;From microchips to batteries and photovoltaics, discovery of inorganic crystals is a fundamental problem in materials science. Graph Networks for Materials Science (GNoME) is a project centered around scaling machine learning methods to tackle this core task. With results recently published, this repository serves to share the discovery of 381,000 novel stable materials with the wider materials science community and hopefully enable exciting new research via the updated convex hull.&lt;/p&gt; &#xA;&lt;p&gt;This is a research project, not an official Google product. Expect bugs as the repository expands and sharp edges. Please help by exploring the structures and let us know what you think!&lt;/p&gt; &#xA;&lt;h3&gt;Contents&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#dataset&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#models&#34;&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#colabs&#34;&gt;&lt;strong&gt;Colabs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#license&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#disclaimer&#34;&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#upcoming&#34;&gt;&lt;strong&gt;Upcoming&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/materials_discovery/main/#citing&#34;&gt;&lt;strong&gt;Citing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;p&gt;The dataset described in the original paper is provided across multiple file formats. For more details, including how to download the dataset, please see our dataset descriptor file in DATASET.md.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Summarized&lt;/strong&gt; A summary of the dataset is provided in CSV format. This file contains compositions and raw energies from Density Functional Theory (DFT) calculations, as well as other popular measurements (e.g. formation energy and decomposition energy).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Structure&lt;/strong&gt; Loading of structures is slightly more cumbersome due to file sizes involved. Due to the organization of the convex hull, only one structure is needed per composition, so results from the summary can be used to pull from the compressed data directory available in the linked Cloud Bucket.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;r²SCAN&lt;/strong&gt; Baseline calculations were performed via PBE functional for the calculations. The paper also reports metrics for binaries and tenaries with the r²SCAN functional. A summary of calculated energies and associated metrics is included for these calculations.&lt;/p&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;We provide model definitions for the two sets of models used in the paper.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GNoME&lt;/strong&gt; were the predominant model behind new materials discovery. This simple message passing architecture was optimized by training on a snapshot of Materials Project from 2018, leading to state-of-the-art results of 21meV/atom.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nequip&lt;/strong&gt; corresponds to the architecture created by Batzner et al. (2022). This architecture was used to train the interatomic potentials described in the paper to learn the dynamics from the large dataset. We provide an implementation in JAX as well as basic configuration parameters for the corresponding architecture.&lt;/p&gt; &#xA;&lt;h3&gt;Colabs&lt;/h3&gt; &#xA;&lt;p&gt;Colab examples of how to interact with the dataset and models will be released to provide an easier interface with both.&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;The Colab notebooks and associated code provided in this repository are licensed under the Apache License, Version 2.0. You may obtain a copy of the License at &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Data contained in the Graph Networks for Materials Exploration Database is available for use under the terms of the Creative Commons Attribution Noncommercial 4.0 International Licence (CC BY NC 4.0). You may obtain a copy of the License at &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34;&gt;https://creativecommons.org/licenses/by-nc/4.0/&lt;/a&gt;. The dataset was created using the Vienna Ab initio Simulation Package (VASP) in order to run calculations from Density Functional Theory.&lt;/p&gt; &#xA;&lt;h3&gt;Upcoming&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Repeated calculations of structures from the Materials Project and other agglomerated datasets (Open Quantum Materials Database, WBM) under consistent settings (defining the complete convex hull)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Example colabs of loading materials from the CSVs and calculating convex hull energies&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Code to visualize structures in a colab notebooks&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reference structures and search paths&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Model training colabs and configs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Additional material properties (e.g. electronic band structure)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;This is not an official Google product.&lt;/p&gt; &#xA;&lt;p&gt;Graph Networks for Materials Exploration Database, Copyright, Google LLC, (2023).&lt;/p&gt; &#xA;&lt;p&gt;Data in the Graph Networks for Materials Exploration Database is for theoretical modeling only, caution should be exercised in its use. The Graph Networks for Materials Exploration Database is not intended for, and is not approved for, any medical or clinical use. The Graph Networks for Materials Exploration Database is experimental in nature and provided on an “as is” basis. To the maximum extent permitted at law, Google disclaims all representations, conditions and warranties, whether express or implied, in relation to the Graph Networks for Materials Exploration Database (including without limitation for non-infringement of third party intellectual property rights, satisfactory quality, merchantability or fitness for a particular purpose), and the user shall hold Google free and harmless in connection with their use of such content.&lt;/p&gt; &#xA;&lt;h3&gt;Citing&lt;/h3&gt; &#xA;&lt;p&gt;If you are using this resource please cite our &lt;a href=&#34;https://www.nature.com/articles/s41586-023-06735-9&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;  @article{merchant2023scaling,&#xA;    title={Scaling deep learning for materials discovery},&#xA;    author={Amil Merchant and Simon Batzner and Samuel S. Schoenholz and Muratahan Aykol and Gowoon Cheon and Ekin Dogus Cubuk},&#xA;    journal={Nature},&#xA;    year={2023},&#xA;    doi={10.1038/s41586-023-06735-9},&#xA;    href={https://www.nature.com/articles/s41586-023-06735-9},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>