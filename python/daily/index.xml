<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-01T01:36:41Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>leptonai/leptonai</title>
    <updated>2024-02-01T01:36:41Z</updated>
    <id>tag:github.com,2024-02-01:/leptonai/leptonai</id>
    <link href="https://github.com/leptonai/leptonai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Pythonic framework to simplify AI service building&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leptonai/leptonai/main/assets/logo.svg?sanitize=true&#34; height=&#34;100&#34;&gt; &#xA;&lt;h1&gt;Lepton AI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Pythonic framework to simplify AI service building&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lepton.ai/&#34;&gt;Homepage&lt;/a&gt; • &lt;a href=&#34;https://dashboard.lepton.ai/playground&#34;&gt;API Playground&lt;/a&gt; • &lt;a href=&#34;https://github.com/leptonai/examples&#34;&gt;Examples&lt;/a&gt; • &lt;a href=&#34;https://lepton.ai/docs/&#34;&gt;Documentation&lt;/a&gt; • &lt;a href=&#34;https://lepton.ai/references&#34;&gt;CLI References&lt;/a&gt; • &lt;a href=&#34;https://twitter.com/leptonai&#34;&gt;Twitter&lt;/a&gt; • &lt;a href=&#34;https://leptonai.medium.com/&#34;&gt;Blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The LeptonAI python library allows you to build an AI service from python code with ease. Key features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A pythonic abstraction &lt;code&gt;Photon&lt;/code&gt;, allowing you to convert research and modeling code into a service with a few lines of code.&lt;/li&gt; &#xA; &lt;li&gt;Simple abstractions to launch models like those on &lt;a href=&#34;https://huggingface.co&#34;&gt;HuggingFace&lt;/a&gt; in few lines of code.&lt;/li&gt; &#xA; &lt;li&gt;Prebuilt examples for common models such as Llama, SDXL, Whisper, and others.&lt;/li&gt; &#xA; &lt;li&gt;AI tailored batteries included such as autobatching, background jobs, etc.&lt;/li&gt; &#xA; &lt;li&gt;A client to automatically call your service like native Python functions.&lt;/li&gt; &#xA; &lt;li&gt;Pythonic configuration specs to be readily shipped in a cloud environment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started with one-liner&lt;/h2&gt; &#xA;&lt;p&gt;Install the library with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -U leptonai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This installs the &lt;code&gt;leptonai&lt;/code&gt; python library, as well as the commandline interface &lt;code&gt;lep&lt;/code&gt;. You can then launch a HuggingFace model, say &lt;code&gt;gpt2&lt;/code&gt;, in one line of code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lep photon run --name gpt2 --model hf:gpt2 --local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have access to the Llama2 model (&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b&#34;&gt;apply for access here&lt;/a&gt;) and you have a reasonably sized GPU, you can launch it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# hint: you can also write `-n` and `-m` for short&#xA;lep photon run -n llama2 -m hf:meta-llama/Llama-2-7b-chat-hf --local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Be sure to use the &lt;code&gt;-hf&lt;/code&gt; version for Llama2, which is compatible with huggingface pipelines.)&lt;/p&gt; &#xA;&lt;p&gt;You can then access the service with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from leptonai.client import Client, local&#xA;c = Client(local(port=8080))&#xA;# Use the following to print the doc&#xA;print(c.run.__doc__)&#xA;print(c.run(inputs=&#34;I enjoy walking with my cute dog&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fully managed Llama2 models and CodeLlama models can be found in the &lt;a href=&#34;https://dashboard.lepton.ai/playground&#34;&gt;playground&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Many standard HuggingFace pipelines are supported - find out more details in the &lt;a href=&#34;https://www.lepton.ai/docs/advanced/prebuilt_photons#hugging-face-photons&#34;&gt;documentation&lt;/a&gt;. Not all HuggingFace models are supported though, as many of them contain custom code and are not standard pipelines. If you find a popular model you would like to support, please &lt;a href=&#34;https://github.com/leptonai/leptonai/issues/new&#34;&gt;open an issue or a PR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Checking out more examples&lt;/h2&gt; &#xA;&lt;p&gt;You can find out more examples from the &lt;a href=&#34;https://github.com/leptonai/examples&#34;&gt;examples repository&lt;/a&gt;. For example, launch the Stable Diffusion XL model with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:leptonai/examples.git&#xA;cd examples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lep photon run -n sdxl -m advanced/sdxl/sdxl.py --local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the service is running, you can access it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from leptonai.client import Client, local&#xA;&#xA;c = Client(local(port=8080))&#xA;&#xA;img_content = c.run(prompt=&#34;a cat launching rocket&#34;, seed=1234)&#xA;with open(&#34;cat.png&#34;, &#34;wb&#34;) as fid:&#xA;    fid.write(img_content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or access the mounted Gradio UI at &lt;a href=&#34;http://localhost:8080/ui&#34;&gt;http://localhost:8080/ui&lt;/a&gt;. Check the &lt;a href=&#34;https://github.com/leptonai/examples/raw/main/advanced/sdxl/README.md&#34;&gt;README file&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;A fully managed SDXL is hosted at &lt;a href=&#34;https://dashboard.lepton.ai/playground/sdxl&#34;&gt;https://dashboard.lepton.ai/playground/sdxl&lt;/a&gt; with API access.&lt;/p&gt; &#xA;&lt;h2&gt;Writing your own photons&lt;/h2&gt; &#xA;&lt;p&gt;Writing your own photon is simple: write a python Photon class and decorate functions with &lt;code&gt;@Photon.handler&lt;/code&gt;. As long as your input and output are JSON serializable, you are good to go. For example, the following code launches a simple echo service:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# my_photon.py&#xA;from leptonai.photon import Photon&#xA;&#xA;class Echo(Photon):&#xA;    @Photon.handler&#xA;    def echo(self, inputs: str) -&amp;gt; str:&#xA;        &#34;&#34;&#34;&#xA;        A simple example to return the original input.&#xA;        &#34;&#34;&#34;&#xA;        return inputs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then launch the service with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;lep photon run -n echo -m my_photon.py --local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can use your service as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from leptonai.client import Client, local&#xA;&#xA;c = Client(local(port=8080))&#xA;&#xA;# will print available paths&#xA;print(c.paths())&#xA;# will print the doc for c.echo. You can also use `c.echo?` in Jupyter.&#xA;print(c.echo.__doc__)&#xA;# will actually call echo.&#xA;c.echo(inputs=&#34;hello world&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details, checkout the &lt;a href=&#34;https://lepton.ai/docs/&#34;&gt;documentation&lt;/a&gt; and the &lt;a href=&#34;https://github.com/leptonai/examples&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions and collaborations are welcome and highly appreciated. Please check out the &lt;a href=&#34;https://github.com/leptonai/leptonai/raw/main/CONTRIBUTING.md&#34;&gt;contributor guide&lt;/a&gt; for how to get involved.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The Lepton AI python library is released under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;p&gt;Developer Note: early development of LeptonAI was in a separate mono-repo, which is why you may see commits from the &lt;code&gt;leptonai/lepton&lt;/code&gt; repo. We intend to use this open source repo as the source of truth going forward.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PKU-YuanGroup/MoE-LLaVA</title>
    <updated>2024-02-01T01:36:41Z</updated>
    <id>tag:github.com,2024-02-01:/PKU-YuanGroup/MoE-LLaVA</id>
    <link href="https://github.com/PKU-YuanGroup/MoE-LLaVA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mixture-of-Experts for Large Vision-Language Models&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://s11.ax1x.com/2023/12/28/piqvDMV.png&#34; width=&#34;250&#34; style=&#34;margin-bottom: 0.2;&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.15947&#34;&gt;MoE-LLaVA: Mixture of Experts for Large Vision-Language Models&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; If you like our project, please give us a star ⭐ on GitHub for latest update. &lt;/h5&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/LanguageBind/MoE-LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20In%20Spaces-blue.svg?sanitize=true&#34; alt=&#34;hf_space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.15947&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2401.15947-b31b1b.svg?logo=arXiv&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/ICylR6n2LhqQRS0CAHFI1A&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-WeChat@%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83-000000?logo=wechat&amp;amp;logoColor=07C160&#34; alt=&#34;jiqizhixin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-yellow&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FPKU-YuanGroup%2FMoE-LLaVA&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=Visitor&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/issues?q=is%3Aopen+is%3Aissue&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PKU-YuanGroup/MoE-LLaVA?color=critical&amp;amp;label=Issues&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/issues?q=is%3Aissue+is%3Aclosed&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed/PKU-YuanGroup/MoE-LLaVA?color=success&amp;amp;label=Issues&#34; alt=&#34;GitHub closed issues&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &lt;/h5&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary&gt;💡 I also have other vision-language projects that may interest you ✨. &lt;/summary&gt;&#xA; &lt;p&gt; &#xA;  &lt;!--  may --&gt; &lt;/p&gt;&#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.10122&#34;&gt;&lt;strong&gt;Video-LLaVA: Learning United Visual Representation by Alignment Before Projection&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, Li Yuan &lt;br&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Github-black?logo=github&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.10122&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2311.10122-b31b1b.svg?logo=arXiv&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.01852&#34;&gt;&lt;strong&gt;LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, Li Yuan &lt;br&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/LanguageBind&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Github-black?logo=github&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/LanguageBind&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/LanguageBind.svg?style=social&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.01852&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2310.01852-b31b1b.svg?logo=arXiv&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;h2&gt;📰 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.01.30]&lt;/strong&gt; 🔥 We release a stronger &lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-2.7B-4e-384&#34;&gt;MoE-LLaVA-Phi2&lt;/a&gt;. &lt;strong&gt;The average performance surpasses LLaVA-1.5-7B by using 3.6B activated parameters,&lt;/strong&gt; checking our &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/#-model-zoo&#34;&gt;model zoo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.01.27]&lt;/strong&gt; 🤗 &lt;a href=&#34;https://huggingface.co/spaces/LanguageBind/MoE-LLaVA&#34;&gt;Hugging Face demo&lt;/a&gt; and &lt;strong&gt;all codes &amp;amp; datasets&lt;/strong&gt; are available now! Welcome to &lt;strong&gt;watch&lt;/strong&gt; 👀 this repository for the latest updates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;😮 Highlights&lt;/h2&gt; &#xA;&lt;p&gt;MoE-LLaVA shows excellent performance in multi-modal learning.&lt;/p&gt; &#xA;&lt;h3&gt;🔥 High performance, but with fewer parameters&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;with just &lt;strong&gt;3B sparsely activated parameters&lt;/strong&gt;, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmarks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/assets/intro0.jpg&#34; width=&#34;55%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;🚀 Simple baseline, learning multi-modal interactions with sparse pathways.&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With the addition of &lt;strong&gt;a simple MoE tuning stage&lt;/strong&gt;, we can complete the training of MoE-LLaVA on &lt;strong&gt;8 V100 GPUs&lt;/strong&gt; within 2 days.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/assets/intro.jpg&#34; width=&#34;65%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;🤗 Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;Highly recommend trying out our web demo by the following command, which incorporates all features currently supported by MoE-LLaVA. We also provide &lt;a href=&#34;https://huggingface.co/spaces/LanguageBind/MoE-LLaVA&#34;&gt;online demo&lt;/a&gt; in Huggingface Spaces.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use phi2&#xA;deepspeed --include localhost:0 moellava/serve/gradio_web_server.py --model-path &#34;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&#34; &#xA;# use qwen&#xA;deepspeed --include localhost:0 moellava/serve/gradio_web_server.py --model-path &#34;LanguageBind/MoE-LLaVA-Qwen-1.8B-4e&#34; &#xA;# use stablelm&#xA;deepspeed --include localhost:0 moellava/serve/gradio_web_server.py --model-path &#34;LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/assets/62638829/8541aac6-9ef6-4fde-aa94-80d0375b9bdb&#34;&gt;https://github.com/PKU-YuanGroup/MoE-LLaVA/assets/62638829/8541aac6-9ef6-4fde-aa94-80d0375b9bdb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use phi2&#xA;deepspeed --include localhost:0 moellava/serve/cli.py --model-path &#34;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&#34;  --image-file &#34;image.jpg&#34;&#xA;# use qwen&#xA;deepspeed --include localhost:0 moellava/serve/cli.py --model-path &#34;LanguageBind/MoE-LLaVA-Qwen-1.8B-4e&#34;  --image-file &#34;image.jpg&#34;&#xA;# use stablelm&#xA;deepspeed --include localhost:0 moellava/serve/cli.py --model-path &#34;LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&#34;  --image-file &#34;image.jpg&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/assets/imagecli.gif&#34;&gt; &#xA;&lt;h2&gt;🐳 Model Zoo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Activated Param&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;Avg&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;VizWiz&lt;/th&gt; &#xA;   &lt;th&gt;SQA&lt;/th&gt; &#xA;   &lt;th&gt;T-VQA&lt;/th&gt; &#xA;   &lt;th&gt;POPE&lt;/th&gt; &#xA;   &lt;th&gt;MM-Bench&lt;/th&gt; &#xA;   &lt;th&gt;MM-Vet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-LLaVA-1.6B×4-Top2&lt;/td&gt; &#xA;   &lt;td&gt;2.0B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&#34;&gt;LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;57.3&lt;/td&gt; &#xA;   &lt;td&gt;76.7&lt;/td&gt; &#xA;   &lt;td&gt;60.3&lt;/td&gt; &#xA;   &lt;td&gt;36.2&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;50.1&lt;/td&gt; &#xA;   &lt;td&gt;85.7&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;   &lt;td&gt;26.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-LLaVA-1.8B×4-Top2&lt;/td&gt; &#xA;   &lt;td&gt;2.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Qwen-1.8B-4e&#34;&gt;LanguageBind/MoE-LLaVA-Qwen-1.8B-4e&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;56.7&lt;/td&gt; &#xA;   &lt;td&gt;76.2&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;32.6&lt;/td&gt; &#xA;   &lt;td&gt;63.1&lt;/td&gt; &#xA;   &lt;td&gt;48.0&lt;/td&gt; &#xA;   &lt;td&gt;87.0&lt;/td&gt; &#xA;   &lt;td&gt;59.6&lt;/td&gt; &#xA;   &lt;td&gt;25.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-LLaVA-2.7B×4-Top2&lt;/td&gt; &#xA;   &lt;td&gt;3.6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&#34;&gt;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;60.3&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;61.1&lt;/td&gt; &#xA;   &lt;td&gt;43.4&lt;/td&gt; &#xA;   &lt;td&gt;68.7&lt;/td&gt; &#xA;   &lt;td&gt;50.2&lt;/td&gt; &#xA;   &lt;td&gt;85.0&lt;/td&gt; &#xA;   &lt;td&gt;65.5&lt;/td&gt; &#xA;   &lt;td&gt;31.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-LLaVA-2.7B×4-Top2-384&lt;/td&gt; &#xA;   &lt;td&gt;3.6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-2.7B-4e-384&#34;&gt;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e-384&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;62.9&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;43.7&lt;/td&gt; &#xA;   &lt;td&gt;70.3&lt;/td&gt; &#xA;   &lt;td&gt;57.0&lt;/td&gt; &#xA;   &lt;td&gt;85.7&lt;/td&gt; &#xA;   &lt;td&gt;68.0&lt;/td&gt; &#xA;   &lt;td&gt;35.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-1.5&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.5-7b&#34;&gt;liuhaotian/llava-v1.5-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;62.0&lt;/td&gt; &#xA;   &lt;td&gt;78.5&lt;/td&gt; &#xA;   &lt;td&gt;62.0&lt;/td&gt; &#xA;   &lt;td&gt;50.0&lt;/td&gt; &#xA;   &lt;td&gt;66.8&lt;/td&gt; &#xA;   &lt;td&gt;58.2&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;30.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!--&#xA;| LLaVA-1.5 | 13B | [liuhaotian/llava-v1.5-13b](https://huggingface.co/liuhaotian/llava-v1.5-13b) | 64.9 | 80.0 | 63.3 | 53.6 | 71.6 | 61.3 | 85.9 | 67.7 | 35.4 |&#xA;--&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain Model&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MoE-LLaVA-1.6B×4-Top2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-StableLM-Pretrain&#34;&gt;LanguageBind/MoE-LLaVA-StableLM-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MoE-LLaVA-1.8B×4-Top2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Qwen-Pretrain&#34;&gt;LanguageBind/MoE-LLaVA-Qwen-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MoE-LLaVA-2.7B×4-Top2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-Pretrain&#34;&gt;LanguageBind/MoE-LLaVA-Phi2-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MoE-LLaVA-2.7B×4-Top2-384&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-384-Pretrain&#34;&gt;LanguageBind/MoE-LLaVA-Phi2-384-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;⚙️ Requirements and Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.10&lt;/li&gt; &#xA; &lt;li&gt;Pytorch == 2.0.1&lt;/li&gt; &#xA; &lt;li&gt;CUDA Version &amp;gt;= 11.7&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transformers == 4.36.2&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tokenizers==0.15.1&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install required packages:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/PKU-YuanGroup/MoE-LLaVA&#xA;cd MoE-LLaVA&#xA;conda create -n moellava python=3.10 -y&#xA;conda activate moellava&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;pip install -e &#34;.[train]&#34;&#xA;pip install flash-attn --no-build-isolation&#xA;&#xA;# Below are optional. For Qwen model.&#xA;git clone https://github.com/Dao-AILab/flash-attention&#xA;cd flash-attention &amp;amp;&amp;amp; pip install .&#xA;# Below are optional. Installing them might be slow.&#xA;# pip install csrc/layer_norm&#xA;# If the version of flash-attn is higher than 2.1.1, the following is not needed.&#xA;# pip install csrc/rotary&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🗝️ Training &amp;amp; Validating&lt;/h2&gt; &#xA;&lt;p&gt;The training &amp;amp; validating instruction is in &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/docs/TRAIN.md&#34;&gt;TRAIN.md&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/docs/EVAL.md&#34;&gt;EVAL.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;💡 Customizing your MoE-LLaVA&lt;/h2&gt; &#xA;&lt;p&gt;The instruction is in &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/docs/CUSTOM.md&#34;&gt;CUSTOM.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;😍 Visualization&lt;/h2&gt; &#xA;&lt;p&gt;The instruction is in &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/docs/VISUALIZATION.md&#34;&gt;VISUALIZATION.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🤖 API&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We open source all codes.&lt;/strong&gt; If you want to load the model (e.g. &lt;code&gt;LanguageBind/MoE-LLaVA&lt;/code&gt;) on local, you can use the following code snippets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using the following command to run the code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deepspeed --include localhost:0 predict.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from moellava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN&#xA;from moellava.conversation import conv_templates, SeparatorStyle&#xA;from moellava.model.builder import load_pretrained_model&#xA;from moellava.utils import disable_torch_init&#xA;from moellava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria&#xA;&#xA;def main():&#xA;    disable_torch_init()&#xA;    image = &#39;moellava/serve/examples/extreme_ironing.jpg&#39;&#xA;    inp = &#39;What is unusual about this image?&#39;&#xA;    model_path = &#39;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&#39;  # LanguageBind/MoE-LLaVA-Qwen-1.8B-4e or LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&#xA;    device = &#39;cuda&#39;&#xA;    load_4bit, load_8bit = False, False  # FIXME: Deepspeed support 4bit or 8bit?&#xA;    model_name = get_model_name_from_path(model_path)&#xA;    tokenizer, model, processor, context_len = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device)&#xA;    image_processor = processor[&#39;image&#39;]&#xA;    conv_mode = &#34;phi&#34;  # qwen or stablelm&#xA;    conv = conv_templates[conv_mode].copy()&#xA;    roles = conv.roles&#xA;    image_tensor = image_processor.preprocess(image, return_tensors=&#39;pt&#39;)[&#39;pixel_values&#39;].to(model.device, dtype=torch.float16)&#xA;&#xA;    print(f&#34;{roles[1]}: {inp}&#34;)&#xA;    inp = DEFAULT_IMAGE_TOKEN + &#39;\n&#39; + inp&#xA;    conv.append_message(conv.roles[0], inp)&#xA;    conv.append_message(conv.roles[1], None)&#xA;    prompt = conv.get_prompt()&#xA;    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=&#39;pt&#39;).unsqueeze(0).cuda()&#xA;    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2&#xA;    keywords = [stop_str]&#xA;    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)&#xA;&#xA;    with torch.inference_mode():&#xA;        output_ids = model.generate(&#xA;            input_ids,&#xA;            images=image_tensor,&#xA;            do_sample=True,&#xA;            temperature=0.2,&#xA;            max_new_tokens=1024,&#xA;            use_cache=True,&#xA;            stopping_criteria=[stopping_criteria])&#xA;&#xA;    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:], skip_special_tokens=True).strip()&#xA;    print(outputs)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    main()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🙌 Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA&#34;&gt;Video-LLaVA&lt;/a&gt; This framework empowers the model to efficiently utilize the united visual tokens.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/LanguageBind&#34;&gt;LanguageBind&lt;/a&gt; An open source five modalities language-based retrieval framework.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👍 Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt; The codebase we built upon and it is an efficient large language and vision assistant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔒 License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The majority of this project is released under the Apache 2.0 license as found in the &lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;The service is a research preview intended for non-commercial use only, subject to the model &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;License&lt;/a&gt; of LLaMA, &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;Terms of Use&lt;/a&gt; of the data generated by OpenAI, and &lt;a href=&#34;https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb&#34;&gt;Privacy Practices&lt;/a&gt; of ShareGPT. Please contact us if you find any potential violation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✏️ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;📝&lt;/span&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{lin2024moellava,&#xA;      title={MoE-LLaVA: Mixture of Experts for Large Vision-Language Models}, &#xA;      author={Bin Lin and Zhenyu Tang and Yang Ye and Jiaxi Cui and Bin Zhu and Peng Jin and Junwu Zhang and Munan Ning and Li Yuan},&#xA;      year={2024},&#xA;      eprint={2401.15947},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{lin2023video,&#xA;  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},&#xA;  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},&#xA;  journal={arXiv preprint arXiv:2311.10122},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;✨ Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#PKU-YuanGroup/MoE-LLaVA&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=PKU-YuanGroup/MoE-LLaVA&amp;amp;type=Date&#34; alt=&#34;Star History&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🤝 Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=PKU-YuanGroup/MoE-LLaVA&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>h4x0r-dz/CVE-2024-23897</title>
    <updated>2024-02-01T01:36:41Z</updated>
    <id>tag:github.com,2024-02-01:/h4x0r-dz/CVE-2024-23897</id>
    <link href="https://github.com/h4x0r-dz/CVE-2024-23897" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CVE-2024-23897&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2024-23897&lt;/h1&gt; &#xA;&lt;p&gt;Jenkins CVE-2024-23897: Arbitrary File Read Vulnerability Leading to RCE&lt;/p&gt; &#xA;&lt;p&gt;Jenkins uses the args4j library to parse command arguments and options on the Jenkins controller when processing CLI commands. This command parser has a feature that replaces an @ character followed by a file path in an argument with the file’s contents (expandAtFiles). This feature is enabled by default and Jenkins 2.441 and earlier, LTS 2.426.2 and earlier does not disable it.&lt;/p&gt; &#xA;&lt;p&gt;This allows attackers to read arbitrary files on the Jenkins controller file system using the default character encoding of the Jenkins controller process.&lt;/p&gt; &#xA;&lt;p&gt;Attackers with Overall/Read permission can read entire files.&lt;/p&gt; &#xA;&lt;p&gt;Attackers without Overall/Read permission can read the first few lines of files. The number of lines that can be read depends on available CLI commands. As of publication of this advisory, the Jenkins security team has found ways to read the first three lines of files in recent releases of Jenkins without having any plugins installed, and has not identified any plugins that would increase this line count.&lt;/p&gt; &#xA;&lt;p&gt;more Info : &lt;a href=&#34;https://www.jenkins.io/security/advisory/2024-01-24/&#34;&gt;https://www.jenkins.io/security/advisory/2024-01-24/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;run : &lt;code&gt;python CVE-2024-23897.py -l host.txt -f /etc/passwd&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/h4x0r-dz/CVE-2024-23897/assets/26070859/9b547349-1783-42a9-9a02-588ab04f4d68&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>