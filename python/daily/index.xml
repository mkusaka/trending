<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-31T01:37:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>databricks/dbrx</title>
    <updated>2024-03-31T01:37:47Z</updated>
    <id>tag:github.com,2024-03-31:/databricks/dbrx</id>
    <link href="https://github.com/databricks/dbrx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code examples and resources for DBRX, a large language model developed by Databricks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DBRX&lt;/h1&gt; &#xA;&lt;p&gt;DBRX is a large language model trained by Databricks, and made available under an open license. This repository contains the minimal code and examples to run inference, as well as a collection of resources and links for using DBRX.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.databricks.com/blog/announcing-dbrx-new-standard-efficient-open-source-customizable-llms&#34;&gt;Founder&#39;s Blog&lt;/a&gt;, &lt;a href=&#34;https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm&#34;&gt;DBRX Technical Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;HuggingFace: &lt;a href=&#34;https://huggingface.co/collections/databricks/&#34;&gt;https://huggingface.co/collections/databricks/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLM Foundry: &lt;a href=&#34;https://github.com/mosaicml/llm-foundry&#34;&gt;https://github.com/mosaicml/llm-foundry&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A reference model code can be found in this repository at &lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/model/modeling_dbrx.py&#34;&gt;modeling_dbrx.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; this model code is supplied for references purposes only, please see the &lt;a href=&#34;https://huggingface.co/collections/databricks/&#34;&gt;HuggingFace&lt;/a&gt; repository for the official supported version.&lt;/p&gt; &#xA;&lt;h2&gt;Model details&lt;/h2&gt; &#xA;&lt;p&gt;DBRX is a Mixture-of-Experts (MoE) model with 132B total parameters and 36B live parameters. We use 16 experts, of which 4 are active during training or inference. DBRX was pre-trained for 12T tokens of text. DBRX has a context length of 32K tokens.&lt;/p&gt; &#xA;&lt;p&gt;The following models are open-sourced:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/databricks/dbrx-base&#34;&gt;DBRX Base&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pre-trained base model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/databricks/dbrx-instruct&#34;&gt;DBRX Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Finetuned model for instruction following&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The model was trained using optimized versions of our open source libraries &lt;a href=&#34;https://www.github.com/mosaicml/composer&#34;&gt;Composer&lt;/a&gt;, &lt;a href=&#34;https://www.github.com/mosaicml/llm-foundry&#34;&gt;LLM Foundry&lt;/a&gt;, &lt;a href=&#34;https://github.com/databricks/megablocks&#34;&gt;MegaBlocks&lt;/a&gt; and &lt;a href=&#34;https://github.com/mosaicml/streaming&#34;&gt;Streaming&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the instruct model, we used the ChatML format. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/MODEL_CARD_dbrx_instruct.md&#34;&gt;DBRX Instruct model card&lt;/a&gt; for more information on this.&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;To download the weights and tokenizer, please first visit the DBRX HuggingFace page and accept the license. Note: access to the Base model requires manual approval.&lt;/p&gt; &#xA;&lt;p&gt;We recommend having at least 320GB of memory to run the model.&lt;/p&gt; &#xA;&lt;p&gt;Then, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt # Or requirements-gpu.txt to use flash attention on GPU(s)&#xA;huggingface-cli login           # Add your Hugging Face token in order to access the model&#xA;python generate.py              # See generate.py to change the prompt and other settings&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more advanced usage, please see LLM Foundry (&lt;a href=&#34;https://github.com/mosaicml/llm-foundry/raw/main/scripts/inference/hf_chat.py&#34;&gt;chat script&lt;/a&gt;, &lt;a href=&#34;https://github.com/mosaicml/llm-foundry/raw/main/scripts/inference/hf_generate.py&#34;&gt;batch generation script&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;If you have any package installation issues, we recommend using our Docker image: &lt;a href=&#34;https://github.com/mosaicml/llm-foundry?tab=readme-ov-file#mosaicml-docker-images&#34;&gt;&lt;code&gt;mosaicml/llm-foundry:2.2.1_cu121_flash2-latest&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Both TensorRT-LLM and vLLM can be used to run optimized inference with DBRX. We have tested both libraries on NVIDIA A100 and H100 systems. To run inference with 16-bit precision, a minimum of 4 x 80GB multi-GPU system is required.&lt;/p&gt; &#xA;&lt;h3&gt;TensorRT-LLM&lt;/h3&gt; &#xA;&lt;p&gt;DBRX support is being added to TensorRT-LLM library: &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/pull/1363&#34;&gt;Pending PR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After merging, instructions to build and run DBRX TensorRT engines will be found at: &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/raw/main/examples/dbrx/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;vLLM&lt;/h3&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://docs.vllm.ai/en/latest/&#34;&gt;vLLM docs&lt;/a&gt; for instructions on how to run DBRX with the vLLM engine.&lt;/p&gt; &#xA;&lt;h3&gt;MLX&lt;/h3&gt; &#xA;&lt;p&gt;If you have an Apple laptop with a sufficiently powerful M-series chip, quantized version of DBRX can be run with MLX. See instructions for running DBRX on MLX &lt;a href=&#34;https://huggingface.co/mlx-community/dbrx-instruct-4bit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Finetune&lt;/h2&gt; &#xA;&lt;p&gt;To finetune DBRX with our open source library &lt;a href=&#34;https://www.github.com/mosaicml/llm-foundry&#34;&gt;LLM Foundry&lt;/a&gt;, please see the instructions in our training script (found &lt;a href=&#34;https://github.com/mosaicml/llm-foundry/tree/main/scripts/train&#34;&gt;here&lt;/a&gt;). We have finetuning support for both:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full parameter finetuning, see the yaml config &lt;a href=&#34;https://github.com/mosaicml/llm-foundry/raw/main/scripts/train/yamls/finetune/dbrx-full-ft.yaml&#34;&gt;dbrx-full-ft.yaml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LoRA finetuning, see the yaml config &lt;a href=&#34;https://github.com/mosaicml/llm-foundry/raw/main/scripts/train/yamls/finetune/dbrx-lora-ft.yaml&#34;&gt;dbrx-lora-ft.yaml&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: LoRA support currently cannot finetune the experts, since the experts are fused. Stay tuned for more.&lt;/p&gt; &#xA;&lt;h2&gt;Model card&lt;/h2&gt; &#xA;&lt;p&gt;The model cards can be found at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/MODEL_CARD_dbrx_base.md&#34;&gt;DBRX Base&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/MODEL_CARD_dbrx_instruct.md&#34;&gt;DBRX Instruct&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;p&gt;DBRX is available on the Databricks platform through:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/machine-learning/foundation-models/supported-models.html#dbrx-instruct&#34;&gt;Mosaic AI Model Serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/large-language-models/ai-playground.html&#34;&gt;Mosaic AI Playground&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other providers have recently added support for DBRX:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://you.com/&#34;&gt;You.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://labs.perplexity.ai/&#34;&gt;Perplexity Labs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The same tools used to train high quality MoE models such as DBRX are available for Databricks customers. Please reach out to us at &lt;a href=&#34;https://www.databricks.com/company/contact&#34;&gt;https://www.databricks.com/company/contact&lt;/a&gt; if you are interested in pre-training, finetuning, or deploying your own DBRX models!&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;For issues with model output, or community discussion, please use the Hugging Face community forum (&lt;a href=&#34;https://huggingface.co/databricks/dbrx-instruct&#34;&gt;instruct&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/databricks/dbrx-base&#34;&gt;base&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;For issues with LLM Foundry, or any of the underlying training libraries, please open an issue on the relevant GitHub repository.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our model weights and code are licensed for both researchers and commercial entities. The &lt;a href=&#34;https://www.databricks.com/legal/open-model-license&#34;&gt;Databricks Open Source License&lt;/a&gt; can be found at &lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;, and our Acceptable Use Policy can be found &lt;a href=&#34;https://www.databricks.com/legal/acceptable-use-policy-open-model&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>WithSecureLabs/drozer</title>
    <updated>2024-03-31T01:37:47Z</updated>
    <id>tag:github.com,2024-03-31:/WithSecureLabs/drozer</id>
    <link href="https://github.com/WithSecureLabs/drozer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Leading Security Assessment Framework for Android.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;drozer&lt;/h1&gt; &#xA;&lt;p&gt;drozer is an security testing framework for Android.&lt;/p&gt; &#xA;&lt;p&gt;drozer allows you to search for security vulnerabilities in apps and devices by assuming the role of an app and interacting with the Android Runtime, other apps&#39; IPC endpoints and the underlying OS.&lt;/p&gt; &#xA;&lt;p&gt;drozer provides tools to help you use, share and understand public Android exploits.&lt;/p&gt; &#xA;&lt;p&gt;drozer is open source software, maintained by WithSecure, and can be downloaded from: &lt;a href=&#34;https://labs.withsecure.com/tools/drozer/&#34;&gt;https://labs.withsecure.com/tools/drozer/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;NOTE&lt;/h2&gt; &#xA;&lt;p&gt;This is an BETA release of a rewritten drozer version, this version is updated to support python3.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the following known issues are present:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Building of custom agents functionality will crash the drozer client. This functionality is considered out of scope for the beta release of the revived drozer project.&lt;/li&gt; &#xA; &lt;li&gt;It is not possible to run drozer on a Windows host; you must run drozer on either a virtual machine or Docker image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker Container&lt;/h2&gt; &#xA;&lt;p&gt;To help with making sure drozer can be run on all systems, a Docker container was created that has a working build of Drozer.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Docker container and basic setup instructions can be found &lt;a href=&#34;https://hub.docker.com/r/withsecurelabs/drozer&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Instructions on building your own Docker container can be found &lt;a href=&#34;https://github.com/WithSecureLabs/drozer/tree/develop/docker&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Manual Building and Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Software pre-requisites&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python3.8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.python.org/pypi/protobuf&#34;&gt;Protobuf&lt;/a&gt; 4.25.2 or greater&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.python.org/pypi/pyOpenSSL&#34;&gt;Pyopenssl&lt;/a&gt; 22.0.0 or greater&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.python.org/pypi/Twisted&#34;&gt;Twisted&lt;/a&gt; 18.9.0 or greater&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/distro/&#34;&gt;Distro&lt;/a&gt; 1.8.0 or greater&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adoptopenjdk.net/releases.html&#34;&gt;Java Development Kit&lt;/a&gt; 11 or greater&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Installing (Kali / Debian)&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;pip&lt;/code&gt; to install the latest release of drozer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install drozer-&amp;lt;version&amp;gt;.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building and Installing (Kali / Debian)&lt;/h3&gt; &#xA;&lt;p&gt;All of thee requirements can be installed via the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install python3 python3-pip python3-protobuf python3-openssl \&#xA;python3-twisted python3-yaml python3-distro git protobuf-compiler \&#xA;libexpat1 libexpat1-dev libpython3-dev python-is-python3 zip default-jdk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then build drozer for Python wheel&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/WithSecureLabs/drozer.git&#xA;cd drozer&#xA;python setup.py bdist_wheel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, install drozer&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install dist/drozer-&amp;lt;version&amp;gt;-py3-none-any.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building and Installing (Arch Linux/BlackArch)&lt;/h3&gt; &#xA;&lt;p&gt;On any arch based installation, until proper pkgbuilds and pip packages are created, please use an &lt;a href=&#34;https://wiki.archlinux.org/title/Python/Virtual_environment&#34;&gt;virtualenv&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/WithSecureLabs/drozer.git&#xA;cd drozer&#xA;virtualenv -p /usr/bin/python3 venv&#xA;source venv/bin/activate&#xA;python setup.py bdist_wheel&#xA;sudo pip install dist/drozer-&amp;lt;version&amp;gt;-py3-none-any.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Protobuf errors&lt;/h3&gt; &#xA;&lt;p&gt;If protobuf complains about the protobuf defintions being out of date. Copy the protobuf definition from &lt;a href=&#34;https://github.com/WithSecureLabs/mercury-common/tree/48e81d5ae65ec38dbe1e4bfe09548203dcf13384&#34;&gt;here&lt;/a&gt; into common/protobuf.proto&lt;/p&gt; &#xA;&lt;p&gt;Then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd common&#xA;protoc --python_out=../src/pysolar/api/ protobuf.proto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installing the Agent&lt;/h3&gt; &#xA;&lt;p&gt;drozer can be installed using Android Debug Bridge (adb).&lt;/p&gt; &#xA;&lt;p&gt;Download the latest drozer Agent &lt;a href=&#34;https://github.com/WithSecureLabs/drozer-agent/releases/latest&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;$ adb install drozer-agent.apk&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Setup for session&lt;/h3&gt; &#xA;&lt;p&gt;You should now have the drozer Console installed on your PC, and the Agent running on your test device. Now, you need to connect the two and you‚Äôre ready to start exploring.&lt;/p&gt; &#xA;&lt;p&gt;We will use the server embedded in the drozer Agent to do this.&lt;/p&gt; &#xA;&lt;p&gt;You need to set up a suitable port forward so that your PC can connect to a TCP socket opened by the Agent inside the device or emulator. By default, drozer uses port 31415:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;$ adb forward tcp:31415 tcp:31415&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now, launch the Agent, select the &#34;Embedded Server&#34; option and tap &#34;Enable&#34; to start the server. You should see a notification that the server has started.&lt;/p&gt; &#xA;&lt;h3&gt;Start a session - running drozer on host&lt;/h3&gt; &#xA;&lt;p&gt;On your PC, connect using the drozer Console:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;$ drozer console connect&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If using a real device, the IP address of the device on the network must be specified:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;$ drozer console connect --server 192.168.0.10&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Command Reference&lt;/h3&gt; &#xA;&lt;p&gt;You should be presented with a drozer command prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;selecting f75640f67144d9a3 (unknown sdk 4.1.1)  &#xA;dz&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The prompt confirms the Android ID of the device you have connected to, along with the manufacturer, model and Android software version.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;run&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Executes a drozer module&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;list&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Show a list of all drozer modules that can be executed in the current session. This hides modules that you do not have suitable permissions to run.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;shell&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Start an interactive Linux shell on the device, in the context of the Agent process.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cd&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mounts a particular namespace as the root of session, to avoid having to repeatedly type the full name of a module.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;clean&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Remove temporary files stored by drozer on the Android device.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;contributors&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Displays a list of people who have contributed to the drozer framework and modules in use on your system.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;echo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Print text to the console.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;exit&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Terminate the drozer session.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;help&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Display help about a particular command or module.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;load&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Load a file containing drozer commands, and execute them in sequence.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;module&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Find and install additional drozer modules from the Internet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;permissions&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Display a list of the permissions granted to the drozer Agent.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;set&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Store a value in a variable that will be passed as an environment variable to any Linux shells spawned by drozer.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;unset&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Remove a named variable that drozer passes to any Linux shells that it spawns.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;drozer is released under a 3-clause BSD License. See LICENSE for full details.&lt;/p&gt; &#xA;&lt;h2&gt;Contacting the Project&lt;/h2&gt; &#xA;&lt;p&gt;drozer is Open Source software, made great by contributions from the community.&lt;/p&gt; &#xA;&lt;p&gt;For full source code, to report bugs, suggest features and contribute patches please see our Github project:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/WithSecureLabs/drozer&#34;&gt;https://github.com/WithSecureLabs/drozer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Bug reports, feature requests, comments and questions can be submitted &lt;a href=&#34;https://github.com/WithSecureLabs/drozer/issues&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TencentARC/BrushNet</title>
    <updated>2024-03-31T01:37:47Z</updated>
    <id>tag:github.com,2024-03-31:/TencentARC/BrushNet</id>
    <link href="https://github.com/TencentARC/BrushNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official implementation of paper &#34;BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BrushNet&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the implementation of the paper &#34;BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion&#34;&lt;/p&gt; &#xA;&lt;p&gt;Keywords: Image Inpainting, Diffusion Models, Image Generation&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/juxuan27&#34;&gt;Xuan Ju&lt;/a&gt;&lt;sup&gt;12&lt;/sup&gt;, &lt;a href=&#34;https://alvinliu0.github.io/&#34;&gt;Xian Liu&lt;/a&gt;&lt;sup&gt;12&lt;/sup&gt;, &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=HzemVzoAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Yuxuan Bian&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/YingShanProfile/&#34;&gt;Ying Shan&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://cure-lab.github.io/&#34;&gt;Qiang Xu&lt;/a&gt;&lt;sup&gt;2*&lt;/sup&gt;&lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;ARC Lab, Tencent PCG &lt;sup&gt;2&lt;/sup&gt;The Chinese University of Hong Kong &lt;sup&gt;*&lt;/sup&gt;Corresponding Author&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://tencentarc.github.io/BrushNet/&#34;&gt;üåêProject Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2403.06976&#34;&gt;üìúArxiv&lt;/a&gt; | &lt;a href=&#34;https://forms.gle/9TgMZ8tm49UYsZ9s5&#34;&gt;üóÑÔ∏èData&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/file/d/1IkEBWcd2Fui2WHcckap4QFPcCI0gkHBh/view&#34;&gt;üìπVideo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/TencentARC/BrushNet&#34;&gt;ü§óHugging Face Demo&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üìñ Table of Contents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#%EF%B8%8F-method-overview&#34;&gt;üõ†Ô∏è Method Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#-getting-started&#34;&gt;üöÄ Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#environment-requirement-&#34;&gt;Environment Requirement üåç&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#data-download-%EF%B8%8F&#34;&gt;Data Download ‚¨áÔ∏è&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#-running-scripts&#34;&gt;üèÉüèº Running Scripts&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#training-&#34;&gt;Training ü§Ø&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#inference-&#34;&gt;Inference üìú&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#evaluation-&#34;&gt;Evaluation üìè&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#-cite-us&#34;&gt;ü§ùüèº Cite Us&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#-acknowledgement&#34;&gt;üíñ Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release trainig and inference code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release checkpoint (sdv1.5)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release checkpoint (sdxl)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release evluation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release gradio demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Method Overview&lt;/h2&gt; &#xA;&lt;p&gt;BrushNet is a diffusion-based text-guided image inpainting model that can be plug-and-play into any pre-trained diffusion model. Our architectural design incorporates two key insights: (1) dividing the masked image features and noisy latent reduces the model&#39;s learning load, and (2) leveraging dense per-pixel control over the entire pre-trained model enhances its suitability for image inpainting tasks. More analysis can be found in the main paper.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/examples/brushnet/src/model.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Requirement üåç&lt;/h3&gt; &#xA;&lt;p&gt;BrushNet has been implemented and tested on Pytorch 1.12.1 with python 3.9.&lt;/p&gt; &#xA;&lt;p&gt;Clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/TencentARC/BrushNet.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend you first use &lt;code&gt;conda&lt;/code&gt; to create virtual environment, and install &lt;code&gt;pytorch&lt;/code&gt; following &lt;a href=&#34;https://pytorch.org/&#34;&gt;official instructions&lt;/a&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n diffusers python=3.9 -y&#xA;conda activate diffusers&#xA;python -m pip install --upgrade pip&#xA;pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can install diffusers (implemented in this repo) with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, you can install required packages thourgh:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd examples/brushnet/&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data Download ‚¨áÔ∏è&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can download the BrushData and BrushBench &lt;a href=&#34;https://forms.gle/9TgMZ8tm49UYsZ9s5&#34;&gt;here&lt;/a&gt; (as well as the EditBench we re-processed), which are used for training and testing the BrushNet. By downloading the data, you are agreeing to the terms and conditions of the license. The data structure should be like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;|-- data&#xA;    |-- BrushData&#xA;        |-- 00200.tar&#xA;        |-- 00201.tar&#xA;        |-- ...&#xA;    |-- BrushDench&#xA;        |-- images&#xA;        |-- mapping_file.json&#xA;    |-- EditBench&#xA;        |-- images&#xA;        |-- mapping_file.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Noted: &lt;em&gt;We only provide a part of the BrushData due to the space limit. Please write an email to &lt;a href=&#34;mailto:juxuan.27@gmail.com&#34;&gt;juxuan.27@gmail.com&lt;/a&gt; if you need the full dataset.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Checkpoints&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checkpoints of BrushNet can be downloaded from &lt;a href=&#34;https://drive.google.com/drive/folders/1fqmS1CEOvXCxNWFrsSYd_jHYXxrydh1n?usp=drive_link&#34;&gt;here&lt;/a&gt;. The ckpt folder contains our pretrained checkpoints and pretrinaed Stable Diffusion checkpoint (e.g., realisticVisionV60B1_v51VAE from &lt;a href=&#34;https://civitai.com/&#34;&gt;Civitai&lt;/a&gt;). You can use &lt;code&gt;scripts/convert_original_stable_diffusion_to_diffusers.py&lt;/code&gt; to process other models downloaded from Civitai. The data structure should be like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;|-- data&#xA;    |-- BrushData&#xA;    |-- BrushDench&#xA;    |-- EditBench&#xA;    |-- ckpt&#xA;        |-- realisticVisionV60B1_v51VAE&#xA;            |-- model_index.json&#xA;            |-- vae&#xA;            |-- ...&#xA;        |-- segmentation_mask_brushnet_ckpt&#xA;        |-- random_mask_brushnet_ckpt&#xA;        |-- ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The checkpoint in &lt;code&gt;segmentation_mask_brushnet_ckpt&lt;/code&gt; provides checkpoints trained on BrushData, which has segmentation prior (mask are with the same shape of objects). The &lt;code&gt;random_mask_brushnet_ckpt&lt;/code&gt; provides a more general ckpt for random mask shape.&lt;/p&gt; &#xA;&lt;h2&gt;üèÉüèº Running Scripts&lt;/h2&gt; &#xA;&lt;h3&gt;Training ü§Ø&lt;/h3&gt; &#xA;&lt;p&gt;You can train with segmentation mask using the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch examples/brushnet/train_brushnet.py \&#xA;--pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \&#xA;--output_dir runs/logs/brushnet_segmentationmask \&#xA;--train_data_dir data/BrushData \&#xA;--resolution 512 \&#xA;--learning_rate 1e-5 \&#xA;--train_batch_size 2 \&#xA;--tracker_project_name brushnet \&#xA;--report_to tensorboard \&#xA;--resume_from_checkpoint latest \&#xA;--validation_steps 300&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use custom dataset, you can process your own data to the format of BrushData and revise &lt;code&gt;--train_data_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can train with random mask using the script (by adding &lt;code&gt;--random_mask&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch examples/brushnet/train_brushnet.py \&#xA;--pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \&#xA;--output_dir runs/logs/brushnet_randommask \&#xA;--train_data_dir data/BrushData \&#xA;--resolution 512 \&#xA;--learning_rate 1e-5 \&#xA;--train_batch_size 2 \&#xA;--tracker_project_name brushnet \&#xA;--report_to tensorboard \&#xA;--resume_from_checkpoint latest \&#xA;--validation_steps 300 \&#xA;--random_mask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference üìú&lt;/h3&gt; &#xA;&lt;p&gt;You can inference with the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/brushnet/test_brushnet.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since BrushNet is trained on Laion, it can only guarantee the performance on general scenarios. We recommend you train on your own data (e.g., product exhibition, virtual try-on) if you have high-quality industrial application requirements. We would also be appreciate if you would like to contribute your trained model!&lt;/p&gt; &#xA;&lt;p&gt;You can also inference through gradio demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/brushnet/app_brushnet.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation üìè&lt;/h3&gt; &#xA;&lt;p&gt;You can evaluate using the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/brushnet/evaluate_brushnet.py \&#xA;--brushnet_ckpt_path data/ckpt/segmentation_mask_brushnet_ckpt \&#xA;--image_save_path runs/evaluation_result/BrushBench/brushnet_segmask/inside \&#xA;--mapping_file data/BrushBench/mapping_file.json \&#xA;--base_dir data/BrushBench \&#xA;--mask_key inpainting_mask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;--mask_key&lt;/code&gt; indicates which kind of mask to use, &lt;code&gt;inpainting_mask&lt;/code&gt; for inside inpainting and &lt;code&gt;outpainting_mask&lt;/code&gt; for outside inpainting. The evaluation results (images and metrics) will be saved in &lt;code&gt;--image_save_path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Noted that you need to ignore the nsfw detector in &lt;code&gt;src/diffusers/pipelines/brushnet/pipeline_brushnet.py#1261&lt;/code&gt; to get the correct evaluation results. Moreover, we find different machine may generate different images, thus providing the results on our machine &lt;a href=&#34;https://drive.google.com/drive/folders/1dK3oIB2UvswlTtnIS1iHfx4s57MevWdZ?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ùüèº Cite Us&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{ju2024brushnet,&#xA;  title={BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion}, &#xA;  author={Xuan Ju and Xian Liu and Xintao Wang and Yuxuan Bian and Ying Shan and Qiang Xu},&#xA;  year={2024},&#xA;  eprint={2403.06976},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üíñ Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span id=&#34;acknowledgement&#34;&gt;&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our code is modified based on &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;, thanks to all the contributors!&lt;/p&gt;</summary>
  </entry>
</feed>