<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-06T01:44:03Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>abacaj/code-eval</title>
    <updated>2023-07-06T01:44:03Z</updated>
    <id>tag:github.com,2023-07-06:/abacaj/code-eval</id>
    <link href="https://github.com/abacaj/code-eval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run evaluation on LLMs using human-eval benchmark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;code-eval&lt;/h1&gt; &#xA;&lt;h2&gt;What&lt;/h2&gt; &#xA;&lt;p&gt;This is a repo I use to run human-eval on code models, adjust as needed. Some scripts were adjusted from wizardcoder repo (&lt;code&gt;process_eval.py&lt;/code&gt;). The evaluation code is duplicated in several files, mostly to handle edge cases around model tokenizing and loading (might eventually clean it up).&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Table is sorted by pass@1 score.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;size&lt;/th&gt; &#xA;   &lt;th&gt;pass@1&lt;/th&gt; &#xA;   &lt;th&gt;pass@10&lt;/th&gt; &#xA;   &lt;th&gt;screenshot&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/sahil2801/replit-code-instruct-glaive&#34;&gt;sahil2801/replit-code-instruct-glaive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;63.5%&lt;/td&gt; &#xA;   &lt;td&gt;67%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/abacaj/code-eval/assets/7272343/6fd7527d-0dc4-4b48-8a57-ad0373074bc5&#34; alt=&#34;instruct-glaive&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/WizardLM/WizardCoder-15B-V1.0&#34;&gt;WizardCoder-15B-V1.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15B&lt;/td&gt; &#xA;   &lt;td&gt;57%&lt;/td&gt; &#xA;   &lt;td&gt;68.9%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/abacaj/code-eval/assets/7272343/0b941ff8-b474-4236-bbc0-89d925bbd34e&#34; alt=&#34;wizardcoder&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoder&#34;&gt;bigcode/starcoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15B&lt;/td&gt; &#xA;   &lt;td&gt;34.6%&lt;/td&gt; &#xA;   &lt;td&gt;48.7%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/abacaj/code-eval/assets/7272343/eb5df978-f56b-4557-a433-8b8fa863a059&#34; alt=&#34;starcoder&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openchat/opencoderplus&#34;&gt;openchat/opencoderplus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15B&lt;/td&gt; &#xA;   &lt;td&gt;27.3%&lt;/td&gt; &#xA;   &lt;td&gt;43.9%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/abacaj/code-eval/assets/7272343/1fa9f5ef-941b-4ea8-981e-c3f258c03fee&#34; alt=&#34;opencoder&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/teknium/Replit-v1-CodeInstruct-3B&#34;&gt;teknium/Replit-v1-CodeInstruct-3B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;25.8%&lt;/td&gt; &#xA;   &lt;td&gt;42.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/abacaj/code-eval/assets/7272343/4fca98d8-2c22-43ce-9639-e998ecb4fedc&#34; alt=&#34;replit-codeinstruct-v1&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/teknium/Replit-v2-CodeInstruct-3B&#34;&gt;teknium/Replit-v2-CodeInstruct-3B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;21.5%&lt;/td&gt; &#xA;   &lt;td&gt;31%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/abacaj/code-eval/assets/7272343/655aaa1d-0715-4fcd-b9ba-a22b5fddb215&#34; alt=&#34;replit-codeinstruct-v2&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/replit/replit-code-v1-3b&#34;&gt;replit-code-v1-3b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;17.1%&lt;/td&gt; &#xA;   &lt;td&gt;29.8%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/abacaj/code-eval/assets/7272343/6b387aa8-db60-4f04-b458-35b010b1145c&#34; alt=&#34;replit-code-v1&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b&#34;&gt;mpt-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;15.9%&lt;/td&gt; &#xA;   &lt;td&gt;23.7%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/abacaj/code-eval/assets/7272343/16965905-a368-4254-aeab-5e44126eba84&#34; alt=&#34;mpt-7b&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Salesforce/xgen-7b-8k-base&#34;&gt;xgen-7b-8k-base&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;14.9%&lt;/td&gt; &#xA;   &lt;td&gt;22.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/abacaj/code-eval/assets/7272343/995c84a9-ee69-43bf-8502-a74eba1d927a&#34; alt=&#34;xgen-7b-8k-base&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-30b&#34;&gt;mpt-30b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;30B&lt;/td&gt; &#xA;   &lt;td&gt;pending&lt;/td&gt; &#xA;   &lt;td&gt;pending&lt;/td&gt; &#xA;   &lt;td&gt;pending&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Why is there a discrepancy on some of the scores between official numbers?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Because it is not obvious or published what prompt or processing the official models used to conduct their evaluation on this benchmark. The goal here is to try and best reproduce those numbers, in many cases it is possible to get very close to the published numbers.&lt;/p&gt; &#xA;&lt;p&gt;All of the scores here were run independently of any published numbers and are reproducible by cloning the repo and following the setup.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Why do some models have a filter_code post generation step?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Base models can in many cases repeat outputs, breaking the benchmark scores. Instruct models don&#39;t have this problem and so you won&#39;t see this step, they tend to output a end of sequence token.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Create python environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m venv env &amp;amp;&amp;amp; source env/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the eval script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# replace script file name for various models:&#xA;# eval_wizard.py&#xA;# eval_opencode.py&#xA;# eval_mpt.py&#xA;# eval_starcoder.py&#xA;# eval_replit.py&#xA;# eval_replit_glaive.py&#xA;# eval_replit_instruct.py&#xA;&#xA;python eval_wizard.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Process the jsonl file to extract code samples from model completions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Only wizard &amp;amp; opencoder require this, they return markdown output with code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# replace args for various models:&#xA;# --path results/wizard --out_path results/wizard/eval.jsonl&#xA;# --path results/opencode --out_path results/opencode/eval.jsonl&#xA;&#xA;python process_eval.py --path results/wizard --out_path results/wizard/processed.jsonl --add_prompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then get the results&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# replace args for various models:&#xA;# results/wizard/processed.jsonl&#xA;# results/starcoder/eval.jsonl&#xA;# results/mpt/eval.jsonl&#xA;# results/opencode/processed.jsonl&#xA;# results/replit_instruct/eval.jsonl&#xA;# results/replit_glaive/eval.jsonl&#xA;# results/replit/eval.jsonl&#xA;&#xA;evaluate_functional_correctness results/wizard/processed.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>0hq/tinyvector</title>
    <updated>2023-07-06T01:44:03Z</updated>
    <id>tag:github.com,2023-07-06:/0hq/tinyvector</id>
    <link href="https://github.com/0hq/tinyvector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A tiny nearest-neighbor embedding database built with SQLite and Pytorch. (In development!)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/0hq/tinyvector/raw/main/assets/TINYVECTORLOGO.png?raw=true&#34; alt=&#34;tinyvector logo&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;tinyvector - the tiny, least-dumb, speedy vector embedding database&lt;/b&gt;. &lt;br&gt; No, you don&#39;t need a vector database. You need tinyvector. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;i&gt;In pre-release: prod-ready by late-July.&lt;/i&gt; &lt;b&gt;&lt;i&gt;Still in development!&lt;/i&gt;&lt;/b&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tiny&lt;/strong&gt;: It&#39;s in the name. It&#39;s just a Flask server, SQLite DB, and Numpy indexes. Extremely easy to customize, under 500 lines of code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Tinyvector wlll have comparable speed to advanced vector databases when it comes to speed on small to medium datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vertically Scales&lt;/strong&gt;: Tinyvector stores all indexes in memory for fast querying. Very easy to scale up to 100 million+ vector dimensions without issue.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: MIT Licensed, free forever.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Soon&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Powerful Queries&lt;/strong&gt;: Tinyvector is being upgraded with full SQL querying functionality, something missing from most other databases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integrated Models&lt;/strong&gt;: Soon you won&#39;t have to bring your own vectors, just generate them on the server automaticaly. Will support SBert, Hugging Face models, OpenAI, Cohere, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python/JS Client&lt;/strong&gt;: We&#39;ll add a comprehensive Python and Javascript package for easy integration with tinyvector in the next two weeks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Versions&lt;/h2&gt; &#xA;&lt;p&gt;🦀 tinyvector in Rust: &lt;a href=&#34;https://github.com/m1guelpf/tinyvector-rs&#34;&gt;tinyvector-rs&lt;/a&gt;&lt;br&gt; 🐍 tinyvector in Python: &lt;a href=&#34;https://github.com/0hq/tinyvector&#34;&gt;tinyvector&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;We&#39;re better than ...&lt;/h2&gt; &#xA;&lt;p&gt;In most cases, most vector databases are overkill for something simple like:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Using embeddings to chat with your documents. Most document search is nowhere close to what you&#39;d need to justify accelerating search speed with &lt;a href=&#34;https://github.com/nmslib/hnswlib&#34;&gt;HNSW&lt;/a&gt; or &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;FAISS&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Doing search for your website or store. Unless you&#39;re selling 1,000,000 items, you don&#39;t need Pinecone.&lt;/li&gt; &#xA; &lt;li&gt;Performing complex search queries on a very large database. Even if you have 2 million embeddings, this might still be the better option due to vector databases struggling with complex filtering. Tinyvector doesn&#39;t support metadata/filtering just yet, but it&#39;s very easy for you to add that yourself.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Embeddings?&lt;/h2&gt; &#xA;&lt;p&gt;What are embeddings?&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;As simple as possible: Embeddings are a way to compare similar things, in the same way humans compare similar things, by converting text into a small list of numbers. Similar pieces of text will have similar numbers, different ones have very different numbers.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Read OpenAI&#39;s &lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings/what-are-embeddings&#34;&gt;explanation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Get involved&lt;/h2&gt; &#xA;&lt;p&gt;tinyvector is going to be growing a lot (don&#39;t worry, will still be tiny). Feel free to make a PR and contribute. If you have questions, just mention &lt;a href=&#34;https://twitter.com/willdepue&#34;&gt;@willdepue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some ideas for first pulls:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add metadata and allow querying/filtering. This is especially important since a lot vector databases literally don&#39;t have a WHERE clause lol (or just an extremely weak one). Not a problem here. &lt;a href=&#34;https://www.pinecone.io/learn/vector-search-filtering&#34;&gt;Read more about this.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Rethinking SQLite and choosing something. NOSQL feels fitting for embeddings?&lt;/li&gt; &#xA; &lt;li&gt;Add embedding functions for easy adding text (sentence transformers, OpenAI, Cohere, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Let&#39;s start GPU accelerating with a Pytorch index. GPUs are great at matmuls -&amp;gt; NN search with a fused kernel. Let&#39;s put 32 million vectors on a single GPU.&lt;/li&gt; &#xA; &lt;li&gt;Help write unit and integration tests.&lt;/li&gt; &#xA; &lt;li&gt;See all &lt;a href=&#34;https://github.com/0hq/tinyvector/issues&#34;&gt;active issues&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Known Issues&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Major bugs:&#xA;Data corruption SQLite error? Stored vectors end up changing. Replicate by creating a table, inserting vectors, creating an index and then screwing around till an error happens. Dims end up unmatched (might be the blob functions or the norm functions most likely, but doesn&#39;t explain why the database is changing).&#xA;PCA is not tested, neither is immutable Brute Force index.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/0hq/tinyvector/main/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>shikras/shikra</title>
    <updated>2023-07-06T01:44:03Z</updated>
    <id>tag:github.com,2023-07-06:/shikras/shikra</id>
    <link href="https://github.com/shikras/shikra" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shikras/shikra/main/#&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/shikras/shikra/main/assets/logo.png&#34; alt=&#34;Logo&#34; width=&#34;130&#34;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;h4 align=&#34;center&#34;&gt;&lt;font color=&#34;#966661&#34;&gt;Shikra&lt;/font&gt;: Unleashing Multimodal LLM’s Referential Dialogue Magic&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/shikras/shikra&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://arxiv.org/abs/2306.15195&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;font color=&#34;#966661&#34;&gt;&lt;strong&gt;Shikra&lt;/strong&gt;&lt;/font&gt;, an MLLM designed to kick off &lt;strong&gt;referential dialogue&lt;/strong&gt; by excelling in spatial coordinate inputs/outputs in natural language, &lt;strong&gt;without&lt;/strong&gt; additional vocabularies, position encoders, pre-/post-detection, or external plug-in models.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shikras/shikra/main/assets/teaser.jpg&#34; alt=&#34;teaser&#34; width=&#34;300px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;[07/03] We released the code, &lt;a href=&#34;https://drive.google.com/file/d/1CNLu1zJKPtliQEYCZlZ8ykH00ppInnyN/view?usp=drive_link&#34;&gt;data&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/shikras/shikra-7b-delta-v1&#34;&gt;Shikra-7B checkpoint&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[06/28] We released &lt;strong&gt;Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic&lt;/strong&gt;, which is designed to kick off &lt;strong&gt;referential dialogue&lt;/strong&gt;. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2306.15195&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/shikras/shikra/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/shikras/shikra/main/#shikra-weights&#34;&gt;Shikra weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shikras/shikra/raw/main/docs/data.md&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n shikra python=3.10&#xA;conda activate shikra&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;configure accelerate&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Shikra weights&lt;/h2&gt; &#xA;&lt;p&gt;We release Shikra weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the Shikra weights.&lt;/p&gt; &#xA;&lt;p&gt;Instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the original LLaMA weights in the huggingface format by following the instructions &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the following scripts to get weights by applying our delta (&lt;a href=&#34;https://huggingface.co/shikras/shikra-7b-delta-v1&#34;&gt;shikra-7b-delta&lt;/a&gt;). It will automatically download delta weights from our Hugging Face account.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python mllm/models/shikra/apply_delta.py \&#xA;    --base /path/to/llama-7b \&#xA;    --target /output/path/to/shikra-7b \&#xA;    --delta shikras/shikra-7b-delta-v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;p&gt;After preparing &lt;a href=&#34;https://github.com/shikras/shikra/raw/main/docs/data.md&#34;&gt;data&lt;/a&gt;, you can train the model using the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch --num_processes 4 \&#xA;        --main_process_port 23786 \&#xA;        mllm/pipeline/finetune.py \&#xA;        config/shikra_pretrain_final19_stage2.py \&#xA;        --cfg-options model_args.model_name_or_path=/path/to/init/checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;mmengine style args and huggingface:Trainer args are supported. For example, you can change epoch and output_dir like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch --num_processes 4 \&#xA;        --main_process_port 23786 \&#xA;        mllm/pipeline/finetune.py \&#xA;        config/shikra_pretrain_final19_stage2.py \&#xA;        --cfg-options model_args.model_name_or_path=/path/to/init/checkpoint \&#xA;        --num_train_epochs 3 \&#xA;        --output_dir /path/to/new/exp/dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;--cfg-options a=balabala b=balabala&lt;/code&gt; is mmengine style argument. They will overwrite the argument predefined in config file. And &lt;code&gt;--num_train_epochs&lt;/code&gt; , &lt;code&gt;--output_dir&lt;/code&gt; are huggingface:Trainer argument.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;After preparing &lt;a href=&#34;https://github.com/shikras/shikra/raw/main/docs/data.md&#34;&gt;data&lt;/a&gt;, you can inference the model using the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch --num_processes 4 \&#xA;        --main_process_port 23786 \&#xA;        mllm/pipeline/finetune.py \&#xA;        config/shikra_eval_multi_pope.py \&#xA;        --cfg-options model_args.model_name_or_path=/path/to/checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;mmengine style args and huggingface:Trainer args are supported. for example, you can change eval batchsize like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch --num_processes 4 \&#xA;        --main_process_port 23786 \&#xA;        mllm/pipeline/finetune.py \&#xA;        config/shikra_eval_multi_pope.py \&#xA;        --cfg-options model_args.model_name_or_path=/path/to/checkpoint \&#xA;        --per_device_eval_batch_size 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;--cfg-options a=balabala b=balabala&lt;/code&gt; is mmengine style argument. They will overwrite the argument predefined in config file. And &lt;code&gt;--per_device_eval_batch_size&lt;/code&gt; is huggingface:Trainer argument.&lt;/p&gt; &#xA;&lt;p&gt;the prediction result will be saved in &lt;code&gt;output_dir/multitest_xxxx_extra_prediction.jsonl&lt;/code&gt;, which hold the same order as the input dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/shikras/shikra/main/assets/shikra_case_1.jpg&#34; alt=&#34;shikra_case_1&#34; style=&#34;zoom: 25%;&#34;&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{chen2023shikra,&#xA;  title={Shikra: Unleashing Multimodal LLM&#39;s Referential Dialogue Magic},&#xA;  author={Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},&#xA;  journal={arXiv preprint arXiv:2306.15195},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt; and &lt;a href=&#34;https://github.com/hiyouga/ChatGLM-Efficient-Tuning&#34;&gt;ChatGLM-Efficient-Tuning&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt;</summary>
  </entry>
</feed>