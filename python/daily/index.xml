<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-11T01:37:50Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>garrettj403/SciencePlots</title>
    <updated>2022-10-11T01:37:50Z</updated>
    <id>tag:github.com,2022-10-11:/garrettj403/SciencePlots</id>
    <link href="https://github.com/garrettj403/SciencePlots" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Matplotlib styles for scientific plotting&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Science Plots&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/SciencePlots&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/SciencePlots.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zenodo.org/badge/latestdoi/144605189&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/144605189.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Matplotlib styles for scientific figures&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo has Matplotlib styles to format your figures for scientific papers, presentations and theses.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/garrettj403/SciencePlots/raw/master/examples/figures/fig1.jpg&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;You can find &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/Gallery&#34;&gt;the full gallery of included styles here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to install SciencePlots is by using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# to install the lastest release (from PyPI)&#xA;pip install SciencePlots&#xA;&#xA;# to install the latest commit (from GitHub)&#xA;pip install git+https://github.com/garrettj403/SciencePlots&#xA;&#xA;# to clone and install from a local copy&#xA;git clone https://github.com/garrettj403/SciencePlots.git&#xA;cd SciencePlots&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The pip installation will automatically move all of the Matplotlib style files &lt;code&gt;*.mplstyle&lt;/code&gt; into the appropriate directory on your computer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SciencePlots requires Latex (&lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/FAQ#installing-latex&#34;&gt;see Latex installation instructions&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;If you would like to use CJK fonts, you will need to install these font separately (&lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/FAQ#installing-cjk-fonts&#34;&gt;see CJK font installation instructions&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt; for more information and troubleshooting.&lt;/p&gt; &#xA;&lt;h2&gt;Using the Styles&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;&#34;science&#34;&lt;/code&gt; is the primary style in this repo. Whenever you want to use it, simply add the following to the top of your python script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt&#xA;&#xA;plt.style.use(&#39;science&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also combine multiple styles together by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.style.use([&#39;science&#39;,&#39;ieee&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this case, the &lt;code&gt;ieee&lt;/code&gt; style will override some of the parameters from the &lt;code&gt;science&lt;/code&gt; style in order to configure the plot for IEEE papers (column width, fontsizes, etc.).&lt;/p&gt; &#xA;&lt;p&gt;To use any of the styles temporarily, you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with plt.style.context(&#39;science&#39;):&#xA;    plt.figure()&#xA;    plt.plot(x, y)&#xA;    plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;The basic &lt;code&gt;science&lt;/code&gt; style is shown below:&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/garrettj403/SciencePlots/raw/master/examples/figures/fig1.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;It can be cascaded with other styles to fine-tune the appearance. For example, the &lt;code&gt;science&lt;/code&gt; + &lt;code&gt;notebook&lt;/code&gt; styles (intended for Jupyter notebooks):&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/garrettj403/SciencePlots/raw/master/examples/figures/fig10.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/Gallery&#34;&gt;the project Wiki&lt;/a&gt; for a full list of available styles.&lt;/p&gt; &#xA;&lt;h2&gt;Specific Styles for Academic Journals&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;science&lt;/code&gt; + &lt;code&gt;ieee&lt;/code&gt; styles for IEEE papers:&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/garrettj403/SciencePlots/raw/master/examples/figures/fig2a.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;IEEE requires figures to be readable when printed in black and white. The &lt;code&gt;ieee&lt;/code&gt; style also sets the figure width to fit within one column of an IEEE paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;science&lt;/code&gt; + &lt;code&gt;nature&lt;/code&gt; styles for Nature articles:&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/garrettj403/SciencePlots/raw/master/examples/figures/fig2c.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nature recommends sans-serif fonts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other languages&lt;/h2&gt; &#xA;&lt;p&gt;SciencePlots currently supports &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/Gallery#traditional-chinese&#34;&gt;traditional Chinese&lt;/a&gt;, &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/Gallery#simplified-chinese&#34;&gt;simplified Chinese&lt;/a&gt;, &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/Gallery#japanese&#34;&gt;Japanese&lt;/a&gt;, &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/Gallery#korean&#34;&gt;Korean&lt;/a&gt; and &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/Gallery#russian&#34;&gt;Russian&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example: Traditional Chinese (&lt;code&gt;science&lt;/code&gt; + &lt;code&gt;no-latex&lt;/code&gt; + &lt;code&gt;cjk-tc-font&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/garrettj403/SciencePlots/raw/master/examples/figures/fig14a.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/FAQ#installing-cjk-fonts&#34;&gt;FAQ&lt;/a&gt; for information on installing CJK fonts.&lt;/p&gt; &#xA;&lt;h2&gt;Other color cycles&lt;/h2&gt; &#xA;&lt;p&gt;SciencePlots comes with a variety of different color cycles. For a full list, &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/Gallery#color-cycles&#34;&gt;see the project Wiki&lt;/a&gt;. Two examples are shown below.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;bright&lt;/code&gt; color cycle (color blind safe):&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/garrettj403/SciencePlots/raw/master/examples/figures/fig6.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;The &lt;code&gt;high-vis&lt;/code&gt; color cycle:&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/garrettj403/SciencePlots/raw/master/examples/figures/fig4.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;h2&gt;Help and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please feel free to contribute to the SciencePlots repo! For example, it would be good to add new styles for different journals and add new color cycles. Before starting a new style or making any changes, please create an issue through the &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/issues&#34;&gt;GitHub issue tracker&lt;/a&gt;. That way we can discuss if the changes are necessary and the best approach.&lt;/p&gt; &#xA;&lt;p&gt;If you need any help with SciencePlots, please first check the &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt; and search through the &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/issues&#34;&gt;previous GitHub issues&lt;/a&gt;. If you can&#39;t find an answer, create a new issue through the &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/issues&#34;&gt;GitHub issue tracker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can checkout &lt;a href=&#34;https://matplotlib.org/tutorials/introductory/customizing.html&#34;&gt;Matplotlib&#39;s documentation&lt;/a&gt; for more information on plotting settings.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;You can find &lt;a href=&#34;https://github.com/garrettj403/SciencePlots/wiki/FAQ&#34;&gt;the FAQ in the project Wiki.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;SciencePlots in Academic Papers&lt;/h2&gt; &#xA;&lt;p&gt;The following papers use &lt;code&gt;SciencePlots&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;J. Garrett, and E. Tong, &lt;a href=&#34;https://ieeexplore.ieee.org/document/9727077&#34;&gt;&#34;Measuring Cryogenic Waveguide Loss in the Terahertz Regime,&#34;&lt;/a&gt; &lt;em&gt;IEEE Trans. THz Sci. Technol.&lt;/em&gt;, vol. 12, no. 3, pp. 293-299, May 2022.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Y. Liu, X. Liu, and Y. Sun, &lt;a href=&#34;https://doi.org/10.1016/j.sedgeo.2021.105980&#34;&gt;&#34;QGrain: An open-source and easy-to-use software for the comprehensive analysis of grain size distributions&#34;&lt;/a&gt;, &lt;em&gt;Sedimentary Geology&lt;/em&gt;, vol. 423, 105980, Aug. 2021.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;J. Garrett, and E. Tong, &lt;a href=&#34;https://ieeexplore.ieee.org/document/9447194&#34;&gt;&#34;A Dispersion-Compensated Algorithm for the Analysis of Electromagnetic Waveguides,&#34;&lt;/a&gt; &lt;em&gt;IEEE Signal Process. Lett.&lt;/em&gt;, vol. 28, pp. 1175-1179, Jun. 2021.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;G. Jegannathan, &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&#34;https://www.mdpi.com/1424-8220/20/24/7105&#34;&gt;&#34;Current-Assisted SPAD with Improved p-n Junction and Enhanced NIR Performance&#34;&lt;/a&gt;, &lt;em&gt;Sensors&lt;/em&gt;, Dec 2020. (&lt;a href=&#34;https://www.mdpi.com/1424-8220/20/24/7105&#34;&gt;open access&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;H. Tian, &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&#34;https://pubs.acs.org/doi/abs/10.1021/acs.jcim.0c00485&#34;&gt;&#34;ivis Dimensionality Reduction Framework for Biomacromolecular Simulations&#34;&lt;/a&gt;, &lt;em&gt;J. Chem. Inf. Model.&lt;/em&gt;, Aug 2020. (&lt;a href=&#34;https://arxiv.org/pdf/2004.10718.pdf&#34;&gt;open access&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;P. Stoltz, &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&#34;https://aip.scitation.org/doi/10.1063/5.0020781&#34;&gt;&#34;A new simple algorithm for space charge limited emission,&#34;&lt;/a&gt; &lt;em&gt;Phys. Plasmas&lt;/em&gt;, vol. 27, no. 9, pp. 093103, Sep. 2020. (&lt;a href=&#34;https://aip.scitation.org/doi/10.1063/5.0020781&#34;&gt;open access&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;J. Garrett, &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9050728&#34;&gt;&#34;A Nonlinear Transmission Line Model for Simulating Distributed SIS Frequency Multipliers,&#34;&lt;/a&gt; &lt;em&gt;IEEE Trans. THz Sci. Technol.&lt;/em&gt;, vol. 10, no. 3, pp. 246-255, May 2020. (&lt;a href=&#34;https://ora.ox.ac.uk/objects/uuid:5ca31c2c-a984-462c-b21a-3fe16eee0d9b/download_file?safe_filename=XXXX_final_JohnGarrett.pdf&amp;amp;type_of_work=Journal+article&#34;&gt;open access&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;J. Garrett, &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&#34;https://ieeexplore.ieee.org/document/8822760/&#34;&gt;&#34;Simulating the Behavior of a 230 GHz SIS Mixer Using Multi-Tone Spectral Domain Analysis,&#34;&lt;/a&gt; &lt;em&gt;IEEE Trans. THz Sci. Technol.&lt;/em&gt;, vol. 9, no. 9, pp. 540-548, Nov. 2019. (&lt;a href=&#34;https://ora.ox.ac.uk/objects/uuid:0fd4537d-258c-454a-bbfb-09b1bcd88d49/download_file?file_format=pdf&amp;amp;safe_filename=XXXX_final.pdf&amp;amp;type_of_work=Journal+article&#34;&gt;open access&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;J. Garrett, &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&#34;https://ieeexplore.ieee.org/document/8760521&#34;&gt;&#34;A Compact and Easy to Fabricate E-plane Waveguide Bend,&#34;&lt;/a&gt; &lt;em&gt;IEEE Microw. Wireless Compon. Lett.&lt;/em&gt;, vol. 29, no. 8, pp. 529-531, Aug. 2019. (&lt;a href=&#34;https://ora.ox.ac.uk/objects/uuid:496855f9-be2a-47cd-b498-1753d8033f50/download_file?file_format=pdf&amp;amp;safe_filename=Waveguide_Bend__IEEE_MWCL_.pdf&amp;amp;type_of_work=Journal+article&#34;&gt;open access&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;J. Garrett, &lt;a href=&#34;https://ora.ox.ac.uk/objects/uuid:d47fbf3b-1cf3-4e58-be97-767b9893066e/download_file?file_format=pdf&amp;amp;safe_filename=GarrettJ_DPhilThesis.pdf&amp;amp;type_of_work=Thesis&#34;&gt;&#34;A 230 GHz Focal Plane Array Using a Wide IF Bandwidth SIS Receiver,&#34;&lt;/a&gt; DPhil thesis, University of Oxford, Oxford, UK, 2018. (&lt;a href=&#34;https://ora.ox.ac.uk/objects/uuid:d47fbf3b-1cf3-4e58-be97-767b9893066e/download_file?file_format=pdf&amp;amp;safe_filename=GarrettJ_DPhilThesis.pdf&amp;amp;type_of_work=Thesis&#34;&gt;open access&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;SciencePlots&lt;/code&gt; in your paper/thesis, feel free to add it to the list!&lt;/p&gt; &#xA;&lt;h2&gt;Citing SciencePlots&lt;/h2&gt; &#xA;&lt;p&gt;You don&#39;t have to cite SciencePlots if you use it but it&#39;s nice if you do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{SciencePlots,&#xA;  author       = {John D. Garrett},&#xA;  title        = {{garrettj403/SciencePlots}},&#xA;  month        = sep,&#xA;  year         = 2021,&#xA;  publisher    = {Zenodo},&#xA;  version      = {1.0.9},&#xA;  doi          = {10.5281/zenodo.4106649},&#xA;  url          = {http://doi.org/10.5281/zenodo.4106649}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmengine</title>
    <updated>2022-10-11T01:37:50Z</updated>
    <id>tag:github.com,2022-10-11:/open-mmlab/mmengine</id>
    <link href="https://github.com/open-mmlab/mmengine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab Foundational Library for Training Deep Learning Models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg&#34; height=&#34;100&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/mmengine/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/mmengine&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/mmengine&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mmengine&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmengine/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmengine.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmengine/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmengine.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmengine/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmengine.svg?sanitize=true&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmengine/issues/new/choose&#34;&gt;ðŸ¤”Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmengine/main/README_zh-CN.md&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;MMEngine is a foundational library for training deep learning models based on PyTorch. It provides a solid engineering foundation and frees developers from writing redundant codes on workflows. It serves as the training engine of all OpenMMLab codebases, which support hundreds of algorithms in various research areas. Moreover, MMEngine is also generic to be applied to non-OpenMMLab projects.&lt;/p&gt; &#xA;&lt;p&gt;Major features:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;A universal and powerful runner&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Supports training different tasks with a small amount of code, e.g., ImageNet can be trained with only 80 lines of code (400 lines of the original PyTorch example)&lt;/li&gt; &#xA;   &lt;li&gt;Easily compatible with models from popular algorithm libraries such as TIMM, TorchVision, and Detectron2&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open architecture with unified interfaces&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Handles different algorithm tasks with unified APIs, e.g., implement a method and apply it to all compatible models.&lt;/li&gt; &#xA;   &lt;li&gt;Provides a unified abstraction for upper-level algorithm libraries, which supports various back-end devices such as Nvidia CUDA, Mac MPS, AMD, MLU, and more for model training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Customizable training process&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Defines the training process just like playing with Legos.&lt;/li&gt; &#xA;   &lt;li&gt;Provides rich components and strategies.&lt;/li&gt; &#xA;   &lt;li&gt;Complete controls on the training process with different levels of APIs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Before installing MMEngine, please ensure that PyTorch has been successfully installed following the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;official guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Install MMEngine&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U openmim&#xA;mim install mmengine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Verify the installation&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -c &#39;from mmengine.utils.dl_utils import collect_env;print(collect_env())&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Taking the training of a ResNet-50 model on the CIFAR-10 dataset as an example, we will use MMEngine to build a complete, configurable training and validation process in less than 80 lines of code.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Build Models&lt;/summary&gt; &#xA; &lt;p&gt;First, we need to define a &lt;strong&gt;model&lt;/strong&gt; which 1) inherits from &lt;code&gt;BaseModel&lt;/code&gt; and 2) accepts an additional argument &lt;code&gt;mode&lt;/code&gt; in the &lt;code&gt;forward&lt;/code&gt; method, in addition to those arguments related to the dataset.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;During training, the value of &lt;code&gt;mode&lt;/code&gt; is &#34;loss,&#34; and the &lt;code&gt;forward&lt;/code&gt; method should return a &lt;code&gt;dict&lt;/code&gt; containing the key &#34;loss&#34;.&lt;/li&gt; &#xA;  &lt;li&gt;During validation, the value of &lt;code&gt;mode&lt;/code&gt; is &#34;predict&#34;, and the forward method should return results containing both predictions and labels.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.nn.functional as F&#xA;import torchvision&#xA;from mmengine.model import BaseModel&#xA;&#xA;class MMResNet50(BaseModel):&#xA;    def __init__(self):&#xA;        super().__init__()&#xA;        self.resnet = torchvision.models.resnet50()&#xA;&#xA;    def forward(self, imgs, labels, mode):&#xA;        x = self.resnet(imgs)&#xA;        if mode == &#39;loss&#39;:&#xA;            return {&#39;loss&#39;: F.cross_entropy(x, labels)}&#xA;        elif mode == &#39;predict&#39;:&#xA;            return x, labels&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Build Datasets&lt;/summary&gt; &#xA; &lt;p&gt;Next, we need to create &lt;strong&gt;Dataset&lt;/strong&gt;s and &lt;strong&gt;DataLoader&lt;/strong&gt;s for training and validation. In this case, we simply use built-in datasets supported in TorchVision.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torchvision.transforms as transforms&#xA;from torch.utils.data import DataLoader&#xA;&#xA;norm_cfg = dict(mean=[0.491, 0.482, 0.447], std=[0.202, 0.199, 0.201])&#xA;train_dataloader = DataLoader(batch_size=32,&#xA;                              shuffle=True,&#xA;                              dataset=torchvision.datasets.CIFAR10(&#xA;                                  &#39;data/cifar10&#39;,&#xA;                                  train=True,&#xA;                                  download=True,&#xA;                                  transform=transforms.Compose([&#xA;                                      transforms.RandomCrop(32, padding=4),&#xA;                                      transforms.RandomHorizontalFlip(),&#xA;                                      transforms.ToTensor(),&#xA;                                      transforms.Normalize(**norm_cfg)&#xA;                                  ])))&#xA;val_dataloader = DataLoader(batch_size=32,&#xA;                            shuffle=False,&#xA;                            dataset=torchvision.datasets.CIFAR10(&#xA;                                &#39;data/cifar10&#39;,&#xA;                                train=False,&#xA;                                download=True,&#xA;                                transform=transforms.Compose([&#xA;                                    transforms.ToTensor(),&#xA;                                    transforms.Normalize(**norm_cfg)&#xA;                                ])))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Build Metrics&lt;/summary&gt; &#xA; &lt;p&gt;To validate and test the model, we need to define a &lt;strong&gt;Metric&lt;/strong&gt; called accuracy to evaluate the model. This metric needs inherit from &lt;code&gt;BaseMetric&lt;/code&gt; and implements the &lt;code&gt;process&lt;/code&gt; and &lt;code&gt;compute_metrics&lt;/code&gt; methods.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mmengine.evaluator import BaseMetric&#xA;&#xA;class Accuracy(BaseMetric):&#xA;    def process(self, data_batch, data_samples):&#xA;        score, gt = data_samples&#xA;        # Save the results of a batch to `self.results`&#xA;        self.results.append({&#xA;            &#39;batch_size&#39;: len(gt),&#xA;            &#39;correct&#39;: (score.argmax(dim=1) == gt).sum().cpu(),&#xA;        })&#xA;    def compute_metrics(self, results):&#xA;        total_correct = sum(item[&#39;correct&#39;] for item in results)&#xA;        total_size = sum(item[&#39;batch_size&#39;] for item in results)&#xA;        # Returns a dictionary with the results of the evaluated metrics,&#xA;        # where the key is the name of the metric&#xA;        return dict(accuracy=100 * total_correct / total_size)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Build a Runner&lt;/summary&gt; &#xA; &lt;p&gt;Finally, we can construct a &lt;strong&gt;Runner&lt;/strong&gt; with previously defined &lt;code&gt;Model&lt;/code&gt;, &lt;code&gt;DataLoader&lt;/code&gt;, and &lt;code&gt;Metrics&lt;/code&gt;, with some other configs, as shown below.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.optim import SGD&#xA;from mmengine.runner import Runner&#xA;&#xA;runner = Runner(&#xA;    model=MMResNet50(),&#xA;    work_dir=&#39;./work_dir&#39;,&#xA;    train_dataloader=train_dataloader,&#xA;    # a wapper to execute back propagation and gradient update, etc.&#xA;    optim_wrapper=dict(optimizer=dict(type=SGD, lr=0.001, momentum=0.9)),&#xA;    # set some training configs like epochs&#xA;    train_cfg=dict(by_epoch=True, max_epochs=5, val_interval=1),&#xA;    val_dataloader=val_dataloader,&#xA;    val_cfg=dict(),&#xA;    val_evaluator=dict(type=Accuracy),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Launch Training&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;runner.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improve MMEngine. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmengine/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmengine/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Projects in OpenMMLab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmclassification&#34;&gt;MMClassification&lt;/a&gt;: OpenMMLab image classification toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt;: OpenMMLab image and video editing toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;: OpenMMLab image and video generative models toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mingyuan-zhang/MotionDiffuse</title>
    <updated>2022-10-11T01:37:50Z</updated>
    <id>tag:github.com,2022-10-11:/mingyuan-zhang/MotionDiffuse</id>
    <link href="https://github.com/mingyuan-zhang/MotionDiffuse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model&lt;/h1&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?user=2QLD4fAAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Mingyuan Zhang&lt;/a&gt;&#xA;  &lt;sup&gt;1&lt;/sup&gt;*â€ƒ &#xA;  &lt;a href=&#34;https://caizhongang.github.io/&#34; target=&#34;_blank&#34;&gt;Zhongang Cai&lt;/a&gt;&#xA;  &lt;sup&gt;1,2&lt;/sup&gt;*â€ƒ &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?user=lSDISOcAAAAJ&amp;amp;hl=zh-CN&#34; target=&#34;_blank&#34;&gt;Liang Pan&lt;/a&gt;&#xA;  &lt;sup&gt;1&lt;/sup&gt;â€ƒ &#xA;  &lt;a href=&#34;https://hongfz16.github.io/&#34; target=&#34;_blank&#34;&gt;Fangzhou Hong&lt;/a&gt;&#xA;  &lt;sup&gt;1&lt;/sup&gt;â€ƒ &#xA;  &lt;a href=&#34;https://gxyes.github.io/&#34; target=&#34;_blank&#34;&gt;Xinying Guo&lt;/a&gt;&#xA;  &lt;sup&gt;1&lt;/sup&gt;â€ƒ &#xA;  &lt;a href=&#34;https://yanglei.me/&#34; target=&#34;_blank&#34;&gt;Lei Yang&lt;/a&gt;&#xA;  &lt;sup&gt;2&lt;/sup&gt;â€ƒ &#xA;  &lt;a href=&#34;https://liuziwei7.github.io/&#34; target=&#34;_blank&#34;&gt;Ziwei Liu&lt;/a&gt;&#xA;  &lt;sup&gt;1+&lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;sup&gt;1&lt;/sup&gt;S-Lab, Nanyang Technological Universityâ€ƒ &#xA;  &lt;sup&gt;2&lt;/sup&gt;SenseTime Researchâ€ƒ &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;   *equal contributionâ€ƒ &#xA;  &lt;sup&gt;+&lt;/sup&gt;corresponding author &#xA; &lt;/div&gt; &#xA; &lt;table&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;24%&#34;&gt;play the guitar&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;24%&#34;&gt;walk sadly&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;24%&#34;&gt;walk happily&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; width=&#34;24%&#34;&gt;check time&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mingyuan-zhang/MotionDiffuse/main/figures/gallery/gen_00.gif&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mingyuan-zhang/MotionDiffuse/main/figures/gallery/gen_03.gif&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mingyuan-zhang/MotionDiffuse/main/figures/gallery/gen_05.gif&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mingyuan-zhang/MotionDiffuse/main/figures/gallery/gen_06.gif&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA; &lt;p&gt;This repository contains the official implementation of &lt;em&gt;MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model&lt;/em&gt;.&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://mingyuan-zhang.github.io/projects/MotionDiffuse.html&#34; target=&#34;_blank&#34;&gt;[Project Page]&lt;/a&gt; â€¢ &lt;a href=&#34;https://arxiv.org/abs/2208.15001&#34; target=&#34;_blank&#34;&gt;[arXiv]&lt;/a&gt; â€¢ &lt;a href=&#34;https://youtu.be/U5PTnw490SA&#34; target=&#34;_blank&#34;&gt;[Video]&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;[10/2022] Code release for text-driven motion generation!&lt;/p&gt; &#xA;&lt;h2&gt;Text-driven Motion Generation&lt;/h2&gt; &#xA;&lt;p&gt;You may refer to &lt;a href=&#34;https://raw.githubusercontent.com/mingyuan-zhang/MotionDiffuse/main/text2motion/README.md&#34;&gt;readme&lt;/a&gt; for detailed introduction.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful for your research, please consider citing the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhang2022motiondiffuse,&#xA;  title={MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model},&#xA;  author={Zhang, Mingyuan and Cai, Zhongang and Pan, Liang and Hong, Fangzhou and Guo, Xinying and Yang, Lei and Liu, Ziwei},&#xA;  journal={arXiv preprint arXiv:2208.15001},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This study is supported by NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry Alignment Fund â€“ Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).&lt;/p&gt;</summary>
  </entry>
</feed>