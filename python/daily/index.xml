<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-19T01:33:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenBB-finance/OpenBB</title>
    <updated>2024-08-19T01:33:28Z</updated>
    <id>tag:github.com,2024-08-19:/OpenBB-finance/OpenBB</id>
    <link href="https://github.com/OpenBB-finance/OpenBB" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Investment Research for Everyone, Everywhere.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;img src=&#34;https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-light.svg?raw=true#gh-light-mode-only&#34; alt=&#34;OpenBB Terminal logo&#34; width=&#34;600&#34;&gt; &#xA;&lt;img src=&#34;https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only&#34; alt=&#34;OpenBB Terminal logo&#34; width=&#34;600&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/openbb_finance&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://discordapp.com/api/guilds/831165782750789672/widget.png?style=shield&#34; alt=&#34;Discord Shield&#34;&gt; &lt;a href=&#34;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&#34; alt=&#34;Open in Dev Containers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codespaces.new/OpenBB-finance/OpenBBTerminal&#34;&gt; &lt;img src=&#34;https://github.com/codespaces/badge.svg?sanitize=true&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/OpenBB-finance/OpenBBTerminal/blob/develop/examples/googleColab.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/openbb/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The first financial Platform that is free and fully open source.&lt;/p&gt; &#xA;&lt;p&gt;Offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.&lt;/p&gt; &#xA;&lt;p&gt;Sign up to the &lt;a href=&#34;https://my.openbb.co/login&#34;&gt;OpenBB Hub&lt;/a&gt; to get the most out of the OpenBB ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;We have also open source an AI financial analyst agent that can access all the data within OpenBB, and that repo can be found &lt;a href=&#34;https://github.com/OpenBB-finance/openbb-agents&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If you are looking for the first AI financial terminal for professionals, the OpenBB Terminal Pro can be found at &lt;a href=&#34;https://pro.openbb.co&#34;&gt;pro.openbb.co&lt;/a&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://pro.openbb.co&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://openbb.co/api/image?src=%252Fassets%252Fimages%252Fhome%252Fhero.png&amp;amp;width=2400&amp;amp;fit=cover&amp;amp;position=center&amp;amp;background%5B%5D=0&amp;amp;background%5B%5D=0&amp;amp;background%5B%5D=0&amp;amp;background%5B%5D=0&amp;amp;quality=100&amp;amp;compressionLevel=9&amp;amp;loop=0&amp;amp;delay=100&amp;amp;crop=null&#34; alt=&#34;Logo&#34; width=&#34;600&#34;&gt; &#xA; &lt;/div&gt; &lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- TABLE OF CONTENTS --&gt; &#xA;&lt;details closed=&#34;closed&#34;&gt; &#xA; &lt;summary&gt;&lt;h2 style=&#34;display: inline-block&#34;&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer&#34;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts&#34;&gt;Contacts&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history&#34;&gt;Star History&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;1. Installation&lt;/h2&gt; &#xA;&lt;p&gt;The OpenBB Platform can be installed as a &lt;a href=&#34;https://pypi.org/project/openbb/&#34;&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please find more about the installation process in the &lt;a href=&#34;https://docs.openbb.co/platform/installation&#34;&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;OpenBB Platform CLI installation&lt;/h3&gt; &#xA;&lt;p&gt;The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your terminal.&lt;/p&gt; &#xA;&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please find more about the installation process in the &lt;a href=&#34;https://docs.openbb.co/cli/installation&#34;&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The OpenBB Platform CLI offers an alternative to the former &lt;a href=&#34;https://github.com/OpenBB-finance/LegacyTerminal&#34;&gt;OpenBB Terminal&lt;/a&gt; as it has the same look and feel while offering the functionalities and extendability of the OpenBB Platform.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;2. Contributing&lt;/h2&gt; &#xA;&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ‚≠êÔ∏è)&lt;/p&gt; &#xA;&lt;h3&gt;Become a Contributor&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More information on our &lt;a href=&#34;https://docs.openbb.co/platform/developer_guide/contributing&#34;&gt;Contributing Documentation&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; &#xA;&lt;p&gt;Before creating a ticket make sure the one you are creating doesn&#39;t exist already &lt;a href=&#34;https://github.com/OpenBB-finance/OpenBB/issues&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D&#34;&gt;Report bug&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D&#34;&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D&#34;&gt;Request a feature&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Provide feedback&lt;/h3&gt; &#xA;&lt;p&gt;We are most active on &lt;a href=&#34;https://openbb.co/discord&#34;&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href=&#34;https://openbb.co/links&#34;&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; &#xA;&lt;h2&gt;3. License&lt;/h2&gt; &#xA;&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href=&#34;https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;4. Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; &#xA;&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; &#xA;&lt;p&gt;The data contained in the OpenBBTerminal is not necessarily accurate.&lt;/p&gt; &#xA;&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; &#xA;&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties. Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; &#xA;&lt;h2&gt;5. Contacts&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions about the terminal or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Any of our social media platforms: &lt;a href=&#34;https://openbb.co/links&#34;&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;6. Star History&lt;/h2&gt; &#xA;&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; &#xA;&lt;p&gt;But for more metrics important to us check &lt;a href=&#34;https://openbb.co/open&#34;&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://api.star-history.com/svg?repos=openbb-finance/OpenBBTerminal&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=openbb-finance/OpenBBTerminal&amp;amp;type=Date&amp;amp;theme=dark&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;7. Contributors&lt;/h2&gt; &#xA;&lt;p&gt;OpenBB wouldn&#39;t be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/OpenBB-finance/OpenBB/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBBTerminal&#34; width=&#34;800&#34;&gt; &lt;/a&gt; &#xA;&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; &#xA;&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</summary>
  </entry>
  <entry>
    <title>ZhengPeng7/BiRefNet</title>
    <updated>2024-08-19T01:33:28Z</updated>
    <id>tag:github.com,2024-08-19:/ZhengPeng7/BiRefNet</id>
    <link href="https://github.com/ZhengPeng7/BiRefNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CAAI AIR&#39;24] Bilateral Reference for High-Resolution Dichotomous Image Segmentation&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Bilateral Reference for High-Resolution Dichotomous Image Segmentation&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=TZRzWOsAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Peng Zheng&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1,4,5,6&lt;/sup&gt;,‚Äâ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=0uPb8MMAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Dehong Gao&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 2&lt;/sup&gt;,‚Äâ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=kakwJ5QAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Deng-Ping Fan&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1*&lt;/sup&gt;,‚Äâ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=9cMQrVsAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Li Liu&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 3&lt;/sup&gt;,‚Äâ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=qQP6WXIAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Jorma Laaksonen&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 4&lt;/sup&gt;,‚Äâ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=pw_0Z_UAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Wanli Ouyang&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 5&lt;/sup&gt;,‚Äâ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=stFCYOAAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Nicu Sebe&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 6&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;1 &lt;/sup&gt;Nankai University‚ÄÇ &#xA; &lt;sup&gt;2 &lt;/sup&gt;Northwestern Polytechnical University‚ÄÇ &#xA; &lt;sup&gt;3 &lt;/sup&gt;National University of Defense Technology‚ÄÇ &#xA; &lt;br&gt; &#xA; &lt;sup&gt;4 &lt;/sup&gt;Aalto University‚ÄÇ &#xA; &lt;sup&gt;5 &lt;/sup&gt;Shanghai AI Laboratory‚ÄÇ &#xA; &lt;sup&gt;6 &lt;/sup&gt;University of Trento‚ÄÇ &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;display: flex; justify-content: center; flex-wrap: wrap;&#34;&gt; &#xA; &lt;a href=&#34;https://arxiv.org/pdf/2401.03407&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-red&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA; &lt;a href=&#34;https://drive.google.com/file/d/1aBnJ_R9lbnC2dm8dqD0-pzP2Cu-U1Xpt/view&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E4%B8%AD%E6%96%87%E7%89%88-Paper-red&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA; &lt;a href=&#34;https://www.birefnet.top&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Page-Project-red&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA; &lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GDrive-Stuff-green&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/ZhengPeng7/BiRefNet/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA; &lt;a href=&#34;https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Space-blue&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA; &lt;a href=&#34;https://huggingface.co/ZhengPeng7/BiRefNet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;display: flex; justify-content: center; flex-wrap: wrap;&#34;&gt; &#xA; &lt;a href=&#34;https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Single_Image_Inference-F9AB00?style=for-the-badge&amp;amp;logo=googlecolab&amp;amp;color=525252&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA; &lt;a href=&#34;https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Inference_&amp;amp;_Evaluation-F9AB00?style=for-the-badge&amp;amp;logo=googlecolab&amp;amp;color=525252&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA; &lt;a href=&#34;https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Box_Guided_Segmentation-F9AB00?style=for-the-badge&amp;amp;logo=googlecolab&amp;amp;color=525252&#34;&gt;&lt;/a&gt;‚ÄÇ &#xA;&lt;/div&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;em&gt;DIS-Sample_1&lt;/em&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;em&gt;DIS-Sample_2&lt;/em&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1ItXaA26iYnE8XQ_GgNLy71MOWePoS2-g&amp;amp;sz=w400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1Z-esCujQF_uEa_YJjkibc3NUrW4aR_d4&amp;amp;sz=w400&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;This repo is the official implementation of &#34;&lt;a href=&#34;https://arxiv.org/pdf/2401.03407&#34;&gt;&lt;strong&gt;Bilateral Reference for High-Resolution Dichotomous Image Segmentation&lt;/strong&gt;&lt;/a&gt;&#34; (&lt;em&gt;&lt;strong&gt;CAAI AIR 2024&lt;/strong&gt;&lt;/em&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;News &lt;span&gt;üì∞&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Aug 19, 2024&lt;/code&gt;:&lt;/strong&gt; We uploaded the ONNX model files of all weights in the &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/releases/tag/v1&#34;&gt;GitHub release&lt;/a&gt; and &lt;a href=&#34;https://drive.google.com/drive/u/0/folders/1kZM55bwsRdS__bdnsXpkmH6QPyza-9-N&#34;&gt;GDrive folder&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jul 30, 2024&lt;/code&gt;:&lt;/strong&gt; Thanks to @not-lain for his kind efforts in adding BiRefNet to the official huggingface.js &lt;a href=&#34;https://github.com/huggingface/huggingface.js/raw/3a8651fbc6508920475564a692bf0e5b601d9343/packages/tasks/src/model-libraries-snippets.ts#L763&#34;&gt;repo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jul 28, 2024&lt;/code&gt;:&lt;/strong&gt; We released the &lt;a href=&#34;https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK&#34;&gt;Colab demo for box-guided segmentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jul 15, 2024&lt;/code&gt;:&lt;/strong&gt; We deployed our BiRefNet on &lt;a href=&#34;https://huggingface.co/ZhengPeng7/BiRefNet&#34;&gt;Hugging Face Models&lt;/a&gt; for users to easily load it in one line code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jun 21, 2024&lt;/code&gt;:&lt;/strong&gt; We released and uploaded the Chinese version of our original paper to my &lt;a href=&#34;https://drive.google.com/file/d/1aBnJ_R9lbnC2dm8dqD0-pzP2Cu-U1Xpt/view&#34;&gt;GDrive&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;May 28, 2024&lt;/code&gt;:&lt;/strong&gt; We hold a &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#model-zoo&#34;&gt;model zoo&lt;/a&gt; with well-trained weights of our BiRefNet in different sizes and for different tasks, including general use, portrait segmentation, DIS, HRSOD, COD, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;May 7, 2024&lt;/code&gt;:&lt;/strong&gt; We also released the &lt;a href=&#34;https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba&#34;&gt;Colab demo for single image inference&lt;/a&gt;. Many thanks to @rishabh063 for his support on it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Apr 9, 2024&lt;/code&gt;:&lt;/strong&gt; Thanks to &lt;a href=&#34;https://fal.ai/&#34;&gt;Features and Labels Inc.&lt;/a&gt; for deploying a cool online BiRefNet &lt;a href=&#34;https://fal.ai/models/fal-ai/birefnet/playground&#34;&gt;inference API&lt;/a&gt; and providing me with strong GPU resources for further experiments!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Mar 7, 2024&lt;/code&gt;:&lt;/strong&gt; We released BiRefNet codes, the well-trained weights for all tasks in the original papers, and all related stuff in my &lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;GDrive folder&lt;/a&gt;. Meanwhile, we also deployed our BiRefNet on &lt;a href=&#34;https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo&#34;&gt;Hugging Face Spaces&lt;/a&gt; for easier online use and released the &lt;a href=&#34;https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl&#34;&gt;Colab demo for inference and evaluation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jan 7, 2024&lt;/code&gt;:&lt;/strong&gt; We released our paper on &lt;a href=&#34;https://arxiv.org/abs/2305.15272&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Load BiRefNet in &lt;em&gt;ONE LINE&lt;/em&gt; by HuggingFace, check more: &lt;a href=&#34;https://huggingface.co/ZhengPeng7/birefnet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue&#34; alt=&#34;BiRefNet&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForImageSegmentation&#xA;birefnet = AutoModelForImageSegmentation.from_pretrained(&#39;zhengpeng7/BiRefNet&#39;, trust_remote_code=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üõ¨&lt;/span&gt; Inference Partner:&lt;/h2&gt; &#xA;&lt;p&gt;We are really happy to collaborate with &lt;a href=&#34;https://fal.ai&#34;&gt;FAL&lt;/a&gt; to deploy the &lt;strong&gt;inference API&lt;/strong&gt; of BiRefNet. You can access this service via the link below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/birefnet&#34;&gt;https://fal.ai/models/fal-ai/birefnet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our BiRefNet has achieved SOTA on many similar HR tasks:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DIS&lt;/strong&gt;: &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te1?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te2?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te3?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te4?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te4&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-vd?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-vd&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Figure of Comparison on DIS Papers with Codes (by the time of this work):&lt;/summary&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1DLt6CFXdT1QSWDj_6jRkyZINXZ4vmyRp&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1gn5GyKFlJbMIkre1JyEdHDSYcrFmcLD0&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=16CVYYOtafEeZhHqv0am2Daku1n_exMP6&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=10K45xwPXmaTG4Ex-29ss9payA9yBnyLn&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=16EuyqKFJOqwMmagvfnbC9hUurL9pYLLB&amp;amp;sz=w1620&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;COD&lt;/strong&gt;:&lt;a href=&#34;https://paperswithcode.com/sota/camouflaged-object-segmentation-on-cod?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-cod&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/camouflaged-object-segmentation-on-nc4k?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-nc4k&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/camouflaged-object-segmentation-on-camo?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-camo&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/camouflaged-object-segmentation-on-chameleon?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-chameleon&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Figure of Comparison on COD Papers with Codes (by the time of this work):&lt;/summary&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1DLt6CFXdT1QSWDj_6jRkyZINXZ4vmyRp&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1gn5GyKFlJbMIkre1JyEdHDSYcrFmcLD0&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=16CVYYOtafEeZhHqv0am2Daku1n_exMP6&amp;amp;sz=w1620&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;HRSOD&lt;/strong&gt;: &lt;a href=&#34;https://paperswithcode.com/sota/rgb-salient-object-detection-on-davis-s?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-davis-s&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/rgb-salient-object-detection-on-hrsod?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-hrsod&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/rgb-salient-object-detection-on-uhrsd?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-uhrsd&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/salient-object-detection-on-duts-te?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/salient-object-detection-on-duts-te&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/salient-object-detection-on-dut-omron?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/salient-object-detection-on-dut-omron&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Figure of Comparison on HRSOD Papers with Codes (by the time of this work):&lt;/summary&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1hNfQtlTAHT4-AVbk_47852zyRp1NOFLs&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1bcVldUAxYkMI3OMTyaP_jNuOugDfYj-d&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1p1zgyVz27cGEqQMtOKzm_6zoYK3Sw_Zk&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1TubAvcoEbH_mHu3I-AxflnB71nkf35jJ&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1A3V9HjVtcMQdnGPwuy-DBVhwKuo0q2lT&amp;amp;sz=w1620&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;Try our online demos for inference:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference and evaluation&lt;/strong&gt; of your given weights: &lt;a href=&#34;https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Online Inference with GUI&lt;/strong&gt; with adjustable resolutions: &lt;a href=&#34;https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Online &lt;strong&gt;Single Image Inference&lt;/strong&gt; on Colab: &lt;a href=&#34;https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://drive.google.com/thumbnail?id=12XmDhKtO1o2fEvBu4OE4ULVB2BK0ecWi&amp;amp;sz=w1620&#34;&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For more general use of our BiRefNet, I managed to extend the original adademic one to more general ones for better application in real life.&lt;/p&gt; &#xA; &lt;p&gt;Datasets and datasets are suggested to download from official pages. But you can also download the packaged ones: &lt;a href=&#34;https://drive.google.com/drive/folders/1hZW6tAGPJwo9mPS7qGGGdpxuvuXiyoMJ&#34;&gt;DIS&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/drive/folders/18_hAE3QM4cwAzEAKXuSNtKjmgFXTQXZN&#34;&gt;HRSOD&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/drive/folders/1EyHmKWsXfaCR9O0BiZEc3roZbRcs4ECO&#34;&gt;COD&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/drive/folders/1cmce_emsS8A5ha5XT2c_CZiJzlLM81ms&#34;&gt;Backbones&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Find performances (almost all metrics) of all models in the &lt;code&gt;exp-TASK_SETTINGS&lt;/code&gt; folders in [&lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;&lt;strong&gt;stuff&lt;/strong&gt;&lt;/a&gt;].&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Models in the original paper, for &lt;b&gt;comparison on benchmarks&lt;/b&gt;:&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Training Sets&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS5K-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1J90LucvDQaS3R_-9E7QUh1mgJ8eQvccb/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;COD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;COD10K-TR, CAMO-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1tM5M72k7a8aKF-dYy-QXaqvfEhbFaWkC/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DUTS-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1f7L0Pb1Y3RkOMbqLCW_zO31dik9AiUFa/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;google-drive&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;UHRSD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;google-drive&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DUTS-TR, HRSOD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1WJooyTkhoDLllaqwbpur_9Hle0XTHEs_/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DUTS-TR, UHRSD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Pu1mv3ORobJatIuUoEuZaWDl2ylP3Gw7/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD-TR, UHRSD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1xEh7fsgWGaS5c3IffMswasv0_u-aVM9E/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DUTS-TR, HRSOD-TR, UHRSD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/13FaxyyOwyCddfZn2vZo1xG1KNZ3cZ-6B/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Models trained with customed data (general, portrait), for &lt;b&gt;general use in practical application&lt;/b&gt;:&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Training Sets&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Test Set&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Metric (S, wF[, HCE])&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;general use&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS5K-TR,DIS-TEs, DUTS-TR_TE,HRSOD-TR_TE,UHRSD-TR_TE, HRS10K-TR_TE, TR-P3M-10k, TE-P3M-500-NP, TE-P3M-500-P, TR-humans&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS-VD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.911, 0.875, 1069&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1_IfUnu8Fpfn-nerB89FzdNXQ7zk6FKxc/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;general use&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS5K-TR,DIS-TEs, DUTS-TR_TE,HRSOD-TR_TE,UHRSD-TR_TE, HRS10K-TR_TE, TR-P3M-10k, TE-P3M-500-NP, TE-P3M-500-P, TR-humans&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_tiny&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS-VD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.882, 0.830, 1175&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1fzInDWiE2n65tmjaHDSZpqhL0VME6-Yl/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;general use&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS5K-TR, DIS-TEs&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS-VD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.907, 0.865, 1059&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1P6NJzG3Jf1sl7js2q1CPC3yqvBn_O8UJ/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;portrait segmentation&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/JizhiziLi/P3M&#34;&gt;P3M-10k&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/schirrmacher/humans&#34;&gt;humans&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;P3M-500-P&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.983, 0.989&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/10qg8uzmUUVO0axHKL641iUJpypr1SQN8/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Segmentation with box &lt;b&gt;guidance&lt;/b&gt;:&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Given box guidance: &lt;a href=&#34;https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Model &lt;b&gt;efficiency&lt;/b&gt;:&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Screenshot from the original paper. All tests are conducted on a single A100 GPU.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1mTfSD_qt-rFO1t8DRQcyIa5cgWLf1w2-&amp;amp;sz=h300&#34;&gt; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1F_OURIWILVe4u1rSz-aqt6ur__bAef25&amp;amp;sz=h300&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;ONNX&lt;/b&gt; conversion:&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;We converted from &lt;code&gt;.pth&lt;/code&gt; weights files to &lt;code&gt;.onnx&lt;/code&gt; files.&lt;br&gt; We referred a lot to the &lt;a href=&#34;https://github.com/Kazuhito00/BiRefNet-ONNX-Sample&#34;&gt;Kazuhito00/BiRefNet-ONNX-Sample&lt;/a&gt;, many thanks to @Kazuhito00.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Check our &lt;a href=&#34;https://colab.research.google.com/drive/1z6OruR52LOvDDpnp516F-N4EyPGrp5om#scrollTo=mj3nbp-Cl6Lw&#34;&gt;Colab demo for ONNX conversion&lt;/a&gt; or the &lt;a href=&#34;https://drive.google.com/file/d/1cgL2qyvOO5q3ySfhytypX46swdQwZLrJ/view?usp=drive_link&#34;&gt;notebook file for local running&lt;/a&gt;, where you can do the conversion/inference by yourself and find all relevant info.&lt;/li&gt; &#xA;  &lt;li&gt;As tested, BiRefNets with SwinL (default backbone) cost &lt;code&gt;~90%&lt;/code&gt; more time (the inference costs &lt;code&gt;~165ms&lt;/code&gt; on an A100 GPU) using ONNX files. Meanwhile, BiRefNets with SwinT (lightweight) cost &lt;code&gt;~75%&lt;/code&gt; more time (the inference costs &lt;code&gt;~93.8ms&lt;/code&gt; on an A100 GPU) using ONNX files. Input resolution is &lt;code&gt;1024x1024&lt;/code&gt; as default.&lt;/li&gt; &#xA;  &lt;li&gt;The results of the original pth files and the converted onnx files are slightly different, which is acceptable.&lt;/li&gt; &#xA;  &lt;li&gt;Pay attention to the compatibility among &lt;code&gt;onnxruntime-gpu, CUDA, and CUDNN&lt;/code&gt; (we use &lt;code&gt;torch==2.0.1, cuda=11.8&lt;/code&gt; here).&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Third-Party Creations&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Concerning edge devices with less computing power, we provide a lightweight version with &lt;code&gt;swin_v1_tiny&lt;/code&gt; as the backbone, which is x4+ faster and x5+ smaller. The details can be found in &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/issues/11#issuecomment-2041033576&#34;&gt;this issue&lt;/a&gt; and links there.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We found there&#39;ve been some 3rd party applications based on our BiRefNet. Many thanks for their contribution to the community!&lt;br&gt; Choose the one you like to try with clicks instead of codes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://fal.ai/models/birefnet&#34;&gt;&lt;strong&gt;fal.ai/birefnet&lt;/strong&gt;&lt;/a&gt;: this project on &lt;code&gt;fal.ai&lt;/code&gt; encapsulates BiRefNet &lt;strong&gt;online&lt;/strong&gt; with more useful options in &lt;strong&gt;UI&lt;/strong&gt; and &lt;strong&gt;API&lt;/strong&gt; to call the model.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1rNk81YV_Pzb2GykrzfGvX6T7KBXR0wrA&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-BiRefNet-ZHO&#34;&gt;&lt;strong&gt;ZHO-ZHO-ZHO/ComfyUI-BiRefNet-ZHO&lt;/strong&gt;&lt;/a&gt;: this project further improves the &lt;strong&gt;UI&lt;/strong&gt; for BiRefNet in ComfyUI, especially for &lt;strong&gt;video data&lt;/strong&gt;.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1GOqEreyS7ENzTPN0RqxEjaA76RpMlkYM&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/3a1c7ab2-9847-4dac-8935-43a2d3cd2671&#34;&gt;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/3a1c7ab2-9847-4dac-8935-43a2d3cd2671&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/viperyl/ComfyUI-BiRefNet&#34;&gt;&lt;strong&gt;viperyl/ComfyUI-BiRefNet&lt;/strong&gt;&lt;/a&gt;: this project packs BiRefNet as &lt;strong&gt;ComfyUI nodes&lt;/strong&gt;, and makes this SOTA model easier use for everyone.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1KfxCQUUa2y9T-aysEaeVVjCUt3Z0zSkL&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/rishabh063&#34;&gt;&lt;strong&gt;Rishabh&lt;/strong&gt;&lt;/a&gt; for offerring a demo for the &lt;a href=&#34;https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba&#34;&gt;easier single image inference on colab&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;More Visual Comparisons&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://twitter.com/ZHOZHO672070&#34;&gt;&lt;strong&gt;twitter.com/ZHOZHO672070&lt;/strong&gt;&lt;/a&gt; for the comparison with more background-removal methods in images:&lt;/p&gt; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1nvVIFt_Ezs-crPSQxUDqkUBz598fTe63&amp;amp;sz=w1620&#34;&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://twitter.com/toyxyz3&#34;&gt;&lt;strong&gt;twitter.com/toyxyz3&lt;/strong&gt;&lt;/a&gt; for the comparison with more background-removal methods in videos:&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/40136198-01cc-4106-81f9-81c985f02e31&#34;&gt;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/40136198-01cc-4106-81f9-81c985f02e31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/1a32860c-0893-49dd-b557-c2e35a83c160&#34;&gt;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/1a32860c-0893-49dd-b557-c2e35a83c160&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h4&gt;Environment Setup&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# PyTorch==2.0.1 is used for faster training with compilation.&#xA;conda create -n birefnet python=3.9 -y &amp;amp;&amp;amp; conda activate birefnet&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Dataset Preparation&lt;/h4&gt; &#xA;&lt;p&gt;Download combined training / test sets I have organized well from: &lt;a href=&#34;https://drive.google.com/drive/folders/1hZW6tAGPJwo9mPS7qGGGdpxuvuXiyoMJ&#34;&gt;DIS&lt;/a&gt;--&lt;a href=&#34;https://drive.google.com/drive/folders/1EyHmKWsXfaCR9O0BiZEc3roZbRcs4ECO&#34;&gt;COD&lt;/a&gt;--&lt;a href=&#34;https://drive.google.com/drive/folders/18_hAE3QM4cwAzEAKXuSNtKjmgFXTQXZN&#34;&gt;HRSOD&lt;/a&gt; or the single official ones in the &lt;code&gt;single_ones&lt;/code&gt; folder, or their official pages. You can also find the same ones on my &lt;strong&gt;BaiduDisk&lt;/strong&gt;: &lt;a href=&#34;https://pan.baidu.com/s/1O_pQIGAE4DKqL93xOxHpxw?pwd=PSWD&#34;&gt;DIS&lt;/a&gt;--&lt;a href=&#34;https://pan.baidu.com/s/1RnxAzaHSTGBC1N6r_RfeqQ?pwd=PSWD&#34;&gt;COD&lt;/a&gt;--&lt;a href=&#34;https://pan.baidu.com/s/1_Del53_0lBuG0DKJJAk4UA?pwd=PSWD&#34;&gt;HRSOD&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Weights Preparation&lt;/h4&gt; &#xA;&lt;p&gt;Download backbone weights from &lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;my google-drive folder&lt;/a&gt; or their official pages.&lt;/p&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Train &amp;amp; Test &amp;amp; Evaluation&#xA;./train_test.sh RUN_NAME GPU_NUMBERS_FOR_TRAINING GPU_NUMBERS_FOR_TEST&#xA;# Example: ./train_test.sh tmp-proj 0,1,2,3,4,5,6,7 0&#xA;&#xA;# See train.sh / test.sh for only training / test-evaluation.&#xA;# After the evaluation, run `gen_best_ep.py` to select the best ckpt from a specific metric (you choose it from Sm, wFm, HCE (DIS only)).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Well-trained weights:&lt;/h4&gt; &#xA;&lt;p&gt;Download the &lt;code&gt;BiRefNet-{TASK}-{EPOCH}.pth&lt;/code&gt; from [&lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;&lt;strong&gt;stuff&lt;/strong&gt;&lt;/a&gt;]. Info of the corresponding (predicted_maps/performance/training_log) weights can be also found in folders like &lt;code&gt;exp-BiRefNet-{TASK_SETTINGS}&lt;/code&gt; in the same directory.&lt;/p&gt; &#xA;&lt;p&gt;You can also download the weights from the release of this repo.&lt;/p&gt; &#xA;&lt;p&gt;The results might be a bit different from those in the original paper, you can see them in the &lt;code&gt;eval_results-BiRefNet-{TASK_SETTINGS}&lt;/code&gt; folder in each &lt;code&gt;exp-xx&lt;/code&gt;, we will update them in the following days. Due to the very high cost I used (A100-80G x 8) which many people cannot afford to (including myself....), I re-trained BiRefNet on a single A100-40G only and achieve the performance on the same level (even better). It means you can directly train the model on a single GPU with 36.5G+ memory. BTW, 5.5G GPU memory is needed for inference in 1024x1024. (I personally paid a lot for renting an A100-40G to re-train BiRefNet on the three tasks... T_T. Hope it can help you.)&lt;/p&gt; &#xA;&lt;p&gt;But if you have more and more powerful GPUs, you can set GPU IDs and increase the batch size in &lt;code&gt;config.py&lt;/code&gt; to accelerate the training. We have made all this kind of things adaptive in scripts to seamlessly switch between single-card training and multi-card training. Enjoy it :)&lt;/p&gt; &#xA;&lt;h4&gt;Some of my messages:&lt;/h4&gt; &#xA;&lt;p&gt;This project was originally built for DIS only. But after the updates one by one, I made it larger and larger with many functions embedded together. Finally, you can &lt;strong&gt;use it for any binary image segmentation tasks&lt;/strong&gt;, such as DIS/COD/SOD, medical image segmentation, anomaly segmentation, etc. You can eaily open/close below things (usually in &lt;code&gt;config.py&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-GPU training: open/close with one variable.&lt;/li&gt; &#xA; &lt;li&gt;Backbone choices: Swin_v1, PVT_v2, ConvNets, ...&lt;/li&gt; &#xA; &lt;li&gt;Weighted losses: BCE, IoU, SSIM, MAE, Reg, ...&lt;/li&gt; &#xA; &lt;li&gt;Adversarial loss for binary segmentation (proposed in my previous work &lt;a href=&#34;https://arxiv.org/pdf/2302.14485&#34;&gt;MCCL&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Training tricks: multi-scale supervision, freezing backbone, multi-scale input...&lt;/li&gt; &#xA; &lt;li&gt;Data collator: loading all in memory, smooth combination of different datasets for combined training and test.&lt;/li&gt; &#xA; &lt;li&gt;... I really hope you enjoy this project and use it in more works to achieve new SOTAs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quantitative Results&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1Ymkh8WN16XMTBOS8dmPTg5eAf-NIl2m5&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1W0mi0ZiYbqsaGuohNXU8Gh7Zj4M3neFg&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Qualitative Results&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1TYZF8pVZc2V0V6g3ik4iAr9iKvJ8BNrf&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1ZGHC32CAdT9cwRloPzOCKWCrVQZvUAlJ&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zheng2024birefnet,&#xA;  title={Bilateral Reference for High-Resolution Dichotomous Image Segmentation},&#xA;  author={Zheng, Peng and Gao, Dehong and Fan, Deng-Ping and Liu, Li and Laaksonen, Jorma and Ouyang, Wanli and Sebe, Nicu},&#xA;  journal={CAAI Artificial Intelligence Research},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Any questions, discussions, or even complaints, feel free to leave issues here or send me e-mails (&lt;a href=&#34;mailto:zhengpeng0108@gmail.com&#34;&gt;zhengpeng0108@gmail.com&lt;/a&gt;). You can also join the Discord Group (&lt;a href=&#34;https://discord.gg/d9NN5sgFrq&#34;&gt;https://discord.gg/d9NN5sgFrq&lt;/a&gt;) or QQ Group (&lt;a href=&#34;https://qm.qq.com/q/y6WPy7WOIK&#34;&gt;https://qm.qq.com/q/y6WPy7WOIK&lt;/a&gt;) if you want to talk a lot publicly.&lt;/p&gt;</summary>
  </entry>
</feed>