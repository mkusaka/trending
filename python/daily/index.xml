<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-26T01:36:41Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Upsonic/Upsonic</title>
    <updated>2025-02-26T01:36:41Z</updated>
    <id>tag:github.com,2025-02-26:/Upsonic/Upsonic</id>
    <link href="https://github.com/Upsonic/Upsonic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Most Reliable AI Agent Framework&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/10a3a9ca-1f39-410c-ac48-a7365de589d9&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;a name=&#34;readme-top&#34;&gt;&lt;/a&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://discord.gg/dNKGm4dfnR&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Discord-Join-7289DA?logo=discord&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/upsonicai&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/upsonicai?style=social&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://trendshift.io/repositories/10584&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/10584&#34; alt=&#34;unclecode%2Fcrawl4ai | Trendshift&#34; style=&#34;width: 100px; height: 20px;&#34; &lt;a href=&#34;https://www.python.org/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Made%20with-Python-1f425f.svg?sanitize=true&#34; alt=&#34;Made_with_python&#34;&gt; &lt;/a&gt; &lt;img src=&#34;https://static.pepy.tech/personalized-badge/upsonic?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=PyPI%20Downloads&#34; alt=&#34;pypi_downloads&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Upsonic is a reliability-focused framework designed for real-world applications. It enables trusted agent workflows in your organization through advanced reliability features, including verification layers, triangular architecture, validator agents, and output evaluation systems.&lt;/p&gt; &#xA;&lt;h1&gt;Why Choose Upsonic?&lt;/h1&gt; &#xA;&lt;p&gt;Upsonic is a next-generation framework that makes agents production-ready by solving three critical challenges:&lt;/p&gt; &#xA;&lt;p&gt;1- &lt;strong&gt;Reliability&lt;/strong&gt;: While other frameworks require expertise and complex coding for reliability features, Upsonic offers easy-to-activate reliability layers without disrupting functionality.&lt;/p&gt; &#xA;&lt;p&gt;2- &lt;strong&gt;Model Context Protocol&lt;/strong&gt;: The MCP allows you to leverage tools with various functionalities developed both officially and by third parties without requiring you to build custom tools from scratch.&lt;/p&gt; &#xA;&lt;p&gt;3- &lt;strong&gt;Secure Runtime&lt;/strong&gt;: Isolated environment to run agents&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/1b276199-ae60-4221-b8e6-b266443a3641&#34; alt=&#34;sdk-server&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Production-Ready Scalability&lt;/strong&gt;: Deploy seamlessly on AWS, GCP, or locally using Docker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task-Centric Design&lt;/strong&gt;: Focus on practical task execution, with options for: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Basic tasks via LLM calls.&lt;/li&gt; &#xA;   &lt;li&gt;Advanced tasks with V1 agents.&lt;/li&gt; &#xA;   &lt;li&gt;Complex automation using V2 agents with MCP integration.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MCP Server Support&lt;/strong&gt;: Utilize multi-client processing for high-performance tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tool-Calling Server&lt;/strong&gt;: Exception-secure tool management with robust server API interactions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Computer Use Integration&lt;/strong&gt;: Execute human-like tasks using Anthropic‚Äôs ‚ÄòComputer Use‚Äô capabilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easily adding tools:&lt;/strong&gt;&amp;nbsp;You can add your custom tools and MCP tools with a single line of code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üìô Documentation&lt;/h1&gt; &#xA;&lt;p&gt;You can access our documentation at &lt;a href=&#34;https://docs.upsonic.ai/&#34;&gt;docs.upsonic.ai&lt;/a&gt; All concepts and examples are available there.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üõ†Ô∏è Getting Started&lt;/h1&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10 or higher&lt;/li&gt; &#xA; &lt;li&gt;Access to OpenAI or Anthropic API keys (Azure and Bedrock Supported)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install upsonic&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Basic Example&lt;/h1&gt; &#xA;&lt;p&gt;Set your OPENAI_API_KEY&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;export OPENAI_API_KEY=sk-***&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start the agent&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from upsonic import Task, Agent&#xA;&#xA;task = Task(&#34;Who developed you?&#34;)&#xA;&#xA;agent = Agent(&#34;Coder&#34;)&#xA;&#xA;agent.print_do(task)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;üìä Reliability Layer&lt;/h2&gt; &#xA;&lt;p&gt;LLM output reliability is critical, particularly for numerical operations and action execution. Upsonic addresses this through a multi-layered reliability system, enabling control agents and verification rounds to ensure output accuracy.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Verifier Agent&lt;/strong&gt;: Validates outputs, tasks, and formats - detecting inconsistencies, numerical errors, and hallucinations&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Editor Agent&lt;/strong&gt;: Works with verifier feedback to revise and refine outputs until they meet quality standards&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rounds&lt;/strong&gt;: Implements iterative quality improvement through scored verification cycles&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Loops&lt;/strong&gt;: Ensures accuracy through controlled feedback loops at critical reliability checkpoints&lt;/p&gt; &#xA;&lt;p&gt;Upsonic is a reliability-focused framework. The results in the table were generated with a small dataset. They show success rates in the transformation of JSON keys. No hard-coded changes were made to the frameworks during testing; only the existing features of each framework were activated and run. GPT-4o was used in the tests.&lt;/p&gt; &#xA;&lt;p&gt;10 transfers were performed for each section. The numbers show the error count. So if it says 7, it means 7 out of 10 were done &lt;strong&gt;incorrectly&lt;/strong&gt;. The table has been created based on initial results. We are expanding the dataset. The tests will become more reliable after creating a larger test set. Reliability benchmark &lt;a href=&#34;https://github.com/Upsonic/Reliability-Benchmark&#34;&gt;repo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Reliability Score %&lt;/th&gt; &#xA;   &lt;th&gt;ASIN Code&lt;/th&gt; &#xA;   &lt;th&gt;HS Code&lt;/th&gt; &#xA;   &lt;th&gt;CIS Code&lt;/th&gt; &#xA;   &lt;th&gt;Marketing URL&lt;/th&gt; &#xA;   &lt;th&gt;Usage URL&lt;/th&gt; &#xA;   &lt;th&gt;Warranty Time&lt;/th&gt; &#xA;   &lt;th&gt;Policy Link&lt;/th&gt; &#xA;   &lt;th&gt;Policy Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Upsonic&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;99.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CrewAI&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;87.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Langgraph&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class ReliabilityLayer:&#xA;  prevent_hallucination = 10&#xA;&#xA;agent = Agent(&#34;Coder&#34;, reliability_layer=ReliabilityLayer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Tool Integration via MCP&lt;/h2&gt; &#xA;&lt;p&gt;Upsonic officially supports &lt;a href=&#34;https://github.com/modelcontextprotocol/servers&#34;&gt;Model Context Protocol (MCP)&lt;/a&gt; and custom tools. You can use hundreds of MCP servers at &lt;a href=&#34;https://glama.ai/mcp/servers&#34;&gt;glama&lt;/a&gt; or &lt;a href=&#34;https://mcp.run&#34;&gt;mcprun&lt;/a&gt; We also support Python functions inside a class as a tool. You can easily generate your integrations with that.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from upsonic import Agent, Task, ObjectResponse&#xA;&#xA;# Define Fetch MCP configuration&#xA;class FetchMCP:&#xA;    command = &#34;uvx&#34;&#xA;    args = [&#34;mcp-server-fetch&#34;]&#xA;&#xA;# Create response format for web content&#xA;class WebContent(ObjectResponse):&#xA;    title: str&#xA;    content: str&#xA;    summary: str&#xA;    word_count: int&#xA;&#xA;# Initialize agent&#xA;web_agent = Agent(&#xA;    &#34;Web Content Analyzer&#34;,&#xA;    model=&#34;openai/gpt-4o&#34;,  # You can use other models&#xA;)&#xA;&#xA;# Create a task to analyze a web page&#xA;task = Task(&#xA;    description=&#34;Fetch and analyze the content from url. Extract the main content, title, and create a brief summary.&#34;,&#xA;    context=[&#34;https://upsonic.ai&#34;],&#xA;    tools=[FetchMCP],&#xA;    response_format=WebContent&#xA;)&#xA;    &#xA;# Usage&#xA;web_agent.print_do(task)&#xA;print(result.title)&#xA;print(result.summary)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Agent with Multi-Task Example&lt;/h2&gt; &#xA;&lt;p&gt;Distribute tasks effectively across agents with our automated task distribution mechanism. This tool matches tasks based on the relationship between agent and task, ensuring collaborative problem-solving across agents and tasks. The output is essential for deploying an AI agent across apps or as a service. Upsonic uses Pydantic BaseClass to define structured outputs for tasks, allowing developers to specify exact response formats for their AI agent tasks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from upsonic import Agent, Task, MultiAgent, ObjectResponse&#xA;from upsonic.tools import Search&#xA;from typing import List&#xA;&#xA;# Targeted Company and Our Company&#xA;our_company = &#34;https://redis.io/&#34;&#xA;targeted_url = &#34;https://upsonic.ai/&#34;&#xA;&#xA;&#xA;# Response formats&#xA;class CompanyResearch(ObjectResponse):&#xA;   industry: str&#xA;   product_focus: str&#xA;   company_values: List[str]&#xA;   recent_news: List[str]&#xA;&#xA;class Mail(ObjectResponse):&#xA;   subject: str&#xA;   content: str&#xA;&#xA;&#xA;# Creating Agents&#xA;researcher = Agent(&#xA;   &#34;Company Researcher&#34;,&#xA;   company_url=our_company&#xA;)&#xA;&#xA;strategist = Agent(&#xA;   &#34;Outreach Strategist&#34;, &#xA;   company_url=our_company&#xA;)&#xA;&#xA;&#xA;# Creating Tasks and connect&#xA;company_task = Task(&#xA;   &#34;Research company website and analyze key information&#34;,&#xA;&#xA;   context=[targeted_url],&#xA;   tools=[Search],&#xA;   response_format=CompanyResearch&#xA;)&#xA;&#xA;position_task = Task(&#xA;   &#34;Analyze Senior Developer position context and requirements&#34;,&#xA;   context=[company_task, targeted_url],&#xA;)&#xA;&#xA;message_task = Task(&#xA;   &#34;Create personalized outreach message using research&#34;,&#xA;   context=[company_task, position_task, targeted_url],&#xA;   response_format=Mail&#xA;)&#xA;&#xA;&#xA;# Run the Tasks over agents&#xA;results = MultiAgent.do(&#xA;   [researcher, strategist],&#xA;   [company_task, position_task, message_task]&#xA;)&#xA;&#xA;&#xA;# Print the results&#xA;print(f&#34;Company Industry: {company_task.response.industry}&#34;)&#xA;print(f&#34;Company Focus: {company_task.response.product_focus}&#34;)&#xA;print(f&#34;Company Values: {company_task.response.company_values}&#34;)&#xA;print(f&#34;Company Recent News: {company_task.response.recent_news}&#34;)&#xA;print(f&#34;Position Analyze: {position_task.response}&#34;)&#xA;print(f&#34;Outreach Message Subject: {message_task.response.subject}&#34;)&#xA;print(f&#34;Outreach Message Content: {message_task.response.content}&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Direct LLM Call&lt;/h2&gt; &#xA;&lt;p&gt;Direct LLM calls offer faster, cheaper solutions for simple tasks. In Upsonic, you can make calls to model providers without any abstraction level and organize structured outputs. You can also use tools with LLM calls.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from upsonic import Direct&#xA;&#xA;Direct.do(task1)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;We use anonymous telemetry to collect usage data. We do this to focus our developments on more accurate points. You can disable it by setting the UPSONIC_TELEMETRY environment variable to false.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#34;UPSONIC_TELEMETRY&#34;] = &#34;False&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>KellerJordan/Muon</title>
    <updated>2025-02-26T01:36:41Z</updated>
    <id>tag:github.com,2025-02-26:/KellerJordan/Muon</id>
    <link href="https://github.com/KellerJordan/Muon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Muon optimizer: +&gt;30% sample efficiency with &lt;3% wallclock overhead&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Muon: An optimizer for the hidden layers of neural networks&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains an implementation of the &lt;code&gt;Muon&lt;/code&gt; optimizer described in &lt;a href=&#34;https://x.com/kellerjordan0/status/1842300916864844014&#34;&gt;this thread&lt;/a&gt; and &lt;a href=&#34;https://kellerjordan.github.io/posts/muon/&#34;&gt;this writeup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/KellerJordan/Muon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Muon is intended to optimize only the internal ‚â•2D parameters of a network. Embeddings, classifier heads, and scalar or vector parameters should be optimized using AdamW.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.90, 0.95), weight_decay=0.01)&#xA;&#xA;from muon import Muon&#xA;# Find ‚â•2D parameters in the body of the network -- these should be optimized by Muon&#xA;muon_params = [p for p in model.body.parameters() if p.ndim &amp;gt;= 2]&#xA;# Find everything else -- these should be optimized by AdamW&#xA;adamw_params = ([p for p in model.body.parameters() if p.ndim &amp;lt; 2]&#xA;              + [*model.head.parameters(), *model.embed.parameters()])&#xA;# Create the optimizer&#xA;optimizers = [Muon(muon_params, lr=0.02, momentum=0.95),&#xA;              torch.optim.AdamW(adamw_params, lr=3e-4, betas=(0.90, 0.95), weight_decay=0.01)]&#xA;...&#xA;&#xA;# in the training step&#xA;for opt in optimizers:&#xA;    opt.step()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll have to replace &lt;code&gt;model.body&lt;/code&gt;, &lt;code&gt;model.head&lt;/code&gt;, and &lt;code&gt;model.embed&lt;/code&gt; with whatever subset is appropriate for your model. E.g., for a ConvNet, &lt;code&gt;muon_params&lt;/code&gt; should be all the convolutional filters, and &lt;code&gt;adamw_params&lt;/code&gt; should be everything else.&lt;/p&gt; &#xA;&lt;h2&gt;Example usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KellerJordan/modded-nanogpt/raw/d700b8724cbda3e7b1e5bcadbc0957f6ad1738fd/train_gpt.py#L519&#34;&gt;Example use of this Muon in the NanoGPT speedrun&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KellerJordan/cifar10-airbench/raw/0e6f9614572d7e8e3c259905aebc7196f91d5d79/research/clean_muon.py#L220&#34;&gt;Example use of a Muon variant in the CIFAR-10 speedrun&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Hyperparameter tuning&lt;/h2&gt; &#xA;&lt;p&gt;Typically, the default values of momentum (0.95), nesterov (True), and ns_steps (5) work well. The only hyperparameter which must be tuned is the learning rate. It should have constant muP scaling, that is, as you scale up the model size, you shouldn&#39;t need to retune the learning rate.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;For a comparison between AdamW, Shampoo, SOAP, and Muon for training a 124M-parameter transformer, see &lt;a href=&#34;https://github.com/KellerJordan/modded-nanogpt/tree/master/records/102924_Optimizers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Accomplishments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KellerJordan/cifar10-airbench&#34;&gt;Lowered the record for training to 94% on CIFAR-10 from 3.3 A100-seconds to 2.7 A100-seconds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://x.com/kellerjordan0/status/1850995958697308307&#34;&gt;Used to train a transformer to GPT-2 (XL) performance in $175 of compute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://x.com/kellerjordan0/status/1842300916864844014&#34;&gt;Improved the training speed record for attaining GPT-2 (small) performance by a factor of 1.35x&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{jordan2024muon,&#xA;  author       = {Keller Jordan and Yuchen Jin and Vlado Boza and You Jiacheng and&#xA;                  Franz Cecista and Laker Newhouse and Jeremy Bernstein},&#xA;  title        = {Muon: An optimizer for hidden layers in neural networks},&#xA;  year         = {2024},&#xA;  url          = {https://kellerjordan.github.io/posts/muon/}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>fla-org/flash-linear-attention</title>
    <updated>2025-02-26T01:36:41Z</updated>
    <id>tag:github.com,2025-02-26:/fla-org/flash-linear-attention</id>
    <link href="https://github.com/fla-org/flash-linear-attention" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üöÄ Efficient implementations of state-of-the-art linear attention models in Torch and Triton&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;&lt;span&gt;üí•&lt;/span&gt; Flash Linear Attention&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/fla-hub&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Models-gray.svg?logo=huggingface&amp;amp;style=flat-square&#34; alt=&#34;hf_model&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/vDaJTmKNcS&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This repo aims at providing a collection of efficient Triton-based implementations for state-of-the-art linear attention models. &lt;strong&gt;Any pull requests are welcome!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;400&#34; alt=&#34;image&#34; src=&#34;https://github.com/fla-org/flash-linear-attention/assets/18402347/02ff2e26-1495-4088-b701-e72cd65ac6cf&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#news&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#token-mixing&#34;&gt;Token Mixing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#fused-modules&#34;&gt;Fused Modules&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#generation&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#hybrid-models&#34;&gt;Hybrid Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#evaluations&#34;&gt;Evaluations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#benchmarks&#34;&gt;Benchmarks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#star-history&#34;&gt;Star History&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2025-02]}$:&lt;/strong&gt; &lt;span&gt;üê≥&lt;/span&gt; Add NSA implementations to &lt;code&gt;fla&lt;/code&gt;. See kernels &lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/ops/nsa&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2025-01]}$:&lt;/strong&gt; &lt;span&gt;üî•&lt;/span&gt; We are migrating to &lt;code&gt;torchtitan&lt;/code&gt;-based training framework. Check out the &lt;a href=&#34;https://github.com/fla-org/flame&#34;&gt;flame&lt;/a&gt; repo for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2025-01]}$:&lt;/strong&gt; &lt;span&gt;üéâ&lt;/span&gt; Add RWKV7 implementations (both kernels and models) to &lt;code&gt;fla&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-12]}$:&lt;/strong&gt; Integrated &lt;code&gt;flash-bidirectional-attention&lt;/code&gt; to &lt;code&gt;fla-org&lt;/code&gt; (&lt;a href=&#34;https://github.com/fla-org/flash-bidirectional-linear-attention&#34;&gt;repo&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-12]}$:&lt;/strong&gt; &lt;span&gt;üéâ&lt;/span&gt; Add Gated DeltaNet implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href=&#34;https://arxiv.org/abs/2412.06464&#34;&gt;paper&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-12]}$:&lt;/strong&gt; &lt;span&gt;üöÄ&lt;/span&gt; &lt;code&gt;fla&lt;/code&gt; now officially supports kernels with variable-length inputs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-11]}$:&lt;/strong&gt; The inputs are now switched from head-first to seq-first format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-11]}$:&lt;/strong&gt; &lt;span&gt;üí•&lt;/span&gt; &lt;code&gt;fla&lt;/code&gt; now provides a flexible way for training hybrid models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-10]}$:&lt;/strong&gt; &lt;span&gt;üî•&lt;/span&gt; Announcing &lt;code&gt;flame&lt;/code&gt;, a minimal and scalable framework for training &lt;code&gt;fla&lt;/code&gt; models. Check out the details &lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/training/README.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-09]}$:&lt;/strong&gt; &lt;code&gt;fla&lt;/code&gt; now includes a fused linear and cross-entropy layer, significantly reducing memory usage during training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-09]}$:&lt;/strong&gt; &lt;span&gt;üéâ&lt;/span&gt; Add GSA implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href=&#34;https://arxiv.org/abs/2409.07146&#34;&gt;paper&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-05]}$:&lt;/strong&gt; &lt;span&gt;üéâ&lt;/span&gt; Add DeltaNet implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href=&#34;https://arxiv.org/abs/2102.11174&#34;&gt;paper&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2024-05]}$:&lt;/strong&gt; &lt;span&gt;üí•&lt;/span&gt; &lt;code&gt;fla&lt;/code&gt; v0.1: a variety of subquadratic kernels/layers/models integrated (RetNet/GLA/Mamba/HGRN/HGRN2/RWKV6, etc., see &lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#models&#34;&gt;Models&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;$\texttt{[2023-12]}$:&lt;/strong&gt; &lt;span&gt;üí•&lt;/span&gt; Launched &lt;code&gt;fla&lt;/code&gt;, offering a collection of implementations for state-of-the-art linear attention models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;Roughly sorted according to the timeline supported in &lt;code&gt;fla&lt;/code&gt;. The recommended training mode is &lt;code&gt;chunk&lt;/code&gt; when available.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Year&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Venue&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Title&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;fla&lt;/code&gt; impl&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;RetNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Retentive network: a successor to transformer for large language models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.08621&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/torchscale/tree/main&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/multiscale_retention.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ICML&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GLA&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Gated Linear Attention Transformers with Hardware-Efficient Training&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.06635&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/berlino/gated_linear_attention&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/gla.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ICML&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Based&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Simple linear attention language models balance the recall-throughput tradeoff&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.18668&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HazyResearch/based&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/based.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ACL&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Rebased&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Linear Transformers with Learnable Kernel Functions are Better In-Context Models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.10644&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/corl-team/rebased/&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/rebased.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NeurIPS&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DeltaNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Parallelizing Linear Transformers with Delta Rule over Sequence Length&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.06484&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/delta_net.py&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/delta_net.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ACL&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ABC&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Attention with Bounded-memory Control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.02488&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/abc.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NeurIPS&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;HGRN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hierarchically Gated Recurrent Neural Network for Sequence Modeling&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://openreview.net/forum?id=P1TCHxJwLB&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenNLPLab/HGRN&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/hgrn.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;COLM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;HGRN2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;HGRN2: Gated Linear RNNs with State Expansion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.07904&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenNLPLab/HGRN2&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/hgrn2.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;COLM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;RWKV6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.05892&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RWKV/RWKV-LM&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/rwkv6.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LightNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.21022&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenNLPLab/LightNet&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/lightnet.py&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2025&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ICLR&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Samba&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.07522&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/Samba&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/models/samba&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ICML&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mamba2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.21060&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/state-spaces/mamba&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/raw/main/fla/models/mamba2&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NeurIPS&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GSA&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Gated Slot Attention for Efficient Linear-Time Sequence Modeling&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.07146&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/tree/main/fla/models/gsa&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/tree/main/fla/models/gsa&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2025&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ICLR&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Gated DeltaNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Gated Delta Networks: Improving Mamba2 with Delta Rule&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2412.06464&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/NVlabs/GatedDeltaNet&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/gated_delta_rule&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2025&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;RWKV7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/rwkv7&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2025&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NSA&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2502.11089&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/nsa&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The following requirements should be satisfied&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; &amp;gt;= 2.5&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/triton&#34;&gt;Triton&lt;/a&gt; &amp;gt;=3.0 (or nightly version, see &lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/FAQs.md&#34;&gt;FAQs&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://einops.rocks/&#34;&gt;einops&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; &amp;gt;=4.45.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;datasets&lt;/a&gt; &amp;gt;=3.3.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Dao-AILab/causal-conv1d&#34;&gt;causal-conv1d&lt;/a&gt; &amp;gt;=1.4.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;As &lt;code&gt;fla&lt;/code&gt; is actively developed now, no released packages are provided at this time. If you do need to use &lt;code&gt;fla&lt;/code&gt; ops/modules and contemplate further explorations, an alternative way is to install the package from source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# uninstall `fla` first to ensure a successful upgrade&#xA;pip uninstall fla &amp;amp;&amp;amp; pip install -U git+https://github.com/fla-org/flash-linear-attention&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or manage &lt;code&gt;fla&lt;/code&gt; with submodules&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git submodule add https://github.com/fla-org/flash-linear-attention.git 3rdparty/flash-linear-attention&#xA;ln -s 3rdparty/flash-linear-attention/fla fla&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Token Mixing&lt;/h3&gt; &#xA;&lt;p&gt;We provide ``token mixing&#39;&#39; linear attention layers in &lt;code&gt;fla.layers&lt;/code&gt; for you to use. You can replace the standard multihead attention layer in your model with other linear attention layers. Example usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; from fla.layers import MultiScaleRetention&#xA;&amp;gt;&amp;gt;&amp;gt; batch_size, num_heads, seq_len, hidden_size = 32, 4, 2048, 1024&#xA;&amp;gt;&amp;gt;&amp;gt; device, dtype = &#39;cuda:0&#39;, torch.bfloat16&#xA;&amp;gt;&amp;gt;&amp;gt; retnet = MultiScaleRetention(hidden_size=hidden_size, num_heads=num_heads).to(device=device, dtype=dtype)&#xA;&amp;gt;&amp;gt;&amp;gt; retnet&#xA;MultiScaleRetention(&#xA;  (q_proj): Linear(in_features=1024, out_features=1024, bias=False)&#xA;  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)&#xA;  (v_proj): Linear(in_features=1024, out_features=2048, bias=False)&#xA;  (g_proj): Linear(in_features=1024, out_features=2048, bias=False)&#xA;  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)&#xA;  (g_norm_swish_gate): FusedRMSNormSwishGate(512, eps=1e-05)&#xA;  (rotary): RotaryEmbedding()&#xA;)&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.randn(batch_size, seq_len, hidden_size).to(device=device, dtype=dtype)&#xA;&amp;gt;&amp;gt;&amp;gt; y, *_ = retnet(x)&#xA;&amp;gt;&amp;gt;&amp;gt; y.shape&#xA;torch.Size([32, 2048, 1024])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide the implementations of models that are compatible with ü§ó Transformers library. Here&#39;s an example of how to initialize a GLA model from the default configs in &lt;code&gt;fla&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from fla.models import GLAConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM&#xA;&amp;gt;&amp;gt;&amp;gt; config = GLAConfig()&#xA;&amp;gt;&amp;gt;&amp;gt; config&#xA;GLAConfig {&#xA;  &#34;attn&#34;: null,&#xA;  &#34;attn_mode&#34;: &#34;chunk&#34;,&#xA;  &#34;bos_token_id&#34;: 1,&#xA;  &#34;clamp_min&#34;: null,&#xA;  &#34;conv_size&#34;: 4,&#xA;  &#34;elementwise_affine&#34;: true,&#xA;  &#34;eos_token_id&#34;: 2,&#xA;  &#34;expand_k&#34;: 0.5,&#xA;  &#34;expand_v&#34;: 1,&#xA;  &#34;feature_map&#34;: null,&#xA;  &#34;fuse_cross_entropy&#34;: true,&#xA;  &#34;fuse_norm&#34;: true,&#xA;  &#34;fuse_swiglu&#34;: true,&#xA;  &#34;hidden_act&#34;: &#34;swish&#34;,&#xA;  &#34;hidden_ratio&#34;: 4,&#xA;  &#34;hidden_size&#34;: 2048,&#xA;  &#34;initializer_range&#34;: 0.02,&#xA;  &#34;intermediate_size&#34;: null,&#xA;  &#34;max_position_embeddings&#34;: 2048,&#xA;  &#34;model_type&#34;: &#34;gla&#34;,&#xA;  &#34;norm_eps&#34;: 1e-06,&#xA;  &#34;num_heads&#34;: 4,&#xA;  &#34;num_hidden_layers&#34;: 24,&#xA;  &#34;num_kv_heads&#34;: null,&#xA;  &#34;tie_word_embeddings&#34;: false,&#xA;  &#34;transformers_version&#34;: &#34;4.48.2&#34;,&#xA;  &#34;use_cache&#34;: true,&#xA;  &#34;use_gk&#34;: true,&#xA;  &#34;use_gv&#34;: false,&#xA;  &#34;use_output_gate&#34;: true,&#xA;  &#34;use_short_conv&#34;: false,&#xA;  &#34;vocab_size&#34;: 32000&#xA;}&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; AutoModelForCausalLM.from_config(config)&#xA;GLAForCausalLM(&#xA;  (model): GLAModel(&#xA;    (embeddings): Embedding(32000, 2048)&#xA;    (layers): ModuleList(&#xA;      (0-23): 24 x GLABlock(&#xA;        (attn_norm): RMSNorm(2048, eps=1e-06)&#xA;        (attn): GatedLinearAttention(&#xA;          (q_proj): Linear(in_features=2048, out_features=1024, bias=False)&#xA;          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)&#xA;          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)&#xA;          (g_proj): Linear(in_features=2048, out_features=2048, bias=False)&#xA;          (gk_proj): Sequential(&#xA;            (0): Linear(in_features=2048, out_features=16, bias=False)&#xA;            (1): Linear(in_features=16, out_features=1024, bias=True)&#xA;          )&#xA;          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)&#xA;          (g_norm_swish_gate): FusedRMSNormSwishGate(512, eps=1e-06)&#xA;        )&#xA;        (mlp_norm): RMSNorm(2048, eps=1e-06)&#xA;        (mlp): GatedMLP(&#xA;          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)&#xA;          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)&#xA;          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)&#xA;        )&#xA;      )&#xA;    )&#xA;    (norm): RMSNorm(2048, eps=1e-06)&#xA;  )&#xA;  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fused Modules&lt;/h3&gt; &#xA;&lt;p&gt;We offer a collection of fused modules in &lt;code&gt;fla.modules&lt;/code&gt; to facilitate faster training:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/rotary.py&#34;&gt;&lt;code&gt;Rotary Embedding&lt;/code&gt;&lt;/a&gt;: rotary positional embeddings as adopted by the Llama architecture, a.k.a., Transformer++.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/layernorm.py&#34;&gt;&lt;code&gt;Norm Layers&lt;/code&gt;&lt;/a&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;RMSNorm&lt;/code&gt;, &lt;code&gt;LayerNorm&lt;/code&gt; and &lt;code&gt;GroupNorm&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;RMSNormLinear&lt;/code&gt;, &lt;code&gt;LayerNormLinear&lt;/code&gt; and &lt;code&gt;GroupNormLinear&lt;/code&gt; to reduce memory usage of intermediate tensors for improved memory efficiency.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_norm_gate.py&#34;&gt;&lt;code&gt;Norm Layers with Gating&lt;/code&gt;&lt;/a&gt;: combine norm layers with element-wise gating, as used by RetNet/GLA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_cross_entropy.py&#34;&gt;&lt;code&gt;Cross Entropy&lt;/code&gt;&lt;/a&gt;: faster Triton implementation of cross entropy loss.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_linear_cross_entropy.py&#34;&gt;&lt;code&gt;Linear Cross Entropy&lt;/code&gt;&lt;/a&gt;: fused linear layer and cross entropy loss to avoid the materialization of large logits tensors. Also refer to implementations by &lt;a href=&#34;https://github.com/mgmalek/efficient_cross_entropy&#34;&gt;mgmalek&lt;/a&gt; and &lt;a href=&#34;https://github.com/linkedin/Liger-Kernel/raw/main/src/liger_kernel/ops/fused_linear_cross_entropy.py&#34;&gt;Liger-Kernel&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_kl_div.py&#34;&gt;&lt;code&gt;Linear KL Divergence&lt;/code&gt;&lt;/a&gt;: fused linear layer and KL divergence loss in a similar vein as CE loss.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;Upon successfully pretraining a model, it becomes accessible for generating text using the ü§ó text generation APIs. In the following, we give a generation example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import fla&#xA;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&amp;gt;&amp;gt;&amp;gt; name = &#39;fla-hub/gla-1.3B-100B&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(name)&#xA;&amp;gt;&amp;gt;&amp;gt; model = AutoModelForCausalLM.from_pretrained(name).cuda()&#xA;&amp;gt;&amp;gt;&amp;gt; input_prompt = &#34;Power goes with permanence. Impermanence is impotence. And rotation is castration.&#34;&#xA;&amp;gt;&amp;gt;&amp;gt; input_ids = tokenizer(input_prompt, return_tensors=&#34;pt&#34;).input_ids.cuda()&#xA;&amp;gt;&amp;gt;&amp;gt; outputs = model.generate(input_ids, max_length=64)&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide a simple script &lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/benchmarks/benchmark_generation.py&#34;&gt;here&lt;/a&gt; for benchmarking the generation speed. Simply run it by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ python -m benchmarks.benchmark_generation \&#xA;  --path &#39;fla-hub/gla-1.3B-100B&#39; \&#xA;  --repetition_penalty 2. \&#xA;  --prompt=&#34;Hello everyone, I&#39;m Songlin Yang&#34;&#xA;&#xA;Prompt:&#xA;Hello everyone, I&#39;m Songlin Yang&#xA;Generated:&#xA;Hello everyone, I&#39;m Songlin Yang.&#xA;I am a 20 year old girl from China who is currently studying in the United States of America for my Master degree and also working as an English teacher at school here on campus since last summer (1st semester). My main goal to be able do well with this course so that we can have&#xA;&#xA;Prompt length: 10, generation length: 64&#xA;Total prompt processing + decoding time: 4593ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All of the pretrained models currently available can be found in &lt;a href=&#34;https://huggingface.co/fla-hub&#34;&gt;&lt;code&gt;fla-hub&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from huggingface_hub import list_models&#xA;&amp;gt;&amp;gt;&amp;gt; for model in list_models(author=&#39;fla-hub&#39;): print(model.id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Hybrid Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;fla&lt;/code&gt; provides a flexible method to incorporate standard attention layers into existing linear attention models. This is easily achieved by specifying the &lt;code&gt;attn&lt;/code&gt; argument in the model configuration.&lt;/p&gt; &#xA;&lt;p&gt;For example, to create a 2-layer Samba model with interleaved Mamba and local attention layers, using a sliding window size of 2048:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from fla.models import SambaConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM&#xA;&amp;gt;&amp;gt;&amp;gt; config = SambaConfig(num_hidden_layers=2)&#xA;&amp;gt;&amp;gt;&amp;gt; config.attn = { &#xA;  &#39;layers&#39;: [1], &#xA;  &#39;num_heads&#39;: 18, &#xA;  &#39;num_kv_heads&#39;: 18,&#xA;  &#39;window_size&#39;: 2048&#xA;}&#xA;&amp;gt;&amp;gt;&amp;gt; config&#xA;SambaConfig {&#xA;  &#34;attn&#34;: {&#xA;    &#34;layers&#34;: [&#xA;      1&#xA;    ],&#xA;    &#34;num_heads&#34;: 18,&#xA;    &#34;num_kv_heads&#34;: 18,&#xA;    &#34;window_size&#34;: 2048&#xA;  },&#xA;  &#34;bos_token_id&#34;: 1,&#xA;  &#34;conv_kernel&#34;: 4,&#xA;  &#34;eos_token_id&#34;: 2,&#xA;  &#34;expand&#34;: 2,&#xA;  &#34;fuse_cross_entropy&#34;: true,&#xA;  &#34;fuse_norm&#34;: true,&#xA;  &#34;hidden_act&#34;: &#34;silu&#34;,&#xA;  &#34;hidden_ratio&#34;: 4,&#xA;  &#34;hidden_size&#34;: 2304,&#xA;  &#34;initializer_range&#34;: 0.02,&#xA;  &#34;intermediate_size&#34;: 4608,&#xA;  &#34;max_position_embeddings&#34;: 2048,&#xA;  &#34;model_type&#34;: &#34;samba&#34;,&#xA;  &#34;norm_eps&#34;: 1e-05,&#xA;  &#34;num_hidden_layers&#34;: 2,&#xA;  &#34;pad_token_id&#34;: 0,&#xA;  &#34;rescale_prenorm_residual&#34;: false,&#xA;  &#34;residual_in_fp32&#34;: false,&#xA;  &#34;state_size&#34;: 16,&#xA;  &#34;tie_word_embeddings&#34;: false,&#xA;  &#34;time_step_floor&#34;: 0.0001,&#xA;  &#34;time_step_init_scheme&#34;: &#34;random&#34;,&#xA;  &#34;time_step_max&#34;: 0.1,&#xA;  &#34;time_step_min&#34;: 0.001,&#xA;  &#34;time_step_rank&#34;: 144,&#xA;  &#34;time_step_scale&#34;: 1.0,&#xA;  &#34;transformers_version&#34;: &#34;4.45.0&#34;,&#xA;  &#34;use_bias&#34;: false,&#xA;  &#34;use_cache&#34;: true,&#xA;  &#34;use_conv_bias&#34;: true,&#xA;  &#34;vocab_size&#34;: 32000&#xA;}&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; AutoModelForCausalLM.from_config(config)&#xA;SambaForCausalLM(&#xA;  (backbone): SambaModel(&#xA;    (embeddings): Embedding(32000, 2304)&#xA;    (layers): ModuleList(&#xA;      (0): SambaBlock(&#xA;        (mixer_norm): RMSNorm(2304, eps=1e-05)&#xA;        (mixer): MambaMixer(&#xA;          (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)&#xA;          (act): SiLU()&#xA;          (in_proj): Linear(in_features=2304, out_features=9216, bias=False)&#xA;          (x_proj): Linear(in_features=4608, out_features=176, bias=False)&#xA;          (dt_proj): Linear(in_features=144, out_features=4608, bias=True)&#xA;          (out_proj): Linear(in_features=4608, out_features=2304, bias=False)&#xA;        )&#xA;        (mlp_norm): RMSNorm(2304, eps=1e-05)&#xA;        (mlp): SambaMLP(&#xA;          (gate_proj): Linear(in_features=2304, out_features=12288, bias=False)&#xA;          (down_proj): Linear(in_features=6144, out_features=2304, bias=False)&#xA;          (act_fn): SiLU()&#xA;        )&#xA;      )&#xA;      (1): SambaBlock(&#xA;        (mixer_norm): RMSNorm(2304, eps=1e-05)&#xA;        (mixer): Attention(&#xA;          (q_proj): Linear(in_features=2304, out_features=2304, bias=False)&#xA;          (k_proj): Linear(in_features=2304, out_features=2304, bias=False)&#xA;          (v_proj): Linear(in_features=2304, out_features=2304, bias=False)&#xA;          (o_proj): Linear(in_features=2304, out_features=2304, bias=False)&#xA;          (rotary): RotaryEmbedding()&#xA;        )&#xA;        (mlp_norm): RMSNorm(2304, eps=1e-05)&#xA;        (mlp): SambaMLP(&#xA;          (gate_proj): Linear(in_features=2304, out_features=12288, bias=False)&#xA;          (down_proj): Linear(in_features=6144, out_features=2304, bias=False)&#xA;          (act_fn): SiLU()&#xA;        )&#xA;      )&#xA;    )&#xA;    (norm_f): RMSNorm(2304, eps=1e-05)&#xA;  )&#xA;  (lm_head): Linear(in_features=2304, out_features=32000, bias=False)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During inference, you &lt;strong&gt;DO NOT&lt;/strong&gt; need to revise anything for generation! The model will produce output as-is, without any need for additional configurations or modifications.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm-evaluation-harness&lt;/a&gt; library allows you to easily perform (zero-shot) model evaluations. Follow the steps below to use this library:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;lm_eval&lt;/code&gt; following &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness/raw/main/README.md&#34;&gt;their instructions&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run evaluation with:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ PATH=&#39;fla-hub/gla-1.3B-100B&#39;&#xA;$ python -m evals.harness --model hf \&#xA;    --model_args pretrained=$PATH,dtype=bfloat16 \&#xA;    --tasks wikitext,lambada_openai,piqa,hellaswag,winogrande,arc_easy,arc_challenge,boolq,sciq,copa,openbookqa \&#xA;    --batch_size 64 \&#xA;    --num_fewshot 0 \&#xA;    --device cuda \&#xA;    --show_config                  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We&#39;ve made &lt;code&gt;fla&lt;/code&gt; compatible with hf-style evaluations, you can call &lt;a href=&#34;https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/evals/harness.py&#34;&gt;evals.harness&lt;/a&gt; to finish the evaluations. Running the command above will provide the task results reported in the GLA paper.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Tip] If you are using &lt;code&gt;lm-evaluation-harness&lt;/code&gt; as an external library and can&#39;t find (almost) any tasks available, before calling &lt;code&gt;lm_eval.evaluate()&lt;/code&gt; or &lt;code&gt;lm_eval.simple_evaluate()&lt;/code&gt;, simply run the following to load the library&#39;s stock tasks!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from lm_eval.tasks import TaskManager; TaskManager().initialize_tasks()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;We compared our Triton-based RetNet implementation with CUDA-based FlashAttention2, using a batch size of 8, 32 heads, and a head dimension of 128, across different sequence lengths. These tests were conducted on a single A100 80GB GPU, as illustrated in the following graph&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# you might have to first install `fla` to enable its import via `pip install -e .`&#xA;$ python benchmark_retention.py&#xA;Performance:&#xA;   seq_len  fused_chunk_fwd  chunk_fwd  parallel_fwd  fused_chunk_fwdbwd  chunk_fwdbwd  parallel_fwdbwd  flash_fwd  flash_fwdbwd&#xA;0    128.0         0.093184   0.185344      0.067584            1.009664      1.591296         1.044480   0.041984      0.282624&#xA;1    256.0         0.165888   0.219136      0.126976            1.024000      1.596928         1.073152   0.074752      0.413696&#xA;2    512.0         0.308224   0.397312      0.265216            1.550336      1.603584         1.301504   0.156672      0.883712&#xA;3   1024.0         0.603136   0.747520      0.706560            3.044864      3.089408         3.529728   0.467968      2.342912&#xA;4   2048.0         1.191424   1.403904      2.141184            6.010880      6.059008        11.009024   1.612800      7.135232&#xA;5   4096.0         2.377728   2.755072      7.392256           11.932672     11.938816        37.792770   5.997568     24.435200&#xA;6   8192.0         4.750336   5.491712     26.402817           23.759359     23.952385       141.014023  22.682114     90.619904&#xA;7  16384.0         9.591296  10.870784    101.262337           47.666176     48.745472       539.853821  91.346947    346.318848&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/fla-org/flash-linear-attention/assets/30831390/36961182-da39-48ba-96a6-84c572ce51d7&#34; alt=&#34;Performance&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository helpful, please cite our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@software{yang2024fla,&#xA;  title  = {FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism},&#xA;  author = {Yang, Songlin and Zhang, Yu},&#xA;  url    = {https://github.com/fla-org/flash-linear-attention},&#xA;  month  = jan,&#xA;  year   = {2024}&#xA;}&#xA;&#xA;@misc{yang2024gated,&#xA;    title         = {Gated Delta Networks: Improving Mamba2 with Delta Rule},&#xA;    author        = {Songlin Yang and Jan Kautz and Ali Hatamizadeh},&#xA;    year          = {2024},&#xA;    eprint        = {2412.06464},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass  = {cs.CL}&#xA;}&#xA;&#xA;@inproceedings{yang2024parallelizing,&#xA;  title     = {Parallelizing Linear Transformers with the Delta Rule over Sequence Length},&#xA;  author    = {Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},&#xA;  booktitle = {Proceedings of NeurIPS},&#xA;  year      = {2024}&#xA;}&#xA;&#xA;@inproceedings{zhang2024gsa,&#xA;  title     = {Gated Slot Attention for Efficient Linear-Time Sequence Modeling},&#xA;  author    = {Zhang, Yu and Yang, Songlin and Zhu, Ruijie and Zhang, Yue and Cui, Leyang and Wang, Yiqiao and Wang, Bolun and Shi, Freda and Wang, Bailin and Bi, Wei and Zhou, Peng and Fu, Guohong},&#xA;  booktitle = {Proceedings of NeurIPS},&#xA;  year      = {2024}&#xA;}&#xA;&#xA;@inproceedings{yang2024gla,&#xA;  title     = {Gated Linear Attention Transformers with Hardware-Efficient Training},&#xA;  author    = {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},&#xA;  booktitle = {Proceedings of ICML},&#xA;  year      = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention/stargazers&#34;&gt;&lt;img src=&#34;https://bytecrank.com/nastyox/reporoster/php/stargazersSVG.php?user=fla-org&amp;amp;repo=flash-linear-attention&#34; alt=&#34;Stargazers repo roster for @fla-org/flash-linear-attention&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#fla-org/flash-linear-attention&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=fla-org/flash-linear-attention&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>