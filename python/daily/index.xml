<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-11T01:33:30Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Yuukiy/JavSP</title>
    <updated>2024-03-11T01:33:30Z</updated>
    <id>tag:github.com,2024-03-11:/Yuukiy/JavSP</id>
    <link href="https://github.com/Yuukiy/JavSP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;汇总多站点数据的AV元数据刮削器&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/Yuukiy/JavSP/raw/master/image/javsp_logo.png?raw=true&#34; alt=&#34;JavSP&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Jav Scraper Package&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;汇总多站点数据的AV元数据刮削器&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;提取影片文件名中的番号信息，自动抓取并汇总多个站点数据的 AV 元数据，按照指定的规则分类整理影片文件，并创建供 Emby、Jellyfin、Kodi 等软件使用的元数据文件&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Docker &amp;amp; WebUI&lt;/strong&gt;: 由于精力所限，目前还没有做Docker的支持。此外，UI界面也不是&lt;a href=&#34;https://github.com/Yuukiy/JavSP/issues/148&#34;&gt;此项目的目标&lt;/a&gt;。如果你需要这两个功能，可以试试&lt;a href=&#34;https://github.com/tetato/JavSP-Docker&#34;&gt;@tetato/JavSP-Docker&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;i18n&lt;/strong&gt;: This project currently supports only Chinese. However, if you&#39;re willing, you can &lt;a href=&#34;https://github.com/Yuukiy/JavSP/discussions/157&#34;&gt;vote here&lt;/a&gt; for the language you&#39;d like to see added&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/Yuukiy/JavSP&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://github.com/996icu/996.ICU/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Anti%20996-blue.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/python-3.8-green.svg?sanitize=true&#34; alt=&#34;Python 3.8&#34;&gt; &lt;a href=&#34;https://github.com/Yuukiy/JavSP/actions/workflows/test-web-funcs.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/Yuukiy/JavSP/test-web-funcs.yml?label=crawlers%20test&#34; alt=&#34;Crawlers test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Yuukiy/JavSP/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/Yuukiy/JavSP&#34; alt=&#34;Latest release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://996.icu&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/link-996.icu-red.svg?sanitize=true&#34; alt=&#34;996.icu&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;功能特点&lt;/h2&gt; &#xA;&lt;p&gt;下面这些是一些已实现或待实现的功能，在逐渐实现和完善，如果想到新的功能点也会加进来。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 自动识别影片番号&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持处理影片分片&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 汇总多个站点的数据生成NFO数据文件&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 每天自动对站点抓取器进行测试&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 多线程并行抓取&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 下载高清封面&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 基于AI人体分析裁剪素人等非常规封面的海报&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 自动检查和更新新版本&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 翻译标题和剧情简介&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 匹配本地字幕&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 使用小缩略图创建文件夹封面&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 保持不同站点间 genre 分类的统一&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 不同的运行模式（抓取数据+整理，仅抓取数据）&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 可选：所有站点均抓取失败时由人工介入&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;想要快速上手？&lt;/p&gt; &lt;p&gt;前往&lt;a href=&#34;https://github.com/Yuukiy/JavSP/releases/latest&#34;&gt;软件发布页&lt;/a&gt;下载最新版本的软件，无需安装额外工具，开箱即用&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;更喜欢源代码？&lt;/p&gt; &lt;p&gt;请确保已安装 Python （此项目以 Python 3.8 开发）&lt;/p&gt; &lt;pre&gt;&lt;code&gt; git clone https://github.com/Yuukiy/JavSP.git&#xA; cd JavSP&#xA; pip install -r requirements.txt&#xA; python JavSP.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用&lt;/h2&gt; &#xA;&lt;p&gt;软件开箱即用，首次运行时会在软件目录下生成默认的配置文件 &lt;code&gt;config.ini&lt;/code&gt;。如果想让软件更符合你的使用需求，也许你需要更改配置文件:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;以任意文本编辑器打开 &lt;code&gt;config.ini&lt;/code&gt;，根据各个配置项的说明选择你需要的配置即可。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;此外软件也支持从命令行指定运行参数（命令行参数的优先级高于配置文件）。运行 &lt;code&gt;JavSP -h&lt;/code&gt; 查看支持的参数列表&lt;/p&gt; &#xA;&lt;p&gt;更详细的使用说明请前往 &lt;a href=&#34;https://github.com/Yuukiy/JavSP/wiki&#34;&gt;JavSP Wiki&lt;/a&gt; 查看&lt;/p&gt; &#xA;&lt;p&gt;如果使用的时候遇到问题也欢迎给我反馈😊&lt;/p&gt; &#xA;&lt;h2&gt;问题反馈&lt;/h2&gt; &#xA;&lt;p&gt;如果使用中遇到了 Bug，请&lt;a href=&#34;https://github.com/Yuukiy/JavSP/issues&#34;&gt;前往 Issue 区反馈&lt;/a&gt;（提问前请先搜索是否已有类似问题）&lt;/p&gt; &#xA;&lt;h2&gt;参与贡献&lt;/h2&gt; &#xA;&lt;p&gt;此项目不需要捐赠。如果你想要帮助改进这个项目，欢迎通过以下方式参与进来（并不仅局限于代码）：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;帮助撰写和改进Wiki&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;帮助完善单元测试数据（不必非要写代码，例如如果你发现有某系列的番号识别不准确，总结一下提issue也是很好的）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;帮助翻译 genre&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bugfix / 新功能？欢迎发 Pull Request&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;要不考虑点个 Star ?（我会很开心的）&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;许可&lt;/h2&gt; &#xA;&lt;p&gt;此项目的所有权利与许可受 GPL-3.0 License 与 &lt;a href=&#34;https://github.com/996icu/996.ICU/raw/master/LICENSE_CN&#34;&gt;Anti 996 License&lt;/a&gt; 共同限制。此外，如果你使用此项目，表明你还额外接受以下条款：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;本软件仅供学习 Python 和技术交流使用&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;请勿在微博、微信等墙内的公共社交平台上宣传此项目&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;用户在使用本软件时，请遵守当地法律法规&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;禁止将本软件用于商业用途&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Yuukiy/JavSP&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Vahe1994/AQLM</title>
    <updated>2024-03-11T01:33:30Z</updated>
    <id>tag:github.com,2024-03-11:/Vahe1994/AQLM</id>
    <link href="https://github.com/Vahe1994/AQLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Pytorch repository for Extreme Compression of Large Language Models via Additive Quantization https://arxiv.org/pdf/2401.06118.pdf&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AQLM&lt;/h1&gt; &#xA;&lt;p&gt;Official PyTorch implementation for &lt;a href=&#34;https://arxiv.org/pdf/2401.06118.pdf&#34;&gt;Extreme Compression of Large Language Models via Additive Quantization&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;Learn how to run the prequantized models using this Google Colab examples:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Basic AQLM &lt;br&gt; generation&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Streaming with &lt;br&gt; GPU/CPU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Inference with CUDA &lt;br&gt; graphs (3x speedup)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Fine-tuning &lt;br&gt; with PEFT&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/colab_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;AQLM In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/streaming_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;AQLM In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/aqlm_cuda_graph.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/aqlm_2bit_training.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;This repository is currently designed to work with models of &lt;code&gt;LLaMA&lt;/code&gt;, &lt;code&gt;Mistral&lt;/code&gt; and &lt;code&gt;Mixtral&lt;/code&gt; families. The models reported below use &lt;strong&gt;full model fine-tuning&lt;/strong&gt; as described in appendix A, with cross-entropy objective with teacher logits.&lt;/p&gt; &#xA;&lt;p&gt;We provide a number of prequantized models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;AQLM scheme&lt;/th&gt; &#xA;   &lt;th&gt;WikiText 2 PPL&lt;/th&gt; &#xA;   &lt;th&gt;Model size, Gb&lt;/th&gt; &#xA;   &lt;th&gt;Hub link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;5.92&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b&lt;/td&gt; &#xA;   &lt;td&gt;2x8&lt;/td&gt; &#xA;   &lt;td&gt;6.69&lt;/td&gt; &#xA;   &lt;td&gt;2.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-7b-AQLM-2Bit-2x8-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b&lt;/td&gt; &#xA;   &lt;td&gt;8x8&lt;/td&gt; &#xA;   &lt;td&gt;6.61&lt;/td&gt; &#xA;   &lt;td&gt;2.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-7b-AQLM-2Bit-8x8-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-13b&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;5.22&lt;/td&gt; &#xA;   &lt;td&gt;4.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-13b-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-70b&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;3.83&lt;/td&gt; &#xA;   &lt;td&gt;18.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-70b-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-70b&lt;/td&gt; &#xA;   &lt;td&gt;2x8&lt;/td&gt; &#xA;   &lt;td&gt;4.21&lt;/td&gt; &#xA;   &lt;td&gt;18.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-70b-AQLM-2Bit-2x8-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral-8x7b&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;3.35&lt;/td&gt; &#xA;   &lt;td&gt;12.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral-8x7b-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;12.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Mixtral-8x7B-Instruct-v0_1-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference kernels&lt;/h3&gt; &#xA;&lt;p&gt;AQLM quantization setpus vary mainly on the number of codebooks used as well as the codebook sizes in bits. The most popular setups, as well as inference kernels they support are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kernel&lt;/th&gt; &#xA;   &lt;th&gt;Number of codebooks&lt;/th&gt; &#xA;   &lt;th&gt;Codebook size, bits&lt;/th&gt; &#xA;   &lt;th&gt;Scheme Notation&lt;/th&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Speedup&lt;/th&gt; &#xA;   &lt;th&gt;Fast GPU inference&lt;/th&gt; &#xA;   &lt;th&gt;Fast CPU inference&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Triton&lt;/td&gt; &#xA;   &lt;td&gt;K&lt;/td&gt; &#xA;   &lt;td&gt;N&lt;/td&gt; &#xA;   &lt;td&gt;KxN&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Up to ~0.7x&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CUDA&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;Best&lt;/td&gt; &#xA;   &lt;td&gt;Up to ~1.3x&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CUDA&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2x8&lt;/td&gt; &#xA;   &lt;td&gt;OK&lt;/td&gt; &#xA;   &lt;td&gt;Up to ~3.0x&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Numba&lt;/td&gt; &#xA;   &lt;td&gt;K&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;Kx8&lt;/td&gt; &#xA;   &lt;td&gt;Good&lt;/td&gt; &#xA;   &lt;td&gt;Up to ~4.0x&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;To run the models, one would have to install an inference library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install aqlm[gpu,cpu]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;, specifying either &lt;code&gt;gpu&lt;/code&gt;, &lt;code&gt;cpu&lt;/code&gt; or both based on one&#39;s inference setting.&lt;/p&gt; &#xA;&lt;p&gt;Then, one can use the familiar &lt;code&gt;.from_pretrained&lt;/code&gt; method provided by the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM&#xA;&#xA;quantized_model = AutoModelForCausalLM.from_pretrained(&#xA;    &#34;ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf&#34;,&#xA;    trust_remote_code=True, torch_dtype=&#34;auto&#34;&#xA;).cuda()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice that &lt;code&gt;torch_dtype&lt;/code&gt; should be set to either &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;&#34;auto&#34;&lt;/code&gt; on GPU and &lt;code&gt;torch.float32&lt;/code&gt; on CPU. After that, the model can be used exactly the same as one would use and unquantized model.&lt;/p&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install packages from &lt;code&gt;requirements.txt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Loading / caching datasets and tokenizer&lt;/h3&gt; &#xA;&lt;p&gt;The script will require downloading and caching locally the relevant tokenizer and the datasets. They will be saved in default Huggingface Datasets directory unless alternative location is provided by env variables. See &lt;a href=&#34;https://huggingface.co/docs/datasets/main/en/cache#cache-directory&#34;&gt;relevant Datasets documentation section&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;When quantizing models with AQLM, we recommend that you use a subset of the original data the model was trained on.&lt;/p&gt; &#xA;&lt;p&gt;For Llama-2 models, the closest available dataset is &lt;a href=&#34;https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample&#34;&gt;RedPajama&lt;/a&gt; . To load subset of RedPajama provide &#34;pajama&#34; in --dataset argument. This will process nsamples data and tokenize it using provided model tokenizer.&lt;/p&gt; &#xA;&lt;p&gt;Additionally we provide tokenized Redpajama for LLama and Solar/Mistral models for 4096 context lengths stored in &lt;a href=&#34;https://huggingface.co/datasets/Vahe1994/AQLM&#34;&gt;Hunggingface&lt;/a&gt; . To load it, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from huggingface_hub import hf_hub_download&#xA;&#xA;hf_hub_download(repo_id=&#34;Vahe1994/AQLM&#34;, filename=&#34;data/name.pth&#34;,repo_type=&#34;dataset&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use downloaded data from HF, place it in data folder(optional) and set correct path to it in &#34;--dataset&#34; argument in main.py.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; These subsets are already processed with the corresponding model tokenizer. If you want to quantize another model (e.g. mistral/mixtral), please re-tokenize the data with provided script in src/datautils.&lt;/p&gt; &#xA;&lt;h3&gt;WandB logging&lt;/h3&gt; &#xA;&lt;p&gt;One can optionally log the data to &lt;code&gt;Weights and Biases&lt;/code&gt; service (wandb). Run &lt;code&gt;pip install wandb&lt;/code&gt; for W&amp;amp;B logging. Specify &lt;code&gt;$WANDB_ENTITY&lt;/code&gt;, &lt;code&gt;$WANDB_PROJECT&lt;/code&gt;, &lt;code&gt;$WANDB_NAME&lt;/code&gt; environment variables prior to running experiments. use &lt;code&gt;--wandb&lt;/code&gt; argument to enable logging&lt;/p&gt; &#xA;&lt;h3&gt;GPU and RAM requirements&lt;/h3&gt; &#xA;&lt;p&gt;This code was developed and tested using a several A100 GPU with 80GB GPU RAM. You can use the &lt;code&gt;--offload activations&lt;/code&gt; option to reduce VRAM usage. For &lt;code&gt;Language Model Evaluation Harness&lt;/code&gt; evaluation one needs to have enough memory to load whole model + activation tensors on one or several devices.&lt;/p&gt; &#xA;&lt;h3&gt;Quantization time&lt;/h3&gt; &#xA;&lt;p&gt;AQLM quantization takes considerably longer to calibrate than simpler quantization methods such as GPTQ. This only impacts quantization time, not inference time.&lt;/p&gt; &#xA;&lt;p&gt;For instance, quantizing a 7B model with default configuration takes about 1 day on a single A100 gpu. Similarly, quantizing a 70B model on a single GPU would take 10-14 days. If you have multiple GPUs with fast interconnect, you can run AQLM multi-gpu to speed up comparison - simply set CUDA_VISIBLE_DEVICES for multiple GPUs. Quantizing 7B model on two gpus reduces quantization time to ~14.5 hours. Similarly, quantizing a 70B model on 8 x A100 GPUs takes 3 days 18 hours.&lt;/p&gt; &#xA;&lt;p&gt;If you need to speed up quantization without adding more GPUs, you may also increase --relative_mse_tolerance , --finetune_relative_mse_tolerance or set --init_max_points_per_centroid . However, that usually comes at a cost of reduced model accuracy.&lt;/p&gt; &#xA;&lt;h3&gt;Model downloading&lt;/h3&gt; &#xA;&lt;p&gt;The code requires the LLaMA model to be downloaded in Huggingface format and saved locally. The scripts below assume that &lt;code&gt;$TRANSFORMERS_CACHE&lt;/code&gt; variable points to the Huggingface Transformers cache folder. To download and cache the models, run this in the same environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;model_name = &#34;meta-llama/Llama-2-7b-hf&#34;  # or whatever else you wish to download&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=&#34;auto&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to quantize a model with AQLM&lt;/h3&gt; &#xA;&lt;p&gt;This script compresses the model and then tests its performance in terms of perplexity using WikiText2, C4, and Penn Treebank datasets.&lt;/p&gt; &#xA;&lt;p&gt;The command to launch the script should look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0   # or e.g. 0,1,2,3&#xA;export MODEL_PATH=&amp;lt;PATH_TO_MODEL_ON_HUB&amp;gt;&#xA;export DATASET_PATH=&amp;lt;INSERT DATASET NAME OR PATH TO CUSTOM DATA&amp;gt;&#xA;export SAVE_PATH=/path/to/save/quantized/model/&#xA;export WANDB_PROJECT=MY_AQ_EXPS&#xA;export WANDB_NAME=COOL_EXP_NAME&#xA;&#xA;python main.py $MODEL_PATH $DATASET_PATH --nsamples=1024 \&#xA; --num_codebooks=1 --nbits_per_codebook=16 --in_group_size=8 \&#xA; --relative_mse_tolerance=0.01 --finetune_relative_mse_tolerance=0.001 \&#xA; --finetune_batch_size=32 --local_batch_size=1 --offload_activations \&#xA; --wandb --save $SAVE_PATH&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Main CLI arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; - by default, the code will use all available GPUs. If you want to use specific GPUs (or one GPU), use this variable.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - a path to either hugginface hub (e.g. meta-llama/Llama-2-7b-hf) or a local folder with transformers model and a tokenizer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DATASET_PATH&lt;/code&gt; - either a path to calibration data (see above) or a standard dataset &lt;code&gt;[c4, ptb, wikitext2]&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;for llama-2 models, you can use &lt;code&gt;DATASET_PATH=./data/red_pajama_n=1024_4096_context_length.pth&lt;/code&gt; for a slice of RedPajama (up to 1024 samples)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--nsamples&lt;/code&gt; - the number of calibration data &lt;em&gt;sequences&lt;/em&gt;. If this parameter is not set, take all calibration data avaialble.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_codebooks&lt;/code&gt; - number of codebooks per layer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--nbits_per_codebook&lt;/code&gt; - each codebook will contain 2 ** nbits_per_codebook vectors&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--in_group_size&lt;/code&gt; - how many weights are quantized together (aka &#34;g&#34; in the arXiv paper)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--finetune_batch_size&lt;/code&gt; - (for fine-tuning only) the total number of sequences used for each optimization step&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--local_batch_size&lt;/code&gt; - when accumulating finetune_batch_size, process this many samples per GPU per forward pass (affects GPU RAM usage)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--relative_mse_tolerance&lt;/code&gt;- (for initial calibration) - stop training when (current_epoch_mse / previous_epoch_mse) &amp;gt; (1 - relative_mse_tolerance)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--finetune_relative_mse_tolerance&lt;/code&gt;- same, but for fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--offload_activations&lt;/code&gt; -- during calibration, move activations from GPU memory to RAM. This reduces VRAM usage while slowing calibration by ~10% (depending on your hardware).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--save&lt;/code&gt; -- path to save/load quantized model. (see also: &lt;code&gt;--load&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--wandb&lt;/code&gt; - if this parameter is set, the code will log results to wandb&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are additional hyperparameters aviailable. Run &lt;code&gt;python main.py --help&lt;/code&gt; for more details on command line arguments, including compression parameters.&lt;/p&gt; &#xA;&lt;h3&gt;Zero-shot benchmarks via LM Evaluation Harness&lt;/h3&gt; &#xA;&lt;p&gt;To perform zero-shot evaluation, we use &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;Language Model Evaluation Harness&lt;/a&gt; framework with slight modifications. This repository contains a copy of LM Evaluation Harness repo from early 2023 in &lt;code&gt;lm-eval-harness&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Before running the code make sure that you have all the requirements and dependencies of &lt;code&gt;lm-eval-harness&lt;/code&gt; installed. To install them run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r lm-evaluation-harness/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The main script launching the evaluation procedure is &lt;code&gt;lmeval.py&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0,1,2,3  # optional: select GPUs&#xA;export QUANTZED_MODEL=&amp;lt;PATH_TO_SAVED_QUANTIZED_MODEL_FROM_MAIN.py&amp;gt;&#xA;export MODEL_PATH=&amp;lt;INSERT_PATH_TO_ORIINAL_MODEL_ON_HUB&amp;gt;&#xA;export DATASET=&amp;lt;INSERT DATASET NAME OR PATH TO CUSTOM DATA&amp;gt;&#xA;export WANDB_PROJECT=MY_AQ_LM_EVAL&#xA;export WANDB_NAME=COOL_EVAL_NAME&#xA;&#xA;python lmeval.py \&#xA;    --model hf-causal \&#xA;    --model_args pretrained=$MODEL_PATH,dtype=float16,use_accelerate=True \&#xA;    --load $QUANTZED_MODEL \&#xA;    --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \&#xA;    --batch_size 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Preparing models for inference&lt;/h3&gt; &#xA;&lt;p&gt;To convert a model into a &lt;em&gt;Hugging Face&lt;/em&gt; compatible format, use &lt;code&gt;convert_to_hf.py&lt;/code&gt; with corresponding arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model&lt;/code&gt; - the original pretrained model (corresponds to &lt;code&gt;MODEL_PATH&lt;/code&gt; of &lt;code&gt;main.py&lt;/code&gt;, e.g. &lt;code&gt;meta-llama/Llama-2-7b-hf&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--in_path&lt;/code&gt; - the folder containing an initially quantized model (corresponds to &lt;code&gt;--save&lt;/code&gt; of &lt;code&gt;main.py&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--out_path&lt;/code&gt; - the folder to save &lt;code&gt;transformers&lt;/code&gt; model to.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The conversion automatically&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you want to contribute something substantial (more than a typo), please open an issue first. We use black and isort for all pull requests. Before committing your code run &lt;code&gt;black . &amp;amp;&amp;amp; isort .&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;If you found this work useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{egiazarian2024extreme,&#xA;      title={Extreme Compression of Large Language Models via Additive Quantization}, &#xA;      author={Vage Egiazarian and Andrei Panferov and Denis Kuznedelev and Elias Frantar and Artem Babenko and Dan Alistarh},&#xA;      year={2024},&#xA;      eprint={2401.06118},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>hatchet-dev/hatchet</title>
    <updated>2024-03-11T01:33:30Z</updated>
    <id>tag:github.com,2024-03-11:/hatchet-dev/hatchet</id>
    <link href="https://github.com/hatchet-dev/hatchet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A distributed, fault-tolerant task queue&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://framerusercontent.com/images/KBMnpSO12CyE6UANhf4mhrg6na0.png?scale-down-to=200&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://framerusercontent.com/images/KBMnpSO12CyE6UANhf4mhrg6na0.png?scale-down-to=200&#34;&gt; &#xA;  &lt;a href=&#34;https://hatchet.run&#34;&gt; &lt;img alt=&#34;Hatchet Logo&#34; src=&#34;https://framerusercontent.com/images/KBMnpSO12CyE6UANhf4mhrg6na0.png?scale-down-to=200&#34;&gt; &lt;/a&gt; &#xA; &lt;/picture&gt; &#xA; &lt;h3&gt;A Distributed, Fault-Tolerant Task Queue&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://docs.hatchet.run&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-docs.hatchet.run-3F16E4&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-purple.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pkg.go.dev/github.com/hatchet-dev/hatchet&#34;&gt;&lt;img src=&#34;https://pkg.go.dev/badge/github.com/hatchet-dev/hatchet.svg?sanitize=true&#34; alt=&#34;Go Reference&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/@hatchet-dev/typescript-sdk&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/dm/%40hatchet-dev%2Ftypescript-sdk&#34; alt=&#34;NPM Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/ZMeUafwH89&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1088927970518909068?style=social&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hatchet_dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/hatchet-dev.svg?style=social&amp;amp;label=Follow%20%40hatchet-dev&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hatchet-dev/hatchet&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hatchet-dev/hatchet?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.hatchet.run&#34;&gt;Documentation&lt;/a&gt; · &lt;a href=&#34;https://hatchet.run&#34;&gt;Website&lt;/a&gt; · &lt;a href=&#34;https://github.com/hatchet-dev/hatchet/issues&#34;&gt;Issues&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;What is Hatchet?&lt;/h3&gt; &#xA;&lt;p&gt;Hatchet replaces difficult to manage legacy queues or pub/sub systems so you can design durable workloads that recover from failure and solve for problems like &lt;strong&gt;concurrency&lt;/strong&gt;, &lt;strong&gt;fairness&lt;/strong&gt;, and &lt;strong&gt;rate limiting&lt;/strong&gt;. Instead of managing your own task queue or pub/sub system, you can use Hatchet to distribute your functions between a set of workers with minimal configuration or infrastructure:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; height=&#34;500&#34; src=&#34;https://github.com/hatchet-dev/hatchet/assets/25448214/c3defa1e-d9d9-4419-94e5-b4ea4a748f8d&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What Makes Hatchet Great?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;⚡️ &lt;strong&gt;Ultra-low Latency and High Throughput Scheduling:&lt;/strong&gt; Hatchet is built on a low-latency queue (&lt;code&gt;25ms&lt;/code&gt; average start), perfectly balancing real-time interaction capabilities with the reliability required for mission-critical tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;☮️ &lt;strong&gt;Concurrency, Fairness, and Rate Limiting:&lt;/strong&gt; Implement FIFO, LIFO, Round Robin, and Priority Queues with Hatchet’s built-in strategies, designed to circumvent common scaling pitfalls with minimal configuration. &lt;a href=&#34;https://docs.hatchet.run&#34;&gt;Read Docs →&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥🧯 &lt;strong&gt;Resilience by Design:&lt;/strong&gt; With customizable retry policies and integrated error handling, Hatchet ensures your operations recover swiftly from transient failures. You can break large jobs down into small tasks so you can finish a run without rerunning work. &lt;a href=&#34;https://docs.hatchet.run&#34;&gt;Read Docs →&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhanced Visibility and Control:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Observability.&lt;/strong&gt; All of your runs are fully searchable, allowing you to quickly identify issues. We track latency, error rates, or custom metrics in your run.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(Practical) Durable Execution.&lt;/strong&gt; Replay events and manually pick up execution from specific steps in your workflow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cron.&lt;/strong&gt; Set recurring schedules for functions runs to execute.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;One-Time Scheduling.&lt;/strong&gt; Schedule a function run to execute at a specific time and date in the future.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spike Protection.&lt;/strong&gt; Smooth out spikes in traffic and only execute what your system can handle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Incremental Streaming.&lt;/strong&gt; Subscribe to updates as your functions progress in the background worker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example Use Cases:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fairness for Generative AI:&lt;/strong&gt; Don&#39;t let busy users overwhelm your system. Hatchet lets you distribute requests to your workers fairly with configurable policies.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Batch Processing for Document Indexing:&lt;/strong&gt; Hatchet can handle large-scale batch processing of documents, images, and other data and resume mid-job on failure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Workflow Orchestration for Multi-Modal Systems:&lt;/strong&gt; Hatchet can handle orchestrating multi-modal inputs and outputs, with full DAG-style execution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Correctness for Event-Based Processing:&lt;/strong&gt; Respond to external events or internal events within your system and replay events automatically.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Hatchet supports your technology stack with open-source SDKs for Python, Typescript, and Go. To get started, see the Hatchet documentation &lt;a href=&#34;https://docs.hatchet.run/home/quickstart/installation&#34;&gt;here&lt;/a&gt;, or check out our quickstart repos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hatchet-dev/hatchet-go-quickstart&#34;&gt;Go SDK Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hatchet-dev/hatchet-python-quickstart&#34;&gt;Python SDK Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hatchet-dev/hatchet-typescript-quickstart&#34;&gt;Typescript SDK Quickstart&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SDK repositories&lt;/h3&gt; &#xA;&lt;p&gt;Hatchet comes with a native Go SDK. The following SDKs are also available:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hatchet-dev/hatchet-typescript&#34;&gt;Typescript SDK&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you encounter any issues with the SDKs, please submit an issue in the respective repository.&lt;/p&gt; &#xA;&lt;h4&gt;Is there a managed cloud version of Hatchet?&lt;/h4&gt; &#xA;&lt;p&gt;Yes, we are offering a have a cloud version to select companies while in beta who are helping to build and shape the product. Please &lt;a href=&#34;mailto:contact@hatchet.run&#34;&gt;reach out&lt;/a&gt; or &lt;a href=&#34;https://hatchet.run/request-access&#34;&gt;request access&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h4&gt;Is there a self-hosted version of Hatchet?&lt;/h4&gt; &#xA;&lt;p&gt;Yes, instructions for self-hosting our open source docker containers can be found in our &lt;a href=&#34;https://docs.hatchet.run/self-hosting/docker-compose&#34;&gt;documentation&lt;/a&gt;. Please &lt;a href=&#34;mailto:contact@hatchet.run&#34;&gt;reach out&lt;/a&gt; if you&#39;re interested in support.&lt;/p&gt; &#xA;&lt;h2&gt;How does this compare to alternatives (Celery, BullMQ)?&lt;/h2&gt; &#xA;&lt;p&gt;Why build another managed queue? We wanted to build something with the benefits of full transactional enqueueing - particularly for dependent, DAG-style execution - and felt strongly that Postgres solves for 99.9% of queueing use-cases better than most alternatives (Celery uses Redis or RabbitMQ as a broker, BullMQ uses Redis). Since the introduction of &lt;code&gt;SKIP LOCKED&lt;/code&gt; and the milestones of recent PG releases (like active-active replication), it&#39;s becoming more feasible to horizontally scale Postgres across multiple regions and vertically scale to 10k TPS or more. Many queues (like BullMQ) are built on Redis and data loss can occur when suffering OOM if you&#39;re not careful, and using PG helps avoid an entire class of problems.&lt;/p&gt; &#xA;&lt;p&gt;We also wanted something that was significantly easier to use and debug for application developers. A lot of times the burden of building task observability falls on the infra/platform team (for example, asking the infra team to build a Grafana view for their tasks based on exported prom metrics). We&#39;re building this type of observability directly into Hatchet.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please submit any bugs that you encounter via Github issues. However, please reach out on &lt;a href=&#34;https://discord.gg/ZMeUafwH89&#34;&gt;Discord&lt;/a&gt; before submitting a feature request - as the project is very early, we&#39;d like to build a solid foundation before adding more complex features.&lt;/p&gt; &#xA;&lt;h2&gt;I&#39;d Like to Contribute&lt;/h2&gt; &#xA;&lt;p&gt;See the contributing docs &lt;a href=&#34;https://docs.hatchet.run/contributing&#34;&gt;here&lt;/a&gt;, and please let us know what you&#39;re interesting in working on in the #contributing channel on &lt;a href=&#34;https://discord.gg/ZMeUafwH89&#34;&gt;Discord&lt;/a&gt;. This will help us shape the direction of the project and will make collaboration much easier!&lt;/p&gt;</summary>
  </entry>
</feed>