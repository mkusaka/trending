<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-27T01:32:44Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Frimkron/mud-pi</title>
    <updated>2024-02-27T01:32:44Z</updated>
    <id>tag:github.com,2024-02-27:/Frimkron/mud-pi</id>
    <link href="https://github.com/Frimkron/mud-pi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple MUD server in Python, for teaching purposes, which could be run on a Raspberry Pi&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MUD Pi&lt;/h1&gt; &#xA;&lt;p&gt;A simple text-based Multi-User Dungeon (MUD) game, which could be run on a Raspberry Pi or other low-end server.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You will need to install &lt;em&gt;Python&lt;/em&gt; (2.7+ or 3.3+) where you wish to run the server. Installers for Windows and Mac can be found at &lt;a href=&#34;http://www.python.org/download/&#34;&gt;http://www.python.org/download/&lt;/a&gt;. There are also tarballs for Linux, although the best way to install on Linux would be via the package manager.&lt;/p&gt; &#xA;&lt;p&gt;To allow players to connect remotely, the server will also need to be connected to the internet.&lt;/p&gt; &#xA;&lt;p&gt;To connect to the server you will need a telnet client. On Mac, Linux, and versions of Windows prior to Windows Vista, the telnet client is usually installed by default. For Windows Vista, 7, 8 or later, you may need to follow &lt;a href=&#34;http://technet.microsoft.com/en-us/library/cc771275%28v=ws.10%29.aspx&#34;&gt;this guide&lt;/a&gt; to install it.&lt;/p&gt; &#xA;&lt;h2&gt;Running the Server&lt;/h2&gt; &#xA;&lt;h3&gt;On Windows&lt;/h3&gt; &#xA;&lt;p&gt;Double click on &lt;code&gt;simplemud.py&lt;/code&gt; - the file will be opened with the Python interpreter. To stop the server, simply close the terminal window.&lt;/p&gt; &#xA;&lt;h3&gt;On Mac OSX and Linux (including Raspberry Pi)&lt;/h3&gt; &#xA;&lt;p&gt;From the terminal, change to the directory containing the script and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python simplemud.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note, if you are connected to the machine via SSH, you will find that the script stops running when you quit the SSH session. A simple way to leave the script running is to use a tool called &lt;code&gt;screen&lt;/code&gt;. Connect via SSH as usual then run &lt;code&gt;screen&lt;/code&gt;. You will enter what looks like a normal shell prompt, but now you can start the python script running and hit &lt;code&gt;ctl+a&lt;/code&gt; followed by &lt;code&gt;d&lt;/code&gt; to leave &lt;em&gt;screen&lt;/em&gt; running in the background. The next time you connect, you can re-attach to your screen session using &lt;code&gt;screen -r&lt;/code&gt;. Alternatively you could &lt;a href=&#34;http://jimmyg.org/blog/2010/python-daemon-init-script.html&#34;&gt;create a daemon script&lt;/a&gt; to run the script in the background every time the server starts.&lt;/p&gt; &#xA;&lt;h2&gt;Connecting to the Server&lt;/h2&gt; &#xA;&lt;p&gt;If the server is running behind a NAT such as a home router, you will need to set up port &lt;strong&gt;1234&lt;/strong&gt; to be forwarded to the machine running the server. See your router&#39;s instructions for how to set this up. There are a large number of setup guides for different models of router here: &lt;a href=&#34;http://portforward.com/english/routers/port_forwarding/&#34;&gt;http://portforward.com/english/routers/port_forwarding/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You will need to know the &lt;em&gt;external&lt;/em&gt; IP address of the machine running the server. This can be discovered by visiting &lt;a href=&#34;http://www.whatsmyip.org&#34;&gt;http://www.whatsmyip.org&lt;/a&gt; from that machine.&lt;/p&gt; &#xA;&lt;p&gt;To connect to the server, open your operating system&#39;s terminal or command prompt and start the telnet client by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;telnet &amp;lt;ip address&amp;gt; 1234&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;&amp;lt;ip address&amp;gt;&lt;/code&gt; is the external IP address of the server, as described above. 1234 is the port number that the server listens on.&lt;/p&gt; &#xA;&lt;p&gt;If you are using Windows Vista, 7, 8 or later and get the message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;telnet&#39; is not recognized as an internal or external command, operable&#xA;program or batch file.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then follow &lt;a href=&#34;http://technet.microsoft.com/en-us/library/cc771275%28v=ws.10%29.aspx&#34;&gt;this guide&lt;/a&gt; to install the Windows telnet client.&lt;/p&gt; &#xA;&lt;p&gt;If all goes well, you should be presented with the message&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;What is your name?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To quit the telnet client, press &lt;code&gt;ctl + ]&lt;/code&gt; to go to the prompt, and then type &lt;code&gt;quit&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;What is Telnet?&lt;/h2&gt; &#xA;&lt;p&gt;Telnet is simple text-based network communication protocol that was invented in 1969 and has since been superseded by other, more secure protocols. It does remain popular for a few specialised uses however, MUD games being one of these uses. A long (and boring) history of the telnet protocol can be found here: &lt;a href=&#34;http://www.cs.utexas.edu/users/chris/think/ARPANET/Telnet/Telnet.shtml&#34;&gt;http://www.cs.utexas.edu/users/chris/think/ARPANET/Telnet/Telnet.shtml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is a MUD?&lt;/h2&gt; &#xA;&lt;p&gt;MUD is short for Multi-User Dungeon. A MUD is a text-based online role-playing game. MUDs were popular in the early 80s and were the precursor to the graphical Massively-Multiplayer Online Role-Playing Games we have today, like World of Warcraft. &lt;a href=&#34;http://www.mudconnect.com&#34;&gt;http://www.mudconnect.com&lt;/a&gt; is a great site for learning more about MUDs.&lt;/p&gt; &#xA;&lt;h2&gt;Extending the Game&lt;/h2&gt; &#xA;&lt;p&gt;MUD Pi is a free and open source project (that&#39;s &lt;em&gt;free&lt;/em&gt; as in &lt;em&gt;freedom&lt;/em&gt;). This means that the source code is included and you are free to read it, copy it, extend it and use it as a starting point for your own MUD game or any other project. See &lt;code&gt;licence.md&lt;/code&gt; for more info.&lt;/p&gt; &#xA;&lt;p&gt;MUD Pi was written in the Python programming language. If you have never used Python before, or are new to programming in general, why not try an online tutorial, such as &lt;a href=&#34;http://www.learnpython.org/&#34;&gt;http://www.learnpython.org/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There are 2 source files in the project. &lt;code&gt;mudserver.py&lt;/code&gt; is a module containing the &lt;code&gt;MudServer&lt;/code&gt; class - a basic server script which handles player connections and sending and receiving messages. &lt;code&gt;simplemud.py&lt;/code&gt; is an example game using &lt;code&gt;MudServer&lt;/code&gt;, with player chat and rooms to move between.&lt;/p&gt; &#xA;&lt;p&gt;The best place to start tweaking the game would be to have a look at &lt;code&gt;simplemud.py&lt;/code&gt;. Why not try adding more rooms to the game world? You&#39;ll find more ideas for things to try in the source code itself.&lt;/p&gt; &#xA;&lt;p&gt;Of course if you&#39;re feeling more adventurous you could take a look at the slightly more advanced networking code in &lt;code&gt;mudserver.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;MUD-Pi-Based Projects&lt;/h2&gt; &#xA;&lt;p&gt;Here are some of the cool projects people have made from MUD-Pi:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://git.savsoul.com/barry/esp8266-Mud&#34;&gt;ESP8266 MUD&lt;/a&gt; by Barry Ruffner&lt;/strong&gt; - a MUD that runs entirely within an ESP8266 microchip, using MicroPython&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/ufosc/MuddySwamp&#34;&gt;MuddySwamp&lt;/a&gt; by the University of&lt;/strong&gt; &lt;strong&gt;Florida Open Source Club&lt;/strong&gt; - a UF-themed MUD&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowpin/dumserver&#34;&gt;Dumserver&lt;/a&gt; by Bartek Radwanski&lt;/strong&gt; - a feature-rich MUD engine&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;MUD Pi was written by Mark Frimston&lt;/p&gt; &#xA;&lt;p&gt;For feedback, please email &lt;a href=&#34;mailto:mfrimston@gmail.com&#34;&gt;mfrimston@gmail.com&lt;/a&gt; or add a comment on the project&#39;s &lt;a href=&#34;http://github.com/frimkron/mud-pi&#34;&gt;Github page&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>czbag/scroll</title>
    <updated>2024-02-27T01:32:44Z</updated>
    <id>tag:github.com,2024-02-27:/czbag/scroll</id>
    <link href="https://github.com/czbag/scroll" rel="alternate"></link>
    <summary type="html">&lt;p&gt;–°–∫—Ä–∏–ø—Ç –ø–æ —Ä–∞–±–æ—Ç–µ —Å–æ Scroll&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://i.imgur.com/Vaah2gJ.png&#34;&gt; &#xA; &lt;h1&gt;Scroll Soft&lt;/h1&gt; &#xA; &lt;p&gt;This software simplifies wallet management on the Scroll network, providing access to a variety of features and a high level of randomization for enhanced security.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;üîî &lt;b&gt;Subscribe to me:&lt;/b&gt; &lt;a href=&#34;https://t.me/sybilwave&#34;&gt;https://t.me/sybilwave&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ü§ë &lt;b&gt;Donate me:&lt;/b&gt; 0x00000b0ddce0bfda4531542ad1f2f5fad7b9cde9&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üöÄ Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/czbag/scroll.git&#xA;&#xA;cd scroll&#xA;&#xA;pip install -r requirements.txt&#xA;&#xA;# Before you start, configure the required modules in modules_settings.py&#xA;&#xA;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üö® Modules&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Make deposit/withdraw with official bridge&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make deposit/withdraw with Orbiter bridge&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make deposit/withdraw with LayerSwap bridge&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make bridge with Nitro&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Wrap/unwrap ETH&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Swap on SkyDrome&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Swap on SyncSwap&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Swap on Zebra&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Swap on XySwap (my ref code is enabled, 1% of the transaction amount goes to me, come not from you, but from the Xy contract! can be turned off in config.py)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;LayerBank lending protocol (deposit/withdraw)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Aave lending protocol (deposit/withdraw)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mint Zerius NFT and bridge this NFT to any chain (layerzero protocol)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mint L2Pass NFT&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mint ZkStars NFT&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Crete Omnisea NFT&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mint NFT on NFTS2ME&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mint Scroll Origins NFT&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Dmail&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;RubyScore Vote&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create Gnosis Safe&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy any contract&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Custom routes - actions to be performed sequentially or randomly&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check gas before starting the module, if gas &amp;gt; specified, the software will wait for&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Logging via logger module&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Settings&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;All basic settings are made in settings.py and modules_settings.py, inside there is information about what and where to write&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the accounts.txt file, specify your private keys&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the rpc.json file at the path zksync/data/rpc.json we can change the rpc to ours&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Info on updates and just a life blog ‚Äì‚Äì &lt;a href=&#34;https://t.me/sybilwave&#34;&gt;https://t.me/sybilwave&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Eladlev/AutoPrompt</title>
    <updated>2024-02-27T01:32:44Z</updated>
    <id>tag:github.com,2024-02-27:/Eladlev/AutoPrompt</id>
    <link href="https://github.com/Eladlev/AutoPrompt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A framework for prompt tuning using Intent-based Prompt Calibration&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- community badges --&gt; &lt;a href=&#34;https://discord.gg/G2rSbAf8uP&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Join-Discord-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- license badge --&gt; &lt;a href=&#34;https://github.com/Eladlev/AutoPrompt/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;üìù AutoPrompt&lt;/h1&gt; &#xA;&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; &#xA;&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Auto Prompt is a prompt optimization framework designed to enhance and perfect your prompts for real-world use cases.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The framework automatically generates high-quality, detailed prompts tailored to user intentions. It employs a refinement (calibration) process, where it iteratively builds a dataset of challenging edge cases and optimizes the prompt accordingly. This approach not only reduces manual effort in prompt engineering but also effectively addresses common issues such as prompt &lt;a href=&#34;https://arxiv.org/abs/2307.09009&#34;&gt;sensitivity&lt;/a&gt; and inherent prompt &lt;a href=&#34;https://arxiv.org/abs/2311.04205&#34;&gt;ambiguity&lt;/a&gt; issues.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our mission:&lt;/strong&gt; Empower users to produce high-quality robust prompts using the power of large language models (LLMs).&lt;/p&gt; &#xA;&lt;h1&gt;Why Auto Prompt?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt Engineering Challenges.&lt;/strong&gt; The quality of LLMs greatly depends on the prompts used. Even &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/#prompt-sensitivity-example&#34;&gt;minor changes&lt;/a&gt; can significantly affect their performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Benchmarking Challenges.&lt;/strong&gt; Creating a benchmark for production-grade prompts is often labour-intensive and time-consuming.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reliable Prompts.&lt;/strong&gt; Auto Prompt generates robust high-quality prompts, offering measured accuracy and performance enhancement using minimal data and annotation steps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modularity and Adaptability.&lt;/strong&gt; With modularity at its core, Auto Prompt integrates seamlessly with popular open-source tools such as LangChain, Wandb, and Argilla, and can be adapted for a variety of tasks, including data synthesis and prompt migration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;System Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/AutoPrompt_Diagram.png&#34; alt=&#34;System Overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The system is designed for real-world scenarios, such as moderation tasks, which are often challenged by imbalanced data distributions. The system implements the &lt;a href=&#34;https://arxiv.org/abs/2402.03099&#34;&gt;Intent-based Prompt Calibration&lt;/a&gt; method. The process begins with a user-provided initial prompt and task description, optionally including user examples. The refinement process iteratively generates diverse samples, annotates them via user/LLM, and evaluates prompt performance, after which an LLM suggests an improved prompt.&lt;/p&gt; &#xA;&lt;p&gt;The optimization process can be extended to content generation tasks by first devising a ranker prompt and then performing the prompt optimization with this learned ranker. The optimization concludes upon reaching the budget or iteration limit.&lt;/p&gt; &#xA;&lt;p&gt;This joint synthetic data generation and prompt optimization approach outperform traditional methods while requiring minimal data and iterations. Learn more in our paper &lt;a href=&#34;https://arxiv.org/abs/2402.03099&#34;&gt;Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases&lt;/a&gt; by E. Levi et al. (2024).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using GPT-4 Turbo, this optimization typically completes in just a few minutes at a cost of under $1.&lt;/strong&gt; To manage costs associated with GPT-4 LLM&#39;s token usage, the framework enables users to set a budget limit for optimization, in USD or token count, configured as illustrated &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/examples.md#steps-to-run-example&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/autoprompt_recording.gif&#34; alt=&#34;pipeline_recording&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/installation.md&#34;&gt;How to install&lt;/a&gt; (Setup instructions)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/examples.md&#34;&gt;Prompt optimization examples&lt;/a&gt; (Use cases: movie review classification, generation, and chat moderation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/how-it-works.md&#34;&gt;How it works&lt;/a&gt; (Explanation of pipelines)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/architecture.md&#34;&gt;Architecture guide&lt;/a&gt; (Overview of main components)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìù Boosts prompt quality with a minimal amount of data and annotation steps.&lt;/li&gt; &#xA; &lt;li&gt;üõ¨ Designed for production use cases like moderation, multi-label classification, and content generation.&lt;/li&gt; &#xA; &lt;li&gt;‚öôÔ∏è Enables seamless migrating of prompts across model versions or LLM providers.&lt;/li&gt; &#xA; &lt;li&gt;üéì Supports prompt squeezing. Combine multiple rules into a single efficient prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;QuickStart&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - Download the project&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:Eladlev/AutoPrompt.git&#xA;cd AutoPrompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Install dependencies&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Use either Conda or pip, depending on your preference. Using Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment_dev.yml&#xA;conda activate AutoPrompt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Configure your LLM.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Set your OpenAI API key by updating the configuration file &lt;code&gt;config/llm_env.yml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you need help locating your API key, visit this &lt;a href=&#34;https://help.openai.com/en/articles/4936850-where-do-i-find-my-api-key&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We recommend using &lt;a href=&#34;https://platform.openai.com/docs/guides/gpt&#34;&gt;OpenAI&#39;s GPT-4&lt;/a&gt; for the LLM. Our framework also supports other providers and open-source models, as discussed &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/installation.md#configure-your-llm&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - Configure your Annotator&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Select an annotation approach for your project. We recommend beginning with a human-in-the-loop method, utilizing &lt;a href=&#34;https://docs.argilla.io/en/latest/index.html&#34;&gt;Argilla&lt;/a&gt;. Follow the &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/installation.md#configure-human-in-the-loop-annotator-&#34;&gt;Argilla setup instructions&lt;/a&gt; to configure your server. Alternatively, you can set up an LLM as your annotator by following these &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/installation.md#configure-llm-annotator-&#34;&gt;configuration steps&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The default predictor LLM, GPT-3.5, for estimating prompt performance, is configured in the &lt;code&gt;predictor&lt;/code&gt; section of &lt;code&gt;config/config_default.yml&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Define your budget in the input config yaml file using the &lt;code&gt;max_usage parameter&lt;/code&gt;. For OpenAI models, &lt;code&gt;max_usage&lt;/code&gt; sets the maximum spend in USD. For other LLMs, it limits the maximum token count.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt; - Run the pipeline&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;First, configure your labels by editing &lt;code&gt;config/config_default.yml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dataset:&#xA;    label_schema: [&#34;Yes&#34;, &#34;No&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a &lt;strong&gt;classification pipeline&lt;/strong&gt;, use the following command from your terminal within the appropriate working directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; python run_pipeline.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the initial prompt and task description are not provided directly as input, you will be guided to provide these details. Alternatively, specify them as command-line arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; python run_pipeline.py \&#xA;    --prompt &#34;Does this movie review contain a spoiler? answer Yes or No&#34; \&#xA;    --task_description &#34;Assistant is an expert classifier that will classify a movie review, and let the user know if it contains a spoiler for the reviewed movie or not.&#34; \&#xA;    --num_steps 30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can track the optimization progress using the &lt;a href=&#34;https://wandb.ai/site&#34;&gt;W&amp;amp;B&lt;/a&gt; dashboard, with setup instructions available &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/installation.md#monitoring-weights-and-biases-setup&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Generation pipeline&lt;/h4&gt; &#xA;&lt;p&gt;To run the generation pipeline, use the following example command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; python run_generation_pipeline.py \&#xA;    --prompt &#34;Write a good and comprehensive movie review about a specific movie.&#34; \&#xA;    --task_description &#34;Assistant is a large language model that is tasked with writing movie reviews.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information, refer to our &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/examples.md#generating-movie-reviews-generation-task&#34;&gt;generation task example&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Enjoy the results. Completion of these steps yields a &lt;strong&gt;refined (calibrated) prompt&lt;/strong&gt; tailored for your task, alongside a &lt;strong&gt;benchmark&lt;/strong&gt; featuring challenging samples, stored in the default &lt;code&gt;dump&lt;/code&gt; path.&lt;/p&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Prompt accuracy may fluctuate during the optimization. To identify the best prompts, we recommend continuous refinement following the initial generation of the benchmark. Set the number of optimization iterations with &lt;code&gt;--num_steps&lt;/code&gt; and control sample generation by specifying &lt;code&gt;max_samples&lt;/code&gt; in the &lt;code&gt;dataset&lt;/code&gt; section. For instance, setting &lt;code&gt;max_samples: 50&lt;/code&gt; and &lt;code&gt;--num_steps 30&lt;/code&gt; limits the benchmark to 50 samples, allowing for 25 additional refinement iterations, assuming 10 samples per iteration.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The framework supports checkpoints for easy resumption of optimization from the last saved state. It automatically saves the most recent optimization state in a &lt;code&gt;dump&lt;/code&gt; path. Use &lt;code&gt;--output_dump&lt;/code&gt; to set this path and &lt;code&gt;--load_path&lt;/code&gt; to resume from a checkpoint.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The iterations include multiple calls to the LLM service, with long prompts and requests for a relatively large amount of generated tokens by the LLM. This might take time ~1 minute (especially in the generative tasks), so please be patient.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If there are some issues with the Argilla server connection/error, try to restart the space.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- &#xA;Meanwhile, the num_initialize_samples and num_generated_samples fields within the meta_prompts section specify the counts for initial and per iteration sample generation, respectively. --&gt; &#xA;&lt;h2&gt;Prompt Sensitivity Example&lt;/h2&gt; &#xA;&lt;p&gt;You write a prompt for identifying movie spoilers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Review the content provided and indicate whether it includes any significant plot revelations or critical points that could reveal important elements of the story or its outcome. Respond with &#34;Yes&#34; if it contains such spoilers or critical insights, and &#34;No&#34; if it refrains from unveiling key story elements.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This prompt scores 81 on your &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/examples.md#filtering-movie-reviews-with-spoilers-classification-task&#34;&gt;benchmark&lt;/a&gt; using GPT-4 LLM. Then, you make a minor modification:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Review the text and determine if it provides essential revelations or critical details about the story that would constitute a spoiler. Respond with &#34;Yes&#34; for the presence of spoilers, and &#34;No&#34; for their absence.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Surprisingly, the second prompt scores 72, representing an 11% drop in accuracy. This illustrates the need for a careful prompt engineering process.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Your contributions are greatly appreciated! If you&#39;re eager to contribute, kindly refer to our &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/contributing.md&#34;&gt;Contributing Guidelines&lt;/a&gt;) for detailed information.&lt;/p&gt; &#xA;&lt;!-- For an insight into our future plans, visit our Project Roadmap. --&gt; &#xA;&lt;p&gt;If you wish to be a part of our journey, we invite you to connect with us through our &lt;a href=&#34;https://discord.gg/G2rSbAf8uP&#34;&gt;Discord Community&lt;/a&gt;. We&#39;re excited to have you onboard!&lt;/p&gt; &#xA;&lt;h2&gt;üõ° Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The AutoPrompt project is provided on an &#34;as-is&#34; basis without any guarantees or warranties, expressed or implied.&lt;/p&gt; &#xA;&lt;p&gt;Our perspective on the optimization and usage of prompts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;The core objective of AutoPrompt is to refine and perfect prompts to achieve high-quality results. This is achieved through an iterative calibration process, which helps in reducing errors and enhancing the performance of LLMs. However, the framework does not guarantee absolute correctness or unbiased results in every instance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;AutoPrompt aims to improve the reliability of prompts and mitigate sensitivity issues, but it does not claim to completely eliminate such issues.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;!-- Our community is committed to exploring the most effective ways to interact with LLMs, fostering a space for diverse views and approaches. --&gt; &#xA;&lt;p&gt;Please note that using LLMs like OpenAI&#39;s GPT-4, supported by AutoPrompt, may lead to significant costs due to token usage. By using AutoPrompt, you acknowledge your responsibility to monitor and manage your token use and expenses. We advise regularly reviewing your LLM provider&#39;s API usage and establishing limits or alerts to prevent unexpected charges. To manage costs associated with GPT-4 LLM&#39;s token usage, the framework enables users to set a budget limit for optimization, in USD or token count, configured as illustrated &lt;a href=&#34;https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/docs/examples.md#steps-to-run-example&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you have used our code in your research, please cite our &lt;a href=&#34;https://arxiv.org/abs/2402.03099&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{2402.03099,&#xA;Author = {Elad Levi and Eli Brosh and Matan Friedmann},&#xA;Title = {Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases},&#xA;Year = {2024},&#xA;Eprint = {arXiv:2402.03099},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This framework is licensed under the &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;‚úâÔ∏è Support / Contact us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/G2rSbAf8uP&#34;&gt;Community Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Our email: &lt;a href=&#34;mailto:autopromptai@gmail.com&#34;&gt;‚Ä´autopromptai@gmail.com‚Ä¨&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>