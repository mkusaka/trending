<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-27T01:36:04Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Intelligent-Internet/ii-agent</title>
    <updated>2025-07-27T01:36:04Z</updated>
    <id>tag:github.com,2025-07-27:/Intelligent-Internet/ii-agent</id>
    <link href="https://github.com/Intelligent-Internet/ii-agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;II-Agent: a new open-source framework to build and deploy intelligent agents&lt;/p&gt;&lt;hr&gt;&lt;img width=&#34;3600&#34; height=&#34;1890&#34; alt=&#34;II-Agent-updated&#34; src=&#34;https://github.com/user-attachments/assets/4e6211d1-c565-42a9-9d53-d809154a9493&#34;&gt; &#xA;&lt;h1&gt;II Agent&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Intelligent-Internet/ii-agent/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Intelligent-Internet/ii-agent?style=social&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/yDWPsshPHB&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/yDWPsshPHB?style=flat&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ii.inc/web/blog/post/ii-agent&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Blog-II--Agent-blue&#34; alt=&#34;Blog&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ii-agent-gaia.ii.inc/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GAIA-Benchmark-green&#34; alt=&#34;GAIA Benchmark&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://deepwiki.com/Intelligent-Internet/ii-agent&#34;&gt;&lt;img src=&#34;https://devin.ai/assets/deepwiki-badge.png&#34; alt=&#34;Ask DeepWiki.com&#34; height=&#34;20&#34;&gt;&lt;/a&gt;&lt;/p&gt;  &#xA;&lt;p&gt;II-Agent is an open-source intelligent assistant designed to streamline and enhance workflows across multiple domains. It represents a significant advancement in how we interact with technologyâ€”shifting from passive tools to intelligent systems capable of independently executing complex tasks.&lt;/p&gt; &#xA;&lt;h3&gt;Discord Join US&lt;/h3&gt; &#xA;&lt;p&gt;ðŸ“¢ Join Our &lt;a href=&#34;https://discord.gg/yDWPsshPHB&#34;&gt;Discord Channel&lt;/a&gt;! Looking forward to seeing you there! ðŸŽ‰&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/2707b106-f37d-41a8-beff-8802b1c9b186&#34;&gt;https://github.com/user-attachments/assets/2707b106-f37d-41a8-beff-8802b1c9b186&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;(New Features) Full-stack Web Agent Show cases!&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/8399f494-1e5a-43ba-9c7b-32861c51075e&#34;&gt;https://github.com/user-attachments/assets/8399f494-1e5a-43ba-9c7b-32861c51075e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/647e6bae-bc62-4c8b-9e6e-a7c8946caf56&#34;&gt;https://github.com/user-attachments/assets/647e6bae-bc62-4c8b-9e6e-a7c8946caf56&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;II Agent is built around providing an agentic interface to leading language models. It offers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A CLI interface for direct command-line interaction&lt;/li&gt; &#xA; &lt;li&gt;A WebSocket server that powers a modern React-based frontend&lt;/li&gt; &#xA; &lt;li&gt;Integration with multiple LLM providers: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Anthropic Claude models (direct API or via Google Cloud Vertex AI)&lt;/li&gt; &#xA;   &lt;li&gt;Google Gemini models (direct API or via Google Cloud Vertex AI)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;GAIA Benchmark Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;II-Agent has been evaluated on the GAIA benchmark, which assesses LLM-based agents operating within realistic scenarios across multiple dimensions including multimodal processing, tool utilization, and web searching.&lt;/p&gt; &#xA;&lt;p&gt;We identified several issues with the GAIA benchmark during our evaluation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Annotation Errors&lt;/strong&gt;: Several incorrect annotations in the dataset (e.g., misinterpreting date ranges, calculation errors)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Outdated Information&lt;/strong&gt;: Some questions reference websites or content no longer accessible&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Language Ambiguity&lt;/strong&gt;: Unclear phrasing leading to different interpretations of questions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Despite these challenges, II-Agent demonstrated strong performance on the benchmark, particularly in areas requiring complex reasoning, tool use, and multi-step planning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Intelligent-Internet/ii-agent/main/assets/gaia.jpg&#34; alt=&#34;GAIA Benchmark&#34;&gt; You can view the full traces of some samples here: &lt;a href=&#34;https://ii-agent-gaia.ii.inc/&#34;&gt;GAIA Benchmark Traces&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker Compose&lt;/li&gt; &#xA; &lt;li&gt;Python 3.10+&lt;/li&gt; &#xA; &lt;li&gt;Node.js 18+ (for frontend)&lt;/li&gt; &#xA; &lt;li&gt;At least one of the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Anthropic API key, or&lt;/li&gt; &#xA;   &lt;li&gt;Google Gemini API key, or&lt;/li&gt; &#xA;   &lt;li&gt;Google Cloud project with Vertex AI API enabled&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;For best performance, we recommend using Claude 4.0 Sonnet or Claude Opus 4.0 models.&lt;/li&gt; &#xA;  &lt;li&gt;For fast and cheap, we recommend using GPT4.1 from OpenAI.&lt;/li&gt; &#xA;  &lt;li&gt;Gemini 2.5 Pro is a good balance between performance and cost.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Docker Installation (Recommended)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA; &lt;li&gt;Run the following command&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;chmod +x start.sh&#xA;./start.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;821&#34; alt=&#34;Screenshot 2025-07-08 at 17 50 34&#34; src=&#34;https://github.com/user-attachments/assets/094f73aa-7384-4500-a670-528853f92ae7&#34;&gt; &#xA;&lt;p&gt;Our II-Agent supports popular models such as Claude, Gemini, and OpenAI. If youâ€™d like to use a model from OpenRouter, simply configure your OpenAI endpoint with your OpenRouter API key. If you are using Vertex, run with these variables&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;chmod +x start.sh&#xA;GOOGLE_APPLICATION_CREDENTIALS=absolute-path-to-credential ./start.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Optional) You can fill Google API credentials to connect to google drive. Press enter to skip these steps&lt;/p&gt; &#xA;&lt;h3&gt;Runtime Environment&lt;/h3&gt; &#xA;&lt;img width=&#34;821&#34; alt=&#34;Screenshot 2025-07-08 at 17 48 08&#34; src=&#34;https://github.com/user-attachments/assets/b1fb9f11-b1ef-4f62-bbea-9b67eba45322&#34;&gt; &#xA;&lt;p&gt;You can now select from a variety of models, set your API key, and configure environmentsâ€”all directly from the frontend settings pageâ€”to equip your agents with powerful tools and capabilities. You can also change the agents&#39; runtime environment. Currently, we support three runtime modes: Local, Docker, and E2B. For full-stack web application development, Docker and E2B are highly recommended, while Local Mode is best suited for lighter tasks such as basic webpage building and research.&lt;/p&gt; &#xA;&lt;p&gt;In addition, agents come equipped with built-in NeonDB and Vercel integration, enabling seamless cloud deployment of full-stack applications using a serverless database and serverless infrastructure.&lt;/p&gt; &#xA;&lt;h2&gt;Core Capabilities&lt;/h2&gt; &#xA;&lt;p&gt;II-Agent is a versatile open-source assistant built to elevate your productivity across domains:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Domain&lt;/th&gt; &#xA;   &lt;th&gt;What IIâ€‘Agent Can Do&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Research &amp;amp; Factâ€‘Checking&lt;/td&gt; &#xA;   &lt;td&gt;Multistep web search, source triangulation, structured noteâ€‘taking, rapid summarization&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Content Generation&lt;/td&gt; &#xA;   &lt;td&gt;Blog &amp;amp; article drafts, lesson plans, creative prose, technical manuals, Website creations&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Data Analysis &amp;amp; Visualization&lt;/td&gt; &#xA;   &lt;td&gt;Cleaning, statistics, trend detection, charting, and automated report generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Software Development&lt;/td&gt; &#xA;   &lt;td&gt;Code synthesis, refactoring, debugging, testâ€‘writing, and stepâ€‘byâ€‘step tutorials across multiple languages&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dynamic Website Development&lt;/td&gt; &#xA;   &lt;td&gt;Full-stack web application creation with live hosting, framework templates, and real-time deployment&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Workflow Automation&lt;/td&gt; &#xA;   &lt;td&gt;Script generation, browser automation, file management, process optimization&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Problem Solving&lt;/td&gt; &#xA;   &lt;td&gt;Decomposition, alternativeâ€‘path exploration, stepwise guidance, troubleshooting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Methods&lt;/h2&gt; &#xA;&lt;p&gt;The II-Agent system represents a sophisticated approach to building versatile AI agents. Our methodology centers on:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Core Agent Architecture and LLM Interaction&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;System prompting with dynamically tailored context&lt;/li&gt; &#xA;   &lt;li&gt;Comprehensive interaction history management&lt;/li&gt; &#xA;   &lt;li&gt;Intelligent context management to handle token limitations&lt;/li&gt; &#xA;   &lt;li&gt;Systematic LLM invocation and capability selection&lt;/li&gt; &#xA;   &lt;li&gt;Iterative refinement through execution cycles&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Planning and Reflection&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Structured reasoning for complex problem-solving&lt;/li&gt; &#xA;   &lt;li&gt;Problem decomposition and sequential thinking&lt;/li&gt; &#xA;   &lt;li&gt;Transparent decision-making process&lt;/li&gt; &#xA;   &lt;li&gt;Hypothesis formation and testing&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Execution Capabilities&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;File system operations with intelligent code editing&lt;/li&gt; &#xA;   &lt;li&gt;Command line execution in a secure environment&lt;/li&gt; &#xA;   &lt;li&gt;Advanced web interaction and browser automation&lt;/li&gt; &#xA;   &lt;li&gt;Task finalization and reporting&lt;/li&gt; &#xA;   &lt;li&gt;Specialized capabilities for various modalities (Experimental) (PDF, audio, image, video, slides)&lt;/li&gt; &#xA;   &lt;li&gt;Deep research integration&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Context Management&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Token usage estimation and optimization&lt;/li&gt; &#xA;   &lt;li&gt;Strategic truncation for lengthy interactions&lt;/li&gt; &#xA;   &lt;li&gt;File-based archival for large outputs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Real-time Communication&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WebSocket-based interface for interactive use&lt;/li&gt; &#xA;   &lt;li&gt;Isolated agent instances per client&lt;/li&gt; &#xA;   &lt;li&gt;Streaming operational events for responsive UX&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Conclusion&lt;/h2&gt; &#xA;&lt;p&gt;The II-Agent framework, architected around the reasoning capabilities of large language models like Claude 4.0 Sonnet or Gemini 2.5 Pro, presents a comprehensive and robust methodology for building versatile AI agents. Through its synergistic combination of a powerful LLM, a rich set of execution capabilities, an explicit mechanism for planning and reflection, and intelligent context management strategies, II-Agent is well-equipped to address a wide spectrum of complex, multi-step tasks. Its open-source nature and extensible design provide a strong foundation for continued research and development in the rapidly evolving field of agentic AI.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to express our sincere gratitude to the following projects and individuals for their invaluable contributions that have helped shape this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AugmentCode&lt;/strong&gt;: We have incorporated and adapted several key components from the &lt;a href=&#34;https://github.com/augmentcode/augment-swebench-agent&#34;&gt;AugmentCode project&lt;/a&gt;. AugmentCode focuses on SWE-bench, a benchmark that tests AI systems on real-world software engineering tasks from GitHub issues in popular open-source projects. Their system provides tools for bash command execution, file operations, and sequential problem-solving capabilities designed specifically for software engineering tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manus&lt;/strong&gt;: Our system prompt architecture draws inspiration from Manus&#39;s work, which has helped us create more effective and contextually aware AI interactions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Index Browser Use&lt;/strong&gt;: We have built upon and extended the functionality of the &lt;a href=&#34;https://github.com/lmnr-ai/index/tree/main&#34;&gt;Index Browser Use project&lt;/a&gt;, particularly in our web interaction and browsing capabilities. Their foundational work has enabled us to create more sophisticated web-based agent behaviors.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are committed to open source collaboration and believe in acknowledging the work that has helped us build this project. If you feel your work has been used in this project but hasn&#39;t been properly acknowledged, please reach out to us.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>googleapis/python-genai</title>
    <updated>2025-07-27T01:36:04Z</updated>
    <id>tag:github.com,2025-07-27:/googleapis/python-genai</id>
    <link href="https://github.com/googleapis/python-genai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Google Gen AI Python SDK provides an interface for developers to integrate Google&#39;s generative models into their Python applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Gen AI SDK&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/google-genai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/google-genai.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/google-genai&#34; alt=&#34;Python support&#34;&gt; &lt;a href=&#34;https://pypistats.org/packages/google-genai&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dw/google-genai&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href=&#34;https://googleapis.github.io/python-genai/&#34;&gt;https://googleapis.github.io/python-genai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Google Gen AI Python SDK provides an interface for developers to integrate Google&#39;s generative models into their Python applications. It supports the &lt;a href=&#34;https://ai.google.dev/gemini-api/docs&#34;&gt;Gemini Developer API&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview&#34;&gt;Vertex AI&lt;/a&gt; APIs.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install google-genai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Imports&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google import genai&#xA;from google.genai import types&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Create a client&lt;/h2&gt; &#xA;&lt;p&gt;Please run one of the following code blocks to create a client for different services (&lt;a href=&#34;https://ai.google.dev/gemini-api/docs&#34;&gt;Gemini Developer API&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview&#34;&gt;Vertex AI&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google import genai&#xA;&#xA;# Only run this block for Gemini Developer API&#xA;client = genai.Client(api_key=&#39;GEMINI_API_KEY&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google import genai&#xA;&#xA;# Only run this block for Vertex AI API&#xA;client = genai.Client(&#xA;    vertexai=True, project=&#39;your-project-id&#39;, location=&#39;us-central1&#39;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Optional) Using environment variables:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can create a client by configuring the necessary environment variables. Configuration setup instructions depends on whether you&#39;re using the Gemini Developer API or the Gemini API in Vertex AI.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gemini Developer API:&lt;/strong&gt; Set the &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; or &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;. It will automatically be picked up by the client. It&#39;s recommended that you set only one of those variables, but if both are set, &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; takes precedence.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GEMINI_API_KEY=&#39;your-api-key&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gemini API on Vertex AI:&lt;/strong&gt; Set &lt;code&gt;GOOGLE_GENAI_USE_VERTEXAI&lt;/code&gt;, &lt;code&gt;GOOGLE_CLOUD_PROJECT&lt;/code&gt; and &lt;code&gt;GOOGLE_CLOUD_LOCATION&lt;/code&gt;, as shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GOOGLE_GENAI_USE_VERTEXAI=true&#xA;export GOOGLE_CLOUD_PROJECT=&#39;your-project-id&#39;&#xA;export GOOGLE_CLOUD_LOCATION=&#39;us-central1&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google import genai&#xA;&#xA;client = genai.Client()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;API Selection&lt;/h3&gt; &#xA;&lt;p&gt;By default, the SDK uses the beta API endpoints provided by Google to support preview features in the APIs. The stable API endpoints can be selected by setting the API version to &lt;code&gt;v1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To set the API version use &lt;code&gt;http_options&lt;/code&gt;. For example, to set the API version to &lt;code&gt;v1&lt;/code&gt; for Vertex AI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google import genai&#xA;from google.genai import types&#xA;&#xA;client = genai.Client(&#xA;    vertexai=True,&#xA;    project=&#39;your-project-id&#39;,&#xA;    location=&#39;us-central1&#39;,&#xA;    http_options=types.HttpOptions(api_version=&#39;v1&#39;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To set the API version to &lt;code&gt;v1alpha&lt;/code&gt; for the Gemini Developer API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google import genai&#xA;from google.genai import types&#xA;&#xA;client = genai.Client(&#xA;    api_key=&#39;GEMINI_API_KEY&#39;,&#xA;    http_options=types.HttpOptions(api_version=&#39;v1alpha&#39;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Faster async client option: Aiohttp&lt;/h3&gt; &#xA;&lt;p&gt;By default we use httpx for both sync and async client implementations. In order to have faster performance, you may install &lt;code&gt;google-genai[aiohttp]&lt;/code&gt;. In Gen AI SDK we configure &lt;code&gt;trust_env=True&lt;/code&gt; to match with the default behavior of httpx. Additional args of &lt;code&gt;aiohttp.ClientSession.request()&lt;/code&gt; (&lt;a href=&#34;https://github.com/aio-libs/aiohttp/raw/v3.12.13/aiohttp/client.py#L170&#34;&gt;see _RequestOptions args&lt;/a&gt;) can be passed through the following way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;http_options = types.HttpOptions(&#xA;    async_client_args={&#39;cookies&#39;: ..., &#39;ssl&#39;: ...},&#xA;)&#xA;&#xA;client=Client(..., http_options=http_options)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Proxy&lt;/h3&gt; &#xA;&lt;p&gt;Both httpx and aiohttp libraries use &lt;code&gt;urllib.request.getproxies&lt;/code&gt; from environment variables. Before client initialization, you may set proxy (and optional SSL_CERT_FILE) by setting the environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export HTTPS_PROXY=&#39;http://username:password@proxy_uri:port&#39;&#xA;export SSL_CERT_FILE=&#39;client.pem&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need &lt;code&gt;socks5&lt;/code&gt; proxy, httpx &lt;a href=&#34;https://www.python-httpx.org/advanced/proxies/#socks&#34;&gt;supports&lt;/a&gt; &lt;code&gt;socks5&lt;/code&gt; proxy if you pass it via args to &lt;code&gt;httpx.Client()&lt;/code&gt;. You may install &lt;code&gt;httpx[socks]&lt;/code&gt; to use it. Then, you can pass it through the following way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;http_options = types.HttpOptions(&#xA;    client_args={&#39;proxy&#39;: &#39;socks5://user:pass@host:port&#39;},&#xA;    async_client_args={&#39;proxy&#39;: &#39;socks5://user:pass@host:port&#39;},,&#xA;)&#xA;&#xA;client=Client(..., http_options=http_options)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Types&lt;/h2&gt; &#xA;&lt;p&gt;Parameter types can be specified as either dictionaries(&lt;code&gt;TypedDict&lt;/code&gt;) or &lt;a href=&#34;https://pydantic.readthedocs.io/en/stable/model.html&#34;&gt;Pydantic Models&lt;/a&gt;. Pydantic model types are available in the &lt;code&gt;types&lt;/code&gt; module.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;client.models&lt;/code&gt; module exposes model inferencing and model getters. See the &#39;Create a client&#39; section above to initialize a client.&lt;/p&gt; &#xA;&lt;h3&gt;Generate Content&lt;/h3&gt; &#xA;&lt;h4&gt;with text content&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;, contents=&#39;Why is the sky blue?&#39;&#xA;)&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;with uploaded file (Gemini Developer API only)&lt;/h4&gt; &#xA;&lt;p&gt;download the file in console.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;python code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;file = client.files.upload(file=&#39;a11.txt&#39;)&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=[&#39;Could you summarize this file?&#39;, file]&#xA;)&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;How to structure &lt;code&gt;contents&lt;/code&gt; argument for &lt;code&gt;generate_content&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The SDK always converts the inputs to the &lt;code&gt;contents&lt;/code&gt; argument into &lt;code&gt;list[types.Content]&lt;/code&gt;. The following shows some common ways to provide your inputs.&lt;/p&gt; &#xA;&lt;h5&gt;Provide a &lt;code&gt;list[types.Content]&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;p&gt;This is the canonical way to provide contents, SDK will not do any conversion.&lt;/p&gt; &#xA;&lt;h5&gt;Provide a &lt;code&gt;types.Content&lt;/code&gt; instance&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;contents = types.Content(&#xA;  role=&#39;user&#39;,&#xA;  parts=[types.Part.from_text(text=&#39;Why is the sky blue?&#39;)]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;SDK converts this to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[&#xA;  types.Content(&#xA;    role=&#39;user&#39;,&#xA;    parts=[types.Part.from_text(text=&#39;Why is the sky blue?&#39;)]&#xA;  )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Provide a string&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;contents=&#39;Why is the sky blue?&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The SDK will assume this is a text part, and it converts this into the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[&#xA;  types.UserContent(&#xA;    parts=[&#xA;      types.Part.from_text(text=&#39;Why is the sky blue?&#39;)&#xA;    ]&#xA;  )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where a &lt;code&gt;types.UserContent&lt;/code&gt; is a subclass of &lt;code&gt;types.Content&lt;/code&gt;, it sets the &lt;code&gt;role&lt;/code&gt; field to be &lt;code&gt;user&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Provide a list of string&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;contents=[&#39;Why is the sky blue?&#39;, &#39;Why is the cloud white?&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The SDK assumes these are 2 text parts, it converts this into a single content, like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[&#xA;  types.UserContent(&#xA;    parts=[&#xA;      types.Part.from_text(text=&#39;Why is the sky blue?&#39;),&#xA;      types.Part.from_text(text=&#39;Why is the cloud white?&#39;),&#xA;    ]&#xA;  )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where a &lt;code&gt;types.UserContent&lt;/code&gt; is a subclass of &lt;code&gt;types.Content&lt;/code&gt;, the &lt;code&gt;role&lt;/code&gt; field in &lt;code&gt;types.UserContent&lt;/code&gt; is fixed to be &lt;code&gt;user&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Provide a function call part&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;contents = types.Part.from_function_call(&#xA;  name=&#39;get_weather_by_location&#39;,&#xA;  args={&#39;location&#39;: &#39;Boston&#39;}&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The SDK converts a function call part to a content with a &lt;code&gt;model&lt;/code&gt; role:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[&#xA;  types.ModelContent(&#xA;    parts=[&#xA;      types.Part.from_function_call(&#xA;        name=&#39;get_weather_by_location&#39;,&#xA;        args={&#39;location&#39;: &#39;Boston&#39;}&#xA;      )&#xA;    ]&#xA;  )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where a &lt;code&gt;types.ModelContent&lt;/code&gt; is a subclass of &lt;code&gt;types.Content&lt;/code&gt;, the &lt;code&gt;role&lt;/code&gt; field in &lt;code&gt;types.ModelContent&lt;/code&gt; is fixed to be &lt;code&gt;model&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Provide a list of function call parts&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;contents = [&#xA;  types.Part.from_function_call(&#xA;    name=&#39;get_weather_by_location&#39;,&#xA;    args={&#39;location&#39;: &#39;Boston&#39;}&#xA;  ),&#xA;  types.Part.from_function_call(&#xA;    name=&#39;get_weather_by_location&#39;,&#xA;    args={&#39;location&#39;: &#39;New York&#39;}&#xA;  ),&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The SDK converts a list of function call parts to the a content with a &lt;code&gt;model&lt;/code&gt; role:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[&#xA;  types.ModelContent(&#xA;    parts=[&#xA;      types.Part.from_function_call(&#xA;        name=&#39;get_weather_by_location&#39;,&#xA;        args={&#39;location&#39;: &#39;Boston&#39;}&#xA;      ),&#xA;      types.Part.from_function_call(&#xA;        name=&#39;get_weather_by_location&#39;,&#xA;        args={&#39;location&#39;: &#39;New York&#39;}&#xA;      )&#xA;    ]&#xA;  )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where a &lt;code&gt;types.ModelContent&lt;/code&gt; is a subclass of &lt;code&gt;types.Content&lt;/code&gt;, the &lt;code&gt;role&lt;/code&gt; field in &lt;code&gt;types.ModelContent&lt;/code&gt; is fixed to be &lt;code&gt;model&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Provide a non function call part&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;contents = types.Part.from_uri(&#xA;  file_uri: &#39;gs://generativeai-downloads/images/scones.jpg&#39;,&#xA;  mime_type: &#39;image/jpeg&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The SDK converts all non function call parts into a content with a &lt;code&gt;user&lt;/code&gt; role.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[&#xA;  types.UserContent(parts=[&#xA;    types.Part.from_uri(&#xA;     file_uri: &#39;gs://generativeai-downloads/images/scones.jpg&#39;,&#xA;      mime_type: &#39;image/jpeg&#39;,&#xA;    )&#xA;  ])&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Provide a list of non function call parts&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;contents = [&#xA;  types.Part.from_text(&#39;What is this image about?&#39;),&#xA;  types.Part.from_uri(&#xA;    file_uri: &#39;gs://generativeai-downloads/images/scones.jpg&#39;,&#xA;    mime_type: &#39;image/jpeg&#39;,&#xA;  )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The SDK will convert the list of parts into a content with a &lt;code&gt;user&lt;/code&gt; role&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[&#xA;  types.UserContent(&#xA;    parts=[&#xA;      types.Part.from_text(&#39;What is this image about?&#39;),&#xA;      types.Part.from_uri(&#xA;        file_uri: &#39;gs://generativeai-downloads/images/scones.jpg&#39;,&#xA;        mime_type: &#39;image/jpeg&#39;,&#xA;      )&#xA;    ]&#xA;  )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Mix types in contents&lt;/h5&gt; &#xA;&lt;p&gt;You can also provide a list of &lt;code&gt;types.ContentUnion&lt;/code&gt;. The SDK leaves items of &lt;code&gt;types.Content&lt;/code&gt; as is, it groups consecutive non function call parts into a single &lt;code&gt;types.UserContent&lt;/code&gt;, and it groups consecutive function call parts into a single &lt;code&gt;types.ModelContent&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you put a list within a list, the inner list can only contain &lt;code&gt;types.PartUnion&lt;/code&gt; items. The SDK will convert the inner list into a single &lt;code&gt;types.UserContent&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;System Instructions and Other Configs&lt;/h3&gt; &#xA;&lt;p&gt;The output of the model can be influenced by several optional settings available in generate_content&#39;s config parameter. For example, increasing &lt;code&gt;max_output_tokens&lt;/code&gt; is essential for longer model responses. To make a model more deterministic, lowering the &lt;code&gt;temperature&lt;/code&gt; parameter reduces randomness, with values near 0 minimizing variability. Capabilities and parameter defaults for each model is shown in the &lt;a href=&#34;https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash&#34;&gt;Vertex AI docs&lt;/a&gt; and &lt;a href=&#34;https://ai.google.dev/gemini-api/docs/models&#34;&gt;Gemini API docs&lt;/a&gt; respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;high&#39;,&#xA;    config=types.GenerateContentConfig(&#xA;        system_instruction=&#39;I say high, you say low&#39;,&#xA;        max_output_tokens=3,&#xA;        temperature=0.3,&#xA;    ),&#xA;)&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Typed Config&lt;/h3&gt; &#xA;&lt;p&gt;All API methods support Pydantic types for parameters as well as dictionaries. You can get the type from &lt;code&gt;google.genai.types&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=types.Part.from_text(text=&#39;Why is the sky blue?&#39;),&#xA;    config=types.GenerateContentConfig(&#xA;        temperature=0,&#xA;        top_p=0.95,&#xA;        top_k=20,&#xA;        candidate_count=1,&#xA;        seed=5,&#xA;        max_output_tokens=100,&#xA;        stop_sequences=[&#39;STOP!&#39;],&#xA;        presence_penalty=0.0,&#xA;        frequency_penalty=0.0,&#xA;    ),&#xA;)&#xA;&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List Base Models&lt;/h3&gt; &#xA;&lt;p&gt;To retrieve tuned models, see &lt;a href=&#34;https://raw.githubusercontent.com/googleapis/python-genai/main/#list-tuned-models&#34;&gt;list tuned models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for model in client.models.list():&#xA;    print(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pager = client.models.list(config={&#39;page_size&#39;: 10})&#xA;print(pager.page_size)&#xA;print(pager[0])&#xA;pager.next_page()&#xA;print(pager[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;List Base Models (Asynchronous)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async for job in await client.aio.models.list():&#xA;    print(job)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async_pager = await client.aio.models.list(config={&#39;page_size&#39;: 10})&#xA;print(async_pager.page_size)&#xA;print(async_pager[0])&#xA;await async_pager.next_page()&#xA;print(async_pager[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Safety Settings&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;Say something bad.&#39;,&#xA;    config=types.GenerateContentConfig(&#xA;        safety_settings=[&#xA;            types.SafetySetting(&#xA;                category=&#39;HARM_CATEGORY_HATE_SPEECH&#39;,&#xA;                threshold=&#39;BLOCK_ONLY_HIGH&#39;,&#xA;            )&#xA;        ]&#xA;    ),&#xA;)&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Function Calling&lt;/h3&gt; &#xA;&lt;h4&gt;Automatic Python function Support&lt;/h4&gt; &#xA;&lt;p&gt;You can pass a Python function directly and it will be automatically called and responded by default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;def get_current_weather(location: str) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Returns the current weather.&#xA;&#xA;    Args:&#xA;      location: The city and state, e.g. San Francisco, CA&#xA;    &#34;&#34;&#34;&#xA;    return &#39;sunny&#39;&#xA;&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;What is the weather like in Boston?&#39;,&#xA;    config=types.GenerateContentConfig(tools=[get_current_weather]),&#xA;)&#xA;&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Disabling automatic function calling&lt;/h4&gt; &#xA;&lt;p&gt;If you pass in a python function as a tool directly, and do not want automatic function calling, you can disable automatic function calling as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;response = client.models.generate_content(&#xA;  model=&#39;gemini-2.0-flash-001&#39;,&#xA;  contents=&#39;What is the weather like in Boston?&#39;,&#xA;  config=types.GenerateContentConfig(&#xA;    tools=[get_current_weather],&#xA;    automatic_function_calling=types.AutomaticFunctionCallingConfig(&#xA;      disable=True&#xA;    ),&#xA;  ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With automatic function calling disabled, you will get a list of function call parts in the response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;function_calls: Optional[List[types.FunctionCall]] = response.function_calls&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Manually declare and invoke a function for function calling&lt;/h4&gt; &#xA;&lt;p&gt;If you don&#39;t want to use the automatic function support, you can manually declare the function and invoke it.&lt;/p&gt; &#xA;&lt;p&gt;The following example shows how to declare a function and pass it as a tool. Then you will receive a function call part in the response.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;function = types.FunctionDeclaration(&#xA;    name=&#39;get_current_weather&#39;,&#xA;    description=&#39;Get the current weather in a given location&#39;,&#xA;    parameters_json_schema={&#xA;        &#39;type&#39;: &#39;object&#39;,&#xA;        &#39;properties&#39;: {&#xA;            &#39;location&#39;: {&#xA;                &#39;type&#39;: &#39;string&#39;,&#xA;                &#39;description&#39;: &#39;The city and state, e.g. San Francisco, CA&#39;,&#xA;            }&#xA;        },&#xA;        &#39;required&#39;: [&#39;location&#39;],&#xA;    },&#xA;)&#xA;&#xA;tool = types.Tool(function_declarations=[function])&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;What is the weather like in Boston?&#39;,&#xA;    config=types.GenerateContentConfig(tools=[tool]),&#xA;)&#xA;&#xA;print(response.function_calls[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After you receive the function call part from the model, you can invoke the function and get the function response. And then you can pass the function response to the model. The following example shows how to do it for a simple function invocation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;user_prompt_content = types.Content(&#xA;    role=&#39;user&#39;,&#xA;    parts=[types.Part.from_text(text=&#39;What is the weather like in Boston?&#39;)],&#xA;)&#xA;function_call_part = response.function_calls[0]&#xA;function_call_content = response.candidates[0].content&#xA;&#xA;&#xA;try:&#xA;    function_result = get_current_weather(&#xA;        **function_call_part.function_call.args&#xA;    )&#xA;    function_response = {&#39;result&#39;: function_result}&#xA;except (&#xA;    Exception&#xA;) as e:  # instead of raising the exception, you can let the model handle it&#xA;    function_response = {&#39;error&#39;: str(e)}&#xA;&#xA;&#xA;function_response_part = types.Part.from_function_response(&#xA;    name=function_call_part.name,&#xA;    response=function_response,&#xA;)&#xA;function_response_content = types.Content(&#xA;    role=&#39;tool&#39;, parts=[function_response_part]&#xA;)&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=[&#xA;        user_prompt_content,&#xA;        function_call_content,&#xA;        function_response_content,&#xA;    ],&#xA;    config=types.GenerateContentConfig(&#xA;        tools=[tool],&#xA;    ),&#xA;)&#xA;&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Function calling with &lt;code&gt;ANY&lt;/code&gt; tools config mode&lt;/h4&gt; &#xA;&lt;p&gt;If you configure function calling mode to be &lt;code&gt;ANY&lt;/code&gt;, then the model will always return function call parts. If you also pass a python function as a tool, by default the SDK will perform automatic function calling until the remote calls exceed the maximum remote call for automatic function calling (default to 10 times).&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to disable automatic function calling in &lt;code&gt;ANY&lt;/code&gt; mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;def get_current_weather(location: str) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Returns the current weather.&#xA;&#xA;    Args:&#xA;      location: The city and state, e.g. San Francisco, CA&#xA;    &#34;&#34;&#34;&#xA;    return &#34;sunny&#34;&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#34;gemini-2.0-flash-001&#34;,&#xA;    contents=&#34;What is the weather like in Boston?&#34;,&#xA;    config=types.GenerateContentConfig(&#xA;        tools=[get_current_weather],&#xA;        automatic_function_calling=types.AutomaticFunctionCallingConfig(&#xA;            disable=True&#xA;        ),&#xA;        tool_config=types.ToolConfig(&#xA;            function_calling_config=types.FunctionCallingConfig(mode=&#39;ANY&#39;)&#xA;        ),&#xA;    ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to set &lt;code&gt;x&lt;/code&gt; number of automatic function call turns, you can configure the maximum remote calls to be &lt;code&gt;x + 1&lt;/code&gt;. Assuming you prefer &lt;code&gt;1&lt;/code&gt; turn for automatic function calling.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;def get_current_weather(location: str) -&amp;gt; str:&#xA;    &#34;&#34;&#34;Returns the current weather.&#xA;&#xA;    Args:&#xA;      location: The city and state, e.g. San Francisco, CA&#xA;    &#34;&#34;&#34;&#xA;    return &#34;sunny&#34;&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#34;gemini-2.0-flash-001&#34;,&#xA;    contents=&#34;What is the weather like in Boston?&#34;,&#xA;    config=types.GenerateContentConfig(&#xA;        tools=[get_current_weather],&#xA;        automatic_function_calling=types.AutomaticFunctionCallingConfig(&#xA;            maximum_remote_calls=2&#xA;        ),&#xA;        tool_config=types.ToolConfig(&#xA;            function_calling_config=types.FunctionCallingConfig(mode=&#39;ANY&#39;)&#xA;        ),&#xA;    ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Model Context Protocol (MCP) support (experimental)&lt;/h4&gt; &#xA;&lt;p&gt;Built-in &lt;a href=&#34;https://modelcontextprotocol.io/introduction&#34;&gt;MCP&lt;/a&gt; support is an experimental feature. You can pass a local MCP server as a tool directly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import asyncio&#xA;from datetime import datetime&#xA;from mcp import ClientSession, StdioServerParameters&#xA;from mcp.client.stdio import stdio_client&#xA;from google import genai&#xA;&#xA;client = genai.Client()&#xA;&#xA;# Create server parameters for stdio connection&#xA;server_params = StdioServerParameters(&#xA;    command=&#34;npx&#34;,  # Executable&#xA;    args=[&#34;-y&#34;, &#34;@philschmid/weather-mcp&#34;],  # MCP Server&#xA;    env=None,  # Optional environment variables&#xA;)&#xA;&#xA;async def run():&#xA;    async with stdio_client(server_params) as (read, write):&#xA;        async with ClientSession(read, write) as session:&#xA;            # Prompt to get the weather for the current day in London.&#xA;            prompt = f&#34;What is the weather in London in {datetime.now().strftime(&#39;%Y-%m-%d&#39;)}?&#34;&#xA;&#xA;            # Initialize the connection between client and server&#xA;            await session.initialize()&#xA;&#xA;            # Send request to the model with MCP function declarations&#xA;            response = await client.aio.models.generate_content(&#xA;                model=&#34;gemini-2.5-flash&#34;,&#xA;                contents=prompt,&#xA;                config=genai.types.GenerateContentConfig(&#xA;                    temperature=0,&#xA;                    tools=[session],  # uses the session, will automatically call the tool using automatic function calling&#xA;                ),&#xA;            )&#xA;            print(response.text)&#xA;&#xA;# Start the asyncio event loop and run the main function&#xA;asyncio.run(run())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;JSON Response Schema&lt;/h3&gt; &#xA;&lt;p&gt;However you define your schema, don&#39;t duplicate it in your input prompt, including by giving examples of expected JSON output. If you do, the generated output might be lower in quality.&lt;/p&gt; &#xA;&lt;h4&gt;JSON Schema support&lt;/h4&gt; &#xA;&lt;p&gt;Schemas can be provided as standard JSON schema.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_profile = {&#xA;    &#39;properties&#39;: {&#xA;        &#39;age&#39;: {&#xA;            &#39;anyOf&#39;: [&#xA;                {&#39;maximum&#39;: 20, &#39;minimum&#39;: 0, &#39;type&#39;: &#39;integer&#39;},&#xA;                {&#39;type&#39;: &#39;null&#39;},&#xA;            ],&#xA;            &#39;title&#39;: &#39;Age&#39;,&#xA;        },&#xA;        &#39;username&#39;: {&#xA;            &#39;description&#39;: &#34;User&#39;s unique name&#34;,&#xA;            &#39;title&#39;: &#39;Username&#39;,&#xA;            &#39;type&#39;: &#39;string&#39;,&#xA;        },&#xA;    },&#xA;    &#39;required&#39;: [&#39;username&#39;, &#39;age&#39;],&#xA;    &#39;title&#39;: &#39;User Schema&#39;,&#xA;    &#39;type&#39;: &#39;object&#39;,&#xA;}&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash&#39;,&#xA;    contents=&#39;Give me information of the United States.&#39;,&#xA;    config={&#xA;        &#39;response_mime_type&#39;: &#39;application/json&#39;,&#xA;        &#39;response_json_schema&#39;: userProfile&#xA;    },&#xA;)&#xA;print(response.parsed)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Pydantic Model Schema support&lt;/h4&gt; &#xA;&lt;p&gt;Schemas can be provided as Pydantic Models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pydantic import BaseModel&#xA;from google.genai import types&#xA;&#xA;&#xA;class CountryInfo(BaseModel):&#xA;    name: str&#xA;    population: int&#xA;    capital: str&#xA;    continent: str&#xA;    gdp: int&#xA;    official_language: str&#xA;    total_area_sq_mi: int&#xA;&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;Give me information for the United States.&#39;,&#xA;    config=types.GenerateContentConfig(&#xA;        response_mime_type=&#39;application/json&#39;,&#xA;        response_schema=CountryInfo,&#xA;    ),&#xA;)&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;Give me information for the United States.&#39;,&#xA;    config=types.GenerateContentConfig(&#xA;        response_mime_type=&#39;application/json&#39;,&#xA;        response_schema={&#xA;            &#39;required&#39;: [&#xA;                &#39;name&#39;,&#xA;                &#39;population&#39;,&#xA;                &#39;capital&#39;,&#xA;                &#39;continent&#39;,&#xA;                &#39;gdp&#39;,&#xA;                &#39;official_language&#39;,&#xA;                &#39;total_area_sq_mi&#39;,&#xA;            ],&#xA;            &#39;properties&#39;: {&#xA;                &#39;name&#39;: {&#39;type&#39;: &#39;STRING&#39;},&#xA;                &#39;population&#39;: {&#39;type&#39;: &#39;INTEGER&#39;},&#xA;                &#39;capital&#39;: {&#39;type&#39;: &#39;STRING&#39;},&#xA;                &#39;continent&#39;: {&#39;type&#39;: &#39;STRING&#39;},&#xA;                &#39;gdp&#39;: {&#39;type&#39;: &#39;INTEGER&#39;},&#xA;                &#39;official_language&#39;: {&#39;type&#39;: &#39;STRING&#39;},&#xA;                &#39;total_area_sq_mi&#39;: {&#39;type&#39;: &#39;INTEGER&#39;},&#xA;            },&#xA;            &#39;type&#39;: &#39;OBJECT&#39;,&#xA;        },&#xA;    ),&#xA;)&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Enum Response Schema&lt;/h3&gt; &#xA;&lt;h4&gt;Text Response&lt;/h4&gt; &#xA;&lt;p&gt;You can set response_mime_type to &#39;text/x.enum&#39; to return one of those enum values as the response.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class InstrumentEnum(Enum):&#xA;  PERCUSSION = &#39;Percussion&#39;&#xA;  STRING = &#39;String&#39;&#xA;  WOODWIND = &#39;Woodwind&#39;&#xA;  BRASS = &#39;Brass&#39;&#xA;  KEYBOARD = &#39;Keyboard&#39;&#xA;&#xA;response = client.models.generate_content(&#xA;      model=&#39;gemini-2.0-flash-001&#39;,&#xA;      contents=&#39;What instrument plays multiple notes at once?&#39;,&#xA;      config={&#xA;          &#39;response_mime_type&#39;: &#39;text/x.enum&#39;,&#xA;          &#39;response_schema&#39;: InstrumentEnum,&#xA;      },&#xA;  )&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;JSON Response&lt;/h4&gt; &#xA;&lt;p&gt;You can also set response_mime_type to &#39;application/json&#39;, the response will be identical but in quotes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from enum import Enum&#xA;&#xA;class InstrumentEnum(Enum):&#xA;  PERCUSSION = &#39;Percussion&#39;&#xA;  STRING = &#39;String&#39;&#xA;  WOODWIND = &#39;Woodwind&#39;&#xA;  BRASS = &#39;Brass&#39;&#xA;  KEYBOARD = &#39;Keyboard&#39;&#xA;&#xA;response = client.models.generate_content(&#xA;      model=&#39;gemini-2.0-flash-001&#39;,&#xA;      contents=&#39;What instrument plays multiple notes at once?&#39;,&#xA;      config={&#xA;          &#39;response_mime_type&#39;: &#39;application/json&#39;,&#xA;          &#39;response_schema&#39;: InstrumentEnum,&#xA;      },&#xA;  )&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate Content (Synchronous Streaming)&lt;/h3&gt; &#xA;&lt;p&gt;Generate content in a streaming format so that the model outputs streams back to you, rather than being returned as one chunk.&lt;/p&gt; &#xA;&lt;h4&gt;Streaming for text content&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for chunk in client.models.generate_content_stream(&#xA;    model=&#39;gemini-2.0-flash-001&#39;, contents=&#39;Tell me a story in 300 words.&#39;&#xA;):&#xA;    print(chunk.text, end=&#39;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Streaming for image content&lt;/h4&gt; &#xA;&lt;p&gt;If your image is stored in &lt;a href=&#34;https://cloud.google.com/storage&#34;&gt;Google Cloud Storage&lt;/a&gt;, you can use the &lt;code&gt;from_uri&lt;/code&gt; class method to create a &lt;code&gt;Part&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;for chunk in client.models.generate_content_stream(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=[&#xA;        &#39;What is this image about?&#39;,&#xA;        types.Part.from_uri(&#xA;            file_uri=&#39;gs://generativeai-downloads/images/scones.jpg&#39;,&#xA;            mime_type=&#39;image/jpeg&#39;,&#xA;        ),&#xA;    ],&#xA;):&#xA;    print(chunk.text, end=&#39;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your image is stored in your local file system, you can read it in as bytes data and use the &lt;code&gt;from_bytes&lt;/code&gt; class method to create a &lt;code&gt;Part&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;YOUR_IMAGE_PATH = &#39;your_image_path&#39;&#xA;YOUR_IMAGE_MIME_TYPE = &#39;your_image_mime_type&#39;&#xA;with open(YOUR_IMAGE_PATH, &#39;rb&#39;) as f:&#xA;    image_bytes = f.read()&#xA;&#xA;for chunk in client.models.generate_content_stream(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=[&#xA;        &#39;What is this image about?&#39;,&#xA;        types.Part.from_bytes(data=image_bytes, mime_type=YOUR_IMAGE_MIME_TYPE),&#xA;    ],&#xA;):&#xA;    print(chunk.text, end=&#39;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate Content (Asynchronous Non Streaming)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;client.aio&lt;/code&gt; exposes all the analogous &lt;a href=&#34;https://docs.python.org/3/library/asyncio.html&#34;&gt;&lt;code&gt;async&lt;/code&gt; methods&lt;/a&gt; that are available on &lt;code&gt;client&lt;/code&gt;. Note that it applies to all the modules.&lt;/p&gt; &#xA;&lt;p&gt;For example, &lt;code&gt;client.aio.models.generate_content&lt;/code&gt; is the &lt;code&gt;async&lt;/code&gt; version of &lt;code&gt;client.models.generate_content&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = await client.aio.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;, contents=&#39;Tell me a story in 300 words.&#39;&#xA;)&#xA;&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate Content (Asynchronous Streaming)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async for chunk in await client.aio.models.generate_content_stream(&#xA;    model=&#39;gemini-2.0-flash-001&#39;, contents=&#39;Tell me a story in 300 words.&#39;&#xA;):&#xA;    print(chunk.text, end=&#39;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Count Tokens and Compute Tokens&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = client.models.count_tokens(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;why is the sky blue?&#39;,&#xA;)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Compute Tokens&lt;/h4&gt; &#xA;&lt;p&gt;Compute tokens is only supported in Vertex AI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = client.models.compute_tokens(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;why is the sky blue?&#39;,&#xA;)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Async&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = await client.aio.models.count_tokens(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;why is the sky blue?&#39;,&#xA;)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Embed Content&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = client.models.embed_content(&#xA;    model=&#39;text-embedding-004&#39;,&#xA;    contents=&#39;why is the sky blue?&#39;,&#xA;)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;# multiple contents with config&#xA;response = client.models.embed_content(&#xA;    model=&#39;text-embedding-004&#39;,&#xA;    contents=[&#39;why is the sky blue?&#39;, &#39;What is your age?&#39;],&#xA;    config=types.EmbedContentConfig(output_dimensionality=10),&#xA;)&#xA;&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Imagen&lt;/h3&gt; &#xA;&lt;h4&gt;Generate Images&lt;/h4&gt; &#xA;&lt;p&gt;Support for generate images in Gemini Developer API is behind an allowlist&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;# Generate Image&#xA;response1 = client.models.generate_images(&#xA;    model=&#39;imagen-3.0-generate-002&#39;,&#xA;    prompt=&#39;An umbrella in the foreground, and a rainy night sky in the background&#39;,&#xA;    config=types.GenerateImagesConfig(&#xA;        number_of_images=1,&#xA;        include_rai_reason=True,&#xA;        output_mime_type=&#39;image/jpeg&#39;,&#xA;    ),&#xA;)&#xA;response1.generated_images[0].image.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Upscale Image&lt;/h4&gt; &#xA;&lt;p&gt;Upscale image is only supported in Vertex AI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;# Upscale the generated image from above&#xA;response2 = client.models.upscale_image(&#xA;    model=&#39;imagen-3.0-generate-001&#39;,&#xA;    image=response1.generated_images[0].image,&#xA;    upscale_factor=&#39;x2&#39;,&#xA;    config=types.UpscaleImageConfig(&#xA;        include_rai_reason=True,&#xA;        output_mime_type=&#39;image/jpeg&#39;,&#xA;    ),&#xA;)&#xA;response2.generated_images[0].image.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Edit Image&lt;/h4&gt; &#xA;&lt;p&gt;Edit image uses a separate model from generate and upscale.&lt;/p&gt; &#xA;&lt;p&gt;Edit image is only supported in Vertex AI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Edit the generated image from above&#xA;from google.genai import types&#xA;from google.genai.types import RawReferenceImage, MaskReferenceImage&#xA;&#xA;raw_ref_image = RawReferenceImage(&#xA;    reference_id=1,&#xA;    reference_image=response1.generated_images[0].image,&#xA;)&#xA;&#xA;# Model computes a mask of the background&#xA;mask_ref_image = MaskReferenceImage(&#xA;    reference_id=2,&#xA;    config=types.MaskReferenceConfig(&#xA;        mask_mode=&#39;MASK_MODE_BACKGROUND&#39;,&#xA;        mask_dilation=0,&#xA;    ),&#xA;)&#xA;&#xA;response3 = client.models.edit_image(&#xA;    model=&#39;imagen-3.0-capability-001&#39;,&#xA;    prompt=&#39;Sunlight and clear sky&#39;,&#xA;    reference_images=[raw_ref_image, mask_ref_image],&#xA;    config=types.EditImageConfig(&#xA;        edit_mode=&#39;EDIT_MODE_INPAINT_INSERTION&#39;,&#xA;        number_of_images=1,&#xA;        include_rai_reason=True,&#xA;        output_mime_type=&#39;image/jpeg&#39;,&#xA;    ),&#xA;)&#xA;response3.generated_images[0].image.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Veo&lt;/h3&gt; &#xA;&lt;p&gt;Support for generating videos is considered public preview&lt;/p&gt; &#xA;&lt;h4&gt;Generate Videos (Text to Video)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;# Create operation&#xA;operation = client.models.generate_videos(&#xA;    model=&#39;veo-2.0-generate-001&#39;,&#xA;    prompt=&#39;A neon hologram of a cat driving at top speed&#39;,&#xA;    config=types.GenerateVideosConfig(&#xA;        number_of_videos=1,&#xA;        duration_seconds=5,&#xA;        enhance_prompt=True,&#xA;    ),&#xA;)&#xA;&#xA;# Poll operation&#xA;while not operation.done:&#xA;    time.sleep(20)&#xA;    operation = client.operations.get(operation)&#xA;&#xA;video = operation.response.generated_videos[0].video&#xA;video.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Generate Videos (Image to Video)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;# Read local image (uses mimetypes.guess_type to infer mime type)&#xA;image = types.Image.from_file(&#34;local/path/file.png&#34;)&#xA;&#xA;# Create operation&#xA;operation = client.models.generate_videos(&#xA;    model=&#39;veo-2.0-generate-001&#39;,&#xA;    # Prompt is optional if image is provided&#xA;    prompt=&#39;Night sky&#39;,&#xA;    image=image,&#xA;    config=types.GenerateVideosConfig(&#xA;        number_of_videos=1,&#xA;        duration_seconds=5,&#xA;        enhance_prompt=True,&#xA;        # Can also pass an Image into last_frame for frame interpolation&#xA;    ),&#xA;)&#xA;&#xA;# Poll operation&#xA;while not operation.done:&#xA;    time.sleep(20)&#xA;    operation = client.operations.get(operation)&#xA;&#xA;video = operation.response.generated_videos[0].video&#xA;video.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Generate Videos (Video to Video)&lt;/h4&gt; &#xA;&lt;p&gt;Currently, only Vertex supports Video to Video generation (Video extension).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;# Read local video (uses mimetypes.guess_type to infer mime type)&#xA;video = types.Video.from_file(&#34;local/path/video.mp4&#34;)&#xA;&#xA;# Create operation&#xA;operation = client.models.generate_videos(&#xA;    model=&#39;veo-2.0-generate-001&#39;,&#xA;    # Prompt is optional if Video is provided&#xA;    prompt=&#39;Night sky&#39;,&#xA;    # Input video must be in GCS&#xA;    video=types.Video(&#xA;        uri=&#34;gs://bucket-name/inputs/videos/cat_driving.mp4&#34;,&#xA;    ),&#xA;    config=types.GenerateVideosConfig(&#xA;        number_of_videos=1,&#xA;        duration_seconds=5,&#xA;        enhance_prompt=True,&#xA;    ),&#xA;)&#xA;&#xA;# Poll operation&#xA;while not operation.done:&#xA;    time.sleep(20)&#xA;    operation = client.operations.get(operation)&#xA;&#xA;video = operation.response.generated_videos[0].video&#xA;video.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Chats&lt;/h2&gt; &#xA;&lt;p&gt;Create a chat session to start a multi-turn conversations with the model. Then, use &lt;code&gt;chat.send_message&lt;/code&gt; function multiple times within the same chat session so that it can reflect on its previous responses (i.e., engage in an ongoing conversation). See the &#39;Create a client&#39; section above to initialize a client.&lt;/p&gt; &#xA;&lt;h3&gt;Send Message (Synchronous Non-Streaming)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chat = client.chats.create(model=&#39;gemini-2.0-flash-001&#39;)&#xA;response = chat.send_message(&#39;tell me a story&#39;)&#xA;print(response.text)&#xA;response = chat.send_message(&#39;summarize the story you told me in 1 sentence&#39;)&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Send Message (Synchronous Streaming)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chat = client.chats.create(model=&#39;gemini-2.0-flash-001&#39;)&#xA;for chunk in chat.send_message_stream(&#39;tell me a story&#39;):&#xA;    print(chunk.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Send Message (Asynchronous Non-Streaming)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chat = client.aio.chats.create(model=&#39;gemini-2.0-flash-001&#39;)&#xA;response = await chat.send_message(&#39;tell me a story&#39;)&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Send Message (Asynchronous Streaming)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chat = client.aio.chats.create(model=&#39;gemini-2.0-flash-001&#39;)&#xA;async for chunk in await chat.send_message_stream(&#39;tell me a story&#39;):&#xA;    print(chunk.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Files&lt;/h2&gt; &#xA;&lt;p&gt;Files are only supported in Gemini Developer API. See the &#39;Create a client&#39; section above to initialize a client.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;!gsutil cp gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf .&#xA;!gsutil cp gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upload&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;file1 = client.files.upload(file=&#39;2312.11805v3.pdf&#39;)&#xA;file2 = client.files.upload(file=&#39;2403.05530.pdf&#39;)&#xA;&#xA;print(file1)&#xA;print(file2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;file1 = client.files.upload(file=&#39;2312.11805v3.pdf&#39;)&#xA;file_info = client.files.get(name=file1.name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Delete&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;file3 = client.files.upload(file=&#39;2312.11805v3.pdf&#39;)&#xA;&#xA;client.files.delete(name=file3.name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Caches&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;client.caches&lt;/code&gt; contains the control plane APIs for cached content. See the &#39;Create a client&#39; section above to initialize a client.&lt;/p&gt; &#xA;&lt;h3&gt;Create&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;if client.vertexai:&#xA;    file_uris = [&#xA;        &#39;gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf&#39;,&#xA;        &#39;gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf&#39;,&#xA;    ]&#xA;else:&#xA;    file_uris = [file1.uri, file2.uri]&#xA;&#xA;cached_content = client.caches.create(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    config=types.CreateCachedContentConfig(&#xA;        contents=[&#xA;            types.Content(&#xA;                role=&#39;user&#39;,&#xA;                parts=[&#xA;                    types.Part.from_uri(&#xA;                        file_uri=file_uris[0], mime_type=&#39;application/pdf&#39;&#xA;                    ),&#xA;                    types.Part.from_uri(&#xA;                        file_uri=file_uris[1],&#xA;                        mime_type=&#39;application/pdf&#39;,&#xA;                    ),&#xA;                ],&#xA;            )&#xA;        ],&#xA;        system_instruction=&#39;What is the sum of the two pdfs?&#39;,&#xA;        display_name=&#39;test cache&#39;,&#xA;        ttl=&#39;3600s&#39;,&#xA;    ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cached_content = client.caches.get(name=cached_content.name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate Content with Caches&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;response = client.models.generate_content(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    contents=&#39;Summarize the pdfs&#39;,&#xA;    config=types.GenerateContentConfig(&#xA;        cached_content=cached_content.name,&#xA;    ),&#xA;)&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tunings&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;client.tunings&lt;/code&gt; contains tuning job APIs and supports supervised fine tuning through &lt;code&gt;tune&lt;/code&gt;. Only supported in Vertex AI. See the &#39;Create a client&#39; section above to initialize a client.&lt;/p&gt; &#xA;&lt;h3&gt;Tune&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vertex AI supports tuning from GCS source or from a Vertex Multimodal Dataset&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;model = &#39;gemini-2.0-flash-001&#39;&#xA;training_dataset = types.TuningDataset(&#xA;  # or gcs_uri=my_vertex_multimodal_dataset&#xA;    gcs_uri=&#39;gs://cloud-samples-data/ai-platform/generative_ai/gemini-1_5/text/sft_train_data.jsonl&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;tuning_job = client.tunings.tune(&#xA;    base_model=model,&#xA;    training_dataset=training_dataset,&#xA;    config=types.CreateTuningJobConfig(&#xA;        epoch_count=1, tuned_model_display_name=&#39;test_dataset_examples model&#39;&#xA;    ),&#xA;)&#xA;print(tuning_job)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get Tuning Job&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tuning_job = client.tunings.get(name=tuning_job.name)&#xA;print(tuning_job)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time&#xA;&#xA;completed_states = set(&#xA;    [&#xA;        &#39;JOB_STATE_SUCCEEDED&#39;,&#xA;        &#39;JOB_STATE_FAILED&#39;,&#xA;        &#39;JOB_STATE_CANCELLED&#39;,&#xA;    ]&#xA;)&#xA;&#xA;while tuning_job.state not in completed_states:&#xA;    print(tuning_job.state)&#xA;    tuning_job = client.tunings.get(name=tuning_job.name)&#xA;    time.sleep(10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Use Tuned Model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = client.models.generate_content(&#xA;    model=tuning_job.tuned_model.endpoint,&#xA;    contents=&#39;why is the sky blue?&#39;,&#xA;)&#xA;&#xA;print(response.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get Tuned Model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tuned_model = client.models.get(model=tuning_job.tuned_model.model)&#xA;print(tuned_model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List Tuned Models&lt;/h3&gt; &#xA;&lt;p&gt;To retrieve base models, see &lt;a href=&#34;https://raw.githubusercontent.com/googleapis/python-genai/main/#list-base-models&#34;&gt;list base models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for model in client.models.list(config={&#39;page_size&#39;: 10, &#39;query_base&#39;: False}):&#xA;    print(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pager = client.models.list(config={&#39;page_size&#39;: 10, &#39;query_base&#39;: False})&#xA;print(pager.page_size)&#xA;print(pager[0])&#xA;pager.next_page()&#xA;print(pager[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Async&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async for job in await client.aio.models.list(config={&#39;page_size&#39;: 10, &#39;query_base&#39;: False}):&#xA;    print(job)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async_pager = await client.aio.models.list(config={&#39;page_size&#39;: 10, &#39;query_base&#39;: False})&#xA;print(async_pager.page_size)&#xA;print(async_pager[0])&#xA;await async_pager.next_page()&#xA;print(async_pager[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Update Tuned Model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import types&#xA;&#xA;model = pager[0]&#xA;&#xA;model = client.models.update(&#xA;    model=model.name,&#xA;    config=types.UpdateModelConfig(&#xA;        display_name=&#39;my tuned model&#39;, description=&#39;my tuned model description&#39;&#xA;    ),&#xA;)&#xA;&#xA;print(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List Tuning Jobs&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for job in client.tunings.list(config={&#39;page_size&#39;: 10}):&#xA;    print(job)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pager = client.tunings.list(config={&#39;page_size&#39;: 10})&#xA;print(pager.page_size)&#xA;print(pager[0])&#xA;pager.next_page()&#xA;print(pager[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Async&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async for job in await client.aio.tunings.list(config={&#39;page_size&#39;: 10}):&#xA;    print(job)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async_pager = await client.aio.tunings.list(config={&#39;page_size&#39;: 10})&#xA;print(async_pager.page_size)&#xA;print(async_pager[0])&#xA;await async_pager.next_page()&#xA;print(async_pager[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Batch Prediction&lt;/h2&gt; &#xA;&lt;p&gt;Only supported in Vertex AI. See the &#39;Create a client&#39; section above to initialize a client.&lt;/p&gt; &#xA;&lt;h3&gt;Create&lt;/h3&gt; &#xA;&lt;p&gt;Vertex AI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specify model and source file only, destination and job display name will be auto-populated&#xA;job = client.batches.create(&#xA;    model=&#39;gemini-2.0-flash-001&#39;,&#xA;    src=&#39;bq://my-project.my-dataset.my-table&#39;,  # or &#34;gs://path/to/input/data&#34;&#xA;)&#xA;&#xA;job&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Gemini Developer API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a batch job with inlined requests&#xA;batch_job = client.batches.create(&#xA;    model=&#34;gemini-2.0-flash&#34;,&#xA;    src=[{&#xA;      &#34;contents&#34;: [{&#xA;        &#34;parts&#34;: [{&#xA;          &#34;text&#34;: &#34;Hello!&#34;,&#xA;        }],&#xA;       &#34;role&#34;: &#34;user&#34;,&#xA;     }],&#xA;     &#34;config:&#34;: {&#34;response_modalities&#34;: [&#34;text&#34;]},&#xA;    }],&#xA;)&#xA;&#xA;job&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to create a batch job with file name. Need to upload a jsonl file. For example myrequests.json:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#34;key&#34;:&#34;request_1&#34;, &#34;request&#34;: {&#34;contents&#34;: [{&#34;parts&#34;: [{&#34;text&#34;:&#xA; &#34;Explain how AI works in a few words&#34;}]}], &#34;generation_config&#34;: {&#34;response_modalities&#34;: [&#34;TEXT&#34;]}}}&#xA;{&#34;key&#34;:&#34;request_2&#34;, &#34;request&#34;: {&#34;contents&#34;: [{&#34;parts&#34;: [{&#34;text&#34;: &#34;Explain how Crypto works in a few words&#34;}]}]}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then upload the file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Upload the file&#xA;file = client.files.upload(&#xA;    file=&#39;myrequest.json&#39;,&#xA;    config=types.UploadFileConfig(display_name=&#39;test_json&#39;)&#xA;)&#xA;&#xA;# Create a batch job with file name&#xA;batch_job = client.batches.create(&#xA;    model=&#34;gemini-2.0-flash&#34;,&#xA;    src=&#34;files/file_name&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get a job by name&#xA;job = client.batches.get(name=job.name)&#xA;&#xA;job.state&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;completed_states = set(&#xA;    [&#xA;        &#39;JOB_STATE_SUCCEEDED&#39;,&#xA;        &#39;JOB_STATE_FAILED&#39;,&#xA;        &#39;JOB_STATE_CANCELLED&#39;,&#xA;        &#39;JOB_STATE_PAUSED&#39;,&#xA;    ]&#xA;)&#xA;&#xA;while job.state not in completed_states:&#xA;    print(job.state)&#xA;    job = client.batches.get(name=job.name)&#xA;    time.sleep(30)&#xA;&#xA;job&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for job in client.batches.list(config=types.ListBatchJobsConfig(page_size=10)):&#xA;    print(job)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pager = client.batches.list(config=types.ListBatchJobsConfig(page_size=10))&#xA;print(pager.page_size)&#xA;print(pager[0])&#xA;pager.next_page()&#xA;print(pager[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Async&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async for job in await client.aio.batches.list(&#xA;    config=types.ListBatchJobsConfig(page_size=10)&#xA;):&#xA;    print(job)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async_pager = await client.aio.batches.list(&#xA;    config=types.ListBatchJobsConfig(page_size=10)&#xA;)&#xA;print(async_pager.page_size)&#xA;print(async_pager[0])&#xA;await async_pager.next_page()&#xA;print(async_pager[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Delete&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Delete the job resource&#xA;delete_job = client.batches.delete(name=job.name)&#xA;&#xA;delete_job&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Error Handling&lt;/h2&gt; &#xA;&lt;p&gt;To handle errors raised by the model service, the SDK provides this &lt;a href=&#34;https://github.com/googleapis/python-genai/raw/main/google/genai/errors.py&#34;&gt;APIError&lt;/a&gt; class.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.genai import errors&#xA;&#xA;try:&#xA;  client.models.generate_content(&#xA;      model=&#34;invalid-model-name&#34;,&#xA;      contents=&#34;What is your name?&#34;,&#xA;  )&#xA;except errors.APIError as e:&#xA;  print(e.code) # 404&#xA;  print(e.message)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Extra Request Body&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;extra_body&lt;/code&gt; field in &lt;code&gt;HttpOptions&lt;/code&gt; accepts a dictionary of additional JSON properties to include in the request body. This can be used to access new or experimental backend features that are not yet formally supported in the SDK. The structure of the dictionary must match the backend API&#39;s request structure.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VertexAI backend API docs: &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/reference/rest&#34;&gt;https://cloud.google.com/vertex-ai/docs/reference/rest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GeminiAPI backend API docs: &lt;a href=&#34;https://ai.google.dev/api/rest&#34;&gt;https://ai.google.dev/api/rest&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = client.models.generate_content(&#xA;    model=&#34;gemini-2.5-pro&#34;,&#xA;    contents=&#34;What is the weather in Boston? and how about Sunnyvale?&#34;,&#xA;    config=types.GenerateContentConfig(&#xA;        tools=[get_current_weather],&#xA;        http_options=types.HttpOptions(extra_body={&#39;tool_config&#39;: {&#39;function_calling_config&#39;: {&#39;mode&#39;: &#39;COMPOSITIONAL&#39;}}}),&#xA;    ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>