<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-25T01:36:11Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>IDEA-Research/detrex</title>
    <updated>2022-09-25T01:36:11Z</updated>
    <id>tag:github.com,2022-09-25:/IDEA-Research/detrex</id>
    <link href="https://github.com/IDEA-Research/detrex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;IDEA Open Source Toolbox for Transformer Based Object Detection Algorithms&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;left&#34;&gt;detrex&lt;/h2&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://detrex.readthedocs.io/en/latest/index.html&#34;&gt; &lt;img alt=&#34;docs&#34; src=&#34;https://img.shields.io/badge/docs-latest-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/IDEA-Research/detrex/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/IDEA-Research/detrex.svg?color=blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/IDEA-Research/detrex/pulls&#34;&gt; &lt;img alt=&#34;PRs Welcome&#34; src=&#34;https://img.shields.io/badge/PRs-welcome-pink.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/IDEA-Research/detrex/issues&#34;&gt; &lt;img alt=&#34;open issues&#34; src=&#34;https://img.shields.io/github/issues/IDEA-Research/detrex&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://detrex.readthedocs.io/en/latest/index.html&#34;&gt;üìòDocumentation&lt;/a&gt; | &lt;a href=&#34;https://detrex.readthedocs.io/en/latest/tutorials/Installation.html&#34;&gt;üõ†Ô∏èInstallation&lt;/a&gt; | &lt;a href=&#34;https://detrex.readthedocs.io/en/latest/tutorials/Model_Zoo.html&#34;&gt;üëÄModel Zoo&lt;/a&gt; | &lt;a href=&#34;https://github.com/IDEA-Research/awesome-detection-transformer&#34;&gt;üöÄAwesome DETR&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/#change-log&#34;&gt;üÜïNews&lt;/a&gt; | &lt;a href=&#34;https://github.com/IDEA-Research/detrex/issues/new/choose&#34;&gt;ü§îReporting Issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;detrex is an open-source toolbox that provides state-of-the-art Transformer-based detection algorithms. It is built on top of &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;Detectron2&lt;/a&gt; and its module design is partially borrowed from &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/detr&#34;&gt;DETR&lt;/a&gt;. Many thanks for their nicely organized code. The main branch works with &lt;strong&gt;Pytorch 1.10+&lt;/strong&gt; or higher (we recommend &lt;strong&gt;Pytorch 1.12&lt;/strong&gt;).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/assets/detr_arch.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; Major Features &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Design.&lt;/strong&gt; detrex decomposes the Transformer-based detection framework into various components which help users easily build their own customized models.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;State-of-the-art Methods.&lt;/strong&gt; detrex provides a series of Transformer-based detection algorithms, including &lt;a href=&#34;https://arxiv.org/abs/2203.03605&#34;&gt;DINO&lt;/a&gt; which reached the SOTA of DETR-like models with &lt;strong&gt;63.3mAP&lt;/strong&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy to Use.&lt;/strong&gt; detrex is designed to be &lt;strong&gt;light-weight&lt;/strong&gt; and easy for users to use:&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://detectron2.readthedocs.io/en/latest/tutorials/lazyconfigs.html&#34;&gt;LazyConfig System&lt;/a&gt; for more flexible syntax and cleaner config files.&lt;/li&gt; &#xA;    &lt;li&gt;Light-weight &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/tools/train_net.py&#34;&gt;training engine&lt;/a&gt; modified from detectron2 &lt;a href=&#34;https://github.com/facebookresearch/detectron2/raw/main/tools/lazyconfig_train_net.py&#34;&gt;lazyconfig_train_net.py&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Apart from detrex, we also released a repo &lt;a href=&#34;https://github.com/IDEA-Research/awesome-detection-transformer&#34;&gt;Awesome Detection Transformer&lt;/a&gt; to present papers about Transformer for detection and segmentation.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Fun Facts&lt;/h2&gt; &#xA;&lt;p&gt;The repo name detrex has several interpretations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;font color=&#34;blue&#34;&gt; &lt;b&gt; detr-ex &lt;/b&gt; &lt;/font&gt;: We take our hats off to DETR and regard this repo as an extension of Transformer-based detection algorithms.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;font color=&#34;#db7093&#34;&gt; &lt;b&gt; det-rex &lt;/b&gt; &lt;/font&gt;: rex literally means &#39;king&#39; in Latin. We hope this repo can help advance the state of the art on object detection by providing the best Transformer-based detection algorithms from the research community.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;font color=&#34;#008000&#34;&gt; &lt;b&gt; de-t.rex &lt;/b&gt; &lt;/font&gt;: de means &#39;the&#39; in German. T.rex, also called Tyrannosaurus Rex, means &#39;king of the tyrant lizards&#39; and connects to our research work &#39;DINO&#39;, which is short for Dinosaur.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://detrex.readthedocs.io/en/latest/tutorials/Installation.html&#34;&gt;Installation Instructions&lt;/a&gt; for the details of installation.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://detrex.readthedocs.io/en/latest/tutorials/Getting_Started.html&#34;&gt;Getting Started with detrex&lt;/a&gt; for the basic usage of detrex.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://detrex.readthedocs.io/en/latest/index.html&#34;&gt;documentation&lt;/a&gt; for full API documentation and tutorials.&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;Results and models are available in &lt;a href=&#34;https://detrex.readthedocs.io/en/latest/tutorials/Model_Zoo.html&#34;&gt;model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; Supported methods &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/projects/detr/&#34;&gt;DETR (ECCV&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/projects/dab_deformable_detr/&#34;&gt;Deformable-DETR (ICLR&#39;2021 Oral)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/projects/conditional_detr/&#34;&gt;Conditional DETR (ICCV&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/projects/dab_detr/&#34;&gt;DAB-DETR (ICLR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/projects/dab_deformable_detr/&#34;&gt;DAB-Deformable-DETR (ICLR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/projects/dn_detr/&#34;&gt;DN-DETR (CVPR&#39;2022 Oral)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/projects/dn_deformable_detr/&#34;&gt;DN-Deformable-DETR (CVPR&#39;2022 Oral)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/projects/dino/&#34;&gt;DINO (ArXiv&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/projects/&#34;&gt;projects&lt;/a&gt; for the details about projects that are built based on detrex.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Change Log&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;strong&gt;beta v0.1.0&lt;/strong&gt; version was released in 21/09/2022. Highlights of the released version:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support various backbones, including: &lt;a href=&#34;https://arxiv.org/abs/2203.11926&#34;&gt;FocalNet&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2103.14030.pdf&#34;&gt;Swin-T&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34;&gt;ResNet&lt;/a&gt; and other &lt;a href=&#34;https://github.com/facebookresearch/detectron2/tree/main/detectron2/modeling/backbone&#34;&gt;detectron2 builtin backbones&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;timm&lt;/a&gt; backbone wrapper and &lt;a href=&#34;https://github.com/pytorch/vision&#34;&gt;torchvision&lt;/a&gt; backbone wrapper.&lt;/li&gt; &#xA; &lt;li&gt;Support various Transformer-based detection algorithms, including: &lt;a href=&#34;https://arxiv.org/abs/2005.12872&#34;&gt;DETR&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.04159&#34;&gt;Deformable-DETR&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2108.06152&#34;&gt;Conditional-DETR&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2201.12329&#34;&gt;DAB-DETR&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2203.01305&#34;&gt;DN-DETR&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2203.03605&#34;&gt;DINO&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support flexible config system based on &lt;a href=&#34;https://detectron2.readthedocs.io/en/latest/tutorials/lazyconfigs.html&#34;&gt;Lazy Configs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/changlog.md&#34;&gt;changelog.md&lt;/a&gt; for details and release history.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/detrex/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;detrex is an open-source toolbox for Transformer-based detection algorithms created by researchers of &lt;strong&gt;IDEACVR&lt;/strong&gt;. We appreciate all contributions to detrex!&lt;/li&gt; &#xA; &lt;li&gt;detrex is built based on &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;Detectron2&lt;/a&gt; and part of its module design is borrowed from &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/detr&#34;&gt;DETR&lt;/a&gt;, and &lt;a href=&#34;https://github.com/fundamentalvision/Deformable-DETR&#34;&gt;Deformable-DETR&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{ren2022detrex,&#xA;  author =       {Tianhe Ren and Shilong Liu and Hao Zhang and&#xA;                  Feng Li and Xingyu Liao and Lei Zhang},&#xA;  title =        {detrex},&#xA;  howpublished = {\url{https://github.com/IDEA-Research/detrex}},&#xA;  year =         {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>advanced-threat-research/Creosote</title>
    <updated>2022-09-25T01:36:11Z</updated>
    <id>tag:github.com,2022-09-25:/advanced-threat-research/Creosote</id>
    <link href="https://github.com/advanced-threat-research/Creosote" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Creosote is our solution to searching for the tarfile vulnerability described by CVE-2007-4559.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Creosote&lt;/h1&gt; &#xA;&lt;p&gt;Creosote is our solution to searching for the tarfile vulnerability described by CVE-2007-4559. The tool recursively traverses the given directory searching for python files. When the tool finds python files it scans them for the tarfile module and then parses the code into an AST to look for vulnerable code.&lt;/p&gt; &#xA;&lt;p&gt;Creosote categorizes all found vulnerabilities under 3 main categories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vuln: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This is the highest confidence level the tool can give, anything marked as a vuln should be analyzed.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Probable Vuln &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Anything marked as probably vulnerable had the structure of a vuln but had some sort of indication of potentially being checked by the program.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Potential Vuln &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This is a catch all to make sure nothing gets missed.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In order to run Creosote you just need to pass it the directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 creosote.py &amp;lt;directory to scan&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Creosote runs on both Linux, macOS, and Windows. The tool has been tested for Python 3.9 and later.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>krea-ai/open-prompts</title>
    <updated>2022-09-25T01:36:11Z</updated>
    <id>tag:github.com,2022-09-25:/krea-ai/open-prompts</id>
    <link href="https://github.com/krea-ai/open-prompts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name=&#34;readme-top&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- PROJECT LOGO --&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/krea_ai/open-prompts&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/static/krea.gif&#34; alt=&#34;Logo&#34; width=&#34;auto&#34; height=&#34;200&#34;&gt; &lt;/a&gt; &#xA; &lt;h3 align=&#34;center&#34;&gt;open prompts&lt;/h3&gt; &#xA; &lt;p align=&#34;center&#34;&gt; open prompt knowledge. &lt;br&gt; &lt;a href=&#34;https://krea.ai&#34;&gt;&lt;strong&gt;explore prompts&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://theprompter.substack.com/&#34;&gt;newsletter&lt;/a&gt; ¬∑ &lt;a href=&#34;https://discord.gg/3mkFbvPYut&#34;&gt;community&lt;/a&gt; ¬∑ &lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#contributing&#34;&gt;contribute&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://twitter.com/krea_ai&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/twitter/follow/krea_ai&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ABOUT THE PROJECT --&gt; &#xA;&lt;h1&gt;Open Prompts&lt;/h1&gt; &#xA;&lt;!-- [![Product Name Screen Shot][product-screenshot]](https://example.com) --&gt; &#xA;&lt;p&gt;Open Prompts contains the data we use to build &lt;a href=&#34;https://krea.ai&#34;&gt;krea.ai&lt;/a&gt;. Now, you can get access to this data too.&lt;/p&gt; &#xA;&lt;p&gt;You can either download a (large) CSV file with image links and meta-data of &amp;gt;10M generations, or access it through &lt;a href=&#34;https://devapi.krea.ai&#34;&gt;our free API&lt;/a&gt; (still in development).&lt;/p&gt; &#xA;&lt;p&gt;Everyone is welcome to &lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#contributing&#34;&gt;contribute&lt;/a&gt; with their own prompts, and &lt;a href=&#34;https://discord.gg/K8anVEWbQC&#34;&gt;ideas&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use this data to implement a semantic search engine with CLIP (like we did), check out &lt;a href=&#34;https://github.com/krea-ai/prompt-search&#34;&gt;prompt-search&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;!-- GETTING STARTED --&gt; &#xA;&lt;h1&gt;About&lt;/h1&gt; &#xA;&lt;p&gt;AI models like Stable Diffusion, DALL-E, or Midjourney, are capable of creating stunning images from text descriptions. They provide us with freedom to produce an image of almost anything we can imagine.&lt;/p&gt; &#xA;&lt;p&gt;Platforms like Lexica, OpenArt, and Krea.ai let us explore millions of AI generated images‚Äîas well as the prompts that produced them. They help you see what words work for generating certain styles and to assess how each AI model interprets different concepts.&lt;/p&gt; &#xA;&lt;p&gt;We are just starting to explore the possibilities of text-to-image models, and we do not necessarily need to re-train them to dramatically improve their results; we can also learn how to prompt them effectively.&lt;/p&gt; &#xA;&lt;p&gt;We hope this repository serves anyone who wants to analyze large datasets of prompts, create datasets to train new models, or build tools that help people create better prompts.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Data&lt;/h1&gt; &#xA;&lt;p&gt;There are three main data sources that you can use.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prompts API&lt;/strong&gt;. We released a (experimental) REST-based API that you can query to find and paginate through prompts‚Äîand its generations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CSV dataset&lt;/strong&gt;. This is a large CSV file that contains more than 10 million generations extracted from the Stability AI Discord during the beta testing of Stable Diffusion v1.3.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;strong&gt;For now, this dataset only includes URLs that are served from the official Discord CDN&lt;/strong&gt;. If you want to download compressed images in &lt;code&gt;webp&lt;/code&gt; format, use the Prompts API where the data has been parsed and stored in our internal servers.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;This repository&lt;/strong&gt;. This repository contains a small but structured set of data that we created for the category section of &lt;a href=&#34;https://www.krea.ai&#34;&gt;krea.ai&lt;/a&gt;. Anyone can &lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#contributing&#34;&gt;contribute&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Prompts API&lt;/h2&gt; &#xA;&lt;p&gt;You can get query data from the dataset using our (experimental) &lt;a href=&#34;https://devapi.krea.ai&#34;&gt;Prompts API&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;CSV file&lt;/h2&gt; &#xA;&lt;p&gt;To download the dataset click &lt;strong&gt;&lt;a href=&#34;https://drive.google.com/file/d/1c4WHxtlzvHYd0UY5WCMJNn2EO-Aiv2A0/view&#34;&gt;this link&lt;/a&gt;&lt;/strong&gt; or execute the following &lt;code&gt;wget&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;wget --load-cookies /tmp/cookies.txt &#34;https://docs.google.com/uc?export=download&amp;amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate &#39;https://docs.google.com/uc?export=download&amp;amp;id=1c4WHxtlzvHYd0UY5WCMJNn2EO-Aiv2A0&#39; -O- | sed -rn &#39;s/.*confirm=([0-9A-Za-z_]+).*/\1\n/p&#39;)&amp;amp;id=1c4WHxtlzvHYd0UY5WCMJNn2EO-Aiv2A0&#34; -O openprompts.csv &amp;amp;&amp;amp; rm /tmp/cookies.txt&#xA;&#xA;# See: https://stackoverflow.com/a/39087286/10391569&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Lite dataset&lt;/h3&gt; &#xA;&lt;p&gt;Since the file is large (&amp;gt;3 GB), you may want to download a ‚Äúlite‚Äù version of it first so you can experiment with the data. You can find the mini-dataset in the &lt;code&gt;data&lt;/code&gt; subfolder (&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/data/1k.csv&#34;&gt;the dataset file&lt;/a&gt; is named &lt;code&gt;1k.csv&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Structure of the dataset&lt;/h3&gt; &#xA;&lt;p&gt;The CSV file has a simple and raw structure. There are two columns: &lt;code&gt;prompt&lt;/code&gt; and &lt;code&gt;raw_data&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import csv&#xA;import json&#xA;&#xA;from pprint import pprint&#xA;&#xA;with open(&#34;dataset.csv&#34;) as f:&#xA;    csv_reader = csv.DictReader(f)&#xA;    for row_number, row in enumerate(csv_reader):&#xA;        if row_number &amp;gt; 0:&#xA;            break&#xA;        datum = row&#xA;&#xA;pprint(datum[&#39;prompt&#39;])&#xA;pprint(datum[&#39;raw_data&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OUTPUT&#xA;(&#39;A portrait photo of a kangaroo wearing an orange hoodie and blue sunglasses &#39;&#xA; &#39;standing on the grass in front of the Sydney Opera House holding a sign on &#39;&#xA; &#39;the chest that says Welcome Friends, subject: kangaroo, subject detail: &#39;&#xA; &#39;wearing orange hoodie, wearing blue sunglasses, subject location: sydney &#39;&#xA; &#39;opera house, subject action: holding sign&#39;)&#xA;&#xA;{&#39;image_uri&#39;: &#39;PENDING&#39;,&#xA; &#39;modifiers&#39;: [&#39;portrait photo&#39;,&#xA;               &#39;kangaroo wearing&#39;,&#xA;               &#39;orange hoodie&#39;,&#xA;               &#39;blue sunglasses standing&#39;,&#xA;               &#39;grass&#39;,&#xA;               &#39;sydney opera house holding&#39;,&#xA;               &#39;sign&#39;,&#xA;               &#39;chest&#39;,&#xA;               &#39;says welcome friends&#39;,&#xA;               &#39;subject kangaroo&#39;,&#xA;               &#39;subject detail wearing orange hoodie&#39;,&#xA;               &#39;wearing blue sunglasses&#39;,&#xA;               &#39;subject location sydney opera house&#39;,&#xA;               &#39;subject action holding sign&#39;],&#xA; &#39;raw_discord_data&#39;: {&#39;cfg_scale&#39;: 15.0,&#xA;                      &#39;content&#39;: &#39;&#39;,&#xA;                      &#39;content_type&#39;: &#39;image/png&#39;,&#xA;                      &#39;height&#39;: 512,&#xA;                      &#39;image_proxy_uri&#39;: &#39;&#39;,&#xA;                      &#39;image_uri&#39;: &#39;https://cdn.discordapp.com/attachments/1005543895024812062/1006343074768769054/A_portrait_photo_of_a_kangaroo_wearing_an_orange_hoodie_and_blue_sunglasses_standing_on_the_grass_in_front_of_the_Sydney_Opera_House_holding_a_sign_-C_15.0_-n_9_-i_-S_556046175_ts-1660001285_idx-4.png&#39;,&#xA;                      &#39;is_grid&#39;: 0,&#xA;                      &#39;num_generations&#39;: 9,&#xA;                      &#39;num_step&#39;: 50,&#xA;                      &#39;seed&#39;: 556046175,&#xA;                      &#39;timestamp&#39;: 1660001285,&#xA;                      &#39;width&#39;: 512},&#xA; &#39;thumbnail_uri&#39;: &#39;PENDING&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We may publish parsing scripts in the future, but we are focused on building more features for &lt;a href=&#34;https://www.krea.ai&#34;&gt;krea.ai&lt;/a&gt; for now. If you know Python, we would love to feature your parsing scripts here. To do so, simply &lt;a href=&#34;https://github.com/krea-ai/open-prompts/fork&#34;&gt;fork the repo and submit a PR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How was the dataset made?&lt;/h3&gt; &#xA;&lt;p&gt;This dataset was created using a crawler and one-time use parsing scripts that mixed our own crawled generations with the &lt;a href=&#34;https://github.com/paperdave/stable-diffusion-sqlite&#34;&gt;dataset&lt;/a&gt; published by &lt;a href=&#34;https://github.com/paperdave/&#34;&gt;paperdave&lt;/a&gt; (thanks Dave!).&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;In-repository dataset&lt;/h2&gt; &#xA;&lt;p&gt;This dataset started as a manual work that we conducted to create the modifiers in &lt;a href=&#34;https://www.krea.ai&#34;&gt;krea.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is smaller than the previous dataset, but it is simpler as well. It is just plain-text files that anyone can edit.&lt;/p&gt; &#xA;&lt;p&gt;We want the best prompt engineers out there to grow it for the benefit of everyone else. For now the instructions for contributing can be found &lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#contributing&#34;&gt;here&lt;/a&gt;, but in the future we will look for a cleaner way to upload prompts to this dataset‚Äîideally including images too!&lt;/p&gt; &#xA;&lt;p&gt;This dataset differentiates between two different kinds of elements: &lt;em&gt;modifiers&lt;/em&gt; and &lt;em&gt;presets&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Modifiers&lt;/h3&gt; &#xA;&lt;p&gt;Modifiers are those parts of a text prompt that contain the stylistic information of it. For example, if we want a prompt to look like a 3D render, we could use &lt;code&gt;octane render&lt;/code&gt;, &lt;code&gt;unreal engine&lt;/code&gt;, or &lt;code&gt;ray tracing&lt;/code&gt; to enhance the style of our generations.&lt;/p&gt; &#xA;&lt;p&gt;Modifiers can be very variate, from very precise colors and shapes to very abstract concepts and emotions‚Äîsome people even find it useful to use emojis! The following is a tree representation of how we have organized the modifiers in this project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ README.md&#xA;‚îú‚îÄ‚îÄ modifiers&#xA;‚îÇ   ‚îú‚îÄ‚îÄ modifier-category-1&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ modifier-subcategory-1.txt&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ modifier-subcategory-2.txt&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ   ‚îú‚îÄ‚îÄ modifier-category-2&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All the modifiers can be found within the folder &lt;code&gt;modifiers&lt;/code&gt;, and they are organized within sub-categories that at its time belong to a parent category. Each of the subfolders within &lt;code&gt;modifiers&lt;/code&gt; represents a different category‚Äîand the name of each subfolder specifies the name of each category. Sub-categories are represented within &lt;code&gt;txt&lt;/code&gt; files where their name represent the name of the sub-category, and they contain a different modifier in each row.&lt;/p&gt; &#xA;&lt;p&gt;The following is an example of how the subcategory &lt;code&gt;3D&lt;/code&gt; from the category &lt;code&gt;digital art&lt;/code&gt; could look like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;artstation&#xA;renderman&#xA;octane render&#xA;3d render&#xA;high quality 3d render&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that each line represents a SINGLE modifier, and that there is nothing else in the file, just modifiers separated by lines.&lt;/p&gt; &#xA;&lt;h3&gt;Presets&lt;/h3&gt; &#xA;&lt;p&gt;Presets are sets of modifiers that work well when used together and they normally share similarities. Organizing sets of modifiers within presets can come handy for speeding up the creation of prompts. For example, if we know that &lt;code&gt;greg rutkowski&lt;/code&gt; creates amazing 3D art, we will probably find ourselves combining it all the time with modifiers such as &lt;code&gt;unreal engine&lt;/code&gt;, &lt;code&gt;3D&lt;/code&gt;, &lt;code&gt;artstation&lt;/code&gt; and even with other similar artists like &lt;code&gt;wlop&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The following is a tree representation of how we have organized the presets in this project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ README.md&#xA;‚îú‚îÄ‚îÄ presets&#xA;‚îÇ   ‚îú‚îÄ‚îÄ preset-author&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preset-title-1.txt&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preset-title-2.txt&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ   ‚îú‚îÄ‚îÄ preset-author-2&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All the presets can be found within a folder within &lt;code&gt;presets&lt;/code&gt;. Each of these folders will contain the name of the author that created each preset. Inside these folders, each preset is created in a different &lt;code&gt;txt&lt;/code&gt; file. Each file contains a different modifier in each row.&lt;/p&gt; &#xA;&lt;p&gt;The following is an example of how the subcategory &lt;code&gt;glossy tubes&lt;/code&gt; from the category &lt;code&gt;krea&lt;/code&gt; could look like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;glossy translucent glass with abstract tubular shapes&#xA;psychedelic texture&#xA;colors range between pastel blue and pastel pink&#xA;highly intricate&#xA;hyper detailed render&#xA;caspar david friedrich&#xA;ArtStation HD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We found that using all these modifiers combined works particularly well.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/krea-ai/open-prompts/fork&#34;&gt;We&lt;/a&gt; &lt;a href=&#34;https://github.com/krea-ai/open-prompts/fork&#34;&gt;love&lt;/a&gt; &lt;a href=&#34;https://github.com/krea-ai/open-prompts/fork&#34;&gt;PRs&lt;/a&gt;! If you want to add your own parsing scripts, modifiers to the in-repository dataset‚Äîor anything really‚Äîsimply &lt;a href=&#34;https://github.com/krea-ai/open-prompts/fork&#34;&gt;fork the repository&lt;/a&gt; and propose changes. We will review them swiftly.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Create your own CLIP Search Engine with &lt;em&gt;Open Prompts&lt;/em&gt;&lt;/h1&gt; &#xA;&lt;p&gt;In our &lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/clip-search&#34;&gt;https://github.com/krea-ai/clip-search&lt;/a&gt; repository you will find everything you need to create a semantic search engine with CLIP.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;!-- LICENSE --&gt; &#xA;&lt;!-- ## License&#xA;&#xA;Distributed under the MIT License. See `LICENSE.txt` for more information.&#xA;&#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; --&gt; &#xA;&lt;!-- CONTACT --&gt; &#xA;&lt;h1&gt;Get in touch&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow and DM us on Twitter: &lt;a href=&#34;https://twitter.com/krea_ai&#34;&gt;@krea_ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join &lt;a href=&#34;https://discord.gg/3mkFbvPYut&#34;&gt;our Discord community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Email either &lt;code&gt;v&lt;/code&gt; or &lt;code&gt;d&lt;/code&gt; (&lt;code&gt;v&lt;/code&gt; at &lt;code&gt;krea&lt;/code&gt; dot &lt;code&gt;ai&lt;/code&gt;; &lt;code&gt;d&lt;/code&gt; at &lt;code&gt;krea&lt;/code&gt; dot &lt;code&gt;ai&lt;/code&gt; respectively)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://twitter.com/blademort&#34;&gt;@blademort&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;art&lt;/strong&gt;: &lt;code&gt;art movements&lt;/code&gt;, &lt;code&gt;art styles&lt;/code&gt;, and &lt;code&gt;descriptive terms&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;general&lt;/strong&gt;: &lt;code&gt;design tools and communities&lt;/code&gt;, and &lt;code&gt;genres&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/krea-ai/open-prompts/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; &#xA;&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</summary>
  </entry>
</feed>