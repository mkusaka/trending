<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-17T01:44:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chatanywhere/GPT_API_free</title>
    <updated>2023-12-17T01:44:28Z</updated>
    <id>tag:github.com,2023-12-17:/chatanywhere/GPT_API_free</id>
    <link href="https://github.com/chatanywhere/GPT_API_free" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free ChatGPT API Key，免费ChatGPT API，支持GPT4 API（免费），ChatGPT国内可用免费转发API，直连无需代理。可以搭配ChatBox等软件/插件使用，极大降低接口使用成本。国内即可无限制畅快聊天。&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/logo.png&#34; alt=&#34;icon&#34; width=&#34;50px&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;GPT-API-free&lt;/h1&gt; &#xA; &lt;p&gt;支持 &lt;strong&gt;GPT-4&lt;/strong&gt; / GPT-3.5-Turbo / GPT-3.5-Turbo-16K / embeddings / DALL·E / whisper / text-davinci&lt;/p&gt; &#xA; &lt;p&gt;国内动态加速 直连无需代理&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8&#34;&gt;快速开始&lt;/a&gt; / &lt;a href=&#34;https://chatanywhere.apifox.cn/&#34;&gt;API文档&lt;/a&gt; / &lt;a href=&#34;https://api.chatanywhere.org/v1/oauth/free/github/render&#34;&gt;申请内测免费Key&lt;/a&gt; / &lt;a href=&#34;https://peiqi.shop/&#34;&gt;支持付费Key&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://qm.qq.com/cgi-bin/qm/qr?k=OFhxu4Z3qI-c-76QJfC2LLXfKGr0g-57&amp;amp;jump_from=webapi&amp;amp;authKey=Kzuf7g4fsE0ZAM7RN+7XvivEANxgDVqDbUs3WI6cB98pt4pFzq/3L8NMiMOy+mo1&#34;&gt;QQ群: 780366686&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;隐私声明&lt;/h2&gt; &#xA;&lt;p&gt;该项目高度重视隐私，致力于保护其用户的隐私。该项目不会以任何方式收集、记录或存储用户输入的任何文本或由 OpenAI 服务器返回的任何文本。该项目不会向 OpenAI 或任何第三方提供有关 API 调用者的身份的任何信息，包括但不限于 IP 地址和用户代理字符串。&lt;/p&gt; &#xA;&lt;p&gt;但OpenAI官方会根据其&lt;a href=&#34;https://platform.openai.com/docs/data-usage-policies&#34;&gt;数据使用政策&lt;/a&gt;保留 30 天的数据。&lt;/p&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年6月14日&lt;/strong&gt; 适配GPT-3.5-Turbo-16K，免费key也支持16k模型；付费key跟随官方价格降低收费。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年6月15日&lt;/strong&gt; 适配0613版本新增的functions。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年6月18日&lt;/strong&gt; 新增对语音转文字模型Whisper支持。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年8月4日&lt;/strong&gt; 免费Key不再支持gpt-3.5-turbo-16k模型调用。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年9月7日&lt;/strong&gt; chatapi.chatanywhere.cn镜像站不再向国内用户提供服务，不影响API的正常使用。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年11月8日&lt;/strong&gt; 支持1106版本各模型，支持TTS文本转语音模型。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年11月19日&lt;/strong&gt; 支持gpt-4-1106-preview模型，价格仅原先gpt-4模型的三分之一到二分之一。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年11月29日&lt;/strong&gt; 开放免费API的gpt-4权限，每天可以免费使用10次。（不保证能长期提供）&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;特点&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;支持Models, Embedding, text-davinci, GPT-3.5-Turbo, GPT-3.5-Turbo-16K(免费版不支持), &lt;em&gt;&lt;strong&gt;GPT-4&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;DALLE&lt;/strong&gt;&lt;/em&gt;(免费版不支持), &lt;em&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/em&gt;(免费版不支持)。（免费版就可以支持AutoGPT, gpt_academic, langchain等）&lt;/li&gt; &#xA; &lt;li&gt;免费版支持GPT-4，一天10次。（免费版gpt-4相对慢一些，付费版更稳定）&lt;/li&gt; &#xA; &lt;li&gt;与官方完全一致的接口标准，兼容各种软件/插件。&lt;/li&gt; &#xA; &lt;li&gt;支持流式响应。&lt;/li&gt; &#xA; &lt;li&gt;国内线路使用动态加速，体验远优于使用代理连接官方。&lt;/li&gt; &#xA; &lt;li&gt;无需科学上网，国内环境直接可用。&lt;/li&gt; &#xA; &lt;li&gt;个人完全免费使用。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🚩注意事项&lt;/h2&gt; &#xA;&lt;p&gt;❗️&lt;em&gt;近期OpenAI频繁出错，如果遇到无回复，报错等情况，可以查看 status.openai.com ，很大可能是OpenAI官方服务问题。&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;❗️&lt;strong&gt;免费API Key仅可用于个人非商业用途，教育，非营利性科研工作中。严禁商用，严禁大规模训练商用模型！训练科研用模型请提前加群联系我们。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;❗️我们将不定期对被滥用的Key进行封禁，如发现自己的key被误封请通过QQ群联系我们。&lt;/p&gt; &#xA;&lt;p&gt;❗️我们的系统仅供内部评估测试使用，商用或面向大众使用请自行承担风险。&lt;/p&gt; &#xA;&lt;p&gt;为了该项目长久发展，免费API Key限制&lt;strong&gt;60请求/小时/IP&amp;amp;Key&lt;/strong&gt;调用频率，也就是说你如果在一个IP下使用多个Key，所有Key的每小时请求数总和不能超过60；同理，你如果将一个Key用于多个IP，这个Key的每小时请求数也不能超过60。(&lt;strong&gt;付费版API没有这个限制&lt;/strong&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;免费使用&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;🚀&lt;a href=&#34;https://api.chatanywhere.org/v1/oauth/free/github/render&#34;&gt;申请领取内测免费API Key&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;转发Host1: &lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt; (国内中转，延时更低，host1和host2二选一)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;转发Host2: &lt;code&gt;https://api.chatanywhere.com.cn&lt;/code&gt; (国内中转，延时更低，host1和host2二选一)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;转发Host3: &lt;code&gt;https://api.chatanywhere.cn&lt;/code&gt; (国外使用,国内需要全局代理)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;我们会定期根据使用量进行相应的扩容，只要不被官方制裁我们会一直提供免费API，如果该项目对你有帮助，还请为我们点一个&lt;em&gt;&lt;strong&gt;Star&lt;/strong&gt;&lt;/em&gt;。如果遇到问题可以在&lt;a href=&#34;https://github.com/chatanywhere/GPT_API_free/issues&#34;&gt;Issues&lt;/a&gt;中反馈，有空会解答。&lt;/p&gt; &#xA;&lt;p&gt;该API Key用于转发API，需要将Host改为&lt;code&gt;api.chatanywhere.tech&lt;/code&gt;(国内首选)或者&lt;code&gt;api.chatanywhere.cn&lt;/code&gt;(国外使用，国内需要全局代理)。&lt;/p&gt; &#xA;&lt;h2&gt;付费版API&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;纯公益提供免费Key显然不是能持久运营下去的方案，所以我们引入付费API Key维持项目的日常开销，以促进项目的良性循环，还望大家理解。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://peiqi.shop/&#34;&gt;购买低价付费Key&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;支持GPT4 API&lt;/strong&gt;，价格仅官方价格85折。&lt;/li&gt; &#xA; &lt;li&gt;性价比高，除了GPT4的其他模型价格相当于官网价格七分之一。&lt;/li&gt; &#xA; &lt;li&gt;同官网计费策略，流式问答使用tiktoken库准确计算Tokens，非流式问答直接使用官方返回Tokens用量计费。&lt;/li&gt; &#xA; &lt;li&gt;余额不会过期，永久有效。根据用户反馈30块钱个人中度使用GPT3.5估计能用一年。&lt;/li&gt; &#xA; &lt;li&gt;所有的接口都保证转发自OpenAI官方接口，非peo、plus等不稳定方案，无水分，不掺假，保证稳定性。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;如何使用&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;由于频繁的恶意请求，我们不再直接提供公共的免费Key，现在需要你使用你的Github账号绑定来领取你自己的免费Key。&lt;/li&gt; &#xA; &lt;li&gt;🚀&lt;a href=&#34;https://api.chatanywhere.org/v1/oauth/free/github/render&#34;&gt;申请领取内测免费API Key&lt;/a&gt; 或 &lt;a href=&#34;https://peiqi.shop/&#34;&gt;购买内测付费API Key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;转发Host1: &lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt; (国内中转，延时更低，host1和host2二选一)&lt;/li&gt; &#xA; &lt;li&gt;转发Host2: &lt;code&gt;https://api.chatanywhere.com.cn&lt;/code&gt; (国内中转，延时更低，host1和host2二选一)&lt;/li&gt; &#xA; &lt;li&gt;转发Host3: &lt;code&gt;https://api.chatanywhere.cn&lt;/code&gt; (国外使用,国内需要全局代理)&lt;/li&gt; &#xA; &lt;li&gt;余额和使用记录查询（通知公告也会发在这里）: &lt;a href=&#34;https://api.chatanywhere.tech/&#34;&gt;余额查询及公告&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;转发API无法直接向官方接口api.openai.com发起请求，需要将请求地址改为api.chatanywhere.tech才可以使用，大部分插件和软件都可以修改。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;常见软件/插件使用方法&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;python openai官方库（使用AutoGPT，langchain等）&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;示例代码请参考&lt;a href=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/demo.py&#34;&gt;demo.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;方法一&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_base = &#34;https://api.chatanywhere.tech/v1&#34;&#xA;# openai.api_base = &#34;https://api.chatanywhere.cn/v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;方法二（方法一不起作用用这个）&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;修改环境变量OPENAI_API_BASE，各个系统怎么改环境变量请自行搜索，修改环境变量后不起作用请重启系统。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_BASE=https://api.chatanywhere.tech/v1&#xA;或 OPENAI_API_BASE=https://api.chatanywhere.cn/v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;开源gpt_academic&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;找到&lt;code&gt;config.py&lt;/code&gt;文件中的&lt;code&gt;API_URL_REDIRECT&lt;/code&gt;配置并修改为以下内容：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;API_URL_REDIRECT = {&#34;https://api.openai.com/v1/chat/completions&#34;: &#34;https://api.chatanywhere.tech/v1/chat/completions&#34;}&#xA;# API_URL_REDIRECT = {&#34;https://api.openai.com/v1/chat/completions&#34;: &#34;https://api.chatanywhere.cn/v1/chat/completions&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;BotGem(AMA)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;ChatGPT桌面应用，支持全平台，&lt;em&gt;&lt;strong&gt;支持gpt-4-vision&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt; &#xA;&lt;p&gt;下载链接：&lt;a href=&#34;https://bytemyth.com/ama&#34;&gt;https://bytemyth.com/ama&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;使用方法：下载安装后在设置中如图设置，并点击更新。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/botgem.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;ChatBox&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;ChatGPT开源桌面应用，支持全部桌面平台。&lt;/p&gt; &#xA;&lt;p&gt;下载链接：&lt;a href=&#34;https://github.com/Bin-Huang/chatbox/releases&#34;&gt;https://github.com/Bin-Huang/chatbox/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;使用方法：如图在设置中填入购买的密钥，并将代理设置为&lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt;即可&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/chatbox.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Zotero插件&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;pdf阅读插件zotero-gpt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;下载链接：&lt;a href=&#34;https://github.com/MuiseDestiny/zotero-gpt/releases&#34;&gt;https://github.com/MuiseDestiny/zotero-gpt/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;安装好插件后使用以下命令设置，还是不会可以去b站搜教程。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/api https://api.chatanywhere.tech&#xA;&#xA;/secretKey 购买的转发key 记住别忘记带sk-&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/zotero-gpt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;翻译插件zotero-pdf-translate&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;下载链接：&lt;a href=&#34;https://github.com/windingwind/zotero-pdf-translate/releases&#34;&gt;https://github.com/windingwind/zotero-pdf-translate/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;接口地址填写: &lt;a href=&#34;https://api.chatanywhere.tech/v1/chat/completions&#34;&gt;https://api.chatanywhere.tech/v1/chat/completions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;不用管状态是否显示可用 填上之后就可以了&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/zotero-pdf-translate.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;浏览器插件ChatGPT Sidebar&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;官网链接：&lt;a href=&#34;https://chatgpt-sidebar.com/&#34;&gt;https://chatgpt-sidebar.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;安装好插件后进入设置页面，如图所示修改设置，将url修改为 &lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt; 。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/sidebar.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Jetbrains插件ChatGPT - Easycode&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/jet1.png&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;安装好插件后在Settings &amp;gt; Tools &amp;gt; OpenAI &amp;gt; GPT 3.5 Turbo中如图所示配置好插件，重点要将Server Settings 修改为 &lt;code&gt;https://api.chatanywhere.tech/v1/chat/completions&lt;/code&gt; 。并勾选Customize Server。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/jet2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Raycast 插件 ChatGPT&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;在 Raycast Store 中找到 ChatGPT 插件，并按照提示安装： &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;安装完成后在该插件配置中的 &lt;code&gt;API Key&lt;/code&gt; 中填入我们的API Key，以及选中 &lt;code&gt;Change API Endpoint&lt;/code&gt;，并在 &lt;code&gt;API Endpoint&lt;/code&gt; 中填入 &lt;code&gt;https://api.chatanywhere.tech/v1&lt;/code&gt; &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast2.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🍺 enjoy it~ &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast4.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;API报错说明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Overload错误&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;具体错误信息：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &#34;error&#34;: {&#xA;    &#34;message&#34;: &#34;That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID xxxxxxxxxxxx in your message.)&#34;,&#xA;    &#34;type&#34;: &#34;server_error&#34;,&#xA;    &#34;param&#34;: null,&#xA;    &#34;code&#34;: null&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;该错误由于OpenAI官方服务器负载高引起，与转发服务器负载无关。一般一段时间后恢复，可以等几秒后再试。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#star-history/star-history&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=chatanywhere/GPT_API_free&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>damo-vilab/i2vgen-xl</title>
    <updated>2023-12-17T01:44:28Z</updated>
    <id>tag:github.com,2023-12-17:/damo-vilab/i2vgen-xl</id>
    <link href="https://github.com/damo-vilab/i2vgen-xl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repo for VGen: a holistic video generation ecosystem for video generation building on diffusion models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VGen&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/damo-vilab/i2vgen-xl/main/source/VGen.jpg&#34; alt=&#34;figure1&#34; title=&#34;figure1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;VGen is an open-source video synthesis codebase developed by the Tongyi Lab of Alibaba Group, featuring state-of-the-art video generative models. This repository includes implementations of the following methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://i2vgen-xl.github.io/&#34;&gt;I2VGen-xl: High-quality image-to-video synthesis via cascaded diffusion models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://videocomposer.github.io/&#34;&gt;VideoComposer: Compositional Video Synthesis with Motion Controllability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://higen-t2v.github.io/&#34;&gt;Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;A Recipe for Scaling up Text-to-Video Generation with Text-free Videos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;InstructVideo: Instructing Video Diffusion Models with Human Feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dreamvideo-t2v.github.io/&#34;&gt;DreamVideo: Composing Your Dream Videos with Customized Subject and Motion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.09109&#34;&gt;VideoLCM: Video Latent Consistency Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.06571&#34;&gt;Modelscope text-to-video technical report&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;VGen can produce high-quality videos from the input text, images, desired motion, desired subjects, and even the feedback signals provided. It also offers a variety of commonly used video generation tools such as visualization, sampling, training, inference, join training using images and videos, acceleration, and more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://i2vgen-xl.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.04145&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/XUi0y7dxqEQ&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441039979087.mp4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/damo-vilab/i2vgen-xl/main/source/logo.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔥News!!!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the high-efficiency video generation method &lt;a href=&#34;https://arxiv.org/abs/2312.09109&#34;&gt;VideoLCM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the code and model of I2VGen-XL and the ModelScope T2V&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the T2V method &lt;a href=&#34;https://higen-t2v.github.io&#34;&gt;HiGen&lt;/a&gt; and customizing T2V method &lt;a href=&#34;https://dreamvideo-t2v.github.io&#34;&gt;DreamVideo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We write an &lt;a href=&#34;https://raw.githubusercontent.com/damo-vilab/i2vgen-xl/main/doc/introduction.pdf&#34;&gt;introduction docment&lt;/a&gt; for VGen and compare I2VGen-XL with SVD.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.11]&lt;/strong&gt; We release a high-quality I2VGen-XL model, please refer to the &lt;a href=&#34;https://i2vgen-xl.github.io&#34;&gt;Webpage&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the technical papers and webpage of &lt;a href=&#34;https://raw.githubusercontent.com/damo-vilab/i2vgen-xl/main/doc/i2vgen-xl.md&#34;&gt;I2VGen-XL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the code and pretrained models that can generate 1280x720 videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release models optimized specifically for the human body and faces&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Updated version can fully maintain the ID and capture large and accurate motions simultaneously&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release other methods and the corresponding models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preparation&lt;/h2&gt; &#xA;&lt;p&gt;The main features of VGen are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Expandability, allowing for easy management of your own experiments.&lt;/li&gt; &#xA; &lt;li&gt;Completeness, encompassing all common components for video generation.&lt;/li&gt; &#xA; &lt;li&gt;Excellent performance, featuring powerful pre-trained models in multiple tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n vgen python=3.8&#xA;conda activate vgen&#xA;pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;We have provided a &lt;strong&gt;demo dataset&lt;/strong&gt; that includes images and videos, along with their lists in &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Please note that the demo images used here are for testing purposes and were not included in the training.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Clone codeb&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/damo-vilab/i2vgen-xl.git&#xA;cd i2vgen-xl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started with VGen&lt;/h2&gt; &#xA;&lt;h3&gt;(1) Train your text-to-video model&lt;/h3&gt; &#xA;&lt;p&gt;Executing the following command to enable distributed training is as easy as that.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_net.py --cfg configs/t2v_train.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the &lt;code&gt;t2v_train.yaml&lt;/code&gt; configuration file, you can specify the data, adjust the video-to-image ratio using &lt;code&gt;frame_lens&lt;/code&gt;, and validate your ideas with different Diffusion settings, and so on.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before the training, you can download any of our open-source models for initialization. Our codebase supports custom initialization and &lt;code&gt;grad_scale&lt;/code&gt; settings, all of which are included in the &lt;code&gt;Pretrain&lt;/code&gt; item in yaml file.&lt;/li&gt; &#xA; &lt;li&gt;During the training, you can view the saved models and intermediate inference results in the &lt;code&gt;workspace/experiments/t2v_train&lt;/code&gt;directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After the training is completed, you can perform inference on the model using the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py --cfg configs/t2v_infer.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can find the videos you generated in the &lt;code&gt;workspace/experiments/test_img_01&lt;/code&gt; directory. For specific configurations such as data, models, seed, etc., please refer to the &lt;code&gt;t2v_infer.yaml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;!-- &lt;table&gt;&#xA;&lt;center&gt;&#xA;  &lt;tr&gt;&#xA;    &lt;td &gt;&lt;center&gt;&#xA;      &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441754174077.mp4&#34;&gt;&lt;/video&gt;&#x9;&#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;    &lt;td &gt;&lt;center&gt;&#xA;      &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441138824052.mp4&#34;&gt;&lt;/video&gt;&#x9;&#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;&lt;/center&gt;&#xA;&lt;/table&gt;&#xA;&lt;/center&gt; --&gt; &#xA;&lt;center&gt; &#xA;&lt;/center&gt;&#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01Ya2I5I25utrJwJ9Jf_!!6000000007587-2-tps-1280-720.png&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i3/O1CN01CrmYaz1zXBetmg3dd_!!6000000006723-2-tps-1280-720.png&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441754174077.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441138824052.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;  &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;  &#xA;&lt;h3&gt;(2) Run the I2VGen-XL model&lt;/h3&gt; &#xA;&lt;p&gt;(i) Download model and test data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;!pip install modelscope&#xA;from modelscope.hub.snapshot_download import snapshot_download&#xA;model_dir = snapshot_download(&#39;damo/I2VGen-XL&#39;, cache_dir=&#39;models/&#39;, revision=&#39;v1.0.0&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(ii) Run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py --cfg configs/i2vgen_xl_infer.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py --cfg configs/i2vgen_xl_infer.yaml  test_list_path data/test_list_for_i2vgen.txt test_model models/i2vgen_xl_00854500.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;test_list_path&lt;/code&gt; represents the input image path and its corresponding caption. Please refer to the specific format and suggestions within demo file &lt;code&gt;data/test_list_for_i2vgen.txt&lt;/code&gt;. &lt;code&gt;test_model&lt;/code&gt; is the path for loading the model. In a few minutes, you can retrieve the high-definition video you wish to create from the &lt;code&gt;workspace/experiments/test_list_for_i2vgen&lt;/code&gt; directory. At present, we find that the current model performs inadequately on &lt;strong&gt;anime images&lt;/strong&gt; and &lt;strong&gt;images with a black background&lt;/strong&gt; due to the lack of relevant training data. We are consistently working to optimize it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt;Due to the compression of our video quality in GIF format, please click &#39;HERE&#39; below to view the original video.&lt;/span&gt;&lt;/p&gt; &#xA;&lt;center&gt; &#xA; &lt;center&gt; &#xA; &lt;/center&gt;&#xA; &lt;table&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01CCEq7K1ZeLpNQqrWu_!!6000000003219-0-tps-1280-720.jpg&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442125067544.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01hIQcvG1spmQMLqBo0_!!6000000005816-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442125067544.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01ZXY7UN23K8q4oQ3uG_!!6000000007236-2-tps-1280-720.png&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441385957074.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01iaSiiv1aJZURUEY53_!!6000000003309-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441385957074.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i3/O1CN01NHpVGl1oat4H54Hjf_!!6000000005242-2-tps-1280-720.png&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442102706767.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;!-- &lt;image muted=&#34;true&#34; height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01DgLj1T240jfpzKoaQ_!!6000000007329-1-tps-1280-704.gif&#34;&gt;&lt;/image&gt;&#x9;&#xA;       --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01DgLj1T240jfpzKoaQ_!!6000000007329-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442102706767.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01odS61s1WW9tXen21S_!!6000000002795-0-tps-1280-720.jpg&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442163934688.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i3/O1CN01Jyk1HT28JkZtpAtY6_!!6000000007912-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442163934688.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt;  &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/center&gt; &#xA;&lt;h3&gt;(3) Other methods&lt;/h3&gt; &#xA;&lt;p&gt;In preparation.&lt;/p&gt; &#xA;&lt;h2&gt;Customize your own approach&lt;/h2&gt; &#xA;&lt;p&gt;Our codebase essentially supports all the commonly used components in video generation. You can manage your experiments flexibly by adding corresponding registration classes, including &lt;code&gt;ENGINE, MODEL, DATASETS, EMBEDDER, AUTO_ENCODER, DISTRIBUTION, VISUAL, DIFFUSION, PRETRAIN&lt;/code&gt;, and can be compatible with all our open-source algorithms according to your own needs. If you have any questions, feel free to give us your feedback at any time.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;p&gt;If this repo is useful to you, please cite our corresponding technical paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{2023i2vgenxl,&#xA;  title={I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models},&#xA;  author={Zhang, Shiwei and Wang, Jiayu and Zhang, Yingya and Zhao, Kang and Yuan, Hangjie and Qing, Zhiwu and Wang, Xiang  and Zhao, Deli and Zhou, Jingren},&#xA;  booktitle={arXiv preprint arXiv:2311.04145},&#xA;  year={2023}&#xA;}&#xA;@article{2023videocomposer,&#xA;  title={VideoComposer: Compositional Video Synthesis with Motion Controllability},&#xA;  author={Wang, Xiang and Yuan, Hangjie and Zhang, Shiwei and Chen, Dayou and Wang, Jiuniu, and Zhang, Yingya, and Shen, Yujun, and Zhao, Deli and Zhou, Jingren},&#xA;  booktitle={arXiv preprint arXiv:2306.02018},&#xA;  year={2023}&#xA;}&#xA;@article{wang2023modelscope,&#xA;  title={Modelscope text-to-video technical report},&#xA;  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},&#xA;  journal={arXiv preprint arXiv:2308.06571},&#xA;  year={2023}&#xA;}&#xA;@article{dreamvideo,&#xA;  title={DreamVideo: Composing Your Dream Videos with Customized Subject and Motion},&#xA;  author={Wei, Yujie and Zhang, Shiwei and Qing, Zhiwu and Yuan, Hangjie and Liu, Zhiheng and Liu, Yu and Zhang, Yingya and Zhou, Jingren and Shan, Hongming},&#xA;  journal={arXiv preprint arXiv:2312.04433},&#xA;  year={2023}&#xA;}&#xA;@article{qing2023higen,&#xA;  title={Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation},&#xA;  author={Qing, Zhiwu and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Wei, Yujie and Zhang, Yingya and Gao, Changxin and Sang, Nong },&#xA;  journal={arXiv preprint arXiv:2312.04483},&#xA;  year={2023}&#xA;}&#xA;@article{wang2023videolcm,&#xA;  title={VideoLCM: Video Latent Consistency Model},&#xA;  author={Wang, Xiang and Zhang, Shiwei and Zhang, Han and Liu, Yu and Zhang, Yingya and Gao, Changxin and Sang, Nong },&#xA;  journal={arXiv preprint arXiv:2312.09109},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to express our gratitude for the contributions of several previous works to the development of VideoComposer. This includes, but is not limited to &lt;a href=&#34;https://arxiv.org/abs/2302.09778&#34;&gt;Composer&lt;/a&gt;, &lt;a href=&#34;https://modelscope.cn/models/damo/text-to-video-synthesis/summary&#34;&gt;ModelScopeT2V&lt;/a&gt;, &lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;OpenCLIP&lt;/a&gt;, &lt;a href=&#34;https://m-bain.github.io/webvid-dataset/&#34;&gt;WebVid-10M&lt;/a&gt;, &lt;a href=&#34;https://laion.ai/blog/laion-400-open-dataset/&#34;&gt;LAION-400M&lt;/a&gt;, &lt;a href=&#34;https://github.com/zhuoinoulu/pidinet&#34;&gt;Pidinet&lt;/a&gt; and &lt;a href=&#34;https://github.com/isl-org/MiDaS&#34;&gt;MiDaS&lt;/a&gt;. We are committed to building upon these foundations in a way that respects their original contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This open-source model is trained with using &lt;a href=&#34;https://m-bain.github.io/webvid-dataset/&#34;&gt;WebVid-10M&lt;/a&gt; and &lt;a href=&#34;https://laion.ai/blog/laion-400-open-dataset/&#34;&gt;LAION-400M&lt;/a&gt; datasets and is intended for &lt;strong&gt;RESEARCH/NON-COMMERCIAL USE ONLY&lt;/strong&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/weak-to-strong</title>
    <updated>2023-12-17T01:44:28Z</updated>
    <id>tag:github.com,2023-12-17:/openai/weak-to-strong</id>
    <link href="https://github.com/openai/weak-to-strong" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;STATUS&lt;/strong&gt;: This codebase is not well tested and does not use the exact same settings we used in the paper, but in our experience gives qualitatively similar results when using large model size gaps and multiple seeds. Expected results can be found for two datasets below. We may update the code significantly in the coming week.&lt;/p&gt; &#xA;&lt;h1&gt;Weak-to-strong generalization&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/weak-to-strong-setup.png&#34; alt=&#34;Our setup and how it relates to superhuman AI alignment&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project contains code for implementing our &lt;a href=&#34;https://cdn.openai.com/papers/weak-to-strong-generalization.pdf&#34;&gt;paper on weak-to-strong generalization&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The primary codebase contains a re-implementation of our weak-to-strong learning setup for binary classification tasks. The codebase contains code for fine-tuning pretrained language models, and also training against the labels from another language model. We support various losses described in the paper as well, such as the confidence auxiliary loss.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;vision&lt;/code&gt; directory contains stand-alone code for weak-to-strong in the vision models setting (AlexNet -&amp;gt; DINO on ImageNet).&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.&lt;/p&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;p&gt;You need to have Python installed on your machine. The project uses &lt;code&gt;pyproject.toml&lt;/code&gt; to manage dependencies. To install the dependencies, you can use a package manager like &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running the Script&lt;/h4&gt; &#xA;&lt;p&gt;The main script of the project is train_weak_to_strong.py. It can be run from the command line using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_weak_to_strong.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script accepts several command-line arguments to customize the training process. Here are some examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_weak_to_strong.py --batch_size 32 --max_ctx 512 --ds_name &#34;sciq&#34; --loss &#34;logconf&#34; --n_docs 1000 --n_test_docs 100 --weak_model_size &#34;gpt2-medium&#34; --strong_model_size &#34;gpt2-large&#34; --seed 42&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Expected results&lt;/h4&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/notebooks/amazon_polarity_None.png&#34; width=&#34;350&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/notebooks/sciq_None.png&#34; width=&#34;350&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/notebooks/Anthropic-hh-rlhf_None.png&#34; width=&#34;350&#34;&gt; &#xA;&lt;h3&gt;Authors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adrien Ecoffet&lt;/li&gt; &#xA; &lt;li&gt;Manas Joglekar&lt;/li&gt; &#xA; &lt;li&gt;Jeffrey Wu&lt;/li&gt; &#xA; &lt;li&gt;Jan Hendrik Kirchner&lt;/li&gt; &#xA; &lt;li&gt;Pavel Izmailov (vision)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the LICENSE.md file for details.&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face for their open-source transformer models&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>