<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-15T01:34:35Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deepseek-ai/DeepSeek-VL</title>
    <updated>2024-03-15T01:34:35Z</updated>
    <id>tag:github.com,2024-03-15:/deepseek-ai/DeepSeek-VL</id>
    <link href="https://github.com/deepseek-ai/DeepSeek-VL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepSeek-VL: Towards Real-World Vision-Language Understanding&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/images/logo.svg?sanitize=true&#34; width=&#34;60%&#34; alt=&#34;DeepSeek LLM&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.deepseek.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Homepage&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/images/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Chat&#34; src=&#34;https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20VL-536af5?color=536af5&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/deepseek-ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Hugging Face&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/Tc7c45Zzu5&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/images/qr.jpeg&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Wechat&#34; src=&#34;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/deepseek_ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/LICENSE-CODE&#34;&gt; &lt;img alt=&#34;Code License&#34; src=&#34;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/LICENSE-MODEL&#34;&gt; &lt;img alt=&#34;Model License&#34; src=&#34;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#3-model-downloads&#34;&gt;Model Download&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#4-quick-start&#34;&gt;Quick Start&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#5-license&#34;&gt;License&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#6-citation&#34;&gt;Citation&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.05525&#34;&gt;&lt;b&gt;Paper Link&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&#34;&gt;&lt;b&gt;Demo&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Introducing DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. DeepSeek-VL possesses general multimodal understanding capabilities, capable of processing logical diagrams, web pages, formula recognition, scientific literature, natural images, and embodied intelligence in complex scenarios.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.05525&#34;&gt;DeepSeek-VL: Towards Real-World Vision-Language Understanding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Haoyu Lu*, Wen Liu*, Bo Zhang**, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan (*Equal Contribution, **Project Lead)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/deepseek-ai/DeepSeek-VL/raw/main/images/sample.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2. Release&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚úÖ &lt;b&gt;2024-03-14&lt;/b&gt;: Demo for DeepSeek-VL-7B available on &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/summary&gt; &#xA; &lt;br&gt;Check out the gradio demo of DeepSeek-VL-7B at &#xA; &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&#34;&gt;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&lt;/a&gt;. Experience its capabilities firsthand! &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚úÖ &lt;b&gt;2024-03-13&lt;/b&gt;: Support DeepSeek-VL gradio demo. &lt;/summary&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚úÖ &lt;b&gt;2024-03-11&lt;/b&gt;: DeepSeek-VL family released, including &lt;code&gt;DeepSeek-VL-7B-base&lt;/code&gt;, &lt;code&gt;DeepSeek-VL-7B-chat&lt;/code&gt;, &lt;code&gt;DeepSeek-VL-1.3B-base&lt;/code&gt;, and &lt;code&gt;DeepSeek-VL-1.3B-chat&lt;/code&gt;.&lt;/summary&gt; &#xA; &lt;br&gt;The release includes a diverse set of models tailored for various applications within the DeepSeek-VL family. The models come in two sizes: 7B and 1.3B parameters, each offering base and chat variants to cater to different needs and integration scenarios. &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;3. Model Downloads&lt;/h2&gt; &#xA;&lt;p&gt;We release the DeepSeek-VL family, including 1.3B-base, 1.3B-chat, 7b-base and 7b-chat models, to the public. To support a broader and more diverse range of research within both academic and commercial communities. Please note that the use of this model is subject to the terms outlined in &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#5-license&#34;&gt;License section&lt;/a&gt;. Commercial usage is permitted under these terms.&lt;/p&gt; &#xA;&lt;h3&gt;Huggingface&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Sequence Length&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-VL-1.3B-base&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-vl-1.3b-base&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-VL-1.3B-chat&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-vl-1.3b-chat&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-VL-7B-base&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-vl-7b-base&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-VL-7B-chat&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-vl-7b-chat&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;4. Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModelForCausalLM&#xA;&#xA;from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM&#xA;from deepseek_vl.utils.io import load_pil_images&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/deepseek-vl-7b-chat&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;&amp;lt;image_placeholder&amp;gt;Describe each stage of this image.&#34;,&#xA;        &#34;images&#34;: [&#34;./images/training_pipelines.jpg&#34;]&#xA;    },&#xA;    {&#xA;        &#34;role&#34;: &#34;Assistant&#34;,&#xA;        &#34;content&#34;: &#34;&#34;&#xA;    }&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation,&#xA;    images=pil_images,&#xA;    force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Chat&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_chat.py --model_path &#34;deepseek-ai/deepseek-vl-7b-chat&#34;&#xA;&#xA;# or local path&#xA;python cli_chat.py --model_path &#34;local model path&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .[gradio]&#xA;&#xA;python deepseek_vl/serve/app_deepseek.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/images/gradio_demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Have Fun!&lt;/p&gt; &#xA;&lt;h2&gt;5. License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is licensed under &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-CODE&#34;&gt;the MIT License&lt;/a&gt;. The use of DeepSeek-VL Base/Chat models is subject to &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-MODEL&#34;&gt;DeepSeek Model License&lt;/a&gt;. DeepSeek-VL series (including Base and Chat) supports commercial use.&lt;/p&gt; &#xA;&lt;h2&gt;6. Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lu2024deepseekvl,&#xA;      title={DeepSeek-VL: Towards Real-World Vision-Language Understanding},&#xA;      author={Haoyu Lu and Wen Liu and Bo Zhang and Bingxuan Wang and Kai Dong and Bo Liu and Jingxiang Sun and Tongzheng Ren and Zhuoshu Li and Hao Yang and Yaofeng Sun and Chengqi Deng and Hanwei Xu and Zhenda Xie and Chong Ruan},&#xA;      year={2024},&#xA;      eprint={2403.05525},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;7. Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href=&#34;mailto:service@deepseek.com&#34;&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Ableton/maxdevtools</title>
    <updated>2024-03-15T01:34:35Z</updated>
    <id>tag:github.com,2024-03-15:/Ableton/maxdevtools</id>
    <link href="https://github.com/Ableton/maxdevtools" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Max device development tools&lt;/h1&gt; &#xA;&lt;h2&gt;Summary&lt;/h2&gt; &#xA;&lt;p&gt;This repository includes resources and tools we use at Ableton when building and maintaining Max devices.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ableton/maxdevtools/main/m4l-production-guidelines/m4l-production-guidelines.md&#34;&gt;Max for Live Production Guidelines&lt;/a&gt; - A collection of suggestions that we recommend keeping in mind in when sharing your Max device.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ableton/maxdevtools/main/patch-code-standard/patch-code-standard.md&#34;&gt;Ableton&#39;s patch code standard&lt;/a&gt; - The patch code standard that Ableton uses for the devices that come with Live.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ableton/maxdevtools/main/maxdiff/README.md&#34;&gt;maxdiff&lt;/a&gt; - A way to get readable git diffs for Max devices and Max patch files.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These are aimed primarily at experienced Max for Live builders. To get started with Max for Live, please have a look at the &lt;a href=&#34;https://www.ableton.com/en/packs/building-max-devices/&#34;&gt;Building Max Devices&lt;/a&gt; Pack.&lt;/p&gt; &#xA;&lt;h2&gt;Maintainers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MattijsKneppers&#34;&gt;@MattijsKneppers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>openai/transformer-debugger</title>
    <updated>2024-03-15T01:34:35Z</updated>
    <id>tag:github.com,2024-03-15:/openai/transformer-debugger</id>
    <link href="https://github.com/openai/transformer-debugger" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Transformer Debugger&lt;/h1&gt; &#xA;&lt;p&gt;Transformer Debugger (TDB) is a tool developed by OpenAI&#39;s &lt;a href=&#34;https://openai.com/blog/introducing-superalignment&#34;&gt;Superalignment team&lt;/a&gt; with the goal of supporting investigations into specific behaviors of small language models. The tool combines &lt;a href=&#34;https://openai.com/research/language-models-can-explain-neurons-in-language-models&#34;&gt;automated interpretability&lt;/a&gt; techniques with &lt;a href=&#34;https://transformer-circuits.pub/2023/monosemantic-features&#34;&gt;sparse autoencoders&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;TDB enables rapid exploration before needing to write code, with the ability to intervene in the forward pass and see how it affects a particular behavior. It can be used to answer questions like, &#34;Why does the model output token A instead of token B for this prompt?&#34; or &#34;Why does attention head H attend to token T for this prompt?&#34; It does so by identifying specific components (neurons, attention heads, autoencoder latents) that contribute to the behavior, showing automatically generated explanations of what causes those components to activate most strongly, and tracing connections between components to help discover circuits.&lt;/p&gt; &#xA;&lt;p&gt;These videos give an overview of TDB and show how it can be used to investigate &lt;a href=&#34;https://arxiv.org/abs/2211.00593&#34;&gt;indirect object identification in GPT-2 small&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/721244075f12439496db5d53439d2f84?sid=8445200e-c49e-4028-8b8e-3ea8d361dec0&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/21b601b8494b40c49b8dc7bfd1dc6829?sid=ee23c00a-9ede-4249-b9d7-c2ba15993556&#34;&gt;Neuron viewer pages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/3478057cec484a1b85471585fef10811?sid=b9c3be4b-7117-405a-8d31-0f9e541dcfb6&#34;&gt;Example: Investigating name mover heads, part 1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/6bd8c6bde84b42a98f9a26a969d4a3ad?sid=4a09ac29-58a2-433e-b55d-762414d9a7fa&#34;&gt;Example: Investigating name mover heads, part 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s in the release?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/transformer-debugger/main/neuron_viewer/README.md&#34;&gt;Neuron viewer&lt;/a&gt;: A React app that hosts TDB as well as pages with information about individual model components (MLP neurons, attention heads and autoencoder latents for both).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/transformer-debugger/main/neuron_explainer/activation_server/README.md&#34;&gt;Activation server&lt;/a&gt;: A backend server that performs inference on a subject model to provide data for TDB. It also reads and serves data from public Azure buckets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/transformer-debugger/main/neuron_explainer/models/README.md&#34;&gt;Models&lt;/a&gt;: A simple inference library for GPT-2 models and their autoencoders, with hooks to grab activations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/transformer-debugger/main/datasets.md&#34;&gt;Collated activation datasets&lt;/a&gt;: top-activating dataset examples for MLP neurons, attention heads and autoencoder latents.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Follow these steps to install the repo. You&#39;ll first need python/pip, as well as node/npm.&lt;/p&gt; &#xA;&lt;p&gt;Though optional, we recommend you use a virtual environment or equivalent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# If you&#39;re already in a venv, deactivate it.&#xA;deactivate&#xA;# Create a new venv.&#xA;python -m venv ~/.virtualenvs/transformer-debugger&#xA;# Activate the new venv.&#xA;source ~/.virtualenvs/transformer-debugger/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once your environment is set up, follow the following steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone git@github.com:openai/transformer-debugger.git&#xA;cd transformer-debugger&#xA;&#xA;# Install neuron_explainer&#xA;pip install -e .&#xA;&#xA;# Set up the pre-commit hooks.&#xA;pre-commit install&#xA;&#xA;# Install neuron_viewer.&#xA;cd neuron_viewer&#xA;npm install&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the TDB app, you&#39;ll then need to follow the instructions to set up the &lt;a href=&#34;https://raw.githubusercontent.com/openai/transformer-debugger/main/neuron_explainer/activation_server/README.md&#34;&gt;activation server backend&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/openai/transformer-debugger/main/neuron_viewer/README.md&#34;&gt;neuron viewer frontend&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Making changes&lt;/h2&gt; &#xA;&lt;p&gt;To validate changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pytest&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;mypy --config=mypy.ini .&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run activation server and neuron viewer and confirm that basic functionality like TDB and neuron viewer pages is still working&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/transformer-debugger/main/terminology.md&#34;&gt;Terminology&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to cite&lt;/h2&gt; &#xA;&lt;p&gt;Please cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Mossing, et al., ‚ÄúTransformer Debugger‚Äù, GitHub, 2024.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;BibTex citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{mossing2024tdb,&#xA;  title={Transformer Debugger},&#xA;  author={Mossing, Dan and Bills, Steven and Tillman, Henk and Dupr√© la Tour, Tom and Cammarata, Nick and Gao, Leo and Achiam, Joshua and Yeh, Catherine and Leike, Jan and Wu, Jeff and Saunders, William},&#xA;  year={2024},&#xA;  publisher={GitHub},&#xA;  howpublished={\url{https://github.com/openai/transformer-debugger}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>