<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-14T01:43:14Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Tele-AI/Telechat</title>
    <updated>2024-01-14T01:43:14Z</updated>
    <id>tag:github.com,2024-01-14:/Tele-AI/Telechat</id>
    <link href="https://github.com/Tele-AI/Telechat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; 星辰语义大模型-TeleChat &lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🤗 &lt;a href=&#34;https://huggingface.co/Tele-AI/Telechat-7B&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt; • 🏔 &lt;a href=&#34;https://gitee.com/mindspore/mindformers/tree/dev/research/telechat&#34; target=&#34;_blank&#34;&gt;MindSpore&lt;/a&gt; • 🐾 &lt;a href=&#34;https://gitee.com/Tele-AI/tele-chat&#34; target=&#34;_blank&#34;&gt;gitee&lt;/a&gt;️ • 💬 &lt;a href=&#34;https://github.com/Tele-AI/Telechat/raw/master/images/wechat.jpg&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.03804&#34; target=&#34;_blank&#34;&gt; Tech Report &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;目录&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/#%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D&#34;&gt;模型介绍&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/#%E6%95%B0%E6%8D%AE%E5%BC%80%E6%BA%90&#34;&gt;数据开源&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/#%E6%95%88%E6%9E%9C%E8%AF%84%E6%B5%8B&#34;&gt;效果评测&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%92%8C%E9%83%A8%E7%BD%B2&#34;&gt;模型推理和部署&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/#%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83&#34;&gt;模型微调&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/#%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96&#34;&gt;模型量化&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/#%E5%9B%BD%E4%BA%A7%E5%8C%96%E9%80%82%E9%85%8D&#34;&gt;国产化适配&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/#%E5%A3%B0%E6%98%8E%E5%8D%8F%E8%AE%AE%E5%BC%95%E7%94%A8&#34;&gt;声明、协议、引用&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;最新动态&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024.1月底开源12B版本模型（待开放）&lt;/li&gt; &#xA; &lt;li&gt;2024.1.11 开源1T中文数据集&lt;/li&gt; &#xA; &lt;li&gt;2024.1.10 开源7B版本chat模型及其量化版本&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;模型介绍&lt;/h1&gt; &#xA;&lt;h3&gt;星辰语义大模型-TeleChat&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;星辰语义大模型TeleChat是由中电信人工智能科技有限公司研发训练的大语言模型，采用1.5万亿 Tokens中英文高质量语料进行训练。&lt;/li&gt; &#xA; &lt;li&gt;本次开源了对话模型&lt;strong&gt;TeleChat-7B-bot&lt;/strong&gt;，以及其&lt;code&gt;huggingface&lt;/code&gt;格式的权重文件。此外，我们还开源了7B模型的int8和int4量化版本。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;模型结构&lt;/h3&gt; &#xA;&lt;p&gt;我们采用标准的 &lt;code&gt;Decoder-only&lt;/code&gt; 结构设计了 &lt;strong&gt;TeleChat&lt;/strong&gt; 模型，并在模型维度做了如下的一些改进：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;位置编码&lt;/strong&gt;：我们使用 &lt;a href=&#34;https://arxiv.org/pdf/2104.09864.pdf&#34;&gt;Rotary Embedding&lt;/a&gt; 的位置编码方法，该方法将相对位置信息依赖集成到 self-attention 中，并且具有较好的位置外推性。Rotary Embedding还可以较好地与Flash-Attention v2 配合使用，将模型的训练速度提升约20%。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;激活函数&lt;/strong&gt;：我们使用 &lt;a href=&#34;https://arxiv.org/pdf/2002.05202.pdf&#34;&gt;SwiGLU&lt;/a&gt; 激活函数来替代GELU激活函数 , 为了减少计算量，将&lt;code&gt;ffn_hidden_size&lt;/code&gt;设置为小于原始SwiGLU中的4倍隐藏层大小。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;层标准化&lt;/strong&gt;: 基于 &lt;a href=&#34;https://arxiv.org/abs/1910.07467&#34;&gt;RMSNorm&lt;/a&gt; 的 Pre-Normalization。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;layer_num&lt;/th&gt; &#xA;   &lt;th&gt;hidden_size&lt;/th&gt; &#xA;   &lt;th&gt;ffn_hidden_size&lt;/th&gt; &#xA;   &lt;th&gt;head_num&lt;/th&gt; &#xA;   &lt;th&gt;是否使用embed-layernorm&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;12288&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;否&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12B&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;5120&lt;/td&gt; &#xA;   &lt;td&gt;12288&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;否&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;我们开源的TeleChat模型：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持deepspeed微调，开源了基于deepspeed的训练代码，支持Zero并行显存优化，同时集成了FlashAttention2&lt;/li&gt; &#xA; &lt;li&gt;多轮能力支持。开源了多轮数据构建方式，针对多轮模型训练集成了针对多轮的mask loss训练方式，更好的聚焦多轮答案，提升问答效果。&lt;/li&gt; &#xA; &lt;li&gt;外推能力提升。开源了8K训练版本模型，采用NTK-aware外推和attention scaling外推方式，可以外推到96K。&lt;/li&gt; &#xA; &lt;li&gt;具备较好的长文生成能力。在工作总结、工作计划、PPT大纲、申论、招标书、邮件、方案、周报、JD写作等长文写作任务上表现较好。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;本次发布版本和下载链接见下表&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;模型版本&lt;/th&gt; &#xA;   &lt;th&gt;下载链接&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B-FP16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tele-AI/Telechat-7B&#34;&gt;TeleChat-FP16&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B-int8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tele-AI/Telechat-7B-int8&#34;&gt;TeleChat-int8&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B-int4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tele-AI/Telechat-7B-int4&#34;&gt;TeleChat-int4&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;镜像下载&lt;/strong&gt; 为了便于大家快速上手，我们提供了可运行的环境镜像，下载地址：&lt;a href=&#34;https://cloud.189.cn/t/EbAriaQfa2mm&#34;&gt;镜像下载&lt;/a&gt; （访问码：2uik）&lt;/p&gt; &#xA;&lt;h1&gt;数据开源&lt;/h1&gt; &#xA;&lt;h3&gt;数据介绍&lt;/h3&gt; &#xA;&lt;p&gt;TeleChat-PTD 是由电信星辰大模型&lt;strong&gt;TeleChat&lt;/strong&gt;预训练语料中抽取出的的综合性大规模中文数据集。数据主要来源于网页、书籍、官方媒体等。 我们使用规则+模型的方式进行了相关的过滤，并对数据进行了相似性去重，尽可能地提取出高质量地数据。&lt;/p&gt; &#xA;&lt;p&gt;TeleChat-PTD 数据集大约公开了2.7亿条数据，数据由纯中文文本构成，原始大小约1TB,压缩后480G，共189个文件。数据集中已经去除了其它冗余信息。&lt;/p&gt; &#xA;&lt;h3&gt;数据格式&lt;/h3&gt; &#xA;&lt;p&gt;数据为jsonl格式，仅有一个字段data&lt;/p&gt; &#xA;&lt;p&gt;data: 单条处理后的预训练数据&lt;/p&gt; &#xA;&lt;h3&gt;数据清洗&lt;/h3&gt; &#xA;&lt;p&gt;数据清洗的工作流程主要是：规则筛选和清洗、去重、高质量数据筛选、数据安全处理这四个步骤。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;规则筛选主要是一些通用的规则和启发式规则，例如对字数长度的筛选等等;&lt;/li&gt; &#xA; &lt;li&gt;去重主要使用相似度去重来将过于相似重复的数据删除;&lt;/li&gt; &#xA; &lt;li&gt;高质量筛选主要使用了BERT、GPT2等模型对数据进行打分筛选出高质量数据;&lt;/li&gt; &#xA; &lt;li&gt;数据清洗主要是针对不良数据进行了识别和去除;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;数据下载&lt;/h3&gt; &#xA;&lt;p&gt;huggingface下载地址：&lt;a href=&#34;https://huggingface.co/datasets/Tele-AI/TeleChat-PTD&#34;&gt;数据下载&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;天翼云盘下载地址：&lt;a href=&#34;https://cloud.189.cn/t/ia2QbaVzYf6z&#34;&gt;数据下载&lt;/a&gt;（访问码：pkg8）&lt;/p&gt; &#xA;&lt;h1&gt;效果评测&lt;/h1&gt; &#xA;&lt;p&gt;TeleChat模型相比同规模模型在评测效果方面也有较好的表现，我们的评测集涵盖了包括MMLU、C-Eval、GAOKAO、AGIEval、CMMLU、 GSM8K、MATH、HumanEval、CHID等数据集，评测能力包括了自然语言理解、知识、数学计算和推理、代码生成等&lt;/p&gt; &#xA;&lt;h2&gt;评测集介绍&lt;/h2&gt; &#xA;&lt;h3&gt;通用能力&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;MMLU 数据集是一个全面的英文评测数据集，涵盖了 57 个学科，包括人文学科、社会科学、自然科学、初等数学、美国历史、计算机科学、法律等等。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CEVAL 数据集是一个全面的中文评估测试集，包括初中、高中、大学和专业难度级别的多项选择题，涵盖了 52 个不同的学科领域。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CMMLU 数据集同样是一个全面的中文评估测试集，涵盖了从基础学科到高级专业水平的67个主题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;AGIEval 数据集是一个专门为评估基础模型在难度较高的标准化考试（如大学入学考试、法学院入学考试、数学竞赛和律师资格考试）的语境中而设计的基准测试，包括中文试题和英文试题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;GAOKAO 数据集是一个基于中国高考题构建的语言模型能力测试集，包括 1781 道客观题和 1030 道主观题。我们只保留了客观题的评测结果。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;推理和代码能力&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;GSM8K 数据集包含了8.5K高质量的小学数学题，能够评估语言模型在数学推理能力上的表现，我们利用&lt;a href=&#34;https://github.com/openai/grade-school-math&#34;&gt;官方&lt;/a&gt;的评测方案在test集上进行了4-shot测试。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;MATH 数据集包含了12.5K具有挑战性的高中数学竞赛题，难度较大，对语言模型的推理能力要求较高，基于&lt;a href=&#34;https://github.com/hendrycks/math&#34;&gt;官方&lt;/a&gt;的评测方案，我们在test集上进行了4-shot测试。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;HumanEval 数据集是一个由openai提供的代码能力测试数据集，它由 164 个编程问题组成，要求根据给定的问题和代码模板，生成正确的代码片段，我们利用&lt;a href=&#34;https://github.com/openai/human-eval&#34;&gt;官方&lt;/a&gt;评测方案在test集上进行了zero-shot测试。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;语言理解能力&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;CSL 是一个中文论文摘要关键词匹配任务，需要模型能够识别中文学术摘要与其关键词之间的匹配情况。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CHID 是一个中文阅读理解任务，要求模型选择出最恰当的成语填补中文片段中的空缺处。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;EPRSTMT 是一个基于电子商务平台上的产品评论的二元情感分析任务。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;评测结果如下&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;C-Eval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CMMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AGIEval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GAOKAO&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GSM8K&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MATH&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;HumanEval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CSL&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CHID&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;EPRSTMT&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zero-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zero-shot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA2-7B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA2-13B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ChatGLM2-6B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ChatGLM3-6B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;InternLM-7B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan2-7B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan2-13B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Qwen-7B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Qwen-14B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TeleChat-7B-chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;说明：CMMLU、AGIEval、GAOKAO、CSL、CHID、EPRSTMT均基于&lt;a href=&#34;https://github.com/open-compass/OpenCompass/&#34;&gt;OpenCompass&lt;/a&gt;平台提供的评测方法进行评估，而对于对比模型，我们同时参考了官方汇报结果和OpenCompass结果。我们使用了自己的评测脚本评测MMLU与CEVAL榜单，具体方法见&lt;code&gt;evaluation/&lt;/code&gt;文件夹。&lt;/p&gt; &#xA;&lt;h1&gt;模型推理和部署&lt;/h1&gt; &#xA;&lt;h3&gt;模型推理&lt;/h3&gt; &#xA;&lt;p&gt;当前模型推理兼容了单卡和多卡推理，以及针对长文推理做了部分优化工作。具体推理操作请参考：&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/docs/tutorial.md&#34;&gt;&lt;strong&gt;tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;模型推理方法示范&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import os&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig&#xA;&amp;gt;&amp;gt;&amp;gt; os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#39;0&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(&#39;../models/7B&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; model = AutoModelForCausalLM.from_pretrained(&#39;../models/7B&#39;, trust_remote_code=True, device_map=&#34;auto&#34;, torch_dtype=torch.float16)&#xA;&amp;gt;&amp;gt;&amp;gt; generate_config = GenerationConfig.from_pretrained(&#39;../models/7B&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; question=&#34;生抽与老抽的区别？&#34;&#xA;&amp;gt;&amp;gt;&amp;gt; answer, history = model.chat(tokenizer = tokenizer, question=question, history=[], generation_config=generate_config, stream=False)&#xA;&amp;gt;&amp;gt;&amp;gt; print(answer)&#xA;生抽和老抽是两种不同的酱油，它们的区别如下：&#xA; &#xA;1. 原料不同：生抽是用大豆、小麦等谷物为原料制成的；而老抽则是用豆酱、面酱等发酵后的调味品为原料制成的。&#xA; &#xA;2. 制作工艺不同：生抽是通过将大豆浸泡在水中，然后经过蒸煮、发酵等过程制成的；而老抽则是在生抽的基础上加入一定比例的盐、糖、味精等调料，再进行发酵制成的。&#xA; &#xA;3. 口感和风味不同：生抽具有咸鲜的味道，口感比较清爽；而老抽则具有特殊的香味和味道，口感相对较重。&#xA; &#xA;总的来说，生抽和老抽都是酱油的不同种类，它们在原料、制作工艺和口感等方面都有所不同。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;模型部署&lt;/h3&gt; &#xA;&lt;p&gt;TeleChat目前提供了API、Web两种部署方式。目前仅提供简单的单卡单并发场景，用于演示和效果测试。基于参考快速上手手册：&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/docs/tutorial.md&#34;&gt;&lt;strong&gt;tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;API: 分为流式接口和json接口，支持传入推理参数&lt;/p&gt; &#xA;&lt;p&gt;Web: 支持流式生成、多轮对话&lt;/p&gt; &#xA;&lt;h3&gt;星辰语义大模型TeleChat能力展示&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;工作计划&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;用户：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;给出以下信息，请你写一篇完善的工作计划。&#xA;1. 分析市场竞争情况，包括主要竞争对手和他们的产品特点。&#xA;2. 制定产品开发路线图，包括关键里程碑和时间表。&#xA;3. 确定资源需求，包括人力、资金和技术支持。&#xA;4. 开展市场调研，以了解潜在客户的需求和反馈。&#xA;5. 制定市场推广策略，包括定价策略、促销活动和渠道选择。&#xA;请输出详细的项目计划。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;TeleChat：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;项目名称：市场竞争分析及产品开发路线图&#xA;&#xA;一、市场竞争分析&#xA;1.1 主要竞争对手分析：&#xA;   - 公司A：主要产品为XXX，具有较高的知名度和用户基础。&#xA;   - 公司B：主要产品为XXX，与公司A有一定的竞争关系。&#xA;   - 公司C：主要产品为XXX，在市场上有一定份额。&#xA;&#xA;1.2 产品特点分析：&#xA;   - 公司A的产品特点是XXX，具有独特的功能和性能优势。&#xA;   - 公司B的产品特点是XXX，具有良好的性价比和用户体验。&#xA;   - 公司C的产品特点是XXX，具有创新性和独特性。&#xA;&#xA;二、产品开发路线图&#xA;2.1 关键里程碑：&#xA;   - 完成市场调研，收集并分析目标客户的需求和偏好。&#xA;   - 设计并完成产品原型，验证其功能和性能是否符合预期。&#xA;   - 进行产品测试和优化，确保产品的质量和稳定性。&#xA;&#xA;2.2 时间表：&#xA;   - 关键里程碑的完成时间：XXXX年X月X日。&#xA;   - 产品原型的设计和测试完成时间：XXXX年X月X日。&#xA;   - 产品测试和优化完成时间：XXXX年X月X日。&#xA;   - 产品发布和上市时间：XXXX年X月X日。&#xA;&#xA;三、资源需求&#xA;3.1 人力资源：&#xA;   - 需要招聘具备相关专业背景和技能的人才，以满足产品研发和市场推广的需要。&#xA;   - 需要建立有效的团队合作机制，提高工作效率和产出。&#xA;&#xA;3.2 资金：&#xA;   - 需要投入足够的资金用于产品研发、市场推广和人力资源等方面。&#xA;   - 可以考虑申请政府或企业的科研基金、风险投资等资金来源。&#xA;&#xA;3.3 技术支持：&#xA;   - 需要建立健全的技术支持体系，包括技术咨询、技术培训和技术服务等。&#xA;   - 可以考虑与高校、研究机构等进行合作，获取外部技术支持和资源。&#xA;&#xA;四、市场调研&#xA;4.1 目标：&#xA;   - 了解潜在客户对产品的需求和期望，为产品开发和市场推广提供参考依据。&#xA;   - 发现潜在的市场机会和竞争威胁，为企业决策提供数据支持。&#xA;&#xA;4.2 方法：&#xA;   - 通过在线问卷调查、面对面访谈等方式，收集潜在客户的意见和反馈。&#xA;   - 分析收集到的数据，提取有价值的信息和洞察。&#xA;&#xA;五、市场推广策略&#xA;5.1 定价策略：&#xA;   - 根据产品成本、市场需求和竞争状况等因素，确定合理的定价策略。&#xA;   - 可以考虑采用差异化定价、价格补贴等策略来吸引目标客户。&#xA;&#xA;5.2 促销活动：&#xA;   - 定期组织产品发布会、促销活动等，提升产品的知名度和销售量。&#xA;   - 可以结合线上线下渠道，采取多种促销手段，如优惠券、折扣、满减等。&#xA;&#xA;5.3 渠道选择：&#xA;   - 根据产品特点和目标客户群体，选择合适的销售渠道，如直销、代理商、电商平台等。&#xA;   - 可以通过市场调研和分析，评估不同渠道的优劣势，并进行渠道选择和整合。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;周报撰写&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;用户：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;请你为一位算法工程师写一篇周报，主要涉及到的内容应包括：&#xA;1.上线新版模型，新增首页新闻推荐功能。&#xA;2.新版模型训练集，加入上周回流的badcase数据&#xA;3.经过线上数据回流评估，获得了95%的用户满意度。&#xA;4.计划下周调研新闻推荐方向的最新进展。&#xA;请确保使用简介、专业的语言进行周报书写。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;TeleChat:&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;周报&#xA;&#xA;日期：XXXX年XX月XX日-XXXX年XX月XX日&#xA;&#xA;本周工作总结：&#xA;&#xA;1. 上线新版模型，新增首页新闻推荐功能&#xA;   在本周，我们成功上线了新版模型，并添加了首页新闻推荐功能。通过该功能，我们可以根据用户的兴趣和偏好，为其提供更加个性化的新闻推荐服务。&#xA;&#xA;2. 新版模型训练集，加入上周回流的badcase数据&#xA;   为了进一步提升模型的准确性和鲁棒性，我们在新版模型的训练集中加入了上周回流的badcase数据。通过对这些数据的分析和处理，我们希望能够更好地识别和解决模型在实际应用中遇到的问题。&#xA;&#xA;3. 经过线上数据回流评估，获得了95%的用户满意度&#xA;   我们对新版模型进行了线上数据回流评估，结果显示用户满意度达到了95%。这表明我们的新版模型在满足用户需求方面取得了显著的进步，并且得到了用户的认可和喜爱。&#xA;&#xA;4. 计划下周调研新闻推荐方向的最新进展&#xA;   为了不断提升新闻推荐系统的性能和效果，我们计划在下周进行新闻推荐方向的最新进展的调研工作。通过对相关领域的研究和技术趋势的了解，我们希望能够及时掌握最新的研究成果，并将其应用于我们的新闻推荐系统中，以进一步提升系统的准确性和个性化程度。&#xA;&#xA;以上是本周的工作总结和下周的计划安排。感谢各位同事在本周的努力和支持，让我们一起继续努力，为公司的发展做出更大的贡献！&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;辅助编程&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;用户：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;请你编写一段java代码，需要使用springcloud feign框架，实现远程调用的功能&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;TeleChat:&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;以下是一个简单的示例代码，演示了如何使用Spring Cloud Feign框架进行远程调用：&lt;/p&gt; &#xA; &lt;p&gt;首先，创建一个Feign客户端类，继承自FeignClient：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;@FeignClient(value = &#34;my-service&#34;) // 替换为实际的Service名称&#xA;public interface MyServiceClient {&#xA;    @RequestMapping(value = &#34;/my-path&#34;, method = RequestMethod.GET)&#xA;    String getMyPath();&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;然后，创建一个控制器类，继承自FeignClient：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;@RestController&#xA;public class MyController {&#xA;    @Autowired&#xA;    private MyServiceClient myServiceClient;&#xA;&#xA;    @RequestMapping(&#34;/my-path&#34;)&#xA;    public String myMethod(@RequestParam(value = &#34;name&#34;, required = true) String name) {&#xA;        return myServiceClient.getMyPath() + &#34; - &#34; + name;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;最后，创建一个服务类，继承自FeignClient：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;@Service&#xA;public class MyService {&#xA;    @Autowired&#xA;    private MyServiceClient myServiceClient;&#xA;&#xA;    public void myMethod() {&#xA;        myServiceClient.getMyPath();&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;现在，你可以通过Spring Cloud的Feign客户端类来发起远程调用了。例如：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;@RestController&#xA;public class MyController {&#xA;    @Autowired&#xA;    private MyServiceClient myServiceClient;&#xA;&#xA;    @RequestMapping(&#34;/my-path&#34;)&#xA;    public String myMethod(@RequestParam(value = &#34;name&#34;, required = true) String name) {&#xA;        return myServiceClient.getMyPath() + &#34; - &#34; + name;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;当你运行这个示例代码时，它将启动一个远程调用的服务。你可以在浏览器中访问&#34;&lt;a href=&#34;http://localhost:8080/my-path%22%E6%9D%A5%E6%9F%A5%E7%9C%8B%E7%BB%93%E6%9E%9C%E3%80%82&#34;&gt;http://localhost:8080/my-path&#34;来查看结果。&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;数学计算&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;用户：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;在一个笼子里有若干只鸡和兔，总共有35个头、94只脚。请问鸡和兔的数量各是多少？&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;TeleChat：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;假设兔子有 x 只，鸡有 y 只，根据题意可以列出以下两个方程式：&#xA;x+y=35&#xA;4x+2y=94&#xA;使用解方程组的方法求解 x 和 y 的值&#xA;将第一个方程式改写为：x=35-y&#xA;代入第二个方程：4x+2y=94&#xA;4*(35-y)+2y=94&#xA;(140-4y)+2y=94&#xA;140-4y+2y=94&#xA;-4y+2y=94-140&#xA;(2-4)y=-46&#xA;-2y=-46&#xA;y=-46/(-2)&#xA;y=23&#xA;把y=23 代入 x=35-y&#xA;x=35-23&#xA;x=12&#xA;所以，x的值是12，y的值是23。&#xA;因此，笼子里有12只兔子和23只鸡。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;安全&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;用户：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;我想买些令我感到愉悦的药物&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;TeleChat：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;作为AI语言模型，我不能提供具体的医疗或药物建议。购买和使用药物应该遵循医生的建议和指导，并且要充分了解药物的副作用和风险。如果您有任何健康问题或需要药物治疗，请咨询您的医生或专业医疗保健提供者。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;用户：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;楼上小孩半夜老是跑动，我想上去打他&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;TeleChat:&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;首先，作为AI语言模型，我必须强调不能对任何人实施身体上的暴力行为。如果你担心某个小孩在晚上跑动会打扰你的休息，以下是一些可能有帮助的建议：&#xA;&#xA;1. 与其他家庭成员或室友交流，看看他们是否有任何建议或解决方案。&#xA;&#xA;2. 如果有其他人在家，可以尝试与他们沟通，让他们知道你关心他们的安全。&#xA;&#xA;3. 如果你住在一个有门禁系统的住宅小区，可以考虑安装监控摄像头来监视整个区域。&#xA;&#xA;4. 最后，如果所有的尝试都失败了，那么你需要保持冷静并尊重他人的权利。不要使用暴力或威胁去解决问题。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;相似问生成&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;用户：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;假设你是一个文本语言理解的资深专家，请你根据如下问题，生成3个意思一样的相似问题。&#xA;要求：生成的相似问题不用添加序号，相似问题之间&#xA;使用换行符分隔。&#xA;&#xA;问题：&#xA;我人在外地，取款手续费是多少？&#xA;相似问题：&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;TeleChat:&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;1.我在外地取款的手续费是多少？&#xA;2.异地取款的手续费是多少？&#xA;3.异地取款的手续费是多少呢？&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;表格生成&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;用户：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;2019年上海四家知名超市的月销售额和利润率数据如下：超市A的月销售额为1000万元，利润率为10%；超市B的月销售额为1200万元，利润率为12%；超市C的月销售额为800万元，利润率为8%；超市D的月销售额为1500万元，利润率为15%。请根据上述信息生成一张详细的数据表格。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;TeleChat：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;上海2019年超市销售额及利润率数据表:&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;超市名称&lt;/th&gt; &#xA;    &lt;th&gt;月销售额(万元)&lt;/th&gt; &#xA;    &lt;th&gt;利润率&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;超市A&lt;/td&gt; &#xA;    &lt;td&gt;1000&lt;/td&gt; &#xA;    &lt;td&gt;10%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;超市B&lt;/td&gt; &#xA;    &lt;td&gt;1200&lt;/td&gt; &#xA;    &lt;td&gt;12%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;超市C&lt;/td&gt; &#xA;    &lt;td&gt;800&lt;/td&gt; &#xA;    &lt;td&gt;8%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;超市D&lt;/td&gt; &#xA;    &lt;td&gt;1500&lt;/td&gt; &#xA;    &lt;td&gt;15%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;阅读理解&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;用户：&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;请对下面文本进行阅读理解并回答问题：李清照的《醉花阴》这首词是为了表达什么情感而写的？&#xA;李清照，宋代女词人，号易安居士，齐州章丘人。出生于书香门第，早期生活优裕，其父李格非藏书甚富。她小时候就在良好的家庭环境中打下文学基础，出嫁后与丈夫赵明诚共同致力于书画金石的搜集整理。金兵入据中原时，流寓南方，境遇孤苦。绍兴二十五年去世。&#xA;李清照所作词，前期多写悠闲生活，后期悲叹身世，情调感伤。艺术上，善用白描手法，自辟途径，语言清丽。论词强调协律，崇尚典雅，提出词“别是一家”之说，反对以作诗文之法作词。能诗，留存不多，部分篇章感时咏史，情辞慷慨，与其词风不同。作品有《李易安集》《易安居士文集》《易安词》，已散佚。后人辑有《漱玉集》《漱玉词》。今有《李清照集》辑本。&#xA;十八岁的李清照与赵明诚结婚，两人情投意合，互相砥砺，把整个身心都放在文学艺术的深造和金石文字的收集研究上。在重阳节，李清照为了抒发自己对丈夫的思念，作了那首著名的《醉花阴》。金兵的横行肆虐激起李清照强烈的爱国情感，她积极主张北伐收复中原，然而南宋王朝的腐朽无能和偏安一隅，使她的希望彻底成为了幻影。在那种离乱的年代，悲愤欲绝的她写下了一首雄浑奔放的《夏日绝句》。&#xA;李清照的一生充满了坎坷与磨难，她的作品也因此充满了深深的感伤与忧虑。然而，她的才华和坚韧的精神却永远值得我们学习和敬仰。她不仅是一位伟大的女词人，更是一位爱国英雄，她的作品和人生都为我们提供了宝贵的历史见证和人文思考。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;TeleChat:&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;李清照的《醉花阴》这首词是为了表达对丈夫赵明诚的思念之情。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;模型微调&lt;/h1&gt; &#xA;&lt;p&gt;以下是一些性能测试供参考。&lt;/p&gt; &#xA;&lt;p&gt;（1）deepspeed-lora最小消耗V100和A100卡数，最大训练长度，训练速度（ samples/s）&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;模型大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;NVIDIA卡型号&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;gpu数&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;最长训练长度&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;训练速度&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数设置&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V100-32G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;单卡&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5120&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.084&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;flash-attn关闭，zero-3，offload，gradient-checkpointing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A100-40G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;单卡&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18432&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.134&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;flash-attn开启，zero-3，offload，gradient-checkpointing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;（2）全参微调deepspeed版本，单机8卡V100和A100，最大训练长度，训练速度（ samples/s）&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;模型大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;NVIDIA卡型号&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;最长训练长度&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;训练速度&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数设置&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;单机8卡V100-32G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5120&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.529&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;flash-attn关闭，zero-3，offload，gradient-checkpointing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;单机8卡A100-40G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18432&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.696&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;flash-attn开启，zero-3，offload，gradient-checkpointing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;（3）全参微调deepspeed版本，单机8卡A100，2048训练长度，训练速度（ samples/s）&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;模型大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;NVIDIA卡型号&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;训练长度&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;训练速度&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数设置&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;单机8卡A100-40G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.866&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;flash-attn开启，zero-3，gradient-checkpointing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;单机训练&lt;/h2&gt; &#xA;&lt;p&gt;以下是TeleChat-7B单机微调的样例脚本。其中训练数据为1000条单轮样例数据，为了测试使用，不保证效果。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;deepspeed --master_port 29500 main.py \&#xA;   --data_path ../../example_datas/single_turn_example.jsonl  \&#xA;   --model_name_or_path ../../models/7B \&#xA;   --with_loss_mask \&#xA;   --data_output_path /tmp/data_files/ \&#xA;   --per_device_train_batch_size 1 \&#xA;   --max_seq_len 2048 \&#xA;   --learning_rate 2e-5 \&#xA;   --weight_decay 0. \&#xA;   --num_train_epochs 1 \&#xA;   --gradient_accumulation_steps 8 \&#xA;   --lr_scheduler_type cosine \&#xA;   --gradient_checkpointing \&#xA;   --warmup_proportion 0.1 \&#xA;   --seed 1233 \&#xA;   --zero_stage 3 \&#xA;   --deepspeed \&#xA;   --output_dir output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;多机训练&lt;/h2&gt; &#xA;&lt;p&gt;多机训练需要给出hostfile，如deepspeed-telechat/sft/my_hostfile所示。脚本中需指定hostfile的路径&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;deepspeed --master_port 29500 --hostfile=my_hostfile main.py \&#xA;   --data_path ../../example_datas/single_turn_example.jsonl  \&#xA;   --model_name_or_path ../../models/7B \&#xA;   --with_loss_mask \&#xA;   --data_output_path /tmp/data_files/ \&#xA;   --per_device_train_batch_size 1 \&#xA;   --max_seq_len 2048 \&#xA;   --learning_rate 2e-5 \&#xA;   --weight_decay 0. \&#xA;   --num_train_epochs 1 \&#xA;   --gradient_accumulation_steps 8 \&#xA;   --lr_scheduler_type cosine \&#xA;   --gradient_checkpointing \&#xA;   --warmup_proportion 0.1 \&#xA;   --seed 1233 \&#xA;   --zero_stage 3 \&#xA;   --deepspeed \&#xA;   --output_dir output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;具体可以参考：&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/docs/tutorial.md&#34;&gt;&lt;strong&gt;tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;分词器&lt;/h2&gt; &#xA;&lt;p&gt;TeleChat的分词算法是BBPE算法，该算法是字节级实现的分词算法，任意Unicode字符都可以被表示。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TeleChat 的分词器词表大小为160256，是中英双语的词表。&lt;/li&gt; &#xA; &lt;li&gt;BBPE算法的实现工具为huggingface实现的tokenizers开源工具，该工具使用RUST重写，能够较快的进行分词&lt;/li&gt; &#xA; &lt;li&gt;我们保留了特殊的token&lt;/li&gt; &#xA; &lt;li&gt;&amp;lt;_end&amp;gt; 标识结束&lt;/li&gt; &#xA; &lt;li&gt;&amp;lt;_user&amp;gt; 标识用户问题&lt;/li&gt; &#xA; &lt;li&gt;&amp;lt;_bot&amp;gt; 标识模型回答&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;模型量化&lt;/h1&gt; &#xA;&lt;p&gt;我们使用基于 &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ&#34;&gt;AutoGPTQ&lt;/a&gt; 的量化方案对自研星辰语义大模型TeleChat做量化，提供Int8和Int4的量化模型。&lt;/p&gt; &#xA;&lt;p&gt;具体量化操作请参考：&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/docs/tutorial.md&#34;&gt;&lt;strong&gt;tutorial&lt;/strong&gt;&lt;/a&gt;，以下是离线量化和量化后推理的样例脚本&lt;/p&gt; &#xA;&lt;h2&gt;8bit离线量化&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoTokenizer&#xA;&amp;gt;&amp;gt;&amp;gt; from auto_gptq import BaseQuantizeConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from modeling_telechat_gptq import TelechatGPTQForCausalLM&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer_path = &#39;../models/7B&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; pretrained_model_dir = &#39;../models/7B&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; quantized_model_dir = &#39;../models/7B_8bit&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)&#xA;&amp;gt;&amp;gt;&amp;gt; calibration_text = [&#34;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&#34;]&#xA;&amp;gt;&amp;gt;&amp;gt; examples = [tokenizer(_) for _ in calibration_text]&#xA;&amp;gt;&amp;gt;&amp;gt; quantize_config = BaseQuantizeConfig( bits=8,  group_size=128,  desc_act=False )&#xA;&amp;gt;&amp;gt;&amp;gt; model = TelechatGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config,trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model.quantize(examples)&#xA;&amp;gt;&amp;gt;&amp;gt; model.save_quantized(quantized_model_dir)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;8bit量化模型推理&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoTokenizer, GenerationConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from modeling_telechat_gptq import TelechatGPTQForCausalLM&#xA;&amp;gt;&amp;gt;&amp;gt; PATH = &#39;../models/7B_8bit&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(PATH, trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model = TelechatGPTQForCausalLM.from_quantized(PATH, device=&#34;cuda:0&#34;, inject_fused_mlp=False, inject_fused_attention=False, trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; generate_config = GenerationConfig.from_pretrained(PATH)&#xA;&amp;gt;&amp;gt;&amp;gt; model.eval()&#xA;&amp;gt;&amp;gt;&amp;gt; question = &#34;生抽与老抽的区别？&#34;&#xA;&amp;gt;&amp;gt;&amp;gt; answer, history = model.chat(tokenizer=tokenizer, question=question, history=[], generation_config=generate_config, stream=False)&#xA;&amp;gt;&amp;gt;&amp;gt; print(&#34;回答:&#34;, answer)&#xA;回答: 生抽和老抽是两种不同的酱油，它们的区别如下：&#xA;&#xA;1. 原料不同：生抽是用大豆、面粉等为原料制成的；而老抽则是用豆豉、盐等为原料制成的。&#xA;&#xA;2. 制作工艺不同：生抽是通过将大豆浸泡在水中，然后经过发酵、蒸煮等过程制成的；而老抽则是在生抽的基础上进行进一步的加工和处理，如加入盐、糖、味精等调料。&#xA;&#xA;3. 口感和风味不同：生抽的口感相对较咸，适合用于烹调肉类、海鲜等；而老抽的风味相对较重，适合用于烹调红烧肉、酱爆鸡丁等菜品。&#xA;&#xA;总的来说，生抽和老抽都是常见的酱油品种，它们在原料、制作工艺和口感等方面都有所不同。选择使用哪种酱油，可以根据个人口味和菜品需求来决定。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4bit离线量化&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoTokenizer&#xA;&amp;gt;&amp;gt;&amp;gt; from auto_gptq import BaseQuantizeConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from modeling_telechat_gptq import TelechatGPTQForCausalLM&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer_path = &#39;../models/7B&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; pretrained_model_dir = &#39;../models/7B&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; quantized_model_dir = &#39;../models/7B_4bit&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)&#xA;&amp;gt;&amp;gt;&amp;gt; calibration_text = [&#34;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&#34;]&#xA;&amp;gt;&amp;gt;&amp;gt; examples = [tokenizer(_) for _ in calibration_text]&#xA;&amp;gt;&amp;gt;&amp;gt; quantize_config = BaseQuantizeConfig( bits=4, group_size=128, desc_act=False )&#xA;&amp;gt;&amp;gt;&amp;gt; model = TelechatGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config,trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model.quantize(examples)&#xA;&amp;gt;&amp;gt;&amp;gt; model.save_quantized(quantized_model_dir)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4bit量化模型推理&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoTokenizer, GenerationConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from modeling_telechat_gptq import TelechatGPTQForCausalLM&#xA;&amp;gt;&amp;gt;&amp;gt; PATH = &#39;../models/7B_4bit&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(PATH, trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model = TelechatGPTQForCausalLM.from_quantized(PATH, device=&#34;cuda:0&#34;, inject_fused_mlp=False, inject_fused_attention=False, trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; generate_config = GenerationConfig.from_pretrained(PATH)&#xA;&amp;gt;&amp;gt;&amp;gt; model.eval()&#xA;&amp;gt;&amp;gt;&amp;gt; question = &#34;生抽与老抽的区别？&#34;&#xA;&amp;gt;&amp;gt;&amp;gt; answer, history = model.chat(tokenizer=tokenizer, question=question, history=[], generation_config=generate_config, stream=False)&#xA;&amp;gt;&amp;gt;&amp;gt; print(&#34;回答:&#34;, answer)&#xA;回答: 生抽和老抽是两种不同的酱油，它们的区别主要体现在以下几个方面：&#xA;&#xA;1. 原料不同：生抽是用大豆、小麦等制成的，而老抽则是用豆豉、盐等制成的。&#xA;&#xA;2. 发酵方式不同：生抽是通过将大豆或小麦浸泡在水中，然后进行发酵制成的；而老抽则是在制作过程中直接将大豆或小麦炒熟后使用。&#xA;&#xA;3. 味道不同：生抽的口感比较鲜美，有咸味和甜味；老抽的味道相对较重，有咸味和苦味。&#xA;&#xA;4. 用途不同：生抽主要用于调味酱料、腌制肉类等；老抽则主要用于烹调菜肴、焖煮食材等。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;国产化适配&lt;/h1&gt; &#xA;&lt;h3&gt;昇腾 Atlas 300I Pro 推理卡：推理适配&lt;/h3&gt; &#xA;&lt;p&gt;当前星辰语义大模型TeleChat已支持昇腾 Atlas 300I Pro 推理卡。具备int8量化能力。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;精度方面，int8量化精度对齐A10；&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;性能方面，具体对比如下：&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;输入输出信息&lt;/th&gt; &#xA;     &lt;th&gt;NPU (tokens/s)&lt;/th&gt; &#xA;     &lt;th&gt;GPU (tokens/s)&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;输入100输出100&lt;/td&gt; &#xA;     &lt;td&gt;15&lt;/td&gt; &#xA;     &lt;td&gt;21&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;输入1000输出100&lt;/td&gt; &#xA;     &lt;td&gt;13&lt;/td&gt; &#xA;     &lt;td&gt;24&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;输入2000输出100&lt;/td&gt; &#xA;     &lt;td&gt;11&lt;/td&gt; &#xA;     &lt;td&gt;19&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;25组case平均&lt;/td&gt; &#xA;     &lt;td&gt;13&lt;/td&gt; &#xA;     &lt;td&gt;18&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Telechat支持基于昇腾Atlas 300I Pro进行推理并且具备int8量化能力，用户所需的推理部署指导、推理镜像下载等、已发布：&lt;a href=&#34;https://gitee.com/ascend/ModelLink/tree/master/speed_infer/pytorch/examples/telechat&#34;&gt;TeleChat-7B&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;昇腾Atlas 800T A2训练服务器+昇思MindSpore框架: 训练、推理适配&lt;/h3&gt; &#xA;&lt;p&gt;当前星辰语义大模型TeleChat已经支持昇腾Atlas 800T A2训练服务器，可基于昇思MindSpore框架进行模型训练和推理。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;效果方面，模型训练效果对齐A100，loss基本吻合；&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;性能方面，具体对比如下：&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;NAME&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;performance(samples/s)&lt;/th&gt; &#xA;     &lt;th&gt;Epochs&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;AMP_Type&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;8p-GPU(A100-40G)&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;8.86&lt;/td&gt; &#xA;     &lt;td&gt;5&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;8p-NPU&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;7.98&lt;/td&gt; &#xA;     &lt;td&gt;5&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;O2&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;p&gt;说明：BatchSize/per-GPU=1，zero-stage=3， seq_length=2048， gradient_accumulation_steps：4&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;TeleChat支持昇腾Atlas 800T A2训练服务器，可基于昇思MindSpore框架进行模型训练，训练所需的modeling、README、脚本已发布：&lt;a href=&#34;https://gitee.com/mindspore/mindformers/tree/dev/research/telechat&#34;&gt;TeleChat-7B-MindSpore&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;昇腾Atlas 800T A2训练服务器+PyTorch框架: 训练、推理适配&lt;/h3&gt; &#xA;&lt;p&gt;当前星辰语义大模型TeleChat已经支持昇腾Atlas 800T A2训练服务器，可基于PyTorch 框架进行模型训练和推理。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;效果方面，模型训练效果对齐A100，loss基本吻合；&lt;/p&gt; &lt;img src=&#34;./images/910B-pytorch训练loss对比.png&#34; width=&#34;300&#34; height=&#34;300&#34; alt=&#34;PyTorch训练loss对比.png&#34;&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;性能方面，具体对比如下：&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;NAME&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;performance(samples/s)&lt;/th&gt; &#xA;     &lt;th&gt;Epochs&lt;/th&gt; &#xA;     &lt;th align=&#34;right&#34;&gt;AMP_Type&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;8p-GPU(A100-40G)&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;10&lt;/td&gt; &#xA;     &lt;td&gt;5&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;8p-NPU&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;8.99&lt;/td&gt; &#xA;     &lt;td&gt;5&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;O2&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;p&gt;说明：BatchSize/per-GPU=2，zero-stage=3，seq_length=2048， gradient_accumulation_steps：2&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;TeleChat支持昇腾Atlas 800T A2训练服务器，可基于PyTorch框架进行模型训练，训练所需的modeling、README、脚本已发布：&lt;a href=&#34;https://gitee.com/ascend/ModelZoo-PyTorch/tree/master/PyTorch/contrib/nlp/Telechat&#34;&gt;TeleChat-7B-PyTorch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;声明、协议、引用&lt;/h1&gt; &#xA;&lt;h3&gt;声明&lt;/h3&gt; &#xA;&lt;p&gt;我们在此声明，不要使用TeleChat模型及其衍生模型进行任何危害国家社会安全或违法的活动。同时，我们也要求使用者不要将TeleChat模型用于没有安全审查和备案的互联网服务。我们希望所有使用者遵守上述原则，确保科技发展在合法合规的环境下进行。&lt;/p&gt; &#xA;&lt;p&gt;我们已经尽我们所能，来确保模型训练过程中使用的数据的合规性。然而，尽管我们已经做出了巨大的努力，但由于模型和数据的复杂性，仍有可能存在一些无法预见的问题。因此，如果由于使用TeleChat开源模型而导致的任何问题，包括但不限于数据安全问题、公共舆论风险，或模型被误导、滥用、传播或不当利用所带来的任何风险和问题，我们将不承担任何责任。&lt;/p&gt; &#xA;&lt;h3&gt;协议&lt;/h3&gt; &#xA;&lt;p&gt;社区使用 TeleChat 模型需要遵循《&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/Telechat/master/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&#34;&gt;TeleChat模型社区许可协议&lt;/a&gt;》。TeleChat模型支持商业用途，如果您计划将 TeleChat 模型或其衍生品用于商业目的，您需要通过以下联系邮箱 &lt;a href=&#34;mailto:tele_ai@chinatelecom.cn&#34;&gt;tele_ai@chinatelecom.cn&lt;/a&gt;，提交《TeleChat模型社区许可协议》要求的申请材料。审核通过后，将特此授予您一个非排他性、全球性、不可转让、不可再许可、可撤销的商用版权许可。&lt;/p&gt; &#xA;&lt;h3&gt;引用&lt;/h3&gt; &#xA;&lt;p&gt;如需引用我们的工作，请使用如下 reference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{wang2024telechat,&#xA;      title={TeleChat Technical Report}, &#xA;      author={Zihan Wang and Xinzhang Liu and Shixuan Liu and Yitong Yao and Yuyao Huang and Zhongjiang He and Xuelong Li and Yongxiang Li and Zhonghao Che and Zhaoxi Zhang and Yan Wang and Xin Wang and Luwen Pu and Huihan Xu and Ruiyu Fang and Yu Zhao and Jie Zhang and Xiaomeng Huang and Zhilong Lu and Jiaxin Peng and Wenjun Zheng and Shiquan Wang and Bingkai Yang and Xuewei he and Zhuoru Jiang and Qiyi Xie and Yanhan Zhang and Zhongqiu Li and Lingling Shi and Weiwei Fu and Yin Zhang and Zilu Huang and Sishi Xiong and Yuxiang Zhang and Chao Wang and Shuangyong Song},&#xA;      year={2024},&#xA;      eprint={2401.03804},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>EntySec/SeaShell</title>
    <updated>2024-01-14T01:43:14Z</updated>
    <id>tag:github.com,2024-01-14:/EntySec/SeaShell</id>
    <link href="https://github.com/EntySec/SeaShell" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SeaShell Framework is an iOS post-exploitation framework that enables you to access the device remotely, control it and extract sensitive information.&lt;/p&gt;&lt;hr&gt;&lt;h3 align=&#34;left&#34;&gt; &lt;img src=&#34;https://github.com/EntySec/SeaShell/raw/main/seashell/data/logo.png&#34; alt=&#34;logo&#34; height=&#34;250px&#34;&gt; &lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://entysec.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/developer-EntySec-blue.svg?sanitize=true&#34; alt=&#34;Developer&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/EntySec/SeaShell&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-Python-blue.svg?sanitize=true&#34; alt=&#34;Language&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/EntySec/SeaShell/forks&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/EntySec/SeaShell?style=flat&amp;amp;color=green&#34; alt=&#34;Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/EntySec/SeaShell/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/EntySec/SeaShell?style=flat&amp;amp;color=yellow&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codefactor.io/repository/github/EntySec/SeaShell&#34;&gt;&lt;img src=&#34;https://www.codefactor.io/repository/github/EntySec/SeaShell/badge&#34; alt=&#34;CodeFactor&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://blog.entysec.com/2023-12-31-seashell-ios-malware/&#34;&gt;SeaShell Framework&lt;/a&gt; is an iOS post-exploitation framework that enables you to access the device remotely, control it and extract sensitive information.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;IPA generator&lt;/strong&gt; - All you need to do is generate an IPA file and install it on a target&#39;s device via &lt;a href=&#34;https://trollstore.app/&#34;&gt;TrollStore&lt;/a&gt; or other IPA installer that bypasses CoreTrust. After app was installed, a target simply need to run an app single time (he may close application completely after this).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Powerful Implant&lt;/strong&gt; - SeaShell Framework uses the advanced and powerful payload with lots of features. It is called &lt;a href=&#34;https://github.com/EntySec/Pwny&#34;&gt;Pwny&lt;/a&gt;. You can extend it by adding your own post-exploitation modules or plugins.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Basic Set&lt;/strong&gt; - SeaShell Framework comes with basic set of post-exploitation modules that may exfiltrate following user data: SMS, VoiceMail, Safari history and much more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Encrypted communication&lt;/strong&gt; - Communication between device and SeaShell is encrypted using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Transport_Layer_Security&#34;&gt;TLS 1.3&lt;/a&gt; encryption by default.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Regular updates&lt;/strong&gt; - SeaShell Framework is being actively updated, so don&#39;t hesitate and leave your &lt;a href=&#34;https://github.com/EntySec/SeaShell/issues/new?assignees=&amp;amp;labels=&amp;amp;projects=&amp;amp;template=feature_request.md&amp;amp;title=&#34;&gt;feature request&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install SeaShell Framework you just need to type this command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install git+https://github.com/EntySec/SeaShell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After this SeaShell can be started with &lt;code&gt;seashell&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;h2&gt;Updating&lt;/h2&gt; &#xA;&lt;p&gt;To update SeaShell and get new commands run this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install --force-reinstall git+https://github.com/EntySec/SeaShell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Generating IPA&lt;/h3&gt; &#xA;&lt;p&gt;Simply generate custom IPA file or patch existing one and install it on target&#39;s iPhone or iPad via &lt;a href=&#34;https://trollstore.app/&#34;&gt;TrollStore&lt;/a&gt; or other IPA installer that bypasses CoreTrust.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;70%&#34; src=&#34;https://raw.githubusercontent.com/EntySec/SeaShell/main/seashell/data/preview/ipa.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Starting listener&lt;/h3&gt; &#xA;&lt;p&gt;Then you will need to start a listener on a host and port you added to your IPA. Once the installed application opens, you will receive a connection.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;70%&#34; src=&#34;https://raw.githubusercontent.com/EntySec/SeaShell/main/seashell/data/preview/listen.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Accessing device&lt;/h3&gt; &#xA;&lt;p&gt;Once you have received the connection, you will be able to communicate with the session through a &lt;a href=&#34;https://github.com/EntySec/Pwny&#34;&gt;Pwny&lt;/a&gt; interactive shell. Use &lt;code&gt;devices -i &amp;lt;id&amp;gt;&lt;/code&gt; to interact and &lt;code&gt;help&lt;/code&gt; to view list of all available commands. You can even extract Safari history like in the example below.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;70%&#34; src=&#34;https://raw.githubusercontent.com/EntySec/SeaShell/main/seashell/data/preview/safari.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Covering them All&lt;/h2&gt; &#xA;&lt;p&gt;Wide range of iOS versions are supported, since all of them are vulnerable to CoreTrust bug. They can be iOS 14, 15, 16 or early 17.&lt;/p&gt; &#xA;&lt;h2&gt;Endless Capabilities&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/EntySec/Pwny&#34;&gt;Pwny&lt;/a&gt; is a powerful implant with plenty of features including evasion, dynamic extensions and much more. It is embedded into the second phase of SeaShell Framework attack. These are all phases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;1.&lt;/strong&gt; IPA file installed and opened.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2.&lt;/strong&gt; Pwny is loaded through &lt;code&gt;posix_spawn()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;3.&lt;/strong&gt; Connection established and Pwny is ready to receive commands.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Issues and Bugs&lt;/h2&gt; &#xA;&lt;p&gt;SeaShell was just released and is in &lt;strong&gt;BETA&lt;/strong&gt; stage for now. If you find a bug or some function that does not work we will be glad if you immediately submit an issue describing a problem. The more details the issue contains the faster we will be able to fix it.&lt;/p&gt; &#xA;&lt;h2&gt;Legal Use&lt;/h2&gt; &#xA;&lt;p&gt;Note that the code and methods provided in this repository must not be used for malicious purposes and should only be used for testing and experimenting with devices &lt;strong&gt;you own&lt;/strong&gt;. Please consider out &lt;a href=&#34;https://github.com/EntySec/SeaShell/raw/main/TERMS_OF_SERVICE.md&#34;&gt;Terms of Service&lt;/a&gt; before using the tool.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>datamllab/LongLM</title>
    <updated>2024-01-14T01:43:14Z</updated>
    <id>tag:github.com,2024-01-14:/datamllab/LongLM</id>
    <link href="https://github.com/datamllab/LongLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning&lt;/h1&gt; &#xA;&lt;p&gt;Implementation of the proposed SelfExtend in &lt;a href=&#34;https://arxiv.org/pdf/2401.01325.pdf&#34;&gt;LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning&lt;/a&gt;. If you find our method useful, please kindly cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{jin2024llm,&#xA;      title={LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning}, &#xA;      author={Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu},&#xA;      year={2024},&#xA;      eprint={2401.01325},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Updates:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[01/11/2024]: We&#39;ve tested the implementation for phi-2. It works. You may find some results on this &lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/194mmki/selfextend_works_for_phi2_now_looks_good/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&#34;&gt;Reddit post&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[01/08/2024]: Add third-party implementations section&lt;/li&gt; &#xA; &lt;li&gt;[01/07/2024]: Add Implementation for Mistral&lt;/li&gt; &#xA; &lt;li&gt;[01/05/2024]: Our proposed method is discussed on this &lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/s/IFOnL7yGNK&#34;&gt;Reddit post&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;1. Overview&lt;/h2&gt; &#xA;&lt;p&gt;This work elicits LLMs&#39; inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs&#39; context window by themselves to fully utilize the inherent ability. We propose Self-Extend to stimulate LLMs&#39; long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model&#39;s self-attention, which means the proposed does not require any training.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;600&#34; src=&#34;https://raw.githubusercontent.com/datamllab/LongLM/master/img/self_ext.jpg&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;2. How to Use SelfExtend&lt;/h2&gt; &#xA;&lt;h3&gt;2.1 Setup&lt;/h3&gt; &#xA;&lt;p&gt;For current Llama Implementation, the python packages used are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;transformers==4.32.0 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, the KV cache structure was changed after version 4.36.0 in the transformers package. We will updata it to 4.36 in the near future. You can modify the implementation by yourself if you use transformers&amp;gt;=4.36.0&lt;/p&gt; &#xA;&lt;p&gt;For Mistral Implementation, the python packages used are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;transformers==4.36.2 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Mistral is similar to Llama, this implementation can be a good example about how to implement Self-Extend with transformers&amp;gt;=4.36.0&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repository to your machine and copy your modeling files into the cloned repo directory.&lt;/p&gt; &#xA;&lt;h3&gt;2.2 Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import llama_self_extend_patch as LlamaSE&#xA;from modify_utils import modify_method_of_instance&#xA;from functools import partial&#xA;&#xA;# Load your model, e.g., loaded_model = AutoModelForCausalLM.from_pretrained(model_path) &#xA;&#xA;# group_size_1 is group_window, group_size_2 is neighbor_window&#xA;self_extend_forward = partial(LlamaSE.self_extend_forward, group_size_1=4, group_size_2=1024)&#xA;modify_method_of_instance(loaded_model, &#34;LlamaAttention&#34;, &#34;forward&#34;, self_extend_forward)&#xA;&#xA;# Inference, e.g., loaded_model.generate(...)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mistral_self_extend_patch as MistralSE&#xA;from modify_utils import modify_method_of_instance&#xA;from functools import partial&#xA;&#xA;# Load your model, e.g., loaded_model = AutoModelForCausalLM.from_pretrained(model_path) &#xA;&#xA;# group_size_1 is group_window, group_size_2 is neighbor_window&#xA;self_extend_forward = partial(MistralSE.self_extend_forward, group_size_1=4, group_size_2=1024)&#xA;modify_method_of_instance(loaded_model, &#34;MistralAttention&#34;, &#34;forward&#34;, self_extend_forward)&#xA;&#xA;# Inference, e.g., loaded_model.generate(...)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.3 Passkey Example&lt;/h3&gt; &#xA;&lt;p&gt;To execute a demonstration of SelfExtend on the Passkey Retrivale task, you can use the command below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python llama_example.py # llama&#xA;&#xA;python mistral_example.py # mistra&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By running the command, you will have the following results:&lt;/p&gt; &#xA;&lt;p&gt;For Llama&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;-----------------------------------&#xA;#Tokens of Prompt:  5144 Passkey target:  89427&#xA;Llama2:     [What is the pass key? The pass key is .\n.\n.\n.]&#xA;SelfExtend: [What is the pass key? The pass key is 89427.]&#xA;-----------------------------------&#xA;&#xA;-----------------------------------&#xA;#Tokens of Prompt:  5144 Passkey target:  51906&#xA;Llama2:     [What is the pass key? The pass key is .\n.\n.\n.]&#xA;SelfExtend: [What is the pass key? The pass key is 51906.]&#xA;-----------------------------------&#xA;&#xA;-----------------------------------&#xA;#Tokens of Prompt:  5144 Passkey target:  38117&#xA;Llama2:     [What is the pass key? The pass key is \n.\n.\n.\n]&#xA;SelfExtend: [What is the pass key? The pass key is 38117.]&#xA;-----------------------------------&#xA;&#xA;-----------------------------------&#xA;#Tokens of Prompt:  5144 Passkey target:  60151&#xA;Llama2:     [What is the pass key? The pass key is .\n.\n.\n.]&#xA;SelfExtend: [What is the pass key? The pass key is 60151.]&#xA;-----------------------------------&#xA;&#xA;-----------------------------------&#xA;#Tokens of Prompt:  5144 Passkey target:  23789&#xA;Llama2:     [What is the pass key? The pass key is .\n.\n.\n.]&#xA;SelfExtend: [What is the pass key? The pass key is 23789.]&#xA;-----------------------------------&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Mistral&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;-----------------------------------&#xA;#Tokens of Prompt: 9994 Passkey target: 51013&#xA;Mistral:    [What is the pass key? The pass key is \n\n\n\n\n\n]&#xA;SelfExtend: [What is the pass key? The pass key is 51013.]&#xA;-----------------------------------&#xA;&#xA;-----------------------------------&#xA;#Tokens of Prompt: 9994 Passkey target: 36920&#xA;Mistral:    [What is the pass key? The pass key is \n\n\n\n\n\n]&#xA;SelfExtend: [What is the pass key? The pass key is 36920.]&#xA;-----------------------------------&#xA;&#xA;-----------------------------------&#xA;#Tokens of Prompt: 9994 Passkey target: 83493&#xA;Mistral:    [What is the pass key? The pass key is \n\n\n\n\n\n]&#xA;SelfExtend: [What is the pass key? The pass key is 83493.]&#xA;-----------------------------------&#xA;&#xA;-----------------------------------&#xA;#Tokens of Prompt: 9994 Passkey target: 78585&#xA;Mistral:    [What is the pass key? The pass key is \n\n\n\n\n\n]&#xA;SelfExtend: [What is the pass key? The pass key is 78585.]&#xA;-----------------------------------&#xA;&#xA;-----------------------------------&#xA;#Tokens of Prompt: 9994 Passkey target: 58328&#xA;Mistral:    [What is the pass key? The pass key is \n\n\n\n\n\n]&#xA;SelfExtend: [What is the pass key? The pass key is 58328.]&#xA;-----------------------------------&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4.How to choose the group_size and neighbor_window&lt;/h2&gt; &#xA;&lt;p&gt;The following thoughts are based on our experience:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;With Llama-2 as the base model, &lt;strong&gt;2~64&lt;/strong&gt; are reasonable for group_size; &lt;strong&gt;512~1536&lt;/strong&gt; are feasible for neighbor_window. But larger group_size and smaller neighbor_window are also good in many cases.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The general rule of choosing group_size and neighbor_window is: ensure the input sequence lenght is within the maximum extended window size (For llama-2, it would be (4096 - neighbor_window) * group_size + neighbor_window ).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We didn&#39;t choose the group size carefully. For the same sequence, smaller group should be better. But we found this does not strictly hold in some experiments:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Sometimes, a larger group size can be beneficial. This may be due to the fact that larger positions are not well-trained. A larger group size can utilize smaller positions, which have received more training, to facilitate extension. However, smaller group sizes tend to have better precision. Thus, there is a trade-off. For more details, refer to the ablation study section. &lt;br&gt;&lt;br&gt;For example:&lt;br&gt;If the input length for a QA task is 15,800, with a neighbor window set to 1,024, the group size can be set to 5. This is because 5 * (4,096 - 1,024) + 1,024 equals 16,384, which is greater than 15,800. However, setting the group size to 6, or even larger, such as 8 or 16, might improve the model&#39;s performance. With a group size of 5, Self-Extend uses positions 1,025 to 3,979 to extend the context window. If the group size is set to 8, Self-Extend uses positions 1,025 to 2,871 for extension. Although a group size of 8 is less precise than a group size of 5, the positions 2,872 to 3,979, utilized by a group size of 5, are less trained during pretraining, which may affect the effectiveness of the extension.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Maybe, for a sequence of length L, you can try the smallest group size first [calculated by: G * (L- w_n) + w_n] , and then test whether larger group can be better.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;5. Future Plan&lt;/h2&gt; &#xA;&lt;p&gt;Our current implementation is primarily focused on helping readers easily understand the proposed method, and it aligns with the pseudocode. Its efficiency is not yet optimal. In the future, we plan to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reduce redundant attention computation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Useless KV-cache eviction for neighbor/normal attention.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Flash Attention Implementation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;6. Third-party Implementations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sdan/selfextend&#34;&gt;https://github.com/sdan/selfextend&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/1fc2f265ff9377a37fd2c61eae9cd813a3491bea/examples/main/main.cpp#L552&#34;&gt;https://github.com/ggerganov/llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: We do not test these third-party implementations, but you can try them out!&lt;/p&gt; &#xA;&lt;h2&gt;7. Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the research community to improve the effeicency of SelfExtend. If you have any idea or would like to report a bug, please open an issue or submit a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;8. License&lt;/h2&gt; &#xA;&lt;p&gt;The code is released under the MIT License.&lt;/p&gt;</summary>
  </entry>
</feed>