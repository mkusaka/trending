<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-21T01:42:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bentoml/OpenLLM</title>
    <updated>2023-06-21T01:42:05Z</updated>
    <id>tag:github.com,2023-06-21:/bentoml/OpenLLM</id>
    <link href="https://github.com/bentoml/OpenLLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bentoml/OpenLLM/main/assets/main-banner.png&#34; alt=&#34;Banner for OpenLLM&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;🦾 OpenLLM&lt;/h1&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/openllm&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/openllm.svg?sanitize=true&#34; alt=&#34;pypi_status&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/bentoml/OpenLLM/actions/workflows/ci.yml&#34;&gt; &lt;img src=&#34;https://github.com/bentoml/OpenLLM/actions/workflows/ci.yml/badge.svg?branch=main&#34; alt=&#34;ci&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://twitter.com/bentomlai&#34;&gt; &lt;img src=&#34;https://badgen.net/badge/icon/@bentomlai/1DA1F2?icon=twitter&amp;amp;label=Follow%20Us&#34; alt=&#34;Twitter&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://l.bentoml.com/join-openllm-discord&#34;&gt; &lt;img src=&#34;https://badgen.net/badge/icon/OpenLLM/7289da?icon=discord&amp;amp;label=Join%20Us&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt;&#xA; &lt;br&gt; &#xA; &lt;p&gt;An open platform for operating large language models (LLMs) in production.&lt;br&gt; Fine-tune, serve, deploy, and monitor any LLMs with ease.&lt;/p&gt; &#xA; &lt;i&gt;&lt;/i&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;📖 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;With OpenLLM, you can run inference with any open-source large-language models, deploy to the cloud or on-premises, and build powerful AI apps.&lt;/p&gt; &#xA;&lt;p&gt;🚂 &lt;strong&gt;State-of-the-art LLMs&lt;/strong&gt;: built-in supports a wide range of open-source LLMs and model runtime, including StableLM, Falcon, Dolly, Flan-T5, ChatGLM, StarCoder and more.&lt;/p&gt; &#xA;&lt;p&gt;🔥 &lt;strong&gt;Flexible APIs&lt;/strong&gt;: serve LLMs over RESTful API or gRPC with one command, query via WebUI, CLI, our Python/Javascript client, or any HTTP client.&lt;/p&gt; &#xA;&lt;p&gt;⛓️ &lt;strong&gt;Freedom To Build&lt;/strong&gt;: First-class support for LangChain, BentoML and Hugging Face that allows you to easily create your own AI apps by composing LLMs with other models and services.&lt;/p&gt; &#xA;&lt;p&gt;🎯 &lt;strong&gt;Streamline Deployment&lt;/strong&gt;: Automatically generate your LLM server Docker Images or deploy as serverless endpoint via &lt;a href=&#34;https://l.bentoml.com/bento-cloud&#34;&gt;☁️ BentoCloud&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;🤖️ &lt;strong&gt;Bring your own LLM&lt;/strong&gt;: Fine-tune any LLM to suit your needs with &lt;code&gt;LLM.tuning()&lt;/code&gt;. (Coming soon)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bentoml/OpenLLM/main/assets/output.gif&#34; alt=&#34;Gif showing OpenLLM Intro&#34;&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🏃‍ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To use OpenLLM, you need to have Python 3.8 (or newer) and &lt;code&gt;pip&lt;/code&gt; installed on your system. We highly recommend using a Virtual Environment to prevent package conflicts.&lt;/p&gt; &#xA;&lt;p&gt;You can install OpenLLM using pip as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To verify if it&#39;s installed correctly, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ openllm -h&#xA;&#xA;Usage: openllm [OPTIONS] COMMAND [ARGS]...&#xA;&#xA;   ██████╗ ██████╗ ███████╗███╗   ██╗██╗     ██╗     ███╗   ███╗&#xA;  ██╔═══██╗██╔══██╗██╔════╝████╗  ██║██║     ██║     ████╗ ████║&#xA;  ██║   ██║██████╔╝█████╗  ██╔██╗ ██║██║     ██║     ██╔████╔██║&#xA;  ██║   ██║██╔═══╝ ██╔══╝  ██║╚██╗██║██║     ██║     ██║╚██╔╝██║&#xA;  ╚██████╔╝██║     ███████╗██║ ╚████║███████╗███████╗██║ ╚═╝ ██║&#xA;   ╚═════╝ ╚═╝     ╚══════╝╚═╝  ╚═══╝╚══════╝╚══════╝╚═╝     ╚═╝&#xA;&#xA;  An open platform for operating large language models in production.&#xA;  Fine-tune, serve, deploy, and monitor any LLMs with ease.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Starting an LLM Server&lt;/h3&gt; &#xA;&lt;p&gt;To start an LLM server, use &lt;code&gt;openllm start&lt;/code&gt;. For example, to start a &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/opt&#34;&gt;&lt;code&gt;OPT&lt;/code&gt;&lt;/a&gt; server, do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openllm start opt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following this, a Web UI will be accessible at &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; where you can experiment with the endpoints and sample input prompts.&lt;/p&gt; &#xA;&lt;p&gt;OpenLLM provides a built-in Python client, allowing you to interact with the model. In a different terminal window or a Jupyter notebook, create a client to start interacting with the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import openllm&#xA;&amp;gt;&amp;gt;&amp;gt; client = openllm.client.HTTPClient(&#39;http://localhost:3000&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; client.query(&#39;Explain to me the difference between &#34;further&#34; and &#34;farther&#34;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use the &lt;code&gt;openllm query&lt;/code&gt; command to query the model from the terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENLLM_ENDPOINT=http://localhost:3000&#xA;openllm query &#39;Explain to me the difference between &#34;further&#34; and &#34;farther&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visit &lt;code&gt;http://localhost:3000/docs.json&lt;/code&gt; for OpenLLM&#39;s API specification.&lt;/p&gt; &#xA;&lt;p&gt;Users can also specify different variants of the model to be served, by providing the &lt;code&gt;--model-id&lt;/code&gt; argument, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openllm start flan-t5 --model-id google/flan-t5-large&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;openllm models&lt;/code&gt; command to see the list of models and their variants supported in OpenLLM.&lt;/p&gt; &#xA;&lt;h2&gt;🧩 Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;The following models are currently supported in OpenLLM. By default, OpenLLM doesn&#39;t include dependencies to run all models. The extra model-specific dependencies can be installed with the instructions below:&lt;/p&gt; &#xA;&lt;!-- update-readme.py: start --&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Installation&lt;/th&gt; &#xA;   &lt;th&gt;Model Ids&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/flan-t5&#34;&gt;flan-t5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;openllm[flan-t5]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-small&#34;&gt;&lt;code&gt;google/flan-t5-small&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-base&#34;&gt;&lt;code&gt;google/flan-t5-base&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-large&#34;&gt;&lt;code&gt;google/flan-t5-large&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-xl&#34;&gt;&lt;code&gt;google/flan-t5-xl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-xxl&#34;&gt;&lt;code&gt;google/flan-t5-xxl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/databrickslabs/dolly&#34;&gt;dolly-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openllm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-3b&#34;&gt;&lt;code&gt;databricks/dolly-v2-3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-7b&#34;&gt;&lt;code&gt;databricks/dolly-v2-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;&lt;code&gt;databricks/dolly-v2-12b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;chatglm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;openllm[chatglm]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/thudm/chatglm-6b&#34;&gt;&lt;code&gt;thudm/chatglm-6b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/thudm/chatglm-6b-int8&#34;&gt;&lt;code&gt;thudm/chatglm-6b-int8&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/thudm/chatglm-6b-int4&#34;&gt;&lt;code&gt;thudm/chatglm-6b-int4&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bigcode-project/starcoder&#34;&gt;starcoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;openllm[starcoder]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoder&#34;&gt;&lt;code&gt;bigcode/starcoder&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoderbase&#34;&gt;&lt;code&gt;bigcode/starcoderbase&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://falconllm.tii.ae/&#34;&gt;falcon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;openllm[falcon]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-7b&#34;&gt;&lt;code&gt;tiiuae/falcon-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b&#34;&gt;&lt;code&gt;tiiuae/falcon-40b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-7b-instruct&#34;&gt;&lt;code&gt;tiiuae/falcon-7b-instruct&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b-instruct&#34;&gt;&lt;code&gt;tiiuae/falcon-40b-instruct&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Stability-AI/StableLM&#34;&gt;stablelm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openllm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b&#34;&gt;&lt;code&gt;stabilityai/stablelm-tuned-alpha-3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b&#34;&gt;&lt;code&gt;stabilityai/stablelm-tuned-alpha-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-base-alpha-3b&#34;&gt;&lt;code&gt;stabilityai/stablelm-base-alpha-3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-base-alpha-7b&#34;&gt;&lt;code&gt;stabilityai/stablelm-base-alpha-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/opt&#34;&gt;opt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openllm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-125m&#34;&gt;&lt;code&gt;facebook/opt-125m&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-350m&#34;&gt;&lt;code&gt;facebook/opt-350m&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-1.3b&#34;&gt;&lt;code&gt;facebook/opt-1.3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-2.7b&#34;&gt;&lt;code&gt;facebook/opt-2.7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-6.7b&#34;&gt;&lt;code&gt;facebook/opt-6.7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-66b&#34;&gt;&lt;code&gt;facebook/opt-66b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;!-- update-readme.py: stop --&gt; &#xA;&lt;h3&gt;Runtime Implementations (Experimental)&lt;/h3&gt; &#xA;&lt;p&gt;Different LLMs may have multiple runtime implementations. For instance, they might use Pytorch (&lt;code&gt;pt&lt;/code&gt;), Tensorflow (&lt;code&gt;tf&lt;/code&gt;), or Flax (&lt;code&gt;flax&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;If you wish to specify a particular runtime for a model, you can do so by setting the &lt;code&gt;OPENLLM_{MODEL_NAME}_FRAMEWORK={runtime}&lt;/code&gt; environment variable before running &lt;code&gt;openllm start&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want to use the Tensorflow (&lt;code&gt;tf&lt;/code&gt;) implementation for the &lt;code&gt;flan-t5&lt;/code&gt; model, you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENLLM_FLAN_T5_FRAMEWORK=tf openllm start flan-t5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; For GPU support on Flax, refers to &lt;a href=&#34;https://github.com/google/jax#pip-installation-gpu-cuda-installed-via-pip-easier&#34;&gt;Jax&#39;s installation&lt;/a&gt; to make sure that you have Jax support for the corresponding CUDA version.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Integrating a New Model&lt;/h3&gt; &#xA;&lt;p&gt;OpenLLM encourages contributions by welcoming users to incorporate their custom LLMs into the ecosystem. Check out &lt;a href=&#34;https://github.com/bentoml/OpenLLM/raw/main/ADDING_NEW_MODEL.md&#34;&gt;Adding a New Model Guide&lt;/a&gt; to see how you can do it yourself.&lt;/p&gt; &#xA;&lt;h2&gt;⚙️ Integrations&lt;/h2&gt; &#xA;&lt;p&gt;OpenLLM is not just a standalone product; it&#39;s a building block designed to easily integrate with other powerful tools. We currently offer integration with &lt;a href=&#34;https://github.com/bentoml/BentoML&#34;&gt;BentoML&lt;/a&gt; and &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;BentoML&lt;/h3&gt; &#xA;&lt;p&gt;OpenLLM models can be integrated as a &lt;a href=&#34;https://docs.bentoml.org/en/latest/concepts/runner.html&#34;&gt;Runner&lt;/a&gt; in your BentoML service. These runners have a &lt;code&gt;generate&lt;/code&gt; method that takes a string as a prompt and returns a corresponding output string. This will allow you to plug and play any OpenLLM models with your existing ML workflow.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import bentoml&#xA;import openllm&#xA;&#xA;model = &#34;opt&#34;&#xA;&#xA;llm_config = openllm.AutoConfig.for_model(model)&#xA;llm_runner = openllm.Runner(model, llm_config=llm_config)&#xA;&#xA;svc = bentoml.Service(&#xA;    name=f&#34;llm-opt-service&#34;, runners=[llm_runner]&#xA;)&#xA;&#xA;@svc.api(input=Text(), output=Text())&#xA;async def prompt(input_text: str) -&amp;gt; str:&#xA;    answer = await llm_runner.generate(input_text)&#xA;    return answer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Hugging Face Agents&lt;/h3&gt; &#xA;&lt;p&gt;OpenLLM seamlessly integrates with Hugging Face Agents.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; The Hugging Face Agent is still at experimental stage. It is recommended to OpenLLM with &lt;code&gt;pip install -r nightly-requirements.generated.txt&lt;/code&gt; to get the latest API update for Hugging Face agent.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import transformers&#xA;&#xA;agent = transformers.HfAgent(&#34;http://localhost:3000/hf/agent&#34;)  # URL that runs the OpenLLM server&#xA;&#xA;agent.run(&#34;Is the following `text` positive or negative?&#34;, text=&#34;I don&#39;t like how this models is generate inputs&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; Only &lt;code&gt;starcoder&lt;/code&gt; is currently supported with Agent integration. The example aboved was also ran with four T4s on EC2 &lt;code&gt;g4dn.12xlarge&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you want to use OpenLLM client to ask questions to the running agent, you can also do so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openllm&#xA;&#xA;client = openllm.client.HTTPClient(&#34;http://localhost:3000&#34;)&#xA;&#xA;client.ask_agent(&#xA;    task=&#34;Is the following `text` positive or negative?&#34;,&#xA;    text=&#34;What are you thinking about?&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LangChain (⏳Coming Soon!)&lt;/h3&gt; &#xA;&lt;p&gt;In future LangChain releases, you&#39;ll be able to effortlessly invoke OpenLLM models, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain.llms import OpenLLM&#xA;llm = OpenLLM.for_model(model_name=&#39;flan-t5&#39;)&#xA;llm(&#34;What is the difference between a duck and a goose?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you have an OpenLLM server deployed elsewhere, you can connect to it by specifying its URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain.llms import OpenLLM&#xA;llm = OpenLLM.for_model(server_url=&#39;http://localhost:8000&#39;, server_type=&#39;http&#39;)&#xA;llm(&#34;What is the difference between a duck and a goose?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🚀 Deploying to Production&lt;/h2&gt; &#xA;&lt;p&gt;To deploy your LLMs into production:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Building a Bento&lt;/strong&gt;: With OpenLLM, you can easily build a Bento for a specific model, like &lt;code&gt;dolly-v2&lt;/code&gt;, using the &lt;code&gt;build&lt;/code&gt; command.:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openllm build dolly-v2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A &lt;a href=&#34;https://docs.bentoml.org/en/latest/concepts/bento.html#what-is-a-bento&#34;&gt;Bento&lt;/a&gt;, in BentoML, is the unit of distribution. It packages your program&#39;s source code, models, files, artifacts, and dependencies.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Containerize your Bento&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bentoml containerize &amp;lt;name:version&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;BentoML offers a comprehensive set of options for deploying and hosting online ML services in production. To learn more, check out the &lt;a href=&#34;https://docs.bentoml.org/en/latest/concepts/deploy.html&#34;&gt;Deploying a Bento&lt;/a&gt; guide.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🍇 Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;OpenLLM collects usage data to enhance user experience and improve the product. We only report OpenLLM&#39;s internal API calls and ensure maximum privacy by excluding sensitive information. We will never collect user code, model data, or stack traces. For usage tracking, check out the &lt;a href=&#34;https://raw.githubusercontent.com/bentoml/OpenLLM/main/src/openllm/utils/analytics.py&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can opt-out of usage tracking by using the &lt;code&gt;--do-not-track&lt;/code&gt; CLI option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openllm [command] --do-not-track&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or by setting environment variable &lt;code&gt;OPENLLM_DO_NOT_TRACK=True&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENLLM_DO_NOT_TRACK=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;👥 Community&lt;/h2&gt; &#xA;&lt;p&gt;Engage with like-minded individuals passionate about LLMs, AI, and more on our &lt;a href=&#34;https://l.bentoml.com/join-openllm-discord&#34;&gt;Discord&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;OpenLLM is actively maintained by the BentoML team. Feel free to reach out and join us in our pursuit to make LLMs more accessible and easy-to-use👉 &lt;a href=&#34;https://l.bentoml.com/join-slack&#34;&gt;Join our Slack community!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🎁 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! If you&#39;re interested in enhancing OpenLLM&#39;s capabilities or have any questions, don&#39;t hesitate to reach out in our &lt;a href=&#34;https://l.bentoml.com/join-openllm-discord&#34;&gt;discord channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Checkout our &lt;a href=&#34;https://github.com/bentoml/OpenLLM/raw/main/DEVELOPMENT.md&#34;&gt;Developer Guide&lt;/a&gt; if you wish to contribute to OpenLLM&#39;s codebase.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jncraton/languagemodels</title>
    <updated>2023-06-21T01:42:05Z</updated>
    <id>tag:github.com,2023-06-21:/jncraton/languagemodels</id>
    <link href="https://github.com/jncraton/languagemodels" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Explore large language models on any computer with 512MB of RAM&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/languagemodels&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/languagemodels.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://languagemodels.netlify.app/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-online-brightgreen&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jncraton/languagemodels/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/jncraton/languagemodels/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;x64 Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jncraton/languagemodels/actions/workflows/pi.yml&#34;&gt;&lt;img src=&#34;https://github.com/jncraton/languagemodels/actions/workflows/pi.yml/badge.svg?sanitize=true&#34; alt=&#34;ARM64 Build&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://app.netlify.com/sites/languagemodels/deploys&#34;&gt;&lt;img src=&#34;https://api.netlify.com/api/v1/badges/722e625a-c6bc-4373-bd88-c017adc58c00/deploy-status&#34; alt=&#34;Netlify Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/jncraton/languagemodels/blob/master/examples/translate.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Python building blocks to explore large language models on any computer with 512MB of RAM&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://replit.com/@jncraton/langaugemodels#main.py&#34;&gt;&lt;img src=&#34;https://replit.com/badge?caption=Try%20with%20Replit&amp;amp;variant=small&#34; alt=&#34;Try with Replit Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jncraton/languagemodels/main/media/hello.gif&#34; alt=&#34;Translation hello world example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Target Audience&lt;/h2&gt; &#xA;&lt;p&gt;This package is designed to be as simple as possible for &lt;strong&gt;learners&lt;/strong&gt; and &lt;strong&gt;educators&lt;/strong&gt; exploring how large language models intersect with modern software development. The interfaces to this package are all simple functions using standard types. The complexity of large language models is hidden from view while providing free local inference using light-weight, open models. All included models are free for educational use, no API keys are required, and all inference is performed locally by default.&lt;/p&gt; &#xA;&lt;h2&gt;Installation and Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;This package can be installed using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install languagemodels&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once installed, you should be able to interact with the package in Python as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import languagemodels as lm&#xA;&amp;gt;&amp;gt;&amp;gt; lm.do(&#34;What color is the sky?&#34;)&#xA;&#39;The color of the sky is blue.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will require downloading a significant amount of data (~250MB) on the first run. Models will be cached for later use and subsequent calls should be quick.&lt;/p&gt; &#xA;&lt;h2&gt;Example Usage&lt;/h2&gt; &#xA;&lt;p&gt;Here are some usage examples as Python REPL sessions. This should work in the REPL, notebooks, or in traditional scripts and applications.&lt;/p&gt; &#xA;&lt;h3&gt;Text Completions&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import languagemodels as lm&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; lm.complete(&#34;She hid in her room until&#34;)&#xA;&#39;she was sure she was safe&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Instruction Following&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import languagemodels as lm&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; lm.do(&#34;Translate to English: Hola, mundo!&#34;)&#xA;&#39;Hello, world!&#39;&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; lm.do(&#34;What is the capital of France?&#34;)&#xA;&#39;Paris.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Chat&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; lm.chat(&#39;&#39;&#39;&#xA;...      System: Respond as a helpful assistant.&#xA;...&#xA;...      User: What time is it?&#xA;...&#xA;...      Assistant:&#xA;...      &#39;&#39;&#39;)&#xA;&#39;I&#39;m sorry, but as an AI language model, I don&#39;t have access to real-time information. Please provide me with the specific time you are asking for so that I can assist you better.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;External Retrieval&lt;/h3&gt; &#xA;&lt;p&gt;Helper functions are provided to retrieve text from external sources that can be used to augment prompt context.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import languagemodels as lm&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; lm.get_wiki(&#39;Chemistry&#39;)&#xA;&#39;Chemistry is the scientific study...&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; lm.get_weather(41.8, -87.6)&#xA;&#39;Partly cloudy with a chance of rain...&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; lm.get_date()&#xA;&#39;Friday, May 12, 2023 at 09:27AM&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s an example showing how this can be used (compare to previous chat example):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; lm.chat(f&#39;&#39;&#39;&#xA;...      System: Respond as a helpful assistant. It is {lm.get_date()}&#xA;...&#xA;...      User: What time is it?&#xA;...&#xA;...      Assistant:&#xA;...      &#39;&#39;&#39;)&#xA;&#39;It is currently Wednesday, June 07, 2023 at 12:53PM.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Semantic Search&lt;/h3&gt; &#xA;&lt;p&gt;Semantic search is provided to retrieve documents that may provide helpful context from a document store.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import languagemodels as lm&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; lm.store_doc(&#34;Mars is a planet&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; lm.store_doc(&#34;The sun is hot&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; lm.load_doc(&#34;What is Mars?&#34;)&#xA;&#39;Mars is a planet&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This can also be used to get a blend of context from stored documents:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import languagemodels as lm&#xA;&amp;gt;&amp;gt;&amp;gt; lm.store_doc(lm.get_wiki(&#34;Python&#34;), &#34;Python&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; lm.store_doc(lm.get_wiki(&#34;C language&#34;), &#34;C&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; lm.store_doc(lm.get_wiki(&#34;Javascript&#34;), &#34;Javascript&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; lm.get_doc_context(&#34;What does it mean for batteries to be included in a language?&#34;)&#xA;&#39;Python: It is often described as a &#34;batteries included&#34; language due to its comprehensive standard library.Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.&#xA;&#xA;C: It was designed to be compiled to provide low-level access to memory and language constructs that map efficiently to machine instructions, all with minimal runtime support.&#xA;&#xA;C: The book The C Programming Language, co-authored by the original language designer, served for many years as the de facto standard for the language.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;p&gt;The models used by this package are 1000x smaller than the largest models in use today. They are useful as learning tools, but if you are expecting ChatGPT or similar performance, you will be very disappointed.&lt;/p&gt; &#xA;&lt;p&gt;The base model should work on any system with 512MB of memory, but this memory limit can be increased. Setting this value higher will require more memory and generate results more slowly, but the results should be superior. Here&#39;s an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import languagemodels as lm&#xA;&amp;gt;&amp;gt;&amp;gt; lm.do(&#34;If I have 7 apples then eat 5, how many apples do I have?&#34;)&#xA;&#39;You have 8 apples.&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; lm.set_max_ram(&#39;4gb&#39;)&#xA;4.0&#xA;&amp;gt;&amp;gt;&amp;gt; lm.do(&#34;If I have 7 apples then eat 5, how many apples do I have?&#34;)&#xA;&#39;I have 2 apples left.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://languagemodels.netlify.app/&#34;&gt;Full documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;p&gt;This package is not meant for advanced usage. If you are looking for something more powerful you could explore &lt;a href=&#34;https://huggingface.co/docs/transformers&#34;&gt;transformers&lt;/a&gt; from Hugging Face. For integrating language models in more complex ways, &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; or &lt;a href=&#34;https://github.com/microsoft/guidance&#34;&gt;guidance&lt;/a&gt; may be helpful.&lt;/p&gt; &#xA;&lt;h2&gt;Projects Ideas&lt;/h2&gt; &#xA;&lt;p&gt;This package can be used to do the heavy lifting for a number of learning projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CLI Chatbot (see &lt;a href=&#34;https://raw.githubusercontent.com/jncraton/languagemodels/main/examples/chat.py&#34;&gt;examples/chat.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Streamlit chatbot (see &lt;a href=&#34;https://raw.githubusercontent.com/jncraton/languagemodels/main/examples/streamlitchat.py&#34;&gt;examples/streamlitchat.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Chatbot with information retrieval&lt;/li&gt; &#xA; &lt;li&gt;Chatbot with access to real-time information&lt;/li&gt; &#xA; &lt;li&gt;Tool use&lt;/li&gt; &#xA; &lt;li&gt;Text classification&lt;/li&gt; &#xA; &lt;li&gt;Extractive question answering&lt;/li&gt; &#xA; &lt;li&gt;Semantic search over documents&lt;/li&gt; &#xA; &lt;li&gt;Document question answering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Several example programs and notebooks are included in the &lt;code&gt;examples&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Attribution&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenNMT/CTranslate2&#34;&gt;CTranslate2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LaMini-Flan-T5-783M&#34;&gt;LaMini-Flan-T5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-large&#34;&gt;Flan-T5&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>princeton-vl/infinigen</title>
    <updated>2023-06-21T01:42:05Z</updated>
    <id>tag:github.com,2023-06-21:/princeton-vl/infinigen</id>
    <link href="https://github.com/princeton-vl/infinigen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Infinite Photorealistic Worlds using Procedural Generation&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;&lt;a href=&#34;https://infinigen.org&#34;&gt;Infinigen: Infinite Photorealistic Worlds using Procedural Generation&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Please visit our website, &lt;a href=&#34;https://infinigen.org&#34;&gt;https://infinigen.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/6tgspeI-GHY&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/princeton-vl/infinigen/main/images/video_thumbnail.png&#34; alt=&#34;Infinigen Trailer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use Infinigen in your work, please cite our &lt;a href=&#34;https://raw.githubusercontent.com/princeton-vl/infinigen/main/%5Bhttps://arxiv.org/abs/2306.09310%5D(https://arxiv.org/abs/2306.09310)&#34;&gt;academic paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.09310&#34;&gt;Infinite Photorealistic Worlds using Procedural Generation&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;http://araistrick.com/&#34;&gt;Alexander Raistrick&lt;/a&gt;*, &lt;a href=&#34;https://www.lahavlipson.com/&#34;&gt;Lahav Lipson&lt;/a&gt;*, &lt;a href=&#34;https://mazeyu.github.io/&#34;&gt;Zeyu Ma&lt;/a&gt;* (*equal contribution, alphabetical order) &lt;br&gt; &lt;a href=&#34;https://www.cs.princeton.edu/~lm5483/&#34;&gt;Lingjie Mei&lt;/a&gt;, &lt;a href=&#34;https://www.cs.princeton.edu/~mingzhew&#34;&gt;Mingzhe Wang&lt;/a&gt;, &lt;a href=&#34;https://zuoym15.github.io/&#34;&gt;Yiming Zuo&lt;/a&gt;, &lt;a href=&#34;https://kkayan.com/&#34;&gt;Karhan Kayan&lt;/a&gt;, &lt;a href=&#34;https://hermera.github.io/&#34;&gt;Hongyu Wen&lt;/a&gt;, &lt;a href=&#34;https://pvl.cs.princeton.edu/people.html&#34;&gt;Beining Han&lt;/a&gt;, &lt;br&gt; &lt;a href=&#34;https://pvl.cs.princeton.edu/people.html&#34;&gt;Yihan Wang&lt;/a&gt;, &lt;a href=&#34;http://www-personal.umich.edu/~alnewell/index.html&#34;&gt;Alejandro Newell&lt;/a&gt;, &lt;a href=&#34;https://heilaw.github.io/&#34;&gt;Hei Law&lt;/a&gt;, &lt;a href=&#34;https://imankgoyal.github.io/&#34;&gt;Ankit Goyal&lt;/a&gt;, &lt;a href=&#34;https://yangky11.github.io/&#34;&gt;Kaiyu Yang&lt;/a&gt;, &lt;a href=&#34;http://www.cs.princeton.edu/~jiadeng&#34;&gt;Jia Deng&lt;/a&gt;&lt;br&gt; Conference on Computer Vision and Pattern Recognition (CVPR) 2023 &lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{infinigen2023infinite,&#xA;  title={Infinite Photorealistic Worlds Using Procedural Generation},&#xA;  author={Raistrick, Alexander and Lipson, Lahav and Ma, Zeyu and Mei, Lingjie and Wang, Mingzhe and Zuo, Yiming and Kayan, Karhan and Wen, Hongyu and Han, Beining and Wang, Yihan and Newell, Alejandro and Law, Hei and Goyal, Ankit and Yang, Kaiyu and Deng, Jia},&#xA;  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;  pages={12630--12641},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Installation is tested and working on the following platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 22.04.2 LTS &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GPUs options tested: CPU only, GTX-1080, RTX-2080, RTX-3090, RTX-4090 (Driver 525, CUDA 12.0)&lt;/li&gt; &#xA;   &lt;li&gt;RAM: 16GB&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MacOS Monterey &amp;amp; Ventura, Apple M1 Pro, 16GB RAM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are working on support for rendering with AMD GPUs. Windows users should use &lt;a href=&#34;https://ubuntu.com/tutorials/install-ubuntu-on-wsl2-on-windows-11-with-gui-support#1-overview&#34;&gt;WSL2&lt;/a&gt;. More instructions coming soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run these commands to get started&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/princeton-vl/infinigen.git&#xA;cd infinigen&#xA;conda create --name infinigen python=3.10&#xA;conda activate infinigen&#xA;bash install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;install.sh&lt;/code&gt; may take significant time to download Blender3.3 and compile all source files.&lt;/p&gt; &#xA;&lt;p&gt;Ignore non-fatal warnings. See &lt;a href=&#34;https://raw.githubusercontent.com/princeton-vl/infinigen/main/#getting-help&#34;&gt;Getting Help&lt;/a&gt; for guidelines on posting github issues&lt;/p&gt; &#xA;&lt;p&gt;Run the following or add it to your &lt;code&gt;~/.bashrc&lt;/code&gt; (Linux/WSL) or &lt;code&gt;~/.bash_profile&lt;/code&gt; (Mac)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# on Linux/WSL&#xA;export BLENDER=&#34;/PATH/TO/infinigen/blender/blender&#34;&#xA;# on MAC&#xA;export BLENDER=&#34;/PATH/TO/infinigen/Blender.app/Contents/MacOS/Blender&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details closed&gt; &#xA; &lt;summary&gt;&lt;b&gt;(Optional) OpenGL Ground Truth Installation&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;The above default install instructions enable you to run the full Infinigen scene generation system&lt;/p&gt; &#xA; &lt;p&gt;This section will allow you to use our own &lt;code&gt;--pipeline_configs opengl_gt&lt;/code&gt; ground truth extraction config, which avoids rendering twice in blender, and provides additional labels such occlusion boundaries, sub-object segmentation, 3D flow and easy 3D bounding boxes. If you do not need ground truth, or do not need these features,skip this section and use &lt;code&gt;--pipeline_configs blender_gt&lt;/code&gt; as shown in &lt;a href=&#34;https://raw.githubusercontent.com/princeton-vl/infinigen/main/#generate-images-in-one-command&#34;&gt;Generate image(s) in one command&lt;/a&gt;. This section is intended for computer vision researchers and power-users, and is currently &lt;em&gt;only supported on Ubuntu&lt;/em&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;git submodule init&#xA;git submodule update&#xA;sudo apt-get install libglm-dev libglew-dev libglfw3-dev libgles2-mesa-dev zlib1g-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you do not have sudo access, you may attempt the following:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;install them manually set your $CPATH variables appropriately.&lt;/li&gt; &#xA;  &lt;li&gt;ask your administrator to install them on your behalf&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Finally, run&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;bash install.sh opengl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details closed&gt; &#xA; &lt;summary&gt;&lt;b&gt;(Optional) Running Infinigen in a Docker Container&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Docker on Linux&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;cd infinigen/docker&#xA;bash make_docker.sh&#xA;docker exec -it infinigen bash&#xA;source setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To disable GPU passthrough use &lt;code&gt;bash make_docker.sh --noGPU&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Docker on Windows&lt;/strong&gt; Install &lt;a href=&#34;https://infinigen.org/docs/installation/intro#setup-for-windows&#34;&gt;WSL2&lt;/a&gt; and &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;Docker Desktop&lt;/a&gt;, with &#34;Use the WSL 2 based engine...&#34; enabled in settings. Keep the Docker Desktop application open while running containers. Then follow instructions as above.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;&#34;Hello World&#34;: Generate your first Infinigen scene&lt;/h3&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;strong&gt;Known issue&lt;/strong&gt; &lt;span&gt;⚠&lt;/span&gt; : We are actively fixing an issue which causes commands not to be reproducible on many platforms. The same command may produce multiple rearranged scenes with different runtimes and memory requirements.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/princeton-vl/infinigen/main/images/Image0048_00_00.png&#34; width=&#34;330&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/princeton-vl/infinigen/main/images/Depth0048_00_00.png&#34; width=&#34;330&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This guide will show you how to generate an image and it&#39;s corresponding depth ground-truth, similar to those shown above.&lt;/p&gt; &#xA;&lt;h4&gt;Generate a scene step by step&lt;/h4&gt; &#xA;&lt;p&gt;Infinigen generates scenes by running multiple tasks (usually executed automatically, like in &lt;a href=&#34;https://raw.githubusercontent.com/princeton-vl/infinigen/main/#generate-images-in-one-command&#34;&gt;Generate image(s) in one command&lt;/a&gt;). Here we will run them one by one to demonstrate. These commands take approximately 10 minutes and 16GB of memory to execute on an M1 Mac or Linux Desktop.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd worldgen&#xA;mkdir outputs&#xA;&#xA;# Generate a scene layout&#xA;$BLENDER -noaudio --background --python generate.py -- --seed 0 --task coarse -g desert simple --output_folder outputs/helloworld/coarse&#xA;&#xA;# Populate unique assets&#xA;$BLENDER -noaudio --background --python generate.py -- --seed 0 --task populate fine_terrain -g desert simple --input_folder outputs/helloworld/coarse --output_folder outputs/helloworld/fine&#xA;&#xA;# Render RGB images&#xA;$BLENDER -noaudio --background --python generate.py -- --seed 0 --task render -g desert simple --input_folder outputs/helloworld/fine --output_folder outputs/helloworld/frames&#xA;&#xA;# Render again for accurate ground-truth&#xA;$BLENDER -noaudio --background --python generate.py -- --seed 0 --task render -g desert simple --input_folder outputs/helloworld/fine --output_folder outputs/helloworld/frames -p render.render_image_func=@flat/render_image &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Output logs should indicate what the code is working on. Use &lt;code&gt;--debug&lt;/code&gt; for even more detail. After each command completes you can inspect it&#39;s &lt;code&gt;--output_folder&lt;/code&gt; for results, including running &lt;code&gt;$BLENDER outputs/helloworld/coarse/scene.blend&lt;/code&gt; or similar to view blender files. We hide many meshes by default for viewport stability; to view them, click &#34;Render&#34; or use the UI to unhide them.&lt;/p&gt; &#xA;&lt;h4&gt;Generate image(s) in one command&lt;/h4&gt; &#xA;&lt;p&gt;We provide &lt;code&gt;tools/manage_datagen_jobs.py&lt;/code&gt;, a utility which runs these or similar steps automatically.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m tools.manage_datagen_jobs --output_folder outputs/hello_world --num_scenes 1 &#xA;--pipeline_configs local_16GB monocular blender_gt --specific_seed 0 --configs desert simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ready to remove the guardrails? Try the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Swap &lt;code&gt;desert&lt;/code&gt; for any of &lt;code&gt;config/scene_types&lt;/code&gt; to get different biome (or write your own crazy config!). You can also add in the name of any file in &lt;code&gt;configs&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Change the &lt;code&gt;--specific_seed&lt;/code&gt; to any number to produce different scenes, or remove it and set --num_scenes 50 to try many random seeds.&lt;/li&gt; &#xA; &lt;li&gt;Remove &lt;code&gt;simple&lt;/code&gt; to generate more detailed (but &lt;em&gt;EXPENSIVE&lt;/em&gt;) scenes, as shown in the trailer.&lt;/li&gt; &#xA; &lt;li&gt;Read and customize &lt;code&gt;generate.py&lt;/code&gt; to understand how infinigen works under the hood.&lt;/li&gt; &#xA; &lt;li&gt;Append &lt;code&gt;-p compose_scene.grass_chance=1.0&lt;/code&gt; to the first command to force grass (or any of &lt;code&gt;generate.py&#39;s&lt;/code&gt; &#39;run_stage&#39; asset names) to appear in the scene. You can modify the kwargs @gin.configurable() python function in the entire repo via this mechanism.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;--configs&lt;/code&gt; enables you to customize the random &lt;em&gt;distribution&lt;/em&gt; of visual content. If you do not select any config in the folder &lt;code&gt;config/scene_types&lt;/code&gt;, the code choose one for you at random.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--pipeline_configs&lt;/code&gt; determines what compute resources will be used, and what render jobs are necessary for each scene. A list of configs are available in &lt;code&gt;tools/pipeline_configs&lt;/code&gt;. You must pick one config to determine compute type (ie &lt;code&gt;local_64GB&lt;/code&gt; or &lt;code&gt;slurm&lt;/code&gt;) and one to determine the dataset type (such as &lt;code&gt;monocular&lt;/code&gt; or &lt;code&gt;monocular_video&lt;/code&gt;). Run &lt;code&gt;python -m tools.manage_datagen_jobs --help&lt;/code&gt; for more options related to dataset generation.&lt;/p&gt; &#xA;&lt;p&gt;If you intend to use CUDA-accelerated terrain (&lt;code&gt;--pipeline_configs enable_gpu&lt;/code&gt;), you must run &lt;code&gt;install.sh&lt;/code&gt; on a CUDA-enabled machine.&lt;/p&gt; &#xA;&lt;p&gt;Infinigen uses &lt;a href=&#34;https://github.com/google/gin-config&#34;&gt;Google&#39;s &#34;Gin Config&#34;&lt;/a&gt; heavily, and we encourage you to consult their documentation to familiarize yourself with its capabilities.&lt;/p&gt; &#xA;&lt;h2&gt;Exploring the Infinigen Codebase&lt;/h2&gt; &#xA;&lt;p&gt;Infinigen has evolved significantly since the version described in our CVPR paper. It now features some procedural code obtained from the internet under CC-0 licenses, which are marked with code comments where applicable - no such code was present in the system for the CVPR version.&lt;/p&gt; &#xA;&lt;p&gt;Infinigen is an ongoing research project, and has some known issues. Through experimenting with Infinigen&#39;s code and config files, you will find scenes which crash or cannot be handled on your hardware. Infinigen scenes are randomized, with a long tail of possible scene complexity and thus compute requirements. If you encounter a scene that does not fit your computing hardware, you should try other seeds, use other config files, or follow up for help.&lt;/p&gt; &#xA;&lt;h2&gt;Coming Soon&lt;/h2&gt; &#xA;&lt;p&gt;Infinigen will evolve rapidly over the coming months. Follow us at &lt;a href=&#34;https://twitter.com/PrincetonVL&#34;&gt;https://twitter.com/PrincetonVL&lt;/a&gt; for updates.&lt;/p&gt; &#xA;&lt;p&gt;There are some aspects of the code used for our launch video that are still being polished and will be released as soon as possible, notably:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fluid simulations for dynamic water and fire&lt;/li&gt; &#xA; &lt;li&gt;Some categories of plants, namely snake plants and spider plants&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tutorials &amp;amp; Documentation&lt;/h3&gt; &#xA;&lt;p&gt;We will add comprehensive tutorials and documentation for all aspects of Infinigen. This README is &lt;strong&gt;preliminary&lt;/strong&gt;, and our docs will be expanded to cover all aspects of the project in detail.&lt;/p&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;We welcome contributions! You can contribute in many ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contribute code to this repository&lt;/strong&gt; - We welcome code contributions. More guidelines coming soon.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contribute procedural generators&lt;/strong&gt; - &lt;code&gt;worldgen/nodes/node_transpiler/dev_script.py&lt;/code&gt; provides tools to convert artist-friendly &lt;a href=&#34;https://docs.blender.org/manual/en/2.79/render/blender_render/materials/nodes/introduction.html&#34;&gt;Blender Nodes&lt;/a&gt; into python code. Tutorials and guidelines coming soon.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contribute pre-generated data&lt;/strong&gt; - Anyone can contribute their computing power to create data and share it with the community. Please stay tuned for a repository of pre-generated data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Help&lt;/h2&gt; &#xA;&lt;p&gt;Please post this repository&#39;s Github Issues page for help. Please run your command with &lt;code&gt;--debug&lt;/code&gt;, and let us know:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is your computing setup, including OS version, CPU, RAM, GPU(s) and any drivers?&lt;/li&gt; &#xA; &lt;li&gt;What version of the code are you using (link a commit hash), and what if any modifications have you made (new configs, code edits)&lt;/li&gt; &#xA; &lt;li&gt;What exact command did you run?&lt;/li&gt; &#xA; &lt;li&gt;What were the output logs of the command you ran? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If using &lt;code&gt;manage_datagen_jobs&lt;/code&gt;, look in &lt;code&gt;outputs/MYJOB/MYSEED/logs/&lt;/code&gt; to find the right one.&lt;/li&gt; &#xA;   &lt;li&gt;What was the exact python error and stacktrace, if applicable?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>