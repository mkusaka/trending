<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-27T01:42:30Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>oobabooga/text-generation-webui</title>
    <updated>2023-02-27T01:42:30Z</updated>
    <id>tag:github.com,2023-02-27:/oobabooga/text-generation-webui</id>
    <link href="https://github.com/oobabooga/text-generation-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, GPT-Neo, and Pygmalion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Text generation web UI&lt;/h1&gt; &#xA;&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, GPT-Neo, and Pygmalion.&lt;/p&gt; &#xA;&lt;p&gt;Its goal is to become the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt; of text generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/oobabooga/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb&#34;&gt;[Try it on Google Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/qa.png&#34; alt=&#34;Image1&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/cai3.png&#34; alt=&#34;Image2&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/gpt4chan.png&#34; alt=&#34;Image3&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/galactica.png&#34; alt=&#34;Image4&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Switch between different models using a dropdown menu.&lt;/li&gt; &#xA; &lt;li&gt;Notebook mode that resembles OpenAI&#39;s playground.&lt;/li&gt; &#xA; &lt;li&gt;Chat mode for conversation and role playing.&lt;/li&gt; &#xA; &lt;li&gt;Generate nice HTML output for GPT-4chan.&lt;/li&gt; &#xA; &lt;li&gt;Generate Markdown output for &lt;a href=&#34;https://github.com/paperswithcode/galai&#34;&gt;GALACTICA&lt;/a&gt;, including LaTeX support.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;a href=&#34;https://huggingface.co/models?search=pygmalionai/pygmalion&#34;&gt;Pygmalion&lt;/a&gt; and custom characters in JSON or TavernAI Character Card formats (&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Pygmalion-chat-model-FAQ&#34;&gt;FAQ&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Advanced chat features (send images, get audio responses with TTS).&lt;/li&gt; &#xA; &lt;li&gt;Stream the text output in real time.&lt;/li&gt; &#xA; &lt;li&gt;Load parameter presets from text files.&lt;/li&gt; &#xA; &lt;li&gt;Load large models in 8-bit mode (&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/20#issuecomment-1411650652&#34;&gt;see here&lt;/a&gt; if you are on Windows).&lt;/li&gt; &#xA; &lt;li&gt;Split large models across your GPU(s), CPU, and disk.&lt;/li&gt; &#xA; &lt;li&gt;CPU mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/FlexGen&#34;&gt;FlexGen offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/DeepSpeed&#34;&gt;DeepSpeed ZeRO-3 offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example.py&#34;&gt;Get responses via API&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Supports softprompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Extensions&#34;&gt;Supports extensions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Running-on-Colab&#34;&gt;Works on Google Colab&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation option 1: conda&lt;/h2&gt; &#xA;&lt;p&gt;Open a terminal and copy and paste these commands one at a time (&lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;install conda&lt;/a&gt; first if you don&#39;t have it already):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n textgen&#xA;conda activate textgen&#xA;conda install torchvision torchaudio pytorch-cuda=11.7 git -c pytorch -c nvidia&#xA;git clone https://github.com/oobabooga/text-generation-webui&#xA;cd text-generation-webui&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The third line assumes that you have an NVIDIA GPU.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have an AMD GPU, replace the third command with this one:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you are running in CPU mode, replace the third command with this one:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision torchaudio git -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation option 2: one-click installers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga-windows.zip&#34;&gt;oobabooga-windows.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga-linux.zip&#34;&gt;oobabooga-linux.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just download the zip above, extract it, and double click on &#34;install&#34;. The web UI and all its dependencies will be installed in the same folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To download a model, double click on &#34;download-model&#34;&lt;/li&gt; &#xA; &lt;li&gt;To start the web UI, double click on &#34;start-webui&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloading models&lt;/h2&gt; &#xA;&lt;p&gt;Models should be placed under &lt;code&gt;models/model-name&lt;/code&gt;. For instance, &lt;code&gt;models/gpt-j-6B&lt;/code&gt; for &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Hugging Face&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;Hugging Face&lt;/a&gt; is the main place to download models. These are some noteworthy examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&amp;amp;search=eleutherai+%2F+gpt-neo&#34;&gt;GPT-Neo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=eleutherai/pythia&#34;&gt;Pythia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/opt&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/galactica&#34;&gt;GALACTICA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=erebus&#34;&gt;*-Erebus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=pygmalion&#34;&gt;Pygmalion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can automatically download a model from HF using the script &lt;code&gt;download-model.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py organization/model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py facebook/opt-1.3b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to download a model manually, note that all you need are the json, txt, and pytorch*.bin (or model*.safetensors) files. The remaining files are not necessary.&lt;/p&gt; &#xA;&lt;h4&gt;GPT-4chan&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/ykilcher/gpt-4chan&#34;&gt;GPT-4chan&lt;/a&gt; has been shut down from Hugging Face, so you need to download it elsewhere. You have two options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Torrent: &lt;a href=&#34;https://archive.org/details/gpt4chan_model_float16&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://archive.org/details/gpt4chan_model&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Direct download: &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model_float16/&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model/&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The 32-bit version is only relevant if you intend to run the model in CPU mode. Otherwise, you should use the 16-bit version.&lt;/p&gt; &#xA;&lt;p&gt;After downloading the model, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Place the files under &lt;code&gt;models/gpt4chan_model_float16&lt;/code&gt; or &lt;code&gt;models/gpt4chan_model&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Place GPT-J 6B&#39;s config.json file in that same folder: &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/raw/main/config.json&#34;&gt;config.json&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download GPT-J 6B&#39;s tokenizer files (they will be automatically detected when you attempt to load GPT-4chan):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py EleutherAI/gpt-j-6B --text-only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the web UI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate textgen&#xA;python server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then browse to&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://localhost:7860/?__theme=dark&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Optionally, you can use the following command-line flags:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Flag&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;show this help message and exit&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model MODEL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Name of the model to load by default.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--notebook&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in notebook mode, where the output is written to the same text box as the input.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cai-chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode with a style similar to Character.AI&#39;s. If the file &lt;code&gt;img_bot.png&lt;/code&gt; or &lt;code&gt;img_bot.jpg&lt;/code&gt; exists in the same folder as server.py, this image will be used as the bot&#39;s profile picture. Similarly, &lt;code&gt;img_me.png&lt;/code&gt; or &lt;code&gt;img_me.jpg&lt;/code&gt; will be used as your profile picture.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use the CPU to generate text.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--load-in-8bit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with 8-bit precision.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--bf16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--auto-devices&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Automatically split the model across the available GPU(s) and CPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;If the model is too large for your GPU(s) and CPU combined, send the remaining layers to the disk.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk-cache-dir DISK_CACHE_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory to save the disk cache to. Defaults to &lt;code&gt;cache/&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--gpu-memory GPU_MEMORY [GPU_MEMORY ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maxmimum GPU memory in GiB to be allocated per GPU. Example: &lt;code&gt;--gpu-memory 10&lt;/code&gt; for a single GPU, &lt;code&gt;--gpu-memory 10 5&lt;/code&gt; for two GPUs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu-memory CPU_MEMORY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maximum CPU memory in GiB to allocate for offloaded weights. Must be an integer number. Defaults to 99.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--flexgen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of FlexGen offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--percent PERCENT [PERCENT ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: allocation percentages. Must be 6 numbers separated by spaces (default: 0, 100, 100, 0, 100, 0).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--compress-weight&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: Whether to compress weight (default: False).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--deepspeed&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of DeepSpeed ZeRO-3 for inference via the Transformers integration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--nvme-offload-dir NVME_OFFLOAD_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Directory to use for ZeRO-3 NVME offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--local_rank LOCAL_RANK&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Optional argument for distributed setups.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--no-stream&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Don&#39;t stream the text output in real time. This improves the text generation performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--settings SETTINGS_FILE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the default interface settings from this json file. See &lt;code&gt;settings-template.json&lt;/code&gt; for an example.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--extensions EXTENSIONS [EXTENSIONS ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The list of extensions to load. If you want to load more than one extension, write the names separated by spaces.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Make the web UI reachable from your local network.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen-port LISTEN_PORT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The listening port that the server will use.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--share&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Create a public URL. This is useful for running the web UI on Google Colab or similar.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--verbose&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Print the prompts to the terminal.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Out of memory errors? &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide&#34;&gt;Check this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Presets&lt;/h2&gt; &#xA;&lt;p&gt;Inference settings presets can be created under &lt;code&gt;presets/&lt;/code&gt; as text files. These files are detected automatically at startup.&lt;/p&gt; &#xA;&lt;p&gt;By default, 10 presets by NovelAI and KoboldAI are included. These were selected out of a sample of 43 presets after applying a K-Means clustering algorithm and selecting the elements closest to the average of each cluster.&lt;/p&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/System-requirements&#34;&gt;wiki&lt;/a&gt; for some examples of VRAM and RAM usage in both GPU and CPU mode.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests, suggestions, and issue reports are welcome.&lt;/p&gt; &#xA;&lt;p&gt;Before reporting a bug, make sure that you have created a conda environment and installed the dependencies exactly as in the &lt;em&gt;Installation&lt;/em&gt; section above.&lt;/p&gt; &#xA;&lt;p&gt;These issues are known:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8-bit doesn&#39;t work properly on Windows or older GPUs.&lt;/li&gt; &#xA; &lt;li&gt;DeepSpeed doesn&#39;t work properly on Windows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For these two, please try commenting on an existing issue instead of creating a new one.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NovelAI and KoboldAI presets: &lt;a href=&#34;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&#34;&gt;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pygmalion preset, code for early stopping in chat mode, code for some of the sliders, --chat mode colors: &lt;a href=&#34;https://github.com/PygmalionAI/gradio-ui/&#34;&gt;https://github.com/PygmalionAI/gradio-ui/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Verbose preset: Anonymous 4chan user.&lt;/li&gt; &#xA; &lt;li&gt;Instruct-Joi preset: &lt;a href=&#34;https://huggingface.co/Rallio67/joi_12B_instruct_alpha&#34;&gt;https://huggingface.co/Rallio67/joi_12B_instruct_alpha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gradio dropdown menu refresh button: &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/llama</title>
    <updated>2023-02-27T01:42:30Z</updated>
    <id>tag:github.com,2023-02-27:/facebookresearch/llama</id>
    <link href="https://github.com/facebookresearch/llama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference code for LLaMA models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaMA&lt;/h1&gt; &#xA;&lt;p&gt;This repository is intended as a minimal, hackable and readable example to load &lt;a href=&#34;https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/&#34;&gt;LLaMA&lt;/a&gt; models and run inference. In order to download the checkpoints and tokenizer, fill this &lt;a href=&#34;https://forms.gle/jk851eBVbX1m5TAv5&#34;&gt;google form&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;In a conda env with pytorch / cuda available, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then in this repository&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download&lt;/h3&gt; &#xA;&lt;p&gt;Once your request is approved, you will receive links to download the tokenizer and model files. Edit the &lt;code&gt;download.sh&lt;/code&gt; script with the signed url provided in the email to download the model weights and tokenizer.&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;The provided &lt;code&gt;example.py&lt;/code&gt; can be run on a single or multi-gpu node with &lt;code&gt;torchrun&lt;/code&gt; and will output completions for two pre-defined prompts. Using &lt;code&gt;TARGET_FOLDER&lt;/code&gt; as defined in &lt;code&gt;download.sh&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node MP example.py --ckpt_dir $TARGET_FOLDER/model_size --tokenizer_path $TARGET_FOLDER/tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Different models require different MP values:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Model Card&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama/main/MODEL_CARD.md&#34;&gt;MODEL_CARD.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>FlareSolverr/FlareSolverr</title>
    <updated>2023-02-27T01:42:30Z</updated>
    <id>tag:github.com,2023-02-27:/FlareSolverr/FlareSolverr</id>
    <link href="https://github.com/FlareSolverr/FlareSolverr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Proxy server to bypass Cloudflare protection&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FlareSolverr&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/FlareSolverr/FlareSolverr/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/FlareSolverr/FlareSolverr&#34; alt=&#34;Latest release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/flaresolverr/flaresolverr/&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/flaresolverr/flaresolverr&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/FlareSolverr/FlareSolverr/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/FlareSolverr/FlareSolverr&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/FlareSolverr/FlareSolverr/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/FlareSolverr/FlareSolverr&#34; alt=&#34;GitHub pull requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.paypal.com/paypalme/diegoheras0xff&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Donate-PayPal-yellow.svg?sanitize=true&#34; alt=&#34;Donate PayPal&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.blockchain.com/btc/address/13Hcv77AdnFWEUZ9qUpoPBttQsUT7q9TTh&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Donate-Bitcoin-f7931a.svg?sanitize=true&#34; alt=&#34;Donate Bitcoin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.blockchain.com/eth/address/0x0D1549BbB00926BF3D92c1A8A58695e982f1BE2E&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Donate-Ethereum-8c8c8c.svg?sanitize=true&#34; alt=&#34;Donate Ethereum&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;FlareSolverr is a proxy server to bypass Cloudflare and DDoS-GUARD protection.&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;FlareSolverr starts a proxy server, and it waits for user requests in an idle state using few resources. When some request arrives, it uses &lt;a href=&#34;https://www.selenium.dev&#34;&gt;Selenium&lt;/a&gt; with the &lt;a href=&#34;https://github.com/ultrafunkamsterdam/undetected-chromedriver&#34;&gt;undetected-chromedriver&lt;/a&gt; to create a web browser (Chrome). It opens the URL with user parameters and waits until the Cloudflare challenge is solved (or timeout). The HTML code and the cookies are sent back to the user, and those cookies can be used to bypass Cloudflare using other HTTP clients.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Web browsers consume a lot of memory. If you are running FlareSolverr on a machine with few RAM, do not make many requests at once. With each request a new browser is launched.&lt;/p&gt; &#xA;&lt;p&gt;It is also possible to use a permanent session. However, if you use sessions, you should make sure to close them as soon as you are done using them.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;It is recommended to install using a Docker container because the project depends on an external browser that is already included within the image.&lt;/p&gt; &#xA;&lt;p&gt;Docker images are available in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GitHub Registry =&amp;gt; &lt;a href=&#34;https://github.com/orgs/FlareSolverr/packages/container/package/flaresolverr&#34;&gt;https://github.com/orgs/FlareSolverr/packages/container/package/flaresolverr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DockerHub =&amp;gt; &lt;a href=&#34;https://hub.docker.com/r/flaresolverr/flaresolverr&#34;&gt;https://hub.docker.com/r/flaresolverr/flaresolverr&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Supported architectures are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Architecture&lt;/th&gt; &#xA;   &lt;th&gt;Tag&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;linux/386&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;x86-64&lt;/td&gt; &#xA;   &lt;td&gt;linux/amd64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM32&lt;/td&gt; &#xA;   &lt;td&gt;linux/arm/v7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM64&lt;/td&gt; &#xA;   &lt;td&gt;linux/arm64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We provide a &lt;code&gt;docker-compose.yml&lt;/code&gt; configuration file. Clone this repository and execute &lt;code&gt;docker-compose up -d&lt;/code&gt; to start the container.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer the &lt;code&gt;docker cli&lt;/code&gt; execute the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d \&#xA;  --name=flaresolverr \&#xA;  -p 8191:8191 \&#xA;  -e LOG_LEVEL=info \&#xA;  --restart unless-stopped \&#xA;  ghcr.io/flaresolverr/flaresolverr:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your host OS is Debian, make sure &lt;code&gt;libseccomp2&lt;/code&gt; version is 2.5.x. You can check the version with &lt;code&gt;sudo apt-cache policy libseccomp2&lt;/code&gt; and update the package with &lt;code&gt;sudo apt install libseccomp2=2.5.1-1~bpo10+1&lt;/code&gt; or &lt;code&gt;sudo apt install libseccomp2=2.5.1-1+deb11u1&lt;/code&gt;. Remember to restart the Docker daemon and the container after the update.&lt;/p&gt; &#xA;&lt;h3&gt;Precompiled binaries&lt;/h3&gt; &#xA;&lt;p&gt;Precompiled binaries are not currently available for v3. Please see &lt;a href=&#34;https://github.com/FlareSolverr/FlareSolverr/issues/660&#34;&gt;https://github.com/FlareSolverr/FlareSolverr/issues/660&lt;/a&gt; for updates, or below for instructions of how to build FlareSolverr from source code.&lt;/p&gt; &#xA;&lt;h3&gt;From source code&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3.10&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.google.com/intl/en_us/chrome/&#34;&gt;Chrome&lt;/a&gt; or &lt;a href=&#34;https://www.chromium.org/getting-involved/download-chromium/&#34;&gt;Chromium&lt;/a&gt; web browser.&lt;/li&gt; &#xA; &lt;li&gt;(Only in Linux / macOS) Install &lt;a href=&#34;https://en.wikipedia.org/wiki/Xvfb&#34;&gt;Xvfb&lt;/a&gt; package.&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository and open a shell in that path.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; command to install FlareSolverr dependencies.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python src/flaresolverr.py&lt;/code&gt; command to start FlareSolverr.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Systemd service&lt;/h3&gt; &#xA;&lt;p&gt;We provide an example Systemd unit file &lt;code&gt;flaresolverr.service&lt;/code&gt; as reference. You have to modify the file to suit your needs: paths, user and environment variables.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Example request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -L -X POST &#39;http://localhost:8191/v1&#39; \&#xA;-H &#39;Content-Type: application/json&#39; \&#xA;--data-raw &#39;{&#xA;  &#34;cmd&#34;: &#34;request.get&#34;,&#xA;  &#34;url&#34;:&#34;http://www.google.com/&#34;,&#xA;  &#34;maxTimeout&#34;: 60000&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Commands&lt;/h3&gt; &#xA;&lt;h4&gt;+ &lt;code&gt;sessions.create&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This will launch a new browser instance which will retain cookies until you destroy it with &lt;code&gt;sessions.destroy&lt;/code&gt;. This comes in handy, so you don&#39;t have to keep solving challenges over and over and you won&#39;t need to keep sending cookies for the browser to use.&lt;/p&gt; &#xA;&lt;p&gt;This also speeds up the requests since it won&#39;t have to launch a new browser instance for every request.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;session&lt;/td&gt; &#xA;   &lt;td&gt;Optional. The session ID that you want to be assigned to the instance. If isn&#39;t set a random UUID will be assigned.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;proxy&lt;/td&gt; &#xA;   &lt;td&gt;Optional, default disabled. Eg: &lt;code&gt;&#34;proxy&#34;: {&#34;url&#34;: &#34;http://127.0.0.1:8888&#34;}&lt;/code&gt;. You must include the proxy schema in the URL: &lt;code&gt;http://&lt;/code&gt;, &lt;code&gt;socks4://&lt;/code&gt; or &lt;code&gt;socks5://&lt;/code&gt;. Authorization (username/password) is not supported.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;+ &lt;code&gt;sessions.list&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Returns a list of all the active sessions. More for debugging if you are curious to see how many sessions are running. You should always make sure to properly close each session when you are done using them as too many may slow your computer down.&lt;/p&gt; &#xA;&lt;p&gt;Example response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;sessions&#34;: [&#xA;    &#34;session_id_1&#34;,&#xA;    &#34;session_id_2&#34;,&#xA;    &#34;session_id_3...&#34;&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;+ &lt;code&gt;sessions.destroy&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This will properly shutdown a browser instance and remove all files associated with it to free up resources for a new session. When you no longer need to use a session you should make sure to close it.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;session&lt;/td&gt; &#xA;   &lt;td&gt;The session ID that you want to be destroyed.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;+ &lt;code&gt;request.get&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;url&lt;/td&gt; &#xA;   &lt;td&gt;Mandatory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;session&lt;/td&gt; &#xA;   &lt;td&gt;Optional. Will send the request from and existing browser instance. If one is not sent it will create a temporary instance that will be destroyed immediately after the request is completed.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;maxTimeout&lt;/td&gt; &#xA;   &lt;td&gt;Optional, default value 60000. Max timeout to solve the challenge in milliseconds.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cookies&lt;/td&gt; &#xA;   &lt;td&gt;Optional. Will be used by the headless browser. Follow &lt;a href=&#34;https://github.com/puppeteer/puppeteer/raw/v3.3.0/docs/api.md#pagesetcookiecookies&#34;&gt;this&lt;/a&gt; format.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;returnOnlyCookies&lt;/td&gt; &#xA;   &lt;td&gt;Optional, default false. Only returns the cookies. Response data, headers and other parts of the response are removed.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;proxy&lt;/td&gt; &#xA;   &lt;td&gt;Optional, default disabled. Eg: &lt;code&gt;&#34;proxy&#34;: {&#34;url&#34;: &#34;http://127.0.0.1:8888&#34;}&lt;/code&gt;. You must include the proxy schema in the URL: &lt;code&gt;http://&lt;/code&gt;, &lt;code&gt;socks4://&lt;/code&gt; or &lt;code&gt;socks5://&lt;/code&gt;. Authorization (username/password) is not supported. (When the &lt;code&gt;session&lt;/code&gt; parameter is set, the proxy is ignored; a session specific proxy can be set in &lt;code&gt;sessions.create&lt;/code&gt;.)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; If you want to use Cloudflare clearance cookie in your scripts, make sure you use the FlareSolverr User-Agent too. If they don&#39;t match you will see the challenge.&lt;/p&gt; &#xA;&lt;p&gt;Example response from running the &lt;code&gt;curl&lt;/code&gt; above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;solution&#34;: {&#xA;        &#34;url&#34;: &#34;https://www.google.com/?gws_rd=ssl&#34;,&#xA;        &#34;status&#34;: 200,&#xA;        &#34;headers&#34;: {&#xA;            &#34;status&#34;: &#34;200&#34;,&#xA;            &#34;date&#34;: &#34;Thu, 16 Jul 2020 04:15:49 GMT&#34;,&#xA;            &#34;expires&#34;: &#34;-1&#34;,&#xA;            &#34;cache-control&#34;: &#34;private, max-age=0&#34;,&#xA;            &#34;content-type&#34;: &#34;text/html; charset=UTF-8&#34;,&#xA;            &#34;strict-transport-security&#34;: &#34;max-age=31536000&#34;,&#xA;            &#34;p3p&#34;: &#34;CP=\&#34;This is not a P3P policy! See g.co/p3phelp for more info.\&#34;&#34;,&#xA;            &#34;content-encoding&#34;: &#34;br&#34;,&#xA;            &#34;server&#34;: &#34;gws&#34;,&#xA;            &#34;content-length&#34;: &#34;61587&#34;,&#xA;            &#34;x-xss-protection&#34;: &#34;0&#34;,&#xA;            &#34;x-frame-options&#34;: &#34;SAMEORIGIN&#34;,&#xA;            &#34;set-cookie&#34;: &#34;1P_JAR=2020-07-16-04; expires=Sat...&#34;&#xA;        },&#xA;        &#34;response&#34;:&#34;&amp;lt;!DOCTYPE html&amp;gt;...&#34;,&#xA;        &#34;cookies&#34;: [&#xA;            {&#xA;                &#34;name&#34;: &#34;NID&#34;,&#xA;                &#34;value&#34;: &#34;204=QE3Ocq15XalczqjuDy52HeseG3zAZuJzID3R57...&#34;,&#xA;                &#34;domain&#34;: &#34;.google.com&#34;,&#xA;                &#34;path&#34;: &#34;/&#34;,&#xA;                &#34;expires&#34;: 1610684149.307722,&#xA;                &#34;size&#34;: 178,&#xA;                &#34;httpOnly&#34;: true,&#xA;                &#34;secure&#34;: true,&#xA;                &#34;session&#34;: false,&#xA;                &#34;sameSite&#34;: &#34;None&#34;&#xA;            },&#xA;            {&#xA;                &#34;name&#34;: &#34;1P_JAR&#34;,&#xA;                &#34;value&#34;: &#34;2020-07-16-04&#34;,&#xA;                &#34;domain&#34;: &#34;.google.com&#34;,&#xA;                &#34;path&#34;: &#34;/&#34;,&#xA;                &#34;expires&#34;: 1597464949.307626,&#xA;                &#34;size&#34;: 19,&#xA;                &#34;httpOnly&#34;: false,&#xA;                &#34;secure&#34;: true,&#xA;                &#34;session&#34;: false,&#xA;                &#34;sameSite&#34;: &#34;None&#34;&#xA;            }&#xA;        ],&#xA;        &#34;userAgent&#34;: &#34;Windows NT 10.0; Win64; x64) AppleWebKit/5...&#34;&#xA;    },&#xA;    &#34;status&#34;: &#34;ok&#34;,&#xA;    &#34;message&#34;: &#34;&#34;,&#xA;    &#34;startTimestamp&#34;: 1594872947467,&#xA;    &#34;endTimestamp&#34;: 1594872949617,&#xA;    &#34;version&#34;: &#34;1.0.0&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;+ &lt;code&gt;request.post&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is the same as &lt;code&gt;request.get&lt;/code&gt; but it takes one more param:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;postData&lt;/td&gt; &#xA;   &lt;td&gt;Must be a string with &lt;code&gt;application/x-www-form-urlencoded&lt;/code&gt;. Eg: &lt;code&gt;a=b&amp;amp;c=d&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Environment variables&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LOG_LEVEL&lt;/td&gt; &#xA;   &lt;td&gt;info&lt;/td&gt; &#xA;   &lt;td&gt;Verbosity of the logging. Use &lt;code&gt;LOG_LEVEL=debug&lt;/code&gt; for more information.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LOG_HTML&lt;/td&gt; &#xA;   &lt;td&gt;false&lt;/td&gt; &#xA;   &lt;td&gt;Only for debugging. If &lt;code&gt;true&lt;/code&gt; all HTML that passes through the proxy will be logged to the console in &lt;code&gt;debug&lt;/code&gt; level.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CAPTCHA_SOLVER&lt;/td&gt; &#xA;   &lt;td&gt;none&lt;/td&gt; &#xA;   &lt;td&gt;Captcha solving method. It is used when a captcha is encountered. See the Captcha Solvers section.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TZ&lt;/td&gt; &#xA;   &lt;td&gt;UTC&lt;/td&gt; &#xA;   &lt;td&gt;Timezone used in the logs and the web browser. Example: &lt;code&gt;TZ=Europe/London&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HEADLESS&lt;/td&gt; &#xA;   &lt;td&gt;true&lt;/td&gt; &#xA;   &lt;td&gt;Only for debugging. To run the web browser in headless mode or visible.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BROWSER_TIMEOUT&lt;/td&gt; &#xA;   &lt;td&gt;40000&lt;/td&gt; &#xA;   &lt;td&gt;If you are experiencing errors/timeouts because your system is slow, you can try to increase this value. Remember to increase the &lt;code&gt;maxTimeout&lt;/code&gt; parameter too.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TEST_URL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.google.com&#34;&gt;https://www.google.com&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlareSolverr makes a request on start to make sure the web browser is working. You can change that URL if it is blocked in your country.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PORT&lt;/td&gt; &#xA;   &lt;td&gt;8191&lt;/td&gt; &#xA;   &lt;td&gt;Listening port. You don&#39;t need to change this if you are running on Docker.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HOST&lt;/td&gt; &#xA;   &lt;td&gt;0.0.0.0&lt;/td&gt; &#xA;   &lt;td&gt;Listening interface. You don&#39;t need to change this if you are running on Docker.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Environment variables are set differently depending on the operating system. Some examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker: Take a look at the Docker section in this document. Environment variables can be set in the &lt;code&gt;docker-compose.yml&lt;/code&gt; file or in the Docker CLI command.&lt;/li&gt; &#xA; &lt;li&gt;Linux: Run &lt;code&gt;export LOG_LEVEL=debug&lt;/code&gt; and then start FlareSolverr in the same shell.&lt;/li&gt; &#xA; &lt;li&gt;Windows: Open &lt;code&gt;cmd.exe&lt;/code&gt;, run &lt;code&gt;set LOG_LEVEL=debug&lt;/code&gt; and then start FlareSolverr in the same shell.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Captcha Solvers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; At this time none of the captcha solvers work. You can check the status in the open issues. Any help is welcome.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes CloudFlare not only gives mathematical computations and browser tests, sometimes they also require the user to solve a captcha. If this is the case, FlareSolverr will return the error &lt;code&gt;Captcha detected but no automatic solver is configured.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;FlareSolverr can be customized to solve the captchas automatically by setting the environment variable &lt;code&gt;CAPTCHA_SOLVER&lt;/code&gt; to the file name of one of the adapters inside the &lt;a href=&#34;https://raw.githubusercontent.com/FlareSolverr/FlareSolverr/master/src/captcha&#34;&gt;/captcha&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Related projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;C# implementation =&amp;gt; &lt;a href=&#34;https://github.com/FlareSolverr/FlareSolverrSharp&#34;&gt;https://github.com/FlareSolverr/FlareSolverrSharp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>