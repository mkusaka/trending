<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-14T01:35:43Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FasterDecoding/Medusa</title>
    <updated>2023-09-14T01:35:43Z</updated>
    <id>tag:github.com,2023-09-14:/FasterDecoding/Medusa</id>
    <link href="https://github.com/FasterDecoding/Medusa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/assets/logo.png&#34; alt=&#34;Medusa&#34; width=&#34;100&#34; align=&#34;left&#34;&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;h1&gt;&amp;nbsp;Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads&lt;/h1&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://sites.google.com/view/&#xA;medusa-llm&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/ROADMAP.md&#34;&gt;&lt;b&gt;Roadmap&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;News&lt;/em&gt; 🔥&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/09] Medusa v0.1 is released! 🎉&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Medusa is a simple framework that democratizes the acceleration techniques for LLM generation with multiple decoding heads.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/assets/medusa_demo.gif&#34; width=&#34;80%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34; width=&#34;80%&#34;&gt; &#xA;  &lt;em&gt;Medusa on Vicuna-7b.&lt;/em&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We aim to tackle the three pain points of popular acceleration techniques like speculative decoding:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Requirement of a good draft model.&lt;/li&gt; &#xA; &lt;li&gt;System complexity.&lt;/li&gt; &#xA; &lt;li&gt;Inefficiency when using sampling-based generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/assets/medusa_pipeline.jpg&#34; width=&#34;60%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;left&#34; width=&#34;80%&#34;&gt; &#xA;  &lt;em&gt;Medusa adds extra &#34;heads&#34; to LLMs to predict multiple future tokens simultaneously. When augmenting a model with Medusa, the original model stays untouched, and only the new heads are fine-tuned during training. During generation, these heads each produce multiple likely words for the corresponding position. These options are then combined and processed using a tree-based attention mechanism. Finally, a typical acceptance scheme is employed to pick the longest plausible prefix from the candidates for further decoding.&lt;/em&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We aim to solve the challenges associated with speculative decoding by implementing the following ideas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instead of introducing a new model, we train multiple decoding heads on the &lt;em&gt;same&lt;/em&gt; model.&lt;/li&gt; &#xA; &lt;li&gt;The training is parameter-efficient so that even the &#34;GPU-Poor&#34; can do it. And since there is no additional model, there is no need to adjust the distributed computing setup.&lt;/li&gt; &#xA; &lt;li&gt;Relaxing the requirement of matching the distribution of the original model makes the non-greedy generation even faster than greedy decoding.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/assets/size_speedup.png&#34; width=&#34;45%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; In this initial release, our primary focus is on optimizing Medusa for a batch size of 1—a setting commonly utilized for local model hosting. In this configuration, Medusa delivers approximately a 2x speed increase across a range of Vicuna models. We are actively working to extend Medusa&#39;s capabilities by integrating it into additional inference frameworks, with the aim of achieving even greater performance gains and extending Medusa to broader settings. &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#contents&#34;&gt;Contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#method-1-with-pip&#34;&gt;Method 1: With pip&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#method-2-from-source&#34;&gt;Method 2: From source&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#model-weights&#34;&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#prepare-the-data&#34;&gt;Prepare the data&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#train-the-model&#34;&gt;Train the model&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#push-to-hugging-face-hub&#34;&gt;Push to Hugging Face Hub&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#codebase-guide&#34;&gt;Codebase Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Method 1: With pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install medusa-llm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Method 2: From source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/FasterDecoding/Medusa.git&#xA;cd Medusa&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Model Weights&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Chat Command&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face Repo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m medusa.inference.cli --model FasterDecoding/medusa-vicuna-7b-v1.3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FasterDecoding/medusa-vicuna-7b-v1.3&#34;&gt;FasterDecoding/medusa-vicuna-7b-v1.3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m medusa.inference.cli --model FasterDecoding/medusa-vicuna-13b-v1.3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FasterDecoding/medusa-vicuna-13b-v1.3&#34;&gt;FasterDecoding/medusa-vicuna-13b-v1.3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m medusa.inference.cli --model FasterDecoding/medusa-vicuna-33b-v1.3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FasterDecoding/medusa-vicuna-33b-v1.3&#34;&gt;FasterDecoding/medusa-vicuna-33b-v1.3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;We currently support single-GPU inference with a batch size of 1, which is the most common setup for local model hosting. We are actively working to extend Medusa&#39;s capabilities by integrating it into other inference frameworks; please don&#39;t hesitate to reach out if you are interested in contributing to this effort.&lt;/p&gt; &#xA;&lt;p&gt;You can use the following command for launching a CLI interface:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python -m medusa.inference.cli --model [path of medusa model]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also pass &lt;code&gt;--load-in-8bit&lt;/code&gt; or &lt;code&gt;--load-in-4bit&lt;/code&gt; to load the base model in quantized format.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;For training, please install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e &#34;.[train]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Prepare the data&lt;/h4&gt; &#xA;&lt;p&gt;We take a public version of the ShareGPT dataset, which is a subset of the Vicuna training data. For other models, you can use the corresponding training dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Train the model&lt;/h4&gt; &#xA;&lt;p&gt;We follow the training setup from &lt;a href=&#34;https://github.com/lm-sys/FastChat#fine-tuning&#34;&gt;FastChat&lt;/a&gt;, but with a much larger learning rate because we freeze the original model and only train the new heads. Here is the training command for the Vicuna-7b model on 4 GPUs. Since we are only training the new heads, the training does not require a lot of memory, and only data parallelism is needed. You can modify the script to fit your own setup. For larger models, we use the same setup. You can also use &lt;code&gt;--load_in_8bit&lt;/code&gt; or &lt;code&gt;--load_in_4bit&lt;/code&gt; to load the base model in quantized format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=4 medusa/train/train.py --model_name_or_path lmsys/vicuna-7b-v1.3 \&#xA;    --data_path ShareGPT_Vicuna_unfiltered/ShareGPT_V4.3_unfiltered_cleaned_split.json \&#xA;    --bf16 True \&#xA;    --output_dir test \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 8 \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;no&#34; \&#xA;    --learning_rate 1e-3 \&#xA;    --weight_decay 0.0 \&#xA;    --warmup_ratio 0.1 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --lazy_preprocess True \&#xA;    --medusa_num_heads 3 \&#xA;    --medusa_num_layers 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Push to Hugging Face Hub&lt;/h4&gt; &#xA;&lt;p&gt;You can use the following command to push your model to the Hugging Face Hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m medusa.hf_utils --folder [path of the model folder] --repo [name of the repo]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{medusa,&#xA;  author = {Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Tri Dao},&#xA;  title = {Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/FasterDecoding/Medusa}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Codebase Guide&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;medusa/model/medusa_model.py&lt;/code&gt; is the key file for Medusa. It contains the &lt;code&gt;MedusaModel&lt;/code&gt; class, which is a wrapper of the original model and the new heads. This class also has implementation of a streaming generation method. If you want to dive into the details of Medusa, this is the place to start.&lt;/p&gt; &#xA;&lt;p&gt;We also provide some illustrative notebooks in &lt;code&gt;notebooks/&lt;/code&gt; to help you understand the codebase.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome community contributions to Medusa. If you have an idea for how to improve it, please open an issue to discuss it with us. When submitting a pull request, please ensure that your changes are well-tested. Please split each major change into a separate pull request. We also have a &lt;a href=&#34;https://raw.githubusercontent.com/FasterDecoding/Medusa/main/ROADMAP.md&#34;&gt;Roadmap&lt;/a&gt; summarizing our future plans for Medusa. Don&#39;t hesitate to reach out if you are interested in contributing to any of the items on the roadmap.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This codebase is influenced by remarkable projects from the LLM community, including &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/&#34;&gt;TinyChat&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vllm&lt;/a&gt; and many others.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hkchengrex/Tracking-Anything-with-DEVA</title>
    <updated>2023-09-14T01:35:43Z</updated>
    <id>tag:github.com,2023-09-14:/hkchengrex/Tracking-Anything-with-DEVA</id>
    <link href="https://github.com/hkchengrex/Tracking-Anything-with-DEVA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICCV 2023] Tracking Anything with Decoupled Video Segmentation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DEVA: Tracking Anything with Decoupled Video Segmentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imgur.com/lw15BGH.png&#34; alt=&#34;titlecard&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hkchengrex.github.io/&#34;&gt;Ho Kei Cheng&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/seoungwugoh/&#34;&gt;Seoung Wug Oh&lt;/a&gt;, &lt;a href=&#34;https://www.brianpricephd.com/&#34;&gt;Brian Price&lt;/a&gt;, &lt;a href=&#34;https://www.alexander-schwing.de/&#34;&gt;Alexander Schwing&lt;/a&gt;, &lt;a href=&#34;https://joonyoung-cv.github.io/&#34;&gt;Joon-Young Lee&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;University of Illinois Urbana-Champaign and Adobe&lt;/p&gt; &#xA;&lt;p&gt;ICCV 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.03903&#34;&gt;[arXiV]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2309.03903.pdf&#34;&gt;[PDF]&lt;/a&gt; &lt;a href=&#34;https://hkchengrex.github.io/Tracking-Anything-with-DEVA/&#34;&gt;[Project Page]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1OsyNVoV_7ETD1zIE8UWxL3NXxu12m_YZ?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Provide long-term, open-vocabulary video segmentation with text-prompts out-of-the-box.&lt;/li&gt; &#xA; &lt;li&gt;Fairly easy to &lt;strong&gt;integrate your own image model&lt;/strong&gt;! Wouldn&#39;t you or your reviewers be interested in seeing examples where your image model also works well on videos &lt;span&gt;😏&lt;/span&gt;? No finetuning is needed!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note (Sep 12 2023):&lt;/strong&gt;&lt;/em&gt; We have improved automatic video segmentation by not querying the points in segmented regions. We correspondingly increased the number of query points per side to 64 and deprecated the &#34;engulf&#34; mode. The old code can be found in the &#34;legacy_engulf&#34; branch. The new code should run a lot faster and capture smaller objects. The text-prompted mode is still recommended for better results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note (Sep 11 2023):&lt;/strong&gt;&lt;/em&gt; We have removed the &#34;pluralize&#34; option as it works weirdly sometimes with GroundingDINO. If needed, please pluralize the prompt yourself.&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;We develop a decoupled video segmentation approach (&lt;strong&gt;DEVA&lt;/strong&gt;), composed of task-specific image-level segmentation and class/task-agnostic bi-directional temporal propagation. Due to this design, we only need an image-level model for the target task and a universal temporal propagation model which is trained once and generalizes across tasks. To effectively combine these two modules, we propose a (semi-)online fusion of segmentation hypotheses from different frames to generate a coherent segmentation. We show that this decoupled formulation compares favorably to end-to-end approaches in several tasks, most notably in large-vocabulary video panoptic segmentation and open-world video segmentation.&lt;/p&gt; &#xA;&lt;h2&gt;Demo Videos&lt;/h2&gt; &#xA;&lt;h3&gt;Demo with Grounded Segment Anything (text prompt: &#34;guinea pigs&#34; and &#34;chicken&#34;):&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/457a9a6a-86c3-4c5a-a3cc-25199427cd11&#34;&gt;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/457a9a6a-86c3-4c5a-a3cc-25199427cd11&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://www.youtube.com/watch?v=FM9SemMfknA&#34;&gt;https://www.youtube.com/watch?v=FM9SemMfknA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo with Grounded Segment Anything (text prompt: &#34;pigs&#34;):&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/9a6dbcd1-2c84-45c8-ac0a-4ad31169881f&#34;&gt;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/9a6dbcd1-2c84-45c8-ac0a-4ad31169881f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://youtu.be/FbK3SL97zf8&#34;&gt;https://youtu.be/FbK3SL97zf8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo with Grounded Segment Anything (text prompt: &#34;capybara&#34;):&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/2ac5acc2-d160-49be-a013-68ad1d4074c5&#34;&gt;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/2ac5acc2-d160-49be-a013-68ad1d4074c5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://youtu.be/couz1CrlTdQ&#34;&gt;https://youtu.be/couz1CrlTdQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo with Segment Anything (automatic points-in-grid prompting); original video follows DEVA result overlaying the video:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/ac6ab425-2f49-4438-bcd4-16e4ccfb0d98&#34;&gt;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/ac6ab425-2f49-4438-bcd4-16e4ccfb0d98&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Source: DAVIS 2017 validation set &#34;soapbox&#34;&lt;/p&gt; &#xA;&lt;h3&gt;Demo with Segment Anything on a out-of-domain example; original video follows DEVA result overlaying the video:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/48542bcd-113c-4454-b512-030df26def08&#34;&gt;https://github.com/hkchengrex/Tracking-Anything-with-DEVA/assets/7107196/48542bcd-113c-4454-b512-030df26def08&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://youtu.be/FQQaSyH9hZI&#34;&gt;https://youtu.be/FQQaSyH9hZI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;(Tested on Ubuntu only)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7+&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 1.12+ and corresponding torchvision&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clone our repository:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hkchengrex/Tracking-Anything-with-DEVA.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install with pip:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Tracking-Anything-with-DEVA&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Download the pretrained models:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Optional) For fast integer program solving in the semi-online setting:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Get your &lt;a href=&#34;https://www.gurobi.com/&#34;&gt;gurobi&lt;/a&gt; licence which is free for academic use. If a license is not found, we fall back to using &lt;a href=&#34;https://github.com/coin-or/pulp&#34;&gt;PuLP&lt;/a&gt; which is slower and is not rigorously tested by us. All experiments are conducted with gurobi.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Optional) For text-prompted/automatic demo:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://github.com/hkchengrex/Grounded-Segment-Anything&#34;&gt;our fork of Grounded-Segment-Anything&lt;/a&gt;. Follow its instructions.&lt;/p&gt; &#xA;&lt;p&gt;Grounding DINO installation might fail silently. Try &lt;code&gt;python -c &#34;from groundingdino.util.inference import Model as GroundingDINOModel&#34;&lt;/code&gt;. If you get a warning about running on CPU mode only, make sure you have &lt;code&gt;CUDA_HOME&lt;/code&gt; set during Grounding DINO installation.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/Tracking-Anything-with-DEVA/main/docs/DEMO.md&#34;&gt;DEMO.md&lt;/a&gt; contains more details on the input arguments and tips on speeding up inference. You can always look at &lt;code&gt;deva/inference/eval_args.py&lt;/code&gt; and &lt;code&gt;deva/ext/ext_eval_args.py&lt;/code&gt; for a full list of arguments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;With gradio:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo/demo_gradio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then visit the link that popped up on the terminal. If executing on a remote server, try &lt;a href=&#34;https://unix.stackexchange.com/questions/115897/whats-ssh-port-forwarding-and-whats-the-difference-between-ssh-local-and-remot&#34;&gt;port forwarding&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have prepared an example in &lt;code&gt;example/vipseg/12_1mWNahzcsAc&lt;/code&gt; (a clip from the VIPSeg dataset). The following two scripts segment the example clip using either Grounded Segment Anything with text prompts or SAM with automatic (points in grid) prompting.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Script (text-prompted):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo/demo_with_text.py --chunk_size 4 \&#xA;--img_path ./example/vipseg/images/12_1mWNahzcsAc \ &#xA;--amp --temporal_setting semionline \&#xA;--size 480 \&#xA;--output ./example/output --prompt person.hat.horse&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Script (automatic):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo/demo_automatic.py --chunk_size 4 \&#xA;--img_path ./example/vipseg/images/12_1mWNahzcsAc \ &#xA;--amp --temporal_setting semionline \&#xA;--size 480 \&#xA;--output ./example/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training and Evaluation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/Tracking-Anything-with-DEVA/main/docs/CUSTOM.md&#34;&gt;Running DEVA with your own detection model.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/Tracking-Anything-with-DEVA/main/docs/EVALUATION.md&#34;&gt;Running DEVA with detections to reproduce the benchmark results.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hkchengrex/Tracking-Anything-with-DEVA/main/docs/TRAINING.md&#34;&gt;Training the DEVA model.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://imgur.com/aouI1WU.png&#34;&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://imgur.com/aCbrA9S.png&#34;&gt; &#xA; &lt;img alt=&#34;separator&#34; src=&#34;https://imgur.com/aCbrA9S.png&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{cheng2023tracking,&#xA;  title={Tracking Anything with Decoupled Video Segmentation},&#xA;  author={Cheng, Ho Kei and Oh, Seoung Wug and Price, Brian and Schwing, Alexander and Lee, Joon-Young},&#xA;  booktitle={ICCV},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;The demo would not be possible without &lt;span&gt;❤️&lt;/span&gt; from the community:&lt;/p&gt; &#xA;&lt;p&gt;Grounded Segment Anything: &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;https://github.com/IDEA-Research/Grounded-Segment-Anything&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Segment Anything: &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;https://github.com/facebookresearch/segment-anything&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;XMem: &lt;a href=&#34;https://github.com/hkchengrex/XMem&#34;&gt;https://github.com/hkchengrex/XMem&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Title card generated with OpenPano: &lt;a href=&#34;https://github.com/ppwwyyxx/OpenPano&#34;&gt;https://github.com/ppwwyyxx/OpenPano&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pnnl/neuromancer</title>
    <updated>2023-09-14T01:35:43Z</updated>
    <id>tag:github.com,2023-09-14:/pnnl/neuromancer</id>
    <link href="https://github.com/pnnl/neuromancer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pytorch-based framework for solving parametric constrained optimization problems, physics-informed system identification, and parametric model predictive control.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeuroMANCER v1.4.1&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Modules with Adaptive Nonlinear Constraints and Efficient Regularizations (NeuroMANCER)&lt;/strong&gt; is an open-source differentiable programming (DP) library for solving parametric constrained optimization problems, physics-informed system identification, and parametric model-based optimal control. NeuroMANCER is written in &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; and allows for systematic integration of machine learning with scientific computing for creating end-to-end differentiable models and algorithms embedded with prior knowledge and physics.&lt;/p&gt; &#xA;&lt;h3&gt;New in v1.4.1&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve made some backwards-compatible changes in order to simplify integration and support multiple symbolic inputs to &lt;code&gt;nn.Modules&lt;/code&gt; in our &lt;code&gt;blocks&lt;/code&gt; interface.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;New Colab Examples:&lt;/strong&gt;&lt;br&gt; Physics-Informed Neural Networks (PINNs) for solving PDEs in NeuroMANCER&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/PDEs/Part_1_PINN_DiffusionEquation.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Part 1: Diffusion Equation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/PDEs/Part_2_PINN_BurgersEquation.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Part 2: Burgers&#39; Equation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/PDEs/Part_3_PINN_BurgersEquation_inverse.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Part 3: Burgers&#39; Equation w/ Parameter Estimation (Inverse Problem)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;System identification for networked ordinary differential equations (ODEs)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/system_identification/rc_net.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Data-driven modeling of physics-structured network ODEs. &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/pnnl/neuromancer/master/#version-141-release-notes&#34;&gt;v1.4.1 release notes&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Features and Examples&lt;/h2&gt; &#xA;&lt;p&gt;Extensive set of tutorials can be found in the &lt;a href=&#34;https://github.com/pnnl/neuromancer/tree/master/examples&#34;&gt;examples&lt;/a&gt; folder. Interactive notebook versions of examples are available on Google Colab! Test out NeuroMANCER functionality before cloning the repository and setting up an environment.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/tutorials/part_1_linear_regression.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Linear regression in PyTorch vs NeuroMANCER.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/tutorials/part_2_variable.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; NeuroMANCER syntax tutorial: variables, constraints, and objectives.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/tutorials/part_3_node.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; NeuroMANCER syntax tutorial: modules, Node, and System class.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/parametric_programming/Part_1_basics.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Learning to solve a constrained optimization problem.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/parametric_programming/Part_4_projectedGradient.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Learning to solve a constrained optimization problem with projected gradient method.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/system_identification/brusselator_parameter.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Parameter estimation of ordinary differential equation (ODE). &lt;/li&gt; &#xA; &lt;li&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/control/Part_1_stabilize_linear_system.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Learning to stabilize a linear dynamical system. &lt;/li&gt; &#xA; &lt;li&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/control/Part_3_ref_tracking_ODE.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Learning to control a nolinear differential equation. &lt;/li&gt; &#xA; &lt;li&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/control/Part_4_systemid_control.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Learning neural ODE model and neural control policy for an unknown dynamical system. &lt;/li&gt; &#xA; &lt;li&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pnnl/neuromancer/blob/master/examples/control/Part_5_neural_Lyapunov.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Learning neural Lyapunov function for a nonlinear dynamical system. &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The documentation for the library can be found &lt;a href=&#34;https://pnnl.github.io/neuromancer/&#34;&gt;online&lt;/a&gt;. There is also an &lt;a href=&#34;https://www.youtube.com/watch?v=YkFKz-DgC98&#34;&gt;introduction video&lt;/a&gt; covering core features of the library.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Neuromancer syntax example for constrained optimization&#xA;import neuromancer as nm&#xA;import torch &#xA;&#xA;# define neural architecture &#xA;func = nm.modules.blocks.MLP(insize=1, outsize=2, &#xA;                             linear_map=nm.slim.maps[&#39;linear&#39;], &#xA;                             nonlin=torch.nn.ReLU, hsizes=[80] * 4)&#xA;# wrap neural net into symbolic representation via the Node class: map(p) -&amp;gt; x&#xA;map = nm.system.Node(func, [&#39;p&#39;], [&#39;x&#39;], name=&#39;map&#39;)&#xA;    &#xA;# define decision variables&#xA;x = nm.constraint.variable(&#34;x&#34;)[:, [0]]&#xA;y = nm.constraint.variable(&#34;x&#34;)[:, [1]]&#xA;# problem parameters sampled in the dataset&#xA;p = nm.constraint.variable(&#39;p&#39;)&#xA;&#xA;# define objective function&#xA;f = (1-x)**2 + (y-x**2)**2&#xA;obj = f.minimize(weight=1.0)&#xA;&#xA;# define constraints&#xA;con_1 = 100.*(x &amp;gt;= y)&#xA;con_2 = 100.*(x**2+y**2 &amp;lt;= p**2)&#xA;&#xA;# create penalty method-based loss function&#xA;loss = nm.loss.PenaltyLoss(objectives=[obj], constraints=[con_1, con_2])&#xA;# construct differentiable constrained optimization problem&#xA;problem = nm.problem.Problem(nodes=[map], loss=loss)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pnnl/neuromancer/master/figs/class_diagram.png&#34; alt=&#34;UML diagram&#34;&gt; &lt;em&gt;UML diagram of NeuroMANCER classes.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;For either pip or conda installation, first clone the neuromancer package. A dedicated virtual environment (conda or otherwise) is recommended.&lt;/p&gt; &#xA;&lt;p&gt;Note: If you have a previous neuromancer env it would be best at this point to create a new environment given the following instructions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone -b master https://github.com/pnnl/neuromancer.git --single-branch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;PIP Install&lt;/h3&gt; &#xA;&lt;p&gt;Recommended installation.&lt;br&gt; Pip installation is broken up into required dependencies for core Neuromancer and dependencies associated with the examples, tests, and generating the documentation. Below we give instructions to install all dependencies in a conda virtual enviroment via pip. You need at least pip version &amp;gt;= 21.3.&lt;/p&gt; &#xA;&lt;h4&gt;Create and activate virtual environment&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n neuromancer python=3.10.4&#xA;conda activate neuromancer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install neuromancer and all dependencies.&lt;/h4&gt; &#xA;&lt;p&gt;From top level directory of cloned neuromancer run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e.[docs,tests,examples]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OR, for zsh users:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;pip install -e.&#39;[docs,tests,examples]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;code&gt;pyproject.toml&lt;/code&gt; file for reference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[project.optional-dependencies]&#xA;tests = [&#34;pytest&#34;, &#34;hypothesis&#34;]&#xA;examples = [&#34;casadi&#34;, &#34;cvxpy&#34;, &#34;imageio&#34;]&#xA;docs = [&#34;sphinx&#34;, &#34;sphinx-rtd-theme&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Note on pip install with &lt;code&gt;examples&lt;/code&gt; on MacOS (Apple M1)&lt;/h4&gt; &#xA;&lt;p&gt;Before CVXPY can be installed on Apple M1, you must install &lt;code&gt;cmake&lt;/code&gt; via Homebrew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;brew install cmake&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://www.cvxpy.org/install/index.html&#34;&gt;CVXPY installation instructions&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Conda install&lt;/h3&gt; &#xA;&lt;p&gt;Conda install is recommended for GPU acceleration. In many cases the following simple install should work for the specified OS&lt;/p&gt; &#xA;&lt;h4&gt;Create environment &amp;amp; install dependencies&lt;/h4&gt; &#xA;&lt;h5&gt;Ubuntu&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f linux_env.yml&#xA;conda activate neuromancer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Windows&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f windows_env.yml&#xA;conda activate neuromancer&#xA;conda install -c defaults intel-openmp -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;MacOS (Apple M1)&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f osxarm64_env.yml&#xA;conda activate neuromancer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Other (manually install all dependencies)&lt;/h5&gt; &#xA;&lt;p&gt;!!! Pay attention to comments for non-Linux OS !!!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n neuromancer python=3.10.4&#xA;conda activate neuromancer&#xA;conda install pytorch pytorch-cuda=11.6 -c pytorch -c nvidia&#xA;## OR (for Mac): conda install pytorch -c pytorch&#xA;conda config --append channels conda-forge&#xA;conda install scipy numpy&#34;&amp;lt;1.24.0&#34; matplotlib scikit-learn pandas dill mlflow pydot=1.4.2 pyts numba&#xA;conda install networkx=3.0 plum-dispatch=1.7.3 &#xA;conda install -c anaconda pytest hypothesis&#xA;conda install cvxpy cvxopt casadi seaborn imageio&#xA;conda install tqdm torchdiffeq toml&#xA;## (for Windows): conda install -c defaults intel-openmp -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install NeuroMANCER package&lt;/h4&gt; &#xA;&lt;p&gt;From the top level directory of cloned neuromancer (in the activated environment where the dependencies have been installed):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e . --no-deps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Test NeuroMANCER install&lt;/h3&gt; &#xA;&lt;p&gt;Run pytest on the &lt;a href=&#34;https://github.com/pnnl/neuromancer/tree/master/tests&#34;&gt;tests folder&lt;/a&gt;. It should take about 2 minutes to run the tests on CPU. There will be a lot of warnings that you can safely ignore. These warnings will be cleaned up in a future release.&lt;/p&gt; &#xA;&lt;h2&gt;Community Development&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions and feedback from the open-source community!&lt;/p&gt; &#xA;&lt;h3&gt;Contributing examples&lt;/h3&gt; &#xA;&lt;p&gt;If you have an example of using NeuroMANCER to solve an interesting problem, or of using NeuroMANCER in a unique way, we would love to see it incorporated into our current library of examples. To submit an example, create a folder for your example/s in the example folder if there isn&#39;t currently an applicable folder and place either your executable python file or notebook file there. Push your code back to github and then submit a pull request. Please make sure to note in a comment at the top of your code if there are additional dependencies to run your example and how to install those dependencies.&lt;/p&gt; &#xA;&lt;h3&gt;Contributing code&lt;/h3&gt; &#xA;&lt;p&gt;We welcome contributions to NeuroMANCER. Please accompany contributions with some lightweight unit tests via pytest (see test/ folder for some examples of easy to compose unit tests using pytest). In addition to unit tests a script utilizing introduced new classes or modules should be placed in the examples folder. To contribute a new feature please submit a pull request.&lt;/p&gt; &#xA;&lt;h3&gt;Reporting issues or bugs&lt;/h3&gt; &#xA;&lt;p&gt;If you find a bug in the code or want to request a new feature, please open an issue.&lt;/p&gt; &#xA;&lt;h2&gt;NeuroMANCER development plan&lt;/h2&gt; &#xA;&lt;p&gt;Here are some upcoming features we plan to develop. Please let us know if you would like to get involved and contribute so we may be able to coordinate on development. If there is a feature that you think would be highly valuable but not included below, please open an issue and let us know your thoughts.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Faster dynamics modeling via Torchscript&lt;/li&gt; &#xA; &lt;li&gt;Control and modelling for networked systems&lt;/li&gt; &#xA; &lt;li&gt;Easy to implement modeling and control with uncertainty quantification&lt;/li&gt; &#xA; &lt;li&gt;Online learning examples&lt;/li&gt; &#xA; &lt;li&gt;Benchmark examples of DPC compared to deep RL&lt;/li&gt; &#xA; &lt;li&gt;Conda and pip package distribution&lt;/li&gt; &#xA; &lt;li&gt;CVXPY-like interface for optimization via Problem.solve method&lt;/li&gt; &#xA; &lt;li&gt;More versatile and simplified time series dataloading&lt;/li&gt; &#xA; &lt;li&gt;Pytorch Lightning trainer compatibility&lt;/li&gt; &#xA; &lt;li&gt;Discovery of governing equations from learned RHS via NODEs and SINDy&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Release notes&lt;/h2&gt; &#xA;&lt;h3&gt;Version 1.4.1 Release Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To simplify integration, interpolation of control input is no longer supported in &lt;code&gt;integrators.py&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The &lt;code&gt;interp_u&lt;/code&gt; parameter of &lt;code&gt;Integrator&lt;/code&gt; and subclasses has been deprecated&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Additional inputs (e.g., &lt;code&gt;u&lt;/code&gt;, &lt;code&gt;t&lt;/code&gt;) can now be passed as &lt;code&gt;*args&lt;/code&gt; (instead of as a single tensor input stacked with &lt;code&gt;x&lt;/code&gt;) in: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Integrator&lt;/code&gt; and subclasses in &lt;code&gt;integrators.py&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Block&lt;/code&gt; - new base class for all other classes in &lt;code&gt;blocks.py&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ODESystem&lt;/code&gt; in &lt;code&gt;ode.py&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;New Physics-Informed Neural Network (PINN) examples for solving PDEs in &lt;code&gt;/examples/PDEs/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;New system identification example for learning physics-structured networked ODEs &lt;code&gt;/examples/system_identification/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fixed a bug in the &lt;code&gt;show(...)&lt;/code&gt; method of the &lt;code&gt;Problem&lt;/code&gt; class&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Version 1.4 Release Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refactored PSL &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Better PSL unit testing coverage&lt;/li&gt; &#xA;   &lt;li&gt;Consistent interfaces across system types&lt;/li&gt; &#xA;   &lt;li&gt;Consistent perturbation signal interface in signals.py&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Refactored Control and System ID learning using Node and System class (system.py) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Classes used for system ID can now be easily interchanged to accommodate downstream control policy learning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Version 1.3.2 Release Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Merged Structured Linear Maps and Pyton Systems Library into Neuromancer &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The code in neuromancer was closely tied to psl and slim. A decision was made to integrate the packages as submodules of neuromancer. This also solves the issue of the package names &#34;psl&#34; and &#34;slim&#34; already being taken on PyPI.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Import changes for psl and slim&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# before&#xA;import psl&#xA;import slim&#xA;&#xA;# now&#xA;from neuromancer import psl&#xA;from neuromancer import slim&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Version 1.3.1 release notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New example scripts and notebooks &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Interactive Colab notebooks for testing Neuromancer functionality without setting up an environment &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/pnnl/neuromancer/master/#examples&#34;&gt;Examples&lt;/a&gt; for links to Colab&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;RC-Network modeling using Graph Neural Time-steppers example: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;See neuromancer/examples/graph_timesteppers/&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Baseline NODE dynamics modeling results for all nonautonomous systems in Python Systems Library &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;See neuromancer/examples/benchmarks/node/&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Updated install instructions for Linux, Windows, and MAC operating systems &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;New linux_env.yml, windows_env.yml, osxarm64_env.yml files for installation of dependencies across OS&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Corresponding releases of SLiM and PSL packages &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure to update these packages if updating Neuromancer&lt;/li&gt; &#xA;   &lt;li&gt;Release 1.4 will roll SLiM and PSL into core Neuromancer for ease of installation and development&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Version 1.3 release notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tutorial &lt;a href=&#34;https://www.youtube.com/channel/UC5oWRFxzUwWrDNzkdWLIb7A&#34;&gt;YouTube videos&lt;/a&gt; to accompany tutorial scripts in examples folder: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HLuqneSnoC8&#34;&gt;examples/system_identification/duffing_parameter.py&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Closed loop control policy learning examples with Neural Ordinary Differential Equations &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;examples/control/ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;vdpo_DPC_cl_fixed_ref.py&lt;/li&gt; &#xA;     &lt;li&gt;two_tank_sysID_DPC_cl_var_ref.py&lt;/li&gt; &#xA;     &lt;li&gt;two_tank_DPC_cl_var_ref.py&lt;/li&gt; &#xA;     &lt;li&gt;two_tank_DPC_cl_fixed_ref.py&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Closed loop control policy learning example with Linear State Space Models. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;examples/control/ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;double_integrator_dpc_ol_fixed_ref.py&lt;/li&gt; &#xA;     &lt;li&gt;vtol_dpc_ol_fixed_ref.py&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;New class for Linear State Space Models (LSSM) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LinearSSM in dynamics.py&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Refactored closed-loop control policy simulations &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;simulator.py&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Interfaces for open and closed loop simulation (evaluation after training) for several classes &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Dynamics&lt;/li&gt; &#xA;   &lt;li&gt;Estimator&lt;/li&gt; &#xA;   &lt;li&gt;Policy&lt;/li&gt; &#xA;   &lt;li&gt;Constraint&lt;/li&gt; &#xA;   &lt;li&gt;PSL Emulator classes&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;New class for closed-loop policy learning of non-autonomous ODE systems &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ControlODE class in ode.py&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Added support for NODE systems &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Torchdiffeq integration with fast adjoint method for NODE optimization&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lead developers&lt;/strong&gt;: Aaron Tuor, Jan Drgona&lt;br&gt; &lt;strong&gt;Active contributors&lt;/strong&gt;: Aaron Tuor, Jan Drgona, James Koch, Madelyn Shapiro, Draguna Vrabie, Seth Briney&lt;br&gt; &lt;strong&gt;Past contributors&lt;/strong&gt;: Mia Skomski, Stefan Dernbach, Zhao Chen, Christian Møldrup Legaard&lt;/p&gt; &#xA;&lt;h2&gt;Publications&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aps.arxiv.org/abs/2207.04962&#34;&gt;James Koch, Zhao Chen, Aaron Tuor, Jan Drgona, Draguna Vrabie, Structural Inference of Networked Dynamical Systems with Universal Differential Equations, arXiv:2207.04962, (2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2405896322015877&#34;&gt;Ján Drgoňa, Sayak Mukherjee, Aaron Tuor, Mahantesh Halappanavar, Draguna Vrabie, Learning Stochastic Parametric Differentiable Predictive Control Policies, IFAC ROCOND conference (2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.10728&#34;&gt;Sayak Mukherjee, Ján Drgoňa, Aaron Tuor, Mahantesh Halappanavar, Draguna Vrabie, Neural Lyapunov Differentiable Predictive Control, IEEE Conference on Decision and Control Conference 2022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.02319&#34;&gt;Wenceslao Shaw Cortez, Jan Drgona, Aaron Tuor, Mahantesh Halappanavar, Draguna Vrabie, Differentiable Predictive Control with Safety Guarantees: A Control Barrier Function Approach, IEEE Conference on Decision and Control Conference 2022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9867379&#34;&gt;Ethan King, Jan Drgona, Aaron Tuor, Shrirang Abhyankar, Craig Bakker, Arnab Bhattacharya, Draguna Vrabie, Koopman-based Differentiable Predictive Control for the Dynamics-Aware Economic Dispatch Problem, 2022 American Control Conference (ACC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0378778821002760&#34;&gt;Drgoňa, J., Tuor, A. R., Chandan, V., &amp;amp; Vrabie, D. L., Physics-constrained deep learning of multi-zone building thermal dynamics. Energy and Buildings, 243, 110992, (2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9482930&#34;&gt;E. Skomski, S. Vasisht, C. Wight, A. Tuor, J. Drgoňa and D. Vrabie, &#34;Constrained Block Nonlinear Neural Dynamical Models,&#34; 2021 American Control Conference (ACC), 2021, pp. 3993-4000, doi: 10.23919/ACC50511.2021.9482930.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v144/skomski21a.html&#34;&gt;Skomski, E., Drgoňa, J., &amp;amp; Tuor, A. (2021, May). Automating Discovery of Physics-Informed Neural State Space Models via Learning and Evolution. In Learning for Dynamics and Control (pp. 980-991). PMLR.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2405896321012933&#34;&gt;Drgoňa, J., Tuor, A., Skomski, E., Vasisht, S., &amp;amp; Vrabie, D. (2021). Deep Learning Explicit Differentiable Predictive Control Laws for Buildings. IFAC-PapersOnLine, 54(6), 14-19.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.10883&#34;&gt;Tuor, A., Drgona, J., &amp;amp; Vrabie, D. (2020). Constrained neural ordinary differential equations with stability guarantees. arXiv preprint arXiv:2004.10883.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0959152422000981&#34;&gt;Drgona, Jan, et al. &#34;Differentiable Predictive Control: An MPC Alternative for Unknown Nonlinear Systems using Constrained Deep Learning.&#34; Journal of Process Control Volume 116, August 2022, Pages 80-92&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9809789&#34;&gt;Drgona, J., Skomski, E., Vasisht, S., Tuor, A., &amp;amp; Vrabie, D. (2020). Dissipative Deep Neural Dynamical Systems, in IEEE Open Journal of Control Systems, vol. 1, pp. 100-112, 2022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.11184&#34;&gt;Drgona, J., Tuor, A., &amp;amp; Vrabie, D., Learning Constrained Adaptive Differentiable Predictive Control Policies With Guarantees, arXiv preprint arXiv:2004.11184, (2020)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite as&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;@article{Neuromancer2023,&#xA;  title={{NeuroMANCER: Neural Modules with Adaptive Nonlinear Constraints and Efficient Regularizations}},&#xA;  author={Tuor, Aaron and Drgona, Jan and Koch, James and Shapiro, Madelyn and Vrabie, Draguna and Briney, Seth},&#xA;  Url= {https://github.com/pnnl/neuromancer}, &#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This research was partially supported by the Mathematics for Artificial Reasoning in Science (MARS) and Data Model Convergence (DMC) initiatives via the Laboratory Directed Research and Development (LDRD) investments at Pacific Northwest National Laboratory (PNNL), by the U.S. Department of Energy, through the Office of Advanced Scientific Computing Research&#39;s “Data-Driven Decision Control for Complex Systems (DnC2S)” project, and through the Energy Efficiency and Renewable Energy, Building Technologies Office under the “Dynamic decarbonization through autonomous physics-centric deep learning and optimization of building operations” and the “Advancing Market-Ready Building Energy Management by Cost-Effective Differentiable Predictive Control” projects. PNNL is a multi-program national laboratory operated for the U.S. Department of Energy (DOE) by Battelle Memorial Institute under Contract No. DE-AC05-76RL0-1830.&lt;/p&gt;</summary>
  </entry>
</feed>