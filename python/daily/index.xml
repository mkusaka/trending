<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-12T01:43:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jaymody/picoGPT</title>
    <updated>2023-02-12T01:43:54Z</updated>
    <id>tag:github.com,2023-02-12:/jaymody/picoGPT</id>
    <link href="https://github.com/jaymody/picoGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An unnecessarily tiny and minimal implementation of GPT-2 in NumPy.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PicoGPT&lt;/h1&gt; &#xA;&lt;p&gt;You&#39;ve seen &lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;openai/gpt-2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ve seen &lt;a href=&#34;https://github.com/karpathy/mingpt&#34;&gt;karpathy/minGPT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ve even seen &lt;a href=&#34;https://github.com/karpathy/nanogpt&#34;&gt;karpathy/nanoGPT&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;But have you seen &lt;a href=&#34;https://github.com/jaymody/picoGPT&#34;&gt;picoGPT&lt;/a&gt;??!?&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;picoGPT&lt;/code&gt; is an unnecessarily tiny and minimal implementation of &lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;GPT-2&lt;/a&gt; in plain &lt;a href=&#34;https://numpy.org&#34;&gt;NumPy&lt;/a&gt;. The entire forward pass code is &lt;a href=&#34;https://github.com/jaymody/picoGPT/raw/main/gpt2_pico.py#L3-L41&#34;&gt;40 lines of code&lt;/a&gt;. I wrote a related &lt;a href=&#34;https://jaykmody.com/blog/gpt-from-scratch/&#34;&gt;blog post&lt;/a&gt; for picoGPT.&lt;/p&gt; &#xA;&lt;p&gt;picoGPT features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast? ‚ùå Nah, picoGPT is megaSLOW üêå&lt;/li&gt; &#xA; &lt;li&gt;Training code? ‚ùå Error, 4Ô∏è‚É£0Ô∏è‚É£4Ô∏è‚É£ not found&lt;/li&gt; &#xA; &lt;li&gt;Batch inference? ‚ùå picoGPT is civilized, single file line, one at a time only&lt;/li&gt; &#xA; &lt;li&gt;top-p sampling? ‚ùå top-k? ‚ùå temperature? ‚ùå categorical sampling?! ‚ùå greedy? ‚úÖ&lt;/li&gt; &#xA; &lt;li&gt;Readable? &lt;code&gt;gpt2.py&lt;/code&gt; ‚úÖ &lt;code&gt;gpt2_pico.py&lt;/code&gt; ‚ùå&lt;/li&gt; &#xA; &lt;li&gt;Smol??? ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ YESS!!! TEENIE TINY in fact ü§è&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A quick breakdown of each of the files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;encoder.py&lt;/code&gt; contains the code for OpenAI&#39;s BPE Tokenizer, taken straight from their &lt;a href=&#34;https://github.com/openai/gpt-2/raw/master/src/encoder.py&#34;&gt;gpt-2 repo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utils.py&lt;/code&gt; contains the code to download and load the GPT-2 model weights, tokenizer, and hyper-parameters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt2.py&lt;/code&gt; contains the actual GPT model and generation code which we can run as a python script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt2_pico.py&lt;/code&gt; is the same as &lt;code&gt;gpt2.py&lt;/code&gt;, but in even fewer lines of code. Why? Because why not üòéüëç.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Dependencies&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re using an M1 Macbook, you&#39;ll need to replace &lt;code&gt;tensorflow&lt;/code&gt; with &lt;code&gt;tensorflow-macos&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Tested on &lt;code&gt;Python 3.9.10&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gpt2.py &#34;Alan Turing theorized that computers would one day become&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which generates&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; the most powerful machines on the planet.&#xA;&#xA;The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also control the number of tokens to generate, the model size (one of &lt;code&gt;[&#34;124M&#34;, &#34;355M&#34;, &#34;774M&#34;, &#34;1558M&#34;]&lt;/code&gt;), and the directory to save the models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gpt2.py \&#xA;    &#34;Alan Turing theorized that computers would one day become&#34; \&#xA;    --n_tokens_to_generate 40 \&#xA;    --model_size &#34;124M&#34; \&#xA;    --models_dir &#34;models&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>MichalGeyer/plug-and-play</title>
    <updated>2023-02-12T01:43:54Z</updated>
    <id>tag:github.com,2023-02-12:/MichalGeyer/plug-and-play</id>
    <link href="https://github.com/MichalGeyer/plug-and-play" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Pytorch Implementation for ‚ÄúPlug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation‚Äù&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation&lt;/h1&gt; &#xA;&lt;h2&gt;[&lt;a href=&#34;https://pnp-diffusion.github.io/&#34; target=&#34;_blank&#34;&gt;Project Page&lt;/a&gt;]&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12572&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-PnP-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MichalGeyer/plug-and-play/main/assets/teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;To plug-and-play diffusion features, please follow these steps:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichalGeyer/plug-and-play/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichalGeyer/plug-and-play/main/#feature-extraction&#34;&gt;Feature extraction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichalGeyer/plug-and-play/main/#running-pnp&#34;&gt;Running PnP&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODO:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Diffusers support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Our codebase is built on &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;CompVis/stable-diffusion&lt;/a&gt; and has shared dependencies and model architecture.&lt;/p&gt; &#xA;&lt;h3&gt;Creating a Conda Environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate pnp-diffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Downloading StableDiffusion Weights&lt;/h3&gt; &#xA;&lt;p&gt;Download the StableDiffusion weights from the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v-1-4-original&#34;&gt;CompVis organization at HuggingFace&lt;/a&gt; (download the &lt;code&gt;sd-v1-4.ckpt&lt;/code&gt; file), and link them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/stable-diffusion-v1/&#xA;ln -s &amp;lt;path/to/model.ckpt&amp;gt; models/ldm/stable-diffusion-v1/model.ckpt &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setting Experiment Root Path&lt;/h3&gt; &#xA;&lt;p&gt;The data of all the experiments is stored in a root directory. The path of this directory is specified in &lt;code&gt;configs/pnp/setup.yaml&lt;/code&gt;, under the &lt;code&gt;config.exp_path_root&lt;/code&gt; key.&lt;/p&gt; &#xA;&lt;h2&gt;Feature Extraction&lt;/h2&gt; &#xA;&lt;p&gt;For generating and extracting the features of an image, first set the parameters for the translation in a yaml config file. An example of extraction configs can be found in &lt;code&gt;configs/pnp/feature-extraction-generated.yaml&lt;/code&gt; for generated images and in &lt;code&gt;configs/pnp/feature-extraction-real.yaml&lt;/code&gt; for real images. Once the arguments are set, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_features_extraction.py --config &amp;lt;extraction_config_path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For real images, the timesteps at which features are saved are determined by the &lt;code&gt;save_feature_timesteps&lt;/code&gt; argument. Note that for running PnP with &lt;code&gt;T&lt;/code&gt; sampling steps for real images, you need to run the extraction with &lt;code&gt;save_feature_timesteps&lt;/code&gt; = &lt;code&gt;T&lt;/code&gt; (since we&#39;re sampling with 999 steps for reconstructing the real image, we need to specify the timesteps at which features are saved).&lt;/p&gt; &#xA;&lt;p&gt;After running the extraction script, an experiment folder is created in &lt;code&gt;&amp;lt;exp_path_root&amp;gt;/&amp;lt;source_experiment_name&amp;gt;&lt;/code&gt;, where &lt;code&gt;source_experiment_name&lt;/code&gt; is specified by the config file. The experiment directory contains the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- &amp;lt;source_experiment_name&amp;gt;&#xA;    - feature_maps         # contains the extracted features&#xA;    - predicted_samples    # predicted clean images for each sampling timestep&#xA;    - samples              # contains the generated/inverted image&#xA;    - translations         # PnP translation results&#xA;    - z_enc.pt             # the initial noisy latent code&#xA;    - args.json            # the config arguments of the experiment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For visualizing the extracted features, see the &lt;a href=&#34;https://raw.githubusercontent.com/MichalGeyer/plug-and-play/main/#feature-visualization&#34;&gt;Feature Visualization&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Running PnP&lt;/h2&gt; &#xA;&lt;p&gt;For running PnP, first set the parameters for the translation in a yaml config file. An example of PnP config can be found in &lt;code&gt;configs/pnp/pnp-generated.yaml&lt;/code&gt; for generated images and in &lt;code&gt;configs/pnp/pnp-real.yaml&lt;/code&gt; for real images. Once the arguments are set, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_pnp.py --config &amp;lt;pnp_config_path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the config parameters, you can control the following aspects in the translation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Structure preservation&lt;/strong&gt; can be controlled by the &lt;code&gt;feature_injection_threshold&lt;/code&gt; parameter (a higher value allows better structure preservation but can also leak details from the source image, ~80% of the total sampling steps generally gives a good tradeoff).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deviation from the guidance image&lt;/strong&gt; can be controlled through the &lt;code&gt;scale&lt;/code&gt;, &lt;code&gt;negative_prompt_alpha&lt;/code&gt; and &lt;code&gt;negative_prompt_schedule&lt;/code&gt; parameters (see the sample config files for details). The effect of negative prompting is minor in case of realistic guidance images, but it can significantly help in case of minimalistic and abstract guidance images (e.g. segmentations).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that you can run a batch of translations by providing multiple target prompts in the &lt;code&gt;prompts&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;h2&gt;Feature Visualization&lt;/h2&gt; &#xA;&lt;h3&gt;ResBlock Features Visualization&lt;/h3&gt; &#xA;&lt;p&gt;For running PCA visualizations on the extracted ResBlock features (Figure 3 in the paper), first set the parameters for the visualization in a yaml config file. An example of visualization config can be found in &lt;code&gt;configs/pnp/feature-pca-vis.yaml&lt;/code&gt;. Once the arguments are set, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_features_pca.py --config &#34;&amp;lt;pca_vis_config_path&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The feature visualizations are saved under &lt;code&gt;&amp;lt;config.exp_path_root&amp;gt;/PCA_features_vis/&amp;lt;experiment_name&amp;gt;&lt;/code&gt; directory, where &lt;code&gt;&amp;lt;experiment_name&amp;gt;&lt;/code&gt; is specified in the visualization config file.&lt;/p&gt; &#xA;&lt;h3&gt;Self-Attention Visualization&lt;/h3&gt; &#xA;&lt;p&gt;To visualize the self-attention maps of a generated/inverted image (Figure 6 in the paper), run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_self_attn_pca.py --block &#34;&amp;lt;visualization_module_name&amp;gt;&#34; --experiment &#34;&amp;lt;experiment_name&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The self-attention visualizations are saved under &lt;code&gt;&amp;lt;config.exp_path_root&amp;gt;/PCA_self_attention_vis/&amp;lt;experiment_name&amp;gt;&lt;/code&gt; directory.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>acheong08/EdgeGPT</title>
    <updated>2023-02-12T01:43:54Z</updated>
    <id>tag:github.com,2023-02-12:/acheong08/EdgeGPT</id>
    <link href="https://github.com/acheong08/EdgeGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Reverse engineered API of Microsoft&#39;s Bing Chat&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://socialify.git.ci/acheong08/EdgeGPT/image?font=Inter&amp;amp;language=1&amp;amp;logo=https%3A%2F%2Fraw.githubusercontent.com%2FHarry-Jing%2FEdgeGPT%2Fmaster%2F.readme%2FBing_favicon.png&amp;amp;owner=1&amp;amp;pattern=Floating%20Cogs&amp;amp;theme=Auto&#34; alt=&#34;EdgeGPT&#34; width=&#34;640&#34; height=&#34;320&#34;&gt; &#xA; &lt;h1&gt;Edge GPT&lt;/h1&gt; &#xA; &lt;p&gt;&lt;em&gt;The reverse engineering the chat feature of the new version of Bing&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/acheong08/EdgeGPT&#34;&gt; &lt;img alt=&#34;PyPI version&#34; src=&#34;https://img.shields.io/pypi/v/EdgeGPT&#34;&gt; &lt;/a&gt; &lt;img alt=&#34;Python version&#34; src=&#34;https://img.shields.io/badge/python-3.7+-blue.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h2&gt;UPDATE 2023/02/12 - Public access available again&lt;/h2&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#edge-gpt&#34;&gt;Edge GPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#&#34;&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#setup&#34;&gt;Setup&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#install-package&#34;&gt;Install package&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#checking-access-optional&#34;&gt;Checking access (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#getting-authentication-optional&#34;&gt;Getting authentication (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#quick-start&#34;&gt;Quick start&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#developer-demo&#34;&gt;Developer demo&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#work-in-progress&#34;&gt;Work in progress&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/acheong08/EdgeGPT/master/#contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Install package&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m pip install EdgeGPT --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;We have a shared token for public use. If you have your own account with access, you can use that instead.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.7+&lt;/li&gt; &#xA; &lt;li&gt;Microsoft Edge (Optional)&lt;/li&gt; &#xA; &lt;li&gt;A Microsoft Account with early access to &lt;a href=&#34;http://bing.com/chat&#34;&gt;http://bing.com/chat&lt;/a&gt; (Optional)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;h3&gt;Checking access (Optional)&lt;/h3&gt; &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install the latest version of Microsoft Edge&lt;/li&gt; &#xA;  &lt;li&gt;Open &lt;a href=&#34;http://bing.com/chat&#34;&gt;http://bing.com/chat&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;If you see a chat feature, you are good to go&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;h3&gt;Getting authentication (Optional)&lt;/h3&gt; &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Open the developer tools (F12)&lt;/li&gt; &#xA;  &lt;li&gt;Go to the Application tab ‚Üí Storage ‚Üí Cookies&lt;/li&gt; &#xA;  &lt;li&gt;Find the cookie named &#34;_U&#34;&lt;/li&gt; &#xA;  &lt;li&gt;Copy the value of the cookie&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Quick start&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt; $ python3 -m EdgeGPT -h&#xA;&#xA;        EdgeGPT - A demo of reverse engineering the Bing GPT chatbot&#xA;        Repo: github.com/acheong08/EdgeGPT&#xA;        By: Antonio Cheong&#xA;&#xA;        !help for help&#xA;&#xA;        Type !exit to exit&#xA;        Enter twice to send message&#xA;&#xA;usage: EdgeGPT.py [-h] [--no-stream] [--bing-cookie BING_COOKIE]&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --no-stream&#xA;  --bing-cookie BING_COOKIE (Optional)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Developer demo&lt;/h3&gt; &#xA;&lt;p&gt;Use Async for the best experience&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/acheong08/EdgeGPT/raw/master/src/EdgeGPT.py#L268-L328&#34;&gt;Reference code&lt;/a&gt; for more advanced example of usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from EdgeGPT import Chatbot&#xA;&#xA;async def main():&#xA;    bot = Chatbot()&#xA;    print(await bot.ask(prompt=&#34;Hello world&#34;))&#xA;    await bot.close()&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Work in progress&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Error handling&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;This project exists thanks to all the people who contribute.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pig#8932 (Discord) - Sharing account with beta access&lt;/li&gt; &#xA; &lt;li&gt;ulysses115#7373 (Discord) - Sharing account with beta access&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jimmy-Z&#34;&gt;Jimmy-Z&lt;/a&gt; - Bugfixes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;a href=&#34;https://github.com/acheong08/EdgeGPT/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=acheong08/EdgeGPT&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>