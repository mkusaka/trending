<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-18T01:41:18Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Winfredy/SadTalker</title>
    <updated>2023-03-18T01:41:18Z</updated>
    <id>tag:github.com,2023-03-18:/Winfredy/SadTalker</id>
    <link href="https://github.com/Winfredy/SadTalker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;（CVPR 2023）SadTalker：Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt; 😭 SadTalker： &lt;span style=&#34;font-size:12px&#34;&gt;Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation &lt;/span&gt; &lt;/h2&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12194&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ArXiv-2211.14758-red&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://sadtalker.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;div&gt; &#xA;  &lt;a target=&#34;_blank&#34;&gt;Wenxuan Zhang &lt;sup&gt;*,1,2&lt;/sup&gt; &lt;/a&gt;  &#xA;  &lt;a href=&#34;https://vinthony.github.io/&#34; target=&#34;_blank&#34;&gt;Xiaodong Cun &lt;sup&gt;*,2&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://xuanwangvc.github.io/&#34; target=&#34;_blank&#34;&gt;Xuan Wang &lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://yzhang2016.github.io/&#34; target=&#34;_blank&#34;&gt;Yong Zhang &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://xishen0220.github.io/&#34; target=&#34;_blank&#34;&gt;Xi Shen &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://yuguo-xjtu.github.io/&#34; target=&#34;_blank&#34;&gt;Yu Guo&lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;  &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=4oXBp9UAAAAJ&#34; target=&#34;_blank&#34;&gt;Ying Shan &lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;  &#xA;  &lt;a target=&#34;_blank&#34;&gt;Fei Wang &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;  &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;sup&gt;1&lt;/sup&gt; Xi&#39;an Jiaotong University   &#xA;  &lt;sup&gt;2&lt;/sup&gt; Tencent AI Lab   &#xA;  &lt;sup&gt;3&lt;/sup&gt; Ant Group   &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;i&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12194&#34; target=&#34;_blank&#34;&gt;CVPR 2023&lt;/a&gt;&lt;/strong&gt;&lt;/i&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4397546/222490039-b1f6156b-bf00-405b-9fda-0c9a9156f991.gif&#34; alt=&#34;sadtalker&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;TL;DR: A realistic and stylized talking head video generation method from a single image and audio&lt;/p&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;📋 Changelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.03.14 Specify the version of package &lt;code&gt;joblib&lt;/code&gt; to remove the errors in using &lt;code&gt;librosa&lt;/code&gt;, &lt;a href=&#34;https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; is online!&lt;/li&gt; &#xA; &lt;li&gt;2023.03.06 Solve some bugs in code and errors in installation&lt;/li&gt; &#xA; &lt;li&gt;2023.03.03 Release the test code for audio-driven single image animation!&lt;/li&gt; &#xA; &lt;li&gt;2023.02.28 SadTalker has been accepted by CVPR 2023!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎼 Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4397546/222490596-4c8a2115-49a7-42ad-a2c3-3bb3288a5f36.png&#34; alt=&#34;main_of_sadtalker&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🚧 TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generating 2D face from a single Image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generating 3D face from Audio.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generating 4D free-view talking examples from audio and a single image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio/Colab Demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; training code of each componments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Audio-driven Anime Avatar.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; interpolate ChatGPT for a conversation demo 🤔&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; integrade with stable-diffusion-web-ui. (stay tunning!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/4397546/222513483-89161f58-83d0-40e4-8e41-96c32b47bd4e.mp4&#34;&gt;https://user-images.githubusercontent.com/4397546/222513483-89161f58-83d0-40e4-8e41-96c32b47bd4e.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔮 Inference Demo!&lt;/h2&gt; &#xA;&lt;h4&gt;Requirements&lt;/h4&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Python 3.8&lt;/li&gt; &#xA;  &lt;li&gt;PyTorch&lt;/li&gt; &#xA;  &lt;li&gt;ffmpeg&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Dependence Installation&lt;/h4&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;git clone https://github.com/Winfredy/SadTalker.git&#xA;cd SadTalker &#xA;conda create -n sadtalker python=3.8&#xA;source activate sadtalker&#xA;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;conda config --add channels conda-forge&#xA;conda install ffmpeg&#xA;pip install ffmpy&#xA;pip install Cmake&#xA;pip install boost&#xA;conda install dlib&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Trained Models&lt;/h4&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;p&gt;Please download our pre-trained model from &lt;a href=&#34;https://drive.google.com/drive/folders/1Wd88VDoLhVzYsQ30_qDVluQr_Xm46yHT?usp=sharing&#34;&gt;google drive&lt;/a&gt; or our &lt;a href=&#34;https://github.com/Winfredy/SadTalker/releases/tag/v0.0.1&#34;&gt;github release page&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;and then, put it in ./checkpoints.&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/auido2exp_00300-model.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained ExpNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/auido2pose_00140-model.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained PoseVAE in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00229-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/facevid2vid_00189-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained face-vid2vid model from &lt;a href=&#34;https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis&#34;&gt;the reappearance of face-vid2vid&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/epoch_20.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained 3DMM extractor in &lt;a href=&#34;https://github.com/microsoft/Deep3DFaceReconstruction&#34;&gt;Deep3DFaceReconstruction&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/wav2lip.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Highly accurate lip-sync model in &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2lip&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/shape_predictor_68_face_landmarks.dat&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face landmark model used in &lt;a href=&#34;http://dlib.net/&#34;&gt;dilb&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/BFM&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;3DMM library file.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/hub&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face detection models used in &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face alignment&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Generating 2D face from a single Image&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --driven_audio &amp;lt;audio.wav&amp;gt; \&#xA;                    --source_image &amp;lt;video.mp4 or picture.png&amp;gt; \&#xA;                    --result_dir &amp;lt;a file to store results&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Generating 3D face from Audio&lt;/h4&gt; &#xA;&lt;p&gt;To do ...&lt;/p&gt; &#xA;&lt;h4&gt;Generating 4D free-view talking examples from audio and a single image&lt;/h4&gt; &#xA;&lt;p&gt;We use &lt;code&gt;camera_yaw&lt;/code&gt;, &lt;code&gt;camera_pitch&lt;/code&gt;, &lt;code&gt;camera_roll&lt;/code&gt; to control camera pose. For example, &lt;code&gt;--camera_yaw -20 30 10&lt;/code&gt; means the camera yaw degree changes from -20 to 30 and then changes from 30 to 10.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --driven_audio &amp;lt;audio.wav&amp;gt; \&#xA;                    --source_image &amp;lt;video.mp4 or picture.png&amp;gt; \&#xA;                    --result_dir &amp;lt;a file to store results&amp;gt; \&#xA;                    --camera_yaw -20 30 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Winfredy/SadTalker/raw/main/free_view_result.gif&#34; alt=&#34;free_view&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🛎 Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhang2022sadtalker,&#xA;  title={SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation},&#xA;  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},&#xA;  journal={arXiv preprint arXiv:2211.12194},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;💗 Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Facerender code borrows heavily from &lt;a href=&#34;https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis&#34;&gt;zhanglonghao&#39;s reproduction of face-vid2vid&lt;/a&gt; and &lt;a href=&#34;https://github.com/RenYurui/PIRender&#34;&gt;PIRender&lt;/a&gt;. We thank the authors for sharing their wonderful code. In training process, We also use the model from &lt;a href=&#34;https://github.com/microsoft/Deep3DFaceReconstruction&#34;&gt;Deep3DFaceReconstruction&lt;/a&gt; and &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2lip&lt;/a&gt;. We thank for their wonderful work.&lt;/p&gt; &#xA;&lt;h2&gt;🥂 Related Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/StyleHEAT&#34;&gt;StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN (ECCV 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Doubiiu/CodeTalker&#34;&gt;CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vinthony/video-retalking&#34;&gt;VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild (SIGGRAPH Asia 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.06281&#34;&gt;DPE: Disentanglement of Pose and Expression for General Video Portrait Editing (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/SPI/&#34;&gt;3D GAN Inversion with Facial Symmetry Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mael-zys/T2M-GPT&#34;&gt;T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📢 Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an official product of Tencent. This repository can only be used for personal/research/non-commercial purposes.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ssbuild/chatglm_finetuning</title>
    <updated>2023-03-18T01:41:18Z</updated>
    <id>tag:github.com,2023-03-18:/ssbuild/chatglm_finetuning</id>
    <link href="https://github.com/ssbuild/chatglm_finetuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;chatglm 6b 大模型微调&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;状态 - 测试中&lt;/h1&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pip install -U deep_training &amp;gt;= 0.0.18 cpm_kernels icetk transformers&amp;gt;=4.26.1 deepspeed&lt;/li&gt; &#xA; &lt;li&gt;最小版本要求 deep_training&amp;gt;=0.0.18.post5&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;更新详情&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ssbuild/deep_training&#34;&gt;deep_training&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;深度学习常规任务例子&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ssbuild/pytorch-task-example&#34;&gt;pytorch-task-example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ssbuild/tf-task-example&#34;&gt;tf-task-example&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ChatGLM 预训练权重&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://huggingface.co/THUDM/chatglm-6b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;数据示例&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;给出一种qa 数据示例 , 如果想跟母模型保持一致，可以参考https://huggingface.co/THUDM/chatglm-6b 数据组织结构.&#xA;单条数据示例1&#xA;{&#xA;    &#34;id&#34;: 0, &#34;paragraph&#34;: [&#xA;        #一轮会话&#xA;        {&#xA;            &#34;q&#34;: &#34;从南京到上海的路线&#34;,&#xA;            &#34;a&#34;: [&#xA;                &#34;你好，南京到上海的路线如下：&#34;,&#xA;                &#34;1. 南京到上海，可以乘坐南京地铁1号线，在南京站乘坐轨道交通1号线。&#34;,&#xA;                &#34;2. 南京到浦东机场，可以搭乘上海地铁1号，在陆家嘴站乘坐地铁1线，在浦东国际机场站乘坐机场快线，前往上海浦东国际机场。&#34;,&#xA;                &#34;3. 上海到南京，可以换乘上海地铁2号线，从南京站换乘地铁2线，再从南京南站换乘地铁1路，然后到达上海站&#34;&#xA;            ]&#xA;        }&#xA;        #二轮....&#xA;    ]&#xA;}&#xA;单条数据示例2&#xA;{&#34;id&#34;: 0, &#34;paragraph&#34;: [&#xA;&#xA;    {&#xA;        &#34;q&#34;: &#34;写一个诗歌，关于冬天&#34;,&#xA;        &#34;a&#34;: [&#xA;            &#34;冬夜寂静冷，&#34;,&#xA;             &#34;云在天边飘，&#34;, &#34;冰封白雪上， &#34;, &#34;寒冷像一场雪。&#34;,&#xA;             &#34; &#34;,&#xA;             &#34;雪花融化成冰，&#34;,&#xA;             &#34;像那雪花飘洒，&#34;,&#xA;             &#34;在寒冷的冬天，&#34;,&#xA;             &#34;感受春天的喜悦。&#34;,&#xA;             &#34; 冬日里，&#34;,&#xA;             &#34;风雪渐消，&#34;,&#xA;             &#34;一片寂静，&#34;,&#xA;             &#34;把快乐和温暖带回家。&#34;&#xA;        ]&#xA;    }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;生成训练record&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python data_utils.py&#xA;&#xA;注:&#xA;num_process_worker 为多进程制作数据 ， 如果数据量较大 ， 适当调大至cpu数量&#xA;dataHelper.make_dataset_with_args(data_args.train_file,mixed_data=False, shuffle=True,mode=&#39;train&#39;,num_process_worker=0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;推理&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt; python infer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;硬件需求&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;量化等级&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;最低 GPU 显存&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP16（无量化）&lt;/td&gt; &#xA;   &lt;td&gt;13 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;10 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;6 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ssbuild/chatglm_finetuning/main/1.png&#34; alt=&#34;inference&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;训练&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;完整参数为 config.json&#xA;若显存不足 ， 可以修改 config_small.json num_layers 层数&#xA;训练精度 可以修改 config_small.json precision 16 32&#xA;python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;是否开启deepspeed&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;启动则将train.py  注释掉 deepspeed_config = None&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/THUDM/ChatGLM-6B&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/vision</title>
    <updated>2023-03-18T01:41:18Z</updated>
    <id>tag:github.com,2023-03-18:/pytorch/vision</id>
    <link href="https://github.com/pytorch/vision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Datasets, Transforms and Models specific to Computer Vision&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;torchvision&lt;/h1&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://pepy.tech/badge/torchvision&#34;&gt;https://pepy.tech/badge/torchvision&lt;/a&gt; :target: &lt;a href=&#34;https://pepy.tech/project/torchvision&#34;&gt;https://pepy.tech/project/torchvision&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/badge/dynamic/json.svg?label=docs&amp;amp;url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&amp;amp;query=%24.info.version&amp;amp;colorB=brightgreen&amp;amp;prefix=v&#34;&gt;https://img.shields.io/badge/dynamic/json.svg?label=docs&amp;amp;url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&amp;amp;query=%24.info.version&amp;amp;colorB=brightgreen&amp;amp;prefix=v&lt;/a&gt; :target: &lt;a href=&#34;https://pytorch.org/vision/stable/index.html&#34;&gt;https://pytorch.org/vision/stable/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;We recommend Anaconda as Python package management system. Please refer to &lt;code&gt;pytorch.org &amp;lt;https://pytorch.org/&amp;gt;&lt;/code&gt;_ for the detail of PyTorch (&lt;code&gt;torch&lt;/code&gt;) installation. The following is the corresponding &lt;code&gt;torchvision&lt;/code&gt; versions and supported Python versions.&lt;/p&gt; &#xA;&lt;p&gt;+--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;torch&lt;/code&gt; | &lt;code&gt;torchvision&lt;/code&gt; | &lt;code&gt;python&lt;/code&gt; | +==========================+==========================+=================================+ | &lt;code&gt;main&lt;/code&gt; / &lt;code&gt;nightly&lt;/code&gt; | &lt;code&gt;main&lt;/code&gt; / &lt;code&gt;nightly&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.11&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;2.0.0&lt;/code&gt; | &lt;code&gt;0.15.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.11&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.13.0&lt;/code&gt; | &lt;code&gt;0.14.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.7.2&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.10&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.12.0&lt;/code&gt; | &lt;code&gt;0.13.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.7&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.10&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.11.0&lt;/code&gt; | &lt;code&gt;0.12.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.7&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.10&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.10.2&lt;/code&gt; | &lt;code&gt;0.11.3&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.10.1&lt;/code&gt; | &lt;code&gt;0.11.2&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.10.0&lt;/code&gt; | &lt;code&gt;0.11.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.9.1&lt;/code&gt; | &lt;code&gt;0.10.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.9.0&lt;/code&gt; | &lt;code&gt;0.10.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.8.2&lt;/code&gt; | &lt;code&gt;0.9.2&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.8.1&lt;/code&gt; | &lt;code&gt;0.9.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.8.0&lt;/code&gt; | &lt;code&gt;0.9.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.7.1&lt;/code&gt; | &lt;code&gt;0.8.2&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.7.0&lt;/code&gt; | &lt;code&gt;0.8.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.7.0&lt;/code&gt; | &lt;code&gt;0.8.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.6.0&lt;/code&gt; | &lt;code&gt;0.7.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.5.1&lt;/code&gt; | &lt;code&gt;0.6.1&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.5.0&lt;/code&gt; | &lt;code&gt;0.6.0&lt;/code&gt; | &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.4.0&lt;/code&gt; | &lt;code&gt;0.5.0&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.3.1&lt;/code&gt; | &lt;code&gt;0.4.2&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.3.0&lt;/code&gt; | &lt;code&gt;0.4.1&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.2.0&lt;/code&gt; | &lt;code&gt;0.4.0&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;1.1.0&lt;/code&gt; | &lt;code&gt;0.3.0&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+ | &lt;code&gt;&amp;lt;=1.0.1&lt;/code&gt; | &lt;code&gt;0.2.2&lt;/code&gt; | &lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt; | +--------------------------+--------------------------+---------------------------------+&lt;/p&gt; &#xA;&lt;p&gt;Anaconda:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install torchvision -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;pip:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install torchvision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From source:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py install&#xA;# or, for OSX&#xA;# MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We don&#39;t officially support building from source using &lt;code&gt;pip&lt;/code&gt;, but &lt;em&gt;if&lt;/em&gt; you do, you&#39;ll need to use the &lt;code&gt;--no-build-isolation&lt;/code&gt; flag. In case building TorchVision from source fails, install the nightly version of PyTorch following the linked guide on the &lt;code&gt;contributing page &amp;lt;https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md#development-installation&amp;gt;&lt;/code&gt;_ and retry the install.&lt;/p&gt; &#xA;&lt;p&gt;By default, GPU support is built if CUDA is found and &lt;code&gt;torch.cuda.is_available()&lt;/code&gt; is true. It&#39;s possible to force building GPU support by setting &lt;code&gt;FORCE_CUDA=1&lt;/code&gt; environment variable, which is useful when building a docker image.&lt;/p&gt; &#xA;&lt;h1&gt;Image Backend&lt;/h1&gt; &#xA;&lt;p&gt;Torchvision currently supports the following image backends:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Pillow&lt;/code&gt;_ (default)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Pillow-SIMD&lt;/code&gt;_ - a &lt;strong&gt;much faster&lt;/strong&gt; drop-in replacement for Pillow with SIMD. If installed will be used as the default.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;accimage&lt;/code&gt;_ - if installed can be activated by calling :code:&lt;code&gt;torchvision.set_image_backend(&#39;accimage&#39;)&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;libpng&lt;/code&gt;_ - can be installed via conda :code:&lt;code&gt;conda install libpng&lt;/code&gt; or any of the package managers for debian-based and RHEL-based Linux distributions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;libjpeg&lt;/code&gt;_ - can be installed via conda :code:&lt;code&gt;conda install jpeg&lt;/code&gt; or any of the package managers for debian-based and RHEL-based Linux distributions. &lt;code&gt;libjpeg-turbo&lt;/code&gt;_ can be used as well.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt; &lt;code&gt;libpng&lt;/code&gt; and &lt;code&gt;libjpeg&lt;/code&gt; must be available at compilation time in order to be available. Make sure that it is available on the standard library locations, otherwise, add the include and library paths in the environment variables &lt;code&gt;TORCHVISION_INCLUDE&lt;/code&gt; and &lt;code&gt;TORCHVISION_LIBRARY&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;p&gt;.. _libpng : &lt;a href=&#34;http://www.libpng.org/pub/png/libpng.html&#34;&gt;http://www.libpng.org/pub/png/libpng.html&lt;/a&gt; .. _Pillow : &lt;a href=&#34;https://python-pillow.org/&#34;&gt;https://python-pillow.org/&lt;/a&gt; .. _Pillow-SIMD : &lt;a href=&#34;https://github.com/uploadcare/pillow-simd&#34;&gt;https://github.com/uploadcare/pillow-simd&lt;/a&gt; .. _accimage: &lt;a href=&#34;https://github.com/pytorch/accimage&#34;&gt;https://github.com/pytorch/accimage&lt;/a&gt; .. _libjpeg: &lt;a href=&#34;http://ijg.org/&#34;&gt;http://ijg.org/&lt;/a&gt; .. _libjpeg-turbo: &lt;a href=&#34;https://libjpeg-turbo.org/&#34;&gt;https://libjpeg-turbo.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Video Backend&lt;/h1&gt; &#xA;&lt;p&gt;Torchvision currently supports the following video backends:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pyav&lt;/code&gt;_ (default) - Pythonic binding for ffmpeg libraries.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;.. _pyav : &lt;a href=&#34;https://github.com/PyAV-Org/PyAV&#34;&gt;https://github.com/PyAV-Org/PyAV&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;video_reader - This needs ffmpeg to be installed and torchvision to be built from source. There shouldn&#39;t be any conflicting version of ffmpeg installed. Currently, this is only supported on Linux.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; conda install -c conda-forge ffmpeg&#xA; python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Using the models on C++&lt;/h1&gt; &#xA;&lt;p&gt;TorchVision provides an example project for how to use the models on C++ using JIT Script.&lt;/p&gt; &#xA;&lt;p&gt;Installation From source:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir build&#xA;cd build&#xA;# Add -DWITH_CUDA=on support for the CUDA if needed&#xA;cmake ..&#xA;make&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once installed, the library can be accessed in cmake (after properly configuring &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt;) via the :code:&lt;code&gt;TorchVision::TorchVision&lt;/code&gt; target:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: rest&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;find_package(TorchVision REQUIRED)&#xA;target_link_libraries(my-target PUBLIC TorchVision::TorchVision)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;TorchVision&lt;/code&gt; package will also automatically look for the &lt;code&gt;Torch&lt;/code&gt; package and add it as a dependency to &lt;code&gt;my-target&lt;/code&gt;, so make sure that it is also available to cmake via the &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For an example setup, take a look at &lt;code&gt;examples/cpp/hello_world&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Python linking is disabled by default when compiling TorchVision with CMake, this allows you to run models without any Python dependency. In some special cases where TorchVision&#39;s operators are used from Python code, you may need to link to Python. This can be done by passing &lt;code&gt;-DUSE_PYTHON=on&lt;/code&gt; to CMake.&lt;/p&gt; &#xA;&lt;h2&gt;TorchVision Operators&lt;/h2&gt; &#xA;&lt;p&gt;In order to get the torchvision operators registered with torch (eg. for the JIT), all you need to do is to ensure that you :code:&lt;code&gt;#include &amp;lt;torchvision/vision.h&amp;gt;&lt;/code&gt; in your project.&lt;/p&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;You can find the API documentation on the pytorch website: &lt;a href=&#34;https://pytorch.org/vision/stable/index.html&#34;&gt;https://pytorch.org/vision/stable/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;See the &lt;code&gt;CONTRIBUTING &amp;lt;CONTRIBUTING.md&amp;gt;&lt;/code&gt;_ file for how to help out.&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer on Datasets&lt;/h1&gt; &#xA;&lt;p&gt;This is a utility library that downloads and prepares public datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have license to use the dataset. It is your responsibility to determine whether you have permission to use the dataset under the dataset&#39;s license.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re a dataset owner and wish to update any part of it (description, citation, etc.), or do not want your dataset to be included in this library, please get in touch through a GitHub issue. Thanks for your contribution to the ML community!&lt;/p&gt; &#xA;&lt;h1&gt;Pre-trained Model License&lt;/h1&gt; &#xA;&lt;p&gt;The pre-trained models provided in this library may have their own licenses or terms and conditions derived from the dataset used for training. It is your responsibility to determine whether you have permission to use the models for your use case.&lt;/p&gt; &#xA;&lt;p&gt;More specifically, SWAG models are released under the CC-BY-NC 4.0 license. See &lt;code&gt;SWAG LICENSE &amp;lt;https://github.com/facebookresearch/SWAG/blob/main/LICENSE&amp;gt;&lt;/code&gt;_ for additional details.&lt;/p&gt; &#xA;&lt;h1&gt;Citing TorchVision&lt;/h1&gt; &#xA;&lt;p&gt;If you find TorchVision useful in your work, please consider citing the following BibTeX entry:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bibtex&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{torchvision2016,&#xA;    title        = {TorchVision: PyTorch&#39;s Computer Vision library},&#xA;    author       = {TorchVision maintainers and contributors},&#xA;    year         = 2016,&#xA;    journal      = {GitHub repository},&#xA;    publisher    = {GitHub},&#xA;    howpublished = {\url{https://github.com/pytorch/vision}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>