<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-14T01:41:06Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>barry-far/V2ray-Configs</title>
    <updated>2024-04-14T01:41:06Z</updated>
    <id>tag:github.com,2024-04-14:/barry-far/V2ray-Configs</id>
    <link href="https://github.com/barry-far/V2ray-Configs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üõ∞Ô∏è‚ú® Free V2ray Configs , Updating Every 10 minutes.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üîí Free V2ray Config üåê&lt;/h1&gt; &#xA;&lt;p&gt;üíª This repository contains a collection of free V2ray configuration files that you can use with your V2ray client to access the internet securely and anonymously.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/barry-far/V2ray-Configs/raw/main/Persian-README.md&#34;&gt;Ÿàÿ±⁄òŸÜ ŸÅÿßÿ±ÿ≥€å&lt;/a&gt; | &lt;a href=&#34;https://github.com/barry-far/V2ray-Configs/raw/main/Chinese-README.md&#34;&gt;‰∏≠ÊñáÁâà&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/barry-far/V2ray-Configs.svg?sanitize=true&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;a href=&#34;https://lbesson.mit-license.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;MIT license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/barry-far/V2ray-Configs/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/barry-far/V2ray-Configs.svg?sanitize=true&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/barry-far/V2ray-Configs/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/barry-far/V2ray-Configs/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;Update Configs&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/repo-size/barry-far/V2ray-Configs&#34; alt=&#34;GitHub repo size&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;üëâ To use one of these configurations, simply open the configs.txt file located in the root directory. This file contains a list of links to various 2ray configuration files that you can use with your V2ray client. Copy &lt;a href=&#34;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/All_Configs_Sub.txt&#34;&gt;THIS LINK&lt;/a&gt; link of the configuration file you want to use from the configs.txt file and import it into your V2ray client. Refer to your client&#39;s documentation for more information on how to do this.&lt;/p&gt; &#xA;&lt;p&gt;‚ùïIf upper link didnt work for you , use this link instead : &lt;a href=&#34;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/All_Configs_base64_Sub.txt&#34;&gt;All_Config_base64_Sub.txt&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just Copy these links to your client to use :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 1:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub1.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 2:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub2.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 3:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub3.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 4:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub4.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 5:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub5.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 6:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub6.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 7:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub7.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 8:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub8.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 9:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub9.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Subscription 10:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub10.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üîí To ensure the best possible performance and security, we recommend subscribing to our latest configurations using the subscription link feature. Simply copy the subscription link from the configs.txt file and paste it into your V2ray client to receive automatic updates every 10 minutes.&lt;/p&gt; &#xA;&lt;p&gt;üóÇÔ∏è For easier configuration management, we have split the configuration files into individual files for each protocol and added them to the Splitted folder. To have easy access to separated Configs use this :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vmess:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Splitted-By-Protocol/vmess.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vless:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Splitted-By-Protocol/vless.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Trojan:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Splitted-By-Protocol/trojan.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ShadowSocks:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Splitted-By-Protocol/ss.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ShadowSocksR:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Splitted-By-Protocol/ssr.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tuic:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Splitted-By-Protocol/tuic.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;hy2:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Splitted-By-Protocol/hysteria2.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Warp (Only Hiddify):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Warp_sub.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‚ùïIf upper links didnt work for you , Checkout this folder: &lt;a href=&#34;https://github.com/barry-far/V2ray-Configs/tree/dev/Base64&#34;&gt;Base64 Folder&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ÑπÔ∏è Additionally, we have created a Wiki with detailed instructions on how to set up V2ray on different operating systems. Visit our Wiki for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements and How to Use&lt;/h2&gt; &#xA;&lt;p&gt;üì≤ Here are best ways to use V2ray on different operating systems:&lt;/p&gt; &#xA;&lt;h3&gt;üíª Windows and üêß Linux&lt;/h3&gt; &#xA;&lt;p&gt;On Windows, we recommend using &lt;a href=&#34;https://github.com/hiddify/hiddify-next&#34;&gt;Hiddify-Next&lt;/a&gt; or &lt;a href=&#34;https://github.com/MatsuriDayo/nekoray&#34;&gt;Nekoray&lt;/a&gt; client. To import a configuration file in &lt;code&gt;nekoray&lt;/code&gt;, simply open the app then on the Program icon, select &#34;Add profile from clipboard&#34;, and paste the configuration link. To import a configuration file in &lt;code&gt;Hiddify&lt;/code&gt;, simply click on the + icon at top right, select &#34;Add from clipboard&#34;, and paste the configuration link.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiddify/Hiddify-Manager/wiki/Tutorial-for-HiddifyNext-app#adding-a-profile-to-the-app&#34;&gt;Visual guide Hiddify&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ü§ñ Android&lt;/h3&gt; &#xA;&lt;p&gt;On Android, we recommend using &lt;a href=&#34;https://github.com/2dust/v2rayNG&#34;&gt;V2rayNG&lt;/a&gt; or &lt;a href=&#34;https://github.com/hiddify/hiddify-next/releases&#34;&gt;HiddifyNext&lt;/a&gt;. To import a configuration file, open the app and click on the &#34;+&#34; icon in the bottom right corner. Then, select &#34;Import Config from Clipboard&#34; and paste the configuration link.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiddify/Hiddify-Manager/wiki/Tutorial-for-HiddifyNext-app#adding-a-profile-to-the-app&#34;&gt;Visual guide Hiddify&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiddify/Hiddify-Manager/wiki/Tutorial-for-V2rayNG-app#add-configs-to-the-app&#34;&gt;Visual guide V2rayNG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üçé Mac and üì± iOS&lt;/h3&gt; &#xA;&lt;p&gt;On you Apple Device, we recommend using the &lt;a href=&#34;https://apps.apple.com/us/app/streisand/id6450534064&#34;&gt;Streisand&lt;/a&gt;. To import the subscription link, simply open the app then at the yop of the page click &#34;+&#34; and choose &#34;import from clipboard&#34; then test configs with holding &#34;subscription&#34; part and choose &#34;Latency&#34; to check all configs. At the end , choose first config &amp;amp; connect.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiddify/Hiddify-Manager/wiki/Tutorial-for-Streisand#add-subscription-link&#34;&gt;Visual guide Streisand&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also use these apps : &lt;a href=&#34;https://apps.apple.com/ca/app/shadowrocket/id932747118&#34;&gt;ShadowRocket&lt;/a&gt; , &lt;a href=&#34;https://apps.apple.com/us/app/v2box-v2ray-client/id6446814690&#34;&gt;V2BOX&lt;/a&gt; , &lt;a href=&#34;https://github.com/hiddify/hiddify-next/releases&#34;&gt;HiddifyNext&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiddify/Hiddify-Manager/wiki/Tutorial-for-ShadowRocket-app#add-subscription-link-to-the-app&#34;&gt;Visual guide ShadowRocket&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiddify/Hiddify-Manager/wiki/Tutorial-for-V2Box-app#add-subscription-links-to-the-app&#34;&gt;Visual guide V2BOX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;‚úçÔ∏è If you have a V2ray configuration file that you would like to contribute to this repository, simply create a new pull request and we will review it.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;üìù This repository is licensed under the MIT license. See LICENSE for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;üôã‚Äç‚ôÄÔ∏è If you have any questions or concerns, please feel free to reach out to us via our discussions tracker.&lt;/p&gt; &#xA;&lt;h2&gt;Special Thanks To&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/MrPooyaX&#34;&gt;@ircfspace&lt;/a&gt; &lt;a href=&#34;https://github.com/hossein-mohseni&#34;&gt;@hossein-mohseni&lt;/a&gt; &lt;a href=&#34;https://github.com/mahdibland&#34;&gt;@mahdibland&lt;/a&gt; &lt;a href=&#34;https://github.com/MrPooyaX&#34;&gt;@MrPooyaX&lt;/a&gt; &lt;a href=&#34;https://github.com/ALIILAPRO&#34;&gt;@ALIILAPRO&lt;/a&gt; &lt;a href=&#34;https://github.com/yebekhe&#34;&gt;@yebekhe&lt;/a&gt; &lt;a href=&#34;https://github.com/soroushmirzaei&#34;&gt;@soroushmirzaei&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>VRSEN/agency-swarm-lab</title>
    <updated>2024-04-14T01:41:06Z</updated>
    <id>tag:github.com,2024-04-14:/VRSEN/agency-swarm-lab</id>
    <link href="https://github.com/VRSEN/agency-swarm-lab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agency Swarm Lab&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Agency Swarm Lab repository! This is a collaborative space where we showcase the incredible capabilities of custom AI agent teams developed using the &lt;a href=&#34;https://github.com/VRSEN/agency-swarm&#34;&gt;Agency Swarm&lt;/a&gt; framework.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Local Installation&lt;/h3&gt; &#xA;&lt;p&gt;To get started with creating your own custom AI agency using the Agency Swarm Lab, follow these detailed steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/VRSEN/agency-swarm-lab.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Global Requirements&lt;/strong&gt;: Navigate to the root directory of the cloned repository and install the global requirements using the &lt;code&gt;requirements.txt&lt;/code&gt; file. This will set up the necessary environment for running the Agency Swarm framework.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd agency-swarm-lab&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Choose an Agency&lt;/strong&gt;: Decide which agency you would like to run or explore. Each agency is contained in its own folder within the repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Agency-Specific Requirements&lt;/strong&gt;: Navigate into the directory of the agency you&#39;ve chosen. Each agency may have its own &lt;code&gt;requirements.txt&lt;/code&gt; file, which specifies additional dependencies necessary for that particular agency.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd path/to/your-chosen-agency&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set Up the &lt;code&gt;.env&lt;/code&gt; File&lt;/strong&gt;: All agencies in the Agency Swarm Lab utilize OpenAI&#39;s API for their operations. To enable this functionality, you must provide your OpenAI API key.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the chosen agency&#39;s folder.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Add your OpenAI API key to this file as follows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&#39;your_openai_api_key_here&#39;&#xA;# ...add other environment variables here if needed&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Dropping this &lt;code&gt;.env&lt;/code&gt; file into the agency folder allows the system to authenticate with OpenAI&#39;s services seamlessly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Your Agency&lt;/strong&gt;: With the environment properly set up, you are now ready to activate your agency. Execute the following command within the agency&#39;s directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python agency.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command starts the operation of your custom AI agency, demonstrating the collaborative power of AI agents in accomplishing complex tasks.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Docker Installation (recommended)&lt;/h3&gt; &#xA;&lt;p&gt;Running agencies in docker is safer as it does not affect your local file system. You will need to ensure that you had docker installed. For installation instructions, please refer to the &lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;official Docker documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone and navigate into the Repository&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/VRSEN/agency-swarm-lab.git&#xA;cd agency-swarm-lab&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Build the Docker Image&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t vrsen/agency-swarm -f path/to/your/Dockerfile .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command breakdown is as follows:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;-t vrsen/agency-swarm&lt;/code&gt; is the name you give to the Docker image that you are generating.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;-f path/to/your/Dockerfile&lt;/code&gt; specifies the path to the Dockerfile that you will use to build the image. The Dockerfile is located in the root of the repository.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the Docker Image&lt;/strong&gt;: Use this command from the root of the repository. Make sure to replace &lt;code&gt;&amp;lt;YourOpenAIKey&amp;gt;&lt;/code&gt; with your actual OpenAI API key.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it -v ./:/app --rm -p 7860:7860 -e OPENAI_API_KEY=&amp;lt;YourOpenAIKey&amp;gt; vrsen/agency-swarm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command breakdown is as follows:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;-it&lt;/code&gt; is used to start an interactive session with the Docker container.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--rm&lt;/code&gt; is used to delete the container after you have finished using it (any files in your mapped folder will be safe).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;-p 7860:7860&lt;/code&gt; port forwards port 7860 for Gradio, should you wish to run Gradio from inside the Docker container after generating the code.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;-v ./:/app&lt;/code&gt; maps the current directory with all the agencies to &lt;code&gt;/app&lt;/code&gt; inside the Docker container.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;-e OPENAI_API_KEY=&amp;lt;YourOpenAIKey&amp;gt;&lt;/code&gt; is where you put your OpenAI API key.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;vrsen/agency-swarm&lt;/code&gt; the name you gave to the Docker image that you generated.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Agency-Specific Requirements&lt;/strong&gt; (inside Docker Image): Navigate into the directory of the agency you&#39;ve chosen inside your docker image and install the requirements.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd path/to/your-chosen-agency&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run agency.py&lt;/strong&gt; (inside Docker Image): With the environment properly set up, you are now ready to activate your agency. Execute the following command within the agency&#39;s directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python agency.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We encourage contributions to the Agency Swarm Lab! If you have developed a custom AI agency using the Agency Swarm framework and would like to share it, please submit a pull request with your project.&lt;/p&gt; &#xA;&lt;h1&gt;Stay Updated&lt;/h1&gt; &#xA;&lt;p&gt;Don&#39;t forget to subscribe to our &lt;a href=&#34;https://youtube.com/@vrsen?si=l_6znuALa3IOl6ft&#34;&gt;YouTube channel&lt;/a&gt; for tutorials and updates on the Agency Swarm framework and the amazing projects being developed with it.&lt;/p&gt; &#xA;&lt;p&gt;Thank you for exploring the Agency Swarm Lab. Together, let&#39;s transform the future of work with AI.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>IDEA-Research/GroundingDINO</title>
    <updated>2024-04-14T01:41:06Z</updated>
    <id>tag:github.com,2024-04-14:/IDEA-Research/GroundingDINO</id>
    <link href="https://github.com/IDEA-Research/GroundingDINO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of the paper &#34;Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&#34;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/grounding_dino_logo.png&#34; width=&#34;30%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;&lt;span&gt;ü¶ï&lt;/span&gt; Grounding DINO&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-object-detection-on-mscoco?p=grounding-dino-marrying-dino-with-grounded&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/zero-shot-object-detection-on-mscoco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-object-detection-on-odinw?p=grounding-dino-marrying-dino-with-grounded&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/zero-shot-object-detection-on-odinw&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://paperswithcode.com/sota/object-detection-on-coco-minival?p=grounding-dino-marrying-dino-with-grounded&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/object-detection-on-coco-minival&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/object-detection-on-coco?p=grounding-dino-marrying-dino-with-grounded&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/object-detection-on-coco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/IDEA-Research&#34;&gt;IDEA-CVR, IDEA-Research&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.lsl.zone/&#34;&gt;Shilong Liu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=U_cvvUwAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Zhaoyang Zeng&lt;/a&gt;, &lt;a href=&#34;https://rentainhe.github.io/&#34;&gt;Tianhe Ren&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=ybRe9GcAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Feng Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=B8hPxMQAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/yangjie-cv&#34;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=Zd7WmXUAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&#34;https://jwyang.github.io/&#34;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=dxN1_X0AAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate&#34;&gt;Hang Su&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=axsP38wAAAAJ&#34;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&#34;https://www.leizhang.org/&#34;&gt;Lei Zhang&lt;/a&gt;&lt;sup&gt;&lt;span&gt;üìß&lt;/span&gt;&lt;/sup&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/#black_nib-citation&#34;&gt;&lt;code&gt;BibTex&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;PyTorch implementation and pretrained models for Grounding DINO. For details, see the paper &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåû&lt;/span&gt; Helpful Tutorial&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üçá&lt;/span&gt; [&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Read our arXiv Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üçé&lt;/span&gt; [&lt;a href=&#34;https://youtu.be/wxWDt5UiwY8&#34;&gt;Watch our simple introduction video on YouTube&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üåº&lt;/span&gt; &amp;nbsp;[&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&#34;&gt;Try the Colab Demo&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üåª&lt;/span&gt; [&lt;a href=&#34;https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo&#34;&gt;Try our Official Huggingface Demo&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üçÅ&lt;/span&gt; [&lt;a href=&#34;https://youtu.be/cMa77r3YrDk&#34;&gt;Watch the Step by Step Tutorial about GroundingDINO by Roboflow AI&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üçÑ&lt;/span&gt; [&lt;a href=&#34;https://youtu.be/C4NqaRBz_Kw&#34;&gt;GroundingDINO: Automated Dataset Annotation and Evaluation by Roboflow AI&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üå∫&lt;/span&gt; [&lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34;&gt;Accelerate Image Annotation with SAM and GroundingDINO by Roboflow AI&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üíÆ&lt;/span&gt; [&lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;Autodistill: Train YOLOv8 with ZERO Annotations based on Grounding-DINO and Grounded-SAM by Roboflow AI&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Grounding DINO Methods | &#xA;[![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499) &#xA;[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wxWDt5UiwY8) --&gt; &#xA;&lt;!-- Grounding DINO Demos |&#xA;[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) --&gt; &#xA;&lt;!-- [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/cMa77r3YrDk)&#xA;[![HuggingFace space](https://img.shields.io/badge/ü§ó-HuggingFace%20Space-cyan.svg)](https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo)&#xA;[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/oEQYStnF2l8)&#xA;[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/C4NqaRBz_Kw) --&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ú®&lt;/span&gt; Highlight Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Semantic-SAM&#34;&gt;Semantic-SAM: a universal image segmentation model to enable segment and recognize anything at any desired granularity.&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OptimalScale/DetGPT&#34;&gt;DetGPT: Detect What You Need via Reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-SAM: Marrying Grounding DINO with Segment Anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb&#34;&gt;Grounding DINO with Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_gligen.ipynb&#34;&gt;Grounding DINO with GLIGEN for Controllable Image Editing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/OpenSeeD&#34;&gt;OpenSeeD: A Simple and Strong Openset Segmentation Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/X-Decoder/tree/xgpt&#34;&gt;X-GPT: Conversational Visual Agent supported by X-Decoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN: Open-Set Grounded Text-to-Image Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA: Large Language and Vision Assistant&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Extensions | [Grounding DINO with Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything); [Grounding DINO with Stable Diffusion](demo/image_editing_with_groundingdino_stablediffusion.ipynb); [Grounding DINO with GLIGEN](demo/image_editing_with_groundingdino_gligen.ipynb)  --&gt; &#xA;&lt;!-- Official PyTorch implementation of [Grounding DINO](https://arxiv.org/abs/2303.05499), a stronger open-set object detector. Code is available now! --&gt; &#xA;&lt;h2&gt;&lt;span&gt;üí°&lt;/span&gt; Highlight&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-Set Detection.&lt;/strong&gt; Detect &lt;strong&gt;everything&lt;/strong&gt; with language!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High Performancce.&lt;/strong&gt; COCO zero-shot &lt;strong&gt;52.5 AP&lt;/strong&gt; (training without COCO data!). COCO fine-tune &lt;strong&gt;63.0 AP&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible.&lt;/strong&gt; Collaboration with Stable Diffusion for Image Editting.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üî•&lt;/span&gt; News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/07/18&lt;/code&gt;&lt;/strong&gt;: We release &lt;a href=&#34;https://github.com/UX-Decoder/Semantic-SAM&#34;&gt;Semantic-SAM&lt;/a&gt;, a universal image segmentation model to enable segment and recognize anything at any desired granularity. &lt;strong&gt;Code&lt;/strong&gt; and &lt;strong&gt;checkpoint&lt;/strong&gt; are available!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/17&lt;/code&gt;&lt;/strong&gt;: We provide an example to evaluate Grounding DINO on COCO zero-shot performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/15&lt;/code&gt;&lt;/strong&gt;: Refer to &lt;a href=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings&#34;&gt;CV in the Wild Readings&lt;/a&gt; for those who are interested in open-set recognition!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/08&lt;/code&gt;&lt;/strong&gt;: We release &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_gligen.ipynb&#34;&gt;demos&lt;/a&gt; to combine &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt; with &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; for more controllable image editings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/08&lt;/code&gt;&lt;/strong&gt;: We release &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb&#34;&gt;demos&lt;/a&gt; to combine &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt; with &lt;a href=&#34;https://github.com/Stability-AI/StableDiffusion&#34;&gt;Stable Diffusion&lt;/a&gt; for image editings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/06&lt;/code&gt;&lt;/strong&gt;: We build a new demo by marrying GroundingDINO with &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt; named &lt;strong&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt;&lt;/strong&gt; aims to support segmentation in GroundingDINO.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/28&lt;/code&gt;&lt;/strong&gt;: A YouTube &lt;a href=&#34;https://youtu.be/cMa77r3YrDk&#34;&gt;video&lt;/a&gt; about Grounding DINO and basic object detection prompt engineering. [&lt;a href=&#34;https://github.com/SkalskiP&#34;&gt;SkalskiP&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/28&lt;/code&gt;&lt;/strong&gt;: Add a &lt;a href=&#34;https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo&#34;&gt;demo&lt;/a&gt; on Hugging Face Space!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/27&lt;/code&gt;&lt;/strong&gt;: Support CPU-only mode. Now the model can run on machines without GPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/25&lt;/code&gt;&lt;/strong&gt;: A &lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&#34;&gt;demo&lt;/a&gt; for Grounding DINO is available at Colab. [&lt;a href=&#34;https://github.com/SkalskiP&#34;&gt;SkalskiP&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/22&lt;/code&gt;&lt;/strong&gt;: Code is available Now!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; Description &lt;/font&gt;&lt;/summary&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Paper&lt;/a&gt; introduction. &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/hero_figure.png&#34; alt=&#34;ODinW&#34; width=&#34;100%&#34;&gt; Marrying &#xA; &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &#xA; &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; &#xA; &lt;img src=&#34;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/GD_GLIGEN.png&#34; alt=&#34;gd_gligen&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚≠ê&lt;/span&gt; Explanations/Tips for Grounding DINO Inputs and Outputs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Grounding DINO accepts an &lt;code&gt;(image, text)&lt;/code&gt; pair as inputs.&lt;/li&gt; &#xA; &lt;li&gt;It outputs &lt;code&gt;900&lt;/code&gt; (by default) object boxes. Each box has similarity scores across all input words. (as shown in Figures below.)&lt;/li&gt; &#xA; &lt;li&gt;We defaultly choose the boxes whose highest similarities are higher than a &lt;code&gt;box_threshold&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We extract the words whose similarities are higher than the &lt;code&gt;text_threshold&lt;/code&gt; as predicted labels.&lt;/li&gt; &#xA; &lt;li&gt;If you want to obtain objects of specific phrases, like the &lt;code&gt;dogs&lt;/code&gt; in the sentence &lt;code&gt;two dogs with a stick.&lt;/code&gt;, you can select the boxes with highest text similarities with &lt;code&gt;dogs&lt;/code&gt; as final outputs.&lt;/li&gt; &#xA; &lt;li&gt;Note that each word can be split to &lt;strong&gt;more than one&lt;/strong&gt; tokens with different tokenlizers. The number of words in a sentence may not equal to the number of text tokens.&lt;/li&gt; &#xA; &lt;li&gt;We suggest separating different category names with &lt;code&gt;.&lt;/code&gt; for Grounding DINO. &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/model_explan1.PNG&#34; alt=&#34;model_explain1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/model_explan2.PNG&#34; alt=&#34;model_explain2&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üè∑&lt;/span&gt; TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release inference code and demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Grounding DINO with Stable Diffusion and GLIGEN demos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release training codes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üõ†&lt;/span&gt; Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;If you have a CUDA environment, please make sure the environment variable &lt;code&gt;CUDA_HOME&lt;/code&gt; is set. It will be compiled under CPU-only mode if no CUDA available.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Please make sure following the installation steps strictly, otherwise the program may produce:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NameError: name &#39;_C&#39; is not defined&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If this happened, please reinstalled the groundingDINO by reclone the git and do all the installation steps again.&lt;/p&gt; &#xA;&lt;h4&gt;how to check cuda:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo $CUDA_HOME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If it print nothing, then it means you haven&#39;t set up the path/&lt;/p&gt; &#xA;&lt;p&gt;Run this so the environment variable will be set under current shell.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_HOME=/path/to/cuda-11.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice the version of cuda should be aligned with your CUDA runtime, for there might exists multiple cuda at the same time.&lt;/p&gt; &#xA;&lt;p&gt;If you want to set the CUDA_HOME permanently, store it using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;export CUDA_HOME=/path/to/cuda&#39; &amp;gt;&amp;gt; ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;after that, source the bashrc file and check CUDA_HOME:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source ~/.bashrc&#xA;echo $CUDA_HOME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this example, /path/to/cuda-11.3 should be replaced with the path where your CUDA toolkit is installed. You can find this by typing &lt;strong&gt;which nvcc&lt;/strong&gt; in your terminal:&lt;/p&gt; &#xA;&lt;p&gt;For instance, if the output is /usr/local/cuda/bin/nvcc, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_HOME=/usr/local/cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;1.Clone the GroundingDINO repository from GitHub.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/IDEA-Research/GroundingDINO.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Change the current directory to the GroundingDINO folder.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd GroundingDINO/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the required dependencies in the current directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download pre-trained model weights.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir weights&#xA;cd weights&#xA;wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ñ∂&lt;/span&gt; Demo&lt;/h2&gt; &#xA;&lt;p&gt;Check your GPU ID (only if you&#39;re using a GPU)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;{GPU ID}&lt;/code&gt;, &lt;code&gt;image_you_want_to_detect.jpg&lt;/code&gt;, and &lt;code&gt;&#34;dir you want to save the output&#34;&lt;/code&gt; with appropriate values in the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES={GPU ID} python demo/inference_on_a_image.py \&#xA;-c groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;-p weights/groundingdino_swint_ogc.pth \&#xA;-i image_you_want_to_detect.jpg \&#xA;-o &#34;dir you want to save the output&#34; \&#xA;-t &#34;chair&#34;&#xA; [--cpu-only] # open it for cpu mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to specify the phrases to detect, here is a demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES={GPU ID} python demo/inference_on_a_image.py \&#xA;-c groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;-p ./groundingdino_swint_ogc.pth \&#xA;-i .asset/cat_dog.jpeg \&#xA;-o logs/1111 \&#xA;-t &#34;There is a cat and a dog in the image .&#34; \&#xA;--token_spans &#34;[[[9, 10], [11, 14]], [[19, 20], [21, 24]]]&#34;&#xA; [--cpu-only] # open it for cpu mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The token_spans specify the start and end positions of a phrases. For example, the first phrase is &lt;code&gt;[[9, 10], [11, 14]]&lt;/code&gt;. &lt;code&gt;&#34;There is a cat and a dog in the image .&#34;[9:10] = &#39;a&#39;&lt;/code&gt;, &lt;code&gt;&#34;There is a cat and a dog in the image .&#34;[11:14] = &#39;cat&#39;&lt;/code&gt;. Hence it refers to the phrase &lt;code&gt;a cat&lt;/code&gt; . Similarly, the &lt;code&gt;[[19, 20], [21, 24]]&lt;/code&gt; refers to the phrase &lt;code&gt;a dog&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;code&gt;demo/inference_on_a_image.py&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running with Python:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from groundingdino.util.inference import load_model, load_image, predict, annotate&#xA;import cv2&#xA;&#xA;model = load_model(&#34;groundingdino/config/GroundingDINO_SwinT_OGC.py&#34;, &#34;weights/groundingdino_swint_ogc.pth&#34;)&#xA;IMAGE_PATH = &#34;weights/dog-3.jpeg&#34;&#xA;TEXT_PROMPT = &#34;chair . person . dog .&#34;&#xA;BOX_TRESHOLD = 0.35&#xA;TEXT_TRESHOLD = 0.25&#xA;&#xA;image_source, image = load_image(IMAGE_PATH)&#xA;&#xA;boxes, logits, phrases = predict(&#xA;    model=model,&#xA;    image=image,&#xA;    caption=TEXT_PROMPT,&#xA;    box_threshold=BOX_TRESHOLD,&#xA;    text_threshold=TEXT_TRESHOLD&#xA;)&#xA;&#xA;annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)&#xA;cv2.imwrite(&#34;annotated_image.jpg&#34;, annotated_frame)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Web UI&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also provide a demo code to integrate Grounding DINO with Gradio Web UI. See the file &lt;code&gt;demo/gradio_app.py&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notebooks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We release &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_gligen.ipynb&#34;&gt;demos&lt;/a&gt; to combine &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt; with &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; for more controllable image editings.&lt;/li&gt; &#xA; &lt;li&gt;We release &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb&#34;&gt;demos&lt;/a&gt; to combine &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt; with &lt;a href=&#34;https://github.com/Stability-AI/StableDiffusion&#34;&gt;Stable Diffusion&lt;/a&gt; for image editings.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;COCO Zero-shot Evaluations&lt;/h2&gt; &#xA;&lt;p&gt;We provide an example to evaluate Grounding DINO zero-shot performance on COCO. The results should be &lt;strong&gt;48.5&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 \&#xA;python demo/test_ap_on_coco.py \&#xA; -c groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA; -p weights/groundingdino_swint_ogc.pth \&#xA; --anno_path /path/to/annoataions/ie/instances_val2017.json \&#xA; --image_dir /path/to/imagedir/ie/val2017&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üß≥&lt;/span&gt; Checkpoints&lt;/h2&gt; &#xA;&lt;!-- insert a table --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;name&lt;/th&gt; &#xA;   &lt;th&gt;backbone&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;box AP on COCO&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;Config&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1&lt;/th&gt; &#xA;   &lt;td&gt;GroundingDINO-T&lt;/td&gt; &#xA;   &lt;td&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td&gt;O365,GoldG,Cap4M&lt;/td&gt; &#xA;   &lt;td&gt;48.4 (zero-shot) / 57.2 (fine-tune)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#34;&gt;GitHub link&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swint_ogc.pth&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/raw/main/groundingdino/config/GroundingDINO_SwinT_OGC.py&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;2&lt;/th&gt; &#xA;   &lt;td&gt;GroundingDINO-B&lt;/td&gt; &#xA;   &lt;td&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td&gt;COCO,O365,GoldG,Cap4M,OpenImage,ODinW-35,RefCOCO&lt;/td&gt; &#xA;   &lt;td&gt;56.7 &lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth&#34;&gt;GitHub link&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swinb_cogcoor.pth&#34;&gt;HF link&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/raw/main/groundingdino/config/GroundingDINO_SwinB_cfg.py&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;span&gt;üéñ&lt;/span&gt; Results&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; COCO Object Detection Results &lt;/font&gt;&lt;/summary&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/COCO.png&#34; alt=&#34;COCO&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; ODinW Object Detection Results &lt;/font&gt;&lt;/summary&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/ODinW.png&#34; alt=&#34;ODinW&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; Marrying Grounding DINO with &lt;a href=&#34;https://github.com/Stability-AI/StableDiffusion&#34;&gt;Stable Diffusion&lt;/a&gt; for Image Editing &lt;/font&gt;&lt;/summary&gt; See our example &#xA; &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/raw/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb&#34;&gt;notebook&lt;/a&gt; for more details. &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/GD_SD.png&#34; alt=&#34;GD_SD&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; Marrying Grounding DINO with &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; for more Detailed Image Editing. &lt;/font&gt;&lt;/summary&gt; See our example &#xA; &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/raw/main/demo/image_editing_with_groundingdino_gligen.ipynb&#34;&gt;notebook&lt;/a&gt; for more details. &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/GD_GLIGEN.png&#34; alt=&#34;GD_GLIGEN&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ü¶ï&lt;/span&gt; Model: Grounding DINO&lt;/h2&gt; &#xA;&lt;p&gt;Includes: a text backbone, an image backbone, a feature enhancer, a language-guided query selection, and a cross-modality decoder.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/arch.png&#34; alt=&#34;arch&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ô•&lt;/span&gt; Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our model is related to &lt;a href=&#34;https://github.com/IDEA-Research/DINO&#34;&gt;DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/GLIP&#34;&gt;GLIP&lt;/a&gt;. Thanks for their great work!&lt;/p&gt; &#xA;&lt;p&gt;We also thank great previous work including DETR, Deformable DETR, SMCA, Conditional DETR, Anchor DETR, Dynamic DETR, DAB-DETR, DN-DETR, etc. More related work are available at &lt;a href=&#34;https://github.com/IDEACVR/awesome-detection-transformer&#34;&gt;Awesome Detection Transformer&lt;/a&gt;. A new toolbox &lt;a href=&#34;https://github.com/IDEA-Research/detrex&#34;&gt;detrex&lt;/a&gt; is available as well.&lt;/p&gt; &#xA;&lt;p&gt;Thanks &lt;a href=&#34;https://github.com/Stability-AI/StableDiffusion&#34;&gt;Stable Diffusion&lt;/a&gt; and &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; for their awesome models.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚úí&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023grounding,&#xA;  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},&#xA;  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},&#xA;  journal={arXiv preprint arXiv:2303.05499},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>