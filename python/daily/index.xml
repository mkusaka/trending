<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-28T01:44:52Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gaomingqi/Track-Anything</title>
    <updated>2023-04-28T01:44:52Z</updated>
    <id>tag:github.com,2023-04-28:/gaomingqi/Track-Anything</id>
    <link href="https://github.com/gaomingqi/Track-Anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Track-Anything is a flexible and interactive tool for video object tracking and segmentation, based on Segment Anything, XMem, and E2FGVI.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/gaomingqi/Track-Anything/master/assets/track-anything-logo.jpg&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a src=&#34;https://img.shields.io/badge/%F0%9F%93%96-Open_in_Spaces-informational.svg?style=flat-square&#34; href=&#34;https://arxiv.org/abs/2304.11968&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%93%96-Arxiv_2304.11968-red.svg?style=flat-square&#34;&gt; &lt;/a&gt; &#xA; &lt;a src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open_in_Spaces-informational.svg?style=flat-square&#34; href=&#34;https://huggingface.co/spaces/watchtowerss/Track-Anything?duplicate=true&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face_Space-informational.svg?style=flat-square&#34;&gt; &lt;/a&gt; &#xA; &lt;a src=&#34;https://img.shields.io/badge/%F0%9F%9A%80-SUSTech_VIP_Lab-important.svg?style=flat-square&#34; href=&#34;https://zhengfenglab.com/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%9A%80-SUSTech_VIP_Lab-important.svg?style=flat-square&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Track-Anything&lt;/strong&gt;&lt;/em&gt; is a flexible and interactive tool for video object tracking and segmentation. It is developed upon &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt;, can specify anything to track and segment via user clicks only. During tracking, users can flexibly change the objects they wanna track or correct the region of interest if there are any ambiguities. These characteristics enable &lt;em&gt;&lt;strong&gt;Track-Anything&lt;/strong&gt;&lt;/em&gt; to be suitable for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Video object tracking and segmentation with shot changes.&lt;/li&gt; &#xA; &lt;li&gt;Visualized development and data annnotation for video object tracking and segmentation.&lt;/li&gt; &#xA; &lt;li&gt;Object-centric downstream video tasks, such as video inpainting and editing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/gaomingqi/Track-Anything/master/assets/avengers.gif&#34; width=&#34;81%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ![avengers]() --&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2023/04/25: We are delighted to introduce &lt;a href=&#34;https://github.com/ttengwang/Caption-Anything&#34;&gt;Caption-Anything&lt;/a&gt; &lt;span&gt;‚úç&lt;/span&gt;, an inventive project from our lab that combines the capabilities of Segment Anything, Visual Captioning, and ChatGPT.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2023/04/20: We deployed &lt;a href=&#34;https://huggingface.co/spaces/watchtowerss/Track-Anything?duplicate=trueg&#34;&gt;[DEMO]&lt;/a&gt; on Hugging Face &lt;span&gt;ü§ó&lt;/span&gt;!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2023/04/14: We made Track-Anything public!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üó∫&lt;/span&gt; Video Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/30309970/234902447-a4c59718-fcfe-443a-bd18-2f3f775cfc13.mp4&#34;&gt;https://user-images.githubusercontent.com/30309970/234902447-a4c59718-fcfe-443a-bd18-2f3f775cfc13.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;span&gt;üïπ&lt;/span&gt; Example - Multiple Object Tracking and Segmentation (with &lt;a href=&#34;https://github.com/hkchengrex/XMem&#34;&gt;XMem&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/39208339/233035206-0a151004-6461-4deb-b782-d1dbfe691493.mp4&#34;&gt;https://user-images.githubusercontent.com/39208339/233035206-0a151004-6461-4deb-b782-d1dbfe691493.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;span&gt;üïπ&lt;/span&gt; Example - Video Object Tracking and Segmentation with Shot Changes (with &lt;a href=&#34;https://github.com/hkchengrex/XMem&#34;&gt;XMem&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/30309970/232848349-f5e29e71-2ea4-4529-ac9a-94b9ca1e7055.mp4&#34;&gt;https://user-images.githubusercontent.com/30309970/232848349-f5e29e71-2ea4-4529-ac9a-94b9ca1e7055.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;span&gt;üïπ&lt;/span&gt; Example - Video Inpainting (with &lt;a href=&#34;https://github.com/MCG-NKU/E2FGVI&#34;&gt;E2FGVI&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/28050374/232959816-07f2826f-d267-4dda-8ae5-a5132173b8f4.mp4&#34;&gt;https://user-images.githubusercontent.com/28050374/232959816-07f2826f-d267-4dda-8ae5-a5132173b8f4.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üíª&lt;/span&gt; Get Started&lt;/h2&gt; &#xA;&lt;h4&gt;Linux &amp;amp; Windows&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Clone the repository:&#xA;git clone https://github.com/gaomingqi/Track-Anything.git&#xA;cd Track-Anything&#xA;&#xA;# Install dependencies: &#xA;pip install -r requirements.txt&#xA;&#xA;# Run the Track-Anything gradio demo.&#xA;python app.py --device cuda:0&#xA;# python app.py --device cuda:0 --sam_model_type vit_b # for lower memory usage&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful for your research or applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{yang2023track,&#xA;      title={Track Anything: Segment Anything Meets Videos}, &#xA;      author={Jinyu Yang and Mingqi Gao and Zhe Li and Shang Gao and Fangjing Wang and Feng Zheng},&#xA;      year={2023},&#xA;      eprint={2304.11968},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üëè&lt;/span&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The project is based on &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt;, &lt;a href=&#34;https://github.com/hkchengrex/XMem&#34;&gt;XMem&lt;/a&gt;, and &lt;a href=&#34;https://github.com/MCG-NKU/E2FGVI&#34;&gt;E2FGVI&lt;/a&gt;. Thanks for the authors for their efforts.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>peterw/Chat-with-Github-Repo</title>
    <updated>2023-04-28T01:44:52Z</updated>
    <id>tag:github.com,2023-04-28:/peterw/Chat-with-Github-Repo</id>
    <link href="https://github.com/peterw/Chat-with-Github-Repo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains two Python scripts that demonstrate how to create a chatbot using Streamlit, OpenAI GPT-3.5-turbo, and Activeloop&#39;s Deep Lake.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chat-with-Github-Repo&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains two Python scripts that demonstrate how to create a chatbot using Streamlit, OpenAI GPT-3.5-turbo, and Activeloop&#39;s Deep Lake.&lt;/p&gt; &#xA;&lt;p&gt;The chatbot searches a dataset stored in Deep Lake to find relevant information and generates responses based on the user&#39;s input.&lt;/p&gt; &#xA;&lt;h2&gt;Files&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;github.py&lt;/code&gt;: This script clones a git repository, processes the text documents, computes embeddings using OpenAIEmbeddings, and stores the embeddings in a DeepLake instance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;chat.py&lt;/code&gt;: This script creates a Streamlit web application that interacts with the user and the DeepLake instance to generate chatbot responses using OpenAI GPT-3.5-turbo.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Before getting started, be sure to sign up for an &lt;a href=&#34;https://www.activeloop.ai/&#34;&gt;Activeloop&lt;/a&gt; and &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt; account and create API keys. You&#39;ll also want to create a Deep Lake dataset, which will generate a dataset path in the format &lt;code&gt;hub://{username}/{repo_name}&lt;/code&gt; (where you define the &lt;code&gt;repo_name&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;To set up and run this project, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the required packages with &lt;code&gt;pip&lt;/code&gt;: &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Copy the &lt;code&gt;.env.example&lt;/code&gt; file to &lt;code&gt;.env&lt;/code&gt; and replace the variables, including API keys, GitHub URL, and site / Deep Lake information.&lt;/li&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;github.py&lt;/code&gt; script to embed the GitHub repo, thus, storing the data in the specified Activeloop Deep Lake.&lt;/li&gt; &#xA; &lt;li&gt;Run the Streamlit chat app, which should default to &lt;code&gt;http://localhost:8502&lt;/code&gt; and allow you to ask questions about the repo: &lt;pre&gt;&lt;code&gt;streamlit run chat.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>alaeddine-13/thinkgpt</title>
    <updated>2023-04-28T01:44:52Z</updated>
    <id>tag:github.com,2023-04-28:/alaeddine-13/thinkgpt</id>
    <link href="https://github.com/alaeddine-13/thinkgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agent techniques to augment your LLM and push it beyong its limits&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ThinkGPT üß†ü§ñ&lt;/h1&gt; &#xA;&lt;p&gt;ThinkGPT is a Python library aimed at implementing Chain of Thoughts for Large Language Models (LLMs), prompting the model to think, reason, and to create generative agents. The library aims to help with the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;solve limited context with long memory and compressed knowledge&lt;/li&gt; &#xA; &lt;li&gt;enhance LLMs&#39; one-shot reasoning with higher order reasoning primitives&lt;/li&gt; &#xA; &lt;li&gt;add intelligent decisions to your code base&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key Features ‚ú®&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thinking building blocks üß±: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Memory üß†: GPTs that can remember experience&lt;/li&gt; &#xA;   &lt;li&gt;Self-refinement üîß: Improve model-generated content by addressing critics&lt;/li&gt; &#xA;   &lt;li&gt;Compress knowledge üåê: Compress knowledge and fit it in LLM&#39;s context either by anstracring rules out of observations or summarize large content&lt;/li&gt; &#xA;   &lt;li&gt;Inference üí°Ô∏è: Make educated guesses based on available information&lt;/li&gt; &#xA;   &lt;li&gt;Natural Language Conditions üìù: Easily express choices and conditions in natural language&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Efficient and Measurable GPT context length üìê&lt;/li&gt; &#xA; &lt;li&gt;Extremely easy setup and pythonic API üéØ thanks to &lt;a href=&#34;https://github.com/docarray/docarray&#34;&gt;DocArray&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation üíª&lt;/h2&gt; &#xA;&lt;p&gt;You can install ThinkGPT using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install git+https://github.com/alaeddine-13/thinkgpt.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;API Documentation üìö&lt;/h2&gt; &#xA;&lt;h3&gt;Basic usage:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from thinkgpt.llm import ThinkGPT&#xA;llm = ThinkGPT(model_name=&#34;gpt-3.5-turbo&#34;)&#xA;# Make the llm object learn new concepts&#xA;llm.memorize([&#39;DocArray is a library for representing, sending and storing multi-modal data.&#39;])&#xA;llm.predict(&#39;what is DocArray ?&#39;, remember=llm.remember(&#39;DocArray definition&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Memorizing and Remembering information&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm.memorize([&#xA;    &#39;DocArray allows you to send your data, in an ML-native way.&#39;,&#xA;    &#39;This means there is native support for Protobuf and gRPC, on top of HTTP and serialization to JSON, JSONSchema, Base64, and Bytes.&#39;,&#xA;])&#xA;&#xA;print(llm.remember(&#39;Sending data with DocArray&#39;, limit=1))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;[&#39;DocArray allows you to send your data, in an ML-native way.&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Predicting with context from long memory&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from examples.knowledge_base import knowledge&#xA;llm.memorize(knowledge)&#xA;llm.predict(&#39;Implement a DocArray schema with 2 fields: image and TorchTensor&#39;, remember=llm.remember(&#39;DocArray schemas and types&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Self-refinement&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(llm.refine(&#xA;    content=&#34;&#34;&#34;&#xA;import re&#xA;    print(&#39;hello world&#39;)&#xA;        &#34;&#34;&#34;,&#xA;    critics=[&#xA;        &#39;File &#34;/Users/user/PyCharm2022.3/scratches/scratch_166.py&#34;, line 2&#39;,&#xA;        &#34;  print(&#39;hello world&#39;)&#34;,&#xA;        &#39;IndentationError: unexpected indent&#39;&#xA;    ],&#xA;    instruction_hint=&#34;Fix the code snippet based on the error provided. Only provide the fixed code snippet between `` and nothing else.&#34;))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;import re&#xA;print(&#39;hello world&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;One of the applications is self-healing code generation implemented by projects like &lt;a href=&#34;https://github.com/jina-ai/gptdeploy&#34;&gt;gptdeploy&lt;/a&gt; and &lt;a href=&#34;https://github.com/biobootloader/wolverine&#34;&gt;wolverine&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Compressing knowledge&lt;/h3&gt; &#xA;&lt;p&gt;In case you want your knowledge to fit into the LLM&#39;s context, you can use the following techniques to compress it:&lt;/p&gt; &#xA;&lt;h4&gt;Summarize content&lt;/h4&gt; &#xA;&lt;p&gt;Summarize content using the LLM itself. We offer 2 methods&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;one-shot summarization using the LLM&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm.summarize(&#xA;  large_content,&#xA;  max_tokens= 1000,&#xA;  instruction_hint= &#39;Pay attention to code snippets, links and scientific terms.&#39;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since this technique relies on summarizing using a single LLM call, you can only pass content that does not exceed the LLM&#39;s context length. 2. Chunked summarization&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm.chunked_summarize(&#xA;  very_large_content,&#xA;  max_tokens= 4096,&#xA;  instruction_hint= &#39;Pay attention to code snippets, links and scientific terms.&#39;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This technique relies on splitting the content into different chunks, summarizing each of those chunks and then combining them all together using an LLM.&lt;/p&gt; &#xA;&lt;h4&gt;Induce rules from observations&lt;/h4&gt; &#xA;&lt;p&gt;Amount to higher level and more general observations from current observations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm.abstract(observations=[&#xA;    &#34;in tunisian, I did not eat is \&#34;ma khditech\&#34;&#34;,&#xA;    &#34;I did not work is \&#34;ma khdemtech\&#34;&#34;,&#xA;    &#34;I did not go is \&#34;ma mchitech\&#34;&#34;,&#xA;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;[&#39;Negation in Tunisian Arabic uses &#34;ma&#34; + verb + &#34;tech&#34; where &#34;ma&#34; means &#34;not&#34; and &#34;tech&#34; at the end indicates the negation in the past tense.&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This can help you end up with compressed knowledge that fits better the limited context length of LLMs. For instance, instead of trying to fit code examples in the LLM&#39;s context, use this to prompt it to understand high level rules and fit them in the context.&lt;/p&gt; &#xA;&lt;h3&gt;Natural language condition&lt;/h3&gt; &#xA;&lt;p&gt;Introduce intelligent conditions to your code and let the LLM make decisions&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm.condition(f&#39;Does this represent an error message ? &#34;IndentationError: unexpected indent&#34;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Natural language select&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, let the LLM choose among a list of options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm.select(&#xA;    question=&#34;Which animal is the king of the jungle?&#34;,&#xA;    options=[&#34;Lion&#34;, &#34;Elephant&#34;, &#34;Tiger&#34;, &#34;Giraffe&#34;]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;[&#39;Lion&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also prompt the LLM to choose an exact number of answers using &lt;code&gt;num_choices&lt;/code&gt;. By default, it&#39;s set to &lt;code&gt;None&lt;/code&gt; which means the LLM will select any number he thinks it&#39;s correct.&lt;/p&gt; &#xA;&lt;h2&gt;Use Cases üöÄ&lt;/h2&gt; &#xA;&lt;p&gt;Find out below example demos you can do with &lt;code&gt;thinkgpt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Teaching ThinkGPT a new language&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from thinkgpt.llm import ThinkGPT&#xA;&#xA;llm = ThinkGPT(model_name=&#34;gpt-3.5-turbo&#34;)&#xA;&#xA;rules = llm.abstract(observations=[&#xA;    &#34;in tunisian, I did not eat is \&#34;ma khditech\&#34;&#34;,&#xA;    &#34;I did not work is \&#34;ma khdemtech\&#34;&#34;,&#xA;    &#34;I did not go is \&#34;ma mchitech\&#34;&#34;,&#xA;], instruction_hint=&#34;output the rule in french&#34;)&#xA;llm.memorize(rules)&#xA;&#xA;llm.memorize(&#34;in tunisian, I studied is \&#34;9rit\&#34;&#34;)&#xA;&#xA;task = &#34;translate to Tunisian: I didn&#39;t study&#34;&#xA;llm.predict(task, remember=llm.remember(task))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;The translation of &#34;I didn&#39;t study&#34; to Tunisian language would be &#34;ma 9ritech&#34;.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Teaching ThinkGPT how to code with &lt;code&gt;thinkgpt&lt;/code&gt; library&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from thinkgpt.llm import ThinkGPT&#xA;from examples.knowledge_base import knowledge&#xA;&#xA;llm = ThinkGPT(model_name=&#34;gpt-3.5-turbo&#34;)&#xA;&#xA;llm.memorize(knowledge)&#xA;&#xA;task = &#39;Implement python code that uses thinkgpt to learn about docarray v2 code and then predict with remembered information about docarray v2. Only give the code between `` and nothing else&#39;&#xA;print(llm.predict(task, remember=llm.remember(task, limit=10, sort_by_order=True)))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Code generated by the LLM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;from thinkgpt.llm import ThinkGPT&#xA;from docarray import BaseDoc&#xA;from docarray.typing import TorchTensor, ImageUrl&#xA;&#xA;llm = ThinkGPT(model_name=&#34;gpt-3.5-turbo&#34;)&#xA;&#xA;# Memorize information&#xA;llm.memorize(&#39;DocArray V2 allows you to represent your data, in an ML-native way&#39;)&#xA;&#xA;&#xA;# Predict with the memory&#xA;memory = llm.remember(&#39;DocArray V2&#39;)&#xA;llm.predict(&#39;write python code about DocArray v2&#39;, remember=memory)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Replay Agent memory and infer new observations&lt;/h3&gt; &#xA;&lt;p&gt;Refer to the following script for an example of an Agent that replays its memory and induces new observations. This concept was introduced in &lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;the Generative Agents: Interactive Simulacra of Human Behavior paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m examples.replay_expand_memory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;new thoughts:&#xA;Klaus Mueller is interested in multiple topics&#xA;Klaus Mueller may have a diverse range of interests and hobbies&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Replay Agent memory, criticize and refine the knowledge in memory&lt;/h3&gt; &#xA;&lt;p&gt;Refer to the following script for an example of an Agent that replays its memory, performs self-criticism and adjusts its memory knowledge based on the criticism.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m examples.replay_criticize_refine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;refined &#34;the second number in Fibonacci sequence is 2&#34; into &#34;Observation: The second number in the Fibonacci sequence is actually 1, not 2, and the sequence starts with 0, 1.&#34;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This technique was mainly implemented in the &lt;a href=&#34;https://arxiv.org/abs/2303.17651&#34;&gt;the Self-Refine: Iterative Refinement with Self-Feedback paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more detailed usage and code examples check &lt;code&gt;./examples&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>