<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-02T01:36:02Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>KillianLucas/open-interpreter</title>
    <updated>2023-09-02T01:36:02Z</updated>
    <id>tag:github.com,2023-09-02:/KillianLucas/open-interpreter</id>
    <link href="https://github.com/KillianLucas/open-interpreter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenAI&#39;s Code Interpreter in your terminal, running locally.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;‚óè Open Interpreter&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/YG7APUyJ5&#34;&gt;&lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1146610656779440188?logo=discord&amp;amp;style=flat&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=license&amp;amp;message=MIT&amp;amp;color=white&amp;amp;style=flat&#34; alt=&#34;License&#34;&gt; &lt;br&gt; &lt;b&gt;Let language models run code on your computer.&lt;/b&gt;&lt;br&gt; An open-source, locally running implementation of OpenAI&#39;s Code Interpreter. &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/KillianLucas/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56&#34; alt=&#34;poster&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install open-interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Open Interpreter&lt;/strong&gt; lets LLMs run code (Python, Javascript, Shell, and more) locally. You can chat with Open Interpreter through a ChatGPT-like interface in your terminal by running &lt;code&gt;$ interpreter&lt;/code&gt; after installing.&lt;/p&gt; &#xA;&lt;p&gt;This provides a natural-language interface to your computer&#39;s general-purpose capabilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create and edit photos, videos, PDFs, etc.&lt;/li&gt; &#xA; &lt;li&gt;Control a Chrome browser to perform research&lt;/li&gt; &#xA; &lt;li&gt;Plot, clean, and analyze large datasets&lt;/li&gt; &#xA; &lt;li&gt;...etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Note: You&#39;ll be asked to approve code before it&#39;s run.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KillianLucas/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60&#34;&gt;https://github.com/KillianLucas/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;An interactive demo is also avaliable on Google Colab:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install open-interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Terminal&lt;/h3&gt; &#xA;&lt;p&gt;After installation, simply run &lt;code&gt;interpreter&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import interpreter&#xA;&#xA;interpreter.chat(&#34;Plot APPL and META&#39;s normalized stock prices&#34;) # Executes a single command&#xA;interpreter.chat() # Starts an interactive chat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Comparison to ChatGPT&#39;s Code Interpreter&lt;/h2&gt; &#xA;&lt;p&gt;OpenAI&#39;s release of &lt;a href=&#34;https://openai.com/blog/chatgpt-plugins#code-interpreter&#34;&gt;Code Interpreter&lt;/a&gt; with GPT-4 presents a fantastic opportunity to accomplish real-world tasks with ChatGPT.&lt;/p&gt; &#xA;&lt;p&gt;However, OpenAI&#39;s service is hosted, closed-source, and heavily restricted:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No internet access.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/&#34;&gt;Limited set of pre-installed packages&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;100 MB maximum upload, 120.0 second runtime limit.&lt;/li&gt; &#xA; &lt;li&gt;State is cleared (along with any generated files or links) when the environment dies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Open Interpreter overcomes these limitations by running on your local environment. It has full access to the internet, isn&#39;t restricted by time or file size, and can utilize any package or library.&lt;/p&gt; &#xA;&lt;p&gt;This combines the power of GPT-4&#39;s Code Interpreter with the flexibility of your local development environment.&lt;/p&gt; &#xA;&lt;h2&gt;Commands&lt;/h2&gt; &#xA;&lt;h4&gt;Interactive Chat&lt;/h4&gt; &#xA;&lt;p&gt;To start an interactive chat in your terminal, either run &lt;code&gt;interpreter&lt;/code&gt; from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or &lt;code&gt;interpreter.chat()&lt;/code&gt; from a .py file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.chat()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Programmatic Chat&lt;/h4&gt; &#xA;&lt;p&gt;For more precise control, you can pass messages directly to &lt;code&gt;.chat(message)&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.chat(&#34;Add subtitles to all videos in /videos.&#34;)&#xA;&#xA;# ... Streams output to your terminal, completes task ...&#xA;&#xA;interpreter.chat(&#34;These look great but can you make the subtitles bigger?&#34;)&#xA;&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Start a New Chat&lt;/h4&gt; &#xA;&lt;p&gt;In Python, Open Interpreter remembers conversation history. If you want to start fresh, you can reset it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.reset()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Save and Restore Chats&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;interpreter.chat()&lt;/code&gt; returns a List of messages when return_messages=True, which can be used to resume a conversation with &lt;code&gt;interpreter.load(messages)&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messages = interpreter.chat(&#34;My name is Killian.&#34;, return_messages=True) # Save messages to &#39;messages&#39;&#xA;interpreter.reset() # Reset interpreter (&#34;Killian&#34; will be forgotten)&#xA;&#xA;interpreter.load(messages) # Resume chat from &#39;messages&#39; (&#34;Killian&#34; will be remembered)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Customize System Message&lt;/h4&gt; &#xA;&lt;p&gt;You can inspect and configure Open Interpreter&#39;s system message to extend its functionality, modify permissions, or give it more context.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.system_message += &#34;&#34;&#34;&#xA;Run shell commands with -y so the user doesn&#39;t have to confirm them.&#xA;&#34;&#34;&#34;&#xA;print(interpreter.system_message)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Change the Model&lt;/h4&gt; &#xA;&lt;p&gt;You can run &lt;code&gt;interpreter&lt;/code&gt; in local mode from the command line to use &lt;code&gt;Code Llama&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter --local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, use fast mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter --fast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, in Python, set the model manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.model = &#34;gpt-3.5-turbo&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Safety Notice&lt;/h2&gt; &#xA;&lt;p&gt;Since generated code is executed in your local environment, it can interact with your files and system settings, potentially leading to unexpected outcomes like data loss or security risks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Open Interpreter will ask for user confirmation before executing code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can run &lt;code&gt;interpreter -y&lt;/code&gt; or set &lt;code&gt;interpreter.auto_run = True&lt;/code&gt; to bypass this confirmation, in which case:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Be cautious when requesting commands that modify files or system settings.&lt;/li&gt; &#xA; &lt;li&gt;Watch Open Interpreter like a self-driving car, and be prepared to end the process by closing your terminal.&lt;/li&gt; &#xA; &lt;li&gt;Consider running Open Interpreter in a restricted environment like Google Colab or Replit. These environments are more isolated, reducing the risks associated with executing arbitrary code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How Does it Work?&lt;/h2&gt; &#xA;&lt;p&gt;Open Interpreter equips a &lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/function-calling&#34;&gt;function-calling language model&lt;/a&gt; with an &lt;code&gt;exec()&lt;/code&gt; function, which accepts a &lt;code&gt;language&lt;/code&gt; (like &#34;python&#34; or &#34;javascript&#34;) and &lt;code&gt;code&lt;/code&gt; to run.&lt;/p&gt; &#xA;&lt;p&gt;We then stream the model&#39;s messages, code, and your system&#39;s outputs to the terminal as Markdown.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This is a community-made project. If it looks exciting to you, please don&#39;t hesitate to contribute!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Open Interpreter is licensed under the MIT License. You are permitted to use, copy, modify, distribute, sublicense and sell copies of the software.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This software is not affiliated with OpenAI.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Having access to a junior programmer working at the speed of your fingertips ... can make new workflows effortless and efficient, as well as open the benefits of programming to new audiences.&lt;/p&gt; &#xA; &lt;p&gt;‚Äî &lt;em&gt;OpenAI&#39;s Code Interpreter Release&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>apple/axlearn</title>
    <updated>2023-09-02T01:36:02Z</updated>
    <id>tag:github.com,2023-09-02:/apple/axlearn</id>
    <link href="https://github.com/apple/axlearn" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The AXLearn Library for Deep Learning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;This library is under active development and the API is subject to change.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;AXLearn is a library built on top of &lt;a href=&#34;https://jax.readthedocs.io/&#34;&gt;JAX&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/xla&#34;&gt;XLA&lt;/a&gt; to support development of large-scale deep learning models.&lt;/p&gt; &#xA;&lt;p&gt;AXLearn takes an object-oriented approach to the software engineering challenges that arise from building, iterating, and maintaining models. The configuration system of the library lets users compose models from reusable building blocks and integrate with other libraries such as &lt;a href=&#34;https://flax.readthedocs.io/&#34;&gt;Flax&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Hugging Face transformers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;AXLearn is built to scale. It supports training of models with up to hundreds of billions of parameters across thousands of accelerators at high utilization. It is also designed to run on public clouds and provides tools to deploy and manage jobs and data. Built on top of &lt;a href=&#34;https://arxiv.org/abs/2105.04663&#34;&gt;GSPMD&lt;/a&gt;, AXLearn adopts a global computation paradigm to allow users to describe computation on a virtual global computer rather than on a per-accelerator basis.&lt;/p&gt; &#xA;&lt;p&gt;AXLearn supports a wide range of applications, including natural language processing, computer vision, and speech recognition and contains baseline configurations for training state-of-the-art models.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>obss/sahi</title>
    <updated>2023-09-02T01:36:02Z</updated>
    <id>tag:github.com,2023-09-02:/obss/sahi</id>
    <link href="https://github.com/obss/sahi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Framework agnostic sliced/tiled inference + interactive ui + error analysis plots&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; SAHI: Slicing Aided Hyper Inference &lt;/h1&gt; &#xA; &lt;h4&gt; A lightweight vision library for performing large scale object detection &amp;amp; instance segmentation &lt;/h4&gt; &#xA; &lt;h4&gt; &lt;img width=&#34;700&#34; alt=&#34;teaser&#34; src=&#34;https://raw.githubusercontent.com/obss/sahi/main/resources/sliced_inference.gif&#34;&gt; &lt;/h4&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://pepy.tech/project/sahi&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/sahi&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://pepy.tech/project/sahi&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/sahi/month&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://badge.fury.io/py/sahi&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/sahi.svg?sanitize=true&#34; alt=&#34;pypi version&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://anaconda.org/conda-forge/sahi&#34;&gt;&lt;img src=&#34;https://anaconda.org/conda-forge/sahi/badges/version.svg?sanitize=true&#34; alt=&#34;conda version&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://github.com/obss/sahi/actions/workflows/package_testing.yml&#34;&gt;&lt;img src=&#34;https://github.com/obss/sahi/actions/workflows/package_testing.yml/badge.svg?sanitize=true&#34; alt=&#34;package testing&#34;&gt;&lt;/a&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://ieeexplore.ieee.org/document/9897990&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/DOI-10.1109%2FICIP46576.2022.9897990-orange.svg?sanitize=true&#34; alt=&#34;ci&#34;&gt;&lt;/a&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://colab.research.google.com/github/obss/sahi/blob/main/demo/inference_for_yolov5.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://huggingface.co/spaces/fcakyon/sahi-yolox&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/obss/sahi/main/resources/hf_spaces_badge.svg?sanitize=true&#34; alt=&#34;HuggingFace Spaces&#34;&gt;&lt;/a&gt; &#xA;  &lt;p&gt;‚Äã&lt;/p&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Overview&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Object detection and instance segmentation are by far the most important fields of applications in Computer Vision. However, detection of small objects and inference on large images are still major issues in practical usage. Here comes the SAHI to help developers overcome these real-world problems with many vision utilities.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/obss/sahi/raw/main/docs/cli.md#predict-command-usage&#34;&gt;predict&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;perform sliced/standard video/image prediction using any &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt;/&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;mmdet&lt;/a&gt;/&lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;detectron2&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=object-detection&amp;amp;sort=downloads&#34;&gt;huggingface&lt;/a&gt; model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/obss/sahi/raw/main/docs/cli.md#predict-fiftyone-command-usage&#34;&gt;predict-fiftyone&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;perform sliced/standard prediction using any &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt;/&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;mmdet&lt;/a&gt;/&lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;detectron2&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=object-detection&amp;amp;sort=downloads&#34;&gt;huggingface&lt;/a&gt; model and explore results in &lt;a href=&#34;https://github.com/voxel51/fiftyone&#34;&gt;fiftyone app&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/obss/sahi/raw/main/docs/cli.md#coco-slice-command-usage&#34;&gt;coco slice&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;automatically slice COCO annotation and image files&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/obss/sahi/raw/main/docs/cli.md#coco-fiftyone-command-usage&#34;&gt;coco fiftyone&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;explore multiple prediction results on your COCO dataset with &lt;a href=&#34;https://github.com/voxel51/fiftyone&#34;&gt;fiftyone ui&lt;/a&gt; ordered by number of misdetections&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/obss/sahi/raw/main/docs/cli.md#coco-evaluate-command-usage&#34;&gt;coco evaluate&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;evaluate classwise COCO AP and AR for given predictions and ground truth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/obss/sahi/raw/main/docs/cli.md#coco-analyse-command-usage&#34;&gt;coco analyse&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;calcualate and export many error analysis plots&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/obss/sahi/raw/main/docs/cli.md#coco-yolov5-command-usage&#34;&gt;coco yolov5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;automatically convert any COCO dataset to &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; format&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Quick Start Examples&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=2005&amp;amp;sciodt=0,5&amp;amp;cites=14065474760484865747&amp;amp;scipsc=&amp;amp;q=&amp;amp;scisbd=1&#34;&gt;üìú List of publications that cite SAHI (currently 40+)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/obss/sahi/discussions/688&#34;&gt;üèÜ List of competition winners that used SAHI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Tutorials&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://medium.com/codable/sahi-a-vision-library-for-performing-sliced-inference-on-large-images-small-objects-c8b086af3b80&#34;&gt;Introduction to SAHI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9897990&#34;&gt;Official paper&lt;/a&gt; (ICIP 2022 oral) (NEW)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/fcakyon/small-object-detection-benchmark&#34;&gt;Pretrained weights and ICIP 2022 paper files&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learnopencv.com/slicing-aided-hyper-inference/&#34;&gt;&#39;Exploring SAHI&#39; Research Article from &#39;learnopencv.com&#39;&lt;/a&gt; (2023) (NEW)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UuOjJKxn-M8&amp;amp;t=270s&#34;&gt;&#39;VIDEO TUTORIAL: Slicing Aided Hyper Inference for Small Object Detection - SAHI&#39;&lt;/a&gt; (2023) (NEW)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/obss/sahi/discussions/626&#34;&gt;Video inference support is live&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/remekkinas/sahi-slicing-aided-hyper-inference-yv5-and-yx&#34;&gt;Kaggle notebook&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://blog.ml6.eu/how-to-detect-small-objects-in-very-large-images-70234bab0f98&#34;&gt;Satellite object detection&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/obss/sahi/discussions/622&#34;&gt;Error analysis plots &amp;amp; evaluation&lt;/a&gt; (NEW)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/obss/sahi/discussions/624&#34;&gt;Interactive result visualization and inspection&lt;/a&gt; (NEW)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://medium.com/codable/convert-any-dataset-to-coco-object-detection-format-with-sahi-95349e1fe2b7&#34;&gt;COCO dataset conversion&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/obss/sahi/main/demo/slicing.ipynb&#34;&gt;Slicing operation notebook&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;YOLOX&lt;/code&gt; + &lt;code&gt;SAHI&lt;/code&gt; demo: &lt;a href=&#34;https://huggingface.co/spaces/fcakyon/sahi-yolox&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/obss/sahi/main/resources/hf_spaces_badge.svg?sanitize=true&#34; alt=&#34;sahi-yolox&#34;&gt;&lt;/a&gt; (RECOMMENDED)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;YOLOv5&lt;/code&gt; + &lt;code&gt;SAHI&lt;/code&gt; walkthrough: &lt;a href=&#34;https://colab.research.google.com/github/obss/sahi/blob/main/demo/inference_for_yolov5.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;sahi-yolov5&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;MMDetection&lt;/code&gt; + &lt;code&gt;SAHI&lt;/code&gt; walkthrough: &lt;a href=&#34;https://colab.research.google.com/github/obss/sahi/blob/main/demo/inference_for_mmdetection.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;sahi-mmdetection&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Detectron2&lt;/code&gt; + &lt;code&gt;SAHI&lt;/code&gt; walkthrough: &lt;a href=&#34;https://colab.research.google.com/github/obss/sahi/blob/main/demo/inference_for_detectron2.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;sahi-detectron2&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;HuggingFace&lt;/code&gt; + &lt;code&gt;SAHI&lt;/code&gt; walkthrough: &lt;a href=&#34;https://colab.research.google.com/github/obss/sahi/blob/main/demo/inference_for_huggingface.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;sahi-huggingface&#34;&gt;&lt;/a&gt; (NEW)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;TorchVision&lt;/code&gt; + &lt;code&gt;SAHI&lt;/code&gt; walkthrough: &lt;a href=&#34;https://colab.research.google.com/github/obss/sahi/blob/main/demo/inference_for_torchvision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;sahi-torchvision&#34;&gt;&lt;/a&gt; (NEW)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/fcakyon/sahi-yolox&#34;&gt;&lt;img width=&#34;600&#34; src=&#34;https://user-images.githubusercontent.com/34196005/144092739-c1d9bade-a128-4346-947f-424ce00e5c4f.gif&#34; alt=&#34;sahi-yolox&#34;&gt;&lt;/a&gt;&lt;/p&gt;  &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;img width=&#34;700&#34; alt=&#34;sahi-installation&#34; src=&#34;https://user-images.githubusercontent.com/34196005/149311602-b44e6fe1-f496-40f2-a7ae-5ea1f66e1550.gif&#34;&gt; &#xA;&lt;details closed&gt; &#xA; &lt;summary&gt; &lt;big&gt;&lt;b&gt;Installation details:&lt;/b&gt;&lt;/big&gt; &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install &lt;code&gt;sahi&lt;/code&gt; using pip:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip install sahi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;On Windows, &lt;code&gt;Shapely&lt;/code&gt; needs to be installed via Conda:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;conda install -c conda-forge shapely&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install your desired version of pytorch and torchvision (cuda 11.3 for detectron2, cuda 11.7 for rest):&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;conda install pytorch=1.10.2 torchvision=0.11.3 cudatoolkit=11.3 -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;conda install pytorch=1.13.1 torchvision=0.14.1 pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install your desired detection framework (yolov5):&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip install yolov5==7.0.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install your desired detection framework (mmdet):&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu117/torch1.13.0/index.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip install mmdet==2.26.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install your desired detection framework (detectron2):&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install your desired detection framework (huggingface):&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip install transformers timm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Framework Agnostic Sliced/Standard Prediction&lt;/h3&gt; &#xA;&lt;img width=&#34;700&#34; alt=&#34;sahi-predict&#34; src=&#34;https://user-images.githubusercontent.com/34196005/149310540-e32f504c-6c9e-4691-8afd-59f3a1a457f0.gif&#34;&gt; &#xA;&lt;p&gt;Find detailed info on &lt;code&gt;sahi predict&lt;/code&gt; command at &lt;a href=&#34;https://raw.githubusercontent.com/obss/sahi/main/docs/cli.md#predict-command-usage&#34;&gt;cli.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Find detailed info on video inference at &lt;a href=&#34;https://github.com/obss/sahi/discussions/626&#34;&gt;video inference tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Find detailed info on image/dataset slicing utilities at &lt;a href=&#34;https://raw.githubusercontent.com/obss/sahi/main/docs/slicing.md&#34;&gt;slicing.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Error Analysis Plots &amp;amp; Evaluation&lt;/h3&gt; &#xA;&lt;img width=&#34;700&#34; alt=&#34;sahi-analyse&#34; src=&#34;https://user-images.githubusercontent.com/34196005/149537858-22b2e274-04e8-4e10-8139-6bdcea32feab.gif&#34;&gt; &#xA;&lt;p&gt;Find detailed info at &lt;a href=&#34;https://github.com/obss/sahi/discussions/622&#34;&gt;Error Analysis Plots &amp;amp; Evaluation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive Visualization &amp;amp; Inspection&lt;/h3&gt; &#xA;&lt;img width=&#34;700&#34; alt=&#34;sahi-fiftyone&#34; src=&#34;https://user-images.githubusercontent.com/34196005/149321540-e6ddd5f3-36dc-4267-8574-a985dd0c6578.gif&#34;&gt; &#xA;&lt;p&gt;Find detailed info at &lt;a href=&#34;https://github.com/obss/sahi/discussions/624&#34;&gt;Interactive Result Visualization and Inspection&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Other utilities&lt;/h3&gt; &#xA;&lt;p&gt;Find detailed info on COCO utilities (yolov5 conversion, slicing, subsampling, filtering, merging, splitting) at &lt;a href=&#34;https://raw.githubusercontent.com/obss/sahi/main/docs/coco.md&#34;&gt;coco.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Find detailed info on MOT utilities (ground truth dataset creation, exporting tracker metrics in mot challenge format) at &lt;a href=&#34;https://raw.githubusercontent.com/obss/sahi/main/docs/mot.md&#34;&gt;mot.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Citation&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If you use this package in your work, please cite it as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{akyon2022sahi,&#xA;  title={Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection},&#xA;  author={Akyon, Fatih Cagatay and Altinuc, Sinan Onur and Temizel, Alptekin},&#xA;  journal={2022 IEEE International Conference on Image Processing (ICIP)},&#xA;  doi={10.1109/ICIP46576.2022.9897990},&#xA;  pages={966-970},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{obss2021sahi,&#xA;  author       = {Akyon, Fatih Cagatay and Cengiz, Cemil and Altinuc, Sinan Onur and Cavusoglu, Devrim and Sahin, Kadir and Eryuksel, Ogulcan},&#xA;  title        = {{SAHI: A lightweight vision library for performing large scale object detection and instance segmentation}},&#xA;  month        = nov,&#xA;  year         = 2021,&#xA;  publisher    = {Zenodo},&#xA;  doi          = {10.5281/zenodo.5718950},&#xA;  url          = {https://doi.org/10.5281/zenodo.5718950}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Contributing&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;sahi&lt;/code&gt; library currently supports all &lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5 models&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmdetection/raw/master/docs/en/model_zoo.md&#34;&gt;MMDetection models&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/detectron2/raw/main/MODEL_ZOO.md&#34;&gt;Detectron2 models&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=object-detection&amp;amp;sort=downloads&#34;&gt;HuggingFace object detection models&lt;/a&gt;. Moreover, it is easy to add new frameworks.&lt;/p&gt; &#xA;&lt;p&gt;All you need to do is, create a new .py file under &lt;a href=&#34;https://github.com/obss/sahi/tree/main/sahi/models&#34;&gt;sahi/models/&lt;/a&gt; folder and create a new class in that .py file that implements &lt;a href=&#34;https://github.com/obss/sahi/raw/7e48bdb6afda26f977b763abdd7d8c9c170636bd/sahi/models/base.py#L12&#34;&gt;DetectionModel class&lt;/a&gt;. You can take the &lt;a href=&#34;https://github.com/obss/sahi/raw/7e48bdb6afda26f977b763abdd7d8c9c170636bd/sahi/models/mmdet.py#L18&#34;&gt;MMDetection wrapper&lt;/a&gt; or &lt;a href=&#34;https://github.com/obss/sahi/raw/7e48bdb6afda26f977b763abdd7d8c9c170636bd/sahi/models/yolov5.py#L17&#34;&gt;YOLOv5 wrapper&lt;/a&gt; as a reference.&lt;/p&gt; &#xA;&lt;p&gt;Before opening a PR:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install required development packages:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#34;[dev]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reformat with black and isort:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m scripts.run_code_style format&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Contributors&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/fcakyon&#34; target=&#34;_blank&#34;&gt;Fatih Cagatay Akyon&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/sinanonur&#34; target=&#34;_blank&#34;&gt;Sinan Onur Altinuc&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/devrimcavusoglu&#34; target=&#34;_blank&#34;&gt;Devrim Cavusoglu&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/cemilcengiz&#34; target=&#34;_blank&#34;&gt;Cemil Cengiz&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/oulcan&#34; target=&#34;_blank&#34;&gt;Ogulcan Eryuksel&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/kadirnar&#34; target=&#34;_blank&#34;&gt;Kadir Nar&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/madenburak&#34; target=&#34;_blank&#34;&gt;Burak Maden&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/PushpakBhoge&#34; target=&#34;_blank&#34;&gt;Pushpak Bhoge&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/mcvarer&#34; target=&#34;_blank&#34;&gt;M. Can V.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/ChristofferEdlund&#34; target=&#34;_blank&#34;&gt;Christoffer Edlund&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/ishworii&#34; target=&#34;_blank&#34;&gt;Ishwor&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/mecevit&#34; target=&#34;_blank&#34;&gt;Mehmet Ecevit&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/ssahinnkadir&#34; target=&#34;_blank&#34;&gt;Kadir Sahin&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/weypro&#34; target=&#34;_blank&#34;&gt;Wey&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/youngjae-avikus&#34; target=&#34;_blank&#34;&gt;Youngjae&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/tureckova&#34; target=&#34;_blank&#34;&gt;Alzbeta Tureckova&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/weiji14&#34; target=&#34;_blank&#34;&gt;Wei Ji&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/aynursusuz&#34; target=&#34;_blank&#34;&gt;Aynur Susuz&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a align=&#34;left&#34; href=&#34;https://github.com/pranavdurai10&#34; target=&#34;_blank&#34;&gt;Pranav Durai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>