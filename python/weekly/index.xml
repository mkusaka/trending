<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-06T01:45:07Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tadata-org/fastapi_mcp</title>
    <updated>2025-07-06T01:45:07Z</updated>
    <id>tag:github.com,2025-07-06:/tadata-org/fastapi_mcp</id>
    <link href="https://github.com/tadata-org/fastapi_mcp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tadata-org/fastapi_mcp&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c&#34; alt=&#34;fastapi-to-mcp&#34; height=&#34;100/&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;span style=&#34;font-size: 0.85em; font-weight: normal;&#34;&gt;Built by &lt;a href=&#34;https://tadata.com&#34;&gt;Tadata&lt;/a&gt;&lt;/span&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; FastAPI-MCP &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/fastapi-mcp/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/fastapi-mcp/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true&#34; alt=&#34;Python Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white&#34; alt=&#34;FastAPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/tadata-org/fastapi_mcp&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tadata-org/fastapi_mcp&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c&#34; alt=&#34;fastapi-mcp-usage&#34; height=&#34;400&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI&#39;s ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hosted Solution&lt;/h2&gt; &#xA;&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href=&#34;https://tadata.com&#34;&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We recommend using &lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv add fastapi-mcp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install fastapi-mcp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Basic Usage&lt;/h2&gt; &#xA;&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastapi import FastAPI&#xA;from fastapi_mcp import FastApiMCP&#xA;&#xA;app = FastAPI()&#xA;&#xA;mcp = FastApiMCP(app)&#xA;&#xA;# Mount the MCP server directly to your FastAPI app&#xA;mcp.mount()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; &#xA;&lt;p&gt;FastAPI-MCP provides &lt;a href=&#34;https://fastapi-mcp.tadata.com/&#34;&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href=&#34;https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples&#34;&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; &#xA;&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; &#xA;&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn&#39;t need to run separately from the MCP server (though &lt;a href=&#34;https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app&#34;&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; &#xA;&lt;h2&gt;Development and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; &#xA;&lt;p&gt;Before you get started, please see our &lt;a href=&#34;https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Join &lt;a href=&#34;https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg&#34;&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; &#xA; &lt;li&gt;uv&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Genesis-Embodied-AI/Genesis</title>
    <updated>2025-07-06T01:45:07Z</updated>
    <id>tag:github.com,2025-07-06:/Genesis-Embodied-AI/Genesis</id>
    <link href="https://github.com/Genesis-Embodied-AI/Genesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A generative world for general-purpose robotics &amp; embodied AI learning.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/big_text.png&#34; alt=&#34;Genesis&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/teaser.png&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/genesis-world/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/genesis-world&#34; alt=&#34;PyPI - Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/projects/genesis-world&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/genesis-world&#34; alt=&#34;PyPI Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis&#34; alt=&#34;GitHub Issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/discussions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis&#34; alt=&#34;GitHub Discussions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/nukCuhB47p&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1322086972302430269?logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/uc?export=view&amp;amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&#34; height=&#34;20&#34; style=&#34;display:inline&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/English-d9d9d9&#34; alt=&#34;README in English&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_FR.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Francais-d9d9d9&#34; alt=&#34;README en Français&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_KR.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%ED%95%9C%EA%B5%AD%EC%96%B4-d9d9d9&#34; alt=&#34;한국어 README&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_CN.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-d9d9d9&#34; alt=&#34;简体中文版自述文件&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_JA.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-d9d9d9&#34; alt=&#34;日本語版 README&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Genesis&lt;/h1&gt; &#xA;&lt;h2&gt;🔥 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2025-07-02] The development of Genesis is now officially supported by &lt;a href=&#34;https://genesis-ai.company/&#34;&gt;Genesis AI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2025-01-09] We released a &lt;a href=&#34;https://github.com/zhouxian/genesis-speed-benchmark&#34;&gt;detailed performance benchmarking and comparison report&lt;/a&gt; on Genesis, together with all the test scripts.&lt;/li&gt; &#xA; &lt;li&gt;[2025-01-08] Released v0.2.1 🎊 🎉&lt;/li&gt; &#xA; &lt;li&gt;[2025-01-08] Created &lt;a href=&#34;https://discord.gg/nukCuhB47p&#34;&gt;Discord&lt;/a&gt; and &lt;a href=&#34;https://drive.google.com/uc?export=view&amp;amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ&#34;&gt;Wechat&lt;/a&gt; group.&lt;/li&gt; &#xA; &lt;li&gt;[2024-12-25] Added a &lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#docker&#34;&gt;docker&lt;/a&gt; including support for the ray-tracing renderer&lt;/li&gt; &#xA; &lt;li&gt;[2024-12-24] Added guidelines for &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/raw/main/.github/CONTRIBUTING.md&#34;&gt;contributing to Genesis&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#what-is-genesis&#34;&gt;What is Genesis?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#key-features&#34;&gt;Key Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#quick-installation&#34;&gt;Quick Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#contributing-to-genesis&#34;&gt;Contributing to Genesis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#license-and-acknowledgments&#34;&gt;License and Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#associated-papers&#34;&gt;Associated Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;What is Genesis?&lt;/h2&gt; &#xA;&lt;p&gt;Genesis is a physics platform designed for general-purpose &lt;em&gt;Robotics/Embodied AI/Physical AI&lt;/em&gt; applications. It is simultaneously multiple things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A &lt;strong&gt;universal physics engine&lt;/strong&gt; re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.&lt;/li&gt; &#xA; &lt;li&gt;A &lt;strong&gt;lightweight&lt;/strong&gt;, &lt;strong&gt;ultra-fast&lt;/strong&gt;, &lt;strong&gt;pythonic&lt;/strong&gt;, and &lt;strong&gt;user-friendly&lt;/strong&gt; robotics simulation platform.&lt;/li&gt; &#xA; &lt;li&gt;A powerful and fast &lt;strong&gt;photo-realistic rendering system&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A &lt;strong&gt;generative data engine&lt;/strong&gt; that transforms user-prompted natural language description into various modalities of data.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Powered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully automated data generation for robotics and beyond.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Currently, we are open-sourcing the &lt;em&gt;underlying physics engine&lt;/em&gt; and the &lt;em&gt;simulation platform&lt;/em&gt;. Our &lt;em&gt;generative framework&lt;/em&gt; is a modular system that incorporates many different generative modules, each handling a certain range of data modalities, routed by a high level agent. Some of the modules integrated existing papers and some are still under submission. Access to our generative feature will be gradually rolled out in the near future. If you are interested, feel free to explore more in the &lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#associated-papers&#34;&gt;paper list&lt;/a&gt; below.&lt;/p&gt; &#xA;&lt;p&gt;Genesis aims to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lower the barrier&lt;/strong&gt; to using physics simulations, making robotics research accessible to everyone. See our &lt;a href=&#34;https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html&#34;&gt;mission statement&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unify diverse physics solvers&lt;/strong&gt; into a single framework to recreate the physical world with the highest fidelity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automate data generation&lt;/strong&gt;, reducing human effort and letting the data flywheel spin on its own.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Project Page: &lt;a href=&#34;https://genesis-embodied-ai.github.io/&#34;&gt;https://genesis-embodied-ai.github.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Over 43 million FPS when simulating a Franka robotic arm with a single RTX 4090 (430,000 times faster than real-time).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt;: Runs on Linux, macOS, Windows, and supports multiple compute backends (CPU, Nvidia/AMD GPUs, Apple Metal).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integration of diverse physics solvers&lt;/strong&gt;: Rigid body, MPM, SPH, FEM, PBD, Stable Fluid.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wide range of material models&lt;/strong&gt;: Simulation and coupling of rigid bodies, liquids, gases, deformable objects, thin-shell objects, and granular materials.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compatibility with various robots&lt;/strong&gt;: Robotic arms, legged robots, drones, &lt;em&gt;soft robots&lt;/em&gt;, and support for loading &lt;code&gt;MJCF (.xml)&lt;/code&gt;, &lt;code&gt;URDF&lt;/code&gt;, &lt;code&gt;.obj&lt;/code&gt;, &lt;code&gt;.glb&lt;/code&gt;, &lt;code&gt;.ply&lt;/code&gt;, &lt;code&gt;.stl&lt;/code&gt;, and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Photo-realistic rendering&lt;/strong&gt;: Native ray-tracing-based rendering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Differentiability&lt;/strong&gt;: Genesis is designed to be fully differentiable. Currently, our MPM solver and Tool Solver support differentiability, with other solvers planned for future versions (starting with rigid &amp;amp; articulated body solver).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Physics-based tactile simulation&lt;/strong&gt;: Differentiable &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/DiffTactile&#34;&gt;tactile sensor simulation&lt;/a&gt; coming soon (expected in version 0.3.0).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-friendliness&lt;/strong&gt;: Designed for simplicity, with intuitive installation and APIs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install &lt;strong&gt;PyTorch&lt;/strong&gt; first following the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;official instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then, install Genesis via PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install genesis-world  # Requires Python&amp;gt;=3.10,&amp;lt;3.13;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the latest version to date, make sure that &lt;code&gt;pip&lt;/code&gt; is up-to-date via &lt;code&gt;pip install --upgrade pip&lt;/code&gt;, then run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/Genesis-Embodied-AI/Genesis.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the package must still be updated manually to sync with main branch.&lt;/p&gt; &#xA;&lt;p&gt;Users seeking to edit the source code of Genesis are encourage to install Genesis in editable mode. First, make sure that &lt;code&gt;genesis-world&lt;/code&gt; has been uninstalled, then clone the repository and install locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Genesis-Embodied-AI/Genesis.git&#xA;cd Genesis&#xA;pip install -e &#34;.[dev]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use Genesis from Docker, you can first build the Docker image as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t genesis -f docker/Dockerfile docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run the examples inside the docker image (mounted to &lt;code&gt;/workspace/examples&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;xhost +local:root # Allow the container to access the display&#xA;&#xA;docker run --gpus all --rm -it \&#xA;-e DISPLAY=$DISPLAY \&#xA;-v /dev/dri:/dev/dri \&#xA;-v /tmp/.X11-unix/:/tmp/.X11-unix \&#xA;-v $PWD:/workspace \&#xA;genesis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;AMD users&lt;/h3&gt; &#xA;&lt;p&gt;AMD users can use Genesis using the &lt;code&gt;docker/Dockerfile.amdgpu&lt;/code&gt; file, which is built by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t genesis-amd -f docker/Dockerfile.amdgpu docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and can then be used by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xhost&#34;&gt;docker run -it --network=host \&#xA; --device=/dev/kfd \&#xA; --device=/dev/dri \&#xA; --group-add=video \&#xA; --ipc=host \&#xA; --cap-add=SYS_PTRACE \&#xA; --security-opt seccomp=unconfined \&#xA; --shm-size 8G \&#xA; -v $PWD:/workspace \&#xA; -e DISPLAY=$DISPLAY \&#xA; genesis-amd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The examples will be accessible from &lt;code&gt;/workspace/examples&lt;/code&gt;. Note: AMD users should use the vulkan backend. This means you will need to call &lt;code&gt;gs.init(vulkan)&lt;/code&gt; to initialise Genesis.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Comprehensive documentation is available in &lt;a href=&#34;https://genesis-world.readthedocs.io/en/latest/user_guide/index.html&#34;&gt;English&lt;/a&gt;, &lt;a href=&#34;https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html&#34;&gt;Chinese&lt;/a&gt;, and &lt;a href=&#34;https://genesis-world.readthedocs.io/ja/latest/user_guide/index.html&#34;&gt;Japanese&lt;/a&gt;. This includes detailed installation steps, tutorials, and API references.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to Genesis&lt;/h2&gt; &#xA;&lt;p&gt;The Genesis project is an open and collaborative effort. We welcome all forms of contributions from the community, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pull requests&lt;/strong&gt; for new features or bug fixes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bug reports&lt;/strong&gt; through GitHub Issues.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Suggestions&lt;/strong&gt; to improve Genesis&#39;s usability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Refer to our &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/raw/main/.github/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Report bugs or request features via GitHub &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/issues&#34;&gt;Issues&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Join discussions or ask questions on GitHub &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis/discussions&#34;&gt;Discussions&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License and Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The Genesis source code is licensed under Apache 2.0.&lt;/p&gt; &#xA;&lt;p&gt;Genesis&#39;s development has been made possible thanks to these open-source projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/taichi-dev/taichi&#34;&gt;Taichi&lt;/a&gt;: High-performance cross-platform compute backend. Kudos to the Taichi team for their technical support!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhouxian/FluidLab&#34;&gt;FluidLab&lt;/a&gt;: Reference MPM solver implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/erizmr/SPH_Taichi&#34;&gt;SPH_Taichi&lt;/a&gt;: Reference SPH solver implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matthias-research.github.io/pages/tenMinutePhysics/index.html&#34;&gt;Ten Minute Physics&lt;/a&gt; and &lt;a href=&#34;https://github.com/WASD4959/PBF3D&#34;&gt;PBF3D&lt;/a&gt;: Reference PBD solver implementations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-deepmind/mujoco&#34;&gt;MuJoCo&lt;/a&gt;: Reference for rigid body dynamics.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/danfis/libccd&#34;&gt;libccd&lt;/a&gt;: Reference for collision detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mmatl/pyrender&#34;&gt;PyRender&lt;/a&gt;: Rasterization-based renderer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LuisaGroup/LuisaCompute&#34;&gt;LuisaCompute&lt;/a&gt; and &lt;a href=&#34;https://github.com/LuisaGroup/LuisaRender&#34;&gt;LuisaRender&lt;/a&gt;: Ray-tracing DSL.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Associated Papers&lt;/h2&gt; &#xA;&lt;p&gt;Genesis is a large scale effort that integrates state-of-the-art technologies of various existing and on-going research work into a single system. Here we include a non-exhaustive list of all the papers that contributed to the Genesis project in one way or another:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Xian, Zhou, et al. &#34;Fluidlab: A differentiable environment for benchmarking complex fluid manipulation.&#34; arXiv preprint arXiv:2303.02346 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Xu, Zhenjia, et al. &#34;Roboninja: Learning an adaptive cutting policy for multi-material objects.&#34; arXiv preprint arXiv:2302.11553 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Yufei, et al. &#34;Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.&#34; arXiv preprint arXiv:2311.01455 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Tsun-Hsuan, et al. &#34;Softzoo: A soft robot co-design benchmark for locomotion in diverse environments.&#34; arXiv preprint arXiv:2303.09555 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Tsun-Hsuan Johnson, et al. &#34;Diffusebot: Breeding soft robots with physics-augmented generative diffusion models.&#34; Advances in Neural Information Processing Systems 36 (2023): 44398-44423.&lt;/li&gt; &#xA; &lt;li&gt;Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. &#34;Gen2sim: Scaling up robot learning in simulation with generative models.&#34; 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.&lt;/li&gt; &#xA; &lt;li&gt;Si, Zilin, et al. &#34;DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation.&#34; arXiv preprint arXiv:2403.08716 (2024).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Yian, et al. &#34;Thin-Shell Object Manipulations With Differentiable Physics Simulations.&#34; arXiv preprint arXiv:2404.00451 (2024).&lt;/li&gt; &#xA; &lt;li&gt;Lin, Chunru, et al. &#34;UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments.&#34; arXiv preprint arXiv:2411.12711 (2024).&lt;/li&gt; &#xA; &lt;li&gt;Zhou, Wenyang, et al. &#34;EMDM: Efficient motion diffusion model for fast and high-quality motion generation.&#34; European Conference on Computer Vision. Springer, Cham, 2025.&lt;/li&gt; &#xA; &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. &#34;Scalable differentiable physics for learning and control.&#34; International Conference on Machine Learning. PMLR, 2020.&lt;/li&gt; &#xA; &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. &#34;Efficient differentiable simulation of articulated bodies.&#34; In International Conference on Machine Learning, PMLR, 2021.&lt;/li&gt; &#xA; &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. &#34;Differentiable simulation of soft multi-body systems.&#34; Advances in Neural Information Processing Systems 34 (2021).&lt;/li&gt; &#xA; &lt;li&gt;Wan, Weilin, et al. &#34;Tlcontrol: Trajectory and language control for human motion synthesis.&#34; arXiv preprint arXiv:2311.17135 (2023).&lt;/li&gt; &#xA; &lt;li&gt;Wang, Yian, et al. &#34;Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting.&#34; arXiv preprint arXiv:2411.09823 (2024).&lt;/li&gt; &#xA; &lt;li&gt;Zheng, Shaokun, et al. &#34;LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures.&#34; ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.&lt;/li&gt; &#xA; &lt;li&gt;Fan, Yingruo, et al. &#34;Faceformer: Speech-driven 3d facial animation with transformers.&#34; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.&lt;/li&gt; &#xA; &lt;li&gt;Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. &#34;ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE.&#34; Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.&lt;/li&gt; &#xA; &lt;li&gt;Dou, Zhiyang, et al. &#34;C· ase: Learning conditional adversarial skill embeddings for physics-based characters.&#34; SIGGRAPH Asia 2023 Conference Papers. 2023.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... and many more on-going work.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Genesis in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{Genesis,&#xA;  author = {Genesis Authors},&#xA;  title = {Genesis: A Generative and Universal Physics Engine for Robotics and Beyond},&#xA;  month = {December},&#xA;  year = {2024},&#xA;  url = {https://github.com/Genesis-Embodied-AI/Genesis}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>fatihak/InkyPi</title>
    <updated>2025-07-06T01:45:07Z</updated>
    <id>tag:github.com,2025-07-06:/fatihak/InkyPi</id>
    <link href="https://github.com/fatihak/InkyPi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;E-Ink Display with a Raspberry Pi and a Web Interface to customize and update the display with various plugins&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;InkyPi&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/docs/images/inky_clock.jpg&#34;&gt; &#xA;&lt;h2&gt;About InkyPi&lt;/h2&gt; &#xA;&lt;p&gt;InkyPi is an open-source, customizable E-Ink display powered by a Raspberry Pi. Designed for simplicity and flexibility, it allows you to effortlessly display the content you care about, with a simple web interface that makes setup and configuration effortless.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Natural paper-like aethetic: crisp, minimalist visuals that are easy on the eyes, with no glare or backlight&lt;/li&gt; &#xA; &lt;li&gt;Web Interface allows you to update and configure the display from any device on your network&lt;/li&gt; &#xA; &lt;li&gt;Minimize distractions: no LEDS, noise, or notifications, just the content you care about&lt;/li&gt; &#xA; &lt;li&gt;Easy installation and configuration, perfect for beginners and makers alike&lt;/li&gt; &#xA; &lt;li&gt;Open source project allowing you to modify, customize, and create your own plugins&lt;/li&gt; &#xA; &lt;li&gt;Set up scheduled playlists to display different plugins at designated times&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plugins&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image Upload: Upload and display any image from your browser&lt;/li&gt; &#xA; &lt;li&gt;Newspaper: Show daily front pages of major newspapers from around the world&lt;/li&gt; &#xA; &lt;li&gt;Clock: Customizable clock faces for displaying time&lt;/li&gt; &#xA; &lt;li&gt;AI Image: Generate images from text prompts using OpenAI&#39;s DALL·E&lt;/li&gt; &#xA; &lt;li&gt;AI Text: Display dynamic text content using OpenAI&#39;s GPT-4o text models&lt;/li&gt; &#xA; &lt;li&gt;Weather: Display current weather conditions and multi-day forecasts with a customizable layout&lt;/li&gt; &#xA; &lt;li&gt;Calendar: Visualize your calendar from Google, Outlook, or Apple Calendar with customizable layouts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And additional plugins coming soon! For documentation on building custom plugins, see &lt;a href=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/docs/building_plugins.md&#34;&gt;Building InkyPi Plugins&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Hardware&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Raspberry Pi (4 | 3 | Zero 2 W) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Recommended to get 40 pin Pre Soldered Header&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MicroSD Card (min 8 GB) like &lt;a href=&#34;https://amzn.to/3G3Tq9W&#34;&gt;this one&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;E-Ink Display: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Inky Impression by Pimoroni &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://collabs.shop/q2jmza&#34;&gt;13.3 Inch Display&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://collabs.shop/q2jmza&#34;&gt;7.3 Inch Display&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://collabs.shop/ns6m6m&#34;&gt;5.7 Inch Display&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://collabs.shop/cpwtbh&#34;&gt;4 Inch Display&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Inky wHAT by Pimoroni &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://collabs.shop/jrzqmf&#34;&gt;4.2 Inch Display&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Waveshare e-Paper Displays &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Spectra 6 (E6) Full Color &lt;strong&gt;&lt;a href=&#34;https://www.waveshare.com/4inch-e-paper-hat-plus-e.htm?&amp;amp;aff_id=111126&#34;&gt;4 inch&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;&lt;a href=&#34;https://www.waveshare.com/7.3inch-e-paper-hat-e.htm?&amp;amp;aff_id=111126&#34;&gt;7.3 inch&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;&lt;a href=&#34;https://www.waveshare.com/13.3inch-e-paper-hat-plus-e.htm?&amp;amp;aff_id=111126&#34;&gt;13.3 inch&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Black and White &lt;strong&gt;&lt;a href=&#34;https://www.waveshare.com/7.5inch-e-paper-hat.htm?&amp;amp;aff_id=111126&#34;&gt;7.5 inch&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;&lt;a href=&#34;https://www.waveshare.com/13.3inch-e-paper-hat-k.htm?&amp;amp;aff_id=111126&#34;&gt;13.3 inch&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;See &lt;a href=&#34;https://www.waveshare.com/product/raspberry-pi/displays/e-paper.htm?&amp;amp;aff_id=111126&#34;&gt;Waveshare e-paper displays&lt;/a&gt; or visit their &lt;a href=&#34;https://amzn.to/3HPRTEZ&#34;&gt;Amazon store&lt;/a&gt; for additional models. Note that some models like the IT8951 based displays are not supported. See later section on &lt;a href=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/#waveshare-display-support&#34;&gt;Waveshare e-Paper&lt;/a&gt; compatibilty for more information.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Picture Frame or 3D Stand &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/docs/community.md&#34;&gt;community.md&lt;/a&gt; for 3D models, custom builds, and other submissions from the community&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disclosure:&lt;/strong&gt; The links above are affiliate links. I may earn a commission from qualifying purchases made through them, at no extra cost to you, which helps maintain and develop this project.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install InkyPi, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/fatihak/InkyPi.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the project directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd InkyPi&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the installation script with sudo:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo bash install/install.sh [-W &amp;lt;waveshare device model&amp;gt;]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Option:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;-W &amp;lt;waveshare device model&amp;gt; - specify this parameter &lt;strong&gt;ONLY&lt;/strong&gt; if installing for a Waveshare display. After the -W option specify the Waveshare device model e.g. epd7in3f.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;e.g. for Inky displays use:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo bash install/install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and for &lt;a href=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/#waveshare-display-support&#34;&gt;Waveshare displays&lt;/a&gt; use:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo bash install/install.sh -W epd7in3f&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;After the installation is complete, the script will prompt you to reboot your Raspberry Pi. Once rebooted, the display will update to show the InkyPi splash screen.&lt;/p&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The installation script requires sudo privileges to install and run the service. We recommend starting with a fresh installation of Raspberry Pi OS to avoid potential conflicts with existing software or configurations.&lt;/li&gt; &#xA; &lt;li&gt;The installation process will automatically enable the required SPI and I2C interfaces on your Raspberry Pi.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details, including instructions on how to image your microSD with Raspberry Pi OS, refer to &lt;a href=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/docs/installation.md&#34;&gt;installation.md&lt;/a&gt;. You can also checkout &lt;a href=&#34;https://youtu.be/L5PvQj1vfC4&#34;&gt;this YouTube tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;To update your InkyPi with the latest code changes, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to the project directory: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd InkyPi&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fetch the latest changes from the repository: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git pull&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run the update script with sudo: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo bash install/update.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This process ensures that any new updates, including code changes and additional dependencies, are properly applied without requiring a full reinstallation.&lt;/p&gt; &#xA;&lt;h2&gt;Uninstall&lt;/h2&gt; &#xA;&lt;p&gt;To install InkyPi, simply run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo bash install/uninstall.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;The InkyPi project is constantly evolving, with many exciting features and improvements planned for the future.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plugins, plugins, plugins&lt;/li&gt; &#xA; &lt;li&gt;Modular layouts to mix and match plugins&lt;/li&gt; &#xA; &lt;li&gt;Support for buttons with customizable action bindings&lt;/li&gt; &#xA; &lt;li&gt;Improved Web UI on mobile devices&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check out the public &lt;a href=&#34;https://trello.com/b/SWJYWqe4/inkypi&#34;&gt;trello board&lt;/a&gt; to explore upcoming features and vote on what you&#39;d like to see next!&lt;/p&gt; &#xA;&lt;h2&gt;Waveshare Display Support&lt;/h2&gt; &#xA;&lt;p&gt;Waveshare offers a range of e-Paper displays, similar to the Inky screens from Pimoroni, but with slightly different requirements. While Inky displays auto-configure via the inky Python library, Waveshare displays require model-specific drivers from their &lt;a href=&#34;https://github.com/waveshareteam/e-Paper/tree/master/RaspberryPi_JetsonNano/python/lib/waveshare_epd&#34;&gt;Python EPD library&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project has been tested with several Waveshare models. &lt;strong&gt;Displays based on the IT8951 controller are not supported&lt;/strong&gt;, and &lt;strong&gt;screens smaller than 4 inches are not recommended&lt;/strong&gt; due to limited resolution.&lt;/p&gt; &#xA;&lt;p&gt;If your display model has a corresponding driver in the link above, it’s likely to be compatible. When running the installation script, use the -W option to specify your display model (without the .py extension). The script will automatically fetch and install the correct driver.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Distributed under the GPL 3.0 License, see &lt;a href=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;This project includes fonts and icons with separate licensing and attribution requirements. See &lt;a href=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/docs/attribution.md&#34;&gt;Attribution&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/docs/troubleshooting.md&#34;&gt;troubleshooting guide&lt;/a&gt;. If you&#39;re still having trouble, feel free to create an issue on the &lt;a href=&#34;https://github.com/fatihak/InkyPi/issues&#34;&gt;GitHub Issues&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re using a Pi Zero W, note that there are known issues during the installation process. See &lt;a href=&#34;https://raw.githubusercontent.com/fatihak/InkyPi/main/docs/troubleshooting.md#known-issues-during-pi-zero-w-installation&#34;&gt;Known Issues during Pi Zero W Installation&lt;/a&gt; section in the troubleshooting guide for additional details..&lt;/p&gt; &#xA;&lt;h2&gt;Sponsoring&lt;/h2&gt; &#xA;&lt;p&gt;InkyPi is maintained and developed with the help of sponsors. If you enjoy the project or find it useful, consider supporting its continued development.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/sponsors/fatihak&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/345274/133218454-014a4101-b36a-48c6-a1f6-342881974938.png&#34; alt=&#34;Become a Patreon&#34; height=&#34;35&#34; width=&#34;auto&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.patreon.com/akzdev&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://c5.patreon.com/external/logo/become_a_patron_button.png&#34; alt=&#34;Become a Patreon&#34; height=&#34;35&#34; width=&#34;auto&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.buymeacoffee.com/akzdev&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://cdn.buymeacoffee.com/buttons/default-orange.png&#34; alt=&#34;Buy Me A Coffee&#34; height=&#34;35&#34; width=&#34;auto&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Check out these similar projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/txoof/PaperPi&#34;&gt;PaperPi&lt;/a&gt; - awesome project that supports waveshare devices &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;shoutout to @txoof for assisting with InkyPi&#39;s installation process&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aceinnolab/Inkycal&#34;&gt;InkyCal&lt;/a&gt; - has modular plugins for building custom dashboards&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tlstommy/PiInk&#34;&gt;PiInk&lt;/a&gt; - inspiration behind InkyPi&#39;s flask web ui&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sjnims/rpi_weather_display&#34;&gt;rpi_weather_display&lt;/a&gt; - alternative eink weather dashboard with advanced power effiency&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>