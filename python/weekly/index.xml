<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-09T01:45:03Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>virattt/ai-hedge-fund</title>
    <updated>2025-03-09T01:45:03Z</updated>
    <id>tag:github.com,2025-03-09:/virattt/ai-hedge-fund</id>
    <link href="https://github.com/virattt/ai-hedge-fund" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; &#xA;&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; &#xA;&lt;p&gt;This system employs several agents working together:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; &#xA; &lt;li&gt;Bill Ackman Agent - An activist investors, takes bold positions and pushes for change&lt;/li&gt; &#xA; &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; &#xA; &lt;li&gt;Charlie Munger Agent - Warren Buffett&#39;s partner, only buys wonderful businesses at fair prices&lt;/li&gt; &#xA; &lt;li&gt;Stanley Druckenmiller Agent - Macro trading legend who hunts for asymmetric opportunities with explosive growth potential&lt;/li&gt; &#xA; &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; &#xA; &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; &#xA; &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; &#xA; &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; &#xA; &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; &#xA; &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; &#xA; &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img width=&#34;1020&#34; alt=&#34;Screenshot 2025-03-08 at 4 45 22‚ÄØPM&#34; src=&#34;https://github.com/user-attachments/assets/d8ab891e-a083-4fed-b514-ccc9322a3e57&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the system simulates trading decisions, it does not actually trade.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/virattt&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/virattt?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; &#xA; &lt;li&gt;No warranties or guarantees provided&lt;/li&gt; &#xA; &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; &#xA; &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; &#xA; &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#running-the-hedge-fund&#34;&gt;Running the Hedge Fund&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#running-the-backtester&#34;&gt;Running the Backtester&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#project-structure&#34;&gt;Project Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests&#34;&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/virattt/ai-hedge-fund.git&#xA;cd ai-hedge-fund&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sSL https://install.python-poetry.org | python3 -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Set up your environment variables:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create .env file for your API keys&#xA;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Set your API keys:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)&#xA;# Get your OpenAI API key from https://platform.openai.com/&#xA;OPENAI_API_KEY=your-openai-api-key&#xA;&#xA;# For running LLMs hosted by groq (deepseek, llama3, etc.)&#xA;# Get your Groq API key from https://groq.com/&#xA;GROQ_API_KEY=your-groq-api-key&#xA;&#xA;# For getting financial data to power the hedge fund&#xA;# Get your Financial Datasets API key from https://financialdatasets.ai/&#xA;FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, or &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; for the hedge fund to work. If you want to use LLMs from all providers, you will need to set all API keys.&lt;/p&gt; &#xA;&lt;p&gt;Financial data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key.&lt;/p&gt; &#xA;&lt;p&gt;For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Running the Hedge Fund&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width=&#34;992&#34; alt=&#34;Screenshot 2025-01-06 at 5 50 17‚ÄØPM&#34; src=&#34;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also specify a &lt;code&gt;--show-reasoning&lt;/code&gt; flag to print the reasoning of each agent to the console.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally specify the start and end dates to make decisions for a specific time period.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running the Backtester&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width=&#34;941&#34; alt=&#34;Screenshot 2025-01-06 at 5 47 52‚ÄØPM&#34; src=&#34;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can optionally specify the start and end dates to backtest over a specific time period.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Project Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;ai-hedge-fund/&#xA;‚îú‚îÄ‚îÄ src/&#xA;‚îÇ   ‚îú‚îÄ‚îÄ agents/                   # Agent definitions and workflow&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bill_ackman.py        # Bill Ackman agent&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fundamentals.py       # Fundamental analysis agent&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ portfolio_manager.py  # Portfolio management agent&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ risk_manager.py       # Risk management agent&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentiment.py          # Sentiment analysis agent&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ technicals.py         # Technical analysis agent&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ valuation.py          # Valuation analysis agent&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ warren_buffett.py     # Warren Buffett agent&#xA;‚îÇ   ‚îú‚îÄ‚îÄ tools/                    # Agent tools&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.py                # API tools&#xA;‚îÇ   ‚îú‚îÄ‚îÄ backtester.py             # Backtesting tools&#xA;‚îÇ   ‚îú‚îÄ‚îÄ main.py # Main entry point&#xA;‚îú‚îÄ‚îÄ pyproject.toml&#xA;‚îú‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repository&lt;/li&gt; &#xA; &lt;li&gt;Create a feature branch&lt;/li&gt; &#xA; &lt;li&gt;Commit your changes&lt;/li&gt; &#xA; &lt;li&gt;Push to the branch&lt;/li&gt; &#xA; &lt;li&gt;Create a Pull Request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; &#xA;&lt;h2&gt;Feature Requests&lt;/h2&gt; &#xA;&lt;p&gt;If you have a feature request, please open an &lt;a href=&#34;https://github.com/virattt/ai-hedge-fund/issues&#34;&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>modelscope/DiffSynth-Studio</title>
    <updated>2025-03-09T01:45:03Z</updated>
    <id>tag:github.com,2025-03-09:/modelscope/DiffSynth-Studio</id>
    <link href="https://github.com/modelscope/DiffSynth-Studio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enjoy the magic of Diffusion models!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DiffSynth Studio&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/DiffSynth/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/DiffSynth&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/modelscope/DiffSynth-Studio/pull/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/modelscope/DiffSynth-Studio/commit/&#34;&gt;&lt;img src=&#34;https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio&#34; alt=&#34;GitHub latest commit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/10946&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/10946&#34; alt=&#34;modelscope%2FDiffSynth-Studio | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Document: &lt;a href=&#34;https://diffsynth-studio.readthedocs.io/zh-cn/latest/index.html&#34;&gt;https://diffsynth-studio.readthedocs.io/zh-cn/latest/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;DiffSynth Studio is a Diffusion engine. We have restructured architectures including Text Encoder, UNet, VAE, among others, maintaining compatibility with models from the open-source community while enhancing computational performance. We provide many interesting features. Enjoy the magic of Diffusion models!&lt;/p&gt; &#xA;&lt;p&gt;Until now, DiffSynth Studio has supported the following models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan-Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stepfun-ai/Step-Video-T2V&#34;&gt;StepVideo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/THUDM/CogVideoX-5b&#34;&gt;CogVideoX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-dev&#34;&gt;FLUX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ExVideo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Kwai-Kolors/Kolors&#34;&gt;Kolors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-3-medium&#34;&gt;Stable Diffusion 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt&#34;&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanDiT&#34;&gt;Hunyuan-DiT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hzwer/ECCV2022-RIFE&#34;&gt;RIFE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;Ip-Adapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guoyww/animatediff/&#34;&gt;AnimateDiff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;Stable Diffusion XL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 25, 2025&lt;/strong&gt; We support Wan-Video, a collection of SOTA video synthesis models open-sourced by Alibaba. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/&#34;&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 17, 2025&lt;/strong&gt; We support &lt;a href=&#34;https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary&#34;&gt;StepVideo&lt;/a&gt;! State-of-the-art video synthesis model! See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/stepvideo/&#34;&gt;./examples/stepvideo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 31, 2024&lt;/strong&gt; We propose EliGen, a novel framework for precise entity-level controlled text-to-image generation, complemented by an inpainting fusion pipeline to extend its capabilities to image inpainting tasks. EliGen seamlessly integrates with existing community models, such as IP-Adapter and In-Context LoRA, enhancing its versatility. For more details, see &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/EntityControl/&#34;&gt;./examples/EntityControl&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2501.01097&#34;&gt;EliGen: Entity-Level Controlled Image Generation with Regional Attention&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model: &lt;a href=&#34;https://www.modelscope.cn/models/DiffSynth-Studio/Eligen&#34;&gt;ModelScope&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/modelscope/EliGen&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Online Demo: &lt;a href=&#34;https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen&#34;&gt;ModelScope EliGen Studio&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Training Dataset: &lt;a href=&#34;https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet&#34;&gt;EliGen Train Set&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 19, 2024&lt;/strong&gt; We implement advanced VRAM management for HunyuanVideo, making it possible to generate videos at a resolution of 129x720x1280 using 24GB of VRAM, or at 129x512x384 resolution with just 6GB of VRAM. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/&#34;&gt;./examples/HunyuanVideo/&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 18, 2024&lt;/strong&gt; We propose ArtAug, an approach designed to improve text-to-image synthesis models through synthesis-understanding interactions. We have trained an ArtAug enhancement module for FLUX.1-dev in the format of LoRA. This model integrates the aesthetic understanding of Qwen2-VL-72B into FLUX.1-dev, leading to an improvement in the quality of generated images.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2412.12888&#34;&gt;https://arxiv.org/abs/2412.12888&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Examples: &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug&#34;&gt;https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model: &lt;a href=&#34;https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1&#34;&gt;ModelScope&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Demo: &lt;a href=&#34;https://modelscope.cn/aigc/imageGeneration?tab=advanced&amp;amp;versionId=7228&amp;amp;modelType=LoRA&amp;amp;sdVersion=FLUX_1&amp;amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0&#34;&gt;ModelScope&lt;/a&gt;, HuggingFace (Coming soon)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 25, 2024&lt;/strong&gt; We provide extensive FLUX ControlNet support. This project supports many different ControlNet models that can be freely combined, even if their structures differ. Additionally, ControlNet models are compatible with high-resolution refinement and partition control techniques, enabling very powerful controllable image generation. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ControlNet/&#34;&gt;&lt;code&gt;./examples/ControlNet/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 8, 2024.&lt;/strong&gt; We release the extended LoRA based on CogVideoX-5B and ExVideo. You can download this model from &lt;a href=&#34;https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1&#34;&gt;ModelScope&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024.&lt;/strong&gt; CogVideoX-5B is supported in this project. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/video_synthesis/&#34;&gt;here&lt;/a&gt;. We provide several interesting features for this text-to-video model, including&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text to video&lt;/li&gt; &#xA;   &lt;li&gt;Video editing&lt;/li&gt; &#xA;   &lt;li&gt;Self-upscaling&lt;/li&gt; &#xA;   &lt;li&gt;Video interpolation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024.&lt;/strong&gt; We have implemented an interesting painter that supports all text-to-image models. Now you can create stunning images using the painter, with assistance from AI!&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use it in our &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#usage-in-webui&#34;&gt;WebUI&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2024.&lt;/strong&gt; FLUX is supported in DiffSynth-Studio.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enable CFG and highres-fix to improve visual quality. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/README.md&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;LoRA, ControlNet, and additional models will be available soon.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 21, 2024.&lt;/strong&gt; üî•üî•üî• We propose ExVideo, a post-tuning technique aimed at enhancing the capability of video generation models. We have extended Stable Video Diffusion to achieve the generation of long videos up to 128 frames.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/ExVideoProjectPage/&#34;&gt;Project Page&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Source code is released in this repo. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Models are released on &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;HuggingFace&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2406.14130&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;You can try ExVideo in this &lt;a href=&#34;https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1&#34;&gt;Demo&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 13, 2024.&lt;/strong&gt; DiffSynth Studio is transferred to ModelScope. The developers have transitioned from &#34;I&#34; to &#34;we&#34;. Of course, I will still participate in development and maintenance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Jan 29, 2024.&lt;/strong&gt; We propose Diffutoon, a fantastic solution for toon shading.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffutoonProjectPage/&#34;&gt;Project Page&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in this project.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (IJCAI 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2401.16224&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dec 8, 2023.&lt;/strong&gt; We decide to develop a new Project, aiming to release the potential of diffusion models, especially in video synthesis. The development of this project is started.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nov 15, 2023.&lt;/strong&gt; We propose FastBlend, a powerful video deflickering algorithm.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The sd-webui extension is released on &lt;a href=&#34;https://github.com/Artiprocher/sd-webui-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Demo videos are shown on Bilibili, including three tasks. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1d94y1W7PE&#34;&gt;Video deflickering&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Lw411m71p&#34;&gt;Video interpolation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1RB4y1Z7LF&#34;&gt;Image-driven video rendering&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;The technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2311.09265&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;An unofficial ComfyUI extension developed by other users is released on &lt;a href=&#34;https://github.com/AInseven/ComfyUI-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Oct 1, 2023.&lt;/strong&gt; We release an early version of this project, namely FastSDXL. A try for building a diffusion engine.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The source codes are released on &lt;a href=&#34;https://github.com/Artiprocher/FastSDXL&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;FastSDXL includes a trainable OLSS scheduler for efficiency improvement. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;The original repo of OLSS is &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;The technical report (CIKM 2023) is released on &lt;a href=&#34;https://arxiv.org/abs/2305.14677&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;A demo video is shown on &lt;a href=&#34;https://www.bilibili.com/video/BV1w8411y7uj&#34;&gt;Bilibili&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Since OLSS requires additional training, we don&#39;t implement it in this project.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Aug 29, 2023.&lt;/strong&gt; We propose DiffSynth, a video synthesis framework.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffSynth.github.io/&#34;&gt;Project Page&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth&#34;&gt;EasyNLP&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (ECML PKDD 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2308.03463&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install from source code (recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git&#xA;cd DiffSynth-Studio&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or install from pypi (There is a delay in the update. If you want to experience the latest features, please do not use this installation method.):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install diffsynth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you encounter issues during installation, it may be caused by the packages we depend on. Please refer to the documentation of the package that caused the problem.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;torch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;sentencepiece&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmake.org&#34;&gt;cmake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.cupy.dev/en/stable/install.html&#34;&gt;cupy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage (in Python code)&lt;/h2&gt; &#xA;&lt;p&gt;The Python examples are in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt;. We provide an overview here.&lt;/p&gt; &#xA;&lt;h3&gt;Download Models&lt;/h3&gt; &#xA;&lt;p&gt;Download the pre-set models. Model IDs can be found in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/diffsynth/configs/model_config.py&#34;&gt;config file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffsynth import download_models&#xA;&#xA;download_models([&#34;FLUX.1-dev&#34;, &#34;Kolors&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download your own models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffsynth.models.downloader import download_from_huggingface, download_from_modelscope&#xA;&#xA;# From Modelscope (recommended)&#xA;download_from_modelscope(&#34;Kwai-Kolors/Kolors&#34;, &#34;vae/diffusion_pytorch_model.fp16.bin&#34;, &#34;models/kolors/Kolors/vae&#34;)&#xA;# From Huggingface&#xA;download_from_huggingface(&#34;Kwai-Kolors/Kolors&#34;, &#34;vae/diffusion_pytorch_model.fp16.safetensors&#34;, &#34;models/kolors/Kolors/vae&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Video Synthesis&lt;/h3&gt; &#xA;&lt;h4&gt;Text-to-video using CogVideoX-5B&lt;/h4&gt; &#xA;&lt;p&gt;CogVideoX-5B is released by ZhiPu. We provide an improved pipeline, supporting text-to-video, video editing, self-upscaling and video interpolation. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/video_synthesis/&#34;&gt;&lt;code&gt;examples/video_synthesis&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The video on the left is generated using the original text-to-video pipeline, while the video on the right is the result after editing and frame interpolation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006&#34;&gt;https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Long Video Synthesis&lt;/h4&gt; &#xA;&lt;p&gt;We trained extended video synthesis models, which can generate 128 frames. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&#34;&gt;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/321ee04b-8c17-479e-8a95-8cbcf21f8d7e&#34;&gt;https://github.com/user-attachments/assets/321ee04b-8c17-479e-8a95-8cbcf21f8d7e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Toon Shading&lt;/h4&gt; &#xA;&lt;p&gt;Render realistic videos in a flatten style and enable video editing features. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/Diffutoon/&#34;&gt;&lt;code&gt;examples/Diffutoon&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Video Stylization&lt;/h4&gt; &#xA;&lt;p&gt;Video stylization without video models. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/diffsynth/&#34;&gt;&lt;code&gt;examples/diffsynth&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Synthesis&lt;/h3&gt; &#xA;&lt;p&gt;Generate high-resolution images, by breaking the limitation of diffusion models! &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/&#34;&gt;&lt;code&gt;examples/image_synthesis&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;LoRA fine-tuning is supported in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/train/&#34;&gt;&lt;code&gt;examples/train&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;FLUX&lt;/th&gt; &#xA;   &lt;th&gt;Stable Diffusion 3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/984561e9-553d-4952-9443-79ce144f379f&#34; alt=&#34;image_1024_cfg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/4df346db-6f91-420a-b4c1-26e205376098&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kolors&lt;/th&gt; &#xA;   &lt;th&gt;Hunyuan-DiT&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/53ef6f41-da11-4701-8665-9f64392607bf&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/60b022c8-df3f-4541-95ab-bf39f2fa8bb5&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Stable Diffusion&lt;/th&gt; &#xA;   &lt;th&gt;Stable Diffusion XL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/6fc84611-8da6-4a1f-8fee-9a34eba3b4a5&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/67687748-e738-438c-aee5-96096f09ac90&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage (in WebUI)&lt;/h2&gt; &#xA;&lt;p&gt;Create stunning images using the painter, with assistance from AI!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/95265d21-cdd6-4125-a7cb-9fbcf6ceb7b0&#34;&gt;https://github.com/user-attachments/assets/95265d21-cdd6-4125-a7cb-9fbcf6ceb7b0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This video is not rendered in real-time.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before launching the WebUI, please download models to the folder &lt;code&gt;./models&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#download-models&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Gradio&lt;/code&gt; version&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gradio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;python apps/gradio/DiffSynth_Studio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/59613157-de51-4109-99b3-97cbffd88076&#34; alt=&#34;20240822102002&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Streamlit&lt;/code&gt; version&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install streamlit streamlit-drawable-canvas&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m streamlit run apps/streamlit/DiffSynth_Studio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>freddyaboulton/fastrtc</title>
    <updated>2025-03-09T01:45:03Z</updated>
    <id>tag:github.com,2025-03-09:/freddyaboulton/fastrtc</id>
    <link href="https://github.com/freddyaboulton/fastrtc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The python library for real-time communication&lt;/p&gt;&lt;hr&gt;&lt;div style=&#34;text-align: center; margin-bottom: 1rem; display: flex; justify-content: center; align-items: center;&#34;&gt; &#xA; &lt;h1 style=&#34;color: white; margin: 0;&#34;&gt;FastRTC&lt;/h1&gt; &#xA; &lt;img src=&#34;https://huggingface.co/datasets/freddyaboulton/bucket/resolve/main/fastrtc_logo_small.png&#34; alt=&#34;FastRTC Logo&#34; style=&#34;margin-right: 10px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div style=&#34;display: flex; flex-direction: row; justify-content: center&#34;&gt; &#xA; &lt;img style=&#34;display: block; padding-right: 5px; height: 20px;&#34; alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/pypi/v/fastrtc&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/freddyaboulton/fastrtc&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/github-white?logo=github&amp;amp;logoColor=black&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3 style=&#34;text-align: center&#34;&gt; The Real-Time Communication Library for Python. &lt;/h3&gt; &#xA;&lt;p&gt;Turn any python function into a real-time audio and video stream over WebRTC or WebSockets.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install fastrtc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to use built-in pause detection (see &lt;a href=&#34;https://fastrtc.org/userguide/audio/#reply-on-pause&#34;&gt;ReplyOnPause&lt;/a&gt;), and text to speech (see &lt;a href=&#34;https://fastrtc.org/userguide/audio/#text-to-speech&#34;&gt;Text To Speech&lt;/a&gt;), install the &lt;code&gt;vad&lt;/code&gt; and &lt;code&gt;tts&lt;/code&gt; extras:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;fastrtc[vad, tts]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üó£Ô∏è Automatic Voice Detection and Turn Taking built-in, only worry about the logic for responding to the user.&lt;/li&gt; &#xA; &lt;li&gt;üíª Automatic UI - Use the &lt;code&gt;.ui.launch()&lt;/code&gt; method to launch the webRTC-enabled built-in Gradio UI.&lt;/li&gt; &#xA; &lt;li&gt;üîå Automatic WebRTC Support - Use the &lt;code&gt;.mount(app)&lt;/code&gt; method to mount the stream on a FastAPI app and get a webRTC endpoint for your own frontend!&lt;/li&gt; &#xA; &lt;li&gt;‚ö°Ô∏è Websocket Support - Use the &lt;code&gt;.mount(app)&lt;/code&gt; method to mount the stream on a FastAPI app and get a websocket endpoint for your own frontend!&lt;/li&gt; &#xA; &lt;li&gt;üìû Automatic Telephone Support - Use the &lt;code&gt;fastphone()&lt;/code&gt; method of the stream to launch the application and get a free temporary phone number!&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ Completely customizable backend - A &lt;code&gt;Stream&lt;/code&gt; can easily be mounted on a FastAPI app so you can easily extend it to fit your production application. See the &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-claude&#34;&gt;Talk To Claude&lt;/a&gt; demo for an example on how to serve a custom JS frontend.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fastrtc.org&#34;&gt;https://fastrtc.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://fastrtc.org/cookbook/&#34;&gt;Cookbook&lt;/a&gt; for examples of how to use the library.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏èüëÄ Gemini Audio Video Chat&lt;/h3&gt; &lt;p&gt;Stream BOTH your webcam video and audio feeds to Google Gemini. You can also upload images to augment your conversation!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/9636dc97-4fee-46bb-abb8-b92e69c08c71&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/gemini-audio-video-chat&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/gemini-audio-video-chat/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è Google Gemini Real Time Voice API&lt;/h3&gt; &lt;p&gt;Talk to Gemini in real time using Google&#39;s voice API.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/ea6d18cb-8589-422b-9bba-56332d9f61de&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-gemini&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-gemini/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è OpenAI Real Time Voice API&lt;/h3&gt; &lt;p&gt;Talk to ChatGPT in real time using OpenAI&#39;s voice API.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/178bdadc-f17b-461a-8d26-e915c632ff80&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-openai&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-openai/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;ü§ñ Hello Computer&lt;/h3&gt; &lt;p&gt;Say computer before asking your question!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/afb2a3ef-c1ab-4cfb-872d-578f895a10d5&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/hello-computer&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/hello-computer/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;ü§ñ Llama Code Editor&lt;/h3&gt; &lt;p&gt;Create and edit HTML pages with just your voice! Powered by SambaNova systems.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/98523cf3-dac8-4127-9649-d91a997e3ef5&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/llama-code-editor&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/llama-code-editor/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è Talk to Claude&lt;/h3&gt; &lt;p&gt;Use the Anthropic and Play.Ht APIs to have an audio conversation with Claude.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/fb6ef07f-3ccd-444a-997b-9bc9bdc035d3&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-claude&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-claude/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üéµ Whisper Transcription&lt;/h3&gt; &lt;p&gt;Have whisper transcribe your speech in real time!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/87603053-acdc-4c8a-810f-f618c49caafb&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/whisper-realtime&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/whisper-realtime/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üì∑ Yolov10 Object Detection&lt;/h3&gt; &lt;p&gt;Run the Yolov10 model on a user webcam stream in real time!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/f82feb74-a071-4e81-9110-a01989447ceb&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/object-detection&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/object-detection/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è Kyutai Moshi&lt;/h3&gt; &lt;p&gt;Kyutai&#39;s moshi is a novel speech-to-speech model for modeling human conversations.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/becc7a13-9e89-4a19-9df2-5fb1467a0137&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/talk-to-moshi&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/talk-to-moshi/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è Hello Llama: Stop Word Detection&lt;/h3&gt; &lt;p&gt;A code editor built with Llama 3.3 70b that is triggered by the phrase &#34;Hello Llama&#34;. Build a Siri-like coding assistant in 100 lines of code!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/3e10cb15-ff1b-4b17-b141-ff0ad852e613&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/hey-llama-code-editor&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/hey-llama-code-editor/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;This is an shortened version of the official &lt;a href=&#34;https://freddyaboulton.github.io/gradio-webrtc/user-guide/&#34;&gt;usage guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;.ui.launch()&lt;/code&gt;: Launch a built-in UI for easily testing and sharing your stream. Built with &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;.fastphone()&lt;/code&gt;: Get a free temporary phone number to call into your stream. Hugging Face token required.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;.mount(app)&lt;/code&gt;: Mount the stream on a &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; app. Perfect for integrating with your already existing production system.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Echo Audio&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastrtc import Stream, ReplyOnPause&#xA;import numpy as np&#xA;&#xA;def echo(audio: tuple[int, np.ndarray]):&#xA;    # The function will be passed the audio until the user pauses&#xA;    # Implement any iterator that yields audio&#xA;    # See &#34;LLM Voice Chat&#34; for a more complete example&#xA;    yield audio&#xA;&#xA;stream = Stream(&#xA;    handler=ReplyOnPause(echo),&#xA;    modality=&#34;audio&#34;, &#xA;    mode=&#34;send-receive&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLM Voice Chat&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from fastrtc import (&#xA;    ReplyOnPause, AdditionalOutputs, Stream,&#xA;    audio_to_bytes, aggregate_bytes_to_16bit&#xA;)&#xA;import gradio as gr&#xA;from groq import Groq&#xA;import anthropic&#xA;from elevenlabs import ElevenLabs&#xA;&#xA;groq_client = Groq()&#xA;claude_client = anthropic.Anthropic()&#xA;tts_client = ElevenLabs()&#xA;&#xA;&#xA;# See &#34;Talk to Claude&#34; in Cookbook for an example of how to keep &#xA;# track of the chat history.&#xA;def response(&#xA;    audio: tuple[int, np.ndarray],&#xA;):&#xA;    prompt = groq_client.audio.transcriptions.create(&#xA;        file=(&#34;audio-file.mp3&#34;, audio_to_bytes(audio)),&#xA;        model=&#34;whisper-large-v3-turbo&#34;,&#xA;        response_format=&#34;verbose_json&#34;,&#xA;    ).text&#xA;    response = claude_client.messages.create(&#xA;        model=&#34;claude-3-5-haiku-20241022&#34;,&#xA;        max_tokens=512,&#xA;        messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],&#xA;    )&#xA;    response_text = &#34; &#34;.join(&#xA;        block.text&#xA;        for block in response.content&#xA;        if getattr(block, &#34;type&#34;, None) == &#34;text&#34;&#xA;    )&#xA;    iterator = tts_client.text_to_speech.convert_as_stream(&#xA;        text=response_text,&#xA;        voice_id=&#34;JBFqnCBsd6RMkjVDRZzb&#34;,&#xA;        model_id=&#34;eleven_multilingual_v2&#34;,&#xA;        output_format=&#34;pcm_24000&#34;&#xA;        &#xA;    )&#xA;    for chunk in aggregate_bytes_to_16bit(iterator):&#xA;        audio_array = np.frombuffer(chunk, dtype=np.int16).reshape(1, -1)&#xA;        yield (24000, audio_array)&#xA;&#xA;stream = Stream(&#xA;    modality=&#34;audio&#34;,&#xA;    mode=&#34;send-receive&#34;,&#xA;    handler=ReplyOnPause(response),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Webcam Stream&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastrtc import Stream&#xA;import numpy as np&#xA;&#xA;&#xA;def flip_vertically(image):&#xA;    return np.flip(image, axis=0)&#xA;&#xA;&#xA;stream = Stream(&#xA;    handler=flip_vertically,&#xA;    modality=&#34;video&#34;,&#xA;    mode=&#34;send-receive&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Object Detection&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastrtc import Stream&#xA;import gradio as gr&#xA;import cv2&#xA;from huggingface_hub import hf_hub_download&#xA;from .inference import YOLOv10&#xA;&#xA;model_file = hf_hub_download(&#xA;    repo_id=&#34;onnx-community/yolov10n&#34;, filename=&#34;onnx/model.onnx&#34;&#xA;)&#xA;&#xA;# git clone https://huggingface.co/spaces/fastrtc/object-detection&#xA;# for YOLOv10 implementation&#xA;model = YOLOv10(model_file)&#xA;&#xA;def detection(image, conf_threshold=0.3):&#xA;    image = cv2.resize(image, (model.input_width, model.input_height))&#xA;    new_image = model.detect_objects(image, conf_threshold)&#xA;    return cv2.resize(new_image, (500, 500))&#xA;&#xA;stream = Stream(&#xA;    handler=detection,&#xA;    modality=&#34;video&#34;, &#xA;    mode=&#34;send-receive&#34;,&#xA;    additional_inputs=[&#xA;        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3)&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running the Stream&lt;/h2&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;h3&gt;Gradio&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;stream.ui.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Telephone (Audio Only)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;```py&#xA;stream.fastphone()&#xA;```&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;FastAPI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;app = FastAPI()&#xA;stream.mount(app)&#xA;&#xA;# Optional: Add routes&#xA;@app.get(&#34;/&#34;)&#xA;async def _():&#xA;    return HTMLResponse(content=open(&#34;index.html&#34;).read())&#xA;&#xA;# uvicorn app:app --host 0.0.0.0 --port 8000&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>