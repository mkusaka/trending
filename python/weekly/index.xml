<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-28T02:07:24Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pengxiao-song/LaWGPT</title>
    <updated>2023-05-28T02:07:24Z</updated>
    <id>tag:github.com,2023-05-28:/pengxiao-song/LaWGPT</id>
    <link href="https://github.com/pengxiao-song/LaWGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ‰ Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. åŸºäºä¸­æ–‡æ³•å¾‹çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LaWGPTï¼šåŸºäºä¸­æ–‡æ³•å¾‹çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/logo/lawgpt.jpeg&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/logo/lawgpt.jpeg&#34; width=&#34;80%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/wiki&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-Wiki-brightgreen&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-beta1.0-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-Linux-9cf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/pengxiao-song/lawgpt&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#pengxiao-song/LaWGPT&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/pengxiao-song/lawgpt?color=yellow&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;https://www.lamda.nju.edu.cn/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/support-NJU--LAMDA-9cf.svg&#34;&gt;&lt;/a&gt; --&gt; &lt;/p&gt; &#xA;&lt;p&gt;LaWGPT æ˜¯ä¸€ç³»åˆ—åŸºäºä¸­æ–‡æ³•å¾‹çŸ¥è¯†çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è¯¥ç³»åˆ—æ¨¡å‹åœ¨é€šç”¨ä¸­æ–‡åŸºåº§æ¨¡å‹ï¼ˆå¦‚ Chinese-LLaMAã€ChatGLM ç­‰ï¼‰çš„åŸºç¡€ä¸Šæ‰©å……æ³•å¾‹é¢†åŸŸä¸“æœ‰è¯è¡¨ã€&lt;strong&gt;å¤§è§„æ¨¡ä¸­æ–‡æ³•å¾‹è¯­æ–™é¢„è®­ç»ƒ&lt;/strong&gt;ï¼Œå¢å¼ºäº†å¤§æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œ&lt;strong&gt;æ„é€ æ³•å¾‹é¢†åŸŸå¯¹è¯é—®ç­”æ•°æ®é›†ã€ä¸­å›½å¸æ³•è€ƒè¯•æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤ç²¾è°ƒ&lt;/strong&gt;ï¼Œæå‡äº†æ¨¡å‹å¯¹æ³•å¾‹å†…å®¹çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è¯¦ç»†å†…å®¹è¯·å‚è€ƒ&lt;a href=&#34;&#34;&gt;æŠ€æœ¯æŠ¥å‘Š&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®æŒç»­å¼€å±•ï¼Œæ³•å¾‹é¢†åŸŸæ•°æ®é›†åŠç³»åˆ—æ¨¡å‹åç»­ç›¸ç»§å¼€æºï¼Œæ•¬è¯·å…³æ³¨ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;æ›´æ–°&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ“£ 2023/05/26ï¼šå¼€æ”¾ &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/discussions&#34;&gt;Discussions è®¨è®ºåŒº&lt;/a&gt;ï¼Œæ¬¢è¿æœ‹å‹ä»¬äº¤æµæ¢è®¨ã€æå‡ºæ„è§ã€åˆ†äº«è§‚ç‚¹ï¼&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ› ï¸ 2023/05/22ï¼šé¡¹ç›®ä¸»åˆ†æ”¯ç»“æ„è°ƒæ•´ï¼Œè¯¦è§&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT#%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84&#34;&gt;é¡¹ç›®ç»“æ„&lt;/a&gt;ï¼›æ”¯æŒ&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/raw/main/scripts/infer.sh&#34;&gt;å‘½ä»¤è¡Œæ‰¹é‡æ¨ç†&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸª´ 2023/05/15ï¼šå‘å¸ƒ &lt;a href=&#34;https://github.com/pengxiao-song/awesome-chinese-legal-resources&#34;&gt;ä¸­æ–‡æ³•å¾‹æ•°æ®æºæ±‡æ€»ï¼ˆAwesome Chinese Legal Resourcesï¼‰&lt;/a&gt; å’Œ &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/raw/main/resources/legal_vocab.txt&#34;&gt;æ³•å¾‹é¢†åŸŸè¯è¡¨&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸŒŸ 2023/05/13ï¼šå…¬å¼€å‘å¸ƒ &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-Legal--Base--7B-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-LaWGPT--7B--beta1.0-yellow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Legal-Base-7B&lt;/strong&gt;ï¼šæ³•å¾‹åŸºåº§æ¨¡å‹ï¼Œä½¿ç”¨ 50w ä¸­æ–‡è£åˆ¤æ–‡ä¹¦æ•°æ®äºŒæ¬¡é¢„è®­ç»ƒ&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;LaWGPT-7B-beta1.0&lt;/strong&gt;ï¼šæ³•å¾‹å¯¹è¯æ¨¡å‹ï¼Œæ„é€  30w é«˜è´¨é‡æ³•å¾‹é—®ç­”æ•°æ®é›†åŸºäº Legal-Base-7B æŒ‡ä»¤ç²¾è°ƒ&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸŒŸ 2023/04/12ï¼šå†…éƒ¨æµ‹è¯• &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-Lawgpt--7B--alpha-yellow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;LaWGPT-7B-alpha&lt;/strong&gt;ï¼šåœ¨ Chinese-LLaMA-7B çš„åŸºç¡€ä¸Šç›´æ¥æ„é€  30w æ³•å¾‹é—®ç­”æ•°æ®é›†æŒ‡ä»¤ç²¾è°ƒ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å¿«é€Ÿå¼€å§‹&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;å‡†å¤‡ä»£ç ï¼Œåˆ›å»ºç¯å¢ƒ&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ä¸‹è½½ä»£ç &#xA;git clone git@github.com:pengxiao-song/LaWGPT.git&#xA;cd LaWGPT&#xA;&#xA;# åˆ›å»ºç¯å¢ƒ&#xA;conda create -n lawgpt python=3.10 -y&#xA;conda activate lawgpt&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¯åŠ¨ web uiï¼ˆå¯é€‰ï¼Œæ˜“äºè°ƒèŠ‚å‚æ•°ï¼‰&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;é¦–å…ˆï¼Œæ‰§è¡ŒæœåŠ¡å¯åŠ¨è„šæœ¬ï¼š&lt;code&gt;bash scripts/webui.sh&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;å…¶æ¬¡ï¼Œè®¿é—® &lt;a href=&#34;http://127.0.0.1:7860&#34;&gt;http://127.0.0.1:7860&lt;/a&gt; ï¼š&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p align=&#34;center&#34;&gt; &lt;img style=&#34;border-radius: 50%; box-shadow: 0 0 10px rgba(0,0,0,0.5); width: 80%;&#34; , src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-03.jpeg&#34;&gt; &lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;å‘½ä»¤è¡Œæ¨ç†ï¼ˆå¯é€‰ï¼Œæ”¯æŒæ‰¹é‡æµ‹è¯•ï¼‰&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;é¦–å…ˆï¼Œå‚è€ƒ &lt;code&gt;resources/example_infer_data.json&lt;/code&gt; æ–‡ä»¶å†…å®¹æ„é€ æµ‹è¯•æ ·æœ¬é›†ï¼›&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;å…¶æ¬¡ï¼Œæ‰§è¡Œæ¨ç†è„šæœ¬ï¼š&lt;code&gt;bash scripts/infer.sh&lt;/code&gt;ã€‚å…¶ä¸­ &lt;code&gt;--infer_data_path&lt;/code&gt; å‚æ•°ä¸ºæµ‹è¯•æ ·æœ¬é›†è·¯å¾„ï¼Œå¦‚æœä¸ºç©ºæˆ–è€…è·¯å¾„å‡ºé”™ï¼Œåˆ™ä»¥äº¤äº’æ¨¡å¼è¿è¡Œã€‚&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;æ³¨æ„ï¼Œä»¥ä¸Šæ­¥éª¤çš„é»˜è®¤æ¨¡å‹ä¸º LaWGPT-7B-alpha ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨ LaWGPT-7B-beta1.0 æ¨¡å‹ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ç”±äº &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; å’Œ &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-LLaMA&lt;/a&gt; å‡æœªå¼€æºæ¨¡å‹æƒé‡ã€‚æ ¹æ®ç›¸åº”å¼€æºè®¸å¯ï¼Œ&lt;strong&gt;æœ¬é¡¹ç›®åªèƒ½å‘å¸ƒ LoRA æƒé‡&lt;/strong&gt;ï¼Œæ— æ³•å‘å¸ƒå®Œæ•´çš„æ¨¡å‹æƒé‡ï¼Œè¯·å„ä½è°…è§£ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æœ¬é¡¹ç›®ç»™å‡º&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/wiki/%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6&#34;&gt;åˆå¹¶æ–¹å¼&lt;/a&gt;ï¼Œè¯·å„ä½è·å–åŸç‰ˆæƒé‡åè‡ªè¡Œé‡æ„æ¨¡å‹ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;é¡¹ç›®ç»“æ„&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;LaWGPT&#xA;â”œâ”€â”€ assets    # é™æ€èµ„æº&#xA;â”œâ”€â”€ resources # é¡¹ç›®èµ„æº&#xA;â”œâ”€â”€ models    # åŸºåº§æ¨¡å‹åŠ lora æƒé‡&#xA;â”‚   â”œâ”€â”€ base_models&#xA;â”‚   â””â”€â”€ lora_weights&#xA;â”œâ”€â”€ outputs   # æŒ‡ä»¤å¾®è°ƒçš„è¾“å‡ºæƒé‡&#xA;â”œâ”€â”€ data      # å®éªŒæ•°æ®&#xA;â”œâ”€â”€ scripts   # è„šæœ¬ç›®å½•&#xA;â”‚   â”œâ”€â”€ finetune.sh # æŒ‡ä»¤å¾®è°ƒè„šæœ¬&#xA;â”‚   â””â”€â”€ webui.sh    # å¯åŠ¨æœåŠ¡è„šæœ¬&#xA;â”œâ”€â”€ templates # prompt æ¨¡æ¿&#xA;â”œâ”€â”€ tools     # å·¥å…·åŒ…&#xA;â”œâ”€â”€ utils&#xA;â”œâ”€â”€ train_clm.py  # äºŒæ¬¡è®­ç»ƒ&#xA;â”œâ”€â”€ finetune.py   # æŒ‡ä»¤å¾®è°ƒ&#xA;â”œâ”€â”€ webui.py      # å¯åŠ¨æœåŠ¡&#xA;â”œâ”€â”€ README.md&#xA;â””â”€â”€ requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;æ•°æ®æ„å»º&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®åŸºäºä¸­æ–‡è£åˆ¤æ–‡ä¹¦ç½‘å…¬å¼€æ³•å¾‹æ–‡ä¹¦æ•°æ®ã€å¸æ³•è€ƒè¯•æ•°æ®ç­‰æ•°æ®é›†å±•å¼€ï¼Œè¯¦æƒ…å‚è€ƒ&lt;a href=&#34;&#34;&gt;ä¸­æ–‡æ³•å¾‹æ•°æ®æ±‡æ€»&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;åˆçº§æ•°æ®ç”Ÿæˆï¼šæ ¹æ® &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca#data-generation-process&#34;&gt;Stanford_alpaca&lt;/a&gt; å’Œ &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct&lt;/a&gt; æ–¹å¼ç”Ÿæˆå¯¹è¯é—®ç­”æ•°æ®&lt;/li&gt; &#xA; &lt;li&gt;çŸ¥è¯†å¼•å¯¼çš„æ•°æ®ç”Ÿæˆï¼šé€šè¿‡ Knowledge-based Self-Instruct æ–¹å¼åŸºäºä¸­æ–‡æ³•å¾‹ç»“æ„åŒ–çŸ¥è¯†ç”Ÿæˆæ•°æ®ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¼•å…¥ ChatGPT æ¸…æ´—æ•°æ®ï¼Œè¾…åŠ©æ„é€ é«˜è´¨é‡æ•°æ®é›†ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;æ¨¡å‹è®­ç»ƒ&lt;/h2&gt; &#xA;&lt;p&gt;LawGPT ç³»åˆ—æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ç¬¬ä¸€é˜¶æ®µï¼šæ‰©å……æ³•å¾‹é¢†åŸŸè¯è¡¨ï¼Œåœ¨å¤§è§„æ¨¡æ³•å¾‹æ–‡ä¹¦åŠæ³•å…¸æ•°æ®ä¸Šé¢„è®­ç»ƒ Chinese-LLaMA&lt;/li&gt; &#xA; &lt;li&gt;ç¬¬äºŒé˜¶æ®µï¼šæ„é€ æ³•å¾‹é¢†åŸŸå¯¹è¯é—®ç­”æ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠæŒ‡ä»¤ç²¾è°ƒ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;äºŒæ¬¡è®­ç»ƒæµç¨‹&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å‚è€ƒ &lt;code&gt;resources/example_instruction_train.json&lt;/code&gt; æ„é€ äºŒæ¬¡è®­ç»ƒæ•°æ®é›†&lt;/li&gt; &#xA; &lt;li&gt;è¿è¡Œ &lt;code&gt;scripts/train_clm.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;æŒ‡ä»¤ç²¾è°ƒæ­¥éª¤&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å‚è€ƒ &lt;code&gt;resources/example_instruction_tune.json&lt;/code&gt; æ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†&lt;/li&gt; &#xA; &lt;li&gt;è¿è¡Œ &lt;code&gt;scripts/finetune.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;è®¡ç®—èµ„æº&lt;/h3&gt; &#xA;&lt;p&gt;8 å¼  Tesla V100-SXM2-32GB ï¼šäºŒæ¬¡è®­ç»ƒé˜¶æ®µè€—æ—¶çº¦ 24h / epochï¼Œå¾®è°ƒé˜¶æ®µè€—æ—¶çº¦ 12h / epoch&lt;/p&gt; &#xA;&lt;h2&gt;æ¨¡å‹è¯„ä¼°&lt;/h2&gt; &#xA;&lt;h3&gt;è¾“å‡ºç¤ºä¾‹&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šé…’é©¾æ’äººæ€ä¹ˆåˆ¤åˆ‘ï¼Ÿ&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/demo07.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šè¯·ç»™å‡ºåˆ¤å†³æ„è§ã€‚&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-05.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šè¯·ä»‹ç»èµŒåšç½ªçš„å®šä¹‰ã€‚&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-06.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šè¯·é—®åŠ ç­å·¥èµ„æ€ä¹ˆç®—ï¼Ÿ&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-04.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šæ°‘é—´å€Ÿè´·å—å›½å®¶ä¿æŠ¤çš„åˆæ³•åˆ©æ¯æ˜¯å¤šå°‘?&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-02.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šæ¬ äº†ä¿¡ç”¨å¡çš„é’±è¿˜ä¸ä¸Šè¦åç‰¢å—ï¼Ÿ&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-01.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šä½ èƒ½å¦å†™ä¸€æ®µæŠ¢åŠ«ç½ªç½ªåçš„æ¡ˆæƒ…æè¿°ï¼Ÿ&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-03.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;å±€é™æ€§&lt;/h3&gt; &#xA;&lt;p&gt;ç”±äºè®¡ç®—èµ„æºã€æ•°æ®è§„æ¨¡ç­‰å› ç´ é™åˆ¶ï¼Œå½“å‰é˜¶æ®µ LawGPT å­˜åœ¨è¯¸å¤šå±€é™æ€§ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æ•°æ®èµ„æºæœ‰é™ã€æ¨¡å‹å®¹é‡è¾ƒå°ï¼Œå¯¼è‡´å…¶ç›¸å¯¹è¾ƒå¼±çš„æ¨¡å‹è®°å¿†å’Œè¯­è¨€èƒ½åŠ›ã€‚å› æ­¤ï¼Œåœ¨é¢å¯¹äº‹å®æ€§çŸ¥è¯†ä»»åŠ¡æ—¶ï¼Œå¯èƒ½ä¼šç”Ÿæˆä¸æ­£ç¡®çš„ç»“æœã€‚&lt;/li&gt; &#xA; &lt;li&gt;è¯¥ç³»åˆ—æ¨¡å‹åªè¿›è¡Œäº†åˆæ­¥çš„äººç±»æ„å›¾å¯¹é½ã€‚å› æ­¤ï¼Œå¯èƒ½äº§ç”Ÿä¸å¯é¢„æµ‹çš„æœ‰å®³å†…å®¹ä»¥åŠä¸ç¬¦åˆäººç±»åå¥½å’Œä»·å€¼è§‚çš„å†…å®¹ã€‚&lt;/li&gt; &#xA; &lt;li&gt;è‡ªæˆ‘è®¤çŸ¥èƒ½åŠ›å­˜åœ¨é—®é¢˜ï¼Œä¸­æ–‡ç†è§£èƒ½åŠ›æœ‰å¾…å¢å¼ºã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;è¯·è¯¸å›åœ¨ä½¿ç”¨å‰äº†è§£ä¸Šè¿°é—®é¢˜ï¼Œä»¥å…é€ æˆè¯¯è§£å’Œä¸å¿…è¦çš„éº»çƒ¦ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;åä½œè€…&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚ä¸‹å„ä½åˆä½œå¼€å±•ï¼ˆæŒ‰å­—æ¯åºæ’åˆ—ï¼‰ï¼š&lt;a href=&#34;https://github.com/herobrine19&#34;&gt;@cainiao&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/njuyxw&#34;&gt;@njuyxw&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/pengxiao-song&#34;&gt;@pengxiao-song&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;å…è´£å£°æ˜&lt;/h2&gt; &#xA;&lt;p&gt;è¯·å„ä½ä¸¥æ ¼éµå®ˆå¦‚ä¸‹çº¦å®šï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®ä»»ä½•èµ„æº&lt;strong&gt;ä»…ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨ï¼Œä¸¥ç¦ä»»ä½•å•†ä¸šç”¨é€”&lt;/strong&gt;ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ¨¡å‹è¾“å‡ºå—å¤šç§ä¸ç¡®å®šæ€§å› ç´ å½±å“ï¼Œæœ¬é¡¹ç›®å½“å‰æ— æ³•ä¿è¯å…¶å‡†ç¡®æ€§ï¼Œ&lt;strong&gt;ä¸¥ç¦ç”¨äºçœŸå®æ³•å¾‹åœºæ™¯&lt;/strong&gt;ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œäº¦ä¸å¯¹å› ä½¿ç”¨ç›¸å…³èµ„æºå’Œè¾“å‡ºç»“æœè€Œå¯èƒ½äº§ç”Ÿçš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;é—®é¢˜åé¦ˆ&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨ GitHub Issue ä¸­æäº¤ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æäº¤é—®é¢˜ä¹‹å‰ï¼Œå»ºè®®æŸ¥é˜… FAQ åŠä»¥å¾€çš„ issue çœ‹æ˜¯å¦èƒ½è§£å†³æ‚¨çš„é—®é¢˜ã€‚&lt;/li&gt; &#xA; &lt;li&gt;è¯·ç¤¼è²Œè®¨è®ºï¼Œæ„å»ºå’Œè°ç¤¾åŒºã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;åä½œè€…ç§‘ç ”ä¹‹ä½™æ¨è¿›é¡¹ç›®è¿›å±•ï¼Œç”±äºäººåŠ›æœ‰é™éš¾ä»¥å®æ—¶åé¦ˆï¼Œç»™è¯¸å›å¸¦æ¥ä¸ä¾¿ï¼Œæ•¬è¯·è°…è§£ï¼&lt;/p&gt; &#xA;&lt;h2&gt;è‡´è°¢&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®åŸºäºå¦‚ä¸‹å¼€æºé¡¹ç›®å±•å¼€ï¼Œåœ¨æ­¤å¯¹ç›¸å…³é¡¹ç›®å’Œå¼€å‘äººå‘˜è¡¨ç¤ºè¯šæŒšçš„æ„Ÿè°¢ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chinese-LLaMA-Alpaca: &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;https://github.com/ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLaMA: &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;https://github.com/facebookresearch/llama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alpaca: &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;alpaca-lora: &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ChatGLM-6B: &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;https://github.com/THUDM/ChatGLM-6B&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æ­¤å¤–ï¼Œæœ¬é¡¹ç›®åŸºäºå¼€æ”¾æ•°æ®èµ„æºï¼Œè¯¦è§ &lt;a href=&#34;https://github.com/pengxiao-song/awesome-chinese-legal-resources&#34;&gt;Awesome Chinese Legal Resources&lt;/a&gt;ï¼Œä¸€å¹¶è¡¨ç¤ºæ„Ÿè°¢ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å¼•ç”¨&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨è¯¥é¡¹ç›®&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>logspace-ai/langflow</title>
    <updated>2023-05-28T02:07:24Z</updated>
    <id>tag:github.com,2023-05-28:/logspace-ai/langflow</id>
    <link href="https://github.com/logspace-ai/langflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;â›“ï¸ LangFlow is a UI for LangChain, designed with react-flow to provide an effortless way to experiment and prototype flows.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;â›“ï¸ LangFlow&lt;/h1&gt; &#xA;&lt;p&gt;~ A User Interface For &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; ~&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/Logspace/LangFlow&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg?sanitize=true&#34; alt=&#34;HuggingFace Spaces&#34;&gt;&lt;/a&gt; &lt;img alt=&#34;GitHub Contributors&#34; src=&#34;https://img.shields.io/github/contributors/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;GitHub Last Commit&#34; src=&#34;https://img.shields.io/github/last-commit/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;&#34; src=&#34;https://img.shields.io/github/repo-size/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;GitHub Issues&#34; src=&#34;https://img.shields.io/github/issues/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;GitHub Pull Requests&#34; src=&#34;https://img.shields.io/github/issues-pr/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;Github License&#34; src=&#34;https://img.shields.io/github/license/logspace-ai/langflow&#34;&gt; &lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/logspace-ai/langflow&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://github.com/logspace-ai/langflow/raw/main/img/langflow-demo.gif?raw=true&#34;&gt;&lt;/a&gt; &#xA;&lt;p&gt;LangFlow is a GUI for &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;, designed with &lt;a href=&#34;https://github.com/wbkd/react-flow&#34;&gt;react-flow&lt;/a&gt; to provide an effortless way to experiment and prototype flows with drag-and-drop components and a chat box.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“¦ Installation&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;b&gt;Locally&lt;/b&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can install LangFlow from pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install langflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m langflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;langflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploy Langflow on Google Cloud Platform&lt;/h3&gt; &#xA;&lt;p&gt;Follow our step-by-step guide to deploy Langflow on Google Cloud Platform (GCP) using Google Cloud Shell. The guide is available in the &lt;a href=&#34;https://raw.githubusercontent.com/logspace-ai/langflow/dev/GCP_DEPLOYMENT.md&#34;&gt;&lt;strong&gt;Langflow in Google Cloud Platform&lt;/strong&gt;&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, click the &lt;strong&gt;&#34;Open in Cloud Shell&#34;&lt;/strong&gt; button below to launch Google Cloud Shell, clone the Langflow repository, and start an &lt;strong&gt;interactive tutorial&lt;/strong&gt; that will guide you through the process of setting up the necessary resources and deploying Langflow on your GCP project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/logspace-ai/langflow&amp;amp;working_dir=scripts&amp;amp;shellonly=true&amp;amp;tutorial=walkthroughtutorial_spot.md&#34;&gt;&lt;img src=&#34;https://gstatic.com/cloudssh/images/open-btn.svg?sanitize=true&#34; alt=&#34;Open in Cloud Shell&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Deploy Langflow on &lt;a href=&#34;https://github.com/jina-ai/langchain-serve&#34;&gt;Jina AI Cloud&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Langflow integrates with langchain-serve to provide a one-command deployment to Jina AI Cloud.&lt;/p&gt; &#xA;&lt;p&gt;Start by installing &lt;code&gt;langchain-serve&lt;/code&gt; with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U langchain-serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;langflow --jcloud&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;ğŸ‰ Langflow server successfully deployed on Jina AI Cloud ğŸ‰&#xA;ğŸ”— Click on the link to open the server (please allow ~1-2 minutes for the server to startup): https://&amp;lt;your-app&amp;gt;.wolf.jina.ai/&#xA;ğŸ“– Read more about managing the server: https://github.com/jina-ai/langchain-serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show complete (example) output&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;  ğŸš€ Deploying Langflow server on Jina AI Cloud&#xA;  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ‰ Flow is available! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®&#xA;  â”‚                                                                          â”‚&#xA;  â”‚   ID                    langflow-e3dd8820ec                              â”‚&#xA;  â”‚   Gateway (Websocket)   wss://langflow-e3dd8820ec.wolf.jina.ai           â”‚&#xA;  â”‚   Dashboard             https://dashboard.wolf.jina.ai/flow/e3dd8820ec   â”‚&#xA;  â”‚                                                                          â”‚&#xA;  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯&#xA;  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®&#xA;  â”‚ App ID       â”‚                     langflow-e3dd8820ec                                      â”‚&#xA;  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤&#xA;  â”‚ Phase        â”‚                            Serving                                           â”‚&#xA;  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤&#xA;  â”‚ Endpoint     â”‚          wss://langflow-e3dd8820ec.wolf.jina.ai                              â”‚&#xA;  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤&#xA;  â”‚ App logs     â”‚                  dashboards.wolf.jina.ai                                     â”‚&#xA;  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤&#xA;  â”‚ Swagger UI   â”‚          https://langflow-e3dd8820ec.wolf.jina.ai/docs                       â”‚&#xA;  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤&#xA;  â”‚ OpenAPI JSON â”‚        https://langflow-e3dd8820ec.wolf.jina.ai/openapi.json                 â”‚&#xA;  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯&#xA;&#xA;  ğŸ‰ Langflow server successfully deployed on Jina AI Cloud ğŸ‰&#xA;  ğŸ”— Click on the link to open the server (please allow ~1-2 minutes for the server to startup): https://langflow-e3dd8820ec.wolf.jina.ai/&#xA;  ğŸ“– Read more about managing the server: https://github.com/jina-ai/langchain-serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;API Usage&lt;/h4&gt; &#xA;&lt;p&gt;You can use Langflow directly on your browser, or use the API endpoints on Jina AI Cloud to interact with the server.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show API usage (with python)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;import requests&#xA;&#xA;FLOW_PATH = &#34;Time_traveller.json&#34;&#xA;&#xA;# HOST = &#39;http://localhost:7860&#39;&#xA;HOST = &#39;https://langflow-f1ed20e309.wolf.jina.ai&#39;&#xA;API_URL = f&#39;{HOST}/predict&#39;&#xA;&#xA;def predict(message):&#xA;    with open(FLOW_PATH, &#34;r&#34;) as f:&#xA;        json_data = json.load(f)&#xA;    payload = {&#39;exported_flow&#39;: json_data, &#39;message&#39;: message}&#xA;    response = requests.post(API_URL, json=payload)&#xA;    return response.json()&#xA;&#xA;&#xA;predict(&#39;Take me to 1920s Bangalore&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;result&#34;: &#34;Great choice! Bangalore in the 1920s was a vibrant city with a rich cultural and political scene. Here are some suggestions for things to see and do:\n\n1. Visit the Bangalore Palace - built in 1887, this stunning palace is a perfect example of Tudor-style architecture. It was home to the Maharaja of Mysore and is now open to the public.\n\n2. Attend a performance at the Ravindra Kalakshetra - this cultural center was built in the 1920s and is still a popular venue for music and dance performances.\n\n3. Explore the neighborhoods of Basavanagudi and Malleswaram - both of these areas have retained much of their old-world charm and are great places to walk around and soak up the atmosphere.\n\n4. Check out the Bangalore Club - founded in 1868, this exclusive social club was a favorite haunt of the British expat community in the 1920s.\n\n5. Attend a meeting of the Indian National Congress - founded in 1885, the INC was a major force in the Indian independence movement and held many meetings and rallies in Bangalore in the 1920s.\n\nHope you enjoy your trip to 1920s Bangalore!&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Read more about resource customization, cost, and management of Langflow apps on Jina AI Cloud in the &lt;strong&gt;&lt;a href=&#34;https://github.com/jina-ai/langchain-serve&#34;&gt;langchain-serve&lt;/a&gt;&lt;/strong&gt; repository.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ğŸ¨ Creating Flows&lt;/h2&gt; &#xA;&lt;p&gt;Creating flows with LangFlow is easy. Simply drag sidebar components onto the canvas and connect them together to create your pipeline. LangFlow provides a range of &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/reference.html&#34;&gt;LangChain components&lt;/a&gt; to choose from, including LLMs, prompt serializers, agents, and chains.&lt;/p&gt; &#xA;&lt;p&gt;Explore by editing prompt parameters, link chains and agents, track an agent&#39;s thought process, and export your flow.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;re done, you can export your flow as a JSON file to use with LangChain. To do so, click the &#34;Export&#34; button in the top right corner of the canvas, then in Python, you can load the flow with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langflow import load_flow_from_json&#xA;&#xA;flow = load_flow_from_json(&#34;path/to/flow.json&#34;)&#xA;# Now you can use it like any chain&#xA;flow(&#34;Hey, have you heard of LangFlow?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ‘‹ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from developers of all levels to our open-source project on GitHub. If you&#39;d like to contribute, please check our &lt;a href=&#34;https://raw.githubusercontent.com/logspace-ai/langflow/dev/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; and help make LangFlow more accessible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#logspace-ai/langflow&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=logspace-ai/langflow&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; &#xA;&lt;p&gt;LangFlow is released under the MIT License. See the LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Rudrabha/Wav2Lip</title>
    <updated>2023-05-28T02:07:24Z</updated>
    <id>tag:github.com,2023-05-28:/Rudrabha/Wav2Lip</id>
    <link href="https://github.com/Rudrabha/Wav2Lip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains the codes of &#34;A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild&#34;, published at ACM Multimedia 2020.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;Wav2Lip&lt;/strong&gt;: &lt;em&gt;Accurately Lip-syncing Videos In The Wild&lt;/em&gt;&lt;/h1&gt; &#xA;&lt;p&gt;For commercial requests, please contact us at &lt;a href=&#34;mailto:radrabha.m@research.iiit.ac.in&#34;&gt;radrabha.m@research.iiit.ac.in&lt;/a&gt; or &lt;a href=&#34;mailto:prajwal.k@research.iiit.ac.in&#34;&gt;prajwal.k@research.iiit.ac.in&lt;/a&gt;. We have an HD model ready that can be used commercially.&lt;/p&gt; &#xA;&lt;p&gt;This code is part of the paper: &lt;em&gt;A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild&lt;/em&gt; published at ACM Multimedia 2020.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/lip-sync-on-lrs2?p=a-lip-sync-expert-is-all-you-need-for-speech&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/lip-sync-on-lrs3?p=a-lip-sync-expert-is-all-you-need-for-speech&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/lip-sync-on-lrw?p=a-lip-sync-expert-is-all-you-need-for-speech&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrw&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ğŸ“‘ Original Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ğŸ“° Project Page&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ğŸŒ€ Demo&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;âš¡ Live Testing&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ğŸ“” Colab Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2008.10010&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/&#34;&gt;Project Page&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://youtu.be/0fXaDCZNOJc&#34;&gt;Demo Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://bhaasha.iiit.ac.in/lipsync&#34;&gt;Interactive Demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1tZpDWXz49W6wDcTprANRGLo2D_EbD5J8?usp=sharing&#34;&gt;Colab Notebook&lt;/a&gt; /&lt;a href=&#34;https://colab.research.google.com/drive/1IjFW1cLevs6Ouyu4Yht4mnR4yeuMqO7Y#scrollTo=MH1m608OymLH&#34;&gt;Updated Collab Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;img src=&#34;https://drive.google.com/uc?export=view&amp;amp;id=1Wn0hPmpo4GRbCIJR8Tf20Akzdi1qjjG9&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Weights of the visual quality disc has been updated in readme!&lt;/li&gt; &#xA; &lt;li&gt;Lip-sync videos to any target speech with high accuracy &lt;span&gt;ğŸ’¯&lt;/span&gt;. Try our &lt;a href=&#34;https://bhaasha.iiit.ac.in/lipsync&#34;&gt;interactive demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;âœ¨&lt;/span&gt; Works for any identity, voice, and language. Also works for CGI faces and synthetic voices.&lt;/li&gt; &#xA; &lt;li&gt;Complete training code, inference code, and pretrained models are available &lt;span&gt;ğŸ’¥&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Or, quick-start with the Google Colab Notebook: &lt;a href=&#34;https://colab.research.google.com/drive/1tZpDWXz49W6wDcTprANRGLo2D_EbD5J8?usp=sharing&#34;&gt;Link&lt;/a&gt;. Checkpoints and samples are available in a Google Drive &lt;a href=&#34;https://drive.google.com/drive/folders/1I-0dNLfFOSFwrfqjNa-SXuwaURHE5K4k?usp=sharing&#34;&gt;folder&lt;/a&gt; as well. There is also a &lt;a href=&#34;https://www.youtube.com/watch?v=Ic0TBhfuOrA&#34;&gt;tutorial video&lt;/a&gt; on this, courtesy of &lt;a href=&#34;https://www.youtube.com/channel/UCmGXH-jy0o2CuhqtpxbaQgA&#34;&gt;What Make Art&lt;/a&gt;. Also, thanks to &lt;a href=&#34;https://eyalgruss.com&#34;&gt;Eyal Gruss&lt;/a&gt;, there is a more accessible &lt;a href=&#34;https://j.mp/wav2lip&#34;&gt;Google Colab notebook&lt;/a&gt; with more useful features. A tutorial collab notebook is present at this &lt;a href=&#34;https://colab.research.google.com/drive/1IjFW1cLevs6Ouyu4Yht4mnR4yeuMqO7Y#scrollTo=MH1m608OymLH&#34;&gt;link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;ğŸ”¥&lt;/span&gt; &lt;span&gt;ğŸ”¥&lt;/span&gt; Several new, reliable evaluation benchmarks and metrics &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip/tree/master/evaluation&#34;&gt;[&lt;code&gt;evaluation/&lt;/code&gt; folder of this repo]&lt;/a&gt; released. Instructions to calculate the metrics reported in the paper are also present.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;All results from this open-source code or our &lt;a href=&#34;https://bhaasha.iiit.ac.in/lipsync&#34;&gt;demo website&lt;/a&gt; should only be used for research/academic/personal purposes only. As the models are trained on the &lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html&#34;&gt;LRS2 dataset&lt;/a&gt;, any form of commercial use is strictly prohibhited. For commercial requests please contact us directly!&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Python 3.6&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ffmpeg: &lt;code&gt;sudo apt-get install ffmpeg&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install necessary packages using &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;. Alternatively, instructions for using a docker image is provided &lt;a href=&#34;https://gist.github.com/xenogenesi/e62d3d13dadbc164124c830e9c453668&#34;&gt;here&lt;/a&gt;. Have a look at &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip/issues/131#issuecomment-725478562&#34;&gt;this comment&lt;/a&gt; and comment on &lt;a href=&#34;https://gist.github.com/xenogenesi/e62d3d13dadbc164124c830e9c453668&#34;&gt;the gist&lt;/a&gt; if you encounter any issues.&lt;/li&gt; &#xA; &lt;li&gt;Face detection &lt;a href=&#34;https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth&#34;&gt;pre-trained model&lt;/a&gt; should be downloaded to &lt;code&gt;face_detection/detection/sfd/s3fd.pth&lt;/code&gt;. Alternative &lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/prajwal_k_research_iiit_ac_in/EZsy6qWuivtDnANIG73iHjIBjMSoojcIV0NULXV-yiuiIg?e=qTasa8&#34;&gt;link&lt;/a&gt; if the above does not work.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting the weights&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link to the model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Wav2Lip&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Highly accurate lip-sync&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Wav2Lip + GAN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Slightly inferior lip-sync, but better visual quality&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Expert Discriminator&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Weights of the expert discriminator&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visual Quality Discriminator&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Weights of the visual disc trained in a GAN setup&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQVqH88dTm1HjlK11eNba5gBbn15WMS0B0EZbDBttqrqkg?e=ic0ljo&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Lip-syncing videos using the pre-trained models (Inference)&lt;/h2&gt; &#xA;&lt;p&gt;You can lip-sync any video to any audio:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --checkpoint_path &amp;lt;ckpt&amp;gt; --face &amp;lt;video.mp4&amp;gt; --audio &amp;lt;an-audio-source&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result is saved (by default) in &lt;code&gt;results/result_voice.mp4&lt;/code&gt;. You can specify it as an argument, similar to several other available options. The audio source can be any file supported by &lt;code&gt;FFMPEG&lt;/code&gt; containing audio data: &lt;code&gt;*.wav&lt;/code&gt;, &lt;code&gt;*.mp3&lt;/code&gt; or even a video file, from which the code will automatically extract the audio.&lt;/p&gt; &#xA;&lt;h5&gt;Tips for better results:&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Experiment with the &lt;code&gt;--pads&lt;/code&gt; argument to adjust the detected face bounding box. Often leads to improved results. You might need to increase the bottom padding to include the chin region. E.g. &lt;code&gt;--pads 0 20 0 0&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you see the mouth position dislocated or some weird artifacts such as two mouths, then it can be because of over-smoothing the face detections. Use the &lt;code&gt;--nosmooth&lt;/code&gt; argument and give another try.&lt;/li&gt; &#xA; &lt;li&gt;Experiment with the &lt;code&gt;--resize_factor&lt;/code&gt; argument, to get a lower resolution video. Why? The models are trained on faces which were at a lower resolution. You might get better, visually pleasing results for 720p videos than for 1080p videos (in many cases, the latter works well too).&lt;/li&gt; &#xA; &lt;li&gt;The Wav2Lip model without GAN usually needs more experimenting with the above two to get the most ideal results, and sometimes, can give you a better result as well.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preparing LRS2 for training&lt;/h2&gt; &#xA;&lt;p&gt;Our models are trained on LRS2. See &lt;a href=&#34;https://raw.githubusercontent.com/Rudrabha/Wav2Lip/master/#training-on-datasets-other-than-lrs2&#34;&gt;here&lt;/a&gt; for a few suggestions regarding training on other datasets.&lt;/p&gt; &#xA;&lt;h5&gt;LRS2 dataset folder structure&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;data_root (mvlrs_v1)&#xA;â”œâ”€â”€ main, pretrain (we use only main folder in this work)&#xA;|&#x9;â”œâ”€â”€ list of folders&#xA;|&#x9;â”‚   â”œâ”€â”€ five-digit numbered video IDs ending with (.mp4)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Place the LRS2 filelists (train, val, test) &lt;code&gt;.txt&lt;/code&gt; files in the &lt;code&gt;filelists/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h5&gt;Preprocess the dataset for fast training&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python preprocess.py --data_root data_root/main --preprocessed_root lrs2_preprocessed/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional options like &lt;code&gt;batch_size&lt;/code&gt; and number of GPUs to use in parallel to use can also be set.&lt;/p&gt; &#xA;&lt;h5&gt;Preprocessed LRS2 folder structure&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;preprocessed_root (lrs2_preprocessed)&#xA;â”œâ”€â”€ list of folders&#xA;|&#x9;â”œâ”€â”€ Folders with five-digit numbered video IDs&#xA;|&#x9;â”‚   â”œâ”€â”€ *.jpg&#xA;|&#x9;â”‚   â”œâ”€â”€ audio.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train!&lt;/h2&gt; &#xA;&lt;p&gt;There are two major steps: (i) Train the expert lip-sync discriminator, (ii) Train the Wav2Lip model(s).&lt;/p&gt; &#xA;&lt;h5&gt;Training the expert discriminator&lt;/h5&gt; &#xA;&lt;p&gt;You can download &lt;a href=&#34;https://raw.githubusercontent.com/Rudrabha/Wav2Lip/master/#getting-the-weights&#34;&gt;the pre-trained weights&lt;/a&gt; if you want to skip this step. To train it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python color_syncnet_train.py --data_root lrs2_preprocessed/ --checkpoint_dir &amp;lt;folder_to_save_checkpoints&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Training the Wav2Lip models&lt;/h5&gt; &#xA;&lt;p&gt;You can either train the model without the additional visual quality disriminator (&amp;lt; 1 day of training) or use the discriminator (~2 days). For the former, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python wav2lip_train.py --data_root lrs2_preprocessed/ --checkpoint_dir &amp;lt;folder_to_save_checkpoints&amp;gt; --syncnet_checkpoint_path &amp;lt;path_to_expert_disc_checkpoint&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train with the visual quality discriminator, you should run &lt;code&gt;hq_wav2lip_train.py&lt;/code&gt; instead. The arguments for both the files are similar. In both the cases, you can resume training as well. Look at &lt;code&gt;python wav2lip_train.py --help&lt;/code&gt; for more details. You can also set additional less commonly-used hyper-parameters at the bottom of the &lt;code&gt;hparams.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Training on datasets other than LRS2&lt;/h2&gt; &#xA;&lt;p&gt;Training on other datasets might require modifications to the code. Please read the following before you raise an issue:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You might not get good results by training/fine-tuning on a few minutes of a single speaker. This is a separate research problem, to which we do not have a solution yet. Thus, we would most likely not be able to resolve your issue.&lt;/li&gt; &#xA; &lt;li&gt;You must train the expert discriminator for your own dataset before training Wav2Lip.&lt;/li&gt; &#xA; &lt;li&gt;If it is your own dataset downloaded from the web, in most cases, needs to be sync-corrected.&lt;/li&gt; &#xA; &lt;li&gt;Be mindful of the FPS of the videos of your dataset. Changes to FPS would need significant code changes.&lt;/li&gt; &#xA; &lt;li&gt;The expert discriminator&#39;s eval loss should go down to ~0.25 and the Wav2Lip eval sync loss should go down to ~0.2 to get good results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When raising an issue on this topic, please let us know that you are aware of all these points.&lt;/p&gt; &#xA;&lt;p&gt;We have an HD model trained on a dataset allowing commercial usage. The size of the generated face will be 192 x 288 in our new model.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Please check the &lt;code&gt;evaluation/&lt;/code&gt; folder for the instructions.&lt;/p&gt; &#xA;&lt;h2&gt;License and Citation&lt;/h2&gt; &#xA;&lt;p&gt;Theis repository can only be used for personal/research/non-commercial purposes. However, for commercial requests, please contact us directly at &lt;a href=&#34;mailto:radrabha.m@research.iiit.ac.in&#34;&gt;radrabha.m@research.iiit.ac.in&lt;/a&gt; or &lt;a href=&#34;mailto:prajwal.k@research.iiit.ac.in&#34;&gt;prajwal.k@research.iiit.ac.in&lt;/a&gt;. We have an HD model trained on a dataset allowing commercial usage. The size of the generated face will be 192 x 288 in our new model. Please cite the following paper if you use this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{10.1145/3394171.3413532,&#xA;author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},&#xA;title = {A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild},&#xA;year = {2020},&#xA;isbn = {9781450379885},&#xA;publisher = {Association for Computing Machinery},&#xA;address = {New York, NY, USA},&#xA;url = {https://doi.org/10.1145/3394171.3413532},&#xA;doi = {10.1145/3394171.3413532},&#xA;booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},&#xA;pages = {484â€“492},&#xA;numpages = {9},&#xA;keywords = {lip sync, talking face generation, video generation},&#xA;location = {Seattle, WA, USA},&#xA;series = {MM &#39;20}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Parts of the code structure is inspired by this &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch&#34;&gt;TTS repository&lt;/a&gt;. We thank the author for this wonderful code. The code for Face Detection has been taken from the &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face_alignment&lt;/a&gt; repository. We thank the authors for releasing their code and models. We thank &lt;a href=&#34;https://github.com/zabique&#34;&gt;zabique&lt;/a&gt; for the tutorial collab notebook.&lt;/p&gt;</summary>
  </entry>
</feed>