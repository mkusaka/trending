<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-22T01:44:17Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ok-oldking/ok-wuthering-waves</title>
    <updated>2025-06-22T01:44:17Z</updated>
    <id>tag:github.com,2025-06-22:/ok-oldking/ok-wuthering-waves</id>
    <link href="https://github.com/ok-oldking/ok-wuthering-waves" rel="alternate"></link>
    <summary type="html">&lt;p&gt;鸣潮 后台自动战斗 自动刷声骸 一键日常 Automation for Wuthering Waves&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ok-oldking/ok-wuthering-waves/master/icon.png&#34; width=&#34;200&#34;&gt; &lt;br&gt; ok-ww &lt;/h1&gt; &#xA; &lt;h3&gt;&lt;i&gt;基于图像识别的鸣潮自动化, 使用windows接口模拟用户点击, 无读取游戏内存或侵入修改游戏文件/数据.&lt;/i&gt;&lt;/h3&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/platfrom-Windows-blue?color=blue&#34; alt=&#34;Static Badge&#34;&gt; &lt;a href=&#34;https://github.com/ok-oldking/ok-wuthering-waves/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/ok-oldking/ok-wuthering-waves&#34; alt=&#34;GitHub release (with filter)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ok-oldking/ok-wuthering-waves/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/ok-oldking/ok-wuthering-waves/total&#34; alt=&#34;GitHub all releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/vVyCatEBgA&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/296598043787132928?color=5865f2&amp;amp;label=%20Discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ok-oldking/ok-wuthering-waves/master/README_en.md&#34;&gt;English Readme&lt;/a&gt; | 中文说明&lt;/h3&gt; &#xA;&lt;p&gt;演示和教程 &lt;a href=&#34;https://youtu.be/h6P1KWjdnB4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&amp;amp;logo=YouTube&amp;amp;logoColor=white&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;免责声明&lt;/h1&gt; &#xA;&lt;p&gt;本软件是一个外部工具，旨在自动化鸣潮的游戏玩法。它仅通过现有用户界面与游戏交互，并遵守相关法律法规。该软件包旨在简化用户与游戏的交互，不会破坏游戏平衡或提供不公平优势，也不会修改任何游戏文件或代码。&lt;/p&gt; &#xA;&lt;p&gt;本软件开源、免费，仅供个人学习交流使用，仅限于个人游戏账号，不得用于任何商业或营利性目的。开发者团队拥有本项目的最终解释权。使用本软件产生的所有问题与本项目及开发者团队无关。若您发现商家使用本软件进行代练并收费，这是商家的个人行为，本软件不授权用于代练服务，产生的问题及后果与本软件无关。本软件不授权任何人进行售卖，售卖的软件可能被加入恶意代码，导致游戏账号或电脑资料被盗，与本软件无关。&lt;/p&gt; &#xA;&lt;p&gt;请注意，根据库洛的《鸣潮》公平运营声明:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;严禁利用任何第三方工具破坏游戏体验。&#xA;我们将严厉打击使用外挂、加速器、作弊软件、宏脚本等违规工具的行为，这些行为包括但不限于自动挂机、技能加速、无敌模式、瞬移、修改游戏数据等操作。&#xA;一经查证，我们将视违规情况和次数，采取包括但不限于扣除违规收益、冻结或永久封禁游戏账号等措施。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;使用方法:下载绿色版7z压缩包(250M左右), 解压后双击ok-ww.exe&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ok-oldking/ok-wuthering-waves/releases&#34;&gt;GitHub下载&lt;/a&gt;, 免费网页直链, 不要点击下载Source Code, 点击下载7z压缩包&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mirrorchyan.com/zh/projects?rid=okww&#34;&gt;Mirror酱下载渠道&lt;/a&gt;, 国内网页直链, 下载需要购买CD-KEY, 已有Mirror酱CD-KEY可免费下载&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pan.quark.cn/s/a1052cec4d13&#34;&gt;夸克网盘&lt;/a&gt;, 免费, 但需要注册并下载夸克网盘客户端&lt;/li&gt; &#xA; &lt;li&gt;加入QQ频道后, 讨论组下载 &lt;a href=&#34;https://pd.qq.com/s/djmm6l44y&#34;&gt;https://pd.qq.com/s/djmm6l44y&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;有多强?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;4K分辨率流畅运行,支持所有16:9分辨率,1600x900以上, 1280x720不支持是因为鸣潮bug, 它的1280x720并不是1280x720. 部分功能也可以在21:9等宽屏分辨率运行&lt;/li&gt; &#xA; &lt;li&gt;可后台运行,可窗口化,可全屏,屏幕缩放比例无要求&lt;/li&gt; &#xA; &lt;li&gt;全角色自动识别，无需配置出招表，一键运行&lt;/li&gt; &#xA; &lt;li&gt;后台自动静音游戏&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;出现问题请检查&lt;/h3&gt; &#xA;&lt;p&gt;有问题点这里, 挨个检查再提问:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;解压问题:&lt;/strong&gt; 将压缩包解压到仅包含英文字符的目录中。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;杀毒软件干扰:&lt;/strong&gt; 将下载和解压目录添加到您的杀毒软件/Windows Defender 白名单中。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;显示设置:&lt;/strong&gt; 关闭显卡滤镜和锐化。使用默认游戏亮度并禁用在游戏上显示FPS(如小飞机)。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;自定义按键绑定:&lt;/strong&gt; 如没有使用默认按键，请在APP设置中设置, 不在设置里的按键不支持。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;版本过旧:&lt;/strong&gt; 确保您使用的是最新版本的 OK-GI。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;性能:&lt;/strong&gt; 在游戏中保持稳定的 60 FPS，如果需要，降低分辨率。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;游戏断线&lt;/strong&gt; 如果经常发现断开服务器链接的问题, 可以先打开游戏5分钟再开始玩, 或者断开后不要退出游戏, 重新登陆&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;进一步帮助:&lt;/strong&gt; 如果问题仍然存在，请提交错误报告。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Python 源码运行&lt;/h3&gt; &#xA;&lt;p&gt;仅支持Python 3.12&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#CPU版本, 使用openvino&#xA;pip install -r requirements.txt --upgrade #install python dependencies, 更新代码后可能需要重新运行&#xA;python main.py # run the release version&#xA;python main_debug.py # run the debug version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;命令行参数&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ok-ww.exe -t 1 -e&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;-t 或 --task 代表启动后自动执行第几个任务, 1就是第一个, 一条龙任务&lt;/li&gt; &#xA; &lt;li&gt;-e 或 --exit 加上代表如果执行完任务之后自动退出&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;加入我们&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;由于基于&lt;a href=&#34;https://github.com/ok-oldking/ok-script&#34;&gt;ok-script&lt;/a&gt;开发，项目代码仅有3000行（Python），简单易维护&lt;/li&gt; &#xA; &lt;li&gt;鸣潮水群 970523295 进群答案:老王同学OK&lt;/li&gt; &#xA; &lt;li&gt;群都满了 加QQ频道 &lt;a href=&#34;https://pd.qq.com/s/djmm6l44y&#34;&gt;https://pd.qq.com/s/djmm6l44y&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;有兴趣开发的请加开发者群926858895, 至少要能看文档, 使用源码运行, 问使用问题和提需求都会被踢&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;相关项目&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ok-oldking/ok-genshin-impact&#34;&gt;ok-genshin-impact&lt;/a&gt; 原神自动化,一键日常,后台剧情 ( 可后台,支持全游戏语言,支持全16: 9分辨率)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ok-oldking/ok-gf2&#34;&gt;ok-gf2&lt;/a&gt; 少前2追放自动化,一键日常,竞技场,兵棋推演,尘烟 (支持PC版后台)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;赞助商(Sponsors)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EXE签名: Free code signing provided by &lt;a href=&#34;https://signpath.io/&#34;&gt;SignPath.io&lt;/a&gt;, certificate by &lt;a href=&#34;https://signpath.org/&#34;&gt;SignPath Foundation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;致谢&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lazydog28/mc_auto_boss&#34;&gt;https://github.com/lazydog28/mc_auto_boss&lt;/a&gt; 后台点击代码&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cyclotruc/gitingest</title>
    <updated>2025-06-22T01:44:17Z</updated>
    <id>tag:github.com,2025-06-22:/cyclotruc/gitingest</id>
    <link href="https://github.com/cyclotruc/gitingest" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Replace &#39;hub&#39; with &#39;ingest&#39; in any github url to get a prompt-friendly extract of a codebase&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gitingest&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitingest.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cyclotruc/gitingest/main/docs/frontpage.png&#34; alt=&#34;Image&#34; title=&#34;Gitingest main page&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cyclotruc/gitingest/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/gitingest&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/gitingest.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/cyclotruc/gitingest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/cyclotruc/gitingest?style=social.svg?sanitize=true&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/gitingest&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/gitingest&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.com/invite/zerRaGK9EC&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/https://discord.com/invite/zerRaGK9EC&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Turn any Git repository into a prompt-friendly text ingest for LLMs.&lt;/p&gt; &#xA;&lt;p&gt;You can also replace &lt;code&gt;hub&lt;/code&gt; with &lt;code&gt;ingest&lt;/code&gt; in any GitHub URL to access the corresponding digest.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitingest.com&#34;&gt;gitingest.com&lt;/a&gt; · &lt;a href=&#34;https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood&#34;&gt;Chrome Extension&lt;/a&gt; · &lt;a href=&#34;https://addons.mozilla.org/firefox/addon/gitingest&#34;&gt;Firefox Add-on&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy code context&lt;/strong&gt;: Get a text digest from a Git repository URL or a directory&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart Formatting&lt;/strong&gt;: Optimized output format for LLM prompts&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Statistics about&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;File and directory structure&lt;/li&gt; &#xA;   &lt;li&gt;Size of the extract&lt;/li&gt; &#xA;   &lt;li&gt;Token count&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CLI tool&lt;/strong&gt;: Run it as a shell command&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python package&lt;/strong&gt;: Import it in your code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📚 Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+&lt;/li&gt; &#xA; &lt;li&gt;For private repositories: A GitHub Personal Access Token (PAT). You can generate one at &lt;a href=&#34;https://github.com/settings/personal-access-tokens&#34;&gt;https://github.com/settings/personal-access-tokens&lt;/a&gt; (Profile → Settings → Developer Settings → Personal Access Tokens → Fine-grained Tokens)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;📦 Installation&lt;/h3&gt; &#xA;&lt;p&gt;Gitingest is available on &lt;a href=&#34;https://pypi.org/project/gitingest/&#34;&gt;PyPI&lt;/a&gt;. You can install it using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gitingest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, it might be a good idea to use &lt;code&gt;pipx&lt;/code&gt; to install it. You can install &lt;code&gt;pipx&lt;/code&gt; using your preferred package manager.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install pipx&#xA;apt install pipx&#xA;scoop install pipx&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using pipx for the first time, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx ensurepath&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install gitingest&#xA;pipx install gitingest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🧩 Browser Extension Usage&lt;/h2&gt; &#xA;&lt;!-- markdownlint-disable MD033 --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood&#34; target=&#34;_blank&#34; title=&#34;Get Gitingest Extension from Chrome Web Store&#34;&gt;&lt;img height=&#34;48&#34; src=&#34;https://github.com/user-attachments/assets/20a6e44b-fd46-4e6c-8ea6-aad436035753&#34; alt=&#34;Available in the Chrome Web Store&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://addons.mozilla.org/firefox/addon/gitingest&#34; target=&#34;_blank&#34; title=&#34;Get Gitingest Extension from Firefox Add-ons&#34;&gt;&lt;img height=&#34;48&#34; src=&#34;https://github.com/user-attachments/assets/c0e99e6b-97cf-4af2-9737-099db7d3538b&#34; alt=&#34;Get The Add-on for Firefox&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://microsoftedge.microsoft.com/addons/detail/nfobhllgcekbmpifkjlopfdfdmljmipf&#34; target=&#34;_blank&#34; title=&#34;Get Gitingest Extension from Microsoft Edge Add-ons&#34;&gt;&lt;img height=&#34;48&#34; src=&#34;https://github.com/user-attachments/assets/204157eb-4cae-4c0e-b2cb-db514419fd9e&#34; alt=&#34;Get from the Edge Add-ons&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- markdownlint-enable MD033 --&gt; &#xA;&lt;p&gt;The extension is open source at &lt;a href=&#34;https://github.com/lcandy2/gitingest-extension&#34;&gt;lcandy2/gitingest-extension&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Issues and feature requests are welcome to the repo.&lt;/p&gt; &#xA;&lt;h2&gt;💡 Command line usage&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;gitingest&lt;/code&gt; command line tool allows you to analyze codebases and create a text dump of their contents.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Basic usage (writes to digest.txt by default)&#xA;gitingest /path/to/directory&#xA;&#xA;# From URL&#xA;gitingest https://github.com/cyclotruc/gitingest&#xA;&#xA;# or from specific subdirectory&#xA;gitingest https://github.com/cyclotruc/gitingest/tree/main/src/gitingest/utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For private repositories, use the &lt;code&gt;--token/-t&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get your token from https://github.com/settings/personal-access-tokens&#xA;gitingest https://github.com/username/private-repo --token github_pat_...&#xA;&#xA;# Or set it as an environment variable&#xA;export GITHUB_TOKEN=github_pat_...&#xA;gitingest https://github.com/username/private-repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the digest is written to a text file (&lt;code&gt;digest.txt&lt;/code&gt;) in your current working directory. You can customize the output in two ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--output/-o &amp;lt;filename&amp;gt;&lt;/code&gt; to write to a specific file.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--output/-o -&lt;/code&gt; to output directly to &lt;code&gt;STDOUT&lt;/code&gt; (useful for piping to other tools).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See more options and usage details with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gitingest --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🐍 Python package usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Synchronous usage&#xA;from gitingest import ingest&#xA;&#xA;summary, tree, content = ingest(&#34;path/to/directory&#34;)&#xA;&#xA;# or from URL&#xA;summary, tree, content = ingest(&#34;https://github.com/cyclotruc/gitingest&#34;)&#xA;&#xA;# or from a specific subdirectory&#xA;summary, tree, content = ingest(&#34;https://github.com/cyclotruc/gitingest/tree/main/src/gitingest/utils&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For private repositories, you can pass a token:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Using token parameter&#xA;summary, tree, content = ingest(&#34;https://github.com/username/private-repo&#34;, token=&#34;github_pat_...&#34;)&#xA;&#xA;# Or set it as an environment variable&#xA;import os&#xA;os.environ[&#34;GITHUB_TOKEN&#34;] = &#34;github_pat_...&#34;&#xA;summary, tree, content = ingest(&#34;https://github.com/username/private-repo&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this won&#39;t write a file but can be enabled with the &lt;code&gt;output&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Asynchronous usage&#xA;from gitingest import ingest_async&#xA;import asyncio&#xA;&#xA;result = asyncio.run(ingest_async(&#34;path/to/directory&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Jupyter notebook usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gitingest import ingest_async&#xA;&#xA;# Use await directly in Jupyter&#xA;summary, tree, content = await ingest_async(&#34;path/to/directory&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is because Jupyter notebooks are asynchronous by default.&lt;/p&gt; &#xA;&lt;h2&gt;🐳 Self-host&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the image:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t gitingest .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the container:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d --name gitingest -p 8000:8000 gitingest&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The application will be available at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are hosting it on a domain, you can specify the allowed hostnames via env variable &lt;code&gt;ALLOWED_HOSTS&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Default: &#34;gitingest.com, *.gitingest.com, localhost, 127.0.0.1&#34;.&#xA;ALLOWED_HOSTS=&#34;example.com, localhost, 127.0.0.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🤝 Contributing&lt;/h2&gt; &#xA;&lt;h3&gt;Non-technical ways to contribute&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create an Issue&lt;/strong&gt;: If you find a bug or have an idea for a new feature, please &lt;a href=&#34;https://github.com/cyclotruc/gitingest/issues/new&#34;&gt;create an issue&lt;/a&gt; on GitHub. This will help us track and prioritize your request.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spread the Word&lt;/strong&gt;: If you like Gitingest, please share it with your friends, colleagues, and on social media. This will help us grow the community and make Gitingest even better.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Use Gitingest&lt;/strong&gt;: The best feedback comes from real-world usage! If you encounter any issues or have ideas for improvement, please let us know by &lt;a href=&#34;https://github.com/cyclotruc/gitingest/issues/new&#34;&gt;creating an issue&lt;/a&gt; on GitHub or by reaching out to us on &lt;a href=&#34;https://discord.com/invite/zerRaGK9EC&#34;&gt;Discord&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Technical ways to contribute&lt;/h3&gt; &#xA;&lt;p&gt;Gitingest aims to be friendly for first time contributors, with a simple Python and HTML codebase. If you need any help while working with the code, reach out to us on &lt;a href=&#34;https://discord.com/invite/zerRaGK9EC&#34;&gt;Discord&lt;/a&gt;. For detailed instructions on how to make a pull request, see &lt;a href=&#34;https://raw.githubusercontent.com/cyclotruc/gitingest/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🛠️ Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tailwindcss.com&#34;&gt;Tailwind CSS&lt;/a&gt; - Frontend&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fastapi/fastapi&#34;&gt;FastAPI&lt;/a&gt; - Backend framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jinja.palletsprojects.com&#34;&gt;Jinja2&lt;/a&gt; - HTML templating&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;tiktoken&lt;/a&gt; - Token estimation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PostHog/posthog&#34;&gt;posthog&lt;/a&gt; - Amazing analytics&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Looking for a JavaScript/FileSystemNode package?&lt;/h3&gt; &#xA;&lt;p&gt;Check out the NPM alternative 📦 Repomix: &lt;a href=&#34;https://github.com/yamadashy/repomix&#34;&gt;https://github.com/yamadashy/repomix&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Project Growth&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#cyclotruc/gitingest&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=cyclotruc/gitingest&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/vggt</title>
    <updated>2025-06-22T01:44:17Z</updated>
    <id>tag:github.com,2025-06-22:/facebookresearch/vggt</id>
    <link href="https://github.com/facebookresearch/vggt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2025 Best Paper Award] VGGT: Visual Geometry Grounded Transformer&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;VGGT: Visual Geometry Grounded Transformer&lt;/h1&gt; &#xA; &lt;a href=&#34;https://jytime.github.io/data/VGGT_CVPR25.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Paper-VGGT&#34; alt=&#34;Paper PDF&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2503.11651&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2503.11651-b31b1b&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://vgg-t.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-green&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/spaces/facebook/vggt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/&#34;&gt;Visual Geometry Group, University of Oxford&lt;/a&gt;&lt;/strong&gt;; &lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://jytime.github.io/&#34;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&#34;https://silent-chen.github.io/&#34;&gt;Minghao Chen&lt;/a&gt;, &lt;a href=&#34;https://nikitakaraevv.github.io/&#34;&gt;Nikita Karaev&lt;/a&gt;, &lt;a href=&#34;https://www.robots.ox.ac.uk/~vedaldi/&#34;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&#34;https://chrirupp.github.io/&#34;&gt;Christian Rupprecht&lt;/a&gt;, &lt;a href=&#34;https://d-novotny.github.io/&#34;&gt;David Novotny&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{wang2025vggt,&#xA;  title={VGGT: Visual Geometry Grounded Transformer},&#xA;  author={Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David},&#xA;  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[June 13, 2025] Honored to receive the Best Paper Award at CVPR 2025! Apologies if I’m slow to respond to queries or GitHub issues these days. If you’re interested, our oral presentation is available &lt;a href=&#34;https://docs.google.com/presentation/d/1JVuPnuZx6RgAy-U5Ezobg73XpBi7FrOh/edit?usp=sharing&amp;amp;ouid=107115712143490405606&amp;amp;rtpof=true&amp;amp;sd=true&#34;&gt;here&lt;/a&gt;. (Note: it’s shared in .pptx format with animations — quite large, but feel free to use it as a template if helpful.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[June 2, 2025] Added a script to run VGGT and save predictions in COLMAP format, with bundle adjustment support optional. The saved COLMAP files can be directly used with &lt;a href=&#34;https://github.com/nerfstudio-project/gsplat&#34;&gt;gsplat&lt;/a&gt; or other NeRF/Gaussian splatting libraries.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[May 3, 2025] Evaluation code for reproducing our camera pose estimation results on Co3D is now available in the &lt;a href=&#34;https://github.com/facebookresearch/vggt/tree/evaluation&#34;&gt;evaluation&lt;/a&gt; branch.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[Apr 13, 2025] Training code is being gradually cleaned and uploaded to the &lt;a href=&#34;https://github.com/facebookresearch/vggt/tree/training&#34;&gt;training&lt;/a&gt; branch. It will be merged into the main branch once finalized.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Visual Geometry Grounded Transformer (VGGT, CVPR 2025) is a feed-forward neural network that directly infers all key 3D attributes of a scene, including extrinsic and intrinsic camera parameters, point maps, depth maps, and 3D point tracks, &lt;strong&gt;from one, a few, or hundreds of its views, within seconds&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;First, clone this repository to your local machine, and install the dependencies (torch, torchvision, numpy, Pillow, and huggingface_hub).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:facebookresearch/vggt.git &#xA;cd vggt&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can install VGGT as a package (&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/vggt/main/docs/package.md&#34;&gt;click here&lt;/a&gt; for details).&lt;/p&gt; &#xA;&lt;p&gt;Now, try the model with just a few lines of code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vggt.models.vggt import VGGT&#xA;from vggt.utils.load_fn import load_and_preprocess_images&#xA;&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) &#xA;dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] &amp;gt;= 8 else torch.float16&#xA;&#xA;# Initialize the model and load the pretrained weights.&#xA;# This will automatically download the model weights the first time it&#39;s run, which may take a while.&#xA;model = VGGT.from_pretrained(&#34;facebook/VGGT-1B&#34;).to(device)&#xA;&#xA;# Load and preprocess example images (replace with your own image paths)&#xA;image_names = [&#34;path/to/imageA.png&#34;, &#34;path/to/imageB.png&#34;, &#34;path/to/imageC.png&#34;]  &#xA;images = load_and_preprocess_images(image_names).to(device)&#xA;&#xA;with torch.no_grad():&#xA;    with torch.cuda.amp.autocast(dtype=dtype):&#xA;        # Predict attributes including cameras, depth maps, and point maps.&#xA;        predictions = model(images)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model weights will be automatically downloaded from Hugging Face. If you encounter issues such as slow loading, you can manually download them &lt;a href=&#34;https://huggingface.co/facebook/VGGT-1B/blob/main/model.pt&#34;&gt;here&lt;/a&gt; and load, or:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = VGGT()&#xA;_URL = &#34;https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt&#34;&#xA;model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Detailed Usage&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to expand&lt;/summary&gt; &#xA; &lt;p&gt;You can also optionally choose which attributes (branches) to predict, as shown below. This achieves the same result as the example above. This example uses a batch size of 1 (processing a single scene), but it naturally works for multiple scenes.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from vggt.utils.pose_enc import pose_encoding_to_extri_intri&#xA;from vggt.utils.geometry import unproject_depth_map_to_point_map&#xA;&#xA;with torch.no_grad():&#xA;    with torch.cuda.amp.autocast(dtype=dtype):&#xA;        images = images[None]  # add batch dimension&#xA;        aggregated_tokens_list, ps_idx = model.aggregator(images)&#xA;                &#xA;    # Predict Cameras&#xA;    pose_enc = model.camera_head(aggregated_tokens_list)[-1]&#xA;    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)&#xA;    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])&#xA;&#xA;    # Predict Depth Maps&#xA;    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)&#xA;&#xA;    # Predict Point Maps&#xA;    point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)&#xA;        &#xA;    # Construct 3D Points from Depth Maps and Cameras&#xA;    # which usually leads to more accurate 3D points than point map branch&#xA;    point_map_by_unprojection = unproject_depth_map_to_point_map(depth_map.squeeze(0), &#xA;                                                                extrinsic.squeeze(0), &#xA;                                                                intrinsic.squeeze(0))&#xA;&#xA;    # Predict Tracks&#xA;    # choose your own points to track, with shape (N, 2) for one scene&#xA;    query_points = torch.FloatTensor([[100.0, 200.0], &#xA;                                        [60.72, 259.94]]).to(device)&#xA;    track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Furthermore, if certain pixels in the input frames are unwanted (e.g., reflective surfaces, sky, or water), you can simply mask them by setting the corresponding pixel values to 0 or 1. Precise segmentation masks aren&#39;t necessary - simple bounding box masks work effectively (check this &lt;a href=&#34;https://github.com/facebookresearch/vggt/issues/47&#34;&gt;issue&lt;/a&gt; for an example).&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Interactive Demo&lt;/h2&gt; &#xA;&lt;p&gt;We provide multiple ways to visualize your 3D reconstructions. Before using these visualization tools, install the required dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements_demo.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Interactive 3D Visualization&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; VGGT typically reconstructs a scene in less than 1 second. However, visualizing 3D points may take tens of seconds due to third-party rendering, independent of VGGT&#39;s processing time. The visualization is slow especially when the number of images is large.&lt;/p&gt; &#xA;&lt;h4&gt;Gradio Web Interface&lt;/h4&gt; &#xA;&lt;p&gt;Our Gradio-based interface allows you to upload images/videos, run reconstruction, and interactively explore the 3D scene in your browser. You can launch this in your local machine or try it on &lt;a href=&#34;https://huggingface.co/spaces/facebook/vggt&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo_gradio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to preview the Gradio interactive interface&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://jytime.github.io/data/vggt_hf_demo_screen.png&#34; alt=&#34;Gradio Web Interface Preview&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Viser 3D Viewer&lt;/h4&gt; &#xA;&lt;p&gt;Run the following command to run reconstruction and visualize the point clouds in viser. Note this script requires a path to a folder containing images. It assumes only image files under the folder. You can set &lt;code&gt;--use_point_map&lt;/code&gt; to use the point cloud from the point map branch, instead of the depth-based point cloud.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo_viser.py --image_folder path/to/your/images/folder&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Exporting to COLMAP Format&lt;/h2&gt; &#xA;&lt;p&gt;We also support exporting VGGT&#39;s predictions directly to COLMAP format, by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Feedforward prediction only&#xA;python demo_colmap.py --scene_dir=/YOUR/SCENE_DIR/ &#xA;&#xA;# With bundle adjustment&#xA;python demo_colmap.py --scene_dir=/YOUR/SCENE_DIR/ --use_ba&#xA;&#xA;# Run with bundle adjustment using reduced parameters for faster processing&#xA;# Reduces max_query_pts from 4096 (default) to 2048 and query_frame_num from 8 (default) to 5&#xA;# Trade-off: Faster execution but potentially less robust reconstruction in complex scenes (you may consider setting query_frame_num equal to your total number of images) &#xA;# See demo_colmap.py for additional bundle adjustment configuration options&#xA;python demo_colmap.py --scene_dir=/YOUR/SCENE_DIR/ --use_ba --max_query_pts=2048 --query_frame_num=5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please ensure that the images are stored in &lt;code&gt;/YOUR/SCENE_DIR/images/&lt;/code&gt;. This folder should contain only the images. Check the examples folder for the desired data structure.&lt;/p&gt; &#xA;&lt;p&gt;The reconstruction result (camera parameters and 3D points) will be automatically saved under &lt;code&gt;/YOUR/SCENE_DIR/sparse/&lt;/code&gt; in the COLMAP format, such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;SCENE_DIR/&#xA;├── images/&#xA;└── sparse/&#xA;    ├── cameras.bin&#xA;    ├── images.bin&#xA;    └── points3D.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Integration with Gaussian Splatting&lt;/h2&gt; &#xA;&lt;p&gt;The exported COLMAP files can be directly used with &lt;a href=&#34;https://github.com/nerfstudio-project/gsplat&#34;&gt;gsplat&lt;/a&gt; for Gaussian Splatting training. Install &lt;code&gt;gsplat&lt;/code&gt; following their official instructions (we recommend &lt;code&gt;gsplat==1.3.0&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;p&gt;An example command to train the model is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd gsplat&#xA;python examples/simple_trainer.py  default --data_factor 1 --data_dir /YOUR/SCENE_DIR/ --result_dir /YOUR/RESULT_DIR/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Zero-shot Single-view Reconstruction&lt;/h2&gt; &#xA;&lt;p&gt;Our model shows surprisingly good performance on single-view reconstruction, although it was never trained for this task. The model does not need to duplicate the single-view image to a pair, instead, it can directly infer the 3D structure from the tokens of the single view image. Feel free to try it with our demos above, which naturally works for single-view reconstruction.&lt;/p&gt; &#xA;&lt;p&gt;We did not quantitatively test monocular depth estimation performance ourselves, but &lt;a href=&#34;https://github.com/kabouzeid&#34;&gt;@kabouzeid&lt;/a&gt; generously provided a comparison of VGGT to recent methods &lt;a href=&#34;https://github.com/facebookresearch/vggt/issues/36&#34;&gt;here&lt;/a&gt;. VGGT shows competitive or better results compared to state-of-the-art monocular approaches such as DepthAnything v2 or MoGe, despite never being explicitly trained for single-view tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Runtime and GPU Memory&lt;/h2&gt; &#xA;&lt;p&gt;We benchmark the runtime and GPU memory usage of VGGT&#39;s aggregator on a single NVIDIA H100 GPU across various input sizes.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Input Frames&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;4&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;8&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;10&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;20&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;50&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;100&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;200&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Time (s)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Memory (GB)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.88&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.63&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note that these results were obtained using Flash Attention 3, which is faster than the default Flash Attention 2 implementation while maintaining almost the same memory usage. Feel free to compile Flash Attention 3 from source to get better performance.&lt;/p&gt; &#xA;&lt;h2&gt;Research Progression&lt;/h2&gt; &#xA;&lt;p&gt;Our work builds upon a series of previous research projects. If you&#39;re interested in understanding how our research evolved, check out our previous works:&lt;/p&gt; &#xA;&lt;table border=&#34;0&#34; cellspacing=&#34;0&#34; cellpadding=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;a href=&#34;https://github.com/jytime/Deep-SfM-Revisited&#34;&gt;Deep SfM Revisited&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;white-space: pre;&#34;&gt;──┐&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;a href=&#34;https://github.com/facebookresearch/PoseDiffusion&#34;&gt;PoseDiffusion&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;white-space: pre;&#34;&gt;─────►&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/facebookresearch/vggsfm&#34;&gt;VGGSfM&lt;/a&gt; ──► &lt;a href=&#34;https://github.com/facebookresearch/vggt&#34;&gt;VGGT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;a href=&#34;https://github.com/facebookresearch/co-tracker&#34;&gt;CoTracker&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;white-space: pre;&#34;&gt;──┘&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to these great repositories: &lt;a href=&#34;https://github.com/facebookresearch/PoseDiffusion&#34;&gt;PoseDiffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/vggsfm&#34;&gt;VGGSfM&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/co-tracker&#34;&gt;CoTracker&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;DINOv2&lt;/a&gt;, &lt;a href=&#34;https://github.com/naver/dust3r&#34;&gt;Dust3r&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/moge&#34;&gt;Moge&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/pytorch3d&#34;&gt;PyTorch3D&lt;/a&gt;, &lt;a href=&#34;https://github.com/xiongzhu666/Sky-Segmentation-and-Post-processing&#34;&gt;Sky Segmentation&lt;/a&gt;, &lt;a href=&#34;https://github.com/DepthAnything/Depth-Anything-V2&#34;&gt;Depth Anything V2&lt;/a&gt;, &lt;a href=&#34;https://github.com/YvanYin/Metric3D&#34;&gt;Metric3D&lt;/a&gt; and many other inspiring works in the community.&lt;/p&gt; &#xA;&lt;h2&gt;Checklist&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release the training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release VGGT-500M and VGGT-200M&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/vggt/main/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt; file for details about the license under which this code is made available.&lt;/p&gt;</summary>
  </entry>
</feed>