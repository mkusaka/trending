<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-06T01:59:15Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>biobootloader/mentat</title>
    <updated>2023-08-06T01:59:15Z</updated>
    <id>tag:github.com,2023-08-06:/biobootloader/mentat</id>
    <link href="https://github.com/biobootloader/mentat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mentat - The AI Coding Assistant&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/bio_bootloader&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/bio_bootloader?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/zbvd9qx9Pb&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/XbPdxAMJte?style=flat&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/mentat-ai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mentat-ai?color=blue&#34; alt=&#34;Stable Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/biobootloader/mentat/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/mentat-ai.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üßô‚Äç‚ôÇÔ∏è Mentat ‚ö°&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;It is by will alone I set my mind in motion&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;The Mentat Mantra&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The Mentats of Dune combine human creativity with computer-like processing - and now you can too.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Mentat is the AI tool that assists you with any coding task, right from your command line.&lt;/p&gt; &#xA;&lt;p&gt;Unlike Copilot, Mentat coordinates edits across multiple locations and files. And unlike ChatGPT, Mentat already has the context of your project - no copy and pasting required!&lt;/p&gt; &#xA;&lt;p&gt;Want help understanding a new codebase? Need to add a new feature? Refactor existing code? Mentat can do it!&lt;/p&gt; &#xA;&lt;h1&gt;üçø Example Videos (üîä on!)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/biobootloader/mentat/assets/128252497/35b027a9-d639-452c-a53c-ef019a645719&#34;&gt;https://github.com/biobootloader/mentat/assets/128252497/35b027a9-d639-452c-a53c-ef019a645719&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See more videos on &lt;a href=&#34;https://twitter.com/bio_bootloader/status/1683906735248125955&#34;&gt;Twitter&lt;/a&gt; or YouTube:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=lODjaWclwpY&#34;&gt;Intro (2 min - same video as above)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qSyTWMFOjPs&#34;&gt;Explaining and editing Llama2.c (3 min)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YJLDIqq8k2A&#34;&gt;More Mentat features (4 min)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;‚öôÔ∏è Setup&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=bVJP8hY8uRM&#34;&gt;Installation and Setup Demonstration Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Before installing, it&#39;s suggested that you create a virtual environment to install it in:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Python 3.10 or higher is required&#xA;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you&#39;ll have to have activated the virtual environment to run mentat if you install it there.&lt;/p&gt; &#xA;&lt;p&gt;There are then 3 install methods. The first two will just let you run it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyPI: &lt;code&gt;python -m pip install mentat-ai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Github: &lt;code&gt;python -m pip install git+https://github.com/biobootloader/mentat.git&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The third option is useful if you&#39;d also like to modify Mentat&#39;s code, as well as run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/biobootloader/mentat.git&#xA;cd mentat&#xA;&#xA;# install with pip in editable mode:&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Add your OpenAI API Key&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;ll need to have API access to GPT-4 to run Mentat. There are a few options to provide Mentat with your OpenAI API key:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file with the line &lt;code&gt;OPENAI_API_KEY=&amp;lt;your-api-key&amp;gt;&lt;/code&gt; in the directory you plan to run mentat in or in &lt;code&gt;~/.mentat/.env&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;export OPENAI_API_KEY=&amp;lt;your key here&amp;gt;&lt;/code&gt; prior to running Mentat&lt;/li&gt; &#xA; &lt;li&gt;Place the previous command in your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.zshrc&lt;/code&gt; to export your key on every terminal startup&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;For custom configuration options see &lt;a href=&#34;https://raw.githubusercontent.com/biobootloader/mentat/main/docs/configuration.md&#34;&gt;configuration.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üöÄ Usage&lt;/h1&gt; &#xA;&lt;p&gt;Run Mentat from within your project directory. Mentat uses git, so if your project doesn&#39;t already have git set up, run &lt;code&gt;git init&lt;/code&gt;. Then you can run Mentat with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;mentat &amp;lt;paths to files or directories&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;List the files you would like Mentat to read and edit as arguments. Mentat will add each of them to context, so be careful not to exceed the GPT-4 token context limit. To add multiple files at once, you can also provide directories as arguments. When a directory is provided, Mentat will add all the contained files, except for ones ignored in your &lt;code&gt;.gitignore&lt;/code&gt;. In addition to files and directories, you can use &lt;a href=&#34;https://docs.python.org/3/library/glob.html&#34;&gt;glob patterns&lt;/a&gt; to add multiple files at once.&lt;/p&gt; &#xA;&lt;h2&gt;Options&lt;/h2&gt; &#xA;&lt;h3&gt;Exclude Files&lt;/h3&gt; &#xA;&lt;p&gt;Exclude given paths, directories, or &lt;a href=&#34;https://docs.python.org/3/library/glob.html&#34;&gt;glob patterns&lt;/a&gt; from Mentat&#39;s context. Takes precedence over included file paths.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mentat path/to/directory --exclude exclude_me.py dir1/dir2 **/*.ts&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ShishirPatil/gorilla</title>
    <updated>2023-08-06T01:59:15Z</updated>
    <id>tag:github.com,2023-08-06:/ShishirPatil/gorilla</id>
    <link href="https://github.com/ShishirPatil/gorilla" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Gorilla: An API store for LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gorilla: Large Language Model Connected with Massive APIs [&lt;a href=&#34;https://shishirpatil.github.io/gorilla/&#34;&gt;Project Website&lt;/a&gt;]&lt;/h1&gt; &#xA;&lt;img src=&#34;https://github.com/ShishirPatil/gorilla/raw/gh-pages/assets/img/logo.png&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;üü¢ Gorilla is Apache 2.0&lt;/strong&gt; With Gorilla being fine-tuned on MPT, and Falcon, you can use Gorilla commercially with no obligations! &lt;span&gt;‚õ≥&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;üöÄ&lt;/span&gt; Try Gorilla in 60s&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üíª&lt;/span&gt; Use &lt;a href=&#34;https://github.com/gorilla-llm/gorilla-cli&#34;&gt;Gorilla in your CLI&lt;/a&gt; with &lt;code&gt;pip install gorilla-cli&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;üóû&lt;/span&gt; Checkout our paper!&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.15334&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2305.15334-%3CCOLOR%3E.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;üëã&lt;/span&gt; Join our Discord!&lt;/strong&gt; &lt;a href=&#34;https://discord.gg/SwTyuTAxX3&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1111172801899012102?label=Discord&amp;amp;logo=discord&amp;amp;logoColor=green&amp;amp;style=flat-square&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Gorilla&lt;/code&gt; enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üíª [06/29] Released &lt;a href=&#34;https://github.com/gorilla-llm/gorilla-cli&#34;&gt;gorilla-cli&lt;/a&gt;, LLMs for your CLI!&lt;/li&gt; &#xA; &lt;li&gt;üü¢ [06/06] Released Commercially usable, Apache 2.0 licensed Gorilla models&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üöÄ&lt;/span&gt; [05/30] Provided the &lt;a href=&#34;https://raw.githubusercontent.com/ShishirPatil/gorilla/main/inference/README.md&#34;&gt;CLI interface&lt;/a&gt; to chat with Gorilla!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üöÄ&lt;/span&gt; [05/28] Released Torch Hub and TensorFlow Hub Models!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üöÄ&lt;/span&gt; [05/27] Released the first Gorilla model! &lt;a href=&#34;https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/gorilla-llm/gorilla-7b-hf-delta-v0&#34;&gt;&lt;span&gt;ü§ó&lt;/span&gt;&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üî•&lt;/span&gt; [05/27] We released the APIZoo contribution guide for community API contributions!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üî•&lt;/span&gt; [05/25] We release the APIBench dataset and the evaluation code of Gorilla!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Inference: Run Gorilla locally &lt;a href=&#34;https://raw.githubusercontent.com/ShishirPatil/gorilla/main/inference/README.md&#34;&gt;&lt;code&gt;inference/README.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Evaluation: We have included prompts and responces for the APIBench with and without retrievers along with the Abstract Syntax Tree (AST) matching evaluation script at &lt;a href=&#34;https://github.com/ShishirPatil/gorilla/tree/main/eval&#34;&gt;evaluation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Repository Organization&lt;/h2&gt; &#xA;&lt;p&gt;Our repository organization is shown below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;data&lt;/code&gt; folder contains all the evaluation APIs &lt;code&gt;(APIBench)&lt;/code&gt; and the community contributed APIs.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;eval&lt;/code&gt; folder contains all our evaluation code as well as the Gorilla outputs.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;inference&lt;/code&gt; folder contains all the inference code for running Gorilla locally.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span style=&#34;color:hr&#34;&gt;[Coming Soon!]&lt;/span&gt; The &lt;code&gt;train&lt;/code&gt; folder contains all the training code associated with Gorilla finetuning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For our dataset collections, all the 1640 API documentation is in &lt;code&gt;data/api&lt;/code&gt;. We also include the &lt;code&gt;APIBench&lt;/code&gt; dataset created by self-instruct in &lt;code&gt;data/apibench&lt;/code&gt;. For evaluation, we convert this into a LLM-friendly chat format, and the questions are in &lt;code&gt;eval/eval-data/questions&lt;/code&gt;, and the corresponding responces are in &lt;code&gt;eval/eval-data/responses&lt;/code&gt;. We have also included the evaluation scripts are in &lt;code&gt;eval/eval-scripts&lt;/code&gt;. This would be entirely sufficient to train Gorilla yourself, and reproduce our results. Please see &lt;a href=&#34;https://github.com/ShishirPatil/gorilla/tree/main/eval&#34;&gt;evaluation&lt;/a&gt; for the details on how to use our evaluation pipeline.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, we have released all the model weights. &lt;code&gt;gorilla-7b-hf-v0&lt;/code&gt; lets you invoke over 925 Hugging Face APIs. Similarly, &lt;code&gt;gorilla-7b-tf-v0&lt;/code&gt; and &lt;code&gt;gorilla-7b-th-v0&lt;/code&gt; have 626 (exhaustive) Tensorflow v2, and 94 (exhaustive) Torch Hub APIs. &lt;code&gt;gorilla-mpt-7b-hf-v0&lt;/code&gt; and &lt;code&gt;gorilla-falcon-7b-hf-v0&lt;/code&gt; are Apache 2.0 licensed models (commercially usable) fine-tuned on MPT-7B and Falcon-7B respectively. We will release a model with all three combined with generic chat capability and community contributed APIs as soon as we can scale our serving infrastructure. You can run Gorilla locally from instructions in the &lt;code&gt;inference/&lt;/code&gt; sub-directory, or we also provide a hosted Gorilla chat completion API (see Colab)! If you have any suggestions, or if you run into any issues please feel free to reach out to us either through Discord or email or raise a Github issue.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gorilla&#xA;‚îú‚îÄ‚îÄ data&#xA;‚îÇ   ‚îú‚îÄ‚îÄ api (TF/HF/TH APIs used in generating apibench)&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ {api_name}_api.jsonl&#xA;‚îÇ   ‚îú‚îÄ‚îÄ apibench (Evaluating LLM models) v-1.0&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ {api_name}_train.jsonl, {api_name}_eval.jsonl&#xA;|   |‚îÄ‚îÄ apizoo (Contributed by the community - evolving)&#xA;‚îÇ   |   ‚îú‚îÄ‚îÄ username1.json&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ username2.json&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ eval&#xA;‚îÇ   ‚îú‚îÄ‚îÄ README.md&#xA;‚îÇ   ‚îú‚îÄ‚îÄ get_llm_responses.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ eval-scripts&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ast_eval_{api_name}.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ eval-data&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ questions&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ API name&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ questions_{api_name}_{eval_metric}.jsonl&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ responses&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ API name&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ responses_{api_name}_Gorilla_FT_{eval_metric}.jsonl&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ responses_{api_name}_Gorilla_RT_{eval_metric}.jsonl&#xA;‚îú‚îÄ‚îÄ inference&#xA;‚îÇ   ‚îú‚îÄ‚îÄ README.md&#xA;‚îÇ   ‚îú‚îÄ‚îÄ serve&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gorilla_cli.py&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conv_template.py&#xA;‚îú‚îÄ‚îÄ train (Coming Soon!)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing Your API&lt;/h2&gt; &#xA;&lt;p&gt;We aim to build an open-source, one-stop-shop for all APIs, LLMs can interact with! Any suggestions and contributions are welcome! Please see the details on &lt;a href=&#34;https://github.com/ShishirPatil/gorilla/tree/main/data/README.md&#34;&gt;how to contribute&lt;/a&gt;. THIS WILL ALWAYS REMAIN OPEN SOURCE.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ(s)&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;I would like to use Gorilla commercially. Is there going to be a Apache 2.0 licensed version?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Yes! We now have models that you can use commercially without any obligations.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Can we use Gorilla with Langchain, Toolformer, AutoGPT etc?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Absolutely! You&#39;ve highlighted a great aspect of our tools. Gorilla is an end-to-end model, specifically tailored to serve correct API calls without requiring any additional coding. It&#39;s designed to work as part of a wider ecosystem and can be flexibly integrated with other tools.&lt;/p&gt; &#xA;&lt;p&gt;Langchain, is a versatile developer tool. Its &#34;agents&#34; can efficiently swap in any LLM, Gorilla included, making it a highly adaptable solution for various needs.&lt;/p&gt; &#xA;&lt;p&gt;AutoGPT, on the other hand, concentrates on the art of prompting GPT series models. It&#39;s worth noting that Gorilla, as a fully fine-tuned model, consistently shows remarkable accuracy, and lowers hallucination, outperforming GPT-4 in making specific API calls.&lt;/p&gt; &#xA;&lt;p&gt;Now, when it comes to ToolFormer, Toolformer zeroes in on a select set of tools, providing specialized functionalities. Gorilla, in contrast, has the capacity to manage thousands of API calls, offering a broader coverage over a more extensive range of tools.&lt;/p&gt; &#xA;&lt;p&gt;The beauty of these tools truly shines when they collaborate, complementing each other&#39;s strengths and capabilities to create an even more powerful and comprehensive solution. This is where your contribution can make a difference. We enthusiastically welcome any inputs to further refine and enhance these tools.&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;How to train your own Gorilla models?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We will release the training code as soon as we can get GPUs to test and finalize the pipeline. Given the demand for our hosted end-points, we have dedicated all of our GPUs to serve the models. If you would like to help with resources get in touch!&lt;/p&gt; &#xA;&lt;h2&gt;Project Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;In the immediate future, we plan to release the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Dataset and Eval Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Opening up the APIZoo for contributions from community&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Hosted Gorilla LLM chat for HF model APIs [May 27, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release weights for HF model APIs [May 27, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Run Gorilla LLM locally [May 28, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release weights for all APIs from APIBench [May 28, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release a commercially usable, Apache 2.0 licensed Gorilla model [Jun 5, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train a model with first batch of community contributed APIs from APIZoo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train SOTA Gorilla LLM with expanded APIBench and APIZoo &lt;span&gt;üöÄ&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Propose a new task you would like to work on &lt;span&gt;ü§©&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Gorilla or APIBench, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@article{patil2023gorilla,&#xA;  title={Gorilla: Large Language Model Connected with Massive APIs},&#xA;  author={Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},&#xA;  year={2023},&#xA;  journal={arXiv preprint arXiv:2305.15334},&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/audiocraft</title>
    <updated>2023-08-06T01:59:15Z</updated>
    <id>tag:github.com,2023-08-06:/facebookresearch/audiocraft</id>
    <link href="https://github.com/facebookresearch/audiocraft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AudioCraft&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_docs/badge.svg?sanitize=true&#34; alt=&#34;docs badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_linter/badge.svg?sanitize=true&#34; alt=&#34;linter badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_tests/badge.svg?sanitize=true&#34; alt=&#34;tests badge&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;AudioCraft is a PyTorch library for deep learning research on audio generation. AudioCraft contains inference and training code for two state-of-the-art AI generative models producing high-quality audio: AudioGen and MusicGen.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;AudioCraft requires Python 3.9, PyTorch 2.0.0. To install AudioCraft, you can run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Best to make sure you have torch installed first, in particular before installing xformers.&#xA;# Don&#39;t run this if you already have PyTorch installed.&#xA;pip install &#39;torch&amp;gt;=2.0&#39;&#xA;# Then proceed to one of the following&#xA;pip install -U audiocraft  # stable release&#xA;pip install -U git+https://git@github.com/facebookresearch/audiocraft#egg=audiocraft  # bleeding edge&#xA;pip install -e .  # or if you cloned the repo locally (mandatory if you want to train).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also recommend having &lt;code&gt;ffmpeg&lt;/code&gt; installed, either through your system or Anaconda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install ffmpeg&#xA;# Or if you are using Anaconda or Miniconda&#xA;conda install &#39;ffmpeg&amp;lt;5&#39; -c  conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;At the moment, AudioCraft contains the training code and inference code for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN.md&#34;&gt;MusicGen&lt;/a&gt;: A state-of-the-art controllable text-to-music model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/AUDIOGEN.md&#34;&gt;AudioGen&lt;/a&gt;: A state-of-the-art text-to-sound model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/ENCODEC.md&#34;&gt;EnCodec&lt;/a&gt;: A state-of-the-art high fidelity neural audio codec.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MBD.md&#34;&gt;Multi Band Diffusion&lt;/a&gt;: An EnCodec compatible decoder using diffusion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training code&lt;/h2&gt; &#xA;&lt;p&gt;AudioCraft contains PyTorch components for deep learning research in audio and training pipelines for the developed models. For a general introduction of AudioCraft design principles and instructions to develop your own training pipeline, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/TRAINING.md&#34;&gt;AudioCraft training documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For reproducing existing work and using the developed training pipelines, refer to the instructions for each specific model that provides pointers to configuration, example grids and model/task-specific information and FAQ.&lt;/p&gt; &#xA;&lt;h2&gt;API documentation&lt;/h2&gt; &#xA;&lt;p&gt;We provide some &lt;a href=&#34;https://facebookresearch.github.io/audiocraft/api_docs/audiocraft/index.html&#34;&gt;API documentation&lt;/a&gt; for AudioCraft.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h4&gt;Is the training code available?&lt;/h4&gt; &#xA;&lt;p&gt;Yes! We provide the training code for &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/ENCODEC.md&#34;&gt;EnCodec&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN.md&#34;&gt;MusicGen&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MBD.md&#34;&gt;Multi Band Diffusion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Where are the models stored?&lt;/h4&gt; &#xA;&lt;p&gt;Hugging Face stored the model in a specific location, which can be overriden by setting the &lt;code&gt;AUDIOCRAFT_CACHE_DIR&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code in this repository is released under the MIT license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/LICENSE&#34;&gt;LICENSE file&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/LICENSE_weights&#34;&gt;LICENSE_weights file&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;For the general framework of AudioCraft, please cite the following.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{copet2023simple,&#xA;    title={Simple and Controllable Music Generation},&#xA;    author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre D√©fossez},&#xA;    year={2023},&#xA;    journal={arXiv preprint arXiv:2306.05284},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When referring to a specific model, please cite as mentioned in the model specific README, e.g &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/MUSICGEN.md&#34;&gt;./docs/MUSICGEN.md&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/docs/AUDIOGEN.md&#34;&gt;./docs/AUDIOGEN.md&lt;/a&gt;, etc.&lt;/p&gt;</summary>
  </entry>
</feed>