<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-11T01:45:35Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/parler-tts</title>
    <updated>2024-08-11T01:45:35Z</updated>
    <id>tag:github.com,2024-08-11:/huggingface/parler-tts</id>
    <link href="https://github.com/huggingface/parler-tts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference and training library for high-quality TTS models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Parler-TTS&lt;/h1&gt; &#xA;&lt;p&gt;Parler-TTS is a lightweight text-to-speech (TTS) model that can generate high-quality, natural sounding speech in the style of a given speaker (gender, pitch, speaking style, etc). It is a reproduction of work from the paper &lt;a href=&#34;https://www.text-description-to-speech.com&#34;&gt;Natural language guidance of high-fidelity text-to-speech with synthetic annotations&lt;/a&gt; by Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively.&lt;/p&gt; &#xA;&lt;p&gt;Contrarily to other TTS models, Parler-TTS is a &lt;strong&gt;fully open-source&lt;/strong&gt; release. All of the datasets, pre-processing, training code and weights are released publicly under permissive license, enabling the community to build on our work and develop their own powerful TTS models.&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the inference and training code for Parler-TTS. It is designed to accompany the &lt;a href=&#34;https://github.com/huggingface/dataspeech&#34;&gt;Data-Speech&lt;/a&gt; repository for dataset annotation.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;08/08/2024:&lt;/strong&gt; We are proud to release two new Parler-TTS checkpoints:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/parler-tts/parler-tts-mini-v1&#34;&gt;Parler-TTS Mini&lt;/a&gt;, an 880M parameter model.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/parler-tts/parler-tts-large-v1&#34;&gt;Parler-TTS Large&lt;/a&gt;, a 2.3B parameter model.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;These checkpoints have been trained on 45k hours of audiobook data.&lt;/p&gt; &#xA; &lt;p&gt;In addition, the code is optimized for much faster generation: we&#39;ve added SDPA and Flash Attention 2 compatibility, as well as the ability to compile the model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ðŸ“– Quick Index&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#-random-voice&#34;&gt;ðŸŽ² Using a random voice&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#-using-a-specific-speaker&#34;&gt;ðŸŽ¯ Using a specific speaker&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/parler-tts/parler_tts&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/parler-tts&#34;&gt;Model weights and datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#-optimizing-inference-speed&#34;&gt;Optimizing inference&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Parler-TTS has light-weight dependencies and can be installed in one line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install git+https://github.com/huggingface/parler-tts.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Apple Silicon users will need to run a follow-up command to make use the nightly PyTorch (2.4) build for bfloat16 support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip3 install --pre torch torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] You can directly try it out in an interactive demo &lt;a href=&#34;https://huggingface.co/spaces/parler-tts/parler_tts&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Using Parler-TTS is as simple as &#34;bonjour&#34;. Simply install the library once:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install git+https://github.com/huggingface/parler-tts.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ðŸŽ² Random voice&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Parler-TTS&lt;/strong&gt; has been trained to generate speech with features that can be controlled with a simple text prompt, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import torch&#xA;from parler_tts import ParlerTTSForConditionalGeneration&#xA;from transformers import AutoTokenizer&#xA;import soundfile as sf&#xA;&#xA;device = &#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;&#xA;model = ParlerTTSForConditionalGeneration.from_pretrained(&#34;parler-tts/parler-tts-mini-v1&#34;).to(device)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;parler-tts/parler-tts-mini-v1&#34;)&#xA;&#xA;prompt = &#34;Hey, how are you doing today?&#34;&#xA;description = &#34;A female speaker delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker&#39;s voice sounding clear and very close up.&#34;&#xA;&#xA;input_ids = tokenizer(description, return_tensors=&#34;pt&#34;).input_ids.to(device)&#xA;prompt_input_ids = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids.to(device)&#xA;&#xA;generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)&#xA;audio_arr = generation.cpu().numpy().squeeze()&#xA;sf.write(&#34;parler_tts_out.wav&#34;, audio_arr, model.config.sampling_rate)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ðŸŽ¯ Using a specific speaker&lt;/h3&gt; &#xA;&lt;p&gt;To ensure speaker consistency across generations, this checkpoint was also trained on 34 speakers, characterized by name (e.g. Jon, Lea, Gary, Jenna, Mike, Laura).&lt;/p&gt; &#xA;&lt;p&gt;To take advantage of this, simply adapt your text description to specify which speaker to use: &lt;code&gt;Jon&#39;s voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import torch&#xA;from parler_tts import ParlerTTSForConditionalGeneration&#xA;from transformers import AutoTokenizer&#xA;import soundfile as sf&#xA;&#xA;device = &#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;&#xA;model = ParlerTTSForConditionalGeneration.from_pretrained(&#34;parler-tts/parler-tts-mini-v1&#34;).to(device)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;parler-tts/parler-tts-mini-v1&#34;)&#xA;&#xA;prompt = &#34;Hey, how are you doing today?&#34;&#xA;description = &#34;Jon&#39;s voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.&#34;&#xA;&#xA;input_ids = tokenizer(description, return_tensors=&#34;pt&#34;).input_ids.to(device)&#xA;prompt_input_ids = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids.to(device)&#xA;&#xA;generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)&#xA;audio_arr = generation.cpu().numpy().squeeze()&#xA;sf.write(&#34;parler_tts_out.wav&#34;, audio_arr, model.config.sampling_rate)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Include the term &#34;very clear audio&#34; to generate the highest quality audio, and &#34;very noisy audio&#34; for high levels of background noise&lt;/li&gt; &#xA; &lt;li&gt;Punctuation can be used to control the prosody of the generations, e.g. use commas to add small breaks in speech&lt;/li&gt; &#xA; &lt;li&gt;The remaining speech features (gender, speaking rate, pitch and reverberation) can be controlled directly through the prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;âœ¨ Optimizing Inference Speed&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve set up an &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/INFERENCE.md&#34;&gt;inference guide&lt;/a&gt; to make generation faster. Think SDPA, torch.compile and streaming!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/parler-tts/assets/52246514/251e2488-fe6e-42c1-81cd-814c5b7795b0&#34;&gt;https://github.com/huggingface/parler-tts/assets/52246514/251e2488-fe6e-42c1-81cd-814c5b7795b0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] The training guide has yet to be adapted to the newest checkpoints.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ylacombe/scripts_and_notebooks/blob/main/Finetuning_Parler_TTS_on_a_single_speaker_dataset.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/&#34;&gt;training folder&lt;/a&gt; contains all the information to train or fine-tune your own Parler-TTS model. It consists of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#1-architecture&#34;&gt;1. An introduction to the Parler-TTS architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#2-getting-started&#34;&gt;2. The first steps to get started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#3-training&#34;&gt;3. A training guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;TL;DR:&lt;/strong&gt; After having followed the &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#requirements&#34;&gt;installation steps&lt;/a&gt;, you can reproduce the Parler-TTS Mini v0.1 training recipe with the following command line:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;accelerate launch ./training/run_parler_tts_training.py ./helpers/training_configs/starting_point_0.01.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This library builds on top of a number of open-source giants, to whom we&#39;d like to extend our warmest thanks for providing these tools!&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively, for publishing such a promising and clear research paper: &lt;a href=&#34;https://arxiv.org/abs/2402.01912&#34;&gt;Natural language guidance of high-fidelity text-to-speech with synthetic annotations&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;the many libraries used, namely &lt;a href=&#34;https://huggingface.co/docs/datasets/v2.17.0/en/index&#34;&gt;ðŸ¤— datasets&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/accelerate/en/index&#34;&gt;ðŸ¤— accelerate&lt;/a&gt;, &lt;a href=&#34;https://github.com/jitsi/jiwer&#34;&gt;jiwer&lt;/a&gt;, &lt;a href=&#34;https://wandb.ai/&#34;&gt;wandb&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;ðŸ¤— transformers&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Descript for the &lt;a href=&#34;https://github.com/descriptinc/descript-audio-codec&#34;&gt;DAC codec model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face ðŸ¤— for providing compute resources and time to explore!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this repository useful, please consider citing this work and also the original Stability AI paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lacombe-etal-2024-parler-tts,&#xA;  author = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},&#xA;  title = {Parler-TTS},&#xA;  year = {2024},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/huggingface/parler-tts}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lyth2024natural,&#xA;      title={Natural language guidance of high-fidelity text-to-speech with synthetic annotations},&#xA;      author={Dan Lyth and Simon King},&#xA;      year={2024},&#xA;      eprint={2402.01912},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.SD}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome, as the project offers many possibilities for improvement and exploration.&lt;/p&gt; &#xA;&lt;p&gt;Namely, we&#39;re looking at ways to improve both quality and speed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Datasets: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Train on more data&lt;/li&gt; &#xA;   &lt;li&gt;Add more features such as accents&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Training: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add PEFT compatibility to do Lora fine-tuning.&lt;/li&gt; &#xA;   &lt;li&gt;Add possibility to train without description column.&lt;/li&gt; &#xA;   &lt;li&gt;Add notebook training.&lt;/li&gt; &#xA;   &lt;li&gt;Explore multilingual training.&lt;/li&gt; &#xA;   &lt;li&gt;Explore mono-speaker finetuning.&lt;/li&gt; &#xA;   &lt;li&gt;Explore more architectures.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Optimization: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Compilation and static cache&lt;/li&gt; &#xA;   &lt;li&gt;Support to FA2 and SDPA&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Evaluation: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add more evaluation metrics&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/ao</title>
    <updated>2024-08-11T01:45:35Z</updated>
    <id>tag:github.com,2024-08-11:/pytorch/ao</id>
    <link href="https://github.com/pytorch/ao" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The missing pytorch dtype and layout library for training and inference&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;torchao: PyTorch Architecture Optimization&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/cudamode&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/cudamode?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#introduction&#34;&gt;Introduction&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#inference&#34;&gt;Inference&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#training&#34;&gt;Training&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#newer-dtypes&#34;&gt;Dtypes&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#composability&#34;&gt;Composability&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#installation&#34;&gt;Installation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#community-contributions&#34;&gt;Community Contributions&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#how-to-contribute&#34;&gt;How to contribute&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;torchao is a library to create and integrate high-performance custom data types, optimization techniques and kernels into your PyTorch workflows with up to &lt;strong&gt;2x speedups&lt;/strong&gt; with &lt;strong&gt;65% less VRAM&lt;/strong&gt; for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#inference&#34;&gt;inference&lt;/a&gt; and support for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#training&#34;&gt;training&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All with no intrusive code changes and minimal accuracy degradation.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;h4&gt;Without intrusive code changes&lt;/h4&gt; &#xA;&lt;p&gt;Quantizing your models is a 1 liner that should work on any model with an &lt;code&gt;nn.Linear&lt;/code&gt; including your favorite HuggingFace model. You can find a more comprehensive usage instructions &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/quantization/&#34;&gt;here&lt;/a&gt; and a HuggingFace inference example &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/scripts/hf_eval.py&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchao.quantization.quant_api import quantize_, int4_weight_only&#xA;quantize_(m, int4_weight_only())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Benchmarks are run on a machine with a single A100 GPU using the script in &lt;code&gt;_models/llama&lt;/code&gt; which generates text in a latency-optimized way (batchsize=1)&lt;/p&gt; &#xA;&lt;p&gt;The models used were &lt;code&gt;meta-llama/Llama-2-7b-chat-hf&lt;/code&gt; and &lt;code&gt;meta-llama/Meta-Llama-3-8B&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Technique&lt;/th&gt; &#xA;   &lt;th&gt;wikitext-perplexity&lt;/th&gt; &#xA;   &lt;th&gt;Tokens/Second&lt;/th&gt; &#xA;   &lt;th&gt;Memory Bandwidth (GB/s)&lt;/th&gt; &#xA;   &lt;th&gt;Peak Memory (GB)&lt;/th&gt; &#xA;   &lt;th&gt;Model Size (GB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;Base (bfloat16)&lt;/td&gt; &#xA;   &lt;td&gt;12.212&lt;/td&gt; &#xA;   &lt;td&gt;105.14&lt;/td&gt; &#xA;   &lt;td&gt;1389.35&lt;/td&gt; &#xA;   &lt;td&gt;13.88&lt;/td&gt; &#xA;   &lt;td&gt;13.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;int8dq&lt;/td&gt; &#xA;   &lt;td&gt;12.262&lt;/td&gt; &#xA;   &lt;td&gt;9.20&lt;/td&gt; &#xA;   &lt;td&gt;60.93&lt;/td&gt; &#xA;   &lt;td&gt;8.33&lt;/td&gt; &#xA;   &lt;td&gt;6.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;int8wo&lt;/td&gt; &#xA;   &lt;td&gt;12.204&lt;/td&gt; &#xA;   &lt;td&gt;150.18&lt;/td&gt; &#xA;   &lt;td&gt;994.40&lt;/td&gt; &#xA;   &lt;td&gt;8.95&lt;/td&gt; &#xA;   &lt;td&gt;6.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;int4wo-64&lt;/td&gt; &#xA;   &lt;td&gt;12.843&lt;/td&gt; &#xA;   &lt;td&gt;199.86&lt;/td&gt; &#xA;   &lt;td&gt;746.66&lt;/td&gt; &#xA;   &lt;td&gt;4.50&lt;/td&gt; &#xA;   &lt;td&gt;3.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;int4wo-64-GPTQ&lt;/td&gt; &#xA;   &lt;td&gt;12.489&lt;/td&gt; &#xA;   &lt;td&gt;199.86&lt;/td&gt; &#xA;   &lt;td&gt;746.66&lt;/td&gt; &#xA;   &lt;td&gt;4.50&lt;/td&gt; &#xA;   &lt;td&gt;3.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;autoquant&lt;/td&gt; &#xA;   &lt;td&gt;12.204&lt;/td&gt; &#xA;   &lt;td&gt;159.22&lt;/td&gt; &#xA;   &lt;td&gt;1069.87&lt;/td&gt; &#xA;   &lt;td&gt;8.91&lt;/td&gt; &#xA;   &lt;td&gt;6.72&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-8B&lt;/td&gt; &#xA;   &lt;td&gt;Base (bfloat16)&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;94.97&lt;/td&gt; &#xA;   &lt;td&gt;1425.55&lt;/td&gt; &#xA;   &lt;td&gt;16.43&lt;/td&gt; &#xA;   &lt;td&gt;15.01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;int8dq&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;8.44&lt;/td&gt; &#xA;   &lt;td&gt;63.45&lt;/td&gt; &#xA;   &lt;td&gt;8.98&lt;/td&gt; &#xA;   &lt;td&gt;7.52&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;int8wo&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;139.76&lt;/td&gt; &#xA;   &lt;td&gt;1051.02&lt;/td&gt; &#xA;   &lt;td&gt;10.42&lt;/td&gt; &#xA;   &lt;td&gt;7.52&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;int4wo-64&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;179.44&lt;/td&gt; &#xA;   &lt;td&gt;757.60&lt;/td&gt; &#xA;   &lt;td&gt;6.62&lt;/td&gt; &#xA;   &lt;td&gt;4.22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;autoquant&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;137.71&lt;/td&gt; &#xA;   &lt;td&gt;1037.74&lt;/td&gt; &#xA;   &lt;td&gt;11.08&lt;/td&gt; &#xA;   &lt;td&gt;7.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;note: Int8 dynamic quantization works best on compute bound as opposed to memory bound models. Some relatable examples might be &lt;a href=&#34;https://github.com/pytorch-labs/segment-anything-fast&#34;&gt;SAM&lt;/a&gt; which is compute bound vs Llama at batchsize=1 which is memory bound.&lt;/p&gt; &#xA;&lt;p&gt;For int4 we make heavy use of &lt;a href=&#34;https://github.com/pytorch/ao/raw/cb3bd8c674f2123af232a0231b5e38ddafa756a8/torchao/dtypes/aqt.py#L526&#34;&gt;tinygemm&lt;/a&gt; of &lt;code&gt;torch.ops.aten._weight_int4pack_mm&lt;/code&gt; to bitpack into a layout optimized for tensor cores&lt;/p&gt; &#xA;&lt;p&gt;And a quick crash course on inference quantization to help parse the above table. Int4 quantization is an ambiguous term because there&#39;s the dtype in which a layer is represented and then the dtype in which the computation is done. For example, if you&#39;re using Weight-Only (wo) int4 quantization that means that the layer will be upcasted to a larger dtype like fp16 so an int4 matrix multiplication is defined as &lt;code&gt;F.linear(input, weight.to(input.dtype))&lt;/code&gt;. Dynamic quantization (DQ) primarily targets activations, enabling on-the-fly quantization from higher precision formats like bf16 to lower precision formats such as int8. This process, when supported by hardware, allows for direct computation, such as performing &lt;code&gt;F.linear(input, weight)&lt;/code&gt;. Naive quantization algorithms are also notoriously sensitive to outliers so we also typically set a group size that applies a scale factor per group of 64 elements in the case of &lt;code&gt;int4wo64&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Sparsifying your model is also a 1 liner that should work on any model with an &lt;code&gt;nn.Linear&lt;/code&gt;. We find that sparsity works best on compute bound models like SAM, specifically the MLP layers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchao.sparsity import sparsify, semi_sparse_weight()&#xA;&#xA;m = sparsify_(m, semi_sparse_weight())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sparsity can also be composed with int8 dynamic quantization for further speedups:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchao.sparsity import sparsify, int8_dynamic_activation_int8_semi_sparse_weight&#xA;&#xA;m = sparsify_(m, int8_dynamic_activation_int8_semi_sparse_weight())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We found that applying int8 dynamic quantization to the attention layers, int8 dynamic quantization + semi sparse (2:4) sparsity to mlp layer 1 and 2:4 sparsity to mlp layer 2 yielded the best configuration. We were able to provide a &lt;strong&gt;1.16x (22.7 -&amp;gt; 26.5 img/s) speedup over our dense baseline, while maintaining 97.5% (0.581 -&amp;gt; 0.567) of the evaluation accuracy (mIOU)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The following benchmarks were ran for &lt;a href=&#34;https://github.com/pytorch-labs/segment-anything-fast&#34;&gt;segment-anything-fast&lt;/a&gt; ViT-h on an NVIDIA-A100-80GB, with batch_size=32 and &lt;code&gt;bfloat16&lt;/code&gt; dtype, with &lt;code&gt;torch.compile=&#34;max_autotune&#34;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Technique&lt;/th&gt; &#xA;   &lt;th&gt;img/s&lt;/th&gt; &#xA;   &lt;th&gt;memory (MiB)&lt;/th&gt; &#xA;   &lt;th&gt;mIoU (coco2017 val)&lt;/th&gt; &#xA;   &lt;th&gt;relative speedup&lt;/th&gt; &#xA;   &lt;th&gt;relative accuracy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-h&lt;/td&gt; &#xA;   &lt;td&gt;baseline (bfloat16, max-autotune)&lt;/td&gt; &#xA;   &lt;td&gt;22.75&lt;/td&gt; &#xA;   &lt;td&gt;15172&lt;/td&gt; &#xA;   &lt;td&gt;0.5811&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;int8 dynamic quant (attn + mlp)&lt;/td&gt; &#xA;   &lt;td&gt;24.91&lt;/td&gt; &#xA;   &lt;td&gt;15154&lt;/td&gt; &#xA;   &lt;td&gt;0.5822&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.09x&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;100.19%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2:4 sparsity (mlp only)&lt;/td&gt; &#xA;   &lt;td&gt;24.81&lt;/td&gt; &#xA;   &lt;td&gt;15632&lt;/td&gt; &#xA;   &lt;td&gt;0.5672&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.10x&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;97.61%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2:4 sparsity (attn + mlp)&lt;/td&gt; &#xA;   &lt;td&gt;24.30&lt;/td&gt; &#xA;   &lt;td&gt;13429&lt;/td&gt; &#xA;   &lt;td&gt;0.5306&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.07x&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;91.31%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;int8 dynamic quant (attn)&lt;br&gt;int8 dynamic quant + 2:4 sparsity (mlp lin1)&lt;br&gt;2:4 sparsity (mlp lin2)&lt;/td&gt; &#xA;   &lt;td&gt;26.46&lt;/td&gt; &#xA;   &lt;td&gt;14865&lt;/td&gt; &#xA;   &lt;td&gt;0.5668&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.16x&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;97.54%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To reproduce our benchmarks please follow these &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/_models/sam/README.md&#34;&gt;instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;With intrusive code changes&lt;/h4&gt; &#xA;&lt;p&gt;In some cases we rewrote popular GenAI models to be significantly faster in native PyTorch as in no C++/CUDA to achieve at the time SOTA inference performance. These involve more intrusive code changes.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8x with in speedups for Image segmentation models with &lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai&#34;&gt;sam-fast&lt;/a&gt; (9.5x with int8 dynamic quantization + 2:4 sparsity)&lt;/li&gt; &#xA; &lt;li&gt;10x speedups for Language models with &lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai-2&#34;&gt;gpt-fast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;3x speedup for Diffusion models with &lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai-3&#34;&gt;sd-fast&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;h4&gt;Float8&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/float8&#34;&gt;torchao.float8&lt;/a&gt; implements training recipes with the scaled float8 dtypes, as laid out in &lt;a href=&#34;https://arxiv.org/abs/2209.05433&#34;&gt;https://arxiv.org/abs/2209.05433&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Sparsity&lt;/h4&gt; &#xA;&lt;p&gt;We&#39;ve added support for semi-structured 2:4 sparsity with 6% end to end speedups on ViT-L&lt;/p&gt; &#xA;&lt;p&gt;The code change is a 1 liner with the full example available &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/sparsity/training/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;swap_linear_with_semi_sparse_linear(model, {&#34;seq.0&#34;: SemiSparseLinear})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Newer dtypes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/mx_formats&#34;&gt;MX&lt;/a&gt; implementing training and inference support with tensors using the &lt;a href=&#34;https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf&#34;&gt;OCP MX spec&lt;/a&gt; data types, which can be described as groupwise scaled float8/float6/float4/int8, with the scales being constrained to powers of two. This work is prototype as the hardware support is not available yet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/dtypes/nf4tensor.py&#34;&gt;nf4&lt;/a&gt; which was used to &lt;a href=&#34;https://github.com/pytorch/torchtune/raw/main/docs/source/tutorials/qlora_finetune.rst&#34;&gt;implement QLoRA&lt;/a&gt; one of the most popular finetuning algorithms without writing custom Triton or CUDA code. Accessible talk &lt;a href=&#34;https://x.com/HamelHusain/status/1800315287574847701&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/quant_llm/&#34;&gt;fp6&lt;/a&gt; for 2x faster inference over fp16 with an easy to use API &lt;code&gt;quantize_(model, fp6_llm_weight_only())&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Composability&lt;/h2&gt; &#xA;&lt;p&gt;A key design principle for us is composability as in any new dtype or layout we provide needs to work with &lt;code&gt;torch.compile()&lt;/code&gt; and needs to work with &lt;code&gt;FSDP&lt;/code&gt;. It shouldn&#39;t matter if the kernels are written in pure PyTorch, CUDA, C++, or Triton - things should just work! And here is our current strategy&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Write the dtype, layout or bit packing logic in pure PyTorch and code-generate efficient kernels with torch.compile. You can inspect those kernels with &lt;code&gt;TORCH_LOGS=&#34;output_code&#34; python your_code.py&lt;/code&gt; and check if a single kernel is being generated and if any unnecessary buffers are being created&lt;/li&gt; &#xA; &lt;li&gt;However once you get a kernel, how do you know how good it is? The best way is to benchmark the compiler generated code with the best kernel on the market. But packaging custom CPP/CUDA kernels that work on multiple devices is tedious but we&#39;ve abstracted all the tedium from you with our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/csrc/&#34;&gt;custom ops support&lt;/a&gt; so if you love writing kernels but hate packaging, we&#39;d love to accept contributions for your custom ops. One key benefit is a kernel written as a custom op will just work with no graph breaks with &lt;code&gt;torch.compile()&lt;/code&gt;. Compilers are great at optimizations like fusions and overhead reduction but it&#39;s challenging for compilers to rewrite the math of an algorithm such that it&#39;s faster but also numerically stable so we are betting on both compilers and custom ops&lt;/li&gt; &#xA; &lt;li&gt;Finally while historically most quantization has been done for inference, there is now a thriving area of research combining distributed algorithms and quantization. One popular example is &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/dtypes/nf4tensor.py&#34;&gt;NF4&lt;/a&gt; which was used to implement the QLoRA algorithm. The NF4 tensor also contains semantics for how it should be sharded over multiple devices so it composes with FSDP. We gave an accessible talk on &lt;a href=&#34;https://x.com/HamelHusain/status/1800315287574847701&#34;&gt;how to do this&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;torchao&lt;/code&gt; makes liberal use of several new features in Pytorch, it&#39;s recommended to use it with the current nightly or latest stable version of PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;Our minimum supported Python version is 3.9 since 3.8 has reached end of life&lt;/p&gt; &#xA;&lt;h4&gt;Install torch&lt;/h4&gt; &#xA;&lt;p&gt;Install torch stable&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or torch nightlies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install torchao&lt;/h4&gt; &#xA;&lt;p&gt;Stable release from Pypi which will default to CUDA 12.1&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;pip install torchao&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Stable Release from the PyTorch index&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;pip install torchao --extra-index-url https://download.pytorch.org/whl/cu121 # full options are cpu/cu118/cu121/cu124&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Nightly Release&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;pip install --pre torchao-nightly --index-url https://download.pytorch.org/whl/nightly/cu121 # full options are cpu/cu118/cu121/cu124&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git clone https://github.com/pytorch/ao&#xA;cd ao&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Community Contributions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jeromeku&#34;&gt;jeromeku&lt;/a&gt; has implemented &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/galore/&#34;&gt;GaLore&lt;/a&gt; a drop for the Adam Optimizer that allows you to finetune llama 7b on a single 4090 card with up to 70% speedups relative to eager PyTorch&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/dora&#34;&gt;DoRA&lt;/a&gt; a newer replacement for QLoRA with more promising convergence characteristics&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/hqq&#34;&gt;Fused int4/fp16 Quant Matmul&lt;/a&gt; which is particularly useful for compute bound kernels showing 4x speedups over tinygemm for larger batch sizes such as 512&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gau-nernst&#34;&gt;gau-nernst&lt;/a&gt; fp6 kernels that are 4x faster than fp16 &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/quant_llm&#34;&gt;torchao/prototype/quant_llm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vayuda&#34;&gt;vayuda&lt;/a&gt; with generic bitpacking kernels that were code generated using pure PyTorch &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/common&#34;&gt;prototype/common&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andreaskoepf&#34;&gt;andreaskopf&lt;/a&gt; and &lt;a href=&#34;https://github.com/melvinebenezer&#34;&gt;melvinebenezer&lt;/a&gt; with &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/dtypes&#34;&gt;1 bit LLMs&lt;/a&gt; Bitnet 1.58 bitpacked into uint2 and fully code-generated with torch.compile&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Blogs and Videos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/accelerating-neural-network-training/&#34;&gt;Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mobiusml.github.io/whisper-static-cache-blog/&#34;&gt;https://mobiusml.github.io/whisper-static-cache-blog/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://x.com/HamelHusain/status/1800315287574847701&#34;&gt;Slaying OOMs at the Mastering LLM&#39;s course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/1u9xUK3G4VM?si=4JcPlw2w8chPXW8J&#34;&gt;Advanced Quantization at CUDA MODE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/live/v_q2JTIqE20?si=mf7HeZ63rS-uYpS6&#34;&gt;Chip Huyen&#39;s GPU Optimization Workshop&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to contribute&lt;/h2&gt; &#xA;&lt;p&gt;This repository is currently under heavy development&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have suggestions on the API or use cases you&#39;d like to be covered, please open an &lt;a href=&#34;https://github.com/pytorch/ao/issues&#34;&gt;issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;d like to co-develop the library with us please join us on #torchao on &lt;a href=&#34;https://discord.gg/cudamode&#34;&gt;discord.gg/cudamode&lt;/a&gt; - there are a lot of dtypes out there and we could use a lot more hands to make them go brrr&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re contributing a feature to ao&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;pip install -r dev-requirements.txt&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;em&gt;most&lt;/em&gt; developers you probably want to skip building custom C++/CUDA extensions for faster iteration&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;USE_CPP=0 python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;torchao&lt;/code&gt; is released under the &lt;a href=&#34;https://github.com/pytorch-labs/ao/raw/main/LICENSE&#34;&gt;BSD 3&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MrForExample/ComfyUI-3D-Pack</title>
    <updated>2024-08-11T01:45:35Z</updated>
    <id>tag:github.com,2024-08-11:/MrForExample/ComfyUI-3D-Pack</id>
    <link href="https://github.com/MrForExample/ComfyUI-3D-Pack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An extensive node suite that enables ComfyUI to process 3D inputs (Mesh &amp; UV Texture, etc) using cutting edge algorithms (3DGS, NeRF, etc.)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI-3D-Pack&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make 3D assets generation in ComfyUI good and convenient as it generates image/video!&lt;/strong&gt; &lt;br&gt; This is an extensive node suite that enables ComfyUI to process 3D inputs (Mesh &amp;amp; UV Texture, etc.) using cutting edge algorithms (3DGS, NeRF, etc.) and models (InstantMesh, CRM, TripoSR, etc.)&lt;/p&gt; &#xA;&lt;span style=&#34;font-size:1.5em;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#Features&#34;&gt;Features&lt;/a&gt; â€” &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#install&#34;&gt;Install&lt;/a&gt; â€” &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt; â€” &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#development&#34;&gt;Development&lt;/a&gt; â€” &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#tips&#34;&gt;Tips&lt;/a&gt; â€” &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/#supporters&#34;&gt;Supporters&lt;/a&gt; &lt;/span&gt; &#xA;&lt;h2&gt;Install:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can be installed directly from &lt;a href=&#34;https://github.com/ltdrdata/ComfyUI-Manager&#34;&gt;ComfyUI-Manager&lt;/a&gt;ðŸš€&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MrForExample/Comfy3D_Pre_Builds&#34;&gt;Pre-builds&lt;/a&gt; are available for: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Windows 10/11, Ubuntu 22.04&lt;/li&gt; &#xA;   &lt;li&gt;Python 3.10/3.11/3.12&lt;/li&gt; &#xA;   &lt;li&gt;CUDA 12.1/11.8&lt;/li&gt; &#xA;   &lt;li&gt;torch 2.3.0+cu121/cu118, torchvision 0.18.0+cu121/cu118&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/install.py&#34;&gt;install.py&lt;/a&gt; will download &amp;amp; install Pre-builds automatically according to your runtime environment, if it couldn&#39;t find corresponding Pre-builds, then &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Pre_Builds/_Build_Scripts/auto_build_all.py&#34;&gt;build script&lt;/a&gt; will start automatically, if automatic build doesn&#39;t work for you, then please check out &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Pre_Builds/README.md#build-required-packages-semi-automatically&#34;&gt;Semi-Automatic Build Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you have any missing node in any open Comfy3D workflow, try simply click &lt;a href=&#34;https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#support-of-missing-nodes-installation&#34;&gt;Install Missing Custom Nodes&lt;/a&gt; in ComfyUI-Manager&lt;/li&gt; &#xA; &lt;li&gt;If for some reason your comfy3d can&#39;t download pre-trained models automatically, you can always download them manually and put them in to correct folder under &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/Checkpoints&#34;&gt;Checkpoints&lt;/a&gt; directory, but please &lt;strong&gt;DON&#39;T&lt;/strong&gt; overwrite any exist .json files&lt;/li&gt; &#xA; &lt;li&gt;Docker install please check &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/DOCKER_INSTRUCTIONS.md&#34;&gt;DOCKER_INSTRUCTIONS.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; at this moment, you&#39;ll still need to install &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Pre_Builds/README.md#build-for-windows&#34;&gt;Visual Studio Build Tools for windows&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Pre_Builds/README.md#build-for-linux&#34;&gt;install &lt;code&gt;gcc g++&lt;/code&gt; for Linux&lt;/a&gt; in order for &lt;code&gt;InstantNGP &amp;amp; Convert 3DGS to Mesh with NeRF and Marching_Cubes&lt;/code&gt; nodes to work, since those two nodes used JIT torch cpp extension that builds in runtime, but I plan to replace those nodes soon&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For use cases please check out &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/&#34;&gt;Example Workflows&lt;/a&gt;. [&lt;strong&gt;Last update: 01/August/2024&lt;/strong&gt;]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; you need to put &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Inputs_Files/&#34;&gt;Example Inputs Files &amp;amp; Folders&lt;/a&gt; under ComfyUI Root Directory\ComfyUI\input folder before you can run the example workflow&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/C0nsumption/Consume-ComfyUI-Workflows/tree/main/assets/tripo_sr/00&#34;&gt;tripoSR-layered-diffusion workflow&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/c0nsumption_&#34;&gt;@Consumption&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;StableFast3D&lt;/strong&gt;: &lt;a href=&#34;https://github.com/Stability-AI/stable-fast-3d&#34;&gt;Stability-AI/stable-fast-3d&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Single image to 3D Mesh with RGB texture&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Note: you need to agree to Stability-AI&#39;s term of usage before been able to download the model weights, if you downloaded model weights manually, then you need to put it under &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/Checkpoints/StableFast3D&#34;&gt;Checkpoints/StableFast3D&lt;/a&gt;, otherwise you can add your huggingface token in &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/Configs/system.conf&#34;&gt;Configs/system.conf&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/stabilityai/stable-fast-3d/tree/main&#34;&gt;https://huggingface.co/stabilityai/stable-fast-3d/tree/main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/user-attachments/assets/3ed3d1ed-4abe-4959-bd79-4431d19c9d47&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CharacterGen&lt;/strong&gt;: &lt;a href=&#34;https://github.com/zjp-shadow/CharacterGen&#34;&gt;zjp-shadow/CharacterGen&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Single front view of a character with arbitrary pose&lt;/li&gt; &#xA;   &lt;li&gt;Can &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/CharacterGen/CharacterGen_to_Unique3D.json&#34;&gt;combine with Unique3D workflow&lt;/a&gt; for better result&lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/zjpshadow/CharacterGen/tree/main&#34;&gt;https://huggingface.co/zjpshadow/CharacterGen/tree/main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/user-attachments/assets/4f0ae0c0-2d29-49f0-a6f2-a636dd4b4dcc&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unique3D&lt;/strong&gt;: &lt;a href=&#34;https://github.com/AiuniAI/Unique3D&#34;&gt;AiuniAI/Unique3D&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Four stages pipeline: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Single image to 4 multi-view images with resulution: 256X256&lt;/li&gt; &#xA;     &lt;li&gt;Consistent Multi-view images Upscale to 512X512, super resolution to 2048X2048&lt;/li&gt; &#xA;     &lt;li&gt;Multi-view images to Normal maps with resulution: 512X512, super resolution to 2048X2048&lt;/li&gt; &#xA;     &lt;li&gt;Multi-view images &amp;amp; Normal maps to 3D mesh with texture&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;To use the &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/Unique3D/Unique3D_All_Stages.json&#34;&gt;All stage Unique3D workflow&lt;/a&gt;, Download Models: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.ckpt&#34;&gt;sdv1.5-pruned-emaonly&lt;/a&gt; and put it into &lt;code&gt;Your ComfyUI root directory/ComfyUI/models/checkpoints&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/Wuvin/Unique3D/tree/main/ckpt/controlnet-tile&#34;&gt;fine-tuned controlnet-tile&lt;/a&gt; and put it into &lt;code&gt;Your ComfyUI root directory/ComfyUI/models/controlnet&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/blob/main/models/ip-adapter_sd15.safetensors&#34;&gt;ip-adapter_sd15&lt;/a&gt; and put it into &lt;code&gt;Your ComfyUI root directory/ComfyUI/models/ipadapter&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/blob/main/models/image_encoder/model.safetensors&#34;&gt;OpenCLIP-ViT-H-14&lt;/a&gt;, rename it to &lt;strong&gt;OpenCLIP-ViT-H-14.safetensors&lt;/strong&gt; and put it into &lt;code&gt;Your ComfyUI root directory/ComfyUI/models/clip_vision&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth&#34;&gt;RealESRGAN_x4plus&lt;/a&gt; and put it into &lt;code&gt;Your ComfyUI root directory/ComfyUI/models/upscale_models&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/spaces/Wuvin/Unique3D/tree/main/ckpt&#34;&gt;https://huggingface.co/spaces/Wuvin/Unique3D/tree/main/ckpt&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/45dd6bfc-4f2b-4b1f-baed-13a1b0722896&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Era3D MVDiffusion Model&lt;/strong&gt;: &lt;a href=&#34;https://github.com/pengHTYX/Era3D&#34;&gt;pengHTYX/Era3D&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Single image to 6 multi-view images &amp;amp; normal maps with resulution: 512X512&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Note: you need at least 16GB vram to run this model&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/pengHTYX/MacLab-Era3D-512-6view/tree/main&#34;&gt;https://huggingface.co/pengHTYX/MacLab-Era3D-512-6view/tree/main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/fc210cac-6c7d-4a55-926c-adb5fb7b0c57&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InstantMesh Reconstruction Model&lt;/strong&gt;: &lt;a href=&#34;https://github.com/TencentARC/InstantMesh&#34;&gt;TencentARC/InstantMesh&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sparse multi-view images with white background to 3D Mesh with RGB texture&lt;/li&gt; &#xA;   &lt;li&gt;Works with arbitrary MVDiffusion models (Probably works best with Zero123++, but also works with CRM MVDiffusion model)&lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/TencentARC/InstantMesh/tree/main&#34;&gt;https://huggingface.co/TencentARC/InstantMesh/tree/main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/a0648a44-f8cb-4f78-9704-a907f9174936&#34;&gt;&lt;/video&gt; &#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/33aecedb-f595-4c12-90dd-89d5f718598e&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero123++&lt;/strong&gt;: &lt;a href=&#34;https://github.com/SUDO-AI-3D/zero123plus&#34;&gt;SUDO-AI-3D/zero123plus&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Single image to 6 view images with resulution: 320X320&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Convolutional Reconstruction Model&lt;/strong&gt;: &lt;a href=&#34;https://github.com/thu-ml/CRM&#34;&gt;thu-ml/CRM&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Three stages pipeline: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Single image to 6 view images (Front, Back, Left, Right, Top &amp;amp; Down)&lt;/li&gt; &#xA;     &lt;li&gt;Single image &amp;amp; 6 view images to 6 same views CCMs (Canonical Coordinate Maps)&lt;/li&gt; &#xA;     &lt;li&gt;6 view images &amp;amp; CCMs to 3D mesh&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Note: For low vram pc, if you can&#39;t fit all three models for each stages into your GPU memory, then you can divide those three stages into different comfy workflow and run them separately&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/sudo-ai/zero123plus-v1.2&#34;&gt;https://huggingface.co/sudo-ai/zero123plus-v1.2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/cf68bb83-9244-44df-9db8-f80eb3fdc29e&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TripoSR&lt;/strong&gt;: &lt;a href=&#34;https://github.com/VAST-AI-Research/TripoSR&#34;&gt;VAST-AI-Research/TripoSR&lt;/a&gt; | &lt;a href=&#34;https://github.com/flowtyone/ComfyUI-Flowty-TripoSR&#34;&gt;ComfyUI-Flowty-TripoSR&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Generate NeRF representation and using marching cube to turn it into 3D mesh&lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/stabilityai/TripoSR/tree/main&#34;&gt;https://huggingface.co/stabilityai/TripoSR/tree/main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/ec4f8df5-5907-4bbf-ba19-c0565fe95a97&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Wonder3D&lt;/strong&gt;: &lt;a href=&#34;https://github.com/xxlong0/Wonder3D&#34;&gt;xxlong0/Wonder3D&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Generate spatial consistent 6 views images &amp;amp; normal maps from a single image&lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/flamehaze1115/wonder3d-v1.0/tree/main&#34;&gt;https://huggingface.co/flamehaze1115/wonder3d-v1.0/tree/main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Outputs/Wonder3D_FatCat_MVs.jpg&#34; alt=&#34;Wonder3D_FatCat_MVs&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Large Multiview Gaussian Model&lt;/strong&gt;: &lt;a href=&#34;https://github.com/3DTopia/LGM&#34;&gt;3DTopia/LGM&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enable single image to 3D Gaussian in less than 30 seconds on a RTX3080 GPU, later you can also convert 3D Gaussian to mesh&lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/ashawkey/LGM/tree/main&#34;&gt;https://huggingface.co/ashawkey/LGM/tree/main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/e221d7f8-49ac-4ed4-809b-d4c790b6270e&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Triplane Gaussian Transformers&lt;/strong&gt;: &lt;a href=&#34;https://github.com/VAST-AI-Research/TriplaneGaussian&#34;&gt;VAST-AI-Research/TriplaneGaussian&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enable single image to 3D Gaussian in less than 10 seconds on a RTX3080 GPU, later you can also convert 3D Gaussian to mesh&lt;/li&gt; &#xA;   &lt;li&gt;Model weights: &lt;a href=&#34;https://huggingface.co/VAST-AI/TriplaneGaussian/tree/main&#34;&gt;https://huggingface.co/VAST-AI/TriplaneGaussian/tree/main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/90e7f298-bdbd-4c15-9378-1ca46cbb4871&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preview 3DGS and 3D Mesh&lt;/strong&gt;: 3D Visualization inside ComfyUI:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Using &lt;a href=&#34;https://github.com/huggingface/gsplat.js/tree/main&#34;&gt;gsplat.js&lt;/a&gt; and &lt;a href=&#34;https://github.com/mrdoob/three.js/tree/dev&#34;&gt;three.js&lt;/a&gt; for 3DGS &amp;amp; 3D Mesh visualization respectively&lt;/li&gt; &#xA;   &lt;li&gt;Custumizable background base on JS library: &lt;a href=&#34;https://github.com/mdbassit/Coloris&#34;&gt;mdbassit/Coloris&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/9f3c56b1-afb3-4bf1-8845-ab1025a87463&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stack Orbit Camera Poses&lt;/strong&gt;: Automatically generate all range of camera pose combinations&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;You can use it to conditioning the &lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/3d/&#34;&gt;StableZero123 (You need to Download the checkpoint first)&lt;/a&gt;, with full range of camera poses in one prompt pass&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;You can use it to generate the orbit camera poses and directly input to other 3D process node (e.g. GaussianSplatting and BakeTextureToMesh)&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Example usage:&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Outputs/Cammy_Cam_Rotate_Clockwise_Camposes.png&#34; width=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Outputs/Cammy_Cam_Rotate_Counter_Clockwise_Camposes.png&#34; width=&#34;256&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Outputs/Cammy_Cam_Rotate_Clockwise.gif&#34; width=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Example_Workflows/_Example_Outputs/Cammy_Cam_Rotate_Counter_Clockwise.gif&#34; width=&#34;256&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Coordinate system:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Azimuth: In top view, from angle 0 rotate 360 degree with step -90 you get (0, -90, -180/180, 90, 0), in this case camera rotates clock-wise, vice versa.&lt;/li&gt; &#xA;     &lt;li&gt;Elevation: 0 when camera points horizontally forward, pointing down to the ground is negitive angle, vice versa.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FlexiCubes&lt;/strong&gt;: &lt;a href=&#34;https://github.com/nv-tlabs/FlexiCubes&#34;&gt;nv-tlabs/FlexiCubes&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multi-View depth &amp;amp; mask (optional normal maps) as inputs&lt;/li&gt; &#xA;   &lt;li&gt;Export to 3D Mesh&lt;/li&gt; &#xA;   &lt;li&gt;Usage guide: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;voxel_grids_resolution&lt;/em&gt;: determine mesh resolution/quality&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;depth_min_distance&lt;/em&gt; &lt;em&gt;depth_max_distance&lt;/em&gt; : distance from object to camera, object parts in the render that is closer(futher) to camera than depth_min_distance(depth_max_distance) will be rendered with pure white(black) RGB value 1, 1, 1(0, 0, 0)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;mask_loss_weight&lt;/em&gt;: Control the silhouette of reconstrocted 3D mesh&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;depth_loss_weight&lt;/em&gt;: Control the shape of reconstrocted 3D mesh, this loss will also affect the mesh deform detail on the surface, so results depends on quality of the depth map&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;normal_loss_weight&lt;/em&gt;: Optional. Use to refine the mesh deform detail on the surface&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;sdf_regularizer_weight&lt;/em&gt;: Helps to remove floaters in areas of the shape that are not supervised by the application objective, such as internal faces when using image supervision only&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;remove_floaters_weight&lt;/em&gt;: This can be increased if you observe artifacts in flat areas&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;cube_stabilizer_weight&lt;/em&gt;: This does not have a significant impact during the optimization of a single shape, however it helps to stabilizing training in somecases&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&#xA;   &lt;video controls autoplay loop src=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/assets/62230687/166bbc1f-04b7-42c8-87bb-302e3f5aabb2&#34;&gt;&lt;/video&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Instant NGP&lt;/strong&gt;: &lt;a href=&#34;https://github.com/nerfstudio-project/nerfacc&#34;&gt;nerfacc&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multi-View images as inputs&lt;/li&gt; &#xA;   &lt;li&gt;Export to 3D Mesh using marching cubes&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;3D Gaussian Splatting&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ashawkey/diff-gaussian-rasterization&#34;&gt;Improved Differential Gaussian Rasterization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Better Compactness-based Densification method from &lt;a href=&#34;https://gsgen3d.github.io/&#34;&gt;Gsgen&lt;/a&gt;,&lt;/li&gt; &#xA;   &lt;li&gt;Support initialize gaussians from given 3D mesh (Optional)&lt;/li&gt; &#xA;   &lt;li&gt;Support mini-batch optimazation&lt;/li&gt; &#xA;   &lt;li&gt;Multi-View images as inputs&lt;/li&gt; &#xA;   &lt;li&gt;Export to standard 3DGS .ply format supported&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Gaussian Splatting Orbit Renderer&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Render 3DGS to images sequences or video, given a 3DGS file and camera poses generated by &lt;strong&gt;Stack Orbit Camera Poses&lt;/strong&gt; node&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mesh Orbit Renderer&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Render 3D mesh to images sequences or video, given a mesh file and camera poses generated by &lt;strong&gt;Stack Orbit Camera Poses&lt;/strong&gt; node&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fitting_Mesh_With_Multiview_Images&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bake Multi-View images into UVTexture of given 3D mesh using &lt;a href=&#34;https://github.com/NVlabs/nvdiffrast&#34;&gt;Nvdiffrast&lt;/a&gt;, supports:&lt;/li&gt; &#xA;   &lt;li&gt;Export to .obj, .ply, .glb&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Save &amp;amp; Load 3D file&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;.obj, .ply, .glb for 3D Mesh&lt;/li&gt; &#xA;   &lt;li&gt;.ply for 3DGS&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Switch Axis for 3DGS &amp;amp; 3D Mesh&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Since different algorithms likely use different coordinate system, so the ability to re-mapping the axis of coordinate is crucial for passing generated result between differnt nodes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/Configs/system.conf&#34;&gt;Customizable system config file&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Custom clients IP address&lt;/li&gt; &#xA;   &lt;li&gt;Add your huggingface user token&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Integrate &lt;a href=&#34;https://github.com/zjp-shadow/CharacterGen&#34;&gt;CharacterGen&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Improve 3DGS/Nerf to Mesh conversion algorithms:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Find better methods to converts 3DGS or Points Cloud to Mesh (Normal maps reconstruction maybe?)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Add &amp;amp; Improve a few best MVS algorithms (e.g 2DGS, etc.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Add camera pose estimation from raw multi-views images&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h4&gt;How to Contribute&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the project&lt;/li&gt; &#xA; &lt;li&gt;Make Improvements/Add New Features&lt;/li&gt; &#xA; &lt;li&gt;Creating a Pull Request to &lt;a href=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack/tree/dev&#34;&gt;dev branch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Project Structure&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/nodes.py&#34;&gt;nodes.py&lt;/a&gt;&lt;/strong&gt;: &lt;br&gt;Contains the interface code for all Comfy3D nodes (i.e. the nodes you can actually seen &amp;amp; use inside ComfyUI), you can add your new nodes here&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/Gen_3D_Modules&#34;&gt;Gen_3D_Modules&lt;/a&gt;&lt;/strong&gt;: &lt;br&gt;A folder that contains the code for all generative models/systems (e.g. multi-view diffusion models, 3D reconstruction models). New 3D generative modules should be added here&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/MVs_Algorithms&#34;&gt;MVs_Algorithms&lt;/a&gt;&lt;/strong&gt;: &lt;br&gt;A folder that contains the code for all multi-view stereo algorithms, i.e. algorighms (e.g. Gaussian Splatting, NeRF and FlexiCubes) that takes multi-view images and convert it to 3D representation (e.g. Gaussians, MLP or Mesh). New MVS algorithms should be added here&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/web&#34;&gt;web&lt;/a&gt;&lt;/strong&gt;: &lt;br&gt;A folder that contains the files &amp;amp; code (html, js, css) for all browser UI related things (e.g. the html layout, style and the core logics for preview 3D Mesh &amp;amp; Gaussians). New web UI should be added here&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/webserver&#34;&gt;webserver&lt;/a&gt;&lt;/strong&gt;: &lt;br&gt;A folder that contains the code for communicate with browser, i.e. deal with web client requests (e.g. Sending 3D Mesh to client when requested with certain url routes). New web server related functions should be added here&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/Configs&#34;&gt;Configs&lt;/a&gt;&lt;/strong&gt;: &lt;br&gt;A folder that contains different config files for different modules, new config should be added here, use a sub folder if there are more than one config to a single module (e.g. &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/Configs/Unique3D_configs&#34;&gt;Unique3D&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/Configs/CRM_configs&#34;&gt;CRM&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/Checkpoints&#34;&gt;Checkpoints&lt;/a&gt;&lt;/strong&gt;: &lt;br&gt;A folder that contains all the pre-trained model and some of the model architecture config files required by diffusers, New checkpoints if could be downloaded automatically by &lt;code&gt;Load_Diffusers Pipeline&lt;/code&gt; node, then it should be added here&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/install.py&#34;&gt;install.py&lt;/a&gt;&lt;/strong&gt;: &lt;br&gt;Main install script, will download &amp;amp; install &lt;a href=&#34;https://github.com/MrForExample/Comfy3D_Pre_Builds&#34;&gt;Pre-builds&lt;/a&gt; automatically according to your runtime environment, if it couldn&#39;t find corresponding Pre-builds, then &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Pre_Builds/_Build_Scripts/auto_build_all.py&#34;&gt;build script&lt;/a&gt; will start automatically, called by ComfyUI-Manager right after it installed the dependencies listed in &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; using pip &lt;br&gt;If the new modules your are trying to add needs some additional packages that cannot be simplly added into &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Pre_Builds/_Build_Scripts/build_config.yaml&#34;&gt;build_config.remote_packages&lt;/a&gt;, then you can try to add it by modify this script&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Pre_Builds&#34;&gt;_Pre_Builds&lt;/a&gt;&lt;/strong&gt;: &lt;br&gt;A folder that contains the files &amp;amp; code for build all required dependencies, if you want to pre-build some additional dependencies, then please check &lt;a href=&#34;https://raw.githubusercontent.com/MrForExample/ComfyUI-3D-Pack/main/_Pre_Builds/README.md&#34;&gt;_Pre_Builds/README.md&lt;/a&gt; for more informations&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenGL (Three.js, Blender) world &amp;amp; camera coordinate system: &lt;pre&gt;&lt;code&gt;    World            Camera        &#xA;  &#xA;    +y              up  target                                              &#xA;    |                |  /                                           &#xA;    |                | /                                           &#xA;    |______+x        |/______right                                      &#xA;    /                /         &#xA;   /                /          &#xA;  /                /           &#xA;+z               forward           &#xA;&#xA;z-axis is pointing towards you and is coming out of the screen&#xA;elevation: in (-90, 90), from +y to +x is (-90, 0)&#xA;azimuth: in (-180, 180), from +z to +x is (0, 90)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you encounter OpenGL errors (e.g., &lt;code&gt;[F glutil.cpp:338] eglInitialize() failed&lt;/code&gt;), then set &lt;code&gt;force_cuda_rasterize&lt;/code&gt; to true on corresponding node&lt;/li&gt; &#xA; &lt;li&gt;If after the installation, your ComfyUI get stucked at starting or running, you could following the instruction in following link to solve the problem: &lt;a href=&#34;https://github.com/lava-nc/lava-dl/discussions/211&#34;&gt;Code Hangs Indefinitely When Evaluating Neuron Models on GPU&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supporters&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/janusch_patas&#34;&gt;MrNeRF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>