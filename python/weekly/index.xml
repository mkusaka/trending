<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-04T02:06:35Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SevaSk/ecoute</title>
    <updated>2023-06-04T02:06:35Z</updated>
    <id>tag:github.com,2023-06-04:/SevaSk/ecoute</id>
    <link href="https://github.com/SevaSk/ecoute" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ecoute is a live transcription tool that provides real-time transcripts for both the user&#39;s microphone input (You) and the user&#39;s speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI&#39;s GPT-3.5 for the user to say based on the live transcription of the conversation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üéß Ecoute&lt;/h1&gt; &#xA;&lt;p&gt;Ecoute is a live transcription tool that provides real-time transcripts for both the user&#39;s microphone input (You) and the user&#39;s speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI&#39;s GPT-3.5 for the user to say based on the live transcription of the conversation.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SevaSk/ecoute/assets/50382291/8ac48927-8a26-49fd-80e9-48f980986208&#34;&gt;https://github.com/SevaSk/ecoute/assets/50382291/8ac48927-8a26-49fd-80e9-48f980986208&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ecoute is designed to help users in their conversations by providing live transcriptions and generating contextually relevant responses. By leveraging the power of OpenAI&#39;s GPT-3.5, Ecoute aims to make communication more efficient and enjoyable.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Follow these steps to set up and run Ecoute on your local machine.&lt;/p&gt; &#xA;&lt;h3&gt;üìã Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;=3.8.0&lt;/li&gt; &#xA; &lt;li&gt;An OpenAI API key&lt;/li&gt; &#xA; &lt;li&gt;Windows OS (Not tested on others)&lt;/li&gt; &#xA; &lt;li&gt;FFmpeg&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If FFmpeg is not installed in your system, you can follow the steps below to install it.&lt;/p&gt; &#xA;&lt;p&gt;First, you need to install Chocolatey, a package manager for Windows. Open your PowerShell as Administrator and run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&#39;https://community.chocolatey.org/install.ps1&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once Chocolatey is installed, you can install FFmpeg by running the following command in your PowerShell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;choco install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please ensure that you run these commands in a PowerShell window with administrator privileges. If you face any issues during the installation, you can visit the official Chocolatey and FFmpeg websites for troubleshooting.&lt;/p&gt; &#xA;&lt;h3&gt;üîß Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/SevaSk/ecoute&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the &lt;code&gt;ecoute&lt;/code&gt; folder:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd ecoute&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required packages:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;keys.py&lt;/code&gt; file in the ecoute directory and add your OpenAI API key:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Option 1: You can utilize a command on your command prompt. Run the following command, ensuring to replace &#34;API KEY&#34; with your actual OpenAI API key:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python -c &#34;with open(&#39;keys.py&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(&#39;OPENAI_API_KEY=\&#34;API KEY\&#34;&#39;)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Option 2: You can create the keys.py file manually. Open up your text editor of choice and enter the following content:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&#34;API KEY&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace &#34;API KEY&#34; with your actual OpenAI API key. Save this file as keys.py within the ecoute directory.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üé¨ Running Ecoute&lt;/h3&gt; &#xA;&lt;p&gt;Run the main script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a more better and faster version that also works with most languages, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Upon initiation, Ecoute will begin transcribing your microphone input and speaker output in real-time, generating a suggested response based on the conversation. Please note that it might take a few seconds for the system to warm up before the transcription becomes real-time.&lt;/p&gt; &#xA;&lt;p&gt;The --api flag will use the whisper api for transcriptions. This significantly enhances transcription speed and accuracy, and it works in most languages (rather than just English without the flag). It&#39;s expected to become the default option in future releases. However, keep in mind that using the Whisper API will consume more OpenAI credits than using the local model. This increased cost is attributed to the advanced features and capabilities that the Whisper API provides. Despite the additional expense, the substantial improvements in speed and transcription accuracy may make it a worthwhile investment for your use case.&lt;/p&gt; &#xA;&lt;h3&gt;‚ö†Ô∏è Limitations&lt;/h3&gt; &#xA;&lt;p&gt;While Ecoute provides real-time transcription and response suggestions, there are several known limitations to its functionality that you should be aware of:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Default Mic and Speaker:&lt;/strong&gt; Ecoute is currently configured to listen only to the default microphone and speaker set in your system. It will not detect sound from other devices or systems. If you wish to use a different mic or speaker, you will need to set it as your default device in your system settings.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Whisper Model&lt;/strong&gt;: If the --api flag is not used, we utilize the &#39;tiny&#39; version of the Whisper ASR model, due to its low resource consumption and fast response times. However, this model may not be as accurate as the larger models in transcribing certain types of speech, including accents or uncommon words.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language&lt;/strong&gt;: If you are not using the --api flag the Whisper model used in Ecoute is set to English. As a result, it may not accurately transcribe non-English languages or dialects. We are actively working to add multi-language support to future versions of the program.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/SevaSk/ecoute/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Feel free to open issues or submit pull requests to improve Ecoute.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PanQiWei/AutoGPTQ</title>
    <updated>2023-06-04T02:06:35Z</updated>
    <id>tag:github.com,2023-06-04:/PanQiWei/AutoGPTQ</id>
    <link href="https://github.com/PanQiWei/AutoGPTQ" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;AutoGPTQ&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/PanQiWei/AutoGPTQ.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/auto-gptq/&#34;&gt; &lt;img alt=&#34;PyPI - Downloads&#34; src=&#34;https://img.shields.io/pypi/dd/auto-gptq&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;b&gt;English&lt;/b&gt; | &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ/raw/main/README_zh.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;News or Update&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;To experience adapter training using &lt;code&gt;auto_gptq&lt;/code&gt; quantized model in advance, you can try &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ/tree/peft_integration&#34;&gt;this branch&lt;/a&gt; and discuss &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ/issues/103&#34;&gt;in here&lt;/a&gt;, examples are &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ/tree/peft_integration/examples/peft&#34;&gt;in here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023-05-25 - (In Progress) - Integrate with ü§ó peft to use gptq quantized model to train adapters, support LoRA, AdaLoRA, AdaptionPrompt, etc.&lt;/li&gt; &#xA; &lt;li&gt;2023-05-30 - (Update) - Support download/upload quantized model from/to ü§ó Hub.&lt;/li&gt; &#xA; &lt;li&gt;2023-05-27 - (Update) - Support quantization and inference for &lt;code&gt;gpt_bigcode&lt;/code&gt;, &lt;code&gt;codegen&lt;/code&gt; and &lt;code&gt;RefineWeb/RefineWebModel&lt;/code&gt;(falcon) model types.&lt;/li&gt; &#xA; &lt;li&gt;2023-05-04 - (Update) - Support using faster cuda kernel when &lt;code&gt;not desc_act or group_size == -1&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;For more histories please turn to &lt;a href=&#34;https://raw.githubusercontent.com/PanQiWei/AutoGPTQ/main/docs/NEWS_OR_UPDATE.md&#34;&gt;here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Performance Comparison&lt;/h2&gt; &#xA;&lt;h3&gt;Inference Speed&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The result is generated using &lt;a href=&#34;https://raw.githubusercontent.com/PanQiWei/AutoGPTQ/main/examples/benchmark/generation_speed.py&#34;&gt;this script&lt;/a&gt;, batch size of input is 1, decode strategy is beam search and enforce the model to generate 512 tokens, speed metric is tokens/s (the larger, the better).&lt;/p&gt; &#xA; &lt;p&gt;The quantized model is loaded using the setup that can gain the fastest inference speed.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;num_beams&lt;/th&gt; &#xA;   &lt;th&gt;fp16&lt;/th&gt; &#xA;   &lt;th&gt;gptq-int4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-7b&lt;/td&gt; &#xA;   &lt;td&gt;1xA100-40G&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;18.87&lt;/td&gt; &#xA;   &lt;td&gt;25.53&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-7b&lt;/td&gt; &#xA;   &lt;td&gt;1xA100-40G&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;68.79&lt;/td&gt; &#xA;   &lt;td&gt;91.30&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon 16b&lt;/td&gt; &#xA;   &lt;td&gt;1xA100-40G&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;12.48&lt;/td&gt; &#xA;   &lt;td&gt;15.25&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon 16b&lt;/td&gt; &#xA;   &lt;td&gt;1xA100-40G&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;42.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon 16b&lt;/td&gt; &#xA;   &lt;td&gt;2xA100-40G&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;06.83&lt;/td&gt; &#xA;   &lt;td&gt;06.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon 16b&lt;/td&gt; &#xA;   &lt;td&gt;2xA100-40G&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;13.10&lt;/td&gt; &#xA;   &lt;td&gt;10.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-j 6b&lt;/td&gt; &#xA;   &lt;td&gt;1xRTX3060-12G&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;29.55&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-j 6b&lt;/td&gt; &#xA;   &lt;td&gt;1xRTX3060-12G&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;47.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Perplexity&lt;/h3&gt; &#xA;&lt;p&gt;For perplexity comparison, you can turn to &lt;a href=&#34;https://github.com/qwopqwop200/GPTQ-for-LLaMa#result&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/qwopqwop200/GPTQ-for-LLaMa#gptq-vs-bitsandbytes&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Quick Installation&lt;/h3&gt; &#xA;&lt;p&gt;You can install the latest stable release of AutoGPTQ from pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install auto-gptq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start from v0.2.0, you can download pre-build wheel that satisfied your environment setup from each version&#39;s release assets and install it to skip building stage for the fastest installation speed. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# firstly, cd the directory where the wheel saved, then execute command below&#xA;pip install auto_gptq-0.2.0+cu118-cp310-cp310-linux_x86_64.whl # install v0.2.0 auto_gptq pre-build wheel for linux in an environment whose python=3.10 and cuda=11.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;disable cuda extensions&lt;/h4&gt; &#xA;&lt;p&gt;By default, cuda extensions will be installed when &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;cuda&lt;/code&gt; is already installed in your machine, if you don&#39;t want to use them, using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;BUILD_CUDA_EXT=0 pip install auto-gptq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And to make sure &lt;code&gt;autogptq_cuda&lt;/code&gt; is not ever in your virtual environment, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip uninstall autogptq_cuda -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;to support LLaMa model&lt;/h4&gt; &#xA;&lt;p&gt;For some people want to try LLaMa and whose &lt;code&gt;transformers&lt;/code&gt; version not meet the newest one that supports it, using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install auto-gptq[llama]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;to support triton speedup&lt;/h4&gt; &#xA;&lt;p&gt;To integrate with &lt;code&gt;triton&lt;/code&gt;, using:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;warning: currently triton only supports linux; 3-bit quantization is not supported when using triton&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install auto-gptq[triton]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install from source&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;click to see details&lt;/summary&gt; &#xA; &lt;p&gt;Clone the source code:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/PanQiWei/AutoGPTQ.git &amp;amp;&amp;amp; cd AutoGPTQ&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then, install from source:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Like quick installation, you can also set &lt;code&gt;BUILD_CUDA_EXT=0&lt;/code&gt; to disable pytorch extension building.&lt;/p&gt; &#xA; &lt;p&gt;Use &lt;code&gt;.[llama]&lt;/code&gt; if you want to try LLaMa model.&lt;/p&gt; &#xA; &lt;p&gt;Use &lt;code&gt;.[triton]&lt;/code&gt; if you want to integrate with triton and it&#39;s available on your operating system.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Quick Tour&lt;/h2&gt; &#xA;&lt;h3&gt;Quantization and Inference&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;warning: this is just a showcase of the usage of basic apis in AutoGPTQ, which uses only one sample to quantize a much small model, quality of quantized model using such little samples may not good.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Below is an example for the simplest use of &lt;code&gt;auto_gptq&lt;/code&gt; to quantize a model and inference after quantization:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, TextGenerationPipeline&#xA;from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig&#xA;import logging&#xA;&#xA;logging.basicConfig(&#xA;    format=&#34;%(asctime)s %(levelname)s [%(name)s] %(message)s&#34;, level=logging.INFO, datefmt=&#34;%Y-%m-%d %H:%M:%S&#34;&#xA;)&#xA;&#xA;pretrained_model_dir = &#34;facebook/opt-125m&#34;&#xA;quantized_model_dir = &#34;opt-125m-4bit&#34;&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)&#xA;examples = [&#xA;    tokenizer(&#xA;        &#34;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&#34;&#xA;    )&#xA;]&#xA;&#xA;quantize_config = BaseQuantizeConfig(&#xA;    bits=4,  # quantize model to 4-bit&#xA;    group_size=128,  # it is recommended to set the value to 128&#xA;    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad &#xA;)&#xA;&#xA;# load un-quantized model, by default, the model will always be loaded into CPU memory&#xA;model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)&#xA;&#xA;# quantize model, the examples should be list of dict whose keys can only be &#34;input_ids&#34; and &#34;attention_mask&#34;&#xA;model.quantize(examples)&#xA;&#xA;# save quantized model&#xA;model.save_quantized(quantized_model_dir)&#xA;&#xA;# save quantized model using safetensors&#xA;model.save_quantized(quantized_model_dir, use_safetensors=True)&#xA;&#xA;# push quantized model to Hugging Face Hub. &#xA;# to use use_auth_token=True, Login first via huggingface-cli login.&#xA;# or pass explcit token with: use_auth_token=&#34;hf_xxxxxxx&#34;&#xA;# (uncomment the following three lines to enable this feature)&#xA;# repo_id = f&#34;YourUserName/{quantized_model_dir}&#34;&#xA;# commit_message = f&#34;AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}&#34;&#xA;# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)&#xA;&#xA;# alternatively you can save and push at the same time&#xA;# (uncomment the following three lines to enable this feature)&#xA;# repo_id = f&#34;YourUserName/{quantized_model_dir}&#34;&#xA;# commit_message = f&#34;AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}&#34;&#xA;# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)&#xA;&#xA;# load quantized model to the first GPU&#xA;model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=&#34;cuda:0&#34;)&#xA;&#xA;# download quantized model from Hugging Face Hub and load to the first GPU&#xA;# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=&#34;cuda:0&#34;, use_safetensors=True, use_triton=False)&#xA;&#xA;# inference with model.generate&#xA;print(tokenizer.decode(model.generate(**tokenizer(&#34;auto_gptq is&#34;, return_tensors=&#34;pt&#34;).to(model.device))[0]))&#xA;&#xA;# or you can also use pipeline&#xA;pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)&#xA;print(pipeline(&#34;auto-gptq is&#34;)[0][&#34;generated_text&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more advanced features of model quantization, please reference to &lt;a href=&#34;https://raw.githubusercontent.com/PanQiWei/AutoGPTQ/main/examples/quantization/quant_with_alpaca.py&#34;&gt;this script&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Customize Model&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Below is an example to extend `auto_gptq` to support `OPT` model, as you will see, it&#39;s very easy:&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from auto_gptq.modeling import BaseGPTQForCausalLM&#xA;&#xA;&#xA;class OPTGPTQForCausalLM(BaseGPTQForCausalLM):&#xA;    # chained attribute name of transformer layer block&#xA;    layers_block_name = &#34;model.decoder.layers&#34;&#xA;    # chained attribute names of other nn modules that in the same level as the transformer layer block&#xA;    outside_layer_modules = [&#xA;        &#34;model.decoder.embed_tokens&#34;, &#34;model.decoder.embed_positions&#34;, &#34;model.decoder.project_out&#34;,&#xA;        &#34;model.decoder.project_in&#34;, &#34;model.decoder.final_layer_norm&#34;&#xA;    ]&#xA;    # chained attribute names of linear layers in transformer layer module&#xA;    # normally, there are four sub lists, for each one the modules in it can be seen as one operation, &#xA;    # and the order should be the order when they are truly executed, in this case (and usually in most cases), &#xA;    # they are: attention q_k_v projection, attention output projection, MLP project input, MLP project output&#xA;    inside_layer_modules = [&#xA;        [&#34;self_attn.k_proj&#34;, &#34;self_attn.v_proj&#34;, &#34;self_attn.q_proj&#34;],&#xA;        [&#34;self_attn.out_proj&#34;],&#xA;        [&#34;fc1&#34;],&#xA;        [&#34;fc2&#34;]&#xA;    ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;After this, you can use &lt;code&gt;OPTGPTQForCausalLM.from_pretrained&lt;/code&gt; and other methods as shown in Basic.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Evaluation on Downstream Tasks&lt;/h3&gt; &#xA;&lt;p&gt;You can use tasks defined in &lt;code&gt;auto_gptq.eval_tasks&lt;/code&gt; to evaluate model&#39;s performance on specific down-stream task before and after quantization.&lt;/p&gt; &#xA;&lt;p&gt;The predefined tasks support all causal-language-models implemented in &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;ü§ó transformers&lt;/a&gt; and in this project.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Below is an example to evaluate `EleutherAI/gpt-j-6b` on sequence-classification task using `cardiffnlp/tweet_sentiment_multilingual` dataset:&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from functools import partial&#xA;&#xA;import datasets&#xA;from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig&#xA;&#xA;from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig&#xA;from auto_gptq.eval_tasks import SequenceClassificationTask&#xA;&#xA;&#xA;MODEL = &#34;EleutherAI/gpt-j-6b&#34;&#xA;DATASET = &#34;cardiffnlp/tweet_sentiment_multilingual&#34;&#xA;TEMPLATE = &#34;Question:What&#39;s the sentiment of the given text? Choices are {labels}.\nText: {text}\nAnswer:&#34;&#xA;ID2LABEL = {&#xA;    0: &#34;negative&#34;,&#xA;    1: &#34;neutral&#34;,&#xA;    2: &#34;positive&#34;&#xA;}&#xA;LABELS = list(ID2LABEL.values())&#xA;&#xA;&#xA;def ds_refactor_fn(samples):&#xA;    text_data = samples[&#34;text&#34;]&#xA;    label_data = samples[&#34;label&#34;]&#xA;&#xA;    new_samples = {&#34;prompt&#34;: [], &#34;label&#34;: []}&#xA;    for text, label in zip(text_data, label_data):&#xA;        prompt = TEMPLATE.format(labels=LABELS, text=text)&#xA;        new_samples[&#34;prompt&#34;].append(prompt)&#xA;        new_samples[&#34;label&#34;].append(ID2LABEL[label])&#xA;&#xA;    return new_samples&#xA;&#xA;&#xA;#  model = AutoModelForCausalLM.from_pretrained(MODEL).eval().half().to(&#34;cuda:0&#34;)&#xA;model = AutoGPTQForCausalLM.from_pretrained(MODEL, BaseQuantizeConfig())&#xA;tokenizer = AutoTokenizer.from_pretrained(MODEL)&#xA;&#xA;task = SequenceClassificationTask(&#xA;        model=model,&#xA;        tokenizer=tokenizer,&#xA;        classes=LABELS,&#xA;        data_name_or_path=DATASET,&#xA;        prompt_col_name=&#34;prompt&#34;,&#xA;        label_col_name=&#34;label&#34;,&#xA;        **{&#xA;            &#34;num_samples&#34;: 1000,  # how many samples will be sampled to evaluation&#xA;            &#34;sample_max_len&#34;: 1024,  # max tokens for each sample&#xA;            &#34;block_max_len&#34;: 2048,  # max tokens for each data block&#xA;            # function to load dataset, one must only accept data_name_or_path as input &#xA;            # and return datasets.Dataset&#xA;            &#34;load_fn&#34;: partial(datasets.load_dataset, name=&#34;english&#34;),  &#xA;            # function to preprocess dataset, which is used for datasets.Dataset.map, &#xA;            # must return Dict[str, list] with only two keys: [prompt_col_name, label_col_name]&#xA;            &#34;preprocess_fn&#34;: ds_refactor_fn,  &#xA;            # truncate label when sample&#39;s length exceed sample_max_len&#xA;            &#34;truncate_prompt&#34;: False  &#xA;        }&#xA;    )&#xA;&#xA;# note that max_new_tokens will be automatically specified internally based on given classes&#xA;print(task.run())&#xA;&#xA;# self-consistency&#xA;print(&#xA;    task.run(&#xA;        generation_config=GenerationConfig(&#xA;            num_beams=3,&#xA;            num_return_sequences=3,&#xA;            do_sample=True&#xA;        )&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Learn More&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PanQiWei/AutoGPTQ/main/docs/tutorial&#34;&gt;tutorials&lt;/a&gt; provide step-by-step guidance to integrate &lt;code&gt;auto_gptq&lt;/code&gt; with your own project and some best practice principles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PanQiWei/AutoGPTQ/main/examples/README.md&#34;&gt;examples&lt;/a&gt; provide plenty of example scripts to use &lt;code&gt;auto_gptq&lt;/code&gt; in different ways.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;you can use &lt;code&gt;model.config.model_type&lt;/code&gt; to compare with the table below to check whether the model you use is supported by &lt;code&gt;auto_gptq&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;for example, model_type of &lt;code&gt;WizardLM&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; and &lt;code&gt;gpt4all&lt;/code&gt; are all &lt;code&gt;llama&lt;/code&gt;, hence they are all supported by &lt;code&gt;auto_gptq&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model type&lt;/th&gt; &#xA;   &lt;th&gt;quantization&lt;/th&gt; &#xA;   &lt;th&gt;inference&lt;/th&gt; &#xA;   &lt;th&gt;peft-lora&lt;/th&gt; &#xA;   &lt;th&gt;peft-adaption_prompt&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bloom&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt_neox&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gptj&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;opt&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt_bigcode&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;codegen&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;falcon(RefinedWebModel/RefinedWeb)&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Evaluation Tasks&lt;/h2&gt; &#xA;&lt;p&gt;Currently, &lt;code&gt;auto_gptq&lt;/code&gt; supports: &lt;code&gt;LanguageModelingTask&lt;/code&gt;, &lt;code&gt;SequenceClassificationTask&lt;/code&gt; and &lt;code&gt;TextSummarizationTask&lt;/code&gt;; more Tasks will come soon!&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Specially thanks &lt;strong&gt;Elias Frantar&lt;/strong&gt;, &lt;strong&gt;Saleh Ashkboos&lt;/strong&gt;, &lt;strong&gt;Torsten Hoefler&lt;/strong&gt; and &lt;strong&gt;Dan Alistarh&lt;/strong&gt; for proposing &lt;strong&gt;GPTQ&lt;/strong&gt; algorithm and open source the &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;code&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Specially thanks &lt;strong&gt;qwopqwop200&lt;/strong&gt;, for code in this project that relevant to quantization are mainly referenced from &lt;a href=&#34;https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda&#34;&gt;GPTQ-for-LLaMa&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#PanQiWei/AutoGPTQ&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=PanQiwei/AutoGPTQ&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kyegomez/tree-of-thoughts</title>
    <updated>2023-06-04T02:06:35Z</updated>
    <id>tag:github.com,2023-06-04:/kyegomez/tree-of-thoughts</id>
    <link href="https://github.com/kyegomez/tree-of-thoughts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Plug in and Play Implementation of Tree of Thoughts: Deliberate Problem Solving with Large Language Models that Elevates Model Reasoning by atleast 70%&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tree of Thoughts üå≥üå≤üå¥üåøüçÉ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/tree-of-thoughts/main/tree-of-thoughts.png&#34; alt=&#34;tree of thoughts banner&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.10601.pdf&#34;&gt;Paper link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tree of Thoughts (ToT) is an all-new powerful and flexible algorithm that advances model reasoning by a whopping 70%. This is an plug in and play verision, connect your own models and enjoy superintelligence!&lt;/p&gt; &#xA;&lt;p&gt;Share this repository by clicking on the following buttons üòä&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/intent/tweet?text=Check%20out%20this%20amazing%20project%20on%20improving%20AI%20reasoning%20-%20Tree%20of%20Thoughts!%20https://github.com/kyegomez/tree-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?style=social&amp;amp;url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Share-LinkedIn-blue?style=social&amp;amp;logo=linkedin&#34; alt=&#34;LinkedIn&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Join Agora, Creators United&lt;/h1&gt; &#xA;&lt;p&gt;This implementation of Tree of Thoughts is brought to you by Agora, Agora advances Humanity with open source SOTA Multi-Modality AI research!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qUtxnK2NMf&#34;&gt;Join our Discord and contribute to this project&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Basic Prompts:&lt;/h1&gt; &#xA;&lt;p&gt;No complex implementations, just pass in one of these prompts to your model: head over to &lt;code&gt;prompts.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&#39;Three experts with exceptional logical thinking skills are collaboratively answering a question using a tree of thoughts method. Each expert will share their thought process in detail, taking into account the previous thoughts of others and admitting any errors. They will iteratively refine and expand upon each other&#39;s ideas, giving credit where it&#39;s due. The process continues until a conclusive answer is found. Organize the entire response in a markdown table format. The question is...&#39;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Set Openai key in an environment file,&lt;/p&gt; &#xA;&lt;p&gt;first create an file called: &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then get your openai key and input it inside the &#39;&#39; as &lt;code&gt;OPENAI_API_KEY=&#39;SK-YOUR KEY&#39;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Method1&lt;/h2&gt; &#xA;&lt;p&gt;Clone this repository with&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;git clone https://github.com/kyegomez/tree-of-thoughts&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd tree-of-thoughts&#xA;python3 -m pip install -r requirements.txt&#xA;cd tree_of_thoughts&#xA;python3 treeofthoughts.py --problem &#34;design an new transportation system for an all-new city&#34; --search_algorithm=&#34;BFS&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add &lt;code&gt; OPENAI_API_KEY=&#39;API KEY&#39;&lt;/code&gt; in the .env!&lt;/p&gt; &#xA;&lt;p&gt;!!!! For much improved performance provide custom few prompt shots in the generate thoughts and generate states! !!!!!&lt;/p&gt; &#xA;&lt;p&gt;And in the &lt;code&gt;examples&lt;/code&gt; folder we have other examples for huggingface transformers + hugginggface pipelines&lt;/p&gt; &#xA;&lt;h2&gt;Method2&lt;/h2&gt; &#xA;&lt;p&gt;or:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install tree-of-thoughts &lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Create a Python script (e.g., example.py) and import the necessary classes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from tree_of_thoughts import OptimizedOpenAILanguageModel&#xA;from tree_of_thoughts import TreeofThoughts&#xA;&#xA;&#xA;api_key = os.getenv(&#34;OPENAI_API_KEY&#34;)&#xA;model = OptimizedOpenAILanguageModel(api_key=api_key) # api_model=&#34;gpt4&#34; # for higher performance base model is not smart&#xA;&#xA;&#xA;#choose search algorithm(&#39;BFS&#39; or &#39;DFS&#39;)&#xA;search_algorithm = &#34;BFS&#34;&#xA;&#xA;#cot or propose&#xA;strategy=&#34;cot&#34;&#xA;&#xA;# value or vote&#xA;evaluation_strategy = &#34;value&#34;&#xA;&#xA;#initialize the class&#xA;tree_of_thoughts = TreeofThoughts(model, search_algorithm)&#xA;&#xA;#enter an problem if you want!&#xA;input_problem = &#34;use 4 numbers and basic arithmetic operations (+-*/) to obtain 24&#34; #note for superior intelligent responses you&#39;ll have to be more explicit in your prompt and select a better model&#xA;    &#xA;&#xA;num_thoughts = 2&#xA;max_steps= 3&#xA;max_states = 5&#xA;value_threshold= 0.5&#xA;&#xA;#call the solve emthod with the input problem and other params&#xA;&#xA;solution = tree_of_thoughts.solve(input_problem, &#xA;    num_thoughts=num_thoughts,&#xA;    max_steps=max_states,&#xA;    max_states=5,&#xA;    value_threshold=value_threshold,&#xA;    )&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or Integrate your own custom language model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;class CustomLanguageModel(AbstractLanguageModel):&#xA;    def __init__(self, model):&#xA;        self.model = model&#xA;&#xA;    def generate_thoughts(self, state, k):&#xA;        #implement the thought generation logic using self.model&#xA;        pass&#xA;&#xA;    def evaluate_states(self, states):&#xA;        #implement state evaluation logic using self.model&#xA;        pass&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the example script&lt;/p&gt; &#xA;&lt;h2&gt;üåü Features:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;General problem-solving framework for language models&lt;/li&gt; &#xA; &lt;li&gt;Supports both breadth-first search (BFS) and depth-first search (DFS) algorithms&lt;/li&gt; &#xA; &lt;li&gt;Easy integration with popular language models like OpenAI and Hugging Face&lt;/li&gt; &#xA; &lt;li&gt;Extensible and adaptable to different problem properties and resource constraints&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Algorithmic Pseudocode&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Define the thought decomposition based on the problem properties.&lt;/li&gt; &#xA; &lt;li&gt;Create a thought generator function G(pŒ∏, s, k) with two strategies: a. Sample i.i.d. thoughts from a CoT prompt. b. Propose thoughts sequentially using a &#34;propose prompt&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Create a state evaluator function V(pŒ∏, S) with two strategies: a. Value each state independently. b. Vote across states.&lt;/li&gt; &#xA; &lt;li&gt;Choose a search algorithm (BFS or DFS) based on the tree structure.&lt;/li&gt; &#xA; &lt;li&gt;Implement the chosen search algorithm.&lt;/li&gt; &#xA; &lt;li&gt;Execute the chosen search algorithm with the input problem, thought generator, state evaluator, and other required parameters.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Tree of Thoughts Class&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class TreeofThoughts:&#xA;    &#xA;    def __init__(self, model, search_algorithm):&#xA;        self.model = model&#xA;        self.search_algorithm = search_algorithm&#xA;&#xA;    def solve(self, x, k, T, b, vth):&#xA;        if self.search_algorithm == &#39;BFS&#39;:&#xA;            return self.tot_bfs(x, k, T, b)&#xA;        elif self.search_algorithm == &#39;DFS&#39;:&#xA;            return self.tot_dfs(x, k, T, vth)&#xA;        else:&#xA;            raise ValueError(&#34;Invalid search algorithm. Choose &#39;BFS&#39; or &#39;DFS&#39;.&#34;)&#xA;&#xA;    def tot_bfs(self, x, k, T, b):&#xA;        S0 = {x}&#xA;        for t in range(1, T + 1):&#xA;            S0_t = {(*s, z) for s in S0 for z in self.model.generate_thoughts(s, k)}&#xA;            Vt = self.model.evaluate_states(S0_t)&#xA;            St = sorted(S0_t, key=lambda s: Vt[s], reverse=True)[:b]&#xA;            S0 = set(St)&#xA;        return self.model.generate_thoughts(max(St, key=lambda s: Vt[s]), 1)&#xA;&#xA;    def tot_dfs(self, x, k, T, vth):&#xA;        output = []&#xA;&#xA;        def dfs(s, t):&#xA;            if t &amp;gt; T:&#xA;                output.append(self.model.generate_thoughts(s, 1))&#xA;                return&#xA;            for s_prime in sorted(self.model.generate_thoughts(s, k)):&#xA;                if self.model.evaluate_states({s_prime})[s_prime] &amp;gt; vth:&#xA;                    dfs((*s, s_prime), t + 1)&#xA;&#xA;        dfs(x, 1)&#xA;        return output&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;h3&gt;OpenAI API&lt;/h3&gt; &#xA;&lt;p&gt;To use Tree of Thoughts with OpenAI&#39;s API, create a custom model class that inherits from &lt;code&gt;AbstractLanguageModel&lt;/code&gt; and implements the required methods using OpenAI&#39;s API. Then, create an instance of the &lt;code&gt;TreeOfThoughts&lt;/code&gt; class with the custom model and the desired search algorithm (&#39;BFS&#39; or &#39;DFS&#39;).&lt;/p&gt; &#xA;&lt;h3&gt;Hugging Face Transformers&lt;/h3&gt; &#xA;&lt;p&gt;To run huggingface transformers with Tree of Thoughts&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/kyegomez/tree-of-thoughts&#xA;cd tree-of-thoughts&#xA;python3 huggingfaceExample.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tree_of_thoughts import HuggingLanguageModel&#xA;&#xA;model_name=&#34;gpt2&#34;&#xA;model_tokenizer=&#34;your tokenizer&#34;&#xA;&#xA;huggingface_model = HuggingLanguageModel(model_name, model_tokenizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class HuggingLanguageModel(AbstractLanguageModel):&#xA;    def __init__(self, model_name):&#xA;        self.model = AutoModelForCausalLM.from_pretrained(model_name)&#xA;        self.tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;&#xA;    def generate_thoughts(self, state, k):&#xA;        state_text = &#39; &#39;.join(state)&#xA;        prompt = f&#34;Given the current state of reasoning: &#39;{state_text}&#39;, generate {k} coherent thoughts to achieve the reasoning process:&#34;&#xA;&#xA;        inputs = self.tokenizer(prompt, return_tensors=&#34;pt&#34;)&#xA;        outputs = self.model.generate(**inputs, num_return_sequences=k)&#xA;        thoughts = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]&#xA;&#xA;        return thoughts&#xA;&#xA;    def evaluate_states(self, states, initial_prompt):&#xA;        state_values = {}&#xA;        for state in states:&#xA;            state_text = &#39; &#39;.join(state)&#xA;            prompt = f&#34;Given the current state of reasoning: &#39;{state_text}&#39;, pessimitically evaluate its value as a float between 0 and 1 based on it&#39;s potential to achieve {initial_prompt}&#34;&#xA;&#xA;            inputs = self.tokenizer(prompt, return_tensors=&#34;pt&#34;)&#xA;            outputs = self.model.generate(**inputs, num_return_sequences=1)&#xA;            value_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)&#xA;&#xA;            try:&#xA;                value = float(value_text)&#xA;            except ValueError:&#xA;                value = 0  # Assign a default value if the conversion fails&#xA;&#xA;            state_values[state] = value&#xA;&#xA;        return state_values&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This algorithm is still infant yet it&#39;s potential remains unimaginable, let&#39;s advance the reasoning of AI&#39;s together under this banner.&lt;/p&gt; &#xA;&lt;h1&gt;Share With Your Network&lt;/h1&gt; &#xA;&lt;p&gt;You can easily share this repository by clicking on the following buttons:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/intent/tweet?text=Check%20out%20this%20amazing%20project%20on%20improving%20AI%20reasoning%20-%20Tree%20of%20Thoughts!%20https://github.com/kyegomez/tree-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?style=social&amp;amp;url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Share-LinkedIn-blue?style=social&amp;amp;logo=linkedin&#34; alt=&#34;LinkedIn&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For Instagram, while it doesn&#39;t directly support sharing of web links, you can share the screenshot of our project and the link in your caption or bio. You can download the project screenshot by clicking the image below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kyegomez/tree-of-thoughts/raw/main/tree-of-thoughts.jpeg&#34;&gt;&lt;img src=&#34;https://github.com/kyegomez/tree-of-thoughts/raw/main/tree-of-thoughts.jpeg&#34; alt=&#34;Tree of Thoughts&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We greatly appreciate any help in spreading the word about our project. Thank you for your support!&lt;/p&gt; &#xA;&lt;h1&gt;Roadmap:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Resilient Prompting: Teach model how to think rather than what to think.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add pruning treshold management for precise bad state cutoff&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Evaluating each thought as soon as thought generated then evaluating an chain of thoughts or the state of thoughts by averaging out the values of each thought evaluation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add Traversal method, which &#34;will incapsulate the run of either dfs or bfs under the hood so that the issue of different args is solved from @ivanzhovannik&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add Delay between generate solutions and generate values&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Dynamic and adaptive parameters, like max steps, num_thoughts, max_states and value threshold that shift depending on the complexity of the user objective.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add Rejected reasoning metadata (thought, state, reasoning_on_state) into generate solutions&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;any other ideas? Please pr request this algorithm is very infant and it&#39;s potential is limitless&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Chain of Thought Hub Evaluation tests!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation:&lt;/h1&gt; &#xA;&lt;h2&gt;input_problem (str):&lt;/h2&gt; &#xA;&lt;p&gt;The initial problem statement or prompt for which the Tree of Thoughts algorithm will generate a solution.&lt;/p&gt; &#xA;&lt;h2&gt;num_thoughts (int, default=5):&lt;/h2&gt; &#xA;&lt;p&gt;The number of thoughts to generate at each state. A higher value of k will result in more thoughts being generated, potentially leading to a more diverse set of solutions. However, increasing k may also increase the computational complexity and time required to find a solution.&lt;/p&gt; &#xA;&lt;h2&gt;max_steps (int, default=3):&lt;/h2&gt; &#xA;&lt;p&gt;The maximum depth of the search tree. A higher value of T allows the algorithm to explore deeper states, potentially leading to better solutions. However, increasing T may also increase the computational complexity and time required to find a solution.&lt;/p&gt; &#xA;&lt;h2&gt;max_states (int, default=5):&lt;/h2&gt; &#xA;&lt;p&gt;The branching factor of the search tree, which determines the maximum number of child nodes for each parent node. A higher value of b allows the algorithm to explore more states, potentially leading to better solutions. However, increasing b may also increase the computational complexity and time required to find a solution.&lt;/p&gt; &#xA;&lt;h2&gt;value_threshold (float, default=0.5):&lt;/h2&gt; &#xA;&lt;p&gt;The value threshold for pruning states. States with a value below this threshold will be discarded, reducing the search space. A higher value of vth will result in a more aggressive pruning strategy, potentially speeding up the search process. However, setting vth too high may cause the algorithm to discard promising states, leading to suboptimal solutions.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to: Shunyu Yao Princeton University, Dian Yu Google DeepMind, Jeffrey Zhao, Google DeepMind, Izhak Shafran Google DeepMind, Thomas L. Griffiths, Princeton University, Yuan Cao Google DeepMind, Karthik Narasimha, Princeton University for sharing this amazing work with the world!&lt;/p&gt; &#xA;&lt;p&gt;And, thanks to Phil Wang or Lucidrains for inspiring me to devote myself to open source AI Research&lt;/p&gt;</summary>
  </entry>
</feed>