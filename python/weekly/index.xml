<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-04T02:02:29Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>keras-team/keras</title>
    <updated>2022-06-04T02:02:29Z</updated>
    <id>tag:github.com,2022-06-04:/keras-team/keras</id>
    <link href="https://github.com/keras-team/keras" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deep Learning for humans&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Keras: Deep Learning for humans&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png&#34; alt=&#34;Keras logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository hosts the development of the Keras library. Read the documentation at &lt;a href=&#34;https://keras.io/&#34;&gt;keras.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About Keras&lt;/h2&gt; &#xA;&lt;p&gt;Keras is a deep learning API written in Python, running on top of the machine learning platform &lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;TensorFlow&lt;/a&gt;. It was developed with a focus on enabling fast experimentation. &lt;em&gt;Being able to go from idea to result as fast as possible is key to doing good research.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Keras is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt; -- but not simplistic. Keras reduces developer &lt;em&gt;cognitive load&lt;/em&gt; to free you to focus on the parts of the problem that really matter.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt; -- Keras adopts the principle of &lt;em&gt;progressive disclosure of complexity&lt;/em&gt;: simple workflows should be quick and easy, while arbitrarily advanced workflows should be &lt;em&gt;possible&lt;/em&gt; via a clear path that builds upon what you&#39;ve already learned.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Powerful&lt;/strong&gt; -- Keras provides industry-strength performance and scalability: it is used by organizations and companies including NASA, YouTube, and Waymo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Keras &amp;amp; TensorFlow 2&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow 2&lt;/a&gt; is an end-to-end, open-source machine learning platform. You can think of it as an infrastructure layer for &lt;a href=&#34;https://en.wikipedia.org/wiki/Differentiable_programming&#34;&gt;differentiable programming&lt;/a&gt;. It combines four key abilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Efficiently executing low-level tensor operations on CPU, GPU, or TPU.&lt;/li&gt; &#xA; &lt;li&gt;Computing the gradient of arbitrary differentiable expressions.&lt;/li&gt; &#xA; &lt;li&gt;Scaling computation to many devices, such as clusters of hundreds of GPUs.&lt;/li&gt; &#xA; &lt;li&gt;Exporting programs (&#34;graphs&#34;) to external runtimes such as servers, browsers, mobile and embedded devices.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Keras is the high-level API of TensorFlow 2: an approachable, highly-productive interface for solving machine learning problems, with a focus on modern deep learning. It provides essential abstractions and building blocks for developing and shipping machine learning solutions with high iteration velocity.&lt;/p&gt; &#xA;&lt;p&gt;Keras empowers engineers and researchers to take full advantage of the scalability and cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs, and you can export your Keras models to run in the browser or on a mobile device.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;First contact with Keras&lt;/h2&gt; &#xA;&lt;p&gt;The core data structures of Keras are &lt;strong&gt;layers&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt;. The simplest type of model is the &lt;a href=&#34;https://raw.githubusercontent.com/keras-team/keras/master/guides/sequential_model/&#34;&gt;&lt;code&gt;Sequential&lt;/code&gt; model&lt;/a&gt;, a linear stack of layers. For more complex architectures, you should use the &lt;a href=&#34;https://raw.githubusercontent.com/keras-team/keras/master/guides/functional_api/&#34;&gt;Keras functional API&lt;/a&gt;, which allows to build arbitrary graphs of layers, or &lt;a href=&#34;https://raw.githubusercontent.com/keras-team/keras/master/guides/making_new_layers_and_models_via_subclassing/&#34;&gt;write models entirely from scratch via subclasssing&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here is the &lt;code&gt;Sequential&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.models import Sequential&#xA;&#xA;model = Sequential()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Stacking layers is as easy as &lt;code&gt;.add()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.layers import Dense&#xA;&#xA;model.add(Dense(units=64, activation=&#39;relu&#39;))&#xA;model.add(Dense(units=10, activation=&#39;softmax&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once your model looks good, configure its learning process with &lt;code&gt;.compile()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.compile(loss=&#39;categorical_crossentropy&#39;,&#xA;              optimizer=&#39;sgd&#39;,&#xA;              metrics=[&#39;accuracy&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.compile(loss=tf.keras.losses.categorical_crossentropy,&#xA;              optimizer=tf.keras.optimizers.SGD(&#xA;                  learning_rate=0.01, momentum=0.9, nesterov=True))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now iterate on your training data in batches:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# x_train and y_train are Numpy arrays.&#xA;model.fit(x_train, y_train, epochs=5, batch_size=32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Evaluate your test loss and metrics in one line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or generate predictions on new data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;classes = model.predict(x_test, batch_size=128)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;What you just saw is the most elementary way to use Keras.&lt;/p&gt; &#xA;&lt;p&gt;However, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas. Keras follows the principle of &lt;strong&gt;progressive disclosure of complexity&lt;/strong&gt;: it makes it easy to get started, yet it makes it possible to handle arbitrarily advanced use cases, only requiring incremental learning at each step.&lt;/p&gt; &#xA;&lt;p&gt;In much the same way that you were able to train &amp;amp; evaluate a simple neural network above in a few lines, you can use Keras to quickly develop new training procedures or exotic model architectures. Here&#39;s a low-level training loop example, combining Keras functionality with the TensorFlow &lt;code&gt;GradientTape&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf&#xA;&#xA;# Prepare an optimizer.&#xA;optimizer = tf.keras.optimizers.Adam()&#xA;# Prepare a loss function.&#xA;loss_fn = tf.keras.losses.kl_divergence&#xA;&#xA;# Iterate over the batches of a dataset.&#xA;for inputs, targets in dataset:&#xA;    # Open a GradientTape.&#xA;    with tf.GradientTape() as tape:&#xA;        # Forward pass.&#xA;        predictions = model(inputs)&#xA;        # Compute the loss value for this batch.&#xA;        loss_value = loss_fn(targets, predictions)&#xA;&#xA;    # Get gradients of loss wrt the weights.&#xA;    gradients = tape.gradient(loss_value, model.trainable_weights)&#xA;    # Update the weights of the model.&#xA;    optimizer.apply_gradients(zip(gradients, model.trainable_weights))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more in-depth tutorials about Keras, you can check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://keras.io/getting_started/intro_to_keras_for_engineers/&#34;&gt;Introduction to Keras for engineers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://keras.io/getting_started/intro_to_keras_for_researchers/&#34;&gt;Introduction to Keras for researchers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://keras.io/guides/&#34;&gt;Developer guides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://keras.io/getting_started/learning_resources/&#34;&gt;Other learning resources&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Keras comes packaged with TensorFlow 2 as &lt;code&gt;tensorflow.keras&lt;/code&gt;. To start using Keras, simply &lt;a href=&#34;https://www.tensorflow.org/install&#34;&gt;install TensorFlow 2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Release and compatibility&lt;/h2&gt; &#xA;&lt;p&gt;Keras has &lt;strong&gt;nightly releases&lt;/strong&gt; (&lt;code&gt;keras-nightly&lt;/code&gt; on PyPI) and &lt;strong&gt;stable releases&lt;/strong&gt; (&lt;code&gt;keras&lt;/code&gt; on PyPI). The nightly Keras releases are usually compatible with the corresponding version of the &lt;code&gt;tf-nightly&lt;/code&gt; releases (e.g. &lt;code&gt;keras-nightly==2.7.0.dev2021100607&lt;/code&gt; should be used with &lt;code&gt;tf-nightly==2.7.0.dev2021100607&lt;/code&gt;). We don&#39;t maintain backward compatibility for nightly releases. For stable releases, each Keras version maps to a specific stable version of TensorFlow.&lt;/p&gt; &#xA;&lt;p&gt;The table below shows the compatibility version mapping between TensorFlow versions and Keras versions.&lt;/p&gt; &#xA;&lt;p&gt;All the release branches can be found on &lt;a href=&#34;https://github.com/keras-team/keras/releases&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All the release binaries can be found on &lt;a href=&#34;https://pypi.org/project/keras/#history&#34;&gt;Pypi&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Keras release&lt;/th&gt; &#xA;   &lt;th&gt;Note&lt;/th&gt; &#xA;   &lt;th&gt;Compatible Tensorflow version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/keras-team/keras/releases/tag/2.4.0&#34;&gt;2.4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Last stable release of multi-backend Keras&lt;/td&gt; &#xA;   &lt;td&gt;&amp;lt; 2.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5-pre&lt;/td&gt; &#xA;   &lt;td&gt;Pre-release (not formal) for standalone Keras repo&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 2.5 &amp;lt; 2.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/keras-team/keras/releases/tag/v2.6.0&#34;&gt;2.6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;First formal release of standalone Keras.&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 2.6 &amp;lt; 2.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0&#34;&gt;2.7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(Upcoming release)&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 2.7 &amp;lt; 2.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nightly&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;tf-nightly&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;You can ask questions and join the development discussion:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In the &lt;a href=&#34;https://discuss.tensorflow.org/&#34;&gt;TensorFlow forum&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;On the &lt;a href=&#34;https://groups.google.com/forum/#!forum/keras-users&#34;&gt;Keras Google group&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;On the &lt;a href=&#34;https://kerasteam.slack.com&#34;&gt;Keras Slack channel&lt;/a&gt;. Use &lt;a href=&#34;https://keras-slack-autojoin.herokuapp.com/&#34;&gt;this link&lt;/a&gt; to request an invitation to the channel.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Opening an issue&lt;/h2&gt; &#xA;&lt;p&gt;You can also post &lt;strong&gt;bug reports and feature requests&lt;/strong&gt; (only) in &lt;a href=&#34;https://github.com/keras-team/keras/issues&#34;&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Opening a PR&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! Before opening a PR, please read &lt;a href=&#34;https://github.com/keras-team/keras/raw/master/CONTRIBUTING.md&#34;&gt;our contributor guide&lt;/a&gt;, and the &lt;a href=&#34;https://github.com/keras-team/governance/raw/master/keras_api_design_guidelines.md&#34;&gt;API design guideline&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>acantril/learn-cantrill-io-labs</title>
    <updated>2022-06-04T02:02:29Z</updated>
    <id>tag:github.com,2022-06-04:/acantril/learn-cantrill-io-labs</id>
    <link href="https://github.com/acantril/learn-cantrill-io-labs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Standard and Advanced Demos for learn.cantrill.io courses&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;learn-cantrill-io-labs&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/acantril/learn-cantrill-io-labs/raw/master/demogrid.png&#34; alt=&#34;DEMOGRID&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome .. this repo stores a collection of freely available demos and advanced demos for AWS (and in the future other cloud platforms) These demos are available in two ways&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;here in instruction &amp;amp; architecture diagram form for free&lt;/li&gt; &#xA; &lt;li&gt;as part of &lt;a href=&#34;https://learn.cantrill.io&#34;&gt;https://learn.cantrill.io&lt;/a&gt; courses - including theory lessons and guided videos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The free versions here are fully functional, with instructions &amp;amp; architecture diagrams and are maintained by me.&lt;/p&gt; &#xA;&lt;p&gt;All demos have a structure ...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;01_LABSETUP&lt;/code&gt; contains assets required for the DEMO&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;02_LABINSTRUCTIONS&lt;/code&gt; contains text instructions and architecture diagrams&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Video guided versions are available in my courses at &lt;a href=&#34;https://learn.cantrill.io&#34;&gt;https://learn.cantrill.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo List (Order of creation!)&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/AWS_HYBRID_AdvancedVPN&#34;&gt;Implement a Dynamic, BGP Based, Highly-Available Site-to-Site VPN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-hybrid-activedirectory&#34;&gt;Implement Hybrid Directory - On-premises &amp;amp; AWS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-hybrid-dns&#34;&gt;Hybrid DNS using Route53 Inbound and Outbound Endpoints&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-patch-manager&#34;&gt;Patching AWS and On-premises using Systems Manager Patch Manager&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-elastic-wordpress-evolution&#34;&gt;Web Application Architecture Evolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-serverless-pet-cuddle-o-tron&#34;&gt;Implement a Serverless Application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-dms-database-migration&#34;&gt;Database Migration using DMS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-cognito-web-identity-federation&#34;&gt;Building a serverless application using Web Identity Federation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>apache/tvm</title>
    <updated>2022-06-04T02:02:29Z</updated>
    <id>tag:github.com,2022-06-04:/apache/tvm</id>
    <link href="https://github.com/apache/tvm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open deep learning compiler stack for cpu, gpu and specialized accelerators&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/tvm-site/main/images/logo/tvm-logo-small.png&#34; width=&#34;128/&#34;&gt; Open Deep Learning Compiler Stack&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tvm.apache.org/docs&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/apache/tvm/main/CONTRIBUTORS.md&#34;&gt;Contributors&lt;/a&gt; | &lt;a href=&#34;https://tvm.apache.org/community&#34;&gt;Community&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/apache/tvm/main/NEWS.md&#34;&gt;Release Notes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ci.tlcpack.ai/job/tvm/job/main/&#34;&gt;&lt;img src=&#34;https://ci.tlcpack.ai/buildStatus/icon?job=tvm/main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apache/tvm/actions?query=workflow%3AWinMacBuild&#34;&gt;&lt;img src=&#34;https://github.com/apache/tvm/workflows/WinMacBuild/badge.svg?sanitize=true&#34; alt=&#34;WinMacBuild&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apache TVM is a compiler stack for deep learning systems. It is designed to close the gap between the productivity-focused deep learning frameworks, and the performance- and efficiency-focused hardware backends. TVM works with deep learning frameworks to provide end to end compilation to different backends.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;TVM is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/apache/tvm/main/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://tvm.apache.org/docs/&#34;&gt;TVM Documentation&lt;/a&gt; site for installation instructions, tutorials, examples, and more. The &lt;a href=&#34;https://tvm.apache.org/docs/tutorial/introduction.html&#34;&gt;Getting Started with TVM&lt;/a&gt; tutorial is a great place to start.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute to TVM&lt;/h2&gt; &#xA;&lt;p&gt;TVM adopts apache committer model, we aim to create an open source project that is maintained and owned by the community. Check out the &lt;a href=&#34;https://tvm.apache.org/docs/contribute/&#34;&gt;Contributor Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We learned a lot from the following projects when building TVM.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/halide/Halide&#34;&gt;Halide&lt;/a&gt;: Part of TVM&#39;s TIR and arithmetic simplification module originates from Halide. We also learned and adapted some part of lowering pipeline from Halide.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/inducer/loopy&#34;&gt;Loopy&lt;/a&gt;: use of integer set analysis and its loop transformation primitives.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Theano/Theano&#34;&gt;Theano&lt;/a&gt;: the design inspiration of symbolic scan operator for recurrence.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>