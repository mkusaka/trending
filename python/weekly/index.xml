<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-25T01:45:54Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>thuml/Time-Series-Library</title>
    <updated>2024-08-25T01:45:54Z</updated>
    <id>tag:github.com,2024-08-25:/thuml/Time-Series-Library</id>
    <link href="https://github.com/thuml/Time-Series-Library" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Library for Advanced Deep Time Series Models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Time Series Library (TSLib)&lt;/h1&gt; &#xA;&lt;p&gt;TSLib is an open-source library for deep learning researchers, especially for deep time series analysis.&lt;/p&gt; &#xA;&lt;p&gt;We provide a neat code base to evaluate advanced deep time series models or develop your model, which covers five mainstream tasks: &lt;strong&gt;long- and short-term forecasting, imputation, anomaly detection, and classification.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🚩&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2024.07) We wrote a comprehensive survey of &lt;a href=&#34;https://arxiv.org/abs/2407.13278&#34;&gt;[Deep Time Series Models]&lt;/a&gt; with a rigorous benchmark based on TSLib. In this paper, we summarized the design principles of current time series models supported by insightful experiments, hoping to be helpful to future research.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🚩&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2024.04) Many thanks for the great work from &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/pull/378&#34;&gt;frecklebars&lt;/a&gt;. The famous sequential model &lt;a href=&#34;https://arxiv.org/abs/2312.00752&#34;&gt;Mamba&lt;/a&gt; has been included in our library. See &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Mamba.py&#34;&gt;this file&lt;/a&gt;, where you need to install &lt;code&gt;mamba_ssm&lt;/code&gt; with pip at first.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🚩&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2024.03) Given the inconsistent look-back length of various papers, we split the long-term forecasting in the leaderboard into two categories: Look-Back-96 and Look-Back-Searching. We recommend researchers read &lt;a href=&#34;https://openreview.net/pdf?id=7oLshfEIC2&#34;&gt;TimeMixer&lt;/a&gt;, which includes both look-back length settings in experiments for scientific rigor.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🚩&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2023.10) We add an implementation to &lt;a href=&#34;https://arxiv.org/abs/2310.06625&#34;&gt;iTransformer&lt;/a&gt;, which is the state-of-the-art model for long-term forecasting. The official code and complete scripts of iTransformer can be found &lt;a href=&#34;https://github.com/thuml/iTransformer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🚩&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2023.09) We added a detailed &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/tutorial/TimesNet_tutorial.ipynb&#34;&gt;tutorial&lt;/a&gt; for &lt;a href=&#34;https://openreview.net/pdf?id=ju_Uqw384Oq&#34;&gt;TimesNet&lt;/a&gt; and this library, which is quite friendly to beginners of deep time series analysis.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🚩&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2023.02) We release the TSlib as a comprehensive benchmark and code base for time series models, which is extended from our previous GitHub repository &lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;Autoformer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Leaderboard for Time Series Analysis&lt;/h2&gt; &#xA;&lt;p&gt;Till March 2024, the top three models for five different tasks are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;br&gt;Ranking&lt;/th&gt; &#xA;   &lt;th&gt;Long-term&lt;br&gt;Forecasting&lt;br&gt;Look-Back-96&lt;/th&gt; &#xA;   &lt;th&gt;Long-term&lt;br&gt;Forecasting&lt;br&gt;Look-Back-Searching&lt;/th&gt; &#xA;   &lt;th&gt;Short-term&lt;br&gt;Forecasting&lt;/th&gt; &#xA;   &lt;th&gt;Imputation&lt;/th&gt; &#xA;   &lt;th&gt;Classification&lt;/th&gt; &#xA;   &lt;th&gt;Anomaly&lt;br&gt;Detection&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🥇 1st&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.06625&#34;&gt;iTransformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/pdf?id=7oLshfEIC2&#34;&gt;TimeMixer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🥈 2nd&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/pdf?id=7oLshfEIC2&#34;&gt;TimeMixer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yuqinie98/PatchTST&#34;&gt;PatchTST&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MAZiqing/FEDformer&#34;&gt;FEDformer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🥉 3rd&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.13504.pdf&#34;&gt;DLinear&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MAZiqing/FEDformer&#34;&gt;FEDformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;Autoformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zhouhaoyi/Informer2020&#34;&gt;Informer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;Autoformer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: We will keep updating this leaderboard.&lt;/strong&gt; If you have proposed advanced and awesome models, you can send us your paper/code link or raise a pull request. We will add them to this repo and update the leaderboard as soon as possible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compared models of this leaderboard.&lt;/strong&gt; ☑ means that their codes have already been included in this repo.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TimeMixer&lt;/strong&gt; - TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=7oLshfEIC2&#34;&gt;[ICLR 2024]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TimeMixer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TSMixer&lt;/strong&gt; - TSMixer: An All-MLP Architecture for Time Series Forecasting &lt;a href=&#34;https://arxiv.org/pdf/2303.06053.pdf&#34;&gt;[arXiv 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TSMixer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;iTransformer&lt;/strong&gt; - iTransformer: Inverted Transformers Are Effective for Time Series Forecasting &lt;a href=&#34;https://arxiv.org/abs/2310.06625&#34;&gt;[ICLR 2024]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/iTransformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;PatchTST&lt;/strong&gt; - A Time Series is Worth 64 Words: Long-term Forecasting with Transformers &lt;a href=&#34;https://openreview.net/pdf?id=Jbdc0vTOcol&#34;&gt;[ICLR 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/PatchTST.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TimesNet&lt;/strong&gt; - TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis &lt;a href=&#34;https://openreview.net/pdf?id=ju_Uqw384Oq&#34;&gt;[ICLR 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TimesNet.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;DLinear&lt;/strong&gt; - Are Transformers Effective for Time Series Forecasting? &lt;a href=&#34;https://arxiv.org/pdf/2205.13504.pdf&#34;&gt;[AAAI 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/DLinear.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;LightTS&lt;/strong&gt; - Less Is More: Fast Multivariate Time Series Forecasting with Light Sampling-oriented MLP Structures &lt;a href=&#34;https://arxiv.org/abs/2207.01186&#34;&gt;[arXiv 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/LightTS.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;ETSformer&lt;/strong&gt; - ETSformer: Exponential Smoothing Transformers for Time-series Forecasting &lt;a href=&#34;https://arxiv.org/abs/2202.01381&#34;&gt;[arXiv 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/ETSformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Non-stationary Transformer&lt;/strong&gt; - Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=ucNDIDRNjjv&#34;&gt;[NeurIPS 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Nonstationary_Transformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;FEDformer&lt;/strong&gt; - FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting &lt;a href=&#34;https://proceedings.mlr.press/v162/zhou22g.html&#34;&gt;[ICML 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/FEDformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Pyraformer&lt;/strong&gt; - Pyraformer: Low-complexity Pyramidal Attention for Long-range Time Series Modeling and Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=0EXmFzUn5I&#34;&gt;[ICLR 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Pyraformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Autoformer&lt;/strong&gt; - Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=I55UqU-M11y&#34;&gt;[NeurIPS 2021]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Autoformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Informer&lt;/strong&gt; - Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17325/17132&#34;&gt;[AAAI 2021]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Informer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Reformer&lt;/strong&gt; - Reformer: The Efficient Transformer &lt;a href=&#34;https://openreview.net/forum?id=rkgNKkHtvB&#34;&gt;[ICLR 2020]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Reformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Transformer&lt;/strong&gt; - Attention is All You Need &lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;[NeurIPS 2017]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Transformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See our latest paper &lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;[TimesNet]&lt;/a&gt; for the comprehensive benchmark. We will release a real-time updated online version soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Newly added baselines.&lt;/strong&gt; We will add them to the leaderboard after a comprehensive evaluation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Mamba&lt;/strong&gt; - Mamba: Linear-Time Sequence Modeling with Selective State Spaces &lt;a href=&#34;https://arxiv.org/abs/2312.00752&#34;&gt;[arXiv 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Mamba.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;SegRNN&lt;/strong&gt; - SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting &lt;a href=&#34;https://arxiv.org/abs/2308.11200.pdf&#34;&gt;[arXiv 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/SegRNN.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Koopa&lt;/strong&gt; - Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors &lt;a href=&#34;https://arxiv.org/pdf/2305.18803.pdf&#34;&gt;[NeurIPS 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Koopa.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;FreTS&lt;/strong&gt; - Frequency-domain MLPs are More Effective Learners in Time Series Forecasting &lt;a href=&#34;https://arxiv.org/pdf/2311.06184.pdf&#34;&gt;[NeurIPS 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/FreTS.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TiDE&lt;/strong&gt; - Long-term Forecasting with TiDE: Time-series Dense Encoder &lt;a href=&#34;https://arxiv.org/pdf/2304.08424.pdf&#34;&gt;[arXiv 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TiDE.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;FiLM&lt;/strong&gt; - FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting &lt;a href=&#34;https://openreview.net/forum?id=zTQdHSQUQWc&#34;&gt;[NeurIPS 2022]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/FiLM.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;MICN&lt;/strong&gt; - MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=zt53IDUR1U&#34;&gt;[ICLR 2023]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/MICN.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Crossformer&lt;/strong&gt; - Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=vSVLM2j9eie&#34;&gt;[ICLR 2023]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Crossformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TFT&lt;/strong&gt; - Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting &lt;a href=&#34;https://arxiv.org/abs/1912.09363&#34;&gt;[arXiv 2019]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TemporalFusionTransformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Python 3.8. For convenience, execute the following command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Prepare Data. You can obtain the well pre-processed datasets from &lt;a href=&#34;https://drive.google.com/drive/folders/13Cg1KYOlzM5C7K8gK8NfC-F3EYxkM3D2?usp=sharing&#34;&gt;[Google Drive]&lt;/a&gt; or&amp;nbsp;&lt;a href=&#34;https://pan.baidu.com/s/1r3KhGd0Q9PJIUZdfEYoymg?pwd=i9iy&#34;&gt;[Baidu Drive]&lt;/a&gt;, Then place the downloaded data in the folder&lt;code&gt;./dataset&lt;/code&gt;. Here is a summary of supported datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/thuml/Time-Series-Library/main/.%5Cpic%5Cdataset.png&#34; height=&#34;200&#34; alt=&#34;&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Train and evaluate model. We provide the experiment scripts for all benchmarks under the folder &lt;code&gt;./scripts/&lt;/code&gt;. You can reproduce the experiment results as the following examples:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# long-term forecast&#xA;bash ./scripts/long_term_forecast/ETT_script/TimesNet_ETTh1.sh&#xA;# short-term forecast&#xA;bash ./scripts/short_term_forecast/TimesNet_M4.sh&#xA;# imputation&#xA;bash ./scripts/imputation/ETT_script/TimesNet_ETTh1.sh&#xA;# anomaly detection&#xA;bash ./scripts/anomaly_detection/PSM/TimesNet.sh&#xA;# classification&#xA;bash ./scripts/classification/TimesNet.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Develop your own model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the model file to the folder &lt;code&gt;./models&lt;/code&gt;. You can follow the &lt;code&gt;./models/Transformer.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Include the newly added model in the &lt;code&gt;Exp_Basic.model_dict&lt;/code&gt; of &lt;code&gt;./exp/exp_basic.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create the corresponding scripts under the folder &lt;code&gt;./scripts&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: The original code for the classification task can be found &lt;a href=&#34;https://github.com/thuml/Flowformer/tree/main/Flowformer_TimeSeries&#34;&gt;here&lt;/a&gt;. It is hard to fuse all five tasks in one library. We are still working on this task.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repo useful, please cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{wu2023timesnet,&#xA;  title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},&#xA;  author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},&#xA;  booktitle={International Conference on Learning Representations},&#xA;  year={2023},&#xA;}&#xA;&#xA;@article{wang2024tssurvey,&#xA;  title={Deep Time Series Models: A Comprehensive Survey and Benchmark},&#xA;  author={Yuxuan Wang and Haixu Wu and Jiaxiang Dong and Yong Liu and Mingsheng Long and Jianmin Wang},&#xA;  booktitle={arXiv preprint arXiv:2407.13278},&#xA;  year={2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or suggestions, feel free to contact our maintenance team:&lt;/p&gt; &#xA;&lt;p&gt;Current:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Haixu Wu (Ph.D. student, &lt;a href=&#34;mailto:wuhx23@mails.tsinghua.edu.cn&#34;&gt;wuhx23@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Yong Liu (Ph.D. student, &lt;a href=&#34;mailto:liuyong21@mails.tsinghua.edu.cn&#34;&gt;liuyong21@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Yuxuan Wang (Ph.D. student, &lt;a href=&#34;mailto:wangyuxu22@mails.tsinghua.edu.cn&#34;&gt;wangyuxu22@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Huikun Weng (Undergraduate, &lt;a href=&#34;mailto:wenghk22@mails.tsinghua.edu.cn&#34;&gt;wenghk22@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Previous:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tengge Hu (Master student, &lt;a href=&#34;mailto:htg21@mails.tsinghua.edu.cn&#34;&gt;htg21@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Haoran Zhang (Master student, &lt;a href=&#34;mailto:z-hr20@mails.tsinghua.edu.cn&#34;&gt;z-hr20@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Jiawei Guo (Undergraduate, &lt;a href=&#34;mailto:guo-jw21@mails.tsinghua.edu.cn&#34;&gt;guo-jw21@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Or describe it in Issues.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is supported by the National Key R&amp;amp;D Program of China (2021YFB1715200).&lt;/p&gt; &#xA;&lt;p&gt;This library is constructed based on the following repos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Forecasting: &lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;https://github.com/thuml/Autoformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Anomaly Detection: &lt;a href=&#34;https://github.com/thuml/Anomaly-Transformer&#34;&gt;https://github.com/thuml/Anomaly-Transformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classification: &lt;a href=&#34;https://github.com/thuml/Flowformer&#34;&gt;https://github.com/thuml/Flowformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All the experiment datasets are public, and we obtain them from the following links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Long-term Forecasting and Imputation: &lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;https://github.com/thuml/Autoformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Short-term Forecasting: &lt;a href=&#34;https://github.com/ServiceNow/N-BEATS&#34;&gt;https://github.com/ServiceNow/N-BEATS&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Anomaly Detection: &lt;a href=&#34;https://github.com/thuml/Anomaly-Transformer&#34;&gt;https://github.com/thuml/Anomaly-Transformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classification: &lt;a href=&#34;https://www.timeseriesclassification.com/&#34;&gt;https://www.timeseriesclassification.com/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;All Thanks To Our Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=thuml/Time-Series-Library&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/VILA</title>
    <updated>2024-08-25T01:45:54Z</updated>
    <id>tag:github.com,2024-08-25:/NVlabs/VILA</id>
    <link href="https://github.com/NVlabs/VILA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;VILA - a multi-image visual language model with training, inference and evaluation recipe, deployable from cloud to edge (Jetson Orin and laptops)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/vila-logo.jpg&#34; width=&#34;20%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;VILA: On Pre-training for Visual Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/CODE_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/MODEL_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/MODEL%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Model License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.10+&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.07533&#34;&gt;VILA arxiv&lt;/a&gt; / &lt;a href=&#34;https://vila-demo.hanlab.ai/&#34;&gt;VILA Demo&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/collections/Efficient-Large-Model/vila-on-pre-training-for-visual-language-models-65d8022a3a52cd9bcd62698e&#34;&gt;VILA Huggingface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💡 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;VILA is a visual language model (VLM) pretrained with interleaved image-text data at scale, enabling &lt;strong&gt;video understanding&lt;/strong&gt; and &lt;strong&gt;multi-image understanding&lt;/strong&gt; capabilities. VILA is deployable on the edge by &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt; 4bit quantization and &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; framework. We find: (1) image-text pairs are not enough, interleaved image-text is essential; (2) unfreezing LLM during interleaved image-text pre-training enables in-context learning; (3)re-blending text-only instruction data is crucial to boost both VLM and text-only performance; (4) token compression extends #video frames. VILA unveils appealing capabilities, including: video reasoning, in-context learning, visual chain-of-thought, and better world knowledge.&lt;/p&gt; &#xA;&lt;h2&gt;💡 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/08] We release &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/LongVILA.md&#34;&gt;LongVILA&lt;/a&gt; that supports long video understanding (Captioning, QA, Needle-in-a-Haystack) up to 1024 frames.&lt;/li&gt; &#xA; &lt;li&gt;[2024/07] VILA1.5 also ranks 1st place (OSS model) on &lt;a href=&#34;https://github.com/JUNJIE99/MLVU&#34;&gt;MLVU test leaderboard&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/06] VILA1.5 is now the best open sourced VLM on &lt;a href=&#34;https://mmmu-benchmark.github.io/#leaderboard&#34;&gt;MMMU leaderboard&lt;/a&gt; and &lt;a href=&#34;https://video-mme.github.io/home_page.html#leaderboard&#34;&gt;Video-MME&lt;/a&gt; leaderboard!&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] We release VILA-1.5, which offers &lt;strong&gt;video understanding capability&lt;/strong&gt;. VILA-1.5 comes with four model sizes: 3B/8B/13B/40B.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] We release &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt;-quantized 4bit VILA-1.5 models. VILA-1.5 is efficiently deployable on diverse NVIDIA GPUs (A100, 4090, 4070 Laptop, Orin, Orin Nano) by &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_trt_llm&#34;&gt;TensorRT-LLM&lt;/a&gt; backends.&lt;/li&gt; &#xA; &lt;li&gt;[2024/03] VILA has been accepted by CVPR 2024!&lt;/li&gt; &#xA; &lt;li&gt;[2024/02] We release &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt;-quantized 4bit VILA models, deployable on Jetson Orin and laptops through &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; and &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine&#34;&gt;TinyChatEngine&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/02] VILA is released. We propose interleaved image-text pretraining that enables &lt;strong&gt;multi-image&lt;/strong&gt; VLM. VILA comes with impressive in-context learning capabilities. We open source everything: including training code, evaluation code, datasets, model ckpts.&lt;/li&gt; &#xA; &lt;li&gt;[2023/12] &lt;a href=&#34;https://arxiv.org/abs/2312.07533&#34;&gt;Paper&lt;/a&gt; is on Arxiv!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;h3&gt;Image QA Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Prec.&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;VizWiz&lt;/th&gt; &#xA;   &lt;th&gt;SQA-I&lt;/th&gt; &#xA;   &lt;th&gt;VQA-T&lt;/th&gt; &#xA;   &lt;th&gt;POPE&lt;/th&gt; &#xA;   &lt;th&gt;MME&lt;/th&gt; &#xA;   &lt;th&gt;MMB&lt;/th&gt; &#xA;   &lt;th&gt;MMB-CN&lt;/th&gt; &#xA;   &lt;th&gt;SEED&lt;/th&gt; &#xA;   &lt;th&gt;SEED-I&lt;/th&gt; &#xA;   &lt;th&gt;MMMU (val)&lt;/th&gt; &#xA;   &lt;th&gt;MMMU (test)&lt;/th&gt; &#xA;   &lt;th&gt;llava-bench&lt;/th&gt; &#xA;   &lt;th&gt;MM-Vet&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;80.4&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;53.5&lt;/td&gt; &#xA;   &lt;td&gt;69.0&lt;/td&gt; &#xA;   &lt;td&gt;60.4&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;1442.44&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;52.7&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;   &lt;td&gt;67.9&lt;/td&gt; &#xA;   &lt;td&gt;33.3&lt;/td&gt; &#xA;   &lt;td&gt;30.8&lt;/td&gt; &#xA;   &lt;td&gt;75.9&lt;/td&gt; &#xA;   &lt;td&gt;35.4&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;80.0&lt;/td&gt; &#xA;   &lt;td&gt;61.1&lt;/td&gt; &#xA;   &lt;td&gt;53.8&lt;/td&gt; &#xA;   &lt;td&gt;67.8&lt;/td&gt; &#xA;   &lt;td&gt;60.4&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;1437.34&lt;/td&gt; &#xA;   &lt;td&gt;63.3&lt;/td&gt; &#xA;   &lt;td&gt;51.4&lt;/td&gt; &#xA;   &lt;td&gt;59.8&lt;/td&gt; &#xA;   &lt;td&gt;66.6&lt;/td&gt; &#xA;   &lt;td&gt;32.7&lt;/td&gt; &#xA;   &lt;td&gt;31.1&lt;/td&gt; &#xA;   &lt;td&gt;75.0&lt;/td&gt; &#xA;   &lt;td&gt;37.3&lt;/td&gt; &#xA;   &lt;td&gt;59.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;79.8&lt;/td&gt; &#xA;   &lt;td&gt;61.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;69.6&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;85.3&lt;/td&gt; &#xA;   &lt;td&gt;1431.65&lt;/td&gt; &#xA;   &lt;td&gt;62.8&lt;/td&gt; &#xA;   &lt;td&gt;52.2&lt;/td&gt; &#xA;   &lt;td&gt;60.0&lt;/td&gt; &#xA;   &lt;td&gt;66.4&lt;/td&gt; &#xA;   &lt;td&gt;32.8&lt;/td&gt; &#xA;   &lt;td&gt;31.3&lt;/td&gt; &#xA;   &lt;td&gt;76.7&lt;/td&gt; &#xA;   &lt;td&gt;38.6&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;79.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;62.3&lt;/td&gt; &#xA;   &lt;td&gt;69.2&lt;/td&gt; &#xA;   &lt;td&gt;63.0&lt;/td&gt; &#xA;   &lt;td&gt;85.8&lt;/td&gt; &#xA;   &lt;td&gt;1417.06&lt;/td&gt; &#xA;   &lt;td&gt;61.6&lt;/td&gt; &#xA;   &lt;td&gt;51.5&lt;/td&gt; &#xA;   &lt;td&gt;59.1&lt;/td&gt; &#xA;   &lt;td&gt;65.7&lt;/td&gt; &#xA;   &lt;td&gt;33.4&lt;/td&gt; &#xA;   &lt;td&gt;30.4&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;36.7&lt;/td&gt; &#xA;   &lt;td&gt;60.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;80.9&lt;/td&gt; &#xA;   &lt;td&gt;61.9&lt;/td&gt; &#xA;   &lt;td&gt;58.7&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;84.4&lt;/td&gt; &#xA;   &lt;td&gt;1577.01&lt;/td&gt; &#xA;   &lt;td&gt;72.3&lt;/td&gt; &#xA;   &lt;td&gt;66.2&lt;/td&gt; &#xA;   &lt;td&gt;64.2&lt;/td&gt; &#xA;   &lt;td&gt;71.4&lt;/td&gt; &#xA;   &lt;td&gt;36.9&lt;/td&gt; &#xA;   &lt;td&gt;36.0&lt;/td&gt; &#xA;   &lt;td&gt;80.0&lt;/td&gt; &#xA;   &lt;td&gt;38.3&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;80.3&lt;/td&gt; &#xA;   &lt;td&gt;61.7&lt;/td&gt; &#xA;   &lt;td&gt;59.3&lt;/td&gt; &#xA;   &lt;td&gt;79.0&lt;/td&gt; &#xA;   &lt;td&gt;65.4&lt;/td&gt; &#xA;   &lt;td&gt;82.9&lt;/td&gt; &#xA;   &lt;td&gt;1593.65&lt;/td&gt; &#xA;   &lt;td&gt;71.0&lt;/td&gt; &#xA;   &lt;td&gt;64.9&lt;/td&gt; &#xA;   &lt;td&gt;64.0&lt;/td&gt; &#xA;   &lt;td&gt;71.1&lt;/td&gt; &#xA;   &lt;td&gt;36.0&lt;/td&gt; &#xA;   &lt;td&gt;36.1&lt;/td&gt; &#xA;   &lt;td&gt;79.0&lt;/td&gt; &#xA;   &lt;td&gt;37.2&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;82.8&lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;65.0&lt;/td&gt; &#xA;   &lt;td&gt;86.3&lt;/td&gt; &#xA;   &lt;td&gt;1569.55&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;   &lt;td&gt;33.6&lt;/td&gt; &#xA;   &lt;td&gt;80.8&lt;/td&gt; &#xA;   &lt;td&gt;44.3&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;82.7&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;   &lt;td&gt;63.3&lt;/td&gt; &#xA;   &lt;td&gt;79.7&lt;/td&gt; &#xA;   &lt;td&gt;64.7&lt;/td&gt; &#xA;   &lt;td&gt;86.7&lt;/td&gt; &#xA;   &lt;td&gt;1531.35&lt;/td&gt; &#xA;   &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;td&gt;66.7&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;37.8&lt;/td&gt; &#xA;   &lt;td&gt;34.0&lt;/td&gt; &#xA;   &lt;td&gt;81.9&lt;/td&gt; &#xA;   &lt;td&gt;46.4&lt;/td&gt; &#xA;   &lt;td&gt;66.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;84.3&lt;/td&gt; &#xA;   &lt;td&gt;64.6&lt;/td&gt; &#xA;   &lt;td&gt;62.2&lt;/td&gt; &#xA;   &lt;td&gt;87.2&lt;/td&gt; &#xA;   &lt;td&gt;73.6&lt;/td&gt; &#xA;   &lt;td&gt;87.3&lt;/td&gt; &#xA;   &lt;td&gt;1726.82&lt;/td&gt; &#xA;   &lt;td&gt;82.4&lt;/td&gt; &#xA;   &lt;td&gt;80.2&lt;/td&gt; &#xA;   &lt;td&gt;69.1&lt;/td&gt; &#xA;   &lt;td&gt;75.8&lt;/td&gt; &#xA;   &lt;td&gt;51.9&lt;/td&gt; &#xA;   &lt;td&gt;46.9&lt;/td&gt; &#xA;   &lt;td&gt;81.3&lt;/td&gt; &#xA;   &lt;td&gt;53.0&lt;/td&gt; &#xA;   &lt;td&gt;72.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;84.1&lt;/td&gt; &#xA;   &lt;td&gt;64.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;86.7&lt;/td&gt; &#xA;   &lt;td&gt;73.2&lt;/td&gt; &#xA;   &lt;td&gt;88.2&lt;/td&gt; &#xA;   &lt;td&gt;1714.79&lt;/td&gt; &#xA;   &lt;td&gt;83.2&lt;/td&gt; &#xA;   &lt;td&gt;79.6&lt;/td&gt; &#xA;   &lt;td&gt;68.9&lt;/td&gt; &#xA;   &lt;td&gt;75.6&lt;/td&gt; &#xA;   &lt;td&gt;49.3&lt;/td&gt; &#xA;   &lt;td&gt;46.2&lt;/td&gt; &#xA;   &lt;td&gt;83.0&lt;/td&gt; &#xA;   &lt;td&gt;51.4&lt;/td&gt; &#xA;   &lt;td&gt;72.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: VQAV2 and VizWiz are test-dev, the average accuracy is calculated over all datasets and MME numbers are divided by 20.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Video QA Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Prec.&lt;/th&gt; &#xA;   &lt;th&gt;Perception Test&lt;/th&gt; &#xA;   &lt;th&gt;ActivityNet&lt;/th&gt; &#xA;   &lt;th&gt;MSVD&lt;/th&gt; &#xA;   &lt;th&gt;MSRVTT&lt;/th&gt; &#xA;   &lt;th&gt;TGIF&lt;/th&gt; &#xA;   &lt;th&gt;EgoSchema (test)&lt;/th&gt; &#xA;   &lt;th&gt;CinePile&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;47&lt;/td&gt; &#xA;   &lt;td&gt;50.2&lt;/td&gt; &#xA;   &lt;td&gt;76.6&lt;/td&gt; &#xA;   &lt;td&gt;57.5&lt;/td&gt; &#xA;   &lt;td&gt;51.7&lt;/td&gt; &#xA;   &lt;td&gt;42.6&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;49.7&lt;/td&gt; &#xA;   &lt;td&gt;50.7&lt;/td&gt; &#xA;   &lt;td&gt;76.9&lt;/td&gt; &#xA;   &lt;td&gt;57.6&lt;/td&gt; &#xA;   &lt;td&gt;51.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;54.1&lt;/td&gt; &#xA;   &lt;td&gt;54.3&lt;/td&gt; &#xA;   &lt;td&gt;78.3&lt;/td&gt; &#xA;   &lt;td&gt;60.1&lt;/td&gt; &#xA;   &lt;td&gt;54.1&lt;/td&gt; &#xA;   &lt;td&gt;50.4&lt;/td&gt; &#xA;   &lt;td&gt;48.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;53.6&lt;/td&gt; &#xA;   &lt;td&gt;54.7&lt;/td&gt; &#xA;   &lt;td&gt;77.9&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;   &lt;td&gt;56&lt;/td&gt; &#xA;   &lt;td&gt;52.2&lt;/td&gt; &#xA;   &lt;td&gt;50.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;54&lt;/td&gt; &#xA;   &lt;td&gt;58&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;63&lt;/td&gt; &#xA;   &lt;td&gt;58.2&lt;/td&gt; &#xA;   &lt;td&gt;58.7&lt;/td&gt; &#xA;   &lt;td&gt;51.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference speed ( Token/sec )&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;A100&lt;/th&gt; &#xA;   &lt;th&gt;4090&lt;/th&gt; &#xA;   &lt;th&gt;Orin&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;104.6&lt;/td&gt; &#xA;   &lt;td&gt;137.6&lt;/td&gt; &#xA;   &lt;td&gt;25.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;182.8&lt;/td&gt; &#xA;   &lt;td&gt;215.5&lt;/td&gt; &#xA;   &lt;td&gt;42.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;104.3&lt;/td&gt; &#xA;   &lt;td&gt;137.2&lt;/td&gt; &#xA;   &lt;td&gt;24.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;180.2&lt;/td&gt; &#xA;   &lt;td&gt;219.3&lt;/td&gt; &#xA;   &lt;td&gt;40.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;57.4&lt;/td&gt; &#xA;   &lt;td&gt;10.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;168.9&lt;/td&gt; &#xA;   &lt;td&gt;150.2&lt;/td&gt; &#xA;   &lt;td&gt;28.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;50.9&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;6.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;115.9&lt;/td&gt; &#xA;   &lt;td&gt;105.7&lt;/td&gt; &#xA;   &lt;td&gt;20.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;57.0&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: Measured using the &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tinychat&#34;&gt;TinyChat&lt;/a&gt; backend at batch size = 1.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h2&gt;VILA Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Video captioning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/156256291/c9520943-2478-4f97-bc95-121d625018a6&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/156256291/c9520943-2478-4f97-bc95-121d625018a6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: Elaborate on the visual and narrative elements of the video in detail.&lt;/p&gt; &#xA;&lt;p&gt;Caption: The video shows a person&#39;s hands working on a white surface. They are folding a piece of fabric with a checkered pattern in shades of blue and white. The fabric is being folded into a smaller, more compact shape. The person&#39;s fingernails are painted red, and they are wearing a black and red garment. There are also a ruler and a pencil on the surface, suggesting that measurements and precision are involved in the process.&lt;/p&gt; &#xA;&lt;h3&gt;In context learning&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/demo_img_1.png&#34; height=&#34;239&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/demo_img_2.png&#34; height=&#34;250&#34;&gt; &#xA;&lt;h3&gt;Multi-image reasoning&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/demo_img_3.png&#34; height=&#34;193&#34;&gt; &#xA;&lt;h3&gt;VILA on Jetson Orin&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/7783214/6079374c-0787-4bc4-b9c6-e1524b4c9dc4&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/7783214/6079374c-0787-4bc4-b9c6-e1524b4c9dc4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;VILA on RTX 4090&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/7783214/80c47742-e873-4080-ad7d-d17c4700539f&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/7783214/80c47742-e873-4080-ad7d-d17c4700539f&lt;/a&gt;&lt;/p&gt;  &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./environment_setup.sh vila&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;VILA training contains three steps, for specific hyperparameters, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/scripts/v1_5&#34;&gt;scripts/v1_5&lt;/a&gt; folder:&lt;/p&gt; &#xA;&lt;h3&gt;Step-1: Alignment&lt;/h3&gt; &#xA;&lt;p&gt;We utilize LLaVA-CC3M-Pretrain-595K dataset to align the textual and visual modalities.&lt;/p&gt; &#xA;&lt;p&gt;The stage 1 script takes in two parameters and it can run on a single 8xA100 node. &lt;code&gt;BASE_MODEL_PATH&lt;/code&gt; points to a online or local huggingface repository, such as &lt;code&gt;NousResearch/Llama-2-7b-hf&lt;/code&gt;. &lt;code&gt;OUTPUT_NAME&lt;/code&gt; points to a target directory under &lt;code&gt;checkpoints&lt;/code&gt;, which will save the trained multimodal projector afterwards.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/1_mm_align.sh [BASE_MODEL_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step-2: Pretraining&lt;/h3&gt; &#xA;&lt;p&gt;We use MMC4 and Coyo dataset to train VLM with interleaved image-text pairs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/2_pretrain_mmc4_coyo.sh [CODE_PATH] [BASE_MODEL_PATH] [STAGE1_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The stage 2 script takes in four arguments. &lt;code&gt;CODE_PATH&lt;/code&gt; is the absolute path to our VILA codebase, &lt;code&gt;BASE_MODEL_PATH&lt;/code&gt; has similar meaning to what is presented in the stage 1 script. &lt;code&gt;STAGE1_PATH&lt;/code&gt; points to the &lt;code&gt;OUTPUT_NAME&lt;/code&gt; of stage 1 (i.e. where the stage 1 checkpoint is stored). &lt;code&gt;OUTPUT_NAME&lt;/code&gt; is the desired folder name under &lt;code&gt;checkpoints&lt;/code&gt; that saves the pretraining checkpoint. The script we provided for this stage is executed on slurm, and we expect it to execute on 16 nodes (128 GPUs).&lt;/p&gt; &#xA;&lt;h3&gt;Step-3: Supervised fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;This is the last stage of VILA training, in which we tune the model to follow multimodal instructions on a subset of M3IT, FLAN and ShareGPT4V. This stage runs on a 8xA100 node.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/3_sft.sh [STAGE2_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The stage 3 script takes in two arguments. &lt;code&gt;STAGE2_PATH&lt;/code&gt; points to the &lt;code&gt;OUTPUT_NAME&lt;/code&gt; of the stage 2 script (i.e. where the stage 2 checkpoint is stored). &lt;code&gt;OUTPUT_NAME&lt;/code&gt; is the desired folder name under &lt;code&gt;checkpoints&lt;/code&gt; that stores the final checkpoint.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations&lt;/h2&gt; &#xA;&lt;h3&gt;Image Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;You can follow &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;Llava1.5 eval&lt;/a&gt; to download all datasets. After downloading all datasets, please put them under &lt;code&gt;playground/data/eval&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please make the following changes to the MME evaluation script. Please search for:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_path = &#34;MME_Benchmark_release_version&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and replace it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_path = os.path.join(script_dir, &#34;MME_Benchmark_release_version&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide a push-the-button script to perform evaluation on all 10 datasets that do not require GPT-assisted evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/v1_5/eval/eval_all.sh [CHECKPOINT_PATH] [MODEL_NAME] [CONV_MODE]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script takes in two parameters, &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; points to the stage 3 model checkpoint, and &lt;code&gt;MODEL_NAME&lt;/code&gt; will be the name of evaluation results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/830/my-submission&#34;&gt;VQAv2&lt;/a&gt; and &lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/2185/my-submission&#34;&gt;Vizwiz&lt;/a&gt; evaluations are hosted on eval.ai. You need to register an account and create a team to be able to submit eval.&lt;/p&gt; &#xA;&lt;p&gt;MMBench and MMBench_CN eval are hosted on another &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;evaluation server&lt;/a&gt;. Make sure you change the name of the file before submitting, otherwise the server caches results and will always return wrong result to you.&lt;/p&gt; &#xA;&lt;p&gt;We provide a quick script to automatically organize the prediction files that need to be submitted to servers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/v1_5/eval/copy_predictions.py [MODEL_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will be able to find the predictions under &lt;code&gt;playground/data/predictions_upload/[MODEL_NAME]&lt;/code&gt; after executing this script.&lt;/p&gt; &#xA;&lt;h3&gt;Video Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the evaluation steps in &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA/raw/main/TRAIN_AND_VALIDATE.md#data-for-validating&#34;&gt;Video-LLaVA&lt;/a&gt; for dataset preparation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/v1_5/eval/video_chatgpt/run_all.sh [CHECKPOINT_PATH] [MODEL_NAME] [CONV_MODE]&#xA;./scripts/v1_5/eval/video_chatgpt/eval_all.sh [MODEL_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;We provide snippets for quick inference with user prompts and images.&lt;/p&gt; &#xA;&lt;p&gt;Llama-3-VILA1.5-8B inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/Llama-3-VILA1.5-8b \&#xA;    --conv-mode llama_3 \&#xA;    --query &#34;&amp;lt;image&amp;gt;\n Please describe the traffic condition.&#34; \&#xA;    --image-file &#34;av.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;VILA1.5-40B inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/VILA1.5-40b \&#xA;    --conv-mode hermes-2 \&#xA;    --query &#34;&amp;lt;image&amp;gt;\n Please describe the traffic condition.&#34; \&#xA;    --image-file &#34;av.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;VILA1.5-3B video inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/VILA1.5-3b \&#xA;    --conv-mode vicuna_v1 \&#xA;    --query &#34;&amp;lt;video&amp;gt;\n Please describe this video.&#34; \&#xA;    --video-file &#34;demo.mp4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quantization and Deployment&lt;/h2&gt; &#xA;&lt;p&gt;Our VILA models are quantized by &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt; into 4 bits for efficient inference on the edge. We provide a push-the-button &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/raw/main/scripts/vila_example.sh&#34;&gt;script&lt;/a&gt; to quantize VILA with AWQ.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA on desktop GPUs and edge GPUs&lt;/h3&gt; &#xA;&lt;p&gt;We support AWQ-quantized 4bit VILA on GPU platforms via &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt;. We provide a &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat#support-vlm-models-vila--llava&#34;&gt;tutorial&lt;/a&gt; to run the model with TinyChat after quantization. We also provide an &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat/serve&#34;&gt;instruction&lt;/a&gt; to launch a Gradio server (powered by TinyChat and AWQ) to serve 4-bit quantized VILA models.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA on laptops&lt;/h3&gt; &#xA;&lt;p&gt;We further support our AWQ-quantized 4bit VILA models on various CPU platforms with both x86 and ARM architectures with our &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine&#34;&gt;TinyChatEngine&lt;/a&gt;. We also provide a detailed &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine/tree/main?tab=readme-ov-file#deploy-vision-language-model-vlm-chatbot-with-tinychatengine&#34;&gt;tutorial&lt;/a&gt; to help the users deploy VILA on different CPUs.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA API server&lt;/h3&gt; &#xA;&lt;p&gt;A simple API server has been provided to serve VILA models. The server is built on top of &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/transformers/&#34;&gt;Huggingface Transformers&lt;/a&gt;. The server can be run with the following command:&lt;/p&gt; &#xA;&lt;h4&gt;With CLI&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore server.py \&#xA;    --port 8000 \&#xA;    --model-path Efficient-Large-Model/VILA1.5-3B \&#xA;    --conv-mode vicuna_v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;With Docker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t vila-server:latest .&#xA;docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \&#xA;    -v ./hub:/root/.cache/huggingface/hub \&#xA;    -it --rm -p 8000:8000 \&#xA;    -e VILA_MODEL_PATH=Efficient-Large-Model/VILA1.5-3B \&#xA;    -e VILA_CONV_MODE=vicuna_v1 \&#xA;    vila-server:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can call the endpoint with the OpenAI SDK as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI(&#xA;    base_url=&#34;http://localhost:8000&#34;,&#xA;    api_key=&#34;fake-key&#34;,&#xA;)&#xA;response = client.chat.completions.create(&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: [&#xA;                {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What’s in this image?&#34;},&#xA;                {&#xA;                    &#34;type&#34;: &#34;image_url&#34;,&#xA;                    &#34;image_url&#34;: {&#xA;                        &#34;url&#34;: &#34;https://blog.logomyway.com/wp-content/uploads/2022/01/NVIDIA-logo.jpg&#34;,&#xA;                        # Or you can pass in a base64 encoded image&#xA;                        # &#34;url&#34;: &#34;data:image/png;base64,&amp;lt;base64_encoded_image&amp;gt;&#34;,&#xA;                    },&#xA;                },&#xA;            ],&#xA;        }&#xA;    ],&#xA;    max_tokens=300,&#xA;    model=&#34;VILA1.5-3B&#34;,&#xA;    # You can pass in extra parameters as follows&#xA;    extra_body={&#34;num_beams&#34;: 1, &#34;use_cache&#34;: False},&#xA;)&#xA;print(response.choices[0].message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: This API server is intended for evaluation purposes only and has not been optimized for production use. It has only been tested on A100 and H100 GPUs.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;We release &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b&#34;&gt;VILA1.5-3B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-s2&#34;&gt;VILA1.5-3B-S2&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/Llama-3-VILA1.5-8b&#34;&gt;Llama-3-VILA1.5-8B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-13b&#34;&gt;VILA1.5-13B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-40b&#34;&gt;VILA1.5-40B&lt;/a&gt; and the 4-bit &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt;-quantized models &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-AWQ&#34;&gt;VILA1.5-3B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-s2-AWQ&#34;&gt;VILA1.5-3B-S2-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/Llama-3-VILA1.5-8b-AWQ&#34;&gt;Llama-3-VILA1.5-8B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-13b-AWQ&#34;&gt;VILA1.5-13B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-40b-AWQ&#34;&gt;VILA1.5-40B-AWQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🔒 License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code is released under the Apache 2.0 license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;The pretrained weights are released under the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en&#34;&gt;CC-BY-NC-SA-4.0 license&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The service is a research preview intended for non-commercial use only, and is subject to the following licenses and terms: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;Model License&lt;/a&gt; of LLaMA. For LLAMA3-VILA checkpoints terms of use, please refer to the &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;LLAMA3 License&lt;/a&gt; for additional details.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;Terms of Use&lt;/a&gt; of the data generated by OpenAI&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/data_prepare/LICENSE&#34;&gt;Dataset Licenses&lt;/a&gt; for each one used during training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Team&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=OI7zFmwAAAAJ&amp;amp;hl=en&#34;&gt;*Yao Lu&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hongxu-yin.github.io/&#34;&gt;*Hongxu Yin&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linji.me/&#34;&gt;*Ji Lin&lt;/a&gt;: OpenAI (work done at Nvidia and MIT)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=6gKEYRgAAAAJ&amp;amp;hl=en&#34;&gt;Wei Ping&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.pmolchanov.com/&#34;&gt;Pavlo Molchanov&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=Wel9l1wAAAAJ&amp;amp;hl=en&#34;&gt;Andrew Tao&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://kentang.net/&#34;&gt;Haotian Tang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ys-2020.github.io/&#34;&gt;Shang Yang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lzhu.me/&#34;&gt;Ligeng Zhu&lt;/a&gt;: Nvidia, MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://weichenwang.me/&#34;&gt;Wei-Chen Wang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://xuefuzhao.github.io/&#34;&gt;Fuzhao Xue&lt;/a&gt;: Nvidia, NUS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://seerkfang.github.io/&#34;&gt;Yunhao Fang&lt;/a&gt;: Nvidia, UCSD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://yukangchen.com/&#34;&gt;Yukang Chen&lt;/a&gt;: Nvidia, CUHK&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/profile?id=~Zhuoyang_Zhang1&#34;&gt;Zhuoyang Zhang&lt;/a&gt;: Nvidia, Tsinghua Univ.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linkedin.com/in/yue-james-shen/&#34;&gt;Yue Shen&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=6xFvyJwAAAAJ&amp;amp;hl=en&#34;&gt;Wei-Ming Chen&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=r5WezOYAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Huizi Mao&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bfshi.github.io/&#34;&gt;Baifeng Shi&lt;/a&gt;: Nvidia, UC Berkeley&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://jankautz.com/&#34;&gt;Jan Kautz&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=62ElavIAAAAJ&amp;amp;hl=en&#34;&gt;Mohammad Shoeybi&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://songhan.mit.edu/&#34;&gt;Song Han&lt;/a&gt;: Nvidia, MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lin2023vila,&#xA;      title={VILA: On Pre-training for Visual Language Models},&#xA;      author={Ji Lin and Hongxu Yin and Wei Ping and Yao Lu and Pavlo Molchanov and Andrew Tao and Huizi Mao and Jan Kautz and Mohammad Shoeybi and Song Han},&#xA;      year={2023},&#xA;      eprint={2312.07533},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;: the codebase we built upon. Thanks for their wonderful work.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenGVLab/InternVL&#34;&gt;InternVL&lt;/a&gt;: for open-sourcing InternViT (used in VILA1.5-40b) and the &lt;a href=&#34;https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets&#34;&gt;InternVL-SFT&lt;/a&gt; data blend (inspired by LLaVA-1.6) used in all VILA1.5 models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the amazing open-sourced large language model!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT&#34;&gt;Video-ChatGPT&lt;/a&gt;: we borrowed video evaluation script from this repository.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/allenai/mmc4&#34;&gt;MMC4&lt;/a&gt;, &lt;a href=&#34;https://github.com/kakaobrain/coyo-dataset&#34;&gt;COYO-700M&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/MMInstruction/M3IT&#34;&gt;M3IT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/FLAN&#34;&gt;OpenORCA/FLAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/google-research-datasets/wit&#34;&gt;WIT&lt;/a&gt;, &lt;a href=&#34;https://github.com/OFA-Sys/gsm8k-ScRel/raw/main/data/train_use.jsonl&#34;&gt;GSM8K-ScRel&lt;/a&gt;, &lt;a href=&#34;https://visualgenome.org/api/v0/api_home.html&#34;&gt;VisualGenome&lt;/a&gt;, &lt;a href=&#34;https://visualcommonsense.com/download/&#34;&gt;VCR&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/derek-thomas/ScienceQA&#34;&gt;ScienceQA&lt;/a&gt;, &lt;a href=&#34;https://github.com/bytedance/Shot2Story/raw/master/DATA.md&#34;&gt;Shot2Story&lt;/a&gt;, &lt;a href=&#34;http://youcook2.eecs.umich.edu/&#34;&gt;Youcook2&lt;/a&gt;, &lt;a href=&#34;https://eric-xw.github.io/vatex-website/download.html&#34;&gt;Vatex&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction&#34;&gt;ShareGPT-Video&lt;/a&gt; for providing datasets used in this research.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/graphrag</title>
    <updated>2024-08-25T01:45:54Z</updated>
    <id>tag:github.com,2024-08-25:/microsoft/graphrag</id>
    <link href="https://github.com/microsoft/graphrag" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modular graph-based Retrieval-Augmented Generation (RAG) system&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GraphRAG&lt;/h1&gt; &#xA;&lt;p&gt;👉 &lt;a href=&#34;https://github.com/Azure-Samples/graphrag-accelerator&#34;&gt;Use the GraphRAG Accelerator solution&lt;/a&gt; &lt;br&gt; 👉 &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/&#34;&gt;Microsoft Research Blog Post&lt;/a&gt;&lt;br&gt; 👉 &lt;a href=&#34;https://microsoft.github.io/graphrag&#34;&gt;Read the docs&lt;/a&gt;&lt;br&gt; 👉 &lt;a href=&#34;https://arxiv.org/pdf/2404.16130&#34;&gt;GraphRAG Arxiv&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/graphrag/&#34;&gt; &lt;img alt=&#34;PyPI - Version&#34; src=&#34;https://img.shields.io/pypi/v/graphrag&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/graphrag/&#34;&gt; &lt;img alt=&#34;PyPI - Downloads&#34; src=&#34;https://img.shields.io/pypi/dm/graphrag&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/microsoft/graphrag/issues&#34;&gt; &lt;img alt=&#34;GitHub Issues&#34; src=&#34;https://img.shields.io/github/issues/microsoft/graphrag&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/microsoft/graphrag/discussions&#34;&gt; &lt;img alt=&#34;GitHub Discussions&#34; src=&#34;https://img.shields.io/github/discussions/microsoft/graphrag&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The GraphRAG project is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using the power of LLMs.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about GraphRAG and how it can be used to enhance your LLM&#39;s ability to reason about your private data, please visit the &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/&#34; target=&#34;_blank&#34;&gt;Microsoft Research Blog Post.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To get started with the GraphRAG system we recommend trying the &lt;a href=&#34;https://github.com/Azure-Samples/graphrag-accelerator&#34;&gt;Solution Accelerator&lt;/a&gt; package. This provides a user-friendly end-to-end experience with Azure resources.&lt;/p&gt; &#xA;&lt;h2&gt;Repository Guidance&lt;/h2&gt; &#xA;&lt;p&gt;This repository presents a methodology for using knowledge graph memory structures to enhance LLM outputs. Please note that the provided code serves as a demonstration and is not an officially supported Microsoft offering.&lt;/p&gt; &#xA;&lt;p&gt;⚠️ &lt;em&gt;Warning: GraphRAG indexing can be an expensive operation, please read all of the documentation to understand the process and costs involved, and start small.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Diving Deeper&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To learn about our contribution guidelines, see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/graphrag/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;To start developing &lt;em&gt;GraphRAG&lt;/em&gt;, see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/graphrag/main/DEVELOPING.md&#34;&gt;DEVELOPING.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join the conversation and provide feedback in the &lt;a href=&#34;https://github.com/microsoft/graphrag/discussions&#34;&gt;GitHub Discussions tab!&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prompt Tuning&lt;/h2&gt; &#xA;&lt;p&gt;Using &lt;em&gt;GraphRAG&lt;/em&gt; with your data out of the box may not yield the best possible results. We strongly recommend to fine-tune your prompts following the &lt;a href=&#34;https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/&#34;&gt;Prompt Tuning Guide&lt;/a&gt; in our documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Responsible AI FAQ&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/graphrag/main/RAI_TRANSPARENCY.md&#34;&gt;RAI_TRANSPARENCY.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/graphrag/main/RAI_TRANSPARENCY.md#what-is-graphrag&#34;&gt;What is GraphRAG?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/graphrag/main/RAI_TRANSPARENCY.md#what-can-graphrag-do&#34;&gt;What can GraphRAG do?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/graphrag/main/RAI_TRANSPARENCY.md#what-are-graphrags-intended-uses&#34;&gt;What are GraphRAG’s intended use(s)?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/graphrag/main/RAI_TRANSPARENCY.md#how-was-graphrag-evaluated-what-metrics-are-used-to-measure-performance&#34;&gt;How was GraphRAG evaluated? What metrics are used to measure performance?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/graphrag/main/RAI_TRANSPARENCY.md#what-are-the-limitations-of-graphrag-how-can-users-minimize-the-impact-of-graphrags-limitations-when-using-the-system&#34;&gt;What are the limitations of GraphRAG? How can users minimize the impact of GraphRAG’s limitations when using the system?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/graphrag/main/RAI_TRANSPARENCY.md#what-operational-factors-and-settings-allow-for-effective-and-responsible-use-of-graphrag&#34;&gt;What operational factors and settings allow for effective and responsible use of GraphRAG?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt; &#xA;&lt;h2&gt;Privacy&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://privacy.microsoft.com/en-us/privacystatement&#34;&gt;Microsoft Privacy Statement&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>