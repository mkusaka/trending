<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-16T01:43:55Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>python-poetry/poetry</title>
    <updated>2024-06-16T01:43:55Z</updated>
    <id>tag:github.com,2024-06-16:/python-poetry/poetry</id>
    <link href="https://github.com/python-poetry/poetry" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python packaging and dependency management made easy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Poetry: Python packaging and dependency management made easy&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://python-poetry.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json&#34; alt=&#34;Poetry&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/poetry/#history&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/poetry?label=stable&#34; alt=&#34;Stable Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/poetry/#history&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/python-poetry/poetry?label=pre-release&amp;amp;include_prereleases&amp;amp;sort=semver&#34; alt=&#34;Pre-release Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/poetry/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/poetry&#34; alt=&#34;Python Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/poetry&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/poetry&#34; alt=&#34;Download Stats&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/awxPgve&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/487711540787675139?logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Poetry helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/python-poetry/poetry/master/assets/install.gif&#34; alt=&#34;Poetry Install&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Poetry replaces &lt;code&gt;setup.py&lt;/code&gt;, &lt;code&gt;requirements.txt&lt;/code&gt;, &lt;code&gt;setup.cfg&lt;/code&gt;, &lt;code&gt;MANIFEST.in&lt;/code&gt; and &lt;code&gt;Pipfile&lt;/code&gt; with a simple &lt;code&gt;pyproject.toml&lt;/code&gt; based project format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[tool.poetry]&#xA;name = &#34;my-package&#34;&#xA;version = &#34;0.1.0&#34;&#xA;description = &#34;The description of the package&#34;&#xA;&#xA;license = &#34;MIT&#34;&#xA;&#xA;authors = [&#xA;    &#34;SÃ©bastien Eustace &amp;lt;sebastien@eustace.io&amp;gt;&#34;&#xA;]&#xA;&#xA;repository = &#34;https://github.com/python-poetry/poetry&#34;&#xA;homepage = &#34;https://python-poetry.org&#34;&#xA;&#xA;# README file(s) are used as the package description&#xA;readme = [&#34;README.md&#34;, &#34;LICENSE&#34;]&#xA;&#xA;# Keywords (translated to tags on the package index)&#xA;keywords = [&#34;packaging&#34;, &#34;poetry&#34;]&#xA;&#xA;[tool.poetry.dependencies]&#xA;# Compatible Python versions&#xA;python = &#34;&amp;gt;=3.8&#34;&#xA;# Standard dependency with semver constraints&#xA;aiohttp = &#34;^3.8.1&#34;&#xA;# Dependency with extras&#xA;requests = { version = &#34;^2.28&#34;, extras = [&#34;security&#34;] }&#xA;# Version-specific dependencies with prereleases allowed&#xA;tomli = { version = &#34;^2.0.1&#34;, python = &#34;&amp;lt;3.11&#34;, allow-prereleases = true }&#xA;# Git dependencies&#xA;cleo = { git = &#34;https://github.com/python-poetry/cleo.git&#34;, branch = &#34;main&#34; }&#xA;# Optional dependencies (installed by extras)&#xA;pendulum = { version = &#34;^2.1.2&#34;, optional = true }&#xA;&#xA;# Dependency groups are supported for organizing your dependencies&#xA;[tool.poetry.group.dev.dependencies]&#xA;pytest = &#34;^7.1.2&#34;&#xA;pytest-cov = &#34;^3.0&#34;&#xA;&#xA;# ...and can be installed only when explicitly requested&#xA;[tool.poetry.group.docs]&#xA;optional = true&#xA;[tool.poetry.group.docs.dependencies]&#xA;Sphinx = &#34;^5.1.1&#34;&#xA;&#xA;# Python-style entrypoints and scripts are easily expressed&#xA;[tool.poetry.scripts]&#xA;my-script = &#34;my_package:main&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Poetry supports multiple installation methods, including a simple script found at &lt;a href=&#34;https://install.python-poetry.org&#34;&gt;install.python-poetry.org&lt;/a&gt;. For full installation instructions, including advanced usage of the script, alternate install methods, and CI best practices, see the full &lt;a href=&#34;https://python-poetry.org/docs/#installation&#34;&gt;installation documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://python-poetry.org/docs/&#34;&gt;Documentation&lt;/a&gt; for the current version of Poetry (as well as the development branch and recently out of support versions) is available from the &lt;a href=&#34;https://python-poetry.org&#34;&gt;official website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Poetry is a large, complex project always in need of contributors. For those new to the project, a list of &lt;a href=&#34;https://github.com/python-poetry/poetry/contribute&#34;&gt;suggested issues&lt;/a&gt; to work on in Poetry and poetry-core is available. The full &lt;a href=&#34;https://python-poetry.org/docs/contributing&#34;&gt;contributing documentation&lt;/a&gt; also provides helpful guidance.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/poetry/#history&#34;&gt;Releases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python-poetry.org&#34;&gt;Official Website&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python-poetry.org/docs/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/python-poetry/poetry/issues&#34;&gt;Issue Tracker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.com/invite/awxPgve&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/python-poetry/poetry-core&#34;&gt;poetry-core&lt;/a&gt;: PEP 517 build-system for Poetry projects, and dependency-free core functionality of the Poetry frontend&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/python-poetry/poetry-plugin-export&#34;&gt;poetry-plugin-export&lt;/a&gt;: Export Poetry projects/lock files to foreign formats like requirements.txt&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/python-poetry/poetry-plugin-bundle&#34;&gt;poetry-plugin-bundle&lt;/a&gt;: Install Poetry projects/lock files to external formats like virtual environments&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/python-poetry/install.python-poetry.org&#34;&gt;install.python-poetry.org&lt;/a&gt;: The official Poetry installation script&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/python-poetry/website&#34;&gt;website&lt;/a&gt;: The official Poetry website and blog&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/lerobot</title>
    <updated>2024-06-16T01:43:55Z</updated>
    <id>tag:github.com,2024-06-16:/huggingface/lerobot</id>
    <link href="https://github.com/huggingface/lerobot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ðŸ¤— LeRobot: End-to-end Learning for Real-World Robotics in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;img alt=&#34;LeRobot, Hugging Face Robotics Library&#34; src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA; &lt;/picture&gt; &lt;br&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/huggingface/lerobot&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/lerobot&#34; alt=&#34;Python versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/status/lerobot&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/lerobot&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/tree/main/examples&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Examples-green.svg?sanitize=true&#34; alt=&#34;Examples&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/s3KuuzsPFb&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p&gt;State-of-the-art Machine Learning for real-world robotics&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;ðŸ¤— LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ¤— LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ¤— LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ¤— LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href=&#34;https://huggingface.co/lerobot&#34;&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/aloha_act.gif&#34; width=&#34;100%&#34; alt=&#34;ACT policy on ALOHA env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/simxarm_tdmpc.gif&#34; width=&#34;100%&#34; alt=&#34;TDMPC policy on SimXArm env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/pusht_diffusion.gif&#34; width=&#34;100%&#34; alt=&#34;Diffusion policy on PushT env&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACT policy on ALOHA env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TDMPC policy on SimXArm env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Diffusion policy on PushT env&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Acknowledgment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to Tony Zaho, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href=&#34;https://tonyzhaozh.github.io/aloha&#34;&gt;ALOHA&lt;/a&gt; and &lt;a href=&#34;https://mobile-aloha.github.io&#34;&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu&#34;&gt;Diffusion Policy&lt;/a&gt; and &lt;a href=&#34;https://umi-gripper.github.io&#34;&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href=&#34;https://github.com/nicklashansen/tdmpc&#34;&gt;TDMPC&lt;/a&gt; and &lt;a href=&#34;https://www.yunhaifeng.com/FOWM&#34;&gt;FOWM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download our source code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/huggingface/lerobot.git &amp;amp;&amp;amp; cd lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href=&#34;https://docs.anaconda.com/free/miniconda/index.html&#34;&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -y -n lerobot python=3.10 &amp;amp;&amp;amp; conda activate lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install ðŸ¤— LeRobot:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Depending on your platform, If you encounter any build errors during this step you may need to install &lt;code&gt;cmake&lt;/code&gt; and &lt;code&gt;build-essential&lt;/code&gt; for building some of our dependencies. On linux: &lt;code&gt;sudo apt-get install cmake build-essential&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For simulations, ðŸ¤— LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-aloha&#34;&gt;aloha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-xarm&#34;&gt;xarm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-pusht&#34;&gt;pusht&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For instance, to install ðŸ¤— LeRobot with aloha and pusht, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;.[aloha, pusht]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://docs.wandb.ai/quickstart&#34;&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(note: you will also need to enable WandB in the configuration. See below.)&lt;/p&gt; &#xA;&lt;h2&gt;Walkthrough&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;â”œâ”€â”€ examples             # contains demonstration examples, start here to learn about LeRobot&#xA;|   â””â”€â”€ advanced         # contains even more examples for those who have mastered the basics&#xA;â”œâ”€â”€ lerobot&#xA;|   â”œâ”€â”€ configs          # contains hydra yaml files with all options that you can override in the command line&#xA;|   |   â”œâ”€â”€ default.yaml   # selected by default, it loads pusht environment and diffusion policy&#xA;|   |   â”œâ”€â”€ env            # various sim environments and their datasets: aloha.yaml, pusht.yaml, xarm.yaml&#xA;|   |   â””â”€â”€ policy         # various policies: act.yaml, diffusion.yaml, tdmpc.yaml&#xA;|   â”œâ”€â”€ common           # contains classes and utilities&#xA;|   |   â”œâ”€â”€ datasets       # various datasets of human demonstrations: aloha, pusht, xarm&#xA;|   |   â”œâ”€â”€ envs           # various sim environments: aloha, pusht, xarm&#xA;|   |   â”œâ”€â”€ policies       # various policies: act, diffusion, tdmpc&#xA;|   |   â””â”€â”€ utils          # various utilities&#xA;|   â””â”€â”€ scripts          # contains functions to execute via command line&#xA;|       â”œâ”€â”€ eval.py                 # load policy and evaluate it on an environment&#xA;|       â”œâ”€â”€ train.py                # train a policy via imitation learning and/or reinforcement learning&#xA;|       â”œâ”€â”€ push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub&#xA;|       â””â”€â”€ visualize_dataset.py    # load a dataset and render its demonstrations&#xA;â”œâ”€â”€ outputs               # contains results of scripts execution: logs, videos, model checkpoints&#xA;â””â”€â”€ tests                 # contains pytest utilities for continuous integration&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualize datasets&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&#34;&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically download data from the Hugging Face hub.&lt;/p&gt; &#xA;&lt;p&gt;You can also locally visualize episodes from a dataset by executing our script from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/visualize_dataset.py \&#xA;    --repo-id lerobot/pusht \&#xA;    --episode-index 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&#34;&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;python lerobot/scripts/visualize_dataset.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluate a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/2_evaluate_pretrained_policy.py&#34;&gt;example 2&lt;/a&gt; that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py \&#xA;    -p lerobot/diffusion_pusht \&#xA;    eval.n_episodes=10 \&#xA;    eval.batch_size=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: After training your own policy, you can re-evaluate the checkpoints with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py -p {OUTPUT_DIR}/checkpoints/last/pretrained_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Train your own policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/3_train_policy.py&#34;&gt;example 3&lt;/a&gt; that illustrates how to train a model using our core library in python, and &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md&#34;&gt;example 4&lt;/a&gt; that shows how to use our training script from command line.&lt;/p&gt; &#xA;&lt;p&gt;In general, you can use our training script to easily train any policy. Here is an example of training the ACT policy on trajectories collected by humans on the Aloha simulation environment for the insertion task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/train.py \&#xA;    policy=act \&#xA;    env=aloha \&#xA;    env.task=AlohaInsertion-v0 \&#xA;    dataset_repo_id=lerobot/aloha_sim_insertion_human \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The experiment directory is automatically generated and will show up in yellow in your terminal. It looks like &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default&lt;/code&gt;. You can manually specify an experiment directory by adding this argument to the &lt;code&gt;train.py&lt;/code&gt; python command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    hydra.run.dir=your/new/experiment/dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the experiment directory there will be a folder called &lt;code&gt;checkpoints&lt;/code&gt; which will have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;checkpoints&#xA;â”œâ”€â”€ 000250  # checkpoint_dir for training step 250&#xA;â”‚   â”œâ”€â”€ pretrained_model  # Hugging Face pretrained model dir&#xA;â”‚   â”‚   â”œâ”€â”€ config.json  # Hugging Face pretrained model config&#xA;â”‚   â”‚   â”œâ”€â”€ config.yaml  # consolidated Hydra config&#xA;â”‚   â”‚   â”œâ”€â”€ model.safetensors  # model weights&#xA;â”‚   â”‚   â””â”€â”€ README.md  # Hugging Face model card&#xA;â”‚   â””â”€â”€ training_state.pth  # optimizer/scheduler/rng state and training step&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use wandb for logging training and evaluation curves, make sure you&#39;ve run &lt;code&gt;wandb login&lt;/code&gt; as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    wandb.enable=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use &lt;code&gt;eval.n_episodes=500&lt;/code&gt; to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h4&gt;Reproduce state-of-the-art (SOTA)&lt;/h4&gt; &#xA;&lt;p&gt;We have organized our configuration files (found under &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/lerobot/configs&#34;&gt;&lt;code&gt;lerobot/configs&lt;/code&gt;&lt;/a&gt;) such that they reproduce SOTA results from a given model variant in their respective original works. Simply running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/train.py policy=diffusion env=pusht&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;reproduces SOTA results for Diffusion Policy on the PushT task.&lt;/p&gt; &#xA;&lt;p&gt;Pretrained policies, along with reproduction details, can be found under the &#34;Models&#34; section of &lt;a href=&#34;https://huggingface.co/lerobot&#34;&gt;https://huggingface.co/lerobot&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to contribute to ðŸ¤— LeRobot, please check out our &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Add a new dataset&lt;/h3&gt; &#xA;&lt;p&gt;To add a dataset to the hub, you need to login using a write-access token, which can be generated from the &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;Hugging Face settings&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then point to your raw dataset folder (e.g. &lt;code&gt;data/aloha_static_pingpong_test_raw&lt;/code&gt;), and push your dataset to the hub with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/push_dataset_to_hub.py \&#xA;--raw-dir data/aloha_static_pingpong_test_raw \&#xA;--out-dir data \&#xA;--repo-id lerobot/aloha_static_pingpong_test \&#xA;--raw-format aloha_hdf5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;python lerobot/scripts/push_dataset_to_hub.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;p&gt;If your dataset format is not supported, implement your own in &lt;code&gt;lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py&lt;/code&gt; by copying examples like &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py&#34;&gt;pusht_zarr&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py&#34;&gt;umi_zarr&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py&#34;&gt;aloha_hdf5&lt;/a&gt;, or &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py&#34;&gt;xarm_pkl&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You first need to find the checkpoint folder located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). Within that there is a &lt;code&gt;pretrained_model&lt;/code&gt; directory which should contain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy&#39;s dataclass config).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href=&#34;https://huggingface.co/docs/safetensors/index&#34;&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config.yaml&lt;/code&gt;: A consolidated Hydra training configuration containing the policy, environment, and dataset configs. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. The environment config is useful for anyone who wants to evaluate your policy. The dataset config just serves as a paper trail for reproducibility.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/scripts/eval.py&#34;&gt;eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; &#xA;&lt;h3&gt;Improve your code with profiling&lt;/h3&gt; &#xA;&lt;p&gt;An example of a code snippet to profile the evaluation of a policy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.profiler import profile, record_function, ProfilerActivity&#xA;&#xA;def trace_handler(prof):&#xA;    prof.export_chrome_trace(f&#34;tmp/trace_schedule_{prof.step_num}.json&#34;)&#xA;&#xA;with profile(&#xA;    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],&#xA;    schedule=torch.profiler.schedule(&#xA;        wait=2,&#xA;        warmup=2,&#xA;        active=3,&#xA;    ),&#xA;    on_trace_ready=trace_handler&#xA;) as prof:&#xA;    with record_function(&#34;eval_policy&#34;):&#xA;        for i in range(num_episodes):&#xA;            prof.step()&#xA;            # insert code to profile, potentially whole body of eval_policy function&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{cadene2024lerobot,&#xA;    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},&#xA;    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},&#xA;    howpublished = &#34;\url{https://github.com/huggingface/lerobot}&#34;,&#xA;    year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>karpathy/nanoGPT</title>
    <updated>2024-06-16T01:43:55Z</updated>
    <id>tag:github.com,2024-06-16:/karpathy/nanoGPT</id>
    <link href="https://github.com/karpathy/nanoGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoGPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/nanogpt.jpg&#34; alt=&#34;nanoGPT&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of &lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;minGPT&lt;/a&gt; that prioritizes teeth over education. Still under active development, but currently the file &lt;code&gt;train.py&lt;/code&gt; reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: &lt;code&gt;train.py&lt;/code&gt; is a ~300-line boilerplate training loop and &lt;code&gt;model.py&lt;/code&gt; a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That&#39;s it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/gpt2_124M_loss.png&#34; alt=&#34;repro124m&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).&lt;/p&gt; &#xA;&lt;h2&gt;install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install torch numpy transformers datasets tiktoken wandb tqdm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org&#34;&gt;pytorch&lt;/a&gt; &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://numpy.org/install/&#34;&gt;numpy&lt;/a&gt; &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; for huggingface transformers &amp;lt;3 (to load GPT-2 checkpoints)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;datasets&lt;/code&gt; for huggingface datasets &amp;lt;3 (if you want to download + preprocess OpenWebText)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tiktoken&lt;/code&gt; for OpenAI&#39;s fast BPE code &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;wandb&lt;/code&gt; for optional logging &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt; for progress bars &amp;lt;3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;quick start&lt;/h2&gt; &#xA;&lt;p&gt;If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python data/shakespeare_char/prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This creates a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I have a GPU&lt;/strong&gt;. Great, we can quickly train a baby GPT with the settings provided in the &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nanoGPT/master/config/train_shakespeare_char.py&#34;&gt;config/train_shakespeare_char.py&lt;/a&gt; config file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python train.py config/train_shakespeare_char.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you peek inside it, you&#39;ll see that we&#39;re training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the &lt;code&gt;--out_dir&lt;/code&gt; directory &lt;code&gt;out-shakespeare-char&lt;/code&gt;. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python sample.py --out_dir=out-shakespeare-char&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This generates a few samples, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ANGELO:&#xA;And cowards it be strawn to my bed,&#xA;And thrust the gates of my threats,&#xA;Because he that ale away, and hang&#39;d&#xA;An one with him.&#xA;&#xA;DUKE VINCENTIO:&#xA;I thank your eyes against it.&#xA;&#xA;DUKE VINCENTIO:&#xA;Then will answer him to save the malm:&#xA;And what have you tyrannous shall do this?&#xA;&#xA;DUKE VINCENTIO:&#xA;If you have done evils of all disposition&#xA;To end his power, the day of thrust for a common men&#xA;That I leave, to fight with over-liking&#xA;Hasting in a roseman.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;lol &lt;code&gt;Â¯\_(ãƒ„)_/Â¯&lt;/code&gt;. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I only have a macbook&lt;/strong&gt; (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly (&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;select it here&lt;/a&gt; when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, since we are running on CPU instead of GPU we must set both &lt;code&gt;--device=cpu&lt;/code&gt; and also turn off PyTorch 2.0 compile with &lt;code&gt;--compile=False&lt;/code&gt;. Then when we evaluate we get a bit more noisy but faster estimate (&lt;code&gt;--eval_iters=20&lt;/code&gt;, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We&#39;ll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with &lt;code&gt;--lr_decay_iters&lt;/code&gt;). Because our network is so small we also ease down on regularization (&lt;code&gt;--dropout=0.0&lt;/code&gt;). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it&#39;s still good fun:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python sample.py --out_dir=out-shakespeare-char --device=cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generates samples like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;GLEORKEN VINGHARD III:&#xA;Whell&#39;s the couse, the came light gacks,&#xA;And the for mought you in Aut fries the not high shee&#xA;bot thou the sought bechive in that to doth groan you,&#xA;No relving thee post mose the wear&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you&#39;re willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (&lt;code&gt;--block_size&lt;/code&gt;), the length of training, etc.&lt;/p&gt; &#xA;&lt;p&gt;Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add &lt;code&gt;--device=mps&lt;/code&gt; (short for &#34;Metal Performance Shaders&#34;); PyTorch then uses the on-chip GPU that can &lt;em&gt;significantly&lt;/em&gt; accelerate training (2-3X) and allow you to use larger networks. See &lt;a href=&#34;https://github.com/karpathy/nanoGPT/issues/28&#34;&gt;Issue 28&lt;/a&gt; for more.&lt;/p&gt; &#xA;&lt;h2&gt;reproducing GPT-2&lt;/h2&gt; &#xA;&lt;p&gt;A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the &lt;a href=&#34;https://openwebtext2.readthedocs.io/en/latest/&#34;&gt;OpenWebText&lt;/a&gt;, an open reproduction of OpenAI&#39;s (private) WebText:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python data/openwebtext/prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This downloads and tokenizes the &lt;a href=&#34;https://huggingface.co/datasets/openwebtext&#34;&gt;OpenWebText&lt;/a&gt; dataset. It will create a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we&#39;re ready to kick off training. To reproduce GPT-2 (124M) you&#39;ll want at least an 8X A100 40GB node and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run on the first (master) node with example IP 123.456.123.456:&#xA;torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py&#xA;# Run on the worker node:&#xA;torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don&#39;t have Infiniband then also prepend &lt;code&gt;NCCL_IB_DISABLE=1&lt;/code&gt; to the above launches. Your multinode training will work, but most likely &lt;em&gt;crawl&lt;/em&gt;. By default checkpoints are periodically written to the &lt;code&gt;--out_dir&lt;/code&gt;. We can sample from the model by simply &lt;code&gt;python sample.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Finally, to train on a single GPU simply run the &lt;code&gt;python train.py&lt;/code&gt; script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You&#39;ll most likely want to tune a number of those variables depending on your needs.&lt;/p&gt; &#xA;&lt;h2&gt;baselines&lt;/h2&gt; &#xA;&lt;p&gt;OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ python train.py config/eval_gpt2.py&#xA;$ python train.py config/eval_gpt2_medium.py&#xA;$ python train.py config/eval_gpt2_large.py&#xA;$ python train.py config/eval_gpt2_xl.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and observe the following losses on train and val:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;params&lt;/th&gt; &#xA;   &lt;th&gt;train loss&lt;/th&gt; &#xA;   &lt;th&gt;val loss&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2&lt;/td&gt; &#xA;   &lt;td&gt;124M&lt;/td&gt; &#xA;   &lt;td&gt;3.11&lt;/td&gt; &#xA;   &lt;td&gt;3.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-medium&lt;/td&gt; &#xA;   &lt;td&gt;350M&lt;/td&gt; &#xA;   &lt;td&gt;2.85&lt;/td&gt; &#xA;   &lt;td&gt;2.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-large&lt;/td&gt; &#xA;   &lt;td&gt;774M&lt;/td&gt; &#xA;   &lt;td&gt;2.66&lt;/td&gt; &#xA;   &lt;td&gt;2.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-xl&lt;/td&gt; &#xA;   &lt;td&gt;1558M&lt;/td&gt; &#xA;   &lt;td&gt;2.56&lt;/td&gt; &#xA;   &lt;td&gt;2.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.&lt;/p&gt; &#xA;&lt;h2&gt;finetuning&lt;/h2&gt; &#xA;&lt;p&gt;Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to &lt;code&gt;data/shakespeare&lt;/code&gt; and run &lt;code&gt;prepare.py&lt;/code&gt; to download the tiny shakespeare dataset and render it into a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt;, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python train.py config/finetune_shakespeare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will load the config parameter overrides in &lt;code&gt;config/finetune_shakespeare.py&lt;/code&gt; (I didn&#39;t tune them much though). Basically, we initialize from a GPT2 checkpoint with &lt;code&gt;init_from&lt;/code&gt; and train as normal, except shorter and with a small learning rate. If you&#39;re running out of memory try decreasing the model size (they are &lt;code&gt;{&#39;gpt2&#39;, &#39;gpt2-medium&#39;, &#39;gpt2-large&#39;, &#39;gpt2-xl&#39;}&lt;/code&gt;) or possibly decreasing the &lt;code&gt;block_size&lt;/code&gt; (context length). The best checkpoint (lowest validation loss) will be in the &lt;code&gt;out_dir&lt;/code&gt; directory, e.g. in &lt;code&gt;out-shakespeare&lt;/code&gt; by default, per the config file. You can then run the code in &lt;code&gt;sample.py --out_dir=out-shakespeare&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;THEODORE:&#xA;Thou shalt sell me to the highest bidder: if I die,&#xA;I sell thee to the first; if I go mad,&#xA;I sell thee to the second; if I&#xA;lie, I sell thee to the third; if I slay,&#xA;I sell thee to the fourth: so buy or sell,&#xA;I tell thee again, thou shalt not sell my&#xA;possession.&#xA;&#xA;JULIET:&#xA;And if thou steal, thou shalt not sell thyself.&#xA;&#xA;THEODORE:&#xA;I do not steal; I sell the stolen goods.&#xA;&#xA;THEODORE:&#xA;Thou know&#39;st not what thou sell&#39;st; thou, a woman,&#xA;Thou art ever a victim, a thing of no worth:&#xA;Thou hast no right, no right, but to be sold.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Whoa there, GPT, entering some dark place over there. I didn&#39;t really tune the hyperparameters in the config too much, feel free to try!&lt;/p&gt; &#xA;&lt;h2&gt;sampling / inference&lt;/h2&gt; &#xA;&lt;p&gt;Use the script &lt;code&gt;sample.py&lt;/code&gt; to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available &lt;code&gt;gpt2-xl&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python sample.py \&#xA;    --init_from=gpt2-xl \&#xA;    --start=&#34;What is the answer to life, the universe, and everything?&#34; \&#xA;    --num_samples=5 --max_new_tokens=100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to sample from a model you trained, use the &lt;code&gt;--out_dir&lt;/code&gt; to point the code appropriately. You can also prompt the model with some text from a file, e.g. &lt;code&gt;python sample.py --start=FILE:prompt.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;efficiency notes&lt;/h2&gt; &#xA;&lt;p&gt;For simple model benchmarking and profiling, &lt;code&gt;bench.py&lt;/code&gt; might be useful. It&#39;s identical to what happens in the meat of the training loop of &lt;code&gt;train.py&lt;/code&gt;, but omits much of the other complexities.&lt;/p&gt; &#xA;&lt;p&gt;Note that the code by default uses &lt;a href=&#34;https://pytorch.org/get-started/pytorch-2.0/&#34;&gt;PyTorch 2.0&lt;/a&gt;. At the time of writing (Dec 29, 2022) this makes &lt;code&gt;torch.compile()&lt;/code&gt; available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!&lt;/p&gt; &#xA;&lt;h2&gt;todos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Investigate and add FSDP instead of DDP&lt;/li&gt; &#xA; &lt;li&gt;Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)&lt;/li&gt; &#xA; &lt;li&gt;Finetune the finetuning script, I think the hyperparams are not great&lt;/li&gt; &#xA; &lt;li&gt;Schedule for linear batch size increase during training&lt;/li&gt; &#xA; &lt;li&gt;Incorporate other embeddings (rotary, alibi)&lt;/li&gt; &#xA; &lt;li&gt;Separate out the optim buffers from model params in checkpoints I think&lt;/li&gt; &#xA; &lt;li&gt;Additional logging around network health (e.g. gradient clip events, magnitudes)&lt;/li&gt; &#xA; &lt;li&gt;Few more investigations around better init etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Note that by default this repo uses PyTorch 2.0 (i.e. &lt;code&gt;torch.compile&lt;/code&gt;). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you&#39;re running into related error messages try to disable this by adding &lt;code&gt;--compile=False&lt;/code&gt; flag. This will slow down the code but at least it will run.&lt;/p&gt; &#xA;&lt;p&gt;For some context on this repository, GPT, and language modeling it might be helpful to watch my &lt;a href=&#34;https://karpathy.ai/zero-to-hero.html&#34;&gt;Zero To Hero series&lt;/a&gt;. Specifically, the &lt;a href=&#34;https://www.youtube.com/watch?v=kCc8FmEb1nY&#34;&gt;GPT video&lt;/a&gt; is popular if you have some prior language modeling context.&lt;/p&gt; &#xA;&lt;p&gt;For more questions/discussions feel free to stop by &lt;strong&gt;#nanoGPT&lt;/strong&gt; on Discord:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/3zy8kqD9Cp&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;amp;style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;All nanoGPT experiments are powered by GPUs on &lt;a href=&#34;https://lambdalabs.com&#34;&gt;Lambda labs&lt;/a&gt;, my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!&lt;/p&gt;</summary>
  </entry>
</feed>