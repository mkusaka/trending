<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-21T03:34:05Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dvlab-research/MiniGemini</title>
    <updated>2024-04-21T03:34:05Z</updated>
    <id>tag:github.com,2024-04-21:/dvlab-research/MiniGemini</id>
    <link href="https://github.com/dvlab-research/MiniGemini" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation for Mini-Gemini&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mini-gemini.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://103.170.5.190:7860/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Demo-violet&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/wcy1122/Mini-Gemini&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/🤗-Open%20In%20Spaces-blue.svg&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.18814.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-green&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B with image understanding, reasoning, and generation simultaneously. We build this repo based on LLaVA.&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[04/15] 🔥 The &lt;a href=&#34;https://huggingface.co/spaces/wcy1122/Mini-Gemini&#34;&gt;Hugging Face demo&lt;/a&gt; is available. It&#39;s a 13B-HD version, welcome to watch and try.&lt;/li&gt; &#xA; &lt;li&gt;[03/28] 🔥 Mini-Gemini is coming! We release the &lt;a href=&#34;https://arxiv.org/pdf/2403.18814.pdf&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;http://103.170.5.190:7860/&#34;&gt;demo&lt;/a&gt;, &lt;a href=&#34;https://github.com/dvlab-research/MiniGemini&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854&#34;&gt;models&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e&#34;&gt;data&lt;/a&gt; for Mini-Gemini!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#model&#34;&gt;Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#preparation&#34;&gt;Preparation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#train&#34;&gt;Train&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;We provide some selected examples in this section. More examples can be found in our &lt;a href=&#34;https://mini-gemini.github.io/&#34;&gt;project page&lt;/a&gt;. Feel free to try our online &lt;a href=&#34;http://103.170.5.190:7860/&#34;&gt;demo&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;100%&#34; src=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/teaser.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the instructions below to install the required packages.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: If you want to use Mini-Gemini-2B, please ensure to install the latest version Transformers (&amp;gt;=4.38.0).&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/dvlab-research/MiniGemini.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n minigemini python=3.10 -y&#xA;conda activate minigemini&#xA;cd MiniGemini&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install additional packages for training cases&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ninja&#xA;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;The framework of Mini-Gemini is conceptually simple: dual vision encoders are utilized to provide low-resolution visual embedding and high-resolution candidates; patch info mining is proposed to conduct patch-level mining between high-resolution regions and low-resolution visual queries; LLM is utilized to marry text with images for both comprehension and generation at the same time.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;98%&#34; src=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/pipeline.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We provide all our fully finetuned models on Stage 1 and 2 data for Mini-Gemini:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LR&lt;/th&gt; &#xA;   &lt;th&gt;HR&lt;/th&gt; &#xA;   &lt;th&gt;Base LLM&lt;/th&gt; &#xA;   &lt;th&gt;Vision Encoder&lt;/th&gt; &#xA;   &lt;th&gt;Finetuning Data&lt;/th&gt; &#xA;   &lt;th&gt;Finetuning schedule&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-2B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Gemma-2B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-2B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-13B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-8x7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-34B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B-HD&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;1536&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-7B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B-HD&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;1536&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-13B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B-HD&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;1536&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-8x7B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B-HD&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;1536&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-34B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Here are the pretrained weights on Stage 1 data only:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LR&lt;/th&gt; &#xA;   &lt;th&gt;HR&lt;/th&gt; &#xA;   &lt;th&gt;Base LLM&lt;/th&gt; &#xA;   &lt;th&gt;Vision Encoder&lt;/th&gt; &#xA;   &lt;th&gt;Pretrain Data&lt;/th&gt; &#xA;   &lt;th&gt;Finetuning schedule&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-2B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Gemma-2B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-2B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-13B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-8x7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-34B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;p&gt;We provide the processed data for Mini-Gemini training. For model pretraining, please download the following the training image-based data and organize them as:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;-&amp;gt;&lt;/code&gt; means put the data in the local folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain&#34;&gt;LLaVA Images&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Pretrain/images&lt;/code&gt;, &lt;code&gt;data/MiniGemini-Finetune/llava/LLaVA-Pretrain/images&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FreedomIntelligence/ALLaVA&#34;&gt;ALLaVA Caption&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Pretrain/ALLaVA-4V&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For model finetuning, please download the following the instruction data and organize them as:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;-&amp;gt;&lt;/code&gt; means put the data in the local folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://images.cocodataset.org/zips/train2017.zip&#34;&gt;COCO train2017&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/coco&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip&#34;&gt;GQA&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/gqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing&#34;&gt;OCR-VQA&lt;/a&gt; (&lt;strong&gt;we save all files as &lt;code&gt;.jpg&lt;/code&gt;&lt;/strong&gt;) -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/ocr_vqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip&#34;&gt;TextVQA&lt;/a&gt; (not included for training) -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/textvqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip&#34;&gt;VisualGenome part1&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip&#34;&gt;VisualGenome part2&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/vg&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/raw/main/projects/ShareGPT4V/docs/Data.md&#34;&gt;ShareGPT4V-100K&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/sam&lt;/code&gt;, &lt;code&gt;share_textvqa&lt;/code&gt;, &lt;code&gt;wikiart&lt;/code&gt;, &lt;code&gt;web-celebrity&lt;/code&gt;, &lt;code&gt;web-landmark&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/laion/gpt4v-dataset&#34;&gt;LAION GPT4V&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/gpt4v-dataset&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FreedomIntelligence/ALLaVA&#34;&gt;ALLaVA Instruction&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Pretrain/ALLaVA-4V&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.docvqa.org/datasets/docvqa&#34;&gt;DocVQA&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/docvqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vis-nlp/ChartQA&#34;&gt;ChartQA&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/chartqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kushalkafle/DVQA_dataset&#34;&gt;DVQA&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/dvqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://allenai.org/data/diagrams&#34;&gt;AI2D&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/ai2d&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For model evaluation, please follow this &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;link&lt;/a&gt; for preparation. We use some extra benchmarks for evaluation. please download the following the training image-based data and organize them as:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;-&amp;gt;&lt;/code&gt; means put the data in the local folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mmmu-benchmark.github.io/&#34;&gt;MMMU&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Eval/MMMU&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-compass/mmbench/&#34;&gt;MMB&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Eval/MMB&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mathvista.github.io/&#34;&gt;MathVista&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Eval/MathVista&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please put the pretrained data, finetuned data, and eval data in &lt;code&gt;MiniGemini-Pretrain&lt;/code&gt;, &lt;code&gt;MiniGemini-Finetune&lt;/code&gt;, and &lt;code&gt;MiniGemini-Eval&lt;/code&gt; subset following &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#structure&#34;&gt;Structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For meta info, please download the following files and organize them as in &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#structure&#34;&gt;Structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data file name&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/YanweiLi/Mini-Gemini-Pretrain&#34;&gt;minigemini_pretrain.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.68 G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/YanweiLi/Mini-Gemini-Instruction&#34;&gt;minigemini_instruction.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.79 G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/YanweiLi/Mini-Gemini-Instruction/blob/main/minigemini_generation_pure_text.json&#34;&gt;minigemini_generation_pure_text.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.04 G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;IMPORTANT: &lt;code&gt;minigemini_generation_pure_text.json&lt;/code&gt; is a generation-related subset. &lt;strong&gt;DO NOT&lt;/strong&gt; merge it with &lt;code&gt;minigemini_instruction.json&lt;/code&gt; as it is already included in it. You may merge this file with your customized LLM/VLM SFT dataset to enable the reasoning generation ability.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Weights&lt;/h3&gt; &#xA;&lt;p&gt;We recommend users to download the pretrained weights from the following link &lt;a href=&#34;https://huggingface.co/openai/clip-vit-large-patch14-336&#34;&gt;CLIP-Vit-L-336&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup&#34;&gt;OpenCLIP-ConvNeXt-L&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/google/gemma-2b-it&#34;&gt;Gemma-2b-it&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/lmsys/vicuna-7b-v1.5&#34;&gt;Vicuna-7b-v1.5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/lmsys/vicuna-13b-v1.5&#34;&gt;Vicuna-13b-v1.5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;&gt;Mixtral-8x7B-Instruct-v0.1&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B&#34;&gt;Nous-Hermes-2-Yi-34B&lt;/a&gt; , and put them in &lt;code&gt;model_zoo&lt;/code&gt; following &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#structure&#34;&gt;Structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Structure&lt;/h3&gt; &#xA;&lt;p&gt;The folder structure should be organized as follows before training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MiniGemini&#xA;├── minigemini&#xA;├── scripts&#xA;├── work_dirs&#xA;│   ├── Mini-Gemini&#xA;│   │   ├── Mini-Gemini-2B&#xA;│   │   ├── ...&#xA;├── model_zoo&#xA;│   ├── LLM&#xA;│   │   ├── gemma&#xA;│   │   │   ├── gemma-2b-it&#xA;│   │   ├── vicuna&#xA;│   │   │   ├── 7B-V1.5&#xA;│   │   │   ├── 13B-V1.5&#xA;│   │   ├── mixtral&#xA;│   │   │   ├── Mixtral-8x7B-Instruct-v0.1&#xA;│   │   ├── Nous-Hermes-2-Yi-34B&#xA;│   ├── OpenAI&#xA;│   │   ├── clip-vit-large-patch14-336&#xA;│   │   ├── openclip-convnext-large-d-320-laion2B-s29B-b131K-ft-soup&#xA;├── data&#xA;│   ├── MiniGemini-Pretrain&#xA;│   │   ├── minigemini_pretrain.json&#xA;│   │   ├── images&#xA;│   │   ├── ALLaVA-4V&#xA;│   ├── MiniGemini-Finetune&#xA;│   │   ├── minigemini_instruction.json&#xA;│   │   ├── llava&#xA;│   │   ├── coco&#xA;│   │   ├── gqa&#xA;│   │   ├── ocr_vqa&#xA;│   │   ├── textvqa&#xA;│   │   ├── vg&#xA;│   │   ├── gpt4v-dataset&#xA;│   │   ├── sam&#xA;│   │   ├── share_textvqa&#xA;│   │   ├── wikiart&#xA;│   │   ├── web-celebrity&#xA;│   │   ├── web-landmark&#xA;│   │   ├── ALLaVA-4V&#xA;│   │   ├── docvqa&#xA;│   │   ├── chartqa&#xA;│   │   ├── dvqa&#xA;│   │   ├── ai2d&#xA;│   ├── MiniGemini-Eval&#xA;│   │   ├── MMMU&#xA;│   │   ├── MMB&#xA;│   │   ├── MathVista&#xA;│   │   ├── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;p&gt;Mini-Gemini training consists of two stages: (1) feature alignment stage: bridge the vision and language tokens; (2) instruction tuning stage: teach the model to follow multimodal instructions.&lt;/p&gt; &#xA;&lt;p&gt;Mini-Gemini is trained on 8 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly. Always keep the global batch size the same: &lt;code&gt;per_device_train_batch_size&lt;/code&gt; x &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; x &lt;code&gt;num_gpus&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please make sure you download and organize the data following &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#preparation&#34;&gt;Preparation&lt;/a&gt; before training.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: Please set &lt;code&gt;hostfile&lt;/code&gt; for 2 machine training and &lt;code&gt;hostfile_4&lt;/code&gt; for 4 machine training.&lt;/p&gt; &#xA;&lt;p&gt;If you want to train and finetune Mini-Gemini, please run the following command for Mini-Gemini-7B with image size 336:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/llama/train/stage_1_2_full_v7b_336_hr_768.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for Mini-Gemini-13B with image size 336:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/llama/train/stage_1_2_full_v13b_336_hr_768.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Because we reuse the pre-trained projecter weights from the Mini-Gemini-7B, you can directly use the Mini-Gemini-7B-HD with image size 672 for stage-2 instruction tuning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/llama/train/stage_2_full_v7b_672_hr_1536.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please find more training scripts of &lt;code&gt;gemma&lt;/code&gt;, &lt;code&gt;llama&lt;/code&gt;, &lt;code&gt;mixtral&lt;/code&gt;, and &lt;code&gt;yi&lt;/code&gt; in &lt;code&gt;scripts/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We perform evaluation on several image-based benchmarks. Please download the evaluation data following &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#preparation&#34;&gt;Preparation&lt;/a&gt; and organize them as in &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#structure&#34;&gt;Structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LLM&lt;/th&gt; &#xA;   &lt;th&gt;Res.&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;TextVQA&lt;/th&gt; &#xA;   &lt;th&gt;MMB&lt;/th&gt; &#xA;   &lt;th&gt;MME&lt;/th&gt; &#xA;   &lt;th&gt;MM-Vet&lt;/th&gt; &#xA;   &lt;th&gt;MMMU_val&lt;/th&gt; &#xA;   &lt;th&gt;MMMU_test&lt;/th&gt; &#xA;   &lt;th&gt;MathVista&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-2B&lt;/td&gt; &#xA;   &lt;td&gt;Gemma-2B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-2B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;56.2&lt;/td&gt; &#xA;   &lt;td&gt;59.8&lt;/td&gt; &#xA;   &lt;td&gt;1341/312&lt;/td&gt; &#xA;   &lt;td&gt;31.1&lt;/td&gt; &#xA;   &lt;td&gt;31.7&lt;/td&gt; &#xA;   &lt;td&gt;29.1&lt;/td&gt; &#xA;   &lt;td&gt;29.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;65.2&lt;/td&gt; &#xA;   &lt;td&gt;69.3&lt;/td&gt; &#xA;   &lt;td&gt;1523/316&lt;/td&gt; &#xA;   &lt;td&gt;40.8&lt;/td&gt; &#xA;   &lt;td&gt;36.1&lt;/td&gt; &#xA;   &lt;td&gt;32.8&lt;/td&gt; &#xA;   &lt;td&gt;31.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-13B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;65.9&lt;/td&gt; &#xA;   &lt;td&gt;68.5&lt;/td&gt; &#xA;   &lt;td&gt;1565/322&lt;/td&gt; &#xA;   &lt;td&gt;46.0&lt;/td&gt; &#xA;   &lt;td&gt;38.1&lt;/td&gt; &#xA;   &lt;td&gt;33.5&lt;/td&gt; &#xA;   &lt;td&gt;37.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-8x7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;69.2&lt;/td&gt; &#xA;   &lt;td&gt;75.6&lt;/td&gt; &#xA;   &lt;td&gt;1639/379&lt;/td&gt; &#xA;   &lt;td&gt;45.8&lt;/td&gt; &#xA;   &lt;td&gt;41.8&lt;/td&gt; &#xA;   &lt;td&gt;37.1&lt;/td&gt; &#xA;   &lt;td&gt;41.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-34B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;70.1&lt;/td&gt; &#xA;   &lt;td&gt;79.6&lt;/td&gt; &#xA;   &lt;td&gt;1666/439&lt;/td&gt; &#xA;   &lt;td&gt;53.0&lt;/td&gt; &#xA;   &lt;td&gt;48.7&lt;/td&gt; &#xA;   &lt;td&gt;43.6&lt;/td&gt; &#xA;   &lt;td&gt;38.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B-HD&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-7B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;68.4&lt;/td&gt; &#xA;   &lt;td&gt;65.8&lt;/td&gt; &#xA;   &lt;td&gt;1546/319&lt;/td&gt; &#xA;   &lt;td&gt;41.3&lt;/td&gt; &#xA;   &lt;td&gt;36.8&lt;/td&gt; &#xA;   &lt;td&gt;32.9&lt;/td&gt; &#xA;   &lt;td&gt;32.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B-HD&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-13B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;70.2&lt;/td&gt; &#xA;   &lt;td&gt;68.6&lt;/td&gt; &#xA;   &lt;td&gt;1597/320&lt;/td&gt; &#xA;   &lt;td&gt;50.5&lt;/td&gt; &#xA;   &lt;td&gt;37.3&lt;/td&gt; &#xA;   &lt;td&gt;35.1&lt;/td&gt; &#xA;   &lt;td&gt;37.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B-HD&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-8x7B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;71.9&lt;/td&gt; &#xA;   &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;td&gt;1633/356&lt;/td&gt; &#xA;   &lt;td&gt;53.5&lt;/td&gt; &#xA;   &lt;td&gt;40.0&lt;/td&gt; &#xA;   &lt;td&gt;37.0&lt;/td&gt; &#xA;   &lt;td&gt;43.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B-HD&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-34B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;74.1&lt;/td&gt; &#xA;   &lt;td&gt;80.6&lt;/td&gt; &#xA;   &lt;td&gt;1659/482&lt;/td&gt; &#xA;   &lt;td&gt;59.3&lt;/td&gt; &#xA;   &lt;td&gt;48.0&lt;/td&gt; &#xA;   &lt;td&gt;44.9&lt;/td&gt; &#xA;   &lt;td&gt;43.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;If you want to evaluate the model on image-based benchmarks, please use the scripts in &lt;code&gt;scripts/MODEL_PATH/eval&lt;/code&gt;. For example, run the following command for TextVQA evaluation with Mini-Gemini-7B-HD:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/llama/eval/textvqa.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please find more evaluation scripts in &lt;code&gt;scripts/MODEL_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;p&gt;Chat with images using Mini-Gemini without the need of Gradio interface. It also supports multiple GPUs, 4-bit and 8-bit quantized inference. With 4-bit quantization. Please make sure you have installed &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; and &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.7/README_en.md&#34;&gt;PaddleOCR&lt;/a&gt; (only for better experience with OCR), and try this for image and generation inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m minigemini.serve.cli \&#xA;    --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \&#xA;    --image-file &amp;lt;path to your image&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or try this better experience with OCR (make sure you have installed &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.7/README_en.md&#34;&gt;PaddleOCR&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m minigemini.serve.cli \&#xA;    --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \&#xA;    --image-file &amp;lt;path to your image&amp;gt; \&#xA;    --ocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or try this for inference with generation (make sure you have installed &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m minigemini.serve.cli \&#xA;    --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \&#xA;    --image-file &amp;lt;path to your image&amp;gt; \&#xA;    --gen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also try 8bit or even 4bit for efficient inference&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m minigemini.serve.cli \&#xA;    --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \&#xA;    --image-file &amp;lt;path to your image&amp;gt; \&#xA;    --gen&#xA;    --load-8bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;Here, we adopt the Gradio UI similar to that in LLaVA to provide a user-friendly interface for Mini-Gemini. To launch a Gradio demo locally, please run the following commands one by one. If you plan to launch multiple model workers to compare between different checkpoints, you only need to launch the controller and the web server &lt;em&gt;ONCE&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.controller --host 0.0.0.0 --port 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You just launched the Gradio web interface. Now, you can open the web interface with the URL printed on the screen. You may notice that there is no model in the model list. Do not worry, as we have not launched any model worker yet. It will be automatically updated when you launch a model worker.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;p&gt;This is the actual &lt;em&gt;worker&lt;/em&gt; that performs the inference on the GPU. Each worker is responsible for a single model specified in &lt;code&gt;--model-path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;. Now, refresh your Gradio web UI, and you will see the model you just launched in the model list.&lt;/p&gt; &#xA;&lt;p&gt;You can launch as many workers as you want, and compare between different models in the same Gradio interface. Please keep the &lt;code&gt;--controller&lt;/code&gt; the same, and modify the &lt;code&gt;--port&lt;/code&gt; and &lt;code&gt;--worker&lt;/code&gt; to a different port number for each worker.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port &amp;lt;different from 40000, say 40001&amp;gt; --worker http://localhost:&amp;lt;change accordingly, i.e. 40001&amp;gt; --model-path work_dirs/Mini-Gemini/Mini-Gemini-34B-HD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using an Apple device with an M1 or M2 chip, you can specify the mps device by using the &lt;code&gt;--device&lt;/code&gt; flag: &lt;code&gt;--device mps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker (Multiple GPUs, when GPU VRAM &amp;lt;= 24GB)&lt;/h4&gt; &#xA;&lt;p&gt;If the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs. Our latest code base will automatically try to use multiple GPUs if you have more than one GPU. You can specify which GPUs to use with &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;. Below is an example of running with the first two GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker (4-bit, 8-bit inference, quantized)&lt;/h4&gt; &#xA;&lt;p&gt;You can launch the model worker with quantized bits (4-bit, 8-bit), which allows you to run the inference with reduced GPU memory footprint. Note that inference with quantized bits may not be as accurate as the full-precision model. Simply append &lt;code&gt;--load-4bit&lt;/code&gt; or &lt;code&gt;--load-8bit&lt;/code&gt; to the &lt;strong&gt;model worker&lt;/strong&gt; command that you are executing. Below is an example of running with 4-bit quantization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD --load-4bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;We provide some examples in this section. More examples can be found in our &lt;a href=&#34;https://mini-gemini.github.io/&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Hi-Resolution Understanding&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;98%&#34; src=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/demo_und.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Generation with Reasoning&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;98%&#34; src=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/demo_gen.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repo useful for your research, please consider citing the paper&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{li2024minigemini,&#xA;  title={Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models},&#xA;  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},&#xA;  journal={arXiv:2403.18814},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the following repos for their great work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This work is built upon the &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;This work utilizes LLMs from &lt;a href=&#34;https://huggingface.co/google/gemma-2b-it&#34;&gt;Gemma&lt;/a&gt;, &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;&gt;Mixtral&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B&#34;&gt;Nous-Hermes&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dvlab-research/MiniGemini/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-yellow.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/MiniGemini/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-orange.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/MiniGemini/raw/main/WEIGHT_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Weight%20License-CC%20By%20NC%204.0-red&#34; alt=&#34;Weight License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The data and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaVA, LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TMElyralab/MuseTalk</title>
    <updated>2024-04-21T03:34:05Z</updated>
    <id>tag:github.com,2024-04-21:/TMElyralab/MuseTalk</id>
    <link href="https://github.com/TMElyralab/MuseTalk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MuseTalk: Real-Time High Quality Lip Synchorization with Latent Space Inpainting&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MuseTalk&lt;/h1&gt; &#xA;&lt;p&gt;MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting &lt;br&gt; Yue Zhang &lt;sup&gt;*&lt;/sup&gt;, Minhao Liu&lt;sup&gt;*&lt;/sup&gt;, Zhaokang Chen, Bin Wu&lt;sup&gt;†&lt;/sup&gt;, Yingjie He, Chao Zhan, Wenjiang Zhou (&lt;sup&gt;*&lt;/sup&gt;Equal Contribution, &lt;sup&gt;†&lt;/sup&gt;Corresponding Author, &lt;a href=&#34;mailto:benbinwu@tencent.com&#34;&gt;benbinwu@tencent.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/TMElyralab/MuseTalk&#34;&gt;github&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/TMElyralab/MuseTalk&#34;&gt;huggingface&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/spaces/TMElyralab/MuseTalk&#34;&gt;space&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;Project (comming soon)&lt;/strong&gt; &lt;strong&gt;Technical report (comming soon)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We introduce &lt;code&gt;MuseTalk&lt;/code&gt;, a &lt;strong&gt;real-time high quality&lt;/strong&gt; lip-syncing model (30fps+ on an NVIDIA Tesla V100). MuseTalk can be applied with input videos, e.g., generated by &lt;a href=&#34;https://github.com/TMElyralab/MuseV&#34;&gt;MuseV&lt;/a&gt;, as a complete virtual human solution.&lt;/p&gt; &#xA;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;MuseTalk&lt;/code&gt; is a real-time high quality audio-driven lip-syncing model trained in the latent space of &lt;code&gt;ft-mse-vae&lt;/code&gt;, which&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;modifies an unseen face according to the input audio, with a size of face region of &lt;code&gt;256 x 256&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;supports audio in various languages, such as Chinese, English, and Japanese.&lt;/li&gt; &#xA; &lt;li&gt;supports real-time inference with 30fps+ on an NVIDIA Tesla V100.&lt;/li&gt; &#xA; &lt;li&gt;supports modification of the center point of the face region proposes, which &lt;strong&gt;SIGNIFICANTLY&lt;/strong&gt; affects generation results.&lt;/li&gt; &#xA; &lt;li&gt;checkpoint available trained on the HDTF dataset.&lt;/li&gt; &#xA; &lt;li&gt;training codes (comming soon).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[04/02/2024] Release MuseTalk project and pretrained models.&lt;/li&gt; &#xA; &lt;li&gt;[04/16/2024] Release Gradio &lt;a href=&#34;https://huggingface.co/spaces/TMElyralab/MuseTalk&#34;&gt;demo&lt;/a&gt; on HuggingFace Spaces (thanks to HF team for their community grant)&lt;/li&gt; &#xA; &lt;li&gt;[04/17/2024] &lt;span&gt;📣&lt;/span&gt; We release a pipeline that utilizes MuseTalk for real-time inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/figs/musetalk_arc.jpg&#34; alt=&#34;Model Structure&#34;&gt; MuseTalk was trained in latent spaces, where the images were encoded by a freezed VAE. The audio was encoded by a freezed &lt;code&gt;whisper-tiny&lt;/code&gt; model. The architecture of the generation network was borrowed from the UNet of the &lt;code&gt;stable-diffusion-v1-4&lt;/code&gt;, where the audio embeddings were fused to the image embeddings by cross-attention.&lt;/p&gt; &#xA;&lt;p&gt;Note that although we use a very similar architecture as Stable Diffusion, MuseTalk is distinct in that it is &lt;strong&gt;NOT&lt;/strong&gt; a diffusion model. Instead, MuseTalk operates by inpainting in the latent space with a single step.&lt;/p&gt; &#xA;&lt;h2&gt;Cases&lt;/h2&gt; &#xA;&lt;h3&gt;MuseV + MuseTalk make human photos alive！&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt;Image&lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt;MuseV&lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt;+MuseTalk&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/musk/musk.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/4a4bb2d1-9d14-4ca9-85c8-7f19c39f712e&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/b2a879c2-e23a-4d39-911d-51f0343218e4&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/yongen/yongen.jpeg&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/57ef9dee-a9fd-4dc8-839b-3fbbbf0ff3f4&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/94d8dcba-1bcd-4b54-9d1d-8b6fc53228f0&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/sit/sit.jpeg&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/5fbab81b-d3f2-4c75-abb5-14c76e51769e&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/f8100f4a-3df8-4151-8de2-291b09269f66&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/man/man.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/a6e7d431-5643-4745-9868-8b423a454153&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/6ccf7bc7-cb48-42de-85bd-076d5ee8a623&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/monalisa/monalisa.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/1568f604-a34f-4526-a13a-7d282aa2e773&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/a40784fc-a885-4c1f-9b7e-8f87b7caf4e0&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/sun1/sun.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/37a3a666-7b90-4244-8d3a-058cb0e44107&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/172f4ff1-d432-45bd-a5a7-a07dec33a26b&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/sun2/sun.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/37a3a666-7b90-4244-8d3a-058cb0e44107&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/85a6873d-a028-4cce-af2b-6c59a1f2971d&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The character of the last two rows, &lt;code&gt;Xinying Sun&lt;/code&gt;, is a supermodel KOL. You can follow her on &lt;a href=&#34;https://www.douyin.com/user/MS4wLjABAAAAWDThbMPN_6Xmm_JgXexbOii1K-httbu2APdG8DvDyM8&#34;&gt;douyin&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Video dubbing&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td width=&#34;70%&#34;&gt;MuseTalk&lt;/td&gt; &#xA;   &lt;td width=&#34;30%&#34;&gt;Original videos&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/4d7c5fa1-3550-4d52-8ed2-52f158150f24&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/www.bilibili.com/video/BV1wT411b7HU&#34;&gt;Link&lt;/a&gt; &#xA;    &lt;href src=&#34;&#34;&gt;&lt;/href&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For video dubbing, we applied a self-developed tool which can identify the talking person.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Some interesting videos!&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt;Image&lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt;MuseV + MuseTalk&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/video1/video1.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/1f02f9c6-8b98-475e-86b8-82ebee82fe0d&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h1&gt;TODO:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; trained models and inference codes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Huggingface Gradio &lt;a href=&#34;https://huggingface.co/spaces/TMElyralab/MuseTalk&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; codes for real-time inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; technical report.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; training codes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; a better model (may take longer).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;We provide a detailed tutorial about the installation and the basic usage of MuseTalk for new users:&lt;/p&gt; &#xA;&lt;h2&gt;Third party integration&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for the third-party integration, which makes installation and use more convenient for everyone. We also hope you note that we have not verified, maintained, or updated third-party. Please refer to this project for specific results.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/chaojie/ComfyUI-MuseTalk&#34;&gt;ComfyUI&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To prepare the Python environment and install additional packages such as opencv, diffusers, mmcv, etc., please follow the steps below:&lt;/p&gt; &#xA;&lt;h3&gt;Build environment&lt;/h3&gt; &#xA;&lt;p&gt;We recommend a python version &amp;gt;=3.10 and cuda version =11.7. Then build environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;mmlab packages&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --no-cache-dir -U openmim &#xA;mim install mmengine &#xA;mim install &#34;mmcv&amp;gt;=2.0.1&#34; &#xA;mim install &#34;mmdet&amp;gt;=3.1.0&#34; &#xA;mim install &#34;mmpose&amp;gt;=1.1.0&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download ffmpeg-static&lt;/h3&gt; &#xA;&lt;p&gt;Download the ffmpeg-static and&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export FFMPEG_PATH=/path/to/ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export FFMPEG_PATH=/musetalk/ffmpeg-4.4-amd64-static&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download weights&lt;/h3&gt; &#xA;&lt;p&gt;You can download weights manually as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our trained &lt;a href=&#34;https://huggingface.co/TMElyralab/MuseTalk&#34;&gt;weights&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the weights of other components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt&#34;&gt;whisper&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/yzd-v/DWPose/tree/main&#34;&gt;dwpose&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;face-parse-bisent&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34;&gt;resnet18&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, these weights should be organized in &lt;code&gt;models&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./models/&#xA;├── musetalk&#xA;│   └── musetalk.json&#xA;│   └── pytorch_model.bin&#xA;├── dwpose&#xA;│   └── dw-ll_ucoco_384.pth&#xA;├── face-parse-bisent&#xA;│   ├── 79999_iter.pth&#xA;│   └── resnet18-5c106cde.pth&#xA;├── sd-vae-ft-mse&#xA;│   ├── config.json&#xA;│   └── diffusion_pytorch_model.bin&#xA;└── whisper&#xA;    └── tiny.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Here, we provide the inference script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m scripts.inference --inference_config configs/inference/test.yaml &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;configs/inference/test.yaml is the path to the inference configuration file, including video_path and audio_path. The video_path should be either a video file, an image file or a directory of images.&lt;/p&gt; &#xA;&lt;p&gt;You are recommended to input video with &lt;code&gt;25fps&lt;/code&gt;, the same fps used when training the model. If your video is far less than 25fps, you are recommended to apply frame interpolation or directly convert the video to 25fps using ffmpeg.&lt;/p&gt; &#xA;&lt;h4&gt;Use of bbox_shift to have adjustable results&lt;/h4&gt; &#xA;&lt;p&gt;&lt;span&gt;🔎&lt;/span&gt; We have found that upper-bound of the mask has an important impact on mouth openness. Thus, to control the mask region, we suggest using the &lt;code&gt;bbox_shift&lt;/code&gt; parameter. Positive values (moving towards the lower half) increase mouth openness, while negative values (moving towards the upper half) decrease mouth openness.&lt;/p&gt; &#xA;&lt;p&gt;You can start by running with the default configuration to obtain the adjustable value range, and then re-run the script within this range.&lt;/p&gt; &#xA;&lt;p&gt;For example, in the case of &lt;code&gt;Xinying Sun&lt;/code&gt;, after running the default configuration, it shows that the adjustable value rage is [-9, 9]. Then, to decrease the mouth openness, we set the value to be &lt;code&gt;-7&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m scripts.inference --inference_config configs/inference/test.yaml --bbox_shift -7 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;span&gt;📌&lt;/span&gt; More technical details can be found in &lt;a href=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/BBOX_SHIFT.md&#34;&gt;bbox_shift&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Combining MuseV and MuseTalk&lt;/h4&gt; &#xA;&lt;p&gt;As a complete solution to virtual human generation, you are suggested to first apply &lt;a href=&#34;https://github.com/TMElyralab/MuseV&#34;&gt;MuseV&lt;/a&gt; to generate a video (text-to-video, image-to-video or pose-to-video) by referring &lt;a href=&#34;https://github.com/TMElyralab/MuseV?tab=readme-ov-file#text2video&#34;&gt;this&lt;/a&gt;. Frame interpolation is suggested to increase frame rate. Then, you can use &lt;code&gt;MuseTalk&lt;/code&gt; to generate a lip-sync video by referring &lt;a href=&#34;https://github.com/TMElyralab/MuseTalk?tab=readme-ov-file#inference&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;span&gt;🆕&lt;/span&gt; Real-time inference&lt;/h4&gt; &#xA;&lt;p&gt;Here, we provide the inference script. This script first applies necessary pre-processing such as face detection, face parsing and VAE encode in advance. During inference, only UNet and the VAE decoder are involved, which makes MuseTalk real-time.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m scripts.realtime_inference --inference_config configs/inference/realtime.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;configs/inference/realtime.yaml is the path to the real-time inference configuration file, including &lt;code&gt;preparation&lt;/code&gt;, &lt;code&gt;video_path&lt;/code&gt; , &lt;code&gt;bbox_shift&lt;/code&gt; and &lt;code&gt;audio_clips&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set &lt;code&gt;preparation&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; in &lt;code&gt;realtime.yaml&lt;/code&gt; to prepare the materials for a new &lt;code&gt;avatar&lt;/code&gt;. (If the &lt;code&gt;bbox_shift&lt;/code&gt; has changed, you also need to re-prepare the materials.)&lt;/li&gt; &#xA; &lt;li&gt;After that, the &lt;code&gt;avatar&lt;/code&gt; will use an audio clip selected from &lt;code&gt;audio_clips&lt;/code&gt; to generate video. &lt;pre&gt;&lt;code&gt;Inferring using: data/audio/yongen.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;While MuseTalk is inferring, sub-threads can simultaneously stream the results to the users. The generation process can achieve 30fps+ on an NVIDIA Tesla V100. &lt;pre&gt;&lt;code&gt;2%|██▍                                                         | 3/141 [00:00&amp;lt;00:32,  4.30it/s]   # inference process&#xA;Displaying the 6-th frame with FPS: 48.58                                                  # display process&#xA;Displaying the 7-th frame with FPS: 48.74&#xA;Displaying the 8-th frame with FPS: 49.17&#xA;3%|███▎                                                        | 4/141 [00:00&amp;lt;00:32,  4.21it/s]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;preparation&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; and run this script if you want to genrate more videos using the same avatar.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you want to generate multiple videos using the same avatar/video, you can also use this script to &lt;strong&gt;SIGNIFICANTLY&lt;/strong&gt; expedite the generation process.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We thank open-source components like &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;https://github.com/IDEA-Research/DWPose&#34;&gt;dwpose&lt;/a&gt;, &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face-alignment&lt;/a&gt;, &lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;face-parsing&lt;/a&gt;, &lt;a href=&#34;https://github.com/yxlijun/S3FD.pytorch&#34;&gt;S3FD&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;MuseTalk has referred much to &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; and &lt;a href=&#34;https://github.com/isaacOnline/whisper/tree/extract-embeddings&#34;&gt;isaacOnline/whisper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;MuseTalk has been built on &lt;a href=&#34;https://github.com/MRzzm/HDTF&#34;&gt;HDTF&lt;/a&gt; datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Thanks for open-sourcing!&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Resolution: Though MuseTalk uses a face region size of 256 x 256, which make it better than other open-source methods, it has not yet reached the theoretical resolution bound. We will continue to deal with this problem.&lt;br&gt; If you need higher resolution, you could apply super resolution models such as &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt; in combination with MuseTalk.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Identity preservation: Some details of the original face are not well preserved, such as mustache, lip shape and color.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jitter: There exists some jitter as the current pipeline adopts single-frame generation.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@article{musetalk,&#xA;  title={MuseTalk: Real-Time High Quality Lip Synchorization with Latent Space Inpainting},&#xA;  author={Zhang, Yue and Liu, Minhao and Chen, Zhaokang and Wu, Bin and He, Yingjie and Zhan, Chao and Zhou, Wenjiang},&#xA;  journal={arxiv},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Disclaimer/License&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;code&lt;/code&gt;: The code of MuseTalk is released under the MIT License. There is no limitation for both academic and commercial usage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: The trained model are available for any purpose, even commercially.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;other opensource model&lt;/code&gt;: Other open-source models used must comply with their license, such as &lt;code&gt;whisper&lt;/code&gt;, &lt;code&gt;ft-mse-vae&lt;/code&gt;, &lt;code&gt;dwpose&lt;/code&gt;, &lt;code&gt;S3FD&lt;/code&gt;, etc..&lt;/li&gt; &#xA; &lt;li&gt;The testdata are collected from internet, which are available for non-commercial research purposes only.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AIGC&lt;/code&gt;: This project strives to impact the domain of AI-driven video generation positively. Users are granted the freedom to create videos using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>aixcoder-plugin/aiXcoder-7B</title>
    <updated>2024-04-21T03:34:05Z</updated>
    <id>tag:github.com,2024-04-21:/aixcoder-plugin/aiXcoder-7B</id>
    <link href="https://github.com/aixcoder-plugin/aiXcoder-7B" rel="alternate"></link>
    <summary type="html">&lt;p&gt;official repository of aiXcoder-7B Code Large Language Model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;aiXcoder-7B Code Large Language Model&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🏠 &lt;a href=&#34;https://www.aixcoder.com/&#34; target=&#34;_blank&#34;&gt;Official website&lt;/a&gt;｜🛠 &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=aixcoder-plugin.aixcoder&#34; target=&#34;_blank&#34;&gt;VS Code Plugin&lt;/a&gt;｜🛠 &lt;a href=&#34;https://plugins.jetbrains.com/plugin/13574-aixcoder-code-completer&#34; target=&#34;_blank&#34;&gt;Jetbrains Plugin&lt;/a&gt;｜🤗 &lt;a href=&#34;https://huggingface.co/aiXcoder/aixcoder-7b-base&#34; target=&#34;_blank&#34;&gt;Model Weights&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/wechat_1.jpg&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/wechat_2.jpg&#34; target=&#34;_blank&#34;&gt;WeChat Official Account&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Welcome to the official repository of aiXcoder-7B Code Large Language Model. This model is designed to understand and generate code across multiple programming languages, offering state-of-the-art performance in code completion, comprehension, generation, and more tasks about programming languages.&lt;/p&gt; &#xA;&lt;p&gt;Table of Contents&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#model-introduction&#34;&gt;Model Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#environment-requirements&#34;&gt;Environment Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#model-weights&#34;&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#inference-example&#34;&gt;Inference Example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#quantized-through-bitsandbytes&#34;&gt;Quantized through bitsandbytes&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#fine-tuning-example&#34;&gt;Fine-tuning example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#data-for-aixcoder-7b&#34;&gt;Data for aiXcoder 7B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#training-hyperparameters&#34;&gt;Training Hyperparameters&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#batch-processing-method&#34;&gt;Batch processing method&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#pre-training-tasks&#34;&gt;Pre-training Tasks&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#details-of-experimental-results&#34;&gt;Details of Experimental Results&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#nl2code-benchmarks&#34;&gt;NL2Code Benchmarks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#code-completion-fill-in-the-middle&#34;&gt;Code Completion (Fill in the Middle)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#cross-file-code-evaluation&#34;&gt;Cross-file Code Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Model Introduction&lt;/h2&gt; &#xA;&lt;p&gt;As the capabilities of large code models are gradually being unearthed, aiXcoder has consistently pondered on how to make these models more beneficial in real development scenarios. To this end, we have open-sourced aiXcoder 7B Base, which has undergone extensive training on 1.2T Unique Tokens, and the model&#39;s pre-training tasks as well as the contextual information have been uniquely designed for real-world code generation contexts.&lt;/p&gt; &#xA;&lt;p&gt;aiXcoder 7B Base stands out as the most effective model in code completion scenarios among all models of similar parameter sizes, and it also surpasses mainstream models like codellama 34B and StarCoder2 15B in the average performance on the multilingual nl2code benchmark.&lt;/p&gt; &#xA;&lt;p&gt;In our ongoing exploration to apply large code models, the release of aiXcoder 7B Base represents a significant milestone. The current version of aiXcoder 7B Base is a foundational model that focuses on improving the efficiency and accuracy of code completion and code generation tasks, aiming to provide robust support for developers in these scenarios. It is important to note that this version has not undergone specific instruct-tuning, which means it might not yet offer optimal performance for specialized higher-level tasks such as test case generation and code debugging.&lt;/p&gt; &#xA;&lt;p&gt;However, we have plans for further development of the aiXcoder model series already in motion. In the near future, we aim to release new versions of the model that have been meticulously instruct-tuned for a wider range of programming tasks, including but not limited to test case generation and code debugging. Through these instruct-tuned models, we anticipate offering developers more comprehensive and deeper programming support, helping them to maximize efficiency at every stage of software development.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_1.png&#34; alt=&#34;table_1&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;aiXcoder 7B surpasses mainstream models in nl2code benchmark. aiXcoder-7B is an enhancement of aiXcoder-7B-Base, fine-tuned on one hundred thousand data entries similar to Evol-instruct for one epoch.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_3.png&#34; alt=&#34;table_3&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;aiXcoder 7B Base surpasses mainstream models in code completion scenarios.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Requirements&lt;/h3&gt; &#xA;&lt;h4&gt;Option 1: Build Env&lt;/h4&gt; &#xA;&lt;p&gt;To run the model inference code, you&#39;ll need the following environment setup:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8 or higher&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 2.1.0 or higher&lt;/li&gt; &#xA; &lt;li&gt;sentencepiece 0.2.0 or higher&lt;/li&gt; &#xA; &lt;li&gt;transformers 4.34.1 or higher (if run inference by transformers library)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please ensure all dependencies are installed using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n aixcoder-7b python=3.11&#xA;conda activate aixcoder-7b&#xA;git clone git@github.com:aixcoder-plugin/aiXcoder-7b.git&#xA;cd aiXcoder-7b&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;requirements.txt&lt;/code&gt; listed all necessary libraries and their versions.&lt;/p&gt; &#xA;&lt;p&gt;To achieve faster inference speeds, especially for large models, we recommend installing &lt;code&gt;flash attention&lt;/code&gt;. &lt;code&gt;Flash attention&lt;/code&gt; is an optimized attention mechanism that significantly reduces computation time for transformer-based models without sacrificing accuracy.&lt;/p&gt; &#xA;&lt;p&gt;Before proceeding, ensure your environment meets the CUDA requirements as &lt;code&gt;flash attention&lt;/code&gt; leverages GPU acceleration. Follow these steps to install &lt;code&gt;flash attention&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:Dao-AILab/flash-attention.git&#xA;cd flash-attention&#xA;MAX_JOBS=8 python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Option 2: Docker&lt;/h4&gt; &#xA;&lt;p&gt;For a consistent and isolated environment, we recommend running the model inference code using Docker. Here&#39;s how to set up and use Docker for our model:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Docker: If you haven&#39;t already, install Docker on your machine.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pull the Docker Image: Pull the Docker image from Docker Hub.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the Container: Once the image is pulled, you can run the model inside a Docker container.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all -it -v /dev/shm:/dev/shm --name aix_instance pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel /bin/bash&#xA;pip install sentencepiece&#xA;git clone git@github.com:aixcoder-plugin/aiXcoder-7b.git&#xA;cd aiXcoder-7b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command starts a container named aix_instance from the pytorch image. You can interact with the model inside this container.&lt;/p&gt; &#xA;&lt;p&gt;To achieve faster inference speeds, especially for large models, we recommend installing &lt;code&gt;flash attention&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:Dao-AILab/flash-attention.git&#xA;cd flash-attention&#xA;MAX_JOBS=8 python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Model Inference: Within the Docker container, you can run the model inference code as described in the Inference Example section.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Using Docker provides a clean, controlled environment that minimizes issues related to software versions and dependencies.&lt;/p&gt; &#xA;&lt;h3&gt;Model Weights&lt;/h3&gt; &#xA;&lt;p&gt;You can download the model weights from the following link:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/aiXcoder/aixcoder-7b-base&#34;&gt;aiXcoder Base Download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;aiXcoder Instruct Download (Comming soon...)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference Example&lt;/h3&gt; &#xA;&lt;h4&gt;Command Line Execution&lt;/h4&gt; &#xA;&lt;p&gt;For a quick start, you can run the model inference directly from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node 1 sess_megatron.py --model_dir &#34;path/to/model_weights_dir&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &#34;path/to/model_weights_dir&#34; with the actual path to your downloaded model weights.&lt;/p&gt; &#xA;&lt;p&gt;or run inference with huggingface&#39;s transformers：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sess_huggingface.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Python Script Execution&lt;/h4&gt; &#xA;&lt;p&gt;Alternatively, you can invoke the model programmatically within your Python scripts. This method provides more flexibility for integrating the model into your applications or workflows. Here&#39;s a simple example on how to do it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from sess_megatron import TestInference&#xA;&#xA;infer = TestInference()&#xA;res = infer.run_infer(&#xA;    # for FIM style input, code_string stands for prefix context&#xA;    code_string=&#34;&#34;&#34;# 快速排序算法&#34;&#34;&#34;, &#xA;    # for FIM style input, later_code stands for suffix context&#xA;    later_code=&#34;\n&#34;,&#xA;    # file_path should be a path from project to file&#xA;    file_path=&#34;test.py&#34;,&#xA;    # max num for generated tokens&#xA;    max_new_tokens=256,&#xA;)&#xA;print(res)&#xA;&#xA;&#34;&#34;&#34;output:&#xA;&#xA;def quick_sort(arr):&#xA;    if len(arr) &amp;lt;= 1:&#xA;        return arr&#xA;    pivot = arr[0]&#xA;    less = [i for i in arr[1:] if i &amp;lt;= pivot]&#xA;    greater = [i for i in arr[1:] if i &amp;gt; pivot]&#xA;    return quick_sort(less) + [pivot] + quick_sort(greater)&#xA;&#xA;&#xA;# 测试&#xA;arr = [3, 2, 1, 4, 5]&#xA;print(quick_sort(arr))  # [1, 2, 3, 4, 5]&#xA;&#34;&#34;&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;&#xA;import torch&#xA;import sys&#xA;from hf_mini.utils import input_wrapper&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;aiXcoder/aixcoder-7b-base&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;aiXcoder/aixcoder-7b-base&#34;, torch_dtype=torch.bfloat16)&#xA;&#xA;&#xA;text = input_wrapper(&#xA;    # for FIM style input, code_string stands for prefix context&#xA;    code_string=&#34;# 快速排序算法&#34;,&#xA;    # for FIM style input, later_code stands for suffix context&#xA;    later_code=&#34;\n# 测试\narr = [3, 2, 1, 4, 5]\nprint(quick_sort(arr))  # [1, 2, 3, 4, 5]&#34;,&#xA;    # file_path should be a path from project to file&#xA;    path=&#34;test.py&#34;&#xA;)&#xA;&#xA;if len(text) == 0:&#xA;    sys.exit()&#xA;&#xA;inputs = tokenizer(text, return_tensors=&#34;pt&#34;, return_token_type_ids=False)&#xA;&#xA;inputs = inputs.to(device)&#xA;model.to(device)&#xA;&#xA;outputs = model.generate(**inputs, max_new_tokens=256)&#xA;print(tokenizer.decode(outputs[0], skip_special_tokens=False))&#xA;&#xA;&#xA;&#xA;&#34;&#34;&#34;output:&#xA;def quick_sort(arr):&#xA;    # 如果数组长度小于等于1，直接返回&#xA;    if len(arr) &amp;lt;= 1:&#xA;        return arr&#xA;    # 选择数组的第一个元素作为基准&#xA;    pivot = arr[0]&#xA;    # 初始化左右指针&#xA;    left, right = 1, len(arr) - 1&#xA;    # 循环直到左指针小于右指针&#xA;    while left &amp;lt; right:&#xA;        # 从右到左找到第一个小于基准的元素，与左指针元素交换&#xA;        if arr[right] &amp;lt; pivot:&#xA;            arr[left], arr[right] = arr[right], arr[left]&#xA;            left += 1&#xA;        # 从左到右找到第一个大于等于基准的元素，与右指针元素交换&#xA;        if arr[left] &amp;gt;= pivot:&#xA;            right -= 1&#xA;    # 将基准元素与左指针元素交换&#xA;    arr[left], arr[0] = arr[0], arr[left]&#xA;    # 对左半部分进行递归排序&#xA;    quick_sort(arr[:left])&#xA;    # 对右半部分进行递归排序&#xA;    quick_sort(arr[left + 1:])&#xA;    return arr&amp;lt;/s&amp;gt;&#xA;&#34;&#34;&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quantized through bitsandbytes&lt;/h3&gt; &#xA;&lt;p&gt;We can also install Bitsandbytes through &lt;code&gt;pip install bitsandbytes acceleration&lt;/code&gt;, and simply add configuration to perform int8 or int4 inference (if you need to further compress the temporary memory applied at runtime, it is recommended to install FlashAttention):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import sys&#xA;import torch&#xA;from hf_mini.utils import input_wrapper&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig    &#xA;&#xA;# to use 4bit use `load_in_4bit=True` instead&#xA;bnb_config = BitsAndBytesConfig(load_in_8bit=True) &#xA;&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;aiXcoder/aixcoder-7b-base&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;aiXcoder/aixcoder-7b-base&#34;, quantization_config=bnb_config, device_map=device, attn_implementation=&#39;flash_attention_2&#39;)&#xA;&#xA;text = input_wrapper(&#xA;    code_string=&#34;# 快速排序算法&#34;,&#xA;    later_code=&#34;\n&#34;,&#xA;    path=&#34;test.py&#34;&#xA;)&#xA;&#xA;if len(text) == 0:&#xA;    sys.exit()&#xA;&#xA;inputs = tokenizer(text, return_tensors=&#34;pt&#34;, return_token_type_ids=False)&#xA;&#xA;inputs = inputs.to(device)    &#xA;&#xA;outputs = model.generate(**inputs, max_new_tokens=256)&#xA;print(f&#34;Model memory footprint: {model.get_memory_footprint() / 2**20:.2f} MB&#34;)&#xA;print(f&#34;Torch max memory allocated: {torch.cuda.max_memory_allocated() / 2**20:.2f} MB&#34;)&#xA;&#xA;&#34;&#34;&#34;&#xA;load_in_4bit=True:&#xA;    - Model memory footprint: 5656.52 MB&#xA;    - Torch max memory allocated: 6448.89 MB&#xA;&#xA;load_in_8bit=True:&#xA;    - Model memory footprint: 9008.52 MB&#xA;    - Torch max memory allocated: 10061.51 MB&#xA;&#34;&#34;&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning example&lt;/h3&gt; &#xA;&lt;p&gt;If you want to fine-tune on your own code, you can quickly get started with training using Huggingface&#39;s PEFT tools. Before doing so, you need to install the necessary libraries with &lt;code&gt;pip install -r requirements_peft.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then, execute the training command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch finetune.py \&#xA;        --model_id &#34;aiXcoder/aixcoder-7b-base&#34; \&#xA;        --dataset_name &#34;bigcode/the-stack-smol&#34; \&#xA;        --subset &#34;data/rust&#34; \&#xA;        --dataset_text_field &#34;content&#34; \&#xA;        --split &#34;train&#34; \&#xA;        --max_seq_length 1024 \&#xA;        --max_steps 10000 \&#xA;        --micro_batch_size 1 \&#xA;        --gradient_accumulation_steps 8 \&#xA;        --learning_rate 5e-6 \&#xA;        --warmup_steps 20 \&#xA;        --fim_rate 0.5 \&#xA;        --num_proc &#34;$(nproc)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the fine-tuning script, we have constructed a simple random FIM (Fill-In-the-Middle) training task that can train the model on the completion and generation capabilities on your own data. It should be noted that the aiXcoder-7b-base uses &lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#pre-training-tasks&#34;&gt;structured FIM&lt;/a&gt; during pre-training, which involves constructing a complete code block as the MIDDLE. However, creating such training data involves syntactic parsing, which may require developers to implement themselves.&lt;/p&gt; &#xA;&lt;h2&gt;Data for aiXcoder 7B&lt;/h2&gt; &#xA;&lt;p&gt;The data for aiXcoder is divided into a core dataset and an extended dataset. The core dataset comprises the programming languages commonly used in development, as well as natural languages closely related to code. The core dataset&#39;s programming languages mainly include nearly a hundred mainstream languages such as C++, Python, Java, and JavaScript, while the natural language component primarily consists of StackOverflow Q&amp;amp;As, technical blogs, code documentation, and computer science papers. The extended data mainly consists of filtered open-source code datasets, high-quality English natural language datasets, and high-quality Chinese natural language datasets.&lt;/p&gt; &#xA;&lt;!-- &lt;br&gt;&#xA;&lt;br&gt;&#xA;&#xA;![table_0](./assets/table_0.png)&#xA;&#xA;&lt;br&gt;&#xA;&lt;br&gt; --&gt; &#xA;&lt;p&gt;The aiXcoder core dataset is mainly used to enhance the performance of the large code model in the aforementioned programming languages, undergoing a rigorous filtering and selection process. Specifically, this process includes the following steps: 1) Selection of raw data; 2) Comprehensive ranking and selection of projects; 3) Code deduplication and the removal of automatically generated code using methods such as MinHashes (Broder, 2000); 4) Identification and handling of personal sensitive information; 5) Cleaning of commented code; 6) Syntactic analysis to filter incorrect or anomalous code files; 7) Static analysis to detect and eliminate 163 types of high-risk bugs and 197 types of defects in mainstream programming languages such as Java, C++, Python, and JavaScript.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Raw Data Selection &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Exclude projects under copyleft licenses.&lt;/li&gt; &#xA;   &lt;li&gt;Deduplicate projects gathered from various code hosting platforms and open-source datasets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Project-Level Comprehensive Ranking &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Calculate project metrics, including the number of Stars, Git Commit counts, and the quantity of Test files.&lt;/li&gt; &#xA;   &lt;li&gt;Exclude the lowest 10% of data based on a comprehensive score.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Code File-Level Filtering &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Remove automatically generated code.&lt;/li&gt; &#xA;   &lt;li&gt;Employ near-deduplication for redundancy removal.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Sensitive Information Removal &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use named entity recognition models to identify and delete sensitive information such as names, IP addresses, account passwords, and URLs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Commented Code &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Randomly deleting large sections of commented code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Syntax Analysis &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Delete code with syntax parsing errors or syntactical errors in the top fifty languages.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Static Analysis &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Utilize static analysis tools to scan for and locate 161 types of Bugs affecting code reliability and maintainability, as well as 197 types of vulnerabilities impacting code security.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# &#34;__init__&#34; method should not return a value&#xA;&#xA;# Noncompliant: a TypeError will be raised&#xA;class MyClass(object):&#xA;    def __init__(self):&#xA;        self.message = &#39;HelloWorld&#39;&#xA;        return self  &#xA;&#xA;# Compliant solution&#xA;class MyClass(object):&#xA;    def __init__(self):&#xA;        self.message = &#39;HelloWorld&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The mentioned code illustrates a bug pattern in Python where the &lt;strong&gt;init&lt;/strong&gt; method should not return a value.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Training Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;Tokenizer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Byte Pair Encoding (BPE) based on bytecode&lt;/li&gt; &#xA; &lt;li&gt;Vocabulary size of 49,152&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Model Structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RoPE (Rotary Positional Embedding) for relative position encoding&lt;/li&gt; &#xA; &lt;li&gt;SwiGLU as the intermediate layer&lt;/li&gt; &#xA; &lt;li&gt;Grouped Query Attention&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Training Parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Structured FIM (Fill in the middle) training tasks make up 70% of the training, while autoregressive training tasks account for 30%&lt;/li&gt; &#xA; &lt;li&gt;Pretraining sequence length of 32,768&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Batch processing method&lt;/h3&gt; &#xA;&lt;p&gt;After preprocessing, our code data is organized by project, with the order of files within a project considering both rules and randomness. Specifically, we attempt to cluster similar or dependent code files together using methods like Calling Graph, K-Means clustering, file path similarity, and TF-IDF distance, to help the model better understand the relationships between code files. However, the ordering of code files also incorporates randomness, since in real programming scenarios, projects are not complete, and code files with similarities or dependencies may not be fully developed yet.&lt;/p&gt; &#xA;&lt;p&gt;By ensuring that the project code files overall exhibit randomness while locally having similar or dependent relationships, we stretch the project code files into a vector and organize the sequence of batches using the Transformer-XL style processing. Even though the sequence length of a single batch has already reached 32,768 during the pre-training process, this method still allows for the extension of the visible sequence length to be even longer.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-training Tasks&lt;/h3&gt; &#xA;&lt;p&gt;Unlike other natural language large models or code models, in the context of code programming, aiXcoder considers the structural characteristics of code itself, aiming to have the model predict complete code nodes. In simple terms, the aiXcoder 7b training tasks combine the fill in the middle (FIM, Bavarian et al., 2022) and parser generator tool techniques. When constructing training data, we parse the code into an abstract syntax tree (AST) and randomly select a complete node to construct a FIM task. The rationale behind this approach is twofold: first, we need to ensure that the input data is relatively complete, with both the preceding and subsequent parts being at the same hierarchical level. Secondly, we also want the model&#39;s predictions to be more complete, with the generated code having a full hierarchical structure.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(20):&#xA;    if i % 5 == 0:&#xA;        print(&#34;Hello World&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/graphviz.svg?sanitize=true&#34; alt=&#34;table_0&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Given that simple code can be parsed into an abstract syntax tree (AST), we will construct structured Fill In the Middle (FIM) training tasks based on the nodes of the AST.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Suppose we select the IF node in the above AST, then we will construct training samples from the IF node and its subtree. The following two examples are equivalent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;# fill in the middle, SPM mode&#xA;&#34;&amp;lt;s&amp;gt;▁&amp;lt;AIX-SPAN-PRE&amp;gt;▁&amp;lt;AIX-SPAN-POST&amp;gt;        print(\&#34;Hello World\&#34;)\n▁&amp;lt;AIX-SPAN-MIDDLE&amp;gt;# the file path is: test.py\n# the code file is written by Python\nfor i in range(20):\n    if i % 5 == 0:&amp;lt;\s&amp;gt;&#34;&#xA;&#xA;# fill in the middle, PSM mode&#xA;&#34;&amp;lt;s&amp;gt;▁&amp;lt;AIX-SPAN-PRE&amp;gt;# the file path is: test.py\n# the code file is written by Python\nfor i in range(20):\n    if ▁&amp;lt;AIX-SPAN-POST&amp;gt;        print(\&#34;Hello World\&#34;)\n▁&amp;lt;AIX-SPAN-MIDDLE&amp;gt;i % 5 == 0:&amp;lt;\s&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Details of Experimental Results&lt;/h2&gt; &#xA;&lt;h3&gt;NL2Code Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;Table 1 shows the performance of the aiXcoder-7B Base model on standalone method generation benchmarks. Our model achieves the current best results among the large-scale pre-trained base models within hundreds of billions of parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_1.png&#34; alt=&#34;table_1&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Code Completion (Fill in the Middle)&lt;/h3&gt; &#xA;&lt;p&gt;Different from the standalone nl2code task in Table 1, in real-world programming scenarios, we need to consider the code completion capability in the context of the cursor position. Generally, various open-source large language models for code incorporate the Fill in the Middle (FIM) mode during pre-training to enhance the model&#39;s ability to generate more accurate results when considering the code context. Therefore, we will use FIM as the default code completion method to evaluate the performance of each model in real-world programming scenarios.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the mainstream evaluation dataset for context-aware code completion is the single-line evaluation method proposed by Santacoder (Ben Allal et al., 2023). This evaluation dataset extracts single lines of code from HumanEval or MultiPL-E and evaluates the Exact Match metric of the model&#39;s generated results, given the complete preceding and following context.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_2.png&#34; alt=&#34;table_2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To further evaluate the code completion capabilities of large language models for code in a more fine-grained manner, aiXcoder has built an evaluation dataset that is larger in size, more diverse in the code being tested, longer in the context length of the code being tested, and closer to real-world development projects. This evaluation dataset will also be open-sourced on GitHub simultaneously. During the evaluation process, we ensure that different large language models for code use the same maximum sequence length of 16K and evaluate the generation performance in different scenarios, such as generating complete method blocks, conditional blocks, loop processing blocks, exception handling blocks, and a total of thirteen cases.&lt;/p&gt; &#xA;&lt;p&gt;Table 3 shows the average generation performance of different models in different languages. The final evaluation results are the average of all completion scenarios and evaluation samples. The aiXcoder 7B Base model achieves the best performance across major programming languages and various evaluation criteria, indicating that aiXcoder 7B Base has the best basic code completion capability among all open-source models of the same scale and is the most suitable base model for providing code completion capabilities in real-world programming scenarios.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_3.png&#34; alt=&#34;table_3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For each evaluation result in Table 3, there are more detailed evaluation dimensions. Tables 4 to 7 show the details of the multi-dimensional evaluation of different models in different languages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Method signature&lt;/strong&gt; indicates the model&#39;s capability to generate method signatures based on context.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Method body&lt;/strong&gt; represents the model&#39;s ability to generate a complete method body based on context, including the function signature.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Single line&lt;/strong&gt; refers to the completion of single lines of code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Method with comment&lt;/strong&gt; denotes generating a corresponding function body based on context, including function signatures and comments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Empty&lt;/strong&gt; indicates the model&#39;s ability to predict emptiness in the case of complete context.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Method body top, mid, bottom&lt;/strong&gt; show the code generation performance respectively in the upper part of the function body, the middle part, and the lower part.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;If, for, while, try, switch statement&lt;/strong&gt; represent the effects of generating conditional code blocks, loop code blocks, exception catch blocks, and conditional branch blocks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_4.png&#34; alt=&#34;table_4&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_5.png&#34; alt=&#34;table_5&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_6.png&#34; alt=&#34;table_6&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_7.png&#34; alt=&#34;table_7&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Cross-file Code Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Another important capability of large language models for code is the ability to understand code context across files, as developers often need to consider information from other files within the current project when writing code. Therefore, we adopted the CrossCodeEval (Ding et al., 2023) evaluation dataset to assess the model&#39;s ability to extract cross-file contextual information.&lt;/p&gt; &#xA;&lt;p&gt;In Table 8, we fix the context length for all models at 16K and format the input using the PSM pattern in FIM. After the model completes inference, all output results are decoded using Greedy Search. First, as a baseline, we evaluate the generation capabilities of various large code models in a single-file scenario.&lt;/p&gt; &#xA;&lt;p&gt;Then, using BM25 as the similarity metric, we search for the three most similar code blocks within the project as prompts to reassess the model&#39;s generation performance. Finally, &#34;w/Ref.&#34; indicates that we assume we know what the correct Reference code looks like, and then search for the three most similar codes within the project as prompts to re-evaluate the model&#39;s generation performance.&lt;/p&gt; &#xA;&lt;p&gt;Ultimately, the aiXcoder-7B model performs very well in all languages, demonstrating our model&#39;s ability to extract contextual information, especially cross-file contextual information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_8.png&#34; alt=&#34;table_8&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The source code in this repository is licensed under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache-2.0&lt;/a&gt; License - see the LICENSE file for details. The model weights are licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt; for academic research use; for commercial use, please apply by sending an email to &lt;a href=&#34;mailto:support@aiXcoder.com&#34;&gt;support@aiXcoder.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank all contributors to the open-source projects and datasets that made this work possible.&lt;/p&gt; &#xA;&lt;p&gt;Thank you for your interest in our Code Large Language Model. We look forward to your contributions and feedback!&lt;/p&gt;</summary>
  </entry>
</feed>