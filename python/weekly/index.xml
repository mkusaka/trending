<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-01T01:44:29Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>frappe/erpnext</title>
    <updated>2024-09-01T01:44:29Z</updated>
    <id>tag:github.com,2024-09-01:/frappe/erpnext</id>
    <link href="https://github.com/frappe/erpnext" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free and Open Source Enterprise Resource Planning (ERP)&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://erpnext.com&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/erpnext-logo.png&#34; height=&#34;128&#34;&gt; &lt;/a&gt; &#xA; &lt;h2&gt;ERPNext&lt;/h2&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt;ERP made simple&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml&#34;&gt;&lt;img src=&#34;https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml/badge.svg?event=schedule&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codetriage.com/frappe/erpnext&#34;&gt;&lt;img src=&#34;https://www.codetriage.com/frappe/erpnext/badges/users.svg?sanitize=true&#34; alt=&#34;Open Source Helpers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/frappe/erpnext&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/frappe/erpnext/branch/develop/graph/badge.svg?token=0TwvyUg3I5&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/frappe/erpnext-worker&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/frappe/erpnext-worker.svg?sanitize=true&#34; alt=&#34;docker pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://erpnext.com&#34;&gt;https://erpnext.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;ERPNext as a monolith includes the following areas for managing businesses:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-accounting&#34;&gt;Accounting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/distribution/warehouse-management-system&#34;&gt;Warehouse Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-crm&#34;&gt;CRM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-sales-purchase&#34;&gt;Sales&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-sales-purchase&#34;&gt;Purchase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-hrms&#34;&gt;HRMS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-projects&#34;&gt;Project Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-help-desk-software&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-asset-management-software&#34;&gt;Asset Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/docs/user/manual/en/quality-management&#34;&gt;Quality Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-manufacturing-erp-software&#34;&gt;Manufacturing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/open-source-website-builder-software&#34;&gt;Website Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/docs/user/manual/en/customize-erpnext&#34;&gt;Customize ERPNext&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/docs/user/manual/en/&#34;&gt;And More&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;ERPNext is built on the &lt;a href=&#34;https://github.com/frappe/frappe&#34;&gt;Frappe Framework&lt;/a&gt;, a full-stack web app framework built with Python &amp;amp; JavaScript.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;max-height: 40px;&#34;&gt; &#xA; &lt;a href=&#34;https://frappecloud.com/erpnext/signup&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/.github/try-on-f-cloud-button.svg?sanitize=true&#34; height=&#34;40&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://labs.play-with-docker.com/?stack=https://raw.githubusercontent.com/frappe/frappe_docker/main/pwd.yml&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/play-with-docker/stacks/master/assets/images/button.png&#34; alt=&#34;Try in PWD&#34; height=&#34;37&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Login for the PWD site: (username: Administrator, password: admin)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Containerized Installation&lt;/h3&gt; &#xA;&lt;p&gt;Use docker to deploy ERPNext in production or for development of &lt;a href=&#34;https://github.com/frappe/frappe&#34;&gt;Frappe&lt;/a&gt; apps. See &lt;a href=&#34;https://github.com/frappe/frappe_docker&#34;&gt;https://github.com/frappe/frappe_docker&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Manual Install&lt;/h3&gt; &#xA;&lt;p&gt;The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See &lt;a href=&#34;https://github.com/frappe/bench&#34;&gt;https://github.com/frappe/bench&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;New passwords will be created for the ERPNext &#34;Administrator&#34; user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).&lt;/p&gt; &#xA;&lt;h2&gt;Learning and community&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://frappe.school&#34;&gt;Frappe School&lt;/a&gt; - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.erpnext.com/&#34;&gt;Official documentation&lt;/a&gt; - Extensive documentation for ERPNext.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discuss.erpnext.com/&#34;&gt;Discussion Forum&lt;/a&gt; - Engage with community of ERPNext users and service providers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext_public.t.me&#34;&gt;Telegram Group&lt;/a&gt; - Get instant help from huge community of users.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/frappe/erpnext/wiki/Issue-Guidelines&#34;&gt;Issue Guidelines&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://erpnext.com/security&#34;&gt;Report Security Vulnerabilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/frappe/erpnext/wiki/Contribution-Guidelines&#34;&gt;Pull Request Requirements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;GNU/General Public License (see &lt;a href=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/license.txt&#34;&gt;license.txt&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;The ERPNext code is licensed as GNU General Public License (v3) and the Documentation is licensed as Creative Commons (CC-BY-SA-3.0) and the copyright is owned by Frappe Technologies Pvt Ltd (Frappe) and Contributors.&lt;/p&gt; &#xA;&lt;p&gt;By contributing to ERPNext, you agree that your contributions will be licensed under its GNU General Public License (v3).&lt;/p&gt; &#xA;&lt;h2&gt;Logo and Trademark Policy&lt;/h2&gt; &#xA;&lt;p&gt;Please read our &lt;a href=&#34;https://raw.githubusercontent.com/frappe/erpnext/develop/TRADEMARK_POLICY.md&#34;&gt;Logo and Trademark Policy&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ostris/ai-toolkit</title>
    <updated>2024-09-01T01:44:29Z</updated>
    <id>tag:github.com,2024-09-01:/ostris/ai-toolkit</id>
    <link href="https://github.com/ostris/ai-toolkit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Various AI scripts. Mostly Stable Diffusion stuff.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Toolkit by Ostris&lt;/h1&gt; &#xA;&lt;h2&gt;IMPORTANT NOTE - READ THIS&lt;/h2&gt; &#xA;&lt;p&gt;This is my research repo. I do a lot of experiments in it and it is possible that I will break things. If something breaks, checkout an earlier commit. This repo can train a lot of things, and it is hard to keep up with all of them.&lt;/p&gt; &#xA;&lt;h2&gt;Support my work&lt;/h2&gt; &#xA;&lt;a href=&#34;https://glif.app&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;glif.app&#34; src=&#34;https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/glif.svg?v=1&#34; width=&#34;256&#34; height=&#34;auto&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;My work on this project would not be possible without the amazing support of &lt;a href=&#34;https://glif.app/&#34;&gt;Glif&lt;/a&gt; and everyone on the team. If you want to support me, support Glif. &lt;a href=&#34;https://glif.app/&#34;&gt;Join the site&lt;/a&gt;, &lt;a href=&#34;https://discord.com/invite/nuR9zZ2nsh&#34;&gt;Join us on Discord&lt;/a&gt;, &lt;a href=&#34;https://x.com/heyglif&#34;&gt;follow us on Twitter&lt;/a&gt; and come make some cool stuff with us&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python &amp;gt;3.10&lt;/li&gt; &#xA; &lt;li&gt;Nvidia GPU with enough ram to do what you need&lt;/li&gt; &#xA; &lt;li&gt;python venv&lt;/li&gt; &#xA; &lt;li&gt;git&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ostris/ai-toolkit.git&#xA;cd ai-toolkit&#xA;git submodule update --init --recursive&#xA;python3 -m venv venv&#xA;source venv/bin/activate&#xA;# .\venv\Scripts\activate on windows&#xA;# install torch first&#xA;pip3 install torch&#xA;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ostris/ai-toolkit.git&#xA;cd ai-toolkit&#xA;git submodule update --init --recursive&#xA;python -m venv venv&#xA;.\venv\Scripts\activate&#xA;pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FLUX.1 Training&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio UI&lt;/h3&gt; &#xA;&lt;p&gt;To get started training locally with a with a custom UI, once you followed the steps above and &lt;code&gt;ai-toolkit&lt;/code&gt; is installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ai-toolkit #in case you are not yet in the ai-toolkit folder&#xA;huggingface-cli login #provide a `write` token to publish your LoRA at the end&#xA;python flux_train_ui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will instantiate a UI that will let you upload your images, caption them, train and publish your LoRA &lt;img src=&#34;https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/lora_ease_ui.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;To get started quickly, check out &lt;a href=&#34;https://x.com/araminta_k&#34;&gt;@araminta_k&lt;/a&gt; tutorial on &lt;a href=&#34;https://www.youtube.com/watch?v=HzGW_Kyermg&#34;&gt;Finetuning Flux Dev on a 3090&lt;/a&gt; with 24GB VRAM.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;You currently need a GPU with &lt;strong&gt;at least 24GB of VRAM&lt;/strong&gt; to train FLUX.1. If you are using it as your GPU to control your monitors, you probably need to set the flag &lt;code&gt;low_vram: true&lt;/code&gt; in the config file under &lt;code&gt;model:&lt;/code&gt;. This will quantize the model on CPU and should allow it to train with monitors attached. Users have gotten it to work on Windows with WSL, but there are some reports of a bug when running on windows natively. I have only tested on linux for now. This is still extremely experimental and a lot of quantizing and tricks had to happen to get it to fit on 24GB at all.&lt;/p&gt; &#xA;&lt;h3&gt;FLUX.1-dev&lt;/h3&gt; &#xA;&lt;p&gt;FLUX.1-dev has a non-commercial license. Which means anything you train will inherit the non-commercial license. It is also a gated model, so you need to accept the license on HF before using it. Otherwise, this will fail. Here are the required steps to setup a license.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Sign into HF and accept the model access here &lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-dev&#34;&gt;black-forest-labs/FLUX.1-dev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Make a file named &lt;code&gt;.env&lt;/code&gt; in the root on this folder&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/settings/tokens/new?&#34;&gt;Get a READ key from huggingface&lt;/a&gt; and add it to the &lt;code&gt;.env&lt;/code&gt; file like so &lt;code&gt;HF_TOKEN=your_key_here&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;FLUX.1-schnell&lt;/h3&gt; &#xA;&lt;p&gt;FLUX.1-schnell is Apache 2.0. Anything trained on it can be licensed however you want and it does not require a HF_TOKEN to train. However, it does require a special adapter to train with it, &lt;a href=&#34;https://huggingface.co/ostris/FLUX.1-schnell-training-adapter&#34;&gt;ostris/FLUX.1-schnell-training-adapter&lt;/a&gt;. It is also highly experimental. For best overall quality, training on FLUX.1-dev is recommended.&lt;/p&gt; &#xA;&lt;p&gt;To use it, You just need to add the assistant to the &lt;code&gt;model&lt;/code&gt; section of your config file like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;      model:&#xA;        name_or_path: &#34;black-forest-labs/FLUX.1-schnell&#34;&#xA;        assistant_lora_path: &#34;ostris/FLUX.1-schnell-training-adapter&#34;&#xA;        is_flux: true&#xA;        quantize: true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to adjust your sample steps since schnell does not require as many&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;      sample:&#xA;        guidance_scale: 1  # schnell does not do guidance&#xA;        sample_steps: 4  # 1 - 4 works well&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy the example config file located at &lt;code&gt;config/examples/train_lora_flux_24gb.yaml&lt;/code&gt; (&lt;code&gt;config/examples/train_lora_flux_schnell_24gb.yaml&lt;/code&gt; for schnell) to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Edit the file following the comments in the file&lt;/li&gt; &#xA; &lt;li&gt;Run the file like so &lt;code&gt;python run.py config/whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;A folder with the name and the training folder from the config file will be created when you start. It will have all checkpoints and images in it. You can stop the training at any time using ctrl+c and when you resume, it will pick back up from the last checkpoint.&lt;/p&gt; &#xA;&lt;p&gt;IMPORTANT. If you press crtl+c while it is saving, it will likely corrupt that checkpoint. So wait until it is done saving&lt;/p&gt; &#xA;&lt;h3&gt;Need help?&lt;/h3&gt; &#xA;&lt;p&gt;Please do not open a bug report unless it is a bug in the code. You are welcome to &lt;a href=&#34;https://discord.gg/VXmU2f5WEU&#34;&gt;Join my Discord&lt;/a&gt; and ask for help there. However, please refrain from PMing me directly with general question or support. Ask in the discord and I will answer when I can.&lt;/p&gt; &#xA;&lt;h2&gt;Training in RunPod&lt;/h2&gt; &#xA;&lt;p&gt;Example RunPod template: &lt;strong&gt;runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You need a minimum of 24GB VRAM, pick a GPU by your preference.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Example config ($0.5/hr):&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1x A40 (48 GB VRAM)&lt;/li&gt; &#xA; &lt;li&gt;19 vCPU 100 GB RAM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Custom overrides (you need some storage to clone FLUX.1, store datasets, store trained models and samples):&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;~120 GB Disk&lt;/li&gt; &#xA; &lt;li&gt;~120 GB Pod Volume&lt;/li&gt; &#xA; &lt;li&gt;Start Jupyter Notebook&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/ostris/ai-toolkit.git&#xA;cd ai-toolkit&#xA;git submodule update --init --recursive&#xA;python -m venv venv&#xA;source venv/bin/activate&#xA;pip install torch&#xA;pip install -r requirements.txt&#xA;pip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Upload your dataset&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new folder in the root, name it &lt;code&gt;dataset&lt;/code&gt; or whatever you like.&lt;/li&gt; &#xA; &lt;li&gt;Drag and drop your .jpg, .jpeg, or .png images and .txt files inside the newly created dataset folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Login into Hugging Face with an Access Token&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Get a READ token from &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;here&lt;/a&gt; and request access to Flux.1-dev model from &lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-dev&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; and paste your token.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;4. Training&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Copy an example config file located at &lt;code&gt;config/examples&lt;/code&gt; to the config folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Edit the config following the comments in the file.&lt;/li&gt; &#xA; &lt;li&gt;Change &lt;code&gt;folder_path: &#34;/path/to/images/folder&#34;&lt;/code&gt; to your dataset path like &lt;code&gt;folder_path: &#34;/workspace/ai-toolkit/your-dataset&#34;&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run the file: &lt;code&gt;python run.py config/whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Screenshot from RunPod&lt;/h3&gt; &#xA;&lt;img width=&#34;1728&#34; alt=&#34;RunPod Training Screenshot&#34; src=&#34;https://github.com/user-attachments/assets/53a1b8ef-92fa-4481-81a7-bde45a14a7b5&#34;&gt; &#xA;&lt;h2&gt;Training in Modal&lt;/h2&gt; &#xA;&lt;h3&gt;1. Setup&lt;/h3&gt; &#xA;&lt;h4&gt;ai-toolkit:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/ostris/ai-toolkit.git&#xA;cd ai-toolkit&#xA;git submodule update --init --recursive&#xA;python -m venv venv&#xA;source venv/bin/activate&#xA;pip install torch&#xA;pip install -r requirements.txt&#xA;pip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Modal:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pip install modal&lt;/code&gt; to install the modal Python package.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;modal setup&lt;/code&gt; to authenticate (if this doesn‚Äôt work, try &lt;code&gt;python -m modal setup&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Hugging Face:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Get a READ token from &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;here&lt;/a&gt; and request access to Flux.1-dev model from &lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-dev&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; and paste your token.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. Upload your dataset&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Drag and drop your dataset folder containing the .jpg, .jpeg, or .png images and .txt files in &lt;code&gt;ai-toolkit&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Configs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Copy an example config file located at &lt;code&gt;config/examples/modal&lt;/code&gt; to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Edit the config following the comments in the file, &lt;strong&gt;&lt;ins&gt;be careful and follow the example &lt;code&gt;/root/ai-toolkit&lt;/code&gt; paths&lt;/ins&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;4. Edit run_modal.py&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Set your entire local &lt;code&gt;ai-toolkit&lt;/code&gt; path at &lt;code&gt;code_mount = modal.Mount.from_local_dir&lt;/code&gt; like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;code_mount = modal.Mount.from_local_dir(&#34;/Users/username/ai-toolkit&#34;, remote_path=&#34;/root/ai-toolkit&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Choose a &lt;code&gt;GPU&lt;/code&gt; and &lt;code&gt;Timeout&lt;/code&gt; in &lt;code&gt;@app.function&lt;/code&gt; &lt;em&gt;(default is A100 40GB and 2 hour timeout)&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;5. Training&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run the config file in your terminal: &lt;code&gt;modal run run_modal.py --config-file-list-str=/root/ai-toolkit/config/whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You can monitor your training in your local terminal, or on &lt;a href=&#34;https://modal.com/&#34;&gt;modal.com&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Models, samples and optimizer will be stored in &lt;code&gt;Storage &amp;gt; flux-lora-models&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;6. Saving the model&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check contents of the volume by running &lt;code&gt;modal volume ls flux-lora-models&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download the content by running &lt;code&gt;modal volume get flux-lora-models your-model-name&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Example: &lt;code&gt;modal volume get flux-lora-models my_first_flux_lora_v1&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Screenshot from Modal&lt;/h3&gt; &#xA;&lt;img width=&#34;1728&#34; alt=&#34;Modal Traning Screenshot&#34; src=&#34;https://github.com/user-attachments/assets/7497eb38-0090-49d6-8ad9-9c8ea7b5388b&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Dataset Preparation&lt;/h2&gt; &#xA;&lt;p&gt;Datasets generally need to be a folder containing images and associated text files. Currently, the only supported formats are jpg, jpeg, and png. Webp currently has issues. The text files should be named the same as the images but with a &lt;code&gt;.txt&lt;/code&gt; extension. For example &lt;code&gt;image2.jpg&lt;/code&gt; and &lt;code&gt;image2.txt&lt;/code&gt;. The text file should contain only the caption. You can add the word &lt;code&gt;[trigger]&lt;/code&gt; in the caption file and if you have &lt;code&gt;trigger_word&lt;/code&gt; in your config, it will be automatically replaced.&lt;/p&gt; &#xA;&lt;p&gt;Images are never upscaled but they are downscaled and placed in buckets for batching. &lt;strong&gt;You do not need to crop/resize your images&lt;/strong&gt;. The loader will automatically resize them and can handle varying aspect ratios.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;EVERYTHING BELOW THIS LINE IS OUTDATED&lt;/h2&gt; &#xA;&lt;p&gt;It may still work like that, but I have not tested it in a while.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Batch Image Generation&lt;/h3&gt; &#xA;&lt;p&gt;A image generator that can take frompts from a config file or form a txt file and generate them to a folder. I mainly needed this for an SDXL test I am doing but added some polish to it so it can be used for generat batch image generation. It all runs off a config file, which you can find an example of in &lt;code&gt;config/examples/generate.example.yaml&lt;/code&gt;. Mere info is in the comments in the example&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;LoRA (lierla), LoCON (LyCORIS) extractor&lt;/h3&gt; &#xA;&lt;p&gt;It is based on the extractor in the &lt;a href=&#34;https://github.com/KohakuBlueleaf/LyCORIS&#34;&gt;LyCORIS&lt;/a&gt; tool, but adding some QOL features and LoRA (lierla) support. It can do multiple types of extractions in one run. It all runs off a config file, which you can find an example of in &lt;code&gt;config/examples/extract.example.yml&lt;/code&gt;. Just copy that file, into the &lt;code&gt;config&lt;/code&gt; folder, and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;. Then you can edit the file to your liking. and call it like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 run.py config/whatever_you_want.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also put a full path to a config file, if you want to keep it somewhere else.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 run.py &#34;/home/user/whatever_you_want.yml&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More notes on how it works are available in the example config file itself. LoRA and LoCON both support extractions of &#39;fixed&#39;, &#39;threshold&#39;, &#39;ratio&#39;, &#39;quantile&#39;. I&#39;ll update what these do and mean later. Most people used fixed, which is traditional fixed dimension extraction.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;process&lt;/code&gt; is an array of different processes to run. You can add a few and mix and match. One LoRA, one LyCON, etc.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;LoRA Rescale&lt;/h3&gt; &#xA;&lt;p&gt;Change &lt;code&gt;&amp;lt;lora:my_lora:4.6&amp;gt;&lt;/code&gt; to &lt;code&gt;&amp;lt;lora:my_lora:1.0&amp;gt;&lt;/code&gt; or whatever you want with the same effect. A tool for rescaling a LoRA&#39;s weights. Should would with LoCON as well, but I have not tested it. It all runs off a config file, which you can find an example of in &lt;code&gt;config/examples/mod_lora_scale.yml&lt;/code&gt;. Just copy that file, into the &lt;code&gt;config&lt;/code&gt; folder, and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;. Then you can edit the file to your liking. and call it like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 run.py config/whatever_you_want.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also put a full path to a config file, if you want to keep it somewhere else.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 run.py &#34;/home/user/whatever_you_want.yml&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More notes on how it works are available in the example config file itself. This is useful when making all LoRAs, as the ideal weight is rarely 1.0, but now you can fix that. For sliders, they can have weird scales form -2 to 2 or even -15 to 15. This will allow you to dile it in so they all have your desired scale&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;LoRA Slider Trainer&lt;/h3&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ostris/ai-toolkit/blob/main/notebooks/SliderTraining.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;This is how I train most of the recent sliders I have on Civitai, you can check them out in my &lt;a href=&#34;https://civitai.com/user/Ostris/models&#34;&gt;Civitai profile&lt;/a&gt;. It is based off the work by &lt;a href=&#34;https://github.com/p1atdev/LECO&#34;&gt;p1atdev/LECO&lt;/a&gt; and &lt;a href=&#34;https://github.com/rohitgandikota/erasing&#34;&gt;rohitgandikota/erasing&lt;/a&gt; But has been heavily modified to create sliders rather than erasing concepts. I have a lot more plans on this, but it is very functional as is. It is also very easy to use. Just copy the example config file in &lt;code&gt;config/examples/train_slider.example.yml&lt;/code&gt; to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;. Then you can edit the file to your liking. and call it like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 run.py config/whatever_you_want.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There is a lot more information in that example file. You can even run the example as is without any modifications to see how it works. It will create a slider that turns all animals into dogs(neg) or cats(pos). Just run it like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 run.py config/examples/train_slider.example.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And you will be able to see how it works without configuring anything. No datasets are required for this method. I will post an better tutorial soon.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Extensions!!&lt;/h2&gt; &#xA;&lt;p&gt;You can now make and share custom extensions. That run within this framework and have all the inbuilt tools available to them. I will probably use this as the primary development method going forward so I dont keep adding and adding more and more features to this base repo. I will likely migrate a lot of the existing functionality as well to make everything modular. There is an example extension in the &lt;code&gt;extensions&lt;/code&gt; folder that shows how to make a model merger extension. All of the code is heavily documented which is hopefully enough to get you started. To make an extension, just copy that example and replace all the things you need to.&lt;/p&gt; &#xA;&lt;h3&gt;Model Merger - Example Extension&lt;/h3&gt; &#xA;&lt;p&gt;It is located in the &lt;code&gt;extensions&lt;/code&gt; folder. It is a fully finctional model merger that can merge as many models together as you want. It is a good example of how to make an extension, but is also a pretty useful feature as well since most mergers can only do one model at a time and this one will take as many as you want to feed it. There is an example config file in there, just copy that to your &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;. and use it like any other config file.&lt;/p&gt; &#xA;&lt;h2&gt;WIP Tools&lt;/h2&gt; &#xA;&lt;h3&gt;VAE (Variational Auto Encoder) Trainer&lt;/h3&gt; &#xA;&lt;p&gt;This works, but is not ready for others to use and therefore does not have an example config. I am still working on it. I will update this when it is ready. I am adding a lot of features for criteria that I have used in my image enlargement work. A Critic (discriminator), content loss, style loss, and a few more. If you don&#39;t know, the VAE for stable diffusion (yes even the MSE one, and SDXL), are horrible at smaller faces and it holds SD back. I will fix this. I&#39;ll post more about this later with better examples later, but here is a quick test of a run through with various VAEs. Just went in and out. It is much worse on smaller faces than shown here.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/VAE_test1.jpg&#34; width=&#34;768&#34; height=&#34;auto&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add proper regs on sliders&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add SDXL support (base model only for now)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add plain erasing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make Textual inversion network trainer (network that spits out TI embeddings)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Change Log&lt;/h2&gt; &#xA;&lt;h4&gt;2023-08-05&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Huge memory rework and slider rework. Slider training is better thant ever with no more ram spikes. I also made it so all 4 parts of the slider algorythm run in one batch so they share gradient accumulation. This makes it much faster and more stable.&lt;/li&gt; &#xA; &lt;li&gt;Updated the example config to be something more practical and more updated to current methods. It is now a detail slide and shows how to train one without a subject. 512x512 slider training for 1.5 should work on 6GB gpu now. Will test soon to verify.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2021-10-20&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows support bug fixes&lt;/li&gt; &#xA; &lt;li&gt;Extensions! Added functionality to make and share custom extensions for training, merging, whatever. check out the example in the &lt;code&gt;extensions&lt;/code&gt; folder. Read more about that above.&lt;/li&gt; &#xA; &lt;li&gt;Model Merging, provided via the example extension.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2023-08-03&lt;/h4&gt; &#xA;&lt;p&gt;Another big refactor to make SD more modular.&lt;/p&gt; &#xA;&lt;p&gt;Made batch image generation script&lt;/p&gt; &#xA;&lt;h4&gt;2023-08-01&lt;/h4&gt; &#xA;&lt;p&gt;Major changes and update. New LoRA rescale tool, look above for details. Added better metadata so Automatic1111 knows what the base model is. Added some experiments and a ton of updates. This thing is still unstable at the moment, so hopefully there are not breaking changes.&lt;/p&gt; &#xA;&lt;p&gt;Unfortunately, I am too lazy to write a proper changelog with all the changes.&lt;/p&gt; &#xA;&lt;p&gt;I added SDXL training to sliders... but.. it does not work properly. The slider training relies on a model&#39;s ability to understand that an unconditional (negative prompt) means you do not want that concept in the output. SDXL does not understand this for whatever reason, which makes separating out concepts within the model hard. I am sure the community will find a way to fix this over time, but for now, it is not going to work properly. And if any of you are thinking &#34;Could we maybe fix it by adding 1 or 2 more text encoders to the model as well as a few more entirely separate diffusion networks?&#34; No. God no. It just needs a little training without every experimental new paper added to it. The KISS principal.&lt;/p&gt; &#xA;&lt;h4&gt;2023-07-30&lt;/h4&gt; &#xA;&lt;p&gt;Added &#34;anchors&#34; to the slider trainer. This allows you to set a prompt that will be used as a regularizer. You can set the network multiplier to force spread consistency at high weights&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SylphAI-Inc/AdalFlow</title>
    <updated>2024-09-01T01:44:29Z</updated>
    <id>tag:github.com,2024-09-01:/SylphAI-Inc/AdalFlow</id>
    <link href="https://github.com/SylphAI-Inc/AdalFlow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AdalFlow: The ‚ÄúPyTorch‚Äù library to auto-optimize any LLM tasks.&lt;/p&gt;&lt;hr&gt;&lt;h4 align=&#34;center&#34;&gt; &lt;img alt=&#34;AdalFlow logo&#34; src=&#34;https://raw.githubusercontent.com/SylphAI-Inc/LightRAG/main/docs/source/_static/images/adalflow-logo.png&#34; style=&#34;width: 100%;&#34;&gt; &lt;/h4&gt; &#xA;&lt;h2&gt; &lt;p align=&#34;center&#34;&gt; ‚ö° The Library to Build and Auto-optimize LLM Applications ‚ö° &lt;/p&gt; &lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1TKw_JHE42Z_AWo8UuRYZCO2iuMgyslTZ?usp=sharing&#34;&gt; &lt;img alt=&#34;Try Quickstart in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;a href=&#34;https://adalflow.sylph.ai/&#34;&gt;All Documentation&lt;/a&gt; | &lt;a href=&#34;https://adalflow.sylph.ai/apis/components/components.model_client.html&#34;&gt;Models&lt;/a&gt; | &lt;a href=&#34;https://adalflow.sylph.ai/apis/components/components.retriever.html&#34;&gt;Retrievers&lt;/a&gt; | &lt;a href=&#34;https://adalflow.sylph.ai/apis/components/components.agent.html&#34;&gt;Agents&lt;/a&gt; | &lt;a href=&#34;https://adalflow.sylph.ai/use_cases/question_answering.html&#34;&gt;Trainer &amp;amp; Optimizers&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/adalflow/&#34;&gt; &lt;img alt=&#34;PyPI Version&#34; src=&#34;https://img.shields.io/pypi/v/adalflow?style=flat-square&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://star-history.com/#SylphAI-Inc/LightRAG&#34;&gt; &lt;img alt=&#34;GitHub stars&#34; src=&#34;https://img.shields.io/github/stars/SylphAI-Inc/LightRAG?style=flat-square&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/SylphAI-Inc/LightRAG/issues&#34;&gt; &lt;img alt=&#34;Open Issues&#34; src=&#34;https://img.shields.io/github/issues-raw/SylphAI-Inc/LightRAG?style=flat-square&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://opensource.org/license/MIT&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/SylphAI-Inc/LightRAG&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/ezzszrRZvT&#34;&gt; &lt;img alt=&#34;discord-invite&#34; src=&#34;https://dcbadge.vercel.app/api/server/ezzszrRZvT?style=flat&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;!-- &lt;a href=&#34;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&#34;&gt;&#xA;        &lt;img alt=&#34;Try Quickstart in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34;&gt;&#xA;    &lt;/a&gt; --&gt; &#xA;&lt;!-- &lt;a href=&#34;https://pypistats.org/packages/lightrag&#34;&gt;&#xA;&lt;img alt=&#34;PyPI Downloads&#34; src=&#34;https://img.shields.io/pypi/dm/lightRAG?style=flat-square&#34;&gt;&#xA;&lt;/a&gt; --&gt; &#xA;&lt;h1&gt;Why AdalFlow&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Embracing a design pattern similar to PyTorch, AdalFlow is powerful, light, modular, and robust. AdalFlow provides &lt;code&gt;Model-agnostic&lt;/code&gt; building blocks to build LLM task pipeline, ranging from RAG, Agents to classical NLP tasks like text classification and named entity recognition. It is easy to get high performance even with just basic manual promting.&lt;/li&gt; &#xA; &lt;li&gt;AdalFlow provides a unified auto-differentiative framework for both zero-shot prompt optimization and few-shot optimization. It advances existing auto-optimization research, including &lt;code&gt;Text-Grad&lt;/code&gt; and &lt;code&gt;DsPy&lt;/code&gt;. Through our research, &lt;code&gt;Text-Grad 2.0&lt;/code&gt; and &lt;code&gt;Learn-to-Reason Few-shot In Context Learning&lt;/code&gt;, AdalFlow &lt;code&gt;Trainer&lt;/code&gt; achieves the highest accuracy while being the most token-efficient.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;!-- It advances existing auto-optimization research, including Text-Grad and DsPy. Through our research, Text-Grad 2.0, and Learn-to-Reason Few-shot In-Context Learning, AdalFlow Trainer achieves the highest accuracy while being the most token-efficient. --&gt; &#xA;&lt;!-- AdalFlow not only helps developers build model-agnostic LLM task pipelines with full control over prompts and output processing, but it also auto-optimizes these pipelines to achieve SOTA accuracy. --&gt; &#xA;&lt;!-- Embracing a design pattern similar to PyTorch, AdalFlow is powerful, light, modular, and robust. --&gt; &#xA;&lt;p&gt;Here is our optimization demonstration on a text classification task:&lt;/p&gt; &#xA;&lt;!-- &lt;p align=&#34;center&#34;&gt;&#xA;  &lt;img src=&#34;docs/source/_static/images/classification_training_map.png&#34; alt=&#34;AdalFlow Auto-optimization&#34; style=&#34;width: 80%;&#34;&gt;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA;  &lt;img src=&#34;docs/source/_static/images/classification_opt_prompt.png&#34; alt=&#34;AdalFlow Auto-optimization&#34; style=&#34;width: 80%;&#34;&gt;&#xA;&lt;/p&gt; --&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;background-color: #f0f0f0;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SylphAI-Inc/LightRAG/main/docs/source/_static/images/classification_training_map.png&#34; style=&#34;width: 80%;&#34; alt=&#34;AdalFlow Auto-optimization&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;background-color: #f0f0f0;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SylphAI-Inc/LightRAG/main/docs/source/_static/images/classification_opt_prompt.png&#34; alt=&#34;AdalFlow Optimized Prompt&#34; style=&#34;width: 80%;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Among all libraries, we achieved the highest accuracy with manual prompting (starting at 82%) and the highest accuracy after optimization.&lt;/p&gt; &#xA;&lt;p&gt;Further reading: &lt;a href=&#34;https://adalflow.sylph.ai/use_cases/classification.html&#34;&gt;Optimize Classification&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Light, Modular, and Model-agnositc Task Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;LLMs are like water; AdalFlow help developers quickly shape them into any applications, from GenAI applications such as chatbots, translation, summarization, code generation, RAG, and autonomous agents to classical NLP tasks like text classification and named entity recognition.&lt;/p&gt; &#xA;&lt;p&gt;Only two fundamental but powerful base classes: &lt;code&gt;Component&lt;/code&gt; for the pipeline and &lt;code&gt;DataClass&lt;/code&gt; for data interaction with LLMs. The result is a library with bare minimum abstraction, providing developers with &lt;em&gt;maximum customizability&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You have full control over the prompt template, the model you use, and the output parsing for your task pipeline.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SylphAI-Inc/LightRAG/main/docs/source/_static/images/AdalFlow_task_pipeline.png&#34; alt=&#34;AdalFlow Task Pipeline&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- LLMs are like water; they can be shaped into anything, from GenAI applications such as chatbots, translation, summarization, code generation, and autonomous agents to classical NLP tasks like text classification and named entity recognition. They interact with the world beyond the model‚Äôs internal knowledge via retrievers, memory, and tools (function calls). Each use case is unique in its data, business logic, and user experience.&#xA;&#xA;Because of this, no library can provide out-of-the-box solutions. Users must build towards their own use case. This requires the library to be modular, robust, and have a clean, readable codebase. The only code you should put into production is code you either 100% trust or are 100% clear about how to customize and iterate. --&gt; &#xA;&lt;!-- This is what AdalFlow is: light, modular, and robust, with a 100% readable codebase. --&gt; &#xA;&lt;p&gt;Further reading: &lt;a href=&#34;https://www.linkedin.com/posts/li-yin-ai_both-ai-research-and-engineering-use-pytorch-activity-7189366364694892544-Uk1U?utm_source=share&amp;amp;utm_medium=member_desktop&#34;&gt;How We Started&lt;/a&gt;, &lt;a href=&#34;https://adalflow.sylph.ai/&#34;&gt;Introduction&lt;/a&gt;, &lt;a href=&#34;https://adalflow.sylph.ai/tutorials/lightrag_design_philosophy.html&#34;&gt;Design Philosophy&lt;/a&gt; and &lt;a href=&#34;https://adalflow.sylph.ai/tutorials/class_hierarchy.html&#34;&gt;Class hierarchy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!--&#xA;&#xA;**PyTorch**&#xA;&#xA;```python&#xA;import torch&#xA;import torch.nn as nn&#xA;&#xA;class Net(nn.Module):&#xA;   def __init__(self):&#xA;      super(Net, self).__init__()&#xA;      self.conv1 = nn.Conv2d(1, 32, 3, 1)&#xA;      self.conv2 = nn.Conv2d(32, 64, 3, 1)&#xA;      self.dropout1 = nn.Dropout2d(0.25)&#xA;      self.dropout2 = nn.Dropout2d(0.5)&#xA;      self.fc1 = nn.Linear(9216, 128)&#xA;      self.fc2 = nn.Linear(128, 10)&#xA;&#xA;   def forward(self, x):&#xA;      x = self.conv1(x)&#xA;      x = self.conv2(x)&#xA;      x = self.dropout1(x)&#xA;      x = self.dropout2(x)&#xA;      x = self.fc1(x)&#xA;      return self.fc2(x)&#xA;``` --&gt; &#xA;&lt;h2&gt;Unified Framework for Auto-Optimization&lt;/h2&gt; &#xA;&lt;p&gt;AdalFlow provides token-efficient and high-performing prompt optimization within a unified framework. To optimize your pipeline, simply define a &lt;code&gt;Parameter&lt;/code&gt; and pass it to our &lt;code&gt;Generator&lt;/code&gt;. Whether you need to optimize task instructions or few-shot demonstrations, our unified framework offers an easy way to &lt;strong&gt;diagnose&lt;/strong&gt;, &lt;strong&gt;visualize&lt;/strong&gt;, &lt;strong&gt;debug&lt;/strong&gt;, and &lt;strong&gt;train&lt;/strong&gt; your pipeline.&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://adalflow.sylph.ai/tutorials/trace_graph.html&#34;&gt;Trace Graph&lt;/a&gt; demonstrates how our auto-differentiation works.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Trainable Task Pipeline&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Just define it as a &lt;code&gt;Parameter&lt;/code&gt; and pass it to our &lt;code&gt;Generator&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SylphAI-Inc/LightRAG/main/docs/source/_static/images/Trainable_task_pipeline.png&#34; alt=&#34;AdalFlow Trainable Task Pipeline&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;AdalComponent &amp;amp; Trainer&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;AdalComponent&lt;/code&gt; acts as the &lt;code&gt;interpreter&lt;/code&gt; between task pipeline and the trainer, defining training and validation steps, optimizers, evaluators, loss functions, backward engine for textual gradients or tracing the demonstrations, the teacher generator.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SylphAI-Inc/LightRAG/main/docs/source/_static/images/trainer.png&#34; alt=&#34;AdalFlow AdalComponent &amp;amp; Trainer&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Quick Install&lt;/h1&gt; &#xA;&lt;p&gt;Install AdalFlow with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install adalflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://adalflow.sylph.ai/get_started/installation.html&#34;&gt;full installation guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;AdalFlow full documentation available at &lt;a href=&#34;https://adalflow.sylph.ai/&#34;&gt;adalflow.sylph.ai&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/posts/li-yin-ai_both-ai-research-and-engineering-use-pytorch-activity-7189366364694892544-Uk1U?utm_source=share&amp;amp;utm_medium=member_desktop&#34;&gt;How We Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adalflow.sylph.ai/&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adalflow.sylph.ai/get_started/installation.html&#34;&gt;Full installation guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adalflow.sylph.ai/tutorials/lightrag_design_philosophy.html&#34;&gt;Design philosophy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adalflow.sylph.ai/tutorials/class_hierarchy.html&#34;&gt;Class hierarchy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adalflow.sylph.ai/tutorials/index.html&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adalflow.sylph.ai/apis/components/components.model_client.html&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adalflow.sylph.ai/apis/components/components.retriever.html&#34;&gt;Supported Retrievers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adalflow.sylph.ai/apis/index.html&#34;&gt;API reference&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;AdalFlow: A Tribute to Ada Lovelace&lt;/h1&gt; &#xA;&lt;p&gt;AdalFlow is named in honor of &lt;a href=&#34;https://en.wikipedia.org/wiki/Ada_Lovelace&#34;&gt;Ada Lovelace&lt;/a&gt;, the pioneering female mathematician who first recognized that machines could do more than just calculations. As a female-led team, we aim to inspire more women to enter the AI field.&lt;/p&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SylphAI-Inc/LightRAG/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=SylphAI-Inc/LightRAG&amp;amp;max=2000&#34; alt=&#34;contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Many existing works greatly inspired this project! Here is a non-exhaustive list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìö &lt;a href=&#34;https://github.com/pytorch/pytorch/&#34;&gt;PyTorch&lt;/a&gt; for design philosophy and design pattern of &lt;code&gt;Component&lt;/code&gt;, &lt;code&gt;Parameter&lt;/code&gt;, &lt;code&gt;Sequential&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;a href=&#34;https://github.com/karpathy/micrograd&#34;&gt;Micrograd&lt;/a&gt;: A tiny autograd engine for our auto-differentiative architecture.&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;a href=&#34;https://github.com/zou-group/textgrad&#34;&gt;Text-Grad&lt;/a&gt; for the &lt;code&gt;Textual Gradient Descent&lt;/code&gt; text optimizer.&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;a href=&#34;https://github.com/stanfordnlp/dspy&#34;&gt;DSPy&lt;/a&gt; for inspiring the &lt;code&gt;__{input/output}__fields&lt;/code&gt; in our &lt;code&gt;DataClass&lt;/code&gt; and the bootstrap few-shot optimizer.&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;a href=&#34;https://github.com/google-deepmind/opro&#34;&gt;ORPO&lt;/a&gt; for adding past text instruction along with its accuracy in the text optimizer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{Yin2024AdalFlow,&#xA;  author = {Li Yin},&#xA;  title = {{AdalFlow: The Library for Large Language Model (LLM) Applications}},&#xA;  month = {7},&#xA;  year = {2024},&#xA;  doi = {10.5281/zenodo.12639531},&#xA;  url = {https://github.com/SylphAI-Inc/LightRAG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>