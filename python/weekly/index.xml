<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-22T02:00:28Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mikel-brostrom/yolov8_tracking</title>
    <updated>2023-01-22T02:00:28Z</updated>
    <id>tag:github.com,2023-01-22:/mikel-brostrom/yolov8_tracking</id>
    <link href="https://github.com/mikel-brostrom/yolov8_tracking" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-time multi-object tracking and segmentation using YOLOv8&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SOTA real-time multi-object tracking and segmentation&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mikel-brostrom/yolov8_tracking/master/trackers/strongsort/results/track_all_seg_1280_025conf.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/actions&#34;&gt;&lt;img src=&#34;https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/workflows/CI%20CPU%20testing/badge.svg?sanitize=true&#34; alt=&#34;CI CPU testing&#34;&gt;&lt;/a&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://doi.org/10.5281/zenodo.7452874&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.7452874.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains a highly configurable two-stage-tracker that adjusts to different deployment scenarios. It can jointly perform multiple object tracking and instance segmentation (MOTS). The detections generated by &lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;YOLOv8&lt;/a&gt;, a family of object detection architectures and models pretrained on the &lt;a href=&#34;https://arxiv.org/abs/1405.0312&#34;&gt;COCO&lt;/a&gt; dataset, are passed to the tracker of your choice. Supported ones at the moment are: &lt;a href=&#34;https://github.com/dyhBUPT/StrongSORT&#34;&gt;StrongSORT&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.13514&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KaiyangZhou/deep-person-reid&#34;&gt;OSNet&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.00953&#34;&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/noahcao/OC_SORT&#34;&gt;OCSORT&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.14360&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ifzhang/ByteTrack&#34;&gt;ByteTrack&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.06864&#34;&gt;&lt;/a&gt;. They can track any object that your Yolov8 model was trained to detect.&lt;/p&gt; &#xA;&lt;h2&gt;Why using this tracking toolbox?&lt;/h2&gt; &#xA;&lt;p&gt;Everything is designed with simplicity and flexibility in mind. We don&#39;t hyperfocus on results on a single dataset, we prioritize real-world results. If you don&#39;t get good tracking results on your custom dataset with the out-of-the-box tracker configurations, use the &lt;code&gt;evolve.py&lt;/code&gt; script for tracker hyperparameter tuning.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules https://github.com/mikel-brostrom/yolov8_tracking.git  # clone recursively&#xA;cd yolov8_tracking&#xA;pip install -r requirements.txt  # install dependencies&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tutorials&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data&#34;&gt;Yolov5 training (link to external repository)&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://kaiyangzhou.github.io/deep-person-reid/user_guide.html&#34;&gt;Deep appearance descriptor training (link to external repository)&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/ReID-multi-framework-model-export&#34;&gt;ReID model export to ONNX, OpenVINO, TensorRT and TorchScript&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/How-to-evaluate-on-custom-tracking-dataset&#34;&gt;Evaluation on custom tracking dataset&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Inference acceleration with Nebullvm&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1J6dl90-zOjNNtcwhw7Yuuxqg5oWp_YJa?usp=sharing&#34;&gt;Yolov5&lt;/a&gt;&amp;nbsp;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1APUZ1ijCiQFBR9xD0gUvFUOC8yOJIvHm?usp=sharing&#34;&gt;ReID&lt;/a&gt;&amp;nbsp;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt;&#xA; &lt;/ul&gt;&#xA;&lt;/details&gt;   &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Experiments&lt;/summary&gt; &#xA; &lt;p&gt;In inverse chronological order:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Evaluation-of-the-params-evolved-for-first-half-of-MOT17-on-the-complete-MOT17&#34;&gt;Evaluation of the params evolved for first half of MOT17 on the complete MOT17&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Segmentation-model-vs-object-detetion-model-on-MOT-metrics&#34;&gt;Segmentation model vs object detetion model on MOT metrics&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Masked-detection-crops-vs-regular-detection-crops-for-ReID-feature-extraction&#34;&gt;Effect of masking objects before feature extraction&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/conf-thres-vs-MOT-metrics&#34;&gt;conf-thres vs HOTA, MOTA and IDF1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Effect-of-KF-updates-ahead-for-tracks-with-no-associations,-on-MOT17&#34;&gt;Effect of KF updates ahead for tracks with no associations on MOT17&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Effect-of-passing-full-image-input-vs-1280-re-scaled-to-StrongSORT-on-MOT17&#34;&gt;Effect of full images vs 1280 input to StrongSORT on MOT17&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/OSNet-architecture-performances-on-MOT16&#34;&gt;Effect of different OSNet architectures on MOT16&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/StrongSORT-vs-BoTSORT-vs-OCSORT&#34;&gt;Yolov5 StrongSORT vs BoTSORT vs OCSORT&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Yolov5 &lt;a href=&#34;https://arxiv.org/abs/2206.14651&#34;&gt;BoTSORT&lt;/a&gt; branch: &lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/tree/botsort&#34;&gt;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/tree/botsort&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/MOT-17-evaluation-(private-detector)&#34;&gt;Yolov5 StrongSORT OSNet vs other trackers MOT17&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Yolov5DeepSORTwithOSNet-vs-Yolov5StrongSORTwithOSNet-ablation-study-on-MOT16&#34;&gt;StrongSORT MOT16 ablation study&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/MOT-16-evaluation&#34;&gt;Yolov5 StrongSORT OSNet vs other trackers MOT16 (deprecated)&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt;&#xA; &lt;/ul&gt;&#xA;&lt;/details&gt;   &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Custom object detection architecture&lt;/summary&gt; &#xA; &lt;p&gt;The trackers provided in this repo can be used with other object detectors than Yolov5. Make sure that the output of your detector has the following format:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(x1,y1, x2, y2, obj, cls0, cls1, ..., clsn)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;pass this directly to the tracker here:&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/raw/a4bc0c38c33023fab9e5481861d9520eb81e28bc/track.py#L189&#34;&gt;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/blob/a4bc0c38c33023fab9e5481861d9520eb81e28bc/track.py#L189&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Tracking&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python track.py --yolo-weights yolov8n.pt     # bboxes only&#xA;                                 yolov8-seg.pt  # bboxes + segmentation masks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tracking methods&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python track.py --tracking-method strongsort&#xA;                                    ocsort&#xA;                                    bytetrack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tracking sources&lt;/summary&gt; &#xA; &lt;p&gt;Tracking can be run on most video formats&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python track.py --source 0  # webcam&#xA;                           img.jpg  # image&#xA;                           vid.mp4  # video&#xA;                           path/  # directory&#xA;                           path/*.jpg  # glob&#xA;                           &#39;https://youtu.be/Zgi9g1ksQHc&#39;  # YouTube&#xA;                           &#39;rtsp://example.com/media.mp4&#39;  # RTSP, RTMP, HTTP stream&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Select Yolov8 model&lt;/summary&gt; &#xA; &lt;p&gt;There is a clear trade-off between model inference speed and overall performance. In order to make it possible to fulfill your inference speed/accuracy needs you can select a Yolov5 family model for automatic download. These model can be further optimized for you needs by the &lt;a href=&#34;https://github.com/ultralytics/yolov5/raw/master/export.py&#34;&gt;export.py&lt;/a&gt; script&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;&#xA;$ python track.py --source 0 --yolo-weights yolov8n.pt --img 640&#xA;                                            yolov8s.tflite&#xA;                                            yolov8m.pt&#xA;                                            yolov8l.onnx &#xA;                                            yolov8x.pt --img 1280&#xA;                                            ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Select ReID model&lt;/summary&gt; &#xA; &lt;p&gt;Some tracking methods combine appearance description and motion in the process of tracking. For those which use appearance, you can choose a ReID model based on your needs from this &lt;a href=&#34;https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO&#34;&gt;ReID model zoo&lt;/a&gt;. These model can be further optimized for you needs by the &lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/raw/master/reid_export.py&#34;&gt;reid_export.py&lt;/a&gt; script&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;&#xA;$ python track.py --source 0 --reid-weights osnet_x0_25_market1501.pt&#xA;                                            mobilenetv2_x1_4_msmt17.engine&#xA;                                            resnet50_msmt17.onnx&#xA;                                            osnet_x1_0_msmt17.pt&#xA;                                            ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Filter tracked classes&lt;/summary&gt; &#xA; &lt;p&gt;By default the tracker tracks all MS COCO classes.&lt;/p&gt; &#xA; &lt;p&gt;If you want to track a subset of the classes that you model predicts, add their corresponding index after the classes flag,&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python track.py --source 0 --yolo-weights yolov8s.pt --classes 16 17  # COCO yolov8 model. Track cats and dogs, only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/&#34;&gt;Here&lt;/a&gt; is a list of all the possible objects that a Yolov8 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Updates with predicted-ahead bbox in StrongSORT&lt;/summary&gt; &#xA; &lt;p&gt;If your use-case contains many occlussions and the motion trajectiories are not too complex, you will most certainly benefit from updating the Kalman Filter by its own predicted state. Select the number of predictions that suits your needs here:&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/raw/b1da64717ef50e1f60df2f1d51e1ff91d3b31ed4/trackers/strong_sort/configs/strong_sort.yaml#L7&#34;&gt;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/blob/b1da64717ef50e1f60df2f1d51e1ff91d3b31ed4/trackers/strong_sort/configs/strong_sort.yaml#L7&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Save the trajectories to you video by:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python track.py --source ... --save-trajectories --save-vid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mikel-brostrom/yolov8_tracking/master/trackers/strong_sort/results/preds_example.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;MOT compliant results&lt;/summary&gt; &#xA; &lt;p&gt;Can be saved to your experiment folder &lt;code&gt;runs/track/&amp;lt;yolo_model&amp;gt;_&amp;lt;deep_sort_model&amp;gt;/&lt;/code&gt; by&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python track.py --source ... --save-txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tracker hyperparameter tuning&lt;/summary&gt; &#xA; &lt;p&gt;We use a fast and elitist multiobjective genetic algorithm for tracker hyperparameter tuning. By default the objectives are: HOTA, MOTA, IDF1. Run it by&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python evolve.py --tracking-method strongsort --benchmark MOT17 --n-trials 100  # tune strongsort for MOT17&#xA;                   --tracking-method ocsort     --benchmark &amp;lt;your-custom-dataset&amp;gt; --objective HOTA, # tune ocsort for maximizing HOTA on your custom tracking dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The set of hyperparameters leading to the best HOTA result are written to the tracker&#39;s config file.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For Yolov5 StrongSORT OSNet bugs and feature requests please visit &lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/issues&#34;&gt;GitHub Issues&lt;/a&gt;. For business inquiries or professional support requests please send an email to: &lt;a href=&#34;mailto:yolov5.deepsort.pytorch@gmail.com&#34;&gt;yolov5.deepsort.pytorch@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmyolo</title>
    <updated>2023-01-22T02:00:28Z</updated>
    <id>tag:github.com,2023-01-22:/open-mmlab/mmyolo</id>
    <link href="https://github.com/open-mmlab/mmyolo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab YOLO series toolbox and benchmark. Implemented RTMDet, YOLOv5, YOLOv6, YOLOv7, YOLOv8,YOLOX, PPYOLOE, etc.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;100%&#34; src=&#34;https://user-images.githubusercontent.com/27466624/213130448-1f8529fd-2247-4ac4-851c-acd0148a49b9.png&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/mmyolo&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mmyolo&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmyolo/workflows/deploy/badge.svg?sanitize=true&#34; alt=&#34;deploy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmyolo&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmyolo/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmyolo.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmyolo.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmyolo.svg?sanitize=true&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/&#34;&gt;ğŸ“˜Documentation&lt;/a&gt; | &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/get_started.html&#34;&gt;ğŸ› ï¸Installation&lt;/a&gt; | &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/model_zoo.html&#34;&gt;ğŸ‘€Model Zoo&lt;/a&gt; | &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/notes/changelog.html&#34;&gt;ğŸ†•Update News&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/issues/new/choose&#34;&gt;ğŸ¤”Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/README_zh-CN.md&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ğŸ“„ Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#--whats-new-&#34;&gt;ğŸ¥³ ğŸš€ What&#39;s New&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-highlight-&#34;&gt;âœ¨ Highlight&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-introduction-&#34;&gt;ğŸ“– Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#%EF%B8%8F-installation-&#34;&gt;ğŸ› ï¸ Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-tutorial-&#34;&gt;ğŸ‘¨â€ğŸ« Tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-overview-of-benchmark-and-model-zoo-&#34;&gt;ğŸ“Š Overview of Benchmark and Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-faq-&#34;&gt;â“ FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-contributing-&#34;&gt;ğŸ™Œ Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-acknowledgement-&#34;&gt;ğŸ¤ Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#%EF%B8%8F-citation-&#34;&gt;ğŸ–Šï¸ Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-license-&#34;&gt;ğŸ« License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#%EF%B8%8F-projects-in-openmmlab-&#34;&gt;ğŸ—ï¸ Projects in OpenMMLab&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ¥³ ğŸš€ What&#39;s New &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;ğŸ’ &lt;strong&gt;v0.4.0&lt;/strong&gt; was released on 18/1/2023:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Implemented &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/configs/yolov8/README.md&#34;&gt;YOLOv8&lt;/a&gt; object detection model, and supports model deployment in &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/projects/easydeploy&#34;&gt;projects/easydeploy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Added Chinese and English versions of &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/docs/en/algorithm_descriptions/yolov8_description.md&#34;&gt;Algorithm principles and implementation with YOLOv8&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For release history and update details, please refer to &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/notes/changelog.html&#34;&gt;changelog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;âœ¨ Highlight &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We are excited to announce our latest work on real-time object recognition tasks, &lt;strong&gt;RTMDet&lt;/strong&gt;, a family of fully convolutional single-stage detectors. RTMDet not only achieves the best parameter-accuracy trade-off on object detection from tiny to extra-large model sizes but also obtains new state-of-the-art performance on instance segmentation and rotated object detection tasks. Details can be found in the &lt;a href=&#34;https://arxiv.org/abs/2212.07784&#34;&gt;technical report&lt;/a&gt;. Pre-trained models are &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/rtmdet&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/real-time-instance-segmentation-on-mscoco?p=rtmdet-an-empirical-study-of-designing-real&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rtmdet-an-empirical-study-of-designing-real/real-time-instance-segmentation-on-mscoco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/object-detection-in-aerial-images-on-dota-1?p=rtmdet-an-empirical-study-of-designing-real&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rtmdet-an-empirical-study-of-designing-real/object-detection-in-aerial-images-on-dota-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/object-detection-in-aerial-images-on-hrsc2016?p=rtmdet-an-empirical-study-of-designing-real&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rtmdet-an-empirical-study-of-designing-real/object-detection-in-aerial-images-on-hrsc2016&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;AP&lt;/th&gt; &#xA;   &lt;th&gt;FPS(TRT FP16 BS1 3090)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Object Detection&lt;/td&gt; &#xA;   &lt;td&gt;COCO&lt;/td&gt; &#xA;   &lt;td&gt;52.8&lt;/td&gt; &#xA;   &lt;td&gt;322&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Instance Segmentation&lt;/td&gt; &#xA;   &lt;td&gt;COCO&lt;/td&gt; &#xA;   &lt;td&gt;44.6&lt;/td&gt; &#xA;   &lt;td&gt;188&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rotated Object Detection&lt;/td&gt; &#xA;   &lt;td&gt;DOTA&lt;/td&gt; &#xA;   &lt;td&gt;78.9(single-scale)/81.3(multi-scale)&lt;/td&gt; &#xA;   &lt;td&gt;121&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/12907710/208044554-1e8de6b5-48d8-44e4-a7b5-75076c7ebb71.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;MMYOLO currently only implements the object detection algorithm, but it has a significant training acceleration compared to the MMDeteciton version. The training speed is 2.6 times faster than the previous version.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“– Introduction &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO is an open source toolbox for YOLO series algorithms based on PyTorch and &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;. It is a part of the &lt;a href=&#34;https://openmmlab.com/&#34;&gt;OpenMMLab&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;The master branch works with &lt;strong&gt;PyTorch 1.6+&lt;/strong&gt;. &lt;img src=&#34;https://user-images.githubusercontent.com/45811724/190993591-bd3f1f11-1c30-4b93-b5f4-05c9ff64ff7f.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Major features&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;ğŸ•¹ï¸ &lt;strong&gt;Unified and convenient benchmark&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMYOLO unifies the implementation of modules in various YOLO algorithms and provides a unified benchmark. Users can compare and analyze in a fair and convenient way.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;ğŸ“š &lt;strong&gt;Rich and detailed documentation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMYOLO provides rich documentation for getting started, model deployment, advanced usages, and algorithm analysis, making it easy for users at different levels to get started and make extensions quickly.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;ğŸ§© &lt;strong&gt;Modular Design&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMYOLO decomposes the framework into different components where users can easily customize a model by combining different modules with various training and testing strategies.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/27466624/199999337-0544a4cb-3cbd-4f3e-be26-bcd9e74db7ff.jpg&#34; alt=&#34;BaseModule-P5&#34;&gt; The figure above is contributed by RangeKing@GitHub, thank you very much! &#xA; &lt;p&gt;And the figure of P6 model is in &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/model_design.md&#34;&gt;model_design.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;ğŸ› ï¸ Installation &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO relies on PyTorch, MMCV, MMEngine, and MMDetection. Below are quick steps for installation. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/get_started.md&#34;&gt;Install Guide&lt;/a&gt; for more detailed instructions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n open-mmlab python=3.8 pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y&#xA;conda activate open-mmlab&#xA;pip install openmim&#xA;mim install &#34;mmengine&amp;gt;=0.3.1&#34;&#xA;mim install &#34;mmcv&amp;gt;=2.0.0rc1,&amp;lt;2.1.0&#34;&#xA;mim install &#34;mmdet&amp;gt;=3.0.0rc5,&amp;lt;3.1.0&#34;&#xA;git clone https://github.com/open-mmlab/mmyolo.git&#xA;cd mmyolo&#xA;# Install albumentations&#xA;pip install -r requirements/albu.txt&#xA;# Install MMYOLO&#xA;mim install -v -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ‘¨â€ğŸ« Tutorial &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO is based on MMDetection and adopts the same code structure and design approach. To get better use of this, please read &lt;a href=&#34;https://mmdetection.readthedocs.io/en/latest/get_started.html&#34;&gt;MMDetection Overview&lt;/a&gt; for the first understanding of MMDetection.&lt;/p&gt; &#xA;&lt;p&gt;The usage of MMYOLO is almost identical to MMDetection and all tutorials are straightforward to use, you can also learn about &lt;a href=&#34;https://mmdetection.readthedocs.io/en/3.x/&#34;&gt;MMDetection User Guide and Advanced Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For different parts from MMDetection, we have also prepared user guides and advanced guides, please read our &lt;a href=&#34;https://mmyolo.readthedocs.io/zenh_CN/latest/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;User Guides&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/user_guides/index.html#train-test&#34;&gt;Train &amp;amp; Test&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/config.md&#34;&gt;Learn about Configs with YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/user_guides/index.html#get-started-to-deployment&#34;&gt;From getting started to deployment&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/custom_dataset.md&#34;&gt;Custom Dataset&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/yolov5_tutorial.md&#34;&gt;From getting started to deployment with YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmdetection.readthedocs.io/en/latest/user_guides/index.html#useful-tools&#34;&gt;Useful Tools&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/visualization.md&#34;&gt;Visualization&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/useful_tools.md&#34;&gt;Useful Tools&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Algorithm description&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/algorithm_descriptions/index.html#essential-basics&#34;&gt;Essential Basics&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/model_design.md&#34;&gt;Model design-related instructions&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/algorithm_descriptions/index.html#algorithm-principles-and-implementation&#34;&gt;Algorithm principles and implementation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/yolov5_description.md&#34;&gt;Algorithm principles and implementation with YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/rtmdet_description.md&#34;&gt;Algorithm principles and implementation with RTMDet&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/yolov8_description.md&#34;&gt;Algorithm principles and implementation with YOLOv8&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deployment Guides&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/deploy/index.html#basic-deployment-guide&#34;&gt;Basic Deployment Guide&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/deploy/basic_deployment_guide.md&#34;&gt;Basic Deployment Guide&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/deploy/index.html#deployment-tutorial&#34;&gt;Deployment Tutorial&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/deploy/yolov5_deployment.md&#34;&gt;YOLOv5 Deployment&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Advanced Guides&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/advanced_guides/data_flow.md&#34;&gt;Data flow&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/advanced_guides/how_to.md&#34;&gt;How to&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/advanced_guides/plugins.md&#34;&gt;Plugins&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“Š Overview of Benchmark and Model Zoo &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Results and models are available in the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/model_zoo.md&#34;&gt;model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;b&gt;Supported Algorithms&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov5&#34;&gt;YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolox&#34;&gt;YOLOX&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/rtmdet&#34;&gt;RTMDet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov6&#34;&gt;YOLOv6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov7&#34;&gt;YOLOv7&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/ppyoloe&#34;&gt;PPYOLOE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov8&#34;&gt;YOLOv8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;Module Components&lt;/b&gt; &#xA; &lt;/div&gt; &#xA; &lt;table align=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;    &lt;td&gt; &lt;b&gt;Backbones&lt;/b&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;b&gt;Necks&lt;/b&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;b&gt;Loss&lt;/b&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;b&gt;Common&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr valign=&#34;top&#34;&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;YOLOv5CSPDarknet&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv8CSPDarknet&lt;/li&gt; &#xA;      &lt;li&gt;YOLOXCSPDarknet&lt;/li&gt; &#xA;      &lt;li&gt;EfficientRep&lt;/li&gt; &#xA;      &lt;li&gt;CSPNeXt&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv7Backbone&lt;/li&gt; &#xA;      &lt;li&gt;PPYOLOECSPResNet&lt;/li&gt; &#xA;      &lt;li&gt;mmdet backbone&lt;/li&gt; &#xA;      &lt;li&gt;mmcls backbone&lt;/li&gt; &#xA;      &lt;li&gt;timm&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;YOLOv5PAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv8PAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv6RepPAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOXPAFPN&lt;/li&gt; &#xA;      &lt;li&gt;CSPNeXtPAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv7PAFPN&lt;/li&gt; &#xA;      &lt;li&gt;PPYOLOECSPPAFPN&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;IoULoss&lt;/li&gt; &#xA;      &lt;li&gt;mmdet loss&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt;   &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;â“ FAQ &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/notes/faq.md&#34;&gt;FAQ&lt;/a&gt; for frequently asked questions.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ™Œ Contributing &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improving MMYOLO. Ongoing projects can be found in our &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/projects&#34;&gt;GitHub Projects&lt;/a&gt;. Welcome community users to participate in these projects. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ¤ Acknowledgement &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedback. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to re-implement existing methods and develop their own new detectors.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=open-mmlab/mmyolo&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ğŸ–Šï¸ Citation &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{mmyolo2022,&#xA;    title={{MMYOLO: OpenMMLab YOLO} series toolbox and benchmark},&#xA;    author={MMYOLO Contributors},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmyolo}},&#xA;    year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ« License &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/LICENSE&#34;&gt;GPL 3.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ—ï¸ Projects in OpenMMLab &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt;: OpenMMLab foundational library for training deep learning models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmclassification&#34;&gt;MMClassification&lt;/a&gt;: OpenMMLab image classification toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmyolo&#34;&gt;MMYOLO&lt;/a&gt;: OpenMMLab YOLO series toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt;: OpenMMLab image and video editing toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;: OpenMMLab image and video generative models toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmeval&#34;&gt;MMEval&lt;/a&gt;: OpenMMLab machine learning evaluation library.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>innnky/so-vits-svc</title>
    <updated>2023-01-22T02:00:28Z</updated>
    <id>tag:github.com,2023-01-22:/innnky/so-vits-svc</id>
    <link href="https://github.com/innnky/so-vits-svc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;åŸºäºvitsä¸softvcçš„æ­Œå£°éŸ³è‰²è½¬æ¢æ¨¡å‹&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SoftVC VITS Singing Voice Conversion&lt;/h1&gt; &#xA;&lt;h2&gt;English docs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/Eng_docs.md&#34;&gt;è‹±è¯­èµ„æ–™&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;æ®ä¸å®Œå…¨ç»Ÿè®¡ï¼Œå¤šè¯´è¯äººä¼¼ä¹ä¼šå¯¼è‡´&lt;strong&gt;éŸ³è‰²æ³„æ¼åŠ é‡&lt;/strong&gt;ï¼Œä¸å»ºè®®è®­ç»ƒè¶…è¿‡5äººçš„æ¨¡å‹ï¼Œç›®å‰çš„å»ºè®®æ˜¯å¦‚æœæƒ³ç‚¼å‡ºæ¥æ›´åƒç›®æ ‡éŸ³è‰²ï¼Œ&lt;strong&gt;å°½å¯èƒ½ç‚¼å•è¯´è¯äººçš„&lt;/strong&gt;&lt;br&gt; æ–­éŸ³é—®é¢˜å·²è§£å†³ï¼ŒéŸ³è´¨æå‡äº†ä¸å°‘&lt;br&gt; 2.0ç‰ˆæœ¬å·²ç»ç§»è‡³ sovits_2.0åˆ†æ”¯&lt;br&gt; 3.0ç‰ˆæœ¬ä½¿ç”¨FreeVCçš„ä»£ç ç»“æ„ï¼Œä¸æ—§ç‰ˆæœ¬ä¸é€šç”¨&lt;br&gt; ä¸&lt;a href=&#34;https://github.com/prophesier/diff-svc&#34;&gt;DiffSVC&lt;/a&gt; ç›¸æ¯”ï¼Œåœ¨è®­ç»ƒæ•°æ®è´¨é‡éå¸¸é«˜æ—¶diffsvcæœ‰ç€æ›´å¥½çš„è¡¨ç°ï¼Œå¯¹äºè´¨é‡å·®ä¸€äº›çš„æ•°æ®é›†ï¼Œæœ¬ä»“åº“å¯èƒ½ä¼šæœ‰æ›´å¥½çš„è¡¨ç°ï¼Œæ­¤å¤–ï¼Œæœ¬ä»“åº“æ¨ç†é€Ÿåº¦ä¸Šæ¯”diffsvcå¿«å¾ˆå¤š&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;æ¨¡å‹ç®€ä»‹&lt;/h2&gt; &#xA;&lt;p&gt;æ­Œå£°éŸ³è‰²è½¬æ¢æ¨¡å‹ï¼Œé€šè¿‡SoftVCå†…å®¹ç¼–ç å™¨æå–æºéŸ³é¢‘è¯­éŸ³ç‰¹å¾ï¼Œä¸F0åŒæ—¶è¾“å…¥VITSæ›¿æ¢åŸæœ¬çš„æ–‡æœ¬è¾“å…¥è¾¾åˆ°æ­Œå£°è½¬æ¢çš„æ•ˆæœã€‚åŒæ—¶ï¼Œæ›´æ¢å£°ç å™¨ä¸º &lt;a href=&#34;https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan&#34;&gt;NSF HiFiGAN&lt;/a&gt; è§£å†³æ–­éŸ³é—®é¢˜&lt;/p&gt; &#xA;&lt;h2&gt;æ³¨æ„&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å½“å‰åˆ†æ”¯æ˜¯32khzç‰ˆæœ¬çš„åˆ†æ”¯ï¼Œ32khzæ¨¡å‹æ¨ç†æ›´å¿«ï¼Œæ˜¾å­˜å ç”¨å¤§å¹…å‡å°ï¼Œæ•°æ®é›†æ‰€å ç¡¬ç›˜ç©ºé—´ä¹Ÿå¤§å¹…é™ä½ï¼Œæ¨èè®­ç»ƒè¯¥ç‰ˆæœ¬æ¨¡å‹&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœè¦è®­ç»ƒ48khzçš„æ¨¡å‹è¯·åˆ‡æ¢åˆ°&lt;a href=&#34;https://github.com/innnky/so-vits-svc/tree/main&#34;&gt;mainåˆ†æ”¯&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;é¢„å…ˆä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;soft vc hubertï¼š&lt;a href=&#34;https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt&#34;&gt;hubert-soft-0d54a1f4.pt&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ”¾åœ¨hubertç›®å½•ä¸‹&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;é¢„è®­ç»ƒåº•æ¨¡æ–‡ä»¶ &lt;a href=&#34;https://huggingface.co/innnky/sovits_pretrained/resolve/main/G_0.pth&#34;&gt;G_0.pth&lt;/a&gt; ä¸ &lt;a href=&#34;https://huggingface.co/innnky/sovits_pretrained/resolve/main/D_0.pth&#34;&gt;D_0.pth&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ”¾åœ¨logs/32k ç›®å½•ä¸‹&lt;/li&gt; &#xA;   &lt;li&gt;é¢„è®­ç»ƒåº•æ¨¡ä¸ºå¿…é€‰é¡¹ï¼Œå› ä¸ºæ®æµ‹è¯•ä»é›¶å¼€å§‹è®­ç»ƒæœ‰æ¦‚ç‡ä¸æ”¶æ•›ï¼ŒåŒæ—¶åº•æ¨¡ä¹Ÿèƒ½åŠ å¿«è®­ç»ƒé€Ÿåº¦&lt;/li&gt; &#xA;   &lt;li&gt;é¢„è®­ç»ƒåº•æ¨¡è®­ç»ƒæ•°æ®é›†åŒ…å«äº‘ç å³éœœ è¾‰å®‡Â·æ˜ŸAI æ´¾è’™ ç»«åœ°å®å®ï¼Œè¦†ç›–ç”·å¥³ç”Ÿå¸¸è§éŸ³åŸŸï¼Œå¯ä»¥è®¤ä¸ºæ˜¯ç›¸å¯¹é€šç”¨çš„åº•æ¨¡&lt;/li&gt; &#xA;   &lt;li&gt;åº•æ¨¡åˆ é™¤äº†optimizer speaker_embedding ç­‰æ— å…³æƒé‡, åªå¯ä»¥ç”¨äºåˆå§‹åŒ–è®­ç»ƒï¼Œæ— æ³•ç”¨äºæ¨ç†&lt;/li&gt; &#xA;   &lt;li&gt;è¯¥åº•æ¨¡å’Œ48khzåº•æ¨¡é€šç”¨&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ä¸€é”®ä¸‹è½½&#xA;# hubert&#xA;wget -P hubert/ https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt&#xA;# Gä¸Dé¢„è®­ç»ƒæ¨¡å‹&#xA;wget -P logs/32k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/G_0.pth&#xA;wget -P logs/32k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/D_0.pth&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;colabä¸€é”®æ•°æ®é›†åˆ¶ä½œã€è®­ç»ƒè„šæœ¬&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1_-gh9i-wCPNlRZw6pYF-9UufetcVrGBX?usp=sharing&#34;&gt;ä¸€é”®colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;æ•°æ®é›†å‡†å¤‡&lt;/h2&gt; &#xA;&lt;p&gt;ä»…éœ€è¦ä»¥ä»¥ä¸‹æ–‡ä»¶ç»“æ„å°†æ•°æ®é›†æ”¾å…¥dataset_rawç›®å½•å³å¯&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dataset_raw&#xA;â”œâ”€â”€â”€speaker0&#xA;â”‚   â”œâ”€â”€â”€xxx1-xxx1.wav&#xA;â”‚   â”œâ”€â”€â”€...&#xA;â”‚   â””â”€â”€â”€Lxx-0xx8.wav&#xA;â””â”€â”€â”€speaker1&#xA;    â”œâ”€â”€â”€xx2-0xxx2.wav&#xA;    â”œâ”€â”€â”€...&#xA;    â””â”€â”€â”€xxx7-xxx007.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;æ•°æ®é¢„å¤„ç†&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;é‡é‡‡æ ·è‡³ 32khz&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python resample.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;è‡ªåŠ¨åˆ’åˆ†è®­ç»ƒé›† éªŒè¯é›† æµ‹è¯•é›† ä»¥åŠè‡ªåŠ¨ç”Ÿæˆé…ç½®æ–‡ä»¶&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python preprocess_flist_config.py&#xA;# æ³¨æ„&#xA;# è‡ªåŠ¨ç”Ÿæˆçš„é…ç½®æ–‡ä»¶ä¸­ï¼Œè¯´è¯äººæ•°é‡n_speakersä¼šè‡ªåŠ¨æŒ‰ç…§æ•°æ®é›†ä¸­çš„äººæ•°è€Œå®š&#xA;# ä¸ºäº†ç»™ä¹‹åæ·»åŠ è¯´è¯äººç•™ä¸‹ä¸€å®šç©ºé—´ï¼Œn_speakersè‡ªåŠ¨è®¾ç½®ä¸º å½“å‰æ•°æ®é›†äººæ•°ä¹˜2&#xA;# å¦‚æœæƒ³å¤šç•™ä¸€äº›ç©ºä½å¯ä»¥åœ¨æ­¤æ­¥éª¤å è‡ªè¡Œä¿®æ”¹ç”Ÿæˆçš„config.jsonä¸­n_speakersæ•°é‡&#xA;# ä¸€æ—¦æ¨¡å‹å¼€å§‹è®­ç»ƒåæ­¤é¡¹ä¸å¯å†æ›´æ”¹&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;ç”Ÿæˆhubertä¸f0&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python preprocess_hubert_f0.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;æ‰§è¡Œå®Œä»¥ä¸Šæ­¥éª¤å dataset ç›®å½•ä¾¿æ˜¯é¢„å¤„ç†å®Œæˆçš„æ•°æ®ï¼Œå¯ä»¥åˆ é™¤dataset_rawæ–‡ä»¶å¤¹äº†&lt;/p&gt; &#xA;&lt;h2&gt;è®­ç»ƒ&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py -c configs/config.json -m 32k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;æ¨ç†&lt;/h2&gt; &#xA;&lt;p&gt;ä½¿ç”¨ &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/inference_main.py&#34;&gt;inference_main.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ›´æ”¹model_pathä¸ºä½ è‡ªå·±è®­ç»ƒçš„æœ€æ–°æ¨¡å‹è®°å½•ç‚¹&lt;/li&gt; &#xA; &lt;li&gt;å°†å¾…è½¬æ¢çš„éŸ³é¢‘æ”¾åœ¨rawæ–‡ä»¶å¤¹ä¸‹&lt;/li&gt; &#xA; &lt;li&gt;clean_names å†™å¾…è½¬æ¢çš„éŸ³é¢‘åç§°&lt;/li&gt; &#xA; &lt;li&gt;trans å¡«å†™å˜è°ƒåŠéŸ³æ•°é‡&lt;/li&gt; &#xA; &lt;li&gt;spk_list å¡«å†™åˆæˆçš„è¯´è¯äººåç§°&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Onnxå¯¼å‡º&lt;/h2&gt; &#xA;&lt;p&gt;ä½¿ç”¨ &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/onnx_export.py&#34;&gt;onnx_export.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ–°å»ºæ–‡ä»¶å¤¹ï¼šcheckpoints å¹¶æ‰“å¼€&lt;/li&gt; &#xA; &lt;li&gt;åœ¨checkpointsæ–‡ä»¶å¤¹ä¸­æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ä½œä¸ºé¡¹ç›®æ–‡ä»¶å¤¹ï¼Œæ–‡ä»¶å¤¹åä¸ºä½ çš„é¡¹ç›®åç§°&lt;/li&gt; &#xA; &lt;li&gt;å°†ä½ çš„æ¨¡å‹æ›´åä¸ºmodel.pthï¼Œé…ç½®æ–‡ä»¶æ›´åä¸ºconfig.jsonï¼Œå¹¶æ”¾ç½®åˆ°åˆšæ‰åˆ›å»ºçš„æ–‡ä»¶å¤¹ä¸‹&lt;/li&gt; &#xA; &lt;li&gt;å°† &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/onnx_export.py&#34;&gt;onnx_export.py&lt;/a&gt; ä¸­path = &#34;NyaruTaffy&#34; çš„ &#34;NyaruTaffy&#34; ä¿®æ”¹ä¸ºä½ çš„é¡¹ç›®åç§°&lt;/li&gt; &#xA; &lt;li&gt;è¿è¡Œ &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/onnx_export.py&#34;&gt;onnx_export.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ç­‰å¾…æ‰§è¡Œå®Œæ¯•ï¼Œåœ¨ä½ çš„é¡¹ç›®æ–‡ä»¶å¤¹ä¸‹ä¼šç”Ÿæˆä¸€ä¸ªmodel.onnxï¼Œå³ä¸ºå¯¼å‡ºçš„æ¨¡å‹ &lt;h3&gt;Onnxæ¨¡å‹æ”¯æŒçš„UI&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NaruseMioShirakana/MoeSS&#34;&gt;MoeSS&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Gradioï¼ˆWebUIï¼‰&lt;/h2&gt; &#xA;&lt;p&gt;ä½¿ç”¨ &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/sovits_gradio.py&#34;&gt;sovits_gradio.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ–°å»ºæ–‡ä»¶å¤¹ï¼šcheckpoints å¹¶æ‰“å¼€&lt;/li&gt; &#xA; &lt;li&gt;åœ¨checkpointsæ–‡ä»¶å¤¹ä¸­æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ä½œä¸ºé¡¹ç›®æ–‡ä»¶å¤¹ï¼Œæ–‡ä»¶å¤¹åä¸ºä½ çš„é¡¹ç›®åç§°&lt;/li&gt; &#xA; &lt;li&gt;å°†ä½ çš„æ¨¡å‹æ›´åä¸ºmodel.pthï¼Œé…ç½®æ–‡ä»¶æ›´åä¸ºconfig.jsonï¼Œå¹¶æ”¾ç½®åˆ°åˆšæ‰åˆ›å»ºçš„æ–‡ä»¶å¤¹ä¸‹&lt;/li&gt; &#xA; &lt;li&gt;è¿è¡Œ &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/sovits_gradio.py&#34;&gt;sovits_gradio.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>