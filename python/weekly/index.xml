<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-18T01:50:20Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hiroi-sora/Umi-OCR</title>
    <updated>2022-09-18T01:50:20Z</updated>
    <id>tag:github.com,2022-09-18:/hiroi-sora/Umi-OCR</id>
    <link href="https://github.com/hiroi-sora/Umi-OCR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OCR批量图片转文字识别软件，带界面，离线运行。可排除图片中水印区域的干扰，提取干净的文本。基于 PaddleOCR 。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Umi-OCR 批量图片转文字工具&lt;/h1&gt; &#xA;&lt;p&gt;适用于 Win10 x64 平台的离线OCR软件。批量导入本地图片 / 读取剪贴板，识别图片中的文本，输出到软件面板或本地 .txt / .md 文件。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;免费&lt;/strong&gt;：本项目所有代码开源，完全免费。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;方便&lt;/strong&gt;：解压即用，无需安装。不需要网络。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;高效&lt;/strong&gt;：OCR识别引擎是C++编译的 &lt;a href=&#34;https://github.com/hiroi-sora/PaddleOCR-json&#34;&gt;PaddleOCR-json&lt;/a&gt; （PP-OCRv2.6 cpu_avx_mkl），比前代提速20%。只要电脑性能足够且支持mkldnn，通常能比在线OCR服务更快。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;精准&lt;/strong&gt;：默认使用PPOCR-v3模型库。除了能准确辨认常规文字，对非常规字形（手写、艺术字、小字、方向不正、杂乱背景等）也有不错的识别率。可设置&lt;strong&gt;忽略区域&lt;/strong&gt;排除水印，进一步提高精准性。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://tupian.li/images/2022/05/31/1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://tupian.li/images/2022/03/30/win-2-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;兼容性&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;系统支持 Win10 x64 。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;不建议使用 Win7 ，识别引擎很可能无法运行。如果想尝试，win7 x64 sp1 打满系统升级补丁+安装vc运行库后有&lt;strong&gt;小概率&lt;/strong&gt;能跑起来。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CPU必须具有AVX指令集。常见的家用CPU一般都满足该条件。&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;AVX&lt;/th&gt; &#xA;     &lt;th&gt;支持的产品系列&lt;/th&gt; &#xA;     &lt;th&gt;不支持&lt;/th&gt; &#xA;     &lt;th&gt;存疑&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;Intel&lt;/td&gt; &#xA;     &lt;td&gt;酷睿Core，至强Xeon&lt;/td&gt; &#xA;     &lt;td&gt;凌动Atom，安腾Itanium&lt;/td&gt; &#xA;     &lt;td&gt;赛扬Celeron，奔腾Pentium&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;AMD&lt;/td&gt; &#xA;     &lt;td&gt;推土机架构及之后的产品，如锐龙Ryzen、速龙Athlon、FX 等&lt;/td&gt; &#xA;     &lt;td&gt;K10架构及之前的产品&lt;/td&gt; &#xA;     &lt;td&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;下载&lt;/h2&gt; &#xA;&lt;p&gt;注意，Umi-OCR 软件本体只含简体中文&amp;amp;英文识别库。下面链接中的 &lt;strong&gt;多国语言识别扩展包&lt;/strong&gt; 可导入 &lt;code&gt;繁中,日,韩,德,法&lt;/code&gt; 语言，请按需下载。&lt;/p&gt; &#xA;&lt;p&gt;Github下载：&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiroi-sora/Umi-OCR/releases/tag/v1.2.6&#34;&gt;Umi-OCR 批量图片转文字 v1.2.6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;蓝奏云下载：（请留意发布日期和版本号）&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wwn.lanzoul.com/b036wwa4d&#34;&gt;https://wwn.lanzoul.com/b036wwa4d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;密码:&lt;code&gt;1111&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Umi-系列图片处理软件&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Umi-OCR 批量图片转文字软件 ◁&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiroi-sora/Umi-CUT&#34;&gt;Umi-CUT 批量图片去黑边/裁剪/压缩软件&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;前言&lt;/h2&gt; &#xA;&lt;p&gt;关于&lt;strong&gt;忽略指定区域&lt;/strong&gt;的特殊功能：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;类似含水印的视频截图、含有UI/按钮的游戏截图等，往往只需要提取字幕区域的文本，而避免提取到水印和UI文本。本软件可设置忽略某些区域内的文字，来实现这一目的。&lt;/p&gt; &#xA; &lt;p&gt;当有大量的影视和游戏截图需要整理归档，或者想翻找包含某一段台词/字幕的截图；将这些图片提取出文字、然后Ctrl+F是一个很有效的方法。这是开发本软件的初衷。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;关于OCR引擎 &lt;a href=&#34;https://github.com/hiroi-sora/PaddleOCR-json&#34;&gt;PaddleOCR-json&lt;/a&gt; ：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;对 &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.6&#34;&gt;PaddleOCR 2.6 cpu_avx_mkl C++&lt;/a&gt; 的封装。效率高于Python版本PPOCR及部分Python编写的OCR引擎，通常比在线OCR服务更快(省去网络传输的时间)。支持更换Paddle官方模型（兼容v2和v3版本）或自己训练的模型，支持修改PPOCR各项参数。通过添加不同的语言模型，软件可识别多国语言。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;简单上手&lt;/h2&gt; &#xA;&lt;h4&gt;准备&lt;/h4&gt; &#xA;&lt;p&gt;下载压缩包并解压全部文件即可。&lt;/p&gt; &#xA;&lt;h4&gt;批量识别本地图片文件&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://tupian.li/images/2022/03/29/rm-2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;打开主程序，将任意 &lt;strong&gt;图片/文件夹&lt;/strong&gt; 拖入窗口中的白色背景表格区域，或点击左上方的 &lt;strong&gt;浏览&lt;/strong&gt; 选择图片。&lt;/li&gt; &#xA; &lt;li&gt;点击右上方 &lt;strong&gt;开始任务&lt;/strong&gt; ，等待进度条走完。&lt;/li&gt; &#xA; &lt;li&gt;点击 &lt;strong&gt;识别内容&lt;/strong&gt; 选项卡查看输出文字，或者前往 &lt;strong&gt;第一张图片的目录&lt;/strong&gt; 查看识别结果txt文件。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;快速识别剪贴板截图&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;按 [Win+Shift+S] 截取一张系统截图，或者在网页等地方复制一张图片。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;切换到 &lt;strong&gt;识别内容&lt;/strong&gt; 选项卡，点击 &lt;strong&gt;剪贴板读取&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;可以在 &lt;strong&gt;设置&lt;/strong&gt; 选项卡中录制并启用 &lt;strong&gt;全局快捷键&lt;/strong&gt;，快速唤起程序识别。若此时程序窗口处在被覆盖的后方或者被最小化，则会自动挪到最前的位置。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;设置说明&lt;/h2&gt; &#xA;&lt;p&gt;点击 &lt;strong&gt;设置&lt;/strong&gt; 选项卡，配置参数。大部分设置项修改后会自动保存。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://tupian.li/images/2022/07/22/setting-1.2.5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;计划任务：&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;识图任务完成后，额外执行的任务。可执行打开生成文件/目录，自动关机/待机等。&lt;/li&gt; &#xA; &lt;li&gt;即使识图任务正在进行中，也可以随意修改这些选项。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;自定义计划任务&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;您可创建自己的计划任务，本质是调用一段cmd命令。&lt;/li&gt; &#xA;  &lt;li&gt;点击 &lt;strong&gt;打开设置文件&lt;/strong&gt; ，在&lt;code&gt;okMission&lt;/code&gt;中添加一项元素。&lt;/li&gt; &#xA;  &lt;li&gt;键为任务名称，值为字典，其中&lt;code&gt;code&lt;/code&gt;为cmd命令。多条命令可用&lt;code&gt;&amp;amp;&lt;/code&gt;分隔。例：&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;&#34;我的任务&#34;: {&#34;code&#34;: &#34;cmd命令1 &amp;amp; 命令2&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;忽略图片中某些区域内的文字：&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;点击 &lt;strong&gt;添加区域&lt;/strong&gt; 展开配置忽略区的新窗口。具体配置方式&lt;a href=&#34;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E5%BF%BD%E7%95%A5%E5%8C%BA%E5%9F%9F%E5%8A%9F%E8%83%BD&#34;&gt;见后&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;点击 &lt;strong&gt;清空区域&lt;/strong&gt; 清空已配置的所有忽略区域参数。&lt;/li&gt; &#xA; &lt;li&gt;已添加区域后，上方标题文字提示当前忽略区域的 &lt;strong&gt;生效分辨率&lt;/strong&gt; 。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;从剪贴板读取图片：&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;点击 &lt;strong&gt;录制按键&lt;/strong&gt; 后按下想要的快捷键，如 &lt;code&gt;ctrl+shift+s&lt;/code&gt; 。然后勾选 &lt;strong&gt;启用全局快捷键&lt;/strong&gt; 。&lt;/li&gt; &#xA; &lt;li&gt;按下快捷键后，程序检查当前剪贴板的第一位是否为图片，是则程序跳到顶层并展示识别文字。&lt;/li&gt; &#xA; &lt;li&gt;请检查并避免全局快捷键与其它程序冲突。&lt;/li&gt; &#xA; &lt;li&gt;可设置识图后 &lt;strong&gt;自动复制识别内容&lt;/strong&gt;（不含任务时间等信息的纯内容文本）。此设置只对剪贴板识图生效，批量任务时无效。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：&lt;code&gt;v1.2.6&lt;/code&gt;版本提高了批量处理的平均速度，但代价是需要花费更长时间进行初始化。这可能影响剪贴板识图的速度（每次剪贴板任务都要重新初始化）。若频繁使用此功能，建议先使用旧版 &lt;code&gt;v1.2.5&lt;/code&gt; 。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;输入设置：&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;递归读取子文件夹中所有图片&lt;/strong&gt; 若勾选，拖入文件夹到处理列表时，会导入所有子文件夹中的图片。否则只会导入一层文件夹下的图片。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;图片后缀正常情况无需改动。&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;图片后缀&lt;/strong&gt; 配置软件允许载入的图片后缀，不同后缀以空格分隔，必须全为小写。 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;如果你有必要添加新的图片后缀，要保证该图片同时满足c++模块的&lt;code&gt;PaddleOCR&lt;/code&gt;和python的&lt;code&gt;PIL&lt;/code&gt;均可识别。比如 &lt;strong&gt;.gif&lt;/strong&gt; 图片，虽然&lt;code&gt;PIL&lt;/code&gt;可以识别，但&lt;code&gt;PaddleOCR&lt;/code&gt;无法识别，载入gif文件会导致软件任务失败，因此不允许载入 &lt;strong&gt;.gif&lt;/strong&gt; 。&lt;/li&gt; &#xA;    &lt;li&gt;不在许可后缀范围内的文件，拖入软件也不会被载入。目前默认的图片后缀为：&lt;code&gt;.jpg .jpe .jpeg .jfif .png .webp .bmp .tif .tiff&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;输出设置：&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;将 &lt;strong&gt;识别内容写入本地文件&lt;/strong&gt; 取消勾选后，不会再生成本地文件，只能在 &lt;strong&gt;识别内容&lt;/strong&gt; 选项卡中查看输出信息。若设置了本次任务完成后自动关机，请务必勾选此项，以免至今为止的努力全部木大。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;其他项正常情况无需改动。&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;输出调试信息&lt;/strong&gt; 若勾选，则会额外输出程序工作状态的内容。&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;忽略无文字的图片&lt;/strong&gt; 若勾选，则不含文字（或文字全被忽略区域屏蔽掉）的图片名称不会出现在输出信息中。 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;若想生成一份用于浏览的markdown文件，则&lt;strong&gt;建议取消勾选&lt;/strong&gt;。&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;生成文件可选择两种风格：&lt;strong&gt;纯文本.txt文件&lt;/strong&gt; 和 &lt;strong&gt;Markdown风格.md文件&lt;/strong&gt; 。前者可用于查找等一般用途。后者在编辑器或浏览器中渲染为图文并茂的页面，可用于浏览和欣赏图集。&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;输出目录&lt;/strong&gt; 和 &lt;strong&gt;输出文件名&lt;/strong&gt; 设置生成的文件的位置和名称。 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;当拖入第一张图片且这两项设置为空时，自动设置输出路径为第一张图片的父目录，输出文件名为 &lt;code&gt;[转文字]_{父目录}.txt&lt;/code&gt;。除非要自定目录和名称，否则&lt;strong&gt;这两项默认留空即可&lt;/strong&gt;。&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;处理列表标签页的 &lt;strong&gt;清空表格&lt;/strong&gt; 按钮，除了会清空已导入的图片列表，还会清空 &lt;strong&gt;输出目录&lt;/strong&gt; 和 &lt;strong&gt;输出文件名&lt;/strong&gt; 设置。这样下次拖入新图片时，就能在新的位置存放输出文件。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;识别器设置：&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;识别语言&lt;/strong&gt; 选择当前识别语言（即OCR参数文件）。英文无需切换，所有语言均支持英文字母识别。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;启动参数&lt;/strong&gt; 可输入字符串配置参数，调整识别过程，适应任务需求。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;如何添加多国语言？&lt;/summary&gt; &#xA; &lt;h5&gt;方法一：下载 [Umi-OCR 多国语言识别扩展包] ，拷贝到软件目录即可。&lt;/h5&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E4%B8%8B%E8%BD%BD&#34;&gt;点此跳转下载位置&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;扩展包内置语言：&lt;code&gt;繁中,日,韩,德,法&lt;/code&gt;&lt;/p&gt; &#xA; &lt;h5&gt;方法二：手动下载添加 PP-OCR 模型库&lt;/h5&gt; &#xA; &lt;ol start=&#34;0&#34;&gt; &#xA;  &lt;li&gt;模型分为三种：det检测，cls方向分类，rec识别。其中det和cls是多语言通用的，只需下载新语言的rec识别模型即可。&lt;/li&gt; &#xA;  &lt;li&gt;前往 PP-OCR系列 &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.6/doc/doc_ch/models_list.md#23-%E5%A4%9A%E8%AF%AD%E8%A8%80%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E6%9B%B4%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD&#34;&gt;V3多语言识别模型列表&lt;/a&gt; ，下载一组&lt;strong&gt;rec识别&lt;/strong&gt;模型。 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;若V3模型列表里没有找到目标语言，可以去&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.6/doc/doc_ch/multi_languages.md#5-%E6%94%AF%E6%8C%81%E8%AF%AD%E7%A7%8D%E5%8F%8A%E7%BC%A9%E5%86%99&#34;&gt;支持语言列表&lt;/a&gt;查看PPOCR有没有提供这种语言。若有，则可能它暂未推出V3模型，可以先使用旧版V2模型。（V3模型网址中的2.x一路换成更小的数字可以查看旧版页面）&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;前往 PP-OCR系列 &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.6/ppocr/utils/dict&#34;&gt;字典列表&lt;/a&gt; ，下载对应语言的字典文件。&lt;/li&gt; &#xA;  &lt;li&gt;将下载好的文件解压放进软件目录的 &lt;code&gt;PaddleOCR-json&lt;/code&gt; 文件夹中。&lt;/li&gt; &#xA;  &lt;li&gt;复制一份 &lt;code&gt;PaddleOCR_json_config_[模板].txt&lt;/code&gt; ，改一下名。&lt;/li&gt; &#xA;  &lt;li&gt;打开复制好的 &lt;code&gt;PaddleOCR_json_config_目标语言.txt&lt;/code&gt; ，将 rec路径 &lt;code&gt;rec_model_dir&lt;/code&gt; 和 字典路径 &lt;code&gt;rec_char_dict_path&lt;/code&gt; 改成目标语言的文件(夹)的名称。若模型库是v2版本，还必须加上一行 &lt;code&gt;rec_img_h 32&lt;/code&gt; 。&lt;/li&gt; &#xA;  &lt;li&gt;回到上一层目录 &lt;code&gt;Umi-OCR&lt;/code&gt; ，打开 &lt;code&gt;Umi-OCR_config.json&lt;/code&gt; ，在 &lt;code&gt;&#34;ocrConfig&#34;&lt;/code&gt; 中添加新语言的信息。键为语言名称，值的 &lt;code&gt;path&lt;/code&gt; 为config txt文件的名称。保持json格式，注意逗号。&lt;/li&gt; &#xA;  &lt;li&gt;打开软件，检查设置页是否已经能选择该识别语言。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;识别不准怎么办？ 高端CPU如何进一步提高速度？&lt;/summary&gt; &#xA; &lt;h5&gt;启动参数说明&lt;/h5&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;若图片中的文字方向不是正朝上，启动参数添加：&lt;code&gt;--cls=1 --use_angle_cls=1&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;为了提高速度，PaddleOCR预先将长度超标的图片进行压缩，再执行文字识别。这可能导致超大分辨率（4k以上）图片的识别准确度下降。若有识别不出文字或漏掉小字的现象发生，启动参数添加：&lt;code&gt;--limit_side_len=压缩阈值&lt;/code&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;压缩阈值建议填图片的最大边长的一半，4000x3000的图片填2000。值越大，识别小字的准确率可能提高，但速度会大幅降低，请谨慎使用。&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;若电脑的CPU大于8核16线程，启动参数添加：&lt;code&gt;--cpu_threads=线程&lt;/code&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;线程建议填CPU线程数，如16。值越大，识别速度可能提高。不应超过CPU线程数。&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;若软件初始化识别引擎时频繁报错或崩溃，且电脑CPU是早期AMD型号，启动参数尝试添加：&lt;code&gt;--enable_mkldnn=0&lt;/code&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;禁用mkldnn加速可能提高对早期AMD的兼容性，但识别速度会大幅降低。Intel和AMD锐龙系列一般不存在这个问题。&lt;/li&gt; &#xA;    &lt;li&gt;若确实无法使用mkldnn，建议使用&lt;code&gt;1.2.5&lt;/code&gt;旧版本Umi-OCR，它在禁用加速时的效率高于&lt;code&gt;1.2.6&lt;/code&gt;。&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;启动参数可以叠加，用空格隔开，如： &lt;code&gt;--cls=1 --use_angle_cls=1 --use_angle_cls=1 --limit_side_len=1920 --cpu_threads=20&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;更多启动参数详见 &lt;a href=&#34;https://github.com/hiroi-sora/PaddleOCR-json#5-%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E8%AF%B4%E6%98%8E&#34;&gt;PaddleOCR-json 配置信息说明&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;忽略区域功能&lt;/h2&gt; &#xA;&lt;p&gt;忽略区域是本软件特色功能，可用于排除图片中水印的干扰，让识别结果只留下所需的文本。&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;展开详情&lt;/summary&gt; &#xA; &lt;p&gt;“忽略区域”是指图片上指定位置与大小的矩形区域，完全处于这些区域内的文字块，将被排除。&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;点击 &lt;strong&gt;设置&lt;/strong&gt; 选项卡中的 &lt;strong&gt;添加忽略区域&lt;/strong&gt; ，进入忽略区域选择窗口。&lt;/li&gt; &#xA;  &lt;li&gt;将任意图片 &lt;strong&gt;拖入&lt;/strong&gt; 该窗口，可预览该图片。将新图片拖入窗口可切换预览，但已绘制的忽略区域不会消失；可切换不同图片来仔细调整忽略区域。&lt;/li&gt; &#xA;  &lt;li&gt;绘制 &lt;strong&gt;忽略区域&lt;/strong&gt; ：拖入图片后，点击选中左起第一按钮 &lt;strong&gt;+忽略区域 A&lt;/strong&gt; ，然后在图片上按住左键拖拽，绘制矩形区域。可 &lt;strong&gt;撤销&lt;/strong&gt; 步骤。&lt;/li&gt; &#xA;  &lt;li&gt;绘制完后，点击 &lt;strong&gt;完成&lt;/strong&gt; 返回软件主窗口。若不想应用此次绘制，则右上角X，取消。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;简单案例见下。&lt;/p&gt; &#xA; &lt;h4&gt;简单排除视频截图中的水印：&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;打开忽略区域设置窗口，拖入任一张截图。 稍等约1秒，面板上会显示出图片，识别到的文字区域会被虚线框起来。发现右上角的水印也被识别到了。 &lt;img src=&#34;https://tupian.li/images/2022/03/30/image04bea2a47232088b.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;  &lt;li&gt;点击选择 &lt;strong&gt;+忽略区域 A&lt;/strong&gt; 。在画面上按住左键拖拽，绘制方框完全包裹住水印区域，范围可以大一些。可绘制多个方框。 &lt;img src=&#34;https://tupian.li/images/2022/03/30/image4a49b65bbd22c4a6.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;  &lt;li&gt;点击 &lt;strong&gt;完成&lt;/strong&gt; 。返回主窗口， &lt;strong&gt;开始任务&lt;/strong&gt; 。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h4&gt;排除游戏截图中的两种UI：&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;假设有一组游戏截图，主要分为两类图片，这两类图片的文字位置和UI位置不太相同： &lt;img src=&#34;https://tupian.li/images/2022/03/30/image1.png&#34; alt=&#34;&#34;&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;甲类（上图左）为对话模式，字数少，要保留的台词文本在画面下方，要排除的UI分布于底端。&lt;/li&gt; &#xA;    &lt;li&gt;乙类（上图右）为历史文本模式，字数多，从上到下都有要保留的文本（与甲类UI位置有重合），要排除的UI分布在两侧。&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;拖入一张甲类图片。选择 &lt;strong&gt;+忽略区域 A&lt;/strong&gt; ，绘制方框包裹住要排除的 &lt;strong&gt;底端UI&lt;/strong&gt; 。可绘制多个方框。 &lt;img src=&#34;https://tupian.li/images/2022/03/30/image2ad97ff898e39d82f.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;  &lt;li&gt;拖入一张乙类图片。选择 &lt;strong&gt;+识别区域&lt;/strong&gt; ，绘制方框包裹住 &lt;strong&gt;小部分要保留的文本&lt;/strong&gt; 。注意只要该区域内含有任意保留文本即可，不需要画得很大，不需要包裹住所有保留文本；不能与甲类图中 &lt;strong&gt;可能存在的任何文本&lt;/strong&gt; 重合。 &lt;img src=&#34;https://tupian.li/images/2022/03/30/image3.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;  &lt;li&gt;然后选择 &lt;strong&gt;+忽略区域 B&lt;/strong&gt; ，绘制方框包裹住乙类图要排除的 &lt;strong&gt;两侧UI&lt;/strong&gt; 。可绘制多个方框。 &lt;img src=&#34;https://tupian.li/images/2022/03/30/image4.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;  &lt;li&gt;点击 &lt;strong&gt;完成&lt;/strong&gt; 。返回主窗口， &lt;strong&gt;开始任务&lt;/strong&gt; 。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h4&gt;忽略区域处理逻辑：&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;忽略区域A&lt;/strong&gt; ：正常情况下，处于 &lt;strong&gt;忽略区域A&lt;/strong&gt; 内的文字 &lt;strong&gt;不会&lt;/strong&gt; 输出。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;识别区域&lt;/strong&gt; ：当识别区域内存在文本时，&lt;strong&gt;忽略区域A失效&lt;/strong&gt; ；即处于忽略区域A内的文字也 &lt;strong&gt;会&lt;/strong&gt; 被输出。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;忽略区域B&lt;/strong&gt; ：当 &lt;strong&gt;忽略区域A失效&lt;/strong&gt; 时，忽略区域B才生效；即处于区域1内的文字 &lt;strong&gt;会&lt;/strong&gt; 输出、区域2内的文字 &lt;strong&gt;不会&lt;/strong&gt; 输出。&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;识别区域&lt;/th&gt; &#xA;      &lt;th&gt;忽略区域A&lt;/th&gt; &#xA;      &lt;th&gt;忽略区域B&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;× 不存在文字&lt;/td&gt; &#xA;      &lt;td&gt;√ 生效&lt;/td&gt; &#xA;      &lt;td&gt;× 失效&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;√ 存在文字&lt;/td&gt; &#xA;      &lt;td&gt;× 失效&lt;/td&gt; &#xA;      &lt;td&gt;√ 生效&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;“忽略区域配置”只针对一种分辨率生效。假如配置的分辨率是1920x1080，那么批量识别图片时，只有符合1920x1080的图片才会排除干扰文本；1920x1079的图片中的文字会全部输出。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;拖入预览的图片必须分辨率相同。假如先拖入1920x1080的图片，再拖入其它分辨率的图片；软件会弹窗警告。只有点击 &lt;strong&gt;清空&lt;/strong&gt; 删除当前已配置的忽略区域，才能拖入其他分辨率图片，并应用此分辨率。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;效率测试 &amp;amp; 开发说明&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;展开详情&lt;/summary&gt; &#xA; &lt;h3&gt;效率测试&lt;/h3&gt; &#xA; &lt;p&gt;测试机器：&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;CPU&lt;/th&gt; &#xA;    &lt;th&gt;TDP&lt;/th&gt; &#xA;    &lt;th&gt;RAM&lt;/th&gt; &#xA;    &lt;th&gt;是否兼容mkldnn&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;r5 4600u&lt;/td&gt; &#xA;    &lt;td&gt;15w&lt;/td&gt; &#xA;    &lt;td&gt;16g&lt;/td&gt; &#xA;    &lt;td&gt;无报错&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;测试集：&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;图片张数&lt;/th&gt; &#xA;    &lt;th&gt;测试条件&lt;/th&gt; &#xA;    &lt;th&gt;分辨率&lt;/th&gt; &#xA;    &lt;th&gt;平均字块数量&lt;/th&gt; &#xA;    &lt;th&gt;平均字符数量&lt;/th&gt; &#xA;    &lt;th&gt;文字语言&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;100&lt;/td&gt; &#xA;    &lt;td&gt;环境相同，多次测量取平均值&lt;/td&gt; &#xA;    &lt;td&gt;1920x1080&lt;/td&gt; &#xA;    &lt;td&gt;15&lt;/td&gt; &#xA;    &lt;td&gt;250&lt;/td&gt; &#xA;    &lt;td&gt;简体中文&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;测试结果：&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Umi-OCR版本&lt;/th&gt; &#xA;    &lt;th&gt;1.2.5&lt;/th&gt; &#xA;    &lt;th&gt;1.2.5&lt;/th&gt; &#xA;    &lt;th&gt;1.2.6&lt;/th&gt; &#xA;    &lt;th&gt;1.2.6&lt;/th&gt; &#xA;    &lt;th&gt;1.2.6&lt;/th&gt; &#xA;    &lt;th&gt;1.2.6&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PaddleOCR-json版本&lt;/td&gt; &#xA;    &lt;td&gt;1.1.1&lt;/td&gt; &#xA;    &lt;td&gt;1.1.1&lt;/td&gt; &#xA;    &lt;td&gt;1.2.0&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;1.2.0&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;1.2.0&lt;/td&gt; &#xA;    &lt;td&gt;1.2.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PP-OCR C++版本&lt;/td&gt; &#xA;    &lt;td&gt;2.1&lt;/td&gt; &#xA;    &lt;td&gt;2.1&lt;/td&gt; &#xA;    &lt;td&gt;2.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;2.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;2.6&lt;/td&gt; &#xA;    &lt;td&gt;2.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;是否开启mkldnn&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PP-OCR模型库版本&lt;/td&gt; &#xA;    &lt;td&gt;v2&lt;/td&gt; &#xA;    &lt;td&gt;v2&lt;/td&gt; &#xA;    &lt;td&gt;v2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;v3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;v3 slim&lt;/td&gt; &#xA;    &lt;td&gt;v3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;总耗时（秒）&lt;/td&gt; &#xA;    &lt;td&gt;90&lt;/td&gt; &#xA;    &lt;td&gt;120&lt;/td&gt; &#xA;    &lt;td&gt;65&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;63&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;170&lt;/td&gt; &#xA;    &lt;td&gt;400&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;平均单张耗时（秒）&lt;/td&gt; &#xA;    &lt;td&gt;0.9&lt;/td&gt; &#xA;    &lt;td&gt;1.2&lt;/td&gt; &#xA;    &lt;td&gt;0.65&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;0.63&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;1.7&lt;/td&gt; &#xA;    &lt;td&gt;4.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;内存占用峰值（MB）&lt;/td&gt; &#xA;    &lt;td&gt;1000&lt;/td&gt; &#xA;    &lt;td&gt;350&lt;/td&gt; &#xA;    &lt;td&gt;1200&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;1700&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;5800&lt;/td&gt; &#xA;    &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;结论：&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;在启用mkldnn情况下，新版本比前代的效率具有显著优势。新版调教倾向于榨干硬件的性能，内存占用高于旧版。&lt;/li&gt; &#xA;  &lt;li&gt;不启用mkldnn时，新版本效率不如前代。故您的CPU若不支持mkldnn（如早期AMD型号），建议使用旧版本Umi-OCR。&lt;/li&gt; &#xA;  &lt;li&gt;虽然Paddle官方文档中说经过压缩剪枝蒸馏量化的slim版模型的性能指标会超过传统算法，但实测 v3 slim 模型的性能远不如原始版本，还可能伴随着内存泄漏的问题。也许是 PP-OCR C++ 引擎不适配。在该问题解决之前，Umi-OCR发行版提供原始版本模型。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;开发说明&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;如果想用接口调用OCR，可试试 &lt;a href=&#34;https://github.com/hiroi-sora/PaddleOCR-json#paddleocr-json-%E5%9B%BE%E7%89%87%E8%BD%AC%E6%96%87%E5%AD%97%E7%A8%8B%E5%BA%8F&#34;&gt;PaddleOCR-json 图片转文字程序&lt;/a&gt; 。&lt;/li&gt; &#xA;  &lt;li&gt;由于PaddleOCR-json只接受硬盘文件，所以读取剪贴板图片时，会先将内存中的图片保存到同目录下的&lt;code&gt;Umi-OCR_temp&lt;/code&gt;。每次任务时清空前一次的缓存。&lt;/li&gt; &#xA;  &lt;li&gt;PPOCR v2.6 (PaddleOCR-json v1.2.0) 版本提高了批量处理的平均速度，但代价是需要花费更长时间进行初始化。提高了启用mkldnn加速时的识别速度，但代价时不开启加速时效率更低。（CPU只要不是特别早期的AMD，一般都能使用mkldnn，但加速幅度可能不如同档次的Intel。）&lt;/li&gt; &#xA;  &lt;li&gt;未来将增加 openblas 版识别引擎，进一步优化AMD的效率。&lt;/li&gt; &#xA;  &lt;li&gt;代码很丑，有空重构 &lt;del&gt;下次一定&lt;/del&gt;&lt;/li&gt; &#xA;  &lt;li&gt;使用&lt;code&gt;pyinstaller&lt;/code&gt;打包，参数为 &lt;code&gt;pyinstaller -F -w -i icon/icon.ico -n &#34;Umi-OCR 批量图片转文字&#34; main.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;h4&gt;已发布&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;展开详情&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;输出内容可选为markdown风格并嵌入图片路径。&lt;/li&gt; &#xA;  &lt;li&gt;设置项能保存。&lt;/li&gt; &#xA;  &lt;li&gt;自动打开输出文件or文件夹。&lt;/li&gt; &#xA;  &lt;li&gt;识别剪贴板中的图片。&lt;/li&gt; &#xA;  &lt;li&gt;任务进行时，禁用部分设置项。&lt;/li&gt; &#xA;  &lt;li&gt;计划任务：完成后自动关机/休眠等。&lt;/li&gt; &#xA;  &lt;li&gt;递归导入文件夹。&lt;/li&gt; &#xA;  &lt;li&gt;优化适配PaddleOCR v3模型。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;正在开发&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wwn.lanzoul.com/b037embad&#34;&gt;新功能测试版下载地址（蓝奏云，密码1111）&lt;/a&gt;（可能含较多Bug）&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 增加OCR引擎进程常驻后台的模式，大幅缩短剪贴板识图等零碎任务的启动时间。&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 监控OCR引擎进程内存占用，并可随时强制停止该进程。&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 自动检测CPU指令集是否兼容。&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 自动检测Windows语言是否兼容。&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 内置截图。&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 可最小化至系统托盘。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;画饼（有生之年）&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;展开详情&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;对图片重命名。&lt;/li&gt; &#xA;  &lt;li&gt;提高初始化速度。&lt;/li&gt; &#xA;  &lt;li&gt;忽略区域能保存预设。&lt;/li&gt; &#xA;  &lt;li&gt;缩减离线OCR模块的体积。&lt;/li&gt; &#xA;  &lt;li&gt;排版优化：模糊匹配同行文段，自定义文段方向。&lt;/li&gt; &#xA;  &lt;li&gt;解决引擎Opencv对不同地区语言Windows的兼容性。&lt;/li&gt; &#xA;  &lt;li&gt;离线OCR模块增加 &lt;code&gt;no_avx&lt;/code&gt; 和 &lt;code&gt;openblas&lt;/code&gt; 版本。&lt;/li&gt; &#xA;  &lt;li&gt;文本纠错。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;h5&gt;v1.2.6 &lt;code&gt;2022.9.1&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;!-- Umi酱生日快乐~ --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更新PaddleOCR-json模块至&lt;code&gt;v1.2.0&lt;/code&gt;，提高识别速度、准确度。&lt;/li&gt; &#xA; &lt;li&gt;调整UI：更方便地用下拉框切换识别语言。&lt;/li&gt; &#xA; &lt;li&gt;调整UI：可以从主窗口任意位置/任意选项卡拖入图片。&lt;/li&gt; &#xA; &lt;li&gt;修正了漏洞：提高程序健壮性，增加启动子进程时的更多异常处理情况。&lt;/li&gt; &#xA; &lt;li&gt;修正了漏洞：彻底解决了对边缘过窄的图片，识别结果不准确的问题 &lt;a href=&#34;https://github.com/hiroi-sora/Umi-OCR/issues/7&#34;&gt;issue #7&lt;/a&gt; 。&lt;/li&gt; &#xA; &lt;li&gt;优化适配PP-OCRv3模型，彻底解决了v3版模型比v2慢、不准的问题 &lt;a href=&#34;https://github.com/hiroi-sora/Umi-OCR/issues/4#issuecomment-1141735773&#34;&gt;issue #4&lt;/a&gt; 。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;v1.2.5 &lt;code&gt;2022.7.22&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;添加新功能：计划任务。识图完成后执行自动关机等任务。&lt;/li&gt; &#xA; &lt;li&gt;添加新功能：可选拖入文件夹时递归导入子文件夹中所有图片。&lt;/li&gt; &#xA; &lt;li&gt;调整UI：添加一些配置文件的快捷入口。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;v1.2.4 &lt;code&gt;2022.6.4&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;添加新功能：可选识别剪贴板图片后自动复制识别的文本。&lt;/li&gt; &#xA; &lt;li&gt;补充功能：快捷键调用剪贴板识图时，若程序窗口被最小化，则恢复前台状态并挪到最前位置。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;v1.2.3 &lt;code&gt;2022.5.31&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;添加新功能：读取剪贴板图片。配置全局快捷键调用该功能。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;v1.2.2 &lt;code&gt;2022.4.30&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;添加新功能：可选任务完成后自动打开输出文件或目录。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;v1.2.1 &lt;code&gt;2022.4.16&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更新PaddleOCR-json模块至&lt;code&gt;v1.1.1&lt;/code&gt;，修正了可能得到错误包围盒的漏洞。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;v1.2.0 &lt;code&gt;2022.4.8&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;可选生成Markdown风格的图文并茂的.md文件，作为索引使用有更佳的观感。当然也可以继续选择生成纯文本.txt文件。&lt;/li&gt; &#xA; &lt;li&gt;修改设置面板的样式，改为滚动面板以容纳更多设置选项。&lt;/li&gt; &#xA; &lt;li&gt;用户修改配置项后可自动保存。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;v1.1.1 &lt;code&gt;2022.3.30&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修正了漏洞：退出 [忽略区域窗口] 时，OCR子进程未关闭。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;v1.1.0 &lt;code&gt;2022.3.30&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;添加新功能：[忽略区域窗口] 以虚线框 展示识别出的文字块。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;v1.0.0 &lt;code&gt;2022.3.28&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;“梦开始的地方”&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>sd-webui/stable-diffusion-webui</title>
    <updated>2022-09-18T01:50:20Z</updated>
    <id>tag:github.com,2022-09-18:/sd-webui/stable-diffusion-webui</id>
    <link href="https://github.com/sd-webui/stable-diffusion-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion web UI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://discord.gg/gyXNe4NySY&#34;&gt;Visit sd-webui&#39;s Discord Server&lt;/a&gt; &lt;a href=&#34;https://discord.gg/gyXNe4NySY&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5977640/190528254-9b5b4423-47ee-4f24-b4f9-fd13fba37518.png&#34; alt=&#34;Discord Server&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;Installation instructions for &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Installation&#34;&gt;Windows&lt;/a&gt;, &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Linux-Automated-Setup-Guide&#34;&gt;Linux&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h3&gt;Have an &lt;strong&gt;issue&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If the issue involves &lt;em&gt;a bug&lt;/em&gt; in &lt;strong&gt;textual-inversion&lt;/strong&gt; create the issue on &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui&#34;&gt;sd-webui/stable-diffusion-webui&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you want to know how to &lt;strong&gt;activate&lt;/strong&gt; or &lt;strong&gt;use&lt;/strong&gt; textual-inversion see &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/hlky/sd-enable-textual-inversion&#34;&gt;hlky/sd-enable-textual-inversion&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;. Activation not working? create the issue on &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui&#34;&gt;sd-webui/stable-diffusion-webui&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Want to contribute?&lt;/h3&gt; &#xA;&lt;p&gt;Gradio version (stable)&lt;/p&gt; &#xA;&lt;p&gt;Open new Pull Requests on &lt;code&gt;dev&lt;/code&gt; branch!&lt;/p&gt; &#xA;&lt;p&gt;for Gradio check out the &lt;a href=&#34;https://gradio.app/docs/&#34;&gt;docs&lt;/a&gt; to contribute&lt;/p&gt; &#xA;&lt;p&gt;Have an issue or feature request with Gradio? open a issue/feature request on github for support: &lt;a href=&#34;https://github.com/gradio-app/gradio/issues&#34;&gt;https://github.com/gradio-app/gradio/issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Need more support with Gradio? We have a discord channel called &lt;code&gt;gradio-stable-diffusion&lt;/code&gt; for Q&amp;amp;A with the gradio authors, to join use this link &lt;a href=&#34;https://discord.gg/Qs8AsnX7Jd&#34;&gt;https://discord.gg/Qs8AsnX7Jd&lt;/a&gt;, then go to &lt;code&gt;role-assignment&lt;/code&gt; and click gradio to join the &lt;code&gt;gradio&lt;/code&gt; channels.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;New features can be added to Gradio or Streamlit versions&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;More documentation about features, troubleshooting, common issues very soon&lt;/h2&gt; &#xA;&lt;h3&gt;Want to help with documentation? Documented something? Use &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Important&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;🔥 NEW! webui.cmd updates with any changes in environment.yaml file so the environment will always be up to date as long as you get the new environment.yaml file 🔥&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🔥&lt;/span&gt; no need to remove environment, delete src folder and create again, MUCH simpler! 🔥&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Questions about &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Upscalers&#34;&gt;Upscalers&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;h3&gt;Questions about &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Optimized-mode&#34;&gt;Optimized mode&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;h3&gt;Questions about &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Command-line-options&#34;&gt;Command line options&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradio GUI: Idiot-proof, fully featured frontend for both txt2img and img2img generation&lt;/li&gt; &#xA; &lt;li&gt;No more manually typing parameters, now all you have to do is write your prompt and adjust sliders&lt;/li&gt; &#xA; &lt;li&gt;GFPGAN Face Correction 🔥: &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui#gfpgan&#34;&gt;Download the model&lt;/a&gt;Automatically correct distorted faces with a built-in GFPGAN option, fixes them in less than half a second&lt;/li&gt; &#xA; &lt;li&gt;RealESRGAN Upscaling 🔥: &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui#realesrgan&#34;&gt;Download the models&lt;/a&gt; Boosts the resolution of images with a built-in RealESRGAN option&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;💻&lt;/span&gt; esrgan/gfpgan on cpu support &lt;span&gt;💻&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Textual inversion 🔥: &lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;info&lt;/a&gt; - requires enabling, see &lt;a href=&#34;https://github.com/hlky/sd-enable-textual-inversion&#34;&gt;here&lt;/a&gt;, script works as usual without it enabled&lt;/li&gt; &#xA; &lt;li&gt;Advanced img2img editor &lt;span&gt;🎨&lt;/span&gt; &lt;span&gt;🔥&lt;/span&gt; &lt;span&gt;🎨&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt; Mask and crop &lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mask painting (NEW) 🖌️: Powerful tool for re-generating only specific parts of an image you want to change&lt;/li&gt; &#xA; &lt;li&gt;More k_diffusion samplers 🔥🔥 : Far greater quality outputs than the default sampler, less distortion and more accurate&lt;/li&gt; &#xA; &lt;li&gt;txt2img samplers: &#34;DDIM&#34;, &#34;PLMS&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39;&lt;/li&gt; &#xA; &lt;li&gt;img2img samplers: &#34;DDIM&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39;&lt;/li&gt; &#xA; &lt;li&gt;Loopback (NEW) ➿: Automatically feed the last generated sample back into img2img&lt;/li&gt; &#xA; &lt;li&gt;Prompt Weighting (NEW) 🏋️: Adjust the strength of different terms in your prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🔥&lt;/span&gt; gpu device selectable with --gpu &#xA;  &lt;id&gt; &#xA;   &lt;span&gt;🔥&lt;/span&gt;&#xA;  &lt;/id&gt;&lt;/li&gt; &#xA; &lt;li&gt;Memory Monitoring 🔥: Shows Vram usage and generation time after outputting.&lt;/li&gt; &#xA; &lt;li&gt;Word Seeds 🔥: Use words instead of seed numbers&lt;/li&gt; &#xA; &lt;li&gt;CFG: Classifier free guidance scale, a feature for fine-tuning your output&lt;/li&gt; &#xA; &lt;li&gt;Launcher Automatic 👑🔥 shortcut to load the model, no more typing in Conda&lt;/li&gt; &#xA; &lt;li&gt;Lighter on Vram: 512x512 img2img &amp;amp; txt2img tested working on 6gb&lt;/li&gt; &#xA; &lt;li&gt;and ????&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Stable Diffusion web UI&lt;/h1&gt; &#xA;&lt;p&gt;A browser interface based on Gradio library for Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;Original script with Gradio UI was written by a kind anonymous user. This is a modification.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/txt2img.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/img2img.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/gfpgan.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/esrgan.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;GFPGAN&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use GFPGAN to improve generated faces, you need to install it separately. Download &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;GFPGANv1.3.pth&lt;/a&gt; and put it into the &lt;code&gt;/stable-diffusion-webui/src/gfpgan/experiments/pretrained_models&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;RealESRGAN&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth&#34;&gt;RealESRGAN_x4plus.pth&lt;/a&gt; and &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth&#34;&gt;RealESRGAN_x4plus_anime_6B.pth&lt;/a&gt;. Put them into the &lt;code&gt;stable-diffusion-webui/src/realesrgan/experiments/pretrained_models&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;LDSR&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;strong&gt;LDSR&lt;/strong&gt; &lt;a href=&#34;https://heibox.uni-heidelberg.de/f/31a76b13ea27482981b4/?dl=1&#34;&gt;project.yaml&lt;/a&gt; and &lt;a href=&#34;https://heibox.uni-heidelberg.de/f/578df07c8fc04ffbadf3/?dl=1&#34;&gt; model last.cpkt&lt;/a&gt;. Rename last.ckpt to model.ckpt and place both under stable-diffusion-webui/src/latent-diffusion/experiments/pretrained_models/&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;p&gt;When launching, you may get a very long warning message related to some weights not being used. You may freely ignore it. After a while, you will get a message like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Running on local URL:  http://127.0.0.1:7860/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open the URL in browser, and you are good to go.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;The script creates a web UI for Stable Diffusion&#39;s txt2img and img2img scripts. Following are features added that are not in original script.&lt;/p&gt; &#xA;&lt;h3&gt;GFPGAN&lt;/h3&gt; &#xA;&lt;p&gt;Lets you improve faces in pictures using the GFPGAN model. There is a checkbox in every tab to use GFPGAN at 100%, and also a separate tab that just allows you to use GFPGAN on any picture, with a slider that controls how strongthe effect is.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/GFPGAN.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;RealESRGAN&lt;/h3&gt; &#xA;&lt;p&gt;Lets you double the resolution of generated images. There is a checkbox in every tab to use RealESRGAN, and you can choose between the regular upscaler and the anime version. There is also a separate tab for using RealESRGAN on any picture.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/RealESRGAN.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Sampling method selection&lt;/h3&gt; &#xA;&lt;p&gt;txt2img samplers: &#34;DDIM&#34;, &#34;PLMS&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39; img2img samplers: &#34;DDIM&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/sampling.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Prompt matrix&lt;/h3&gt; &#xA;&lt;p&gt;Separate multiple prompts using the &lt;code&gt;|&lt;/code&gt; character, and the system will produce an image for every combination of them. For example, if you use &lt;code&gt;a busy city street in a modern city|illustration|cinematic lighting&lt;/code&gt; prompt, there are four combinations possible (first part of prompt is always kept):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city, illustration&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city, cinematic lighting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city, illustration, cinematic lighting&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Four images will be produced, in this order, all with same seed and each with corresponding prompt: &lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/prompt-matrix.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Another example, this time with 5 prompts and 16 variations: &lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/prompt_matrix.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use this feature, batch count will be ignored, because the number of pictures to produce depends on your prompts, but batch size will still work (generating multiple pictures at the same time for a small speed boost).&lt;/p&gt; &#xA;&lt;h3&gt;Flagging (Broken after UI changed to gradio.Blocks() see &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/issues/50&#34;&gt;Flag button missing from new UI&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;Click the Flag button under the output section, and generated images will be saved to &lt;code&gt;log/images&lt;/code&gt; directory, and generation parameters will be appended to a csv file &lt;code&gt;log/log.csv&lt;/code&gt; in the &lt;code&gt;/sd&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;but every image is saved, why would I need this?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you&#39;re like me, you experiment a lot with prompts and settings, and only few images are worth saving. You can just save them using right click in browser, but then you won&#39;t be able to reproduce them later because you will not know what exact prompt created the image. If you use the flag button, generation paramerters will be written to csv file, and you can easily find parameters for an image by searching for its filename.&lt;/p&gt; &#xA;&lt;h3&gt;Copy-paste generation parameters&lt;/h3&gt; &#xA;&lt;p&gt;A text output provides generation parameters in an easy to copy-paste form for easy sharing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/kopipe.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you generate multiple pictures, the displayed seed will be the seed of the first one.&lt;/p&gt; &#xA;&lt;h3&gt;Correct seeds for batches&lt;/h3&gt; &#xA;&lt;p&gt;If you use a seed of 1000 to generate two batches of two images each, four generated images will have seeds: &lt;code&gt;1000, 1001, 1002, 1003&lt;/code&gt;. Previous versions of the UI would produce &lt;code&gt;1000, x, 1001, x&lt;/code&gt;, where x is an iamge that can&#39;t be generated by any seed.&lt;/p&gt; &#xA;&lt;h3&gt;Resizing&lt;/h3&gt; &#xA;&lt;p&gt;There are three options for resizing input images in img2img mode:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Just resize - simply resizes source image to target resolution, resulting in incorrect aspect ratio&lt;/li&gt; &#xA; &lt;li&gt;Crop and resize - resize source image preserving aspect ratio so that entirety of target resolution is occupied by it, and crop parts that stick out&lt;/li&gt; &#xA; &lt;li&gt;Resize and fill - resize source image preserving aspect ratio so that it entirely fits target resolution, and fill empty space by rows/columns from source image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example: &lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/resizing.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Loading&lt;/h3&gt; &#xA;&lt;p&gt;Gradio&#39;s loading graphic has a very negative effect on the processing speed of the neural network. My RTX 3090 makes images about 10% faster when the tab with gradio is not active. By default, the UI now hides loading progress animation and replaces it with static &#34;Loading...&#34; text, which achieves the same effect. Use the --no-progressbar-hiding commandline option to revert this and show loading animations.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt validation&lt;/h3&gt; &#xA;&lt;p&gt;Stable Diffusion has a limit for input text length. If your prompt is too long, you will get a warning in the text output field, showing which parts of your text were truncated and ignored by the model.&lt;/p&gt; &#xA;&lt;h3&gt;Loopback&lt;/h3&gt; &#xA;&lt;p&gt;A checkbox for img2img allowing to automatically feed output image as input for the next batch. Equivalent to saving output image, and replacing input image with it. Batch count setting controls how many iterations of this you get.&lt;/p&gt; &#xA;&lt;p&gt;Usually, when doing this, you would choose one of many images for the next iteration yourself, so the usefulness of this feature may be questionable, but I&#39;ve managed to get some very nice outputs with it that I wasn&#39;t abble to get otherwise.&lt;/p&gt; &#xA;&lt;p&gt;Example: (cherrypicked result; original picture by anon)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/loopback.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;--help&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --outdir [OUTDIR]     dir to write results to&#xA;  --outdir_txt2img [OUTDIR_TXT2IMG]&#xA;                        dir to write txt2img results to (overrides --outdir)&#xA;  --outdir_img2img [OUTDIR_IMG2IMG]&#xA;                        dir to write img2img results to (overrides --outdir)&#xA;  --save-metadata       Whether to embed the generation parameters in the sample images&#xA;  --skip-grid           do not save a grid, only individual samples. Helpful when evaluating lots of samples&#xA;  --skip-save           do not save indiviual samples. For speed measurements.&#xA;  --n_rows N_ROWS       rows in the grid; use -1 for autodetect and 0 for n_rows to be same as batch_size (default:&#xA;                        -1)&#xA;  --config CONFIG       path to config which constructs model&#xA;  --ckpt CKPT           path to checkpoint of model&#xA;  --precision {full,autocast}&#xA;                        evaluate at this precision&#xA;  --gfpgan-dir GFPGAN_DIR&#xA;                        GFPGAN directory&#xA;  --realesrgan-dir REALESRGAN_DIR&#xA;                        RealESRGAN directory&#xA;  --realesrgan-model REALESRGAN_MODEL&#xA;                        Upscaling model for RealESRGAN&#xA;  --no-verify-input     do not verify input to check if it&#39;s too long&#xA;  --no-half             do not switch the model to 16-bit floats&#xA;  --no-progressbar-hiding&#xA;                        do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware&#xA;                        accleration in browser)&#xA;  --defaults DEFAULTS   path to configuration file providing UI defaults, uses same format as cli parameter&#xA;  --gpu GPU             choose which GPU to use if you have multiple&#xA;  --extra-models-cpu    run extra models (GFGPAN/ESRGAN) on cpu&#xA;  --esrgan-cpu          run ESRGAN on cpu&#xA;  --gfpgan-cpu          run GFPGAN on cpu&#xA;  --cli CLI             don&#39;t launch web server, take Python function kwargs from this file.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Stable Diffusion was made possible thanks to a collaboration with &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and &lt;a href=&#34;https://runwayml.com/&#34;&gt;Runway&lt;/a&gt; and builds upon our previous work:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;Björn Ommer&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CVPR &#39;22 Oral&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;which is available on &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;GitHub&lt;/a&gt;. PDF at &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt;. Please also visit our &lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;Project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/assets/stable-samples/txt2img/merged-0006.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/#stable-diffusion-v1&#34;&gt;Stable Diffusion&lt;/a&gt; is a latent text-to-image diffusion model. Thanks to a generous compute donation from &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and support from &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; database. Similar to Google&#39;s &lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Imagen&lt;/a&gt;, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See &lt;a href=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/#stable-diffusion-v1&#34;&gt;this section&lt;/a&gt; below and the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Stable Diffusion v1&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.&lt;/p&gt; &#xA;&lt;p&gt;*Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>nateraw/stable-diffusion-videos</title>
    <updated>2022-09-18T01:50:20Z</updated>
    <id>tag:github.com,2022-09-18:/nateraw/stable-diffusion-videos</id>
    <link href="https://github.com/nateraw/stable-diffusion-videos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Create 🔥 videos with Stable Diffusion by exploring the latent space and morphing between text prompts&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;stable-diffusion-videos&lt;/h1&gt; &#xA;&lt;p&gt;Try it yourself in Colab: &lt;a href=&#34;https://colab.research.google.com/github/nateraw/stable-diffusion-videos/blob/main/stable_diffusion_videos.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; - morphing between &#34;blueberry spaghetti&#34; and &#34;strawberry spaghetti&#34;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/32437151/188721341-6f28abf9-699b-46b0-a72e-fa2a624ba0bb.mp4&#34;&gt;https://user-images.githubusercontent.com/32437151/188721341-6f28abf9-699b-46b0-a72e-fa2a624ba0bb.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it Works&lt;/h2&gt; &#xA;&lt;h3&gt;The Notebook/App&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://colab.research.google.com/github/nateraw/stable-diffusion-videos/blob/main/stable_diffusion_videos.ipynb&#34;&gt;in-browser Colab demo&lt;/a&gt; allows you to generate videos by interpolating the latent space of &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can either dream up different versions of the same prompt, or morph between different text prompts (with seeds set for each for reproducibility).&lt;/p&gt; &#xA;&lt;p&gt;The app is built with &lt;a href=&#34;https://gradio.app/&#34;&gt;Gradio&lt;/a&gt;, which allows you to interact with the model in a web app. Here&#39;s how I suggest you use it:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the &#34;Images&#34; tab to generate images you like.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Find two images you want to morph between&lt;/li&gt; &#xA;   &lt;li&gt;These images should use the same settings (guidance scale, scheduler, height, width)&lt;/li&gt; &#xA;   &lt;li&gt;Keep track of the seeds/settings you used so you can reproduce them&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate videos using the &#34;Videos&#34; tab&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Using the images you found from the step above, provide the prompts/seeds you recorded&lt;/li&gt; &#xA;   &lt;li&gt;Set the &lt;code&gt;num_walk_steps&lt;/code&gt; - for testing you can use a small number like 3 or 5, but to get great results you&#39;ll want to use something larger (60-200 steps).&lt;/li&gt; &#xA;   &lt;li&gt;You can set the &lt;code&gt;output_dir&lt;/code&gt; to the directory you wish to save to&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Python Package&lt;/h3&gt; &#xA;&lt;h4&gt;Setup&lt;/h4&gt; &#xA;&lt;p&gt;Install the package&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install stable_diffusion_videos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Authenticate with Hugging Face&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Programatic Usage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_diffusion_videos import walk&#xA;&#xA;walk(&#xA;    prompts=[&#39;a cat&#39;, &#39;a dog&#39;],&#xA;    seeds=[42, 1337],&#xA;    output_dir=&#39;dreams&#39;,     # Where images/videos will be saved&#xA;    name=&#39;animals_test&#39;,     # Subdirectory of output_dir where images/videos will be saved&#xA;    guidance_scale=8.5,      # Higher adheres to prompt more, lower lets model take the wheel&#xA;    num_steps=5,             # Change to 60-200 for better results...3-5 for testing&#xA;    num_inference_steps=50, &#xA;    scheduler=&#39;klms&#39;,        # One of: &#34;klms&#34;, &#34;default&#34;, &#34;ddim&#34;&#xA;    disable_tqdm=False,      # Set to True to disable tqdm progress bar&#xA;    make_video=True,         # If false, just save images&#xA;    use_lerp_for_text=True,  # Use lerp for text embeddings instead of slerp&#xA;    do_loop=False,           # Change to True if you want last prompt to loop back to first prompt&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run the App Locally&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_diffusion_videos import interface&#xA;&#xA;interface.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This work built off of &lt;a href=&#34;https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355&#34;&gt;a script&lt;/a&gt; shared by &lt;a href=&#34;https://github.com/karpathy&#34;&gt;@karpathy&lt;/a&gt;. The script was modified to &lt;a href=&#34;https://gist.github.com/nateraw/c989468b74c616ebbc6474aa8cdd9e53&#34;&gt;this gist&lt;/a&gt;, which was then updated/modified to this repo.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;You can file any issues/feature requests &lt;a href=&#34;https://github.com/nateraw/stable-diffusion-videos/issues&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Enjoy 🤗&lt;/p&gt; &#xA;&lt;h2&gt;Extras&lt;/h2&gt; &#xA;&lt;h3&gt;Upsample with Real-ESRGAN&lt;/h3&gt; &#xA;&lt;p&gt;You can also 4x upsample your images with &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;First, you&#39;ll need to install it...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install realesrgan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you&#39;ll be able to use &lt;code&gt;upsample=True&lt;/code&gt; in the &lt;code&gt;walk&lt;/code&gt; function, like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_diffusion_videos import walk&#xA;&#xA;walk([&#39;a cat&#39;, &#39;a dog&#39;], [234, 345], upsample=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above may cause you to run out of VRAM. No problem, you can do upsampling separately.&lt;/p&gt; &#xA;&lt;p&gt;To upsample an individual image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_diffusion_videos import PipelineRealESRGAN&#xA;&#xA;pipe = PipelineRealESRGAN.from_pretrained(&#39;nateraw/real-esrgan&#39;)&#xA;enhanced_image = pipe(&#39;your_file.jpg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, to do a whole folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_diffusion_videos import PipelineRealESRGAN&#xA;&#xA;pipe = PipelineRealESRGAN.from_pretrained(&#39;nateraw/real-esrgan&#39;)&#xA;pipe.upsample_imagefolder(&#39;path/to/images/&#39;, &#39;path/to/output_dir&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>