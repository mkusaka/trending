<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-19T01:42:04Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lllyasviel/IC-Light</title>
    <updated>2024-05-19T01:42:04Z</updated>
    <id>tag:github.com,2024-05-19:/lllyasviel/IC-Light</id>
    <link href="https://github.com/lllyasviel/IC-Light" rel="alternate"></link>
    <summary type="html">&lt;p&gt;More relighting!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;IC-Light&lt;/h1&gt; &#xA;&lt;p&gt;IC-Light is a project to manipulate the illumination of images.&lt;/p&gt; &#xA;&lt;p&gt;The name &#34;IC-Light&#34; stands for &lt;strong&gt;&#34;Imposing Consistent Light&#34;&lt;/strong&gt; (we will briefly describe this at the end of this page).&lt;/p&gt; &#xA;&lt;p&gt;Currently, we release two types of models: text-conditioned relighting model and background-conditioned model. Both types take foreground images as inputs.&lt;/p&gt; &#xA;&lt;h1&gt;Get Started&lt;/h1&gt; &#xA;&lt;p&gt;Below script will run the text-conditioned relighting model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/IC-Light.git&#xA;cd IC-Light&#xA;conda create -n iclight python=3.10&#xA;conda activate iclight&#xA;pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121&#xA;pip install -r requirements.txt&#xA;python gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, to use background-conditioned demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_demo_bg.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model downloading is automatic.&lt;/p&gt; &#xA;&lt;p&gt;Note that the &#34;gradio_demo.py&#34; has an official &lt;a href=&#34;https://huggingface.co/spaces/lllyasviel/IC-Light&#34;&gt;huggingFace Space here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Screenshot&lt;/h1&gt; &#xA;&lt;h3&gt;Text-Conditioned Model&lt;/h3&gt; &#xA;&lt;p&gt;(Note that the &#34;Lighting Preference&#34; are just initial latents - eg., if the Lighting Preference is &#34;Left&#34; then initial latent is left white right black.)&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, warm atmosphere, at home, bedroom&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/87265483-aa26-4d2e-897d-b58892f5fdd7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunshine from window&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/148c4a6d-82e7-4e3a-bf44-5c9a24538afc&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;beautiful woman, detailed face, neon, Wong Kar-wai, warm&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/f53c9de2-534a-42f4-8272-6d16a021fc01&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunshine, outdoor, warm atmosphere&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/25d6ea24-a736-4a0b-b42d-700fe8b2101e&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunshine, outdoor, warm atmosphere&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/dd30387b-0490-46ee-b688-2191fb752e68&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunshine from window&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/6c9511ca-f97f-401a-85f3-92b4442000e3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, shadow from window&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/e73701d5-890e-4b15-91ee-97f16ea3c450&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunset over sea&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/ff26ac3d-1b12-4447-b51f-73f7a5122a05&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: handsome boy, detailed face, neon light, city&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/d7795e02-46f7-444f-93e7-4d6460840437&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, light and shadow&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/706f70a8-d1a0-4e0b-b3ac-804e8e231c0f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(beautiful woman, detailed face, soft studio lighting)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/fe0a72df-69d4-4e11-b661-fb8b84d0274d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: Buddha, detailed face, sci-fi RGB glowing, cyberpunk&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/68d60c68-ce23-4902-939e-11629ccaf39a&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: Buddha, detailed face, natural lighting&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/1841d23d-0a0d-420b-a5ab-302da9c47c17&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: toy, detailed face, shadow from window&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Bottom&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/dcb97439-ea6b-483e-8e68-cf5d320368c7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: toy, detailed face, sunset over sea&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/4f78b897-621d-4527-afa7-78d62c576100&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: dog, magic lit, sci-fi RGB glowing, studio lighting&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Bottom&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/1db9cac9-8d3f-4f40-82e2-e3b0cafd8613&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: mysteriou human, warm atmosphere, warm atmosphere, at home, bedroom&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/5d5aa7e5-8cbd-4e1f-9f27-2ecc3c30563a&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Background-Conditioned Model&lt;/h3&gt; &#xA;&lt;p&gt;The background conditioned model does not require careful prompting. One can just use simple prompts like &#34;handsome man, cinematic lighting&#34;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/0b2a889f-682b-4393-b1ec-2cabaa182010&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/477ca348-bd47-46ff-81e6-0ffc3d05feb2&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/5bc9d8d9-02cd-442e-a75c-193f115f2ad8&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/a35e4c57-e199-40e2-893b-cb1c549612a9&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;A more structured visualization:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/c1daafb5-ac8b-461c-bff2-899e4c671ba3&#34; alt=&#34;r1&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Imposing Consistent Light&lt;/h1&gt; &#xA;&lt;p&gt;In HDR space, illumination has a property that all light transports are independent.&lt;/p&gt; &#xA;&lt;p&gt;As a result, the blending of appearances of different light sources is equivalent to the appearance with mixed light sources:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/27c67787-998e-469f-862f-047344e100cd&#34; alt=&#34;cons&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Using the above &lt;a href=&#34;https://www.pauldebevec.com/Research/LS/&#34;&gt;light stage&lt;/a&gt; as an example, the two images from the &#34;appearance mixture&#34; and &#34;light source mixture&#34; are consistent (mathematically equivalent in HDR space, ideally).&lt;/p&gt; &#xA;&lt;p&gt;We imposed such consistency (using MLPs in latent space) when training the relighting models.&lt;/p&gt; &#xA;&lt;p&gt;As a result, the model is able to produce highly consistent relight - &lt;strong&gt;so&lt;/strong&gt; consistent that different relightings can even be merged as normal maps! Despite the fact that the models are latent diffusion.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/25068f6a-f945-4929-a3d6-e8a152472223&#34; alt=&#34;r2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;From left to right are inputs, model outputs relighting, devided shadow image, and merged normal maps. Note that the model is not trained with any normal map data. This normal estimation comes from the consistency of relighting.&lt;/p&gt; &#xA;&lt;p&gt;You can reproduce this experiment using this button (it is 4x slower because it relight image 4 times)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/d9c37bf7-2136-446c-a9a5-5a341e4906de&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/fcf5dd55-0309-4e8e-9721-d55931ea77f0&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below are bigger images (feel free to try yourself to get more results!)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/12335218-186b-4c61-b43a-79aea9df8b21&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/2daab276-fdfa-4b0c-abcb-e591f575598a&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For reference, &lt;a href=&#34;https://fuxiao0719.github.io/projects/geowizard/&#34;&gt;geowizard&lt;/a&gt; (geowizard is a really great work!):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/4ba1a96d-e218-42ab-83ae-a7918d56ee5f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;And, &lt;a href=&#34;https://arxiv.org/pdf/2402.18848&#34;&gt;switchlight&lt;/a&gt; (switchlight is another great work!):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/fbdd961f-0b26-45d2-802e-ffd734affab8&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Model Notes&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;iclight_sd15_fc.safetensors&lt;/strong&gt; - The default relighting model, conditioned on text and foreground. You can use initial latent to influence the relighting.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;iclight_sd15_fcon.safetensors&lt;/strong&gt; - Same as &#34;iclight_sd15_fc.safetensors&#34; but trained with offset noise. Note that the default &#34;iclight_sd15_fc.safetensors&#34; outperform this model slightly in a user study. And this is the reason why the default model is the model without offset noise.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;iclight_sd15_fbc.safetensors&lt;/strong&gt; - Relighting model conditioned with text, foreground, and background.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Cite&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Misc{iclight,&#xA;  author = {Lvmin Zhang and Anyi Rao and Maneesh Agrawala},&#xA;  title  = {IC-Light GitHub Page},&#xA;  year   = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Related Work&lt;/h1&gt; &#xA;&lt;p&gt;Also read ...&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://augmentedperception.github.io/total_relighting/&#34;&gt;Total Relighting: Learning to Relight Portraits for Background Replacement&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.06886&#34;&gt;Relightful Harmonization: Lighting-aware Portrait Background Replacement&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.18848&#34;&gt;SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Shubhamsaboo/awesome-llm-apps</title>
    <updated>2024-05-19T01:42:04Z</updated>
    <id>tag:github.com,2024-05-19:/Shubhamsaboo/awesome-llm-apps</id>
    <link href="https://github.com/Shubhamsaboo/awesome-llm-apps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Collection of awesome LLM apps with RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://unwindai.substack.com&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind.png&#34; width=&#34;600px&#34; alt=&#34;Unwind AI&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.linkedin.com/in/shubhamsaboo/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Follow%20Shubham Saboo-blue?logo=linkedin&amp;amp;style=flat-square&#34; alt=&#34;LinkedIn&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/Saboo_Shubham_&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/Shubham Saboo&#34; alt=&#34;Twitter&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;ğŸŒŸ Awesome LLM Apps&lt;/h1&gt; &#xA;&lt;p&gt;A curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and even open-source models like LLaMA that you can run locally on your computer.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ¤” Why Awesome LLM Apps?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”¥ Explore apps that combines LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“ Learn from well-documented projects and contribute to the growing opensource ecosystem of LLM-powered applications.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“‚ Featured Projects&lt;/h2&gt; &#xA;&lt;h3&gt;ğŸ’» Local Lllama-3 with RAG&lt;/h3&gt; &#xA;&lt;p&gt;Chat with any webpage using local Llama-3 and Retrieval Augmented Generation (RAG) in a Streamlit app. Enjoy 100% free and offline functionality.&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ’¬ Chat with GitHub Repo&lt;/h3&gt; &#xA;&lt;p&gt;Engage in natural conversations with your GitHub repositories using GPT-4. Uncover valuable insights and documentation effortlessly.&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ“¨ Chat with Gmail&lt;/h3&gt; &#xA;&lt;p&gt;Interact with your Gmail inbox using natural language. Get accurate answers to your questions based on the content of your emails with Retrieval Augmented Generation (RAG).&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ“ Chat with Substack Newsletter&lt;/h3&gt; &#xA;&lt;p&gt;Chat with a Substack newsletter using OpenAI&#39;s API and the Embedchain library in a Streamlit app. Leverage GPT-4 for precise answers based on newsletter content.&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ“„ Chat with PDF&lt;/h3&gt; &#xA;&lt;p&gt;Engage in intelligent conversation and question-answering based on the content of your PDF documents. Simply upload and start asking questions.&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ“½ï¸ Chat with YouTube Videos&lt;/h3&gt; &#xA;&lt;p&gt;Dive into video content with interactive conversation and question-answering based on YouTube videos. Provide a URL and engage with the video&#39;s content through natural language.&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ’» Web Scraping AI Agent&lt;/h3&gt; &#xA;&lt;p&gt;Intelligently scrape websites using OpenAI API and the scrapegraphai library. Specify the URL and extraction requirements, and let the AI agent handle the rest.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸš€ Getting Started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Navigate to the desired project directory&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd awesome-llm-apps/chat_with_gmail &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the required dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Follow the project-specific instructions in each project&#39;s README.md file to set up and run the app.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸ¤ Contributing to Opensource&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new &lt;a href=&#34;https://github.com/Shubhamsaboo/awesome-llm-apps/issues&#34;&gt;GitHub Issue&lt;/a&gt; or submit a pull request. Make sure to follow the existing project structure and include a detailed README.md for each new app.&lt;/p&gt; &#xA;&lt;h3&gt;Thank you community for the support ğŸ™&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ğŸŒŸ &lt;strong&gt;Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM applications with RAG.&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>decodingml/llm-twin-course</title>
    <updated>2024-05-19T01:42:04Z</updated>
    <id>tag:github.com,2024-05-19:/decodingml/llm-twin-course</id>
    <link href="https://github.com/decodingml/llm-twin-course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ¤– ğ—Ÿğ—²ğ—®ğ—¿ğ—» for ğ—³ğ—¿ğ—²ğ—² how to ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± an end-to-end ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—»-ğ—¿ğ—²ğ—®ğ—±ğ˜† ğ—Ÿğ—Ÿğ—  &amp; ğ—¥ğ—”ğ—š ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º using ğ—Ÿğ—Ÿğ— ğ—¢ğ—½ğ˜€ best practices: ~ ğ˜´ğ˜°ğ˜¶ğ˜³ğ˜¤ğ˜¦ ğ˜¤ğ˜°ğ˜¥ğ˜¦ + 11 ğ˜©ğ˜¢ğ˜¯ğ˜¥ğ˜´-ğ˜°ğ˜¯ ğ˜­ğ˜¦ğ˜´ğ˜´ğ˜°ğ˜¯ğ˜´&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;LLM Twin Course: Building Your Production-Ready AI Replica&lt;/h2&gt; &#xA; &lt;h1&gt;An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin&lt;/h1&gt; &#xA; &lt;h3&gt;From data gathering to productionizing LLMs using LLMOps good practices.&lt;/h3&gt; &#xA; &lt;i&gt;by &lt;a href=&#34;https://github.com/iusztinpaul&#34;&gt;Paul Iusztin&lt;/a&gt;, &lt;a href=&#34;https://github.com/alexandruvesa&#34;&gt;Alexandru Vesa&lt;/a&gt; and &lt;a href=&#34;https://github.com/Joywalker&#34;&gt;Alexandru Razvant&lt;/a&gt;&lt;/i&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/decodingml/llm-twin-course/main/media/cover.png&#34; alt=&#34;Your image description&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Why is this course different?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;By finishing the &lt;strong&gt;&#34;LLM Twin: Building Your Production-Ready AI Replica&#34;&lt;/strong&gt; free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Why should you care? ğŸ«µ&lt;/p&gt; &#xA; &lt;p&gt;â†’ &lt;strong&gt;No more isolated scripts or Notebooks!&lt;/strong&gt; Learn production ML by building and deploying an end-to-end production-grade LLM system.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;What will you learn to build by the end of this&amp;nbsp;course?&lt;/h2&gt; &#xA;&lt;p&gt;You will &lt;strong&gt;learn&lt;/strong&gt; how to &lt;strong&gt;architect&lt;/strong&gt; and &lt;strong&gt;build a real-world LLM system&lt;/strong&gt; from &lt;strong&gt;start&lt;/strong&gt; to &lt;strong&gt;finish&lt;/strong&gt;â€Š-â€Šfrom &lt;strong&gt;data collection&lt;/strong&gt; to &lt;strong&gt;deployment&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will also &lt;strong&gt;learn&lt;/strong&gt; to &lt;strong&gt;leverage MLOps best practices&lt;/strong&gt;, such as experiment trackers, model registries, prompt monitoring, and versioning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The end goal?&lt;/strong&gt; Build and deploy your own LLM twin.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What is an LLM Twin?&lt;/strong&gt; It is an AI character that learns to write like somebody by incorporating its style and personality into an LLM.&lt;/p&gt; &#xA;&lt;h2&gt;The architecture of the LLM twin is split into 4 Python microservices:&lt;/h2&gt; &#xA;&lt;h3&gt;The data collection pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Crawl your digital data from various social media platforms.&lt;/li&gt; &#xA; &lt;li&gt;Clean, normalize and load the data to a &lt;a href=&#34;https://www.mongodb.com/&#34;&gt;Mongo NoSQL DB&lt;/a&gt; through a series of ETL pipelines.&lt;/li&gt; &#xA; &lt;li&gt;Send database changes to a &lt;a href=&#34;https://www.rabbitmq.com/&#34;&gt;RabbitMQ&lt;/a&gt; queue using the CDC pattern.&lt;/li&gt; &#xA; &lt;li&gt;â˜ï¸ Deployed on &lt;a href=&#34;https://aws.amazon.com/&#34;&gt;AWS&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;The feature pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Consume messages from a queue through a &lt;a href=&#34;https://github.com/bytewax/bytewax?utm_source=github&amp;amp;utm_medium=decodingml&amp;amp;utm_campaign=2024_q1&#34;&gt;Bytewax&lt;/a&gt; streaming pipeline.&lt;/li&gt; &#xA; &lt;li&gt;Every message will be cleaned, chunked, embedded (using &lt;a href=&#34;https://github.com/superlinked/superlinked-alpha?utm_source=community&amp;amp;utm_medium=github&amp;amp;utm_campaign=oscourse&#34;&gt;Superlinked&lt;/a&gt;, and loaded into a &lt;a href=&#34;https://qdrant.tech/?utm_source=decodingml&amp;amp;utm_medium=referral&amp;amp;utm_campaign=llm-course&#34;&gt;Qdrant&lt;/a&gt; vector DB in real-time.&lt;/li&gt; &#xA; &lt;li&gt;â˜ï¸ Deployed on &lt;a href=&#34;https://aws.amazon.com/&#34;&gt;AWS&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;The training pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a custom dataset based on your digital data.&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune an LLM using QLoRA.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://www.comet.com/signup/?utm_source=decoding_ml&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Comet ML&#39;s&lt;/a&gt; experiment tracker to monitor the experiments.&lt;/li&gt; &#xA; &lt;li&gt;Evaluate and save the best model to &lt;a href=&#34;https://www.comet.com/signup/?utm_source=decoding_ml&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Comet&#39;s&lt;/a&gt; model registry.&lt;/li&gt; &#xA; &lt;li&gt;â˜ï¸ Deployed on &lt;a href=&#34;https://www.qwak.com/lp/end-to-end-mlops/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=decodingml&#34;&gt;Qwak&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;The inference pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Load and quantize the fine-tuned LLM from &lt;a href=&#34;https://www.comet.com/signup/?utm_source=decoding_ml&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Comet&#39;s&lt;/a&gt; model registry.&lt;/li&gt; &#xA; &lt;li&gt;Deploy it as a REST API.&lt;/li&gt; &#xA; &lt;li&gt;Enhance the prompts using RAG.&lt;/li&gt; &#xA; &lt;li&gt;Generate content using your LLM twin.&lt;/li&gt; &#xA; &lt;li&gt;Monitor the LLM using &lt;a href=&#34;https://www.comet.com/signup/?framework=llm&amp;amp;utm_source=decoding_ml&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Comet&#39;s&lt;/a&gt; prompt monitoring dashboard.&lt;/li&gt; &#xA; &lt;li&gt;â˜ï¸ Deployed on &lt;a href=&#34;https://www.qwak.com/lp/end-to-end-mlops/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=decodingml&#34;&gt;Qwak&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/decodingml/llm-twin-course/main/media/architecture.png&#34; alt=&#34;Your image description&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Along the 4 microservices, you will learn to integrate 3 serverless tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.comet.com/signup/?utm_source=decoding_ml&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Comet ML&lt;/a&gt; as your ML Platform;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qdrant.tech/?utm_source=decodingml&amp;amp;utm_medium=referral&amp;amp;utm_campaign=llm-course&#34;&gt;Qdrant&lt;/a&gt; as your vector DB;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.qwak.com/lp/end-to-end-mlops/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=decodingml&#34;&gt;Qwak&lt;/a&gt; as your ML infrastructure;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Who is this&amp;nbsp;for?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audience:&lt;/strong&gt; MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Level:&lt;/strong&gt; intermediate&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt; basic knowledge of Python, ML, and the cloud&lt;/p&gt; &#xA;&lt;h2&gt;How will you&amp;nbsp;learn?&lt;/h2&gt; &#xA;&lt;p&gt;The course contains &lt;strong&gt;11 hands-on written lessons&lt;/strong&gt; and the &lt;strong&gt;open-source code&lt;/strong&gt; you can access on GitHub.&lt;/p&gt; &#xA;&lt;p&gt;You can read everything and try out the code at your own pace.&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Costs?&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;strong&gt;articles&lt;/strong&gt; and &lt;strong&gt;code&lt;/strong&gt; are &lt;strong&gt;completely free&lt;/strong&gt;. They will always remain free.&lt;/p&gt; &#xA;&lt;p&gt;But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.&lt;/p&gt; &#xA;&lt;p&gt;The cloud computing platforms (&lt;a href=&#34;https://aws.amazon.com/&#34;&gt;AWS&lt;/a&gt;, &lt;a href=&#34;https://www.qwak.com/lp/end-to-end-mlops/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=decodingml&#34;&gt;Qwak&lt;/a&gt;) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.&lt;/p&gt; &#xA;&lt;p&gt;For the other serverless tools (&lt;a href=&#34;https://qdrant.tech/?utm_source=decodingml&amp;amp;utm_medium=referral&amp;amp;utm_campaign=llm-course&#34;&gt;Qdrant&lt;/a&gt;, &lt;a href=&#34;https://www.comet.com/signup/?utm_source=decoding_ml&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Comet&lt;/a&gt;), we will stick to their freemium version, which is free of charge.&lt;/p&gt; &#xA;&lt;h2&gt;Lessons&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;The course is a work in progress. We plan to release a new lesson every 2 weeks.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;code&gt;To understand the entire code step-by-step, check out our articles&lt;/code&gt; â†“&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The course is split into 11 lessons. Every Medium article will be its own lesson.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;System Design&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f&#34;&gt;An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Data Engineering: Gather &amp;amp; store the data for your LLM twin&lt;/h3&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/decodingml/the-importance-of-data-pipelines-in-the-era-of-generative-ai-673e1505a861&#34;&gt;The Importance of Data Pipelines in the Era of Generative AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/decodingml/the-3nd-out-of-11-lessons-of-the-llm-twin-free-course-ba82752dad5a&#34;&gt;Change Data Capture: Enabling Event-Driven Architectures&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Feature Pipeline: prepare data for LLM fine-tuning &amp;amp; RAG&lt;/h3&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/decodingml/sota-python-streaming-pipelines-for-fine-tuning-llms-and-rag-in-real-time-82eb07795b87&#34;&gt;SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG â€” in Real-Time!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2&#34;&gt;The 4 Advanced RAG Algorithms You Must Know to Implement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Training Pipeline: fine-tune your LLM twin&lt;/h3&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/decodingml/the-role-of-feature-stores-in-fine-tuning-llms-22bd60afd4b9&#34;&gt;The Role of Feature Stores in Fine-Tuning LLMs: From raw data to instruction dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuning LLM [Module 3]&amp;nbsp;â€¦WIP&lt;/li&gt; &#xA; &lt;li&gt;LLM evaluation [Module 4]&amp;nbsp;â€¦WIP&lt;/li&gt; &#xA; &lt;li&gt;Quantization [Module 5]&amp;nbsp;â€¦WIP&amp;nbsp;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Inference Pipeline: serve your LLM twin&lt;/h3&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;Build the digital twin inference pipeline [Module 6]&amp;nbsp;â€¦WIP&lt;/li&gt; &#xA; &lt;li&gt;Deploy the digital twin as a REST API [Module 6]&amp;nbsp;â€¦WIP&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Meet your teachers!&lt;/h2&gt; &#xA;&lt;p&gt;The course is created under the &lt;a href=&#34;https://decodingml.substack.com/&#34;&gt;Decoding ML&lt;/a&gt; umbrella by:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/iusztinpaul&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/iusztinpaul.png&#34; width=&#34;100&#34; style=&#34;border-radius:50%;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;strong&gt;Paul Iusztin&lt;/strong&gt;&lt;br&gt; &lt;i&gt;Senior ML &amp;amp; MLOps Engineer&lt;/i&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/alexandruvesa&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/alexandruvesa.png&#34; width=&#34;100&#34; style=&#34;border-radius:50%;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;strong&gt;Alexandru Vesa&lt;/strong&gt;&lt;br&gt; &lt;i&gt;Senior AI Engineer&lt;/i&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Joywalker&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/Joywalker.png&#34; width=&#34;100&#34; style=&#34;border-radius:50%;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;strong&gt;RÄƒzvanÈ› Alexandru&lt;/strong&gt;&lt;br&gt; &lt;i&gt;Senior ML Engineer&lt;/i&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This course is an open-source project released under the MIT license. Thus, as long you distribute our LICENSE and acknowledge our work, you can safely clone or fork this project and use it as a source of inspiration for whatever you want (e.g., university projects, college degree projects, personal projects, etc.).&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ† Contribution&lt;/h2&gt; &#xA;&lt;p&gt;A big &#34;Thank you ğŸ™&#34; to all our contributors! This course is possible only because of their efforts.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/decodingml/llm-twin-course/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=decodingml/llm-twin-course&#34;&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
</feed>