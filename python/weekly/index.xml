<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-15T02:03:23Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>helblazer811/ManimML</title>
    <updated>2023-01-15T02:03:23Z</updated>
    <id>tag:github.com,2023-01-15:/helblazer811/ManimML</id>
    <link href="https://github.com/helblazer811/ManimML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ManimML is a project focused on providing animations and visualizations of common machine learning concepts with the Manim Community Library.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ManimML&lt;/h1&gt; &#xA;&lt;a href=&#34;https://github.com/helblazer811/ManimMachineLearning&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/helblazer811/ManimML/main/examples/media/ManimMLLogo.gif&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/helblazer811/ManimMachineLearning/raw/main/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/helblazer811/ManimMachineLearning&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/github/v/release/helblazer811/ManimMachineLearning&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/helblazer811/ManimMachineLearning&#34; alt=&#34;GitHub tag&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/manim-ml&#34; alt=&#34;Pypi Downloads&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ManimML is a project focused on providing animations and visualizations of common machine learning concepts with the &lt;a href=&#34;https://www.manim.community/&#34;&gt;Manim Community Library&lt;/a&gt;. We want this project to be a compilation of primitive visualizations that can be easily combined to create videos about complex machine learning concepts. Additionally, we want to provide a set of abstractions which allow users to focus on explanations instead of software engineering.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/helblazer811/ManimML/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/helblazer811/ManimML/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;First you will want to &lt;a href=&#34;https://docs.manim.community/en/stable/installation.html&#34;&gt;install manim&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then install the package form source or &lt;code&gt;pip install manim_ml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you can run the following to generate the example videos from python scripts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;manim -pqh src/vae.py VAEScene&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Checkout the &lt;code&gt;examples&lt;/code&gt; directory for some example videos with source code.&lt;/p&gt; &#xA;&lt;h3&gt;Convolutional Neural Network&lt;/h3&gt; &#xA;&lt;p&gt;This is a visualization of a Convolutional Neural Network.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/helblazer811/ManimML/main/examples/media/CNNScene.gif&#34;&gt; &#xA;&lt;h3&gt;Neural Networks&lt;/h3&gt; &#xA;&lt;p&gt;This is a visualization of a Variational Autoencoder made using ManimML. It has a Pytorch style list of layers that can be composed in arbitrary order. The following video is made with the code from below.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/helblazer811/ManimML/main/examples/media/VAEScene.gif&#34;&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class VariationalAutoencoderScene(Scene):&#xA;&#xA;    def construct(self):&#xA;        embedding_layer = EmbeddingLayer(dist_theme=&#34;ellipse&#34;).scale(2)&#xA;        &#xA;        image = Image.open(&#39;images/image.jpeg&#39;)&#xA;        numpy_image = np.asarray(image)&#xA;        # Make nn&#xA;        neural_network = NeuralNetwork([&#xA;            ImageLayer(numpy_image, height=1.4),&#xA;            FeedForwardLayer(5),&#xA;            FeedForwardLayer(3),&#xA;            embedding_layer,&#xA;            FeedForwardLayer(3),&#xA;            FeedForwardLayer(5),&#xA;            ImageLayer(numpy_image, height=1.4),&#xA;        ], layer_spacing=0.1)&#xA;&#xA;        neural_network.scale(1.3)&#xA;&#xA;        self.play(Create(neural_network))&#xA;        self.play(neural_network.make_forward_pass_animation(run_time=15))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generative Adversarial Network&lt;/h3&gt; &#xA;&lt;p&gt;This is a visualization of a Generative Adversarial Network made using ManimML.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/helblazer811/ManimML/main/examples/media/GANScene.gif&#34;&gt; &#xA;&lt;h3&gt;VAE Disentanglement&lt;/h3&gt; &#xA;&lt;p&gt;This is a visualization of disentanglement with a Variational Autoencoder&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/helblazer811/ManimML/main/examples/media/DisentanglementScene.gif&#34;&gt;</summary>
  </entry>
  <entry>
    <title>jerryjliu/gpt_index</title>
    <updated>2023-01-15T02:03:23Z</updated>
    <id>tag:github.com,2023-01-15:/jerryjliu/gpt_index</id>
    <link href="https://github.com/jerryjliu/gpt_index" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An index created by GPT to organize external information and answer queries!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üóÇÔ∏è Ô∏èGPT Index&lt;/h1&gt; &#xA;&lt;p&gt;GPT Index is a project consisting of a set of &lt;em&gt;data structures&lt;/em&gt; that are created using LLMs and can be traversed using LLMs in order to answer queries.&lt;/p&gt; &#xA;&lt;p&gt;PyPi: &lt;a href=&#34;https://pypi.org/project/gpt-index/&#34;&gt;https://pypi.org/project/gpt-index/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Documentation: &lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/&#34;&gt;https://gpt-index.readthedocs.io/en/latest/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!&lt;/p&gt; &#xA;&lt;h4&gt;Context&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLMs are a phenomenal piece of technology for knowledge generation and reasoning.&lt;/li&gt; &#xA; &lt;li&gt;A big limitation of LLMs is context size (e.g. OpenAI&#39;s &lt;code&gt;davinci&lt;/code&gt; model for GPT-3 has a &lt;a href=&#34;https://openai.com/api/pricing/&#34;&gt;limit&lt;/a&gt; of 4096 tokens. Large, but not infinite).&lt;/li&gt; &#xA; &lt;li&gt;The ability to feed &#34;knowledge&#34; to LLMs is restricted to this limited prompt size and model weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Thought&lt;/strong&gt;: what if LLMs could have access to potentially a much larger database of knowledge without retraining/finetuning?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Proposed Solution&lt;/h4&gt; &#xA;&lt;p&gt;That&#39;s where &lt;strong&gt;GPT Index&lt;/strong&gt; comes in. GPT Index is a simple, flexible interface between your external data and LLMs. It resolves the following pain points:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provides simple data structures to resolve prompt size limitations.&lt;/li&gt; &#xA; &lt;li&gt;Offers data connectors to your external data sources.&lt;/li&gt; &#xA; &lt;li&gt;Offers you a comprehensive toolset trading off cost and performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;At the core of GPT Index is a &lt;strong&gt;data structure&lt;/strong&gt;. Instead of relying on world knowledge encoded in model weights, a GPT Index data structure does the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uses a pre-trained LLM primarily for &lt;em&gt;reasoning&lt;/em&gt;/&lt;em&gt;summarization&lt;/em&gt; instead of prior knowledge.&lt;/li&gt; &#xA; &lt;li&gt;Takes a large corpus of text data as input and builds a structured index over it (using an LLM or heuristics).&lt;/li&gt; &#xA; &lt;li&gt;Allow users to &lt;em&gt;query&lt;/em&gt; the index in order to synthesize an answer to the question - this requires both &lt;em&gt;traversal&lt;/em&gt; of the index as well as a synthesis of the answer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üí° Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Interesting in contributing? See our &lt;a href=&#34;https://raw.githubusercontent.com/jerryjliu/gpt_index/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Full documentation can be found here: &lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/&#34;&gt;https://gpt-index.readthedocs.io/en/latest/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please check it out for the most up-to-date tutorials, how-to guides, references, and other resources!&lt;/p&gt; &#xA;&lt;h2&gt;üíª Example Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gpt-index&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Examples are in the &lt;code&gt;examples&lt;/code&gt; folder. Indices are in the &lt;code&gt;indices&lt;/code&gt; folder (see list of indices below).&lt;/p&gt; &#xA;&lt;p&gt;To build a tree index:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gpt_index import GPTTreeIndex, SimpleDirectoryReader&#xA;documents = SimpleDirectoryReader(&#39;data&#39;).load_data()&#xA;index = GPTTreeIndex(documents)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To save to and load from disk:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# save to disk&#xA;index.save_to_disk(&#39;index.json&#39;)&#xA;# load from disk&#xA;index = GPTTreeIndex.load_from_disk(&#39;index.json&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To query:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;index.query(&#34;&amp;lt;question_text&amp;gt;?&#34;, child_branch_factor=1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîß Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;The main third-party package requirements are &lt;code&gt;tiktoken&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt;, and &lt;code&gt;langchain&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All requirements should be contained within the &lt;code&gt;setup.py&lt;/code&gt; file. To run the package locally without building the wheel, simply run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>blakeblackshear/frigate</title>
    <updated>2023-01-15T02:03:23Z</updated>
    <id>tag:github.com,2023-01-15:/blakeblackshear/frigate</id>
    <link href="https://github.com/blakeblackshear/frigate" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVR with realtime local object detection for IP cameras&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; alt=&#34;logo&#34; src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/frigate.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Frigate - NVR With Realtime Object Detection for IP Cameras&lt;/h1&gt; &#xA;&lt;p&gt;A complete and local NVR designed for &lt;a href=&#34;https://www.home-assistant.io&#34;&gt;Home Assistant&lt;/a&gt; with AI object detection. Uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras.&lt;/p&gt; &#xA;&lt;p&gt;Use of a &lt;a href=&#34;https://coral.ai/products/&#34;&gt;Google Coral Accelerator&lt;/a&gt; is optional, but highly recommended. The Coral will outperform even the best CPUs and can process 100+ FPS with very little overhead.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tight integration with Home Assistant via a &lt;a href=&#34;https://github.com/blakeblackshear/frigate-hass-integration&#34;&gt;custom component&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Designed to minimize resource use and maximize performance by only looking for objects when and where it is necessary&lt;/li&gt; &#xA; &lt;li&gt;Leverages multiprocessing heavily with an emphasis on realtime over processing every frame&lt;/li&gt; &#xA; &lt;li&gt;Uses a very low overhead motion detection to determine where to run object detection&lt;/li&gt; &#xA; &lt;li&gt;Object detection with TensorFlow runs in separate processes for maximum FPS&lt;/li&gt; &#xA; &lt;li&gt;Communicates over MQTT for easy integration into other systems&lt;/li&gt; &#xA; &lt;li&gt;Records video with retention settings based on detected objects&lt;/li&gt; &#xA; &lt;li&gt;24/7 recording&lt;/li&gt; &#xA; &lt;li&gt;Re-streaming via RTSP to reduce the number of connections to your camera&lt;/li&gt; &#xA; &lt;li&gt;WebRTC &amp;amp; MSE support for low-latency live view&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;View the documentation at &lt;a href=&#34;https://docs.frigate.video&#34;&gt;https://docs.frigate.video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Donations&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to make a donation to support development, please use &lt;a href=&#34;https://github.com/sponsors/blakeblackshear&#34;&gt;Github Sponsors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;Integration into Home Assistant&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/media_browser.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/media_browser.png&#34; height=&#34;400&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/notification.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/notification.png&#34; height=&#34;400&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Also comes with a builtin UI:&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/home-ui.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/home-ui.png&#34; height=&#34;400&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/camera-ui.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/camera-ui.png&#34; height=&#34;400&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/events-ui.png&#34; alt=&#34;Events&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>