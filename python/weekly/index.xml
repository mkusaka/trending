<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-16T01:50:17Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>volcengine/verl</title>
    <updated>2025-02-16T01:50:17Z</updated>
    <id>tag:github.com,2025-02-16:/volcengine/verl</id>
    <link href="https://github.com/volcengine/verl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;verl: Volcano Engine Reinforcement Learning for LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1 style=&#34;text-align: center;&#34;&gt;verl: Volcano Engine Reinforcement Learning for LLM&lt;/h1&gt; &#xA;&lt;p&gt;verl is a flexible, efficient and production-ready RL training library for large language models (LLMs).&lt;/p&gt; &#xA;&lt;p&gt;verl is the open-source version of &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.19256v2&#34;&gt;HybridFlow: A Flexible and Efficient RLHF Framework&lt;/a&gt;&lt;/strong&gt; paper.&lt;/p&gt; &#xA;&lt;p&gt;verl is flexible and easy to use with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy extension of diverse RL algorithms&lt;/strong&gt;: The Hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Seamless integration of existing LLM infra with modular APIs&lt;/strong&gt;: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM and vLLM. Moreover, users can easily extend to other LLM training and inference frameworks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible device mapping&lt;/strong&gt;: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Readily integration with popular HuggingFace models&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;verl is fast with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;State-of-the-art throughput&lt;/strong&gt;: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient actor model resharding with 3D-HybridEngine&lt;/strong&gt;: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://verl.readthedocs.io/en/latest/index.html&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2409.19256v2&#34;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA&#34;&gt;&lt;b&gt;Slack&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG&#34;&gt;&lt;b&gt;Wechat&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://x.com/verl_project&#34;&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;&#34;&gt;&lt;b&gt;Slides&lt;/b&gt;&lt;/a&gt; | --&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2025/2] We will present verl in the &lt;a href=&#34;https://lu.ma/ji7atxux&#34;&gt;Bytedance/NVIDIA/Anyscale Ray Meetup&lt;/a&gt; in bay area on Feb 13th. Come join us in person!&lt;/li&gt; &#xA; &lt;li&gt;[2025/1] &lt;a href=&#34;https://team.doubao.com/zh/special/doubao_1_5_pro&#34;&gt;Doubao-1.5-pro&lt;/a&gt; is released with SOTA-level performance on LLM &amp;amp; VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).&lt;/li&gt; &#xA; &lt;li&gt;[2024/12] The team presented &lt;a href=&#34;https://neurips.cc/Expo/Conferences/2024/workshop/100677&#34;&gt;Post-training LLMs: From Algorithms to Infrastructure&lt;/a&gt; at NeurIPS 2024. &lt;a href=&#34;https://github.com/eric-haibin-lin/verl-data/tree/neurips&#34;&gt;Slides&lt;/a&gt; and &lt;a href=&#34;https://neurips.cc/Expo/Conferences/2024/workshop/100677&#34;&gt;video&lt;/a&gt; available.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] verl is presented at Ray Summit. &lt;a href=&#34;https://www.youtube.com/watch?v=MrhMcXkXvJU&amp;amp;list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&amp;amp;index=37&#34;&gt;Youtube video&lt;/a&gt; available.&lt;/li&gt; &#xA; &lt;li&gt;[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;FSDP&lt;/strong&gt; and &lt;strong&gt;Megatron-LM&lt;/strong&gt; for training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt; and &lt;strong&gt;TGI&lt;/strong&gt; for rollout generation, &lt;strong&gt;SGLang&lt;/strong&gt; support coming soon.&lt;/li&gt; &#xA; &lt;li&gt;huggingface models support&lt;/li&gt; &#xA; &lt;li&gt;Supervised fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;Reinforcement learning from human feedback with &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/ppo_trainer&#34;&gt;PPO&lt;/a&gt;, &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/grpo_trainer&#34;&gt;GRPO&lt;/a&gt;, and &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/remax_trainer&#34;&gt;ReMax&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support model-based reward and function-based reward (verifiable reward)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;flash-attention, &lt;a href=&#34;https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh&#34;&gt;sequence packing&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh&#34;&gt;long context&lt;/a&gt; support via DeepSpeed Ulysses, &lt;a href=&#34;https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh&#34;&gt;Liger-kernel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;scales up to 70B models and hundreds of GPUs&lt;/li&gt; &#xA; &lt;li&gt;experiment tracking with wandb, swanlab and mlflow&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Upcoming Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reward model training&lt;/li&gt; &#xA; &lt;li&gt;DPO training&lt;/li&gt; &#xA; &lt;li&gt;DeepSeek integration with Megatron backend&lt;/li&gt; &#xA; &lt;li&gt;SGLang integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Checkout this &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/ppo_trainer/verl_getting_started.ipynb&#34;&gt;Jupyter Notebook&lt;/a&gt; to get started with PPO training with a single 24GB L4 GPU (&lt;strong&gt;FREE&lt;/strong&gt; GPU quota provided by &lt;a href=&#34;https://lightning.ai/hlin-verl/studios/verl-getting-started&#34;&gt;Lighting Studio&lt;/a&gt;)!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quickstart:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/start/install.html&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/start/quickstart.html&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/hybrid_flow.html&#34;&gt;Programming Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running a PPO example step-by-step:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data and Reward Preparation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/preparation/prepare_data.html&#34;&gt;Prepare Data for Post-Training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/preparation/reward_function.html&#34;&gt;Implement Reward Function for Dataset&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Understanding the PPO Example &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html&#34;&gt;PPO Example Architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/examples/config.html&#34;&gt;Config Explanation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/examples/gsm8k_example.html&#34;&gt;Run GSM8K Example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reproducible algorithm baselines:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/experiment/ppo.html&#34;&gt;PPO and GRPO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;For code explanation and advance usage (extension):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PPO Trainer and Workers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/workers/ray_trainer.html&#34;&gt;PPO Ray Trainer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html&#34;&gt;PyTorch FSDP Backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/index.html&#34;&gt;Megatron-LM Backend&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Advance Usage and Extension &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/placement.html&#34;&gt;Ray API design tutorial&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/dpo_extension.html&#34;&gt;Extend to Other RL(HF) algorithms&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html&#34;&gt;Add Models with the FSDP Backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/megatron_extension.html&#34;&gt;Add Models with the Megatron-LM Backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/split_placement&#34;&gt;Deployment using Separate GPU Resources&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance Tuning Guide&lt;/h2&gt; &#xA;&lt;p&gt;The performance is essential for on-policy RL algorithm. We write a detailed performance tuning guide to allow people tune the performance. See &lt;a href=&#34;https://verl.readthedocs.io/en/latest/perf/perf_tuning.html&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;vLLM v0.7 testing version&lt;/h2&gt; &#xA;&lt;p&gt;We have released a testing version of veRL that supports vLLM&amp;gt;=0.7.0. Please refer to &lt;a href=&#34;https://github.com/volcengine/verl/docs/README_vllm0.7.md&#34;&gt;this document&lt;/a&gt; for installation guide and more information.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution Guide&lt;/h2&gt; &#xA;&lt;p&gt;Contributions from the community are welcome!&lt;/p&gt; &#xA;&lt;h3&gt;Code formatting&lt;/h3&gt; &#xA;&lt;p&gt;We use yapf (Google style) to enforce strict code formatting when reviewing PRs. To reformat you code locally, make sure you installed &lt;strong&gt;latest&lt;/strong&gt; &lt;code&gt;yapf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install yapf --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, make sure you are at top level of verl repo and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/format.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation and acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;If you find the project helpful, please cite:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.19256v2&#34;&gt;HybridFlow: A Flexible and Efficient RLHF Framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf&#34;&gt;A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@article{sheng2024hybridflow,&#xA;  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},&#xA;  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},&#xA;  year    = {2024},&#xA;  journal = {arXiv preprint arXiv: 2409.19256}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and supported by Anyscale, Bytedance, LMSys.org, Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, and University of Hong Kong.&lt;/p&gt; &#xA;&lt;h2&gt;Awesome work using verl&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.09302&#34;&gt;Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.21236&#34;&gt;Flaming-hot Initiation with Regular Execution Sampling for Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PRIME-RL/PRIME/&#34;&gt;Process Reinforcement Through Implicit Rewards&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jiayi-Pan/TinyZero&#34;&gt;TinyZero&lt;/a&gt;: a reproduction of DeepSeek R1 Zero recipe for reasoning tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZihanWang314/ragen&#34;&gt;RAGEN&lt;/a&gt;: a general-purpose reasoning agent training framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Unakar/Logic-RL&#34;&gt;Logic R1&lt;/a&gt;: a reproduced DeepSeek R1 Zero on 2K Tiny Logic Puzzle Dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/agentica-project/deepscaler&#34;&gt;deepscaler&lt;/a&gt;: iterative context scaling with GRPO&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HKUNLP/critic-rl&#34;&gt;critic-rl&lt;/a&gt;: Teaching Language Models to Critique via Reinforcement Learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are HIRING! Send us an &lt;a href=&#34;mailto:haibin.lin@bytedance.com&#34;&gt;email&lt;/a&gt; if you are interested in internship/FTE opportunities in MLSys/LLM reasoning/multimodal alignment.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Huanshere/VideoLingo</title>
    <updated>2025-02-16T01:50:17Z</updated>
    <id>tag:github.com,2025-02-16:/Huanshere/VideoLingo</id>
    <link href="https://github.com/Huanshere/VideoLingo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | Netflixçº§å­—å¹•åˆ‡å‰²ã€ç¿»è¯‘ã€å¯¹é½ã€ç”šè‡³åŠ ä¸Šé…éŸ³ï¼Œä¸€é”®å…¨è‡ªåŠ¨è§†é¢‘æ¬è¿AIå­—å¹•ç»„&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/logo.png&#34; alt=&#34;VideoLingo Logo&#34; height=&#34;140&#34;&gt; &#xA; &lt;h1&gt;Connect the World, Frame by Frame&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/12200&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/12200&#34; alt=&#34;Huanshere%2FVideoLingo | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/README.md&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt;ï½œ&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.zh.md&#34;&gt;&lt;strong&gt;ç®€ä½“ä¸­æ–‡&lt;/strong&gt;&lt;/a&gt;ï½œ&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.zh-TW.md&#34;&gt;&lt;strong&gt;ç¹é«”ä¸­æ–‡&lt;/strong&gt;&lt;/a&gt;ï½œ&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.ja.md&#34;&gt;&lt;strong&gt;æ—¥æœ¬èª&lt;/strong&gt;&lt;/a&gt;ï½œ&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.es.md&#34;&gt;&lt;strong&gt;EspaÃ±ol&lt;/strong&gt;&lt;/a&gt;ï½œ&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.ru.md&#34;&gt;&lt;strong&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/strong&gt;&lt;/a&gt;ï½œ&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.fr.md&#34;&gt;&lt;strong&gt;FranÃ§ais&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ğŸŒŸ Overview (&lt;a href=&#34;https://videolingo.io&#34;&gt;Try VL Now!&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;VideoLingo is an all-in-one video translation, localization, and dubbing tool aimed at generating Netflix-quality subtitles. It eliminates stiff machine translations and multi-line subtitles while adding high-quality dubbing, enabling global knowledge sharing across language barriers.&lt;/p&gt; &#xA;&lt;p&gt;Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ¥ YouTube video download via yt-dlp&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ™ï¸ Word-level and Low-illusion subtitle recognition with WhisperX&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“ NLP and AI-powered subtitle segmentation&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“š Custom + AI-generated terminology for coherent translation&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”„ 3-step Translate-Reflect-Adaptation for cinematic quality&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;âœ… Netflix-standard, Single-line subtitles Only&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ—£ï¸ Dubbing with GPT-SoVITS, Azure, OpenAI, and more&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸš€ One-click startup and processing in Streamlit&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸŒ Multi-language support in Streamlit UI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ“ Detailed logging with progress resumption&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Difference from similar projects: &lt;strong&gt;Single-line subtitles only, superior translation quality, seamless dubbing experience&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ¥ Demo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;Russian Translation&lt;/h3&gt; &#xA;    &lt;hr&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/25264b5b-6931-4d39-948c-5a1e4ce42fa7&#34;&gt;https://github.com/user-attachments/assets/25264b5b-6931-4d39-948c-5a1e4ce42fa7&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;GPT-SoVITS Dubbing&lt;/h3&gt; &#xA;    &lt;hr&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c&#34;&gt;https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Language Support&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Input Language Support(more to come):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ğŸ‡ºğŸ‡¸ English ğŸ¤© | ğŸ‡·ğŸ‡º Russian ğŸ˜Š | ğŸ‡«ğŸ‡· French ğŸ¤© | ğŸ‡©ğŸ‡ª German ğŸ¤© | ğŸ‡®ğŸ‡¹ Italian ğŸ¤© | ğŸ‡ªğŸ‡¸ Spanish ğŸ¤© | ğŸ‡¯ğŸ‡µ Japanese ğŸ˜ | ğŸ‡¨ğŸ‡³ Chinese* ğŸ˜Š&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;*Chinese uses a separate punctuation-enhanced whisper model, for now...&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Translation supports all languages, while dubbing language depends on the chosen TTS method.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You don&#39;t have to read the whole docs, &lt;a href=&#34;https://share.fastgpt.in/chat/share?shareId=066w11n3r9aq6879r4z0v9rh&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt; is an online AI agent to help you.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For Windows users with NVIDIA GPU, follow these steps before installation:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.76_windows.exe&#34;&gt;CUDA Toolkit 12.6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://developer.download.nvidia.com/compute/cudnn/9.3.0/local_installers/cudnn_9.3.0_windows.exe&#34;&gt;CUDNN 9.3.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Add &lt;code&gt;C:\Program Files\NVIDIA\CUDNN\v9.3\bin\12.6&lt;/code&gt; to your system PATH&lt;/li&gt; &#xA;  &lt;li&gt;Restart your computer&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; FFmpeg is required. Please install it via package managers:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Windows: &lt;code&gt;choco install ffmpeg&lt;/code&gt; (via &lt;a href=&#34;https://chocolatey.org/&#34;&gt;Chocolatey&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;macOS: &lt;code&gt;brew install ffmpeg&lt;/code&gt; (via &lt;a href=&#34;https://brew.sh/&#34;&gt;Homebrew&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;Linux: &lt;code&gt;sudo apt install ffmpeg&lt;/code&gt; (Debian/Ubuntu)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Huanshere/VideoLingo.git&#xA;cd VideoLingo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install dependencies(requires &lt;code&gt;python=3.10&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n videolingo python=3.10.0 -y&#xA;conda activate videolingo&#xA;python install.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Start the application&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;streamlit run st.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, you can use Docker (requires CUDA 12.4 and NVIDIA Driver version &amp;gt;550), see &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/docker.en-US.md&#34;&gt;Docker docs&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t videolingo .&#xA;docker run -d -p 8501:8501 --gpus all videolingo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;APIs&lt;/h2&gt; &#xA;&lt;p&gt;VideoLingo supports OpenAI-Like API format and various TTS interfaces:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLM: &lt;code&gt;claude-3-5-sonnet-20240620&lt;/code&gt;, &lt;code&gt;deepseek-chat(v3)&lt;/code&gt;, &lt;code&gt;gemini-2.0-flash-exp&lt;/code&gt;, &lt;code&gt;gpt-4o&lt;/code&gt;, ... (sorted by performance)&lt;/li&gt; &#xA; &lt;li&gt;WhisperX: Run whisperX locally or use 302.ai API&lt;/li&gt; &#xA; &lt;li&gt;TTS: &lt;code&gt;azure-tts&lt;/code&gt;, &lt;code&gt;openai-tts&lt;/code&gt;, &lt;code&gt;siliconflow-fishtts&lt;/code&gt;, &lt;strong&gt;&lt;code&gt;fish-tts&lt;/code&gt;&lt;/strong&gt;, &lt;code&gt;GPT-SoVITS&lt;/code&gt;, &lt;code&gt;edge-tts&lt;/code&gt;, &lt;code&gt;*custom-tts&lt;/code&gt;(You can modify your own TTS in custom_tts.py!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; VideoLingo works with &lt;strong&gt;&lt;a href=&#34;https://gpt302.saaslink.net/C2oHR9&#34;&gt;302.ai&lt;/a&gt;&lt;/strong&gt; - one API key for all services (LLM, WhisperX, TTS). Or run locally with Ollama and Edge-TTS for free, no API needed!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For detailed installation, API configuration, and batch mode instructions, please refer to the documentation: &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/start.en-US.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/start.zh-CN.md&#34;&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Current Limitations&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;WhisperX transcription performance may be affected by video background noise, as it uses wav2vac model for alignment. For videos with loud background music, please enable Voice Separation Enhancement. Additionally, subtitles ending with numbers or special characters may be truncated early due to wav2vac&#39;s inability to map numeric characters (e.g., &#34;1&#34;) to their spoken form (&#34;one&#34;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Using weaker models can lead to errors during intermediate processes due to strict JSON format requirements for responses. If this error occurs, please delete the &lt;code&gt;output&lt;/code&gt; folder and retry with a different LLM, otherwise repeated execution will read the previous erroneous response causing the same error.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The dubbing feature may not be 100% perfect due to differences in speech rates and intonation between languages, as well as the impact of the translation step. However, this project has implemented extensive engineering processing for speech rates to ensure the best possible dubbing results.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multilingual video transcription recognition will only retain the main language&lt;/strong&gt;. This is because whisperX uses a specialized model for a single language when forcibly aligning word-level subtitles, and will delete unrecognized languages.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cannot dub multiple characters separately&lt;/strong&gt;, as whisperX&#39;s speaker distinction capability is not sufficiently reliable.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. Special thanks to the following open source projects for their contributions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/m-bain/whisperX&#34;&gt;whisperX&lt;/a&gt;, &lt;a href=&#34;https://github.com/yt-dlp/yt-dlp&#34;&gt;yt-dlp&lt;/a&gt;, &lt;a href=&#34;https://github.com/mangiucugna/json_repair&#34;&gt;json_repair&lt;/a&gt;, &lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“¬ Contact Me&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit &lt;a href=&#34;https://github.com/Huanshere/VideoLingo/issues&#34;&gt;Issues&lt;/a&gt; or &lt;a href=&#34;https://github.com/Huanshere/VideoLingo/pulls&#34;&gt;Pull Requests&lt;/a&gt; on GitHub&lt;/li&gt; &#xA; &lt;li&gt;DM me on Twitter: &lt;a href=&#34;https://twitter.com/Huanshere&#34;&gt;@Huanshere&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Email me at: &lt;a href=&#34;mailto:team@videolingo.io&#34;&gt;team@videolingo.io&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;â­ Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Huanshere/VideoLingo&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Huanshere/VideoLingo&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt;If you find VideoLingo helpful, please give me a â­ï¸!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>RockChinQ/LangBot</title>
    <updated>2025-02-16T01:50:17Z</updated>
    <id>tag:github.com,2025-02-16:/RockChinQ/LangBot</id>
    <link href="https://github.com/RockChinQ/LangBot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ˜ä¸°å¯Œç”Ÿæ€ã€ğŸ§©æ”¯æŒæ‰©å±•ã€ğŸ¦„å¤šæ¨¡æ€ - å¤§æ¨¡å‹åŸç”Ÿå³æ—¶é€šä¿¡æœºå™¨äººå¹³å° | é€‚é… QQ / å¾®ä¿¡ï¼ˆä¼ä¸šå¾®ä¿¡ã€ä¸ªäººå¾®ä¿¡ï¼‰/ é£ä¹¦ / é’‰é’‰ / Discord / Telegram ç­‰æ¶ˆæ¯å¹³å° | æ”¯æŒ OpenAI GPTã€ChatGPTã€DeepSeekã€Difyã€Claudeã€Geminiã€Ollamaã€LM Studioã€SiliconFlowã€Qwenã€Moonshotã€ChatGLM ç­‰ LLM çš„æœºå™¨äºº / Agent | LLM-based instant messaging bots platform, supports Discord, Telegram, WeChat, Lark, DingTalk, QQ, OpenAI ChatGPT, DeepSeek&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://langbot.app&#34;&gt; &lt;img src=&#34;https://docs.langbot.app/social.png&#34; alt=&#34;LangBot&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/12901&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/12901&#34; alt=&#34;RockChinQ%2FLangBot | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://docs.langbot.app&#34;&gt;é¡¹ç›®ä¸»é¡µ&lt;/a&gt; ï½œ &lt;a href=&#34;https://docs.langbot.app/insight/intro.htmll&#34;&gt;åŠŸèƒ½ä»‹ç»&lt;/a&gt; ï½œ &lt;a href=&#34;https://docs.langbot.app/insight/guide.html&#34;&gt;éƒ¨ç½²æ–‡æ¡£&lt;/a&gt; ï½œ &lt;a href=&#34;https://docs.langbot.app/usage/faq.html&#34;&gt;å¸¸è§é—®é¢˜&lt;/a&gt; ï½œ &lt;a href=&#34;https://docs.langbot.app/plugin/plugin-intro.html&#34;&gt;æ’ä»¶ä»‹ç»&lt;/a&gt; ï½œ &lt;a href=&#34;https://github.com/RockChinQ/LangBot/issues/new?assignees=&amp;amp;labels=%E7%8B%AC%E7%AB%8B%E6%8F%92%E4%BB%B6&amp;amp;projects=&amp;amp;template=submit-plugin.yml&amp;amp;title=%5BPlugin%5D%3A+%E8%AF%B7%E6%B1%82%E7%99%BB%E8%AE%B0%E6%96%B0%E6%8F%92%E4%BB%B6&#34;&gt;æäº¤æ’ä»¶&lt;/a&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt;&#xA;   ğŸ˜é«˜ç¨³å®šã€ğŸ§©æ”¯æŒæ‰©å±•ã€ğŸ¦„å¤šæ¨¡æ€ - å¤§æ¨¡å‹åŸç”Ÿå³æ—¶é€šä¿¡æœºå™¨äººå¹³å°ğŸ¤– &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/wdNEHETs87&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1335141740050649118?logo=discord&amp;amp;labelColor=%20%235462eb&amp;amp;logoColor=%20%23f5f5f5&amp;amp;color=%20%235462eb&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://qm.qq.com/q/PF9OuQCCcM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E7%A4%BE%E5%8C%BAQQ%E7%BE%A4-1030838208-blue&#34; alt=&#34;QQ Group&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RockChinQ/LangBot/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/RockChinQ/LangBot&#34; alt=&#34;GitHub release (latest by date)&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.qchatgpt.rockchin.top%2Fapi%2Fv2%2Fview%2Frealtime%2Fcount_query%3Fminute%3D10080&amp;amp;query=%24.data.count&amp;amp;label=%E4%BD%BF%E7%94%A8%E9%87%8F%EF%BC%887%E6%97%A5%EF%BC%89&#34; alt=&#34;Dynamic JSON Badge&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12-blue.svg?sanitize=true&#34; alt=&#34;python&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RockChinQ/LangBot/master/README.md&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/RockChinQ/LangBot/master/README_EN.md&#34;&gt;English&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/RockChinQ/LangBot/master/README_JP.md&#34;&gt;æ—¥æœ¬èª&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;âœ¨ ç‰¹æ€§&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ’¬ å¤§æ¨¡å‹å¯¹è¯ã€Agentï¼šæ”¯æŒå¤šç§å¤§æ¨¡å‹ï¼Œé€‚é…ç¾¤èŠå’Œç§èŠï¼›å…·æœ‰å¤šè½®å¯¹è¯ã€å·¥å…·è°ƒç”¨ã€å¤šæ¨¡æ€èƒ½åŠ›ï¼Œå¹¶æ·±åº¦é€‚é… &lt;a href=&#34;https://dify.ai&#34;&gt;Dify&lt;/a&gt;ã€‚ç›®å‰æ”¯æŒ QQã€QQé¢‘é“ã€ä¼ä¸šå¾®ä¿¡ã€ä¸ªäººå¾®ä¿¡ã€é£ä¹¦ã€Discordã€Telegram ç­‰å¹³å°ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ğŸ› ï¸ é«˜ç¨³å®šæ€§ã€åŠŸèƒ½å®Œå¤‡ï¼šåŸç”Ÿæ”¯æŒè®¿é—®æ§åˆ¶ã€é™é€Ÿã€æ•æ„Ÿè¯è¿‡æ»¤ç­‰æœºåˆ¶ï¼›é…ç½®ç®€å•ï¼Œæ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ğŸ§© æ’ä»¶æ‰©å±•ã€æ´»è·ƒç¤¾åŒºï¼šæ”¯æŒäº‹ä»¶é©±åŠ¨ã€ç»„ä»¶æ‰©å±•ç­‰æ’ä»¶æœºåˆ¶ï¼›ä¸°å¯Œç”Ÿæ€ï¼Œç›®å‰å·²æœ‰æ•°åä¸ª&lt;a href=&#34;https://docs.langbot.app/plugin/plugin-intro.html&#34;&gt;æ’ä»¶&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ğŸ˜» [New] Web ç®¡ç†é¢æ¿ï¼šæ”¯æŒé€šè¿‡æµè§ˆå™¨ç®¡ç† LangBot å®ä¾‹ï¼Œå…·ä½“æ”¯æŒåŠŸèƒ½ï¼ŒæŸ¥çœ‹&lt;a href=&#34;https://docs.langbot.app/webui/intro.html&#34;&gt;æ–‡æ¡£&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“¦ å¼€å§‹ä½¿ç”¨&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;p&gt;åœ¨æ‚¨å¼€å§‹ä»»ä½•æ–¹å¼éƒ¨ç½²ä¹‹å‰ï¼Œè¯·åŠ¡å¿…é˜…è¯»&lt;a href=&#34;https://docs.langbot.app/insight/guide.html&#34;&gt;æ–°æ‰‹æŒ‡å¼•&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Docker Compose éƒ¨ç½²&lt;/h4&gt; &#xA;&lt;p&gt;é€‚åˆç†Ÿæ‚‰ Docker çš„ç”¨æˆ·ï¼ŒæŸ¥çœ‹æ–‡æ¡£&lt;a href=&#34;https://docs.langbot.app/deploy/langbot/docker.html&#34;&gt;Docker éƒ¨ç½²&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h4&gt;å®å¡”é¢æ¿éƒ¨ç½²&lt;/h4&gt; &#xA;&lt;p&gt;å·²ä¸Šæ¶å®å¡”é¢æ¿ï¼Œè‹¥æ‚¨å·²å®‰è£…å®å¡”é¢æ¿ï¼Œå¯ä»¥æ ¹æ®&lt;a href=&#34;https://docs.langbot.app/deploy/langbot/one-click/bt.html&#34;&gt;æ–‡æ¡£&lt;/a&gt;ä½¿ç”¨ã€‚&lt;/p&gt; &#xA;&lt;h4&gt;Zeabur äº‘éƒ¨ç½²&lt;/h4&gt; &#xA;&lt;p&gt;ç¤¾åŒºè´¡çŒ®çš„ Zeabur æ¨¡æ¿ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zeabur.com/zh-CN/templates/ZKTBDH&#34;&gt;&lt;img src=&#34;https://zeabur.com/button.svg?sanitize=true&#34; alt=&#34;Deploy on Zeabur&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Railway äº‘éƒ¨ç½²&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://railway.app/template/yRrAyL?referralCode=vogKPF&#34;&gt;&lt;img src=&#34;https://railway.com/button.svg?sanitize=true&#34; alt=&#34;Deploy on Railway&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;æ‰‹åŠ¨éƒ¨ç½²&lt;/h4&gt; &#xA;&lt;p&gt;ç›´æ¥ä½¿ç”¨å‘è¡Œç‰ˆè¿è¡Œï¼ŒæŸ¥çœ‹æ–‡æ¡£&lt;a href=&#34;https://docs.langbot.app/deploy/langbot/manual.html&#34;&gt;æ‰‹åŠ¨éƒ¨ç½²&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“¸ æ•ˆæœå±•ç¤º&lt;/h2&gt; &#xA;&lt;img alt=&#34;å›å¤æ•ˆæœï¼ˆå¸¦æœ‰è”ç½‘æ’ä»¶ï¼‰&#34; src=&#34;https://docs.langbot.app/QChatGPT-0516.png&#34; width=&#34;500px&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;WebUI Demo: &lt;a href=&#34;https://demo.langbot.dev/&#34;&gt;https://demo.langbot.dev/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ç™»å½•ä¿¡æ¯ï¼šé‚®ç®±ï¼š&lt;code&gt;demo@langbot.app&lt;/code&gt; å¯†ç ï¼š&lt;code&gt;langbot123456&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;æ³¨æ„ï¼šä»…å±•ç¤ºwebuiæ•ˆæœï¼Œå…¬å¼€ç¯å¢ƒï¼Œè¯·ä¸è¦åœ¨å…¶ä¸­å¡«å…¥æ‚¨çš„ä»»ä½•æ•æ„Ÿä¿¡æ¯ã€‚&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ”Œ ç»„ä»¶å…¼å®¹æ€§&lt;/h2&gt; &#xA;&lt;h3&gt;æ¶ˆæ¯å¹³å°&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;å¹³å°&lt;/th&gt; &#xA;   &lt;th&gt;çŠ¶æ€&lt;/th&gt; &#xA;   &lt;th&gt;å¤‡æ³¨&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QQ ä¸ªäººå·&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;QQ ä¸ªäººå·ç§èŠã€ç¾¤èŠ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QQ å®˜æ–¹æœºå™¨äºº&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;QQ å®˜æ–¹æœºå™¨äººï¼Œæ”¯æŒé¢‘é“ã€ç§èŠã€ç¾¤èŠ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä¼ä¸šå¾®ä¿¡&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä¸ªäººå¾®ä¿¡&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;ä½¿ç”¨ &lt;a href=&#34;https://github.com/Devo919/Gewechat&#34;&gt;Gewechat&lt;/a&gt; æ¥å…¥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¾®ä¿¡å…¬ä¼—å·&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é£ä¹¦&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é’‰é’‰&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Discord&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Telegram&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WhatsApp&lt;/td&gt; &#xA;   &lt;td&gt;ğŸš§&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;ğŸš§: æ­£åœ¨å¼€å‘ä¸­&lt;/p&gt; &#xA;&lt;h3&gt;å¤§æ¨¡å‹&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ¨¡å‹&lt;/th&gt; &#xA;   &lt;th&gt;çŠ¶æ€&lt;/th&gt; &#xA;   &lt;th&gt;å¤‡æ³¨&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://platform.openai.com/&#34;&gt;OpenAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;å¯æ¥å…¥ä»»ä½• OpenAI æ¥å£æ ¼å¼æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepseek.com/&#34;&gt;DeepSeek&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.moonshot.cn/&#34;&gt;Moonshot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.anthropic.com/&#34;&gt;Anthropic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://x.ai/&#34;&gt;xAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://open.bigmodel.cn/&#34;&gt;æ™ºè°±AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dify.ai&#34;&gt;Dify&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;LLMOps å¹³å°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;æœ¬åœ°å¤§æ¨¡å‹è¿è¡Œå¹³å°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LMStudio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;æœ¬åœ°å¤§æ¨¡å‹è¿è¡Œå¹³å°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.gitee.com/&#34;&gt;GiteeAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;å¤§æ¨¡å‹æ¥å£èšåˆå¹³å°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://siliconflow.cn/&#34;&gt;SiliconFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;å¤§æ¨¡å‹èšåˆå¹³å°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bailian.console.aliyun.com/&#34;&gt;é˜¿é‡Œäº‘ç™¾ç‚¼&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;å¤§æ¨¡å‹èšåˆå¹³å°, LLMOps å¹³å°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ğŸ˜˜ ç¤¾åŒºè´¡çŒ®&lt;/h2&gt; &#xA;&lt;p&gt;LangBot ç¦»ä¸å¼€ä»¥ä¸‹è´¡çŒ®è€…å’Œç¤¾åŒºå†…æ‰€æœ‰äººçš„è´¡çŒ®ï¼Œæˆ‘ä»¬æ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®å’Œåé¦ˆã€‚&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/RockChinQ/LangBot/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=RockChinQ/LangBot&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>