<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-30T01:49:51Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>babysor/MockingBird</title>
    <updated>2022-10-30T01:49:51Z</updated>
    <id>tag:github.com,2022-10-30:/babysor/MockingBird</id>
    <link href="https://github.com/babysor/MockingBird" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üöÄAIÊãüÂ£∞: 5ÁßíÂÜÖÂÖãÈöÜÊÇ®ÁöÑÂ£∞Èü≥Âπ∂ÁîüÊàê‰ªªÊÑèËØ≠Èü≥ÂÜÖÂÆπ Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg&#34; alt=&#34;mockingbird&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://choosealicense.com/licenses/mit/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?style=flat&#34; alt=&#34;MIT License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/babysor/MockingBird/main/README-CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;üåç &lt;strong&gt;Chinese&lt;/strong&gt; supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.&lt;/p&gt; &#xA;&lt;p&gt;ü§© &lt;strong&gt;PyTorch&lt;/strong&gt; worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060&lt;/p&gt; &#xA;&lt;p&gt;üåç &lt;strong&gt;Windows + Linux&lt;/strong&gt; run in both Windows OS and linux OS (even in M1 MACOS)&lt;/p&gt; &#xA;&lt;p&gt;ü§© &lt;strong&gt;Easy &amp;amp; Awesome&lt;/strong&gt; effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder&lt;/p&gt; &#xA;&lt;p&gt;üåç &lt;strong&gt;Webserver Ready&lt;/strong&gt; to serve your result with remote calling&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV17Q4y1B7mY/&#34;&gt;DEMO VIDEO&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Ongoing Works(Helps Needed)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Major upgrade on GUI/Client and unifying web and toolbox [X] Init framework &lt;code&gt;./mkgui&lt;/code&gt; and &lt;a href=&#34;https://vaj2fgg8yn.feishu.cn/docs/doccnvotLWylBub8VJIjKzoEaee&#34;&gt;tech design&lt;/a&gt; [X] Add demo part of Voice Cloning and Conversion [X] Add preprocessing and training for Voice Conversion [ ] Add preprocessing and training for Encoder/Synthesizer/Vocoder&lt;/li&gt; &#xA; &lt;li&gt;Major upgrade on model backend based on ESPnet2(not yet started)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install Requirements&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Follow the original repo to test if you got all environment ready. **Python 3.7 or higher ** is needed to run the toolbox.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you get an &lt;code&gt;ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 )&lt;/code&gt; This error is probably due to a low version of python, try using 3.9 and it will install successfully&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://ffmpeg.org/download.html#get-packages&#34;&gt;ffmpeg&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the remaining necessary packages.&lt;/li&gt; &#xA; &lt;li&gt;Install webrtcvad &lt;code&gt;pip install webrtcvad-wheels&lt;/code&gt;(If you need)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note that we are using the pretrained encoder/vocoder but synthesizer since the original model is incompatible with the Chinese symbols. It means the demo_cli is not working at this moment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2. Prepare your models&lt;/h3&gt; &#xA;&lt;p&gt;You can either train your models or use existing ones:&lt;/p&gt; &#xA;&lt;h4&gt;2.1 Train encoder with your dataset (Optional)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Preprocess with the audios and the mel spectrograms: &lt;code&gt;python encoder_preprocess.py &amp;lt;datasets_root&amp;gt;&lt;/code&gt; Allowing parameter &lt;code&gt;--dataset {dataset}&lt;/code&gt; to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the encoder: &lt;code&gt;python encoder_train.py my_run &amp;lt;datasets_root&amp;gt;/SV2TTS/encoder&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For training, the encoder uses visdom. You can disable it with &lt;code&gt;--no_visdom&lt;/code&gt;, but it&#39;s nice to have. Run &#34;visdom&#34; in a separate CLI/process to start your visdom server.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2.2 Train synthesizer with your dataset&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download dataset and unzip: make sure you can access all .wav in folder&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Preprocess with the audios and the mel spectrograms: &lt;code&gt;python pre.py &amp;lt;datasets_root&amp;gt;&lt;/code&gt; Allowing parameter &lt;code&gt;--dataset {dataset}&lt;/code&gt; to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the synthesizer: &lt;code&gt;python synthesizer_train.py mandarin &amp;lt;datasets_root&amp;gt;/SV2TTS/synthesizer&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to next step when you see attention line show and loss meet your need in training folder &lt;em&gt;synthesizer/saved_models/&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2.3 Use pretrained model of synthesizer&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Thanks to the community, some models will be shared:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;author&lt;/th&gt; &#xA;   &lt;th&gt;Download link&lt;/th&gt; &#xA;   &lt;th&gt;Preview Video&lt;/th&gt; &#xA;   &lt;th&gt;Info&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@author&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&#34;&gt;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&#34;&gt;Baidu&lt;/a&gt; 4j5d&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;75k steps trained by multiple datasets&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@author&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&#34;&gt;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&#34;&gt;Baidu&lt;/a&gt; codeÔºöom7f&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25k steps trained by multiple datasets, only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@FawenYo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing&#34;&gt;https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing&lt;/a&gt; &lt;a href=&#34;https://u.teknik.io/AYxWf.pt&#34;&gt;https://u.teknik.io/AYxWf.pt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3&#34;&gt;input&lt;/a&gt; &lt;a href=&#34;https://github.com/babysor/MockingBird/wiki/audio/export.wav&#34;&gt;output&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;200k steps with local accent of Taiwan, only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@miven&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ&#34;&gt;https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ&lt;/a&gt; code: 2021 &lt;a href=&#34;https://www.aliyundrive.com/s/AwPsbo8mcSP&#34;&gt;https://www.aliyundrive.com/s/AwPsbo8mcSP&lt;/a&gt; code: z2m0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1uh411B7AD/&#34;&gt;https://www.bilibili.com/video/BV1uh411B7AD/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;2.4 Train vocoder (Optional)&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;note: vocoder has little difference in effect, so you may not need to train a new one.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Preprocess the data: &lt;code&gt;python vocoder_preprocess.py &amp;lt;datasets_root&amp;gt; -m &amp;lt;synthesizer_model_path&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; replace with your dataset rootÔºå&lt;code&gt;&amp;lt;synthesizer_model_path&amp;gt;&lt;/code&gt;replace with directory of your best trained models of sythensizer, e.g. &lt;em&gt;sythensizer\saved_mode\xxx&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the wavernn vocoder: &lt;code&gt;python vocoder_train.py mandarin &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the hifigan vocoder &lt;code&gt;python vocoder_train.py mandarin &amp;lt;datasets_root&amp;gt; hifigan&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Launch&lt;/h3&gt; &#xA;&lt;h4&gt;3.1 Using the web server&lt;/h4&gt; &#xA;&lt;p&gt;You can then try to run:&lt;code&gt;python web.py&lt;/code&gt; and open it in browser, default as &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2 Using the Toolbox&lt;/h4&gt; &#xA;&lt;p&gt;You can then try the toolbox: &lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3 Using the command line&lt;/h4&gt; &#xA;&lt;p&gt;You can then try the command: &lt;code&gt;python gen_voice.py &amp;lt;text_file.txt&amp;gt; your_wav_file.wav&lt;/code&gt; you may need to install cn2an by &#34;pip install cn2an&#34; for better digital number result.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repository is forked from &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning&#34;&gt;Real-Time-Voice-Cloning&lt;/a&gt; which only support English.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;Designation&lt;/th&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Implementation source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1803.09017&#34;&gt;1803.09017&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GlobalStyleToken (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;2010.05646&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HiFi-GAN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02297&#34;&gt;2106.02297&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fre-GAN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Fre-GAN: Adversarial Frequency-consistent Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1806.04558.pdf&#34;&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1802.08435.pdf&#34;&gt;1802.08435&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.10135.pdf&#34;&gt;1703.10135&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.10467.pdf&#34;&gt;1710.10467&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GE2E (encoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;F Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;h4&gt;1.Where can I download the dataset?&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Original Source&lt;/th&gt; &#xA;   &lt;th&gt;Alternative Sources&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;aidatatang_200zh&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.openslr.org/62/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;magicdata&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.openslr.org/68/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing&#34;&gt;Google Drive (Dev set)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;aishell3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/93/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;data_aishell&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/33/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;After unzip aidatatang_200zh, you need to unzip all the files under &lt;code&gt;aidatatang_200zh\corpus\train&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2.What is&lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt;?&lt;/h4&gt; &#xA;&lt;p&gt;If the dataset path is &lt;code&gt;D:\data\aidatatang_200zh&lt;/code&gt;,then &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is&lt;code&gt;D:\data&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.Not enough VRAM&lt;/h4&gt; &#xA;&lt;p&gt;Train the synthesizerÔºöadjust the batch_size in &lt;code&gt;synthesizer/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule&#xA;                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)&#xA;                (2,  2e-4,  80_000,  12),   #&#xA;                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames&#xA;                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)&#xA;                (2,  1e-5, 640_000,  12)],  # lr = learning rate&#xA;//After&#xA;tts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule&#xA;                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)&#xA;                (2,  2e-4,  80_000,  8),   #&#xA;                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames&#xA;                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)&#xA;                (2,  1e-5, 640_000,  8)],  # lr = learning rate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train Vocoder-Preprocess the dataÔºöadjust the batch_size in &lt;code&gt;synthesizer/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;### Data Preprocessing&#xA;        max_mel_frames = 900,&#xA;        rescale = True,&#xA;        rescaling_max = 0.9,&#xA;        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.&#xA;//After&#xA;### Data Preprocessing&#xA;        max_mel_frames = 900,&#xA;        rescale = True,&#xA;        rescaling_max = 0.9,&#xA;        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train Vocoder-Train the vocoderÔºöadjust the batch_size in &lt;code&gt;vocoder/wavernn/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;# Training&#xA;voc_batch_size = 100&#xA;voc_lr = 1e-4&#xA;voc_gen_at_checkpoint = 5&#xA;voc_pad = 2&#xA;&#xA;//After&#xA;# Training&#xA;voc_batch_size = 6&#xA;voc_lr = 1e-4&#xA;voc_gen_at_checkpoint = 5&#xA;voc_pad =2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4.If it happens &lt;code&gt;RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to issue &lt;a href=&#34;https://github.com/babysor/MockingBird/issues/37&#34;&gt;#37&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5. How to improve CPU and GPU occupancy rate?&lt;/h4&gt; &#xA;&lt;p&gt;Adjust the batch_size as appropriate to improve&lt;/p&gt; &#xA;&lt;h4&gt;6. What if it happens &lt;code&gt;the page file is too small to complete the operation&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to this &lt;a href=&#34;https://www.youtube.com/watch?v=Oh6dga-Oy10&amp;amp;ab_channel=CodeProf&#34;&gt;video&lt;/a&gt; and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.&lt;/p&gt; &#xA;&lt;h4&gt;7. When should I stop during training?&lt;/h4&gt; &#xA;&lt;p&gt;FYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps. &lt;img src=&#34;https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png&#34; alt=&#34;attention_step_20500_sample_1&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png&#34; alt=&#34;step-135500-mel-spectrogram_sample_1&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TheLastBen/fast-stable-diffusion</title>
    <updated>2022-10-30T01:49:51Z</updated>
    <id>tag:github.com,2022-10-30:/TheLastBen/fast-stable-diffusion</id>
    <link href="https://github.com/TheLastBen/fast-stable-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;fast-stable-diffusion, +25-50% speed increase + memory efficient + DreamBooth&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;fast-stable-diffusion Colabs, +25-50% speed increase, AUTOMATIC1111 + DreamBooth&lt;/h1&gt; &#xA;&lt;p&gt;Colab adaptations AUTOMATIC1111 Webui and Dreambooth, train your model using this easy simple and fast colab, all you have to do is enter you huggingface token once, and it will cache all the files in GDrive, including the trained model and you will be able to use it directly from the colab, make sure you use high quality reference pictures for the training, enjoy !!&lt;/p&gt; &#xA;&lt;center&gt;&#xA; &lt;b&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp; &amp;nbsp;AUTOMATIC1111 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;DreamBooth &lt;p&gt;&lt;br&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_AUTOMATIC1111.ipynb&#34;&gt; &lt;img src=&#34;https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/1.jpg&#34;&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast-DreamBooth.ipynb&#34;&gt;&lt;img src=&#34;https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/4.jpg&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Other Webuis&lt;/h2&gt; &lt;p&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;HLKY &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Relaxed mode&lt;/b&gt;&lt;/p&gt;&lt;b&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_hlky.ipynb&#34;&gt;&lt;img src=&#34;https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/2.jpg&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_relaxed.ipynb&#34;&gt; &lt;img src=&#34;https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/3.jpg&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/b&gt;&lt;/b&gt;&#xA;&lt;/center&gt;</summary>
  </entry>
  <entry>
    <title>dortania/OpenCore-Legacy-Patcher</title>
    <updated>2022-10-30T01:49:51Z</updated>
    <id>tag:github.com,2022-10-30:/dortania/OpenCore-Legacy-Patcher</id>
    <link href="https://github.com/dortania/OpenCore-Legacy-Patcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Experience macOS just like before&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/dortania/OpenCore-Legacy-Patcher/main/images/OC-Patcher.png&#34; alt=&#34;OpenCore Patcher Logo&#34; width=&#34;256&#34;&gt; &#xA; &lt;h1&gt;OpenCore Legacy Patcher&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;A Python-based project revolving around &lt;a href=&#34;https://github.com/acidanthera/OpenCorePkg&#34;&gt;Acidanthera&#39;s OpenCorePkg&lt;/a&gt; and &lt;a href=&#34;https://github.com/acidanthera/Lilu&#34;&gt;Lilu&lt;/a&gt; for both running and unlocking features in macOS on supported and unsupported Macs.&lt;/p&gt; &#xA;&lt;p&gt;Our project&#39;s main goal is to breath new life to Macs no longer supported by Apple, allowing for the installation and usage of macOS Big Sur and newer on machines as old as 2007.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/dortania/OpenCore-Legacy-Patcher/total?color=white&amp;amp;style=plastic&#34; alt=&#34;GitHub all releases&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/languages/top/dortania/OpenCore-Legacy-Patcher?color=4B8BBE&amp;amp;style=plastic&#34; alt=&#34;GitHub top language&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/417165963327176704?color=7289da&amp;amp;label=discord&amp;amp;style=plastic&#34; alt=&#34;Discord&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Noteworthy features of OpenCore Legacy Patcher:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for macOS Big Sur and Monterey&lt;/li&gt; &#xA; &lt;li&gt;Native Over the Air(OTA) System Updates&lt;/li&gt; &#xA; &lt;li&gt;Supports Penryn and newer Macs&lt;/li&gt; &#xA; &lt;li&gt;Full support for WPA Wifi and Personal Hotspot on BCM943224 and newer chipsets&lt;/li&gt; &#xA; &lt;li&gt;System Integrity Protection, FileVault 2, .im4m Secure Boot and Vaulting&lt;/li&gt; &#xA; &lt;li&gt;Recovery OS, Safe Mode and Single-user Mode booting on non-native OSes&lt;/li&gt; &#xA; &lt;li&gt;Unlocks features such as Sidecar and AirPlay to Mac even on native Macs&lt;/li&gt; &#xA; &lt;li&gt;Enable enhanced SATA and NVMe power management on non-stock hardware&lt;/li&gt; &#xA; &lt;li&gt;Zero firmware patching required (ie. APFS ROM patching)&lt;/li&gt; &#xA; &lt;li&gt;Graphics acceleration for both Metal and non-Metal GPUs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Note: Only clean-installs and upgrades are supported, macOS Big Sur installs already patched with other patchers, such as &lt;a href=&#34;https://github.com/BenSova/Patched-Sur&#34;&gt;Patched Sur&lt;/a&gt; or &lt;a href=&#34;https://github.com/StarPlayrX/bigmac&#34;&gt;bigmac&lt;/a&gt;, cannot be used due to broken file integrity with APFS snapshots and SIP.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can however reinstall macOS with this patcher and retain your original data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note 2: Currently OpenCore Legacy Patcher officially supports patching to run macOS Big Sur and Monterey installs. For older OSes, OpenCore may function however support is currently not provided from Dortania.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For macOS Mojave and Catalina support, we recommend the use of &lt;a href=&#34;http://dosdude1.com&#34;&gt;dosdude1&#39;s patchers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To start using the project, please see our in-depth guide:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dortania.github.io/OpenCore-Legacy-Patcher/&#34;&gt;OpenCore Legacy Patcher Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;To get aid with the patcher, we recommend joining the &lt;a href=&#34;https://discord.gg/rqdPgH8xSN&#34;&gt;OpenCore Patcher Paradise Discord Server&lt;/a&gt;. We&#39;re actively there and is the quickest way to receive help.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please review our docs on &lt;a href=&#34;https://dortania.github.io/OpenCore-Legacy-Patcher/DEBUG.html&#34;&gt;how to debug with OpenCore&lt;/a&gt; to gather important information to help others with troubleshooting.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running from source&lt;/h2&gt; &#xA;&lt;p&gt;To run the project from source, see here: &lt;a href=&#34;https://raw.githubusercontent.com/dortania/OpenCore-Legacy-Patcher/main/SOURCE.md&#34;&gt;Build and run from source&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Acidanthera&#34;&gt;Acidanthera&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenCorePkg as well as many of the core kexts and tools&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DhinakG&#34;&gt;DhinakG&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Main co-author&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Khronokernel&#34;&gt;Khronokernel&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Main co-author&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ausdauersportler&#34;&gt;Ausdauersportler&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;iMacs Metal GPUs Upgrade Patch set and documentation&lt;/li&gt; &#xA;   &lt;li&gt;Great amounts of help debugging and code suggestions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vit9696&#34;&gt;vit9696&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Endless amount of help troubleshooting, determining fixes and writing patches&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ASentientBot&#34;&gt;ASentientBot&lt;/a&gt;, &lt;a href=&#34;https://github.com/educovas&#34;&gt;EduCovas&lt;/a&gt; and &lt;a href=&#34;https://github.com/moosethegoose2213&#34;&gt;ASentientHedgehog&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Legacy Acceleration Patch set and documentation, &lt;a href=&#34;https://github.com/moraea&#34;&gt;Moraea Organization&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cdf&#34;&gt;cdf&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Mac Pro on OpenCore Patch set and documentation&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/cdf/Innie&#34;&gt;Innie&lt;/a&gt; and &lt;a href=&#34;https://github.com/cdf/NightShiftEnabler&#34;&gt;NightShiftEnabler&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://forums.macrumors.com/members/syncretic.1173816/&#34;&gt;Syncretic&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://forums.macrumors.com/threads/mp3-1-others-sse-4-2-emulation-to-enable-amd-metal-driver.2206682/&#34;&gt;AAAMouSSE&lt;/a&gt;, &lt;a href=&#34;https://forums.macrumors.com/threads/mp3-1-others-sse-4-2-emulation-to-enable-amd-metal-driver.2206682/post-28447707&#34;&gt;telemetrap&lt;/a&gt; and &lt;a href=&#34;https://github.com/reenigneorcim/SurPlus&#34;&gt;SurPlus&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dosdude1&#34;&gt;dosdude1&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Main author of &lt;a href=&#34;https://github.com/dortania/OCLP-GUI&#34;&gt;original GUI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Development of previous patchers, laying out much of what needs to be patched&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/parrotgeek1&#34;&gt;parrotgeek1&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/dortania/OpenCore-Legacy-Patcher/raw/4a8f61a01da72b38a4b2250386cc4b497a31a839/payloads/Config/config.plist#L1222-L1281&#34;&gt;VMM Patch Set&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BarryKN&#34;&gt;BarryKN&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Development of previous patchers, laying out much of what needs to be patched&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mariobrostech&#34;&gt;mario_bros_tech&lt;/a&gt; and the rest of the Unsupported Mac Discord &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Catalyst that started OpenCore Legacy Patcher&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/arter97/&#34;&gt;arter97&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/arter97/SimpleMSR/&#34;&gt;SimpleMSR&lt;/a&gt; to disable firmware throttling in Nehalem+ MacBooks without batteries&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mrmacintosh.com&#34;&gt;Mr.Macintosh&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Endless hours helping architect and troubleshoot many portions of the project&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/flagersgit&#34;&gt;flagers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Aid with Nvidia Web Driver research and development&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MacRumors and Unsupported Mac Communities &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Endless testing, reporting issues&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Apple &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;for macOS and many of the kexts, frameworks and other binaries we reimplemented into newer OSes&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>