<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-23T01:48:19Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>langchain-ai/local-deep-researcher</title>
    <updated>2025-03-23T01:48:19Z</updated>
    <id>tag:github.com,2025-03-23:/langchain-ai/local-deep-researcher</id>
    <link href="https://github.com/langchain-ai/local-deep-researcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fully local web research and report writing assistant&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Local Deep Researcher&lt;/h1&gt; &#xA;&lt;p&gt;Local Deep Researcher is a fully local web research assistant that uses any LLM hosted by &lt;a href=&#34;https://ollama.com/search&#34;&gt;Ollama&lt;/a&gt; or &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LMStudio&lt;/a&gt;. Give it a topic and it will generate a web search query, gather web search results, summarize the results of web search, reflect on the summary to examine knowledge gaps, generate a new search query to address the gaps, and repeat for a user-defined number of cycles. It will provide the user a final markdown summary with all sources used to generate the summary.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/1c6b28f8-6b64-42ba-a491-1ab2875d50ea&#34; alt=&#34;ollama-deep-research&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Short summary video: &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/02084902-f067-4658-9683-ff312cab7944&#34; controls&gt;&lt;/video&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“º Video Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;See it in action or build it yourself? Check out these helpful video tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sGUjmyfof4Q&#34;&gt;Overview of Local Deep Researcher with R1&lt;/a&gt; - Load and test &lt;a href=&#34;https://api-docs.deepseek.com/news/news250120&#34;&gt;DeepSeek R1&lt;/a&gt; &lt;a href=&#34;https://ollama.com/library/deepseek-r1&#34;&gt;distilled models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XGuTzHoqlj8&#34;&gt;Building Local Deep Researcher from Scratch&lt;/a&gt; - Overview of how this is built.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸš€ Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/langchain-ai/local-deep-researcher.git&#xA;cd local-deep-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then edit the &lt;code&gt;.env&lt;/code&gt; file to customize the environment variables according to your needs. These environment variables control the model selection, search tools, and other configuration settings. When you run the application, these values will be automatically loaded via &lt;code&gt;python-dotenv&lt;/code&gt; (because &lt;code&gt;langgraph.json&lt;/code&gt; point to the &#34;env&#34; file).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Selecting local model with Ollama&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the Ollama app for Mac &lt;a href=&#34;https://ollama.com/download&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pull a local LLM from &lt;a href=&#34;https://ollama.com/search&#34;&gt;Ollama&lt;/a&gt;. As an &lt;a href=&#34;https://ollama.com/library/deepseek-r1:8b&#34;&gt;example&lt;/a&gt;:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama pull deepseek-r1:8b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Optionally, update the &lt;code&gt;.env&lt;/code&gt; file with the following Ollama configuration settings.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If set, these values will take precedence over the defaults set in the &lt;code&gt;Configuration&lt;/code&gt; class in &lt;code&gt;configuration.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;OLLAMA_BASE_URL=&#34;url&#34; # Ollama service endpoint, defaults to `http://localhost:11434` &#xA;OLLAMA_MODEL=model # the model to use, defaults to `llama3.2` if not set&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Selecting local model with LMStudio&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download and install LMStudio from &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In LMStudio:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Download and load your preferred model (e.g., qwen_qwq-32b)&lt;/li&gt; &#xA;   &lt;li&gt;Go to the &#34;Local Server&#34; tab&lt;/li&gt; &#xA;   &lt;li&gt;Start the server with the OpenAI-compatible API&lt;/li&gt; &#xA;   &lt;li&gt;Note the server URL (default: &lt;a href=&#34;http://localhost:1234/v1&#34;&gt;http://localhost:1234/v1&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Optionally, update the &lt;code&gt;.env&lt;/code&gt; file with the following LMStudio configuration settings.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If set, these values will take precedence over the defaults set in the &lt;code&gt;Configuration&lt;/code&gt; class in &lt;code&gt;configuration.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;LLM_PROVIDER=lmstudio&#xA;LOCAL_LLM=qwen_qwq-32b  # Use the exact model name as shown in LMStudio&#xA;LMSTUDIO_BASE_URL=http://localhost:1234/v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Selecting search tool&lt;/h3&gt; &#xA;&lt;p&gt;By default, it will use &lt;a href=&#34;https://duckduckgo.com/&#34;&gt;DuckDuckGo&lt;/a&gt; for web search, which does not require an API key. But you can also use &lt;a href=&#34;https://docs.searxng.org/&#34;&gt;SearXNG&lt;/a&gt;, &lt;a href=&#34;https://tavily.com/&#34;&gt;Tavily&lt;/a&gt; or &lt;a href=&#34;https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api&#34;&gt;Perplexity&lt;/a&gt; by adding their API keys to the environment file. Optionally, update the &lt;code&gt;.env&lt;/code&gt; file with the following search tool configuration and API keys. If set, these values will take precedence over the defaults set in the &lt;code&gt;Configuration&lt;/code&gt; class in &lt;code&gt;configuration.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;SEARCH_API=xxx # the search API to use, such as `duckduckgo` (default)&#xA;TAVILY_API_KEY=xxx # the tavily API key to use&#xA;PERPLEXITY_API_KEY=xxx # the perplexity API key to use&#xA;MAX_WEB_RESEARCH_LOOPS=xxx # the maximum number of research loop steps, defaults to `3`&#xA;FETCH_FULL_PAGE=xxx # fetch the full page content (with `duckduckgo`), defaults to `false`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running with LangGraph Studio&lt;/h3&gt; &#xA;&lt;h4&gt;Mac&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(Recommended) Create a virtual environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .venv&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Launch LangGraph server:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install uv package manager&#xA;curl -LsSf https://astral.sh/uv/install.sh | sh&#xA;uvx --refresh --from &#34;langgraph-cli[inmem]&#34; --with-editable . --python 3.11 langgraph dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(Recommended) Create a virtual environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;code&gt;Python 3.11&lt;/code&gt; (and add to PATH during installation).&lt;/li&gt; &#xA; &lt;li&gt;Restart your terminal to ensure Python is available, then create and activate a virtual environment:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;python -m venv .venv&#xA;.venv\Scripts\Activate.ps1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Launch LangGraph server:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;# Install dependencies&#xA;pip install -e .&#xA;pip install -U &#34;langgraph-cli[inmem]&#34;            &#xA;&#xA;# Start the LangGraph server&#xA;langgraph dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the LangGraph Studio UI&lt;/h3&gt; &#xA;&lt;p&gt;When you launch LangGraph server, you should see the following output and Studio will open in your browser:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Ready!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;API: &lt;a href=&#34;http://127.0.0.1:2024&#34;&gt;http://127.0.0.1:2024&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Docs: &lt;a href=&#34;http://127.0.0.1:2024/docs&#34;&gt;http://127.0.0.1:2024/docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;LangGraph Studio Web UI: &lt;a href=&#34;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&#34;&gt;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Open &lt;code&gt;LangGraph Studio Web UI&lt;/code&gt; via the URL above. In the &lt;code&gt;configuration&lt;/code&gt; tab, you can directly set various assistant configurations. Keep in mind that the priority order for configuration values is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;1. Environment variables (highest priority)&#xA;2. LangGraph UI configuration&#xA;3. Default values in the Configuration class (lowest priority)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;1621&#34; alt=&#34;Screenshot 2025-01-24 at 10 08 31 PM&#34; src=&#34;https://github.com/user-attachments/assets/7cfd0e04-28fd-4cfa-aee5-9a556d74ab21&#34;&gt; &#xA;&lt;p&gt;Give the assistant a topic for research, and you can visualize its process!&lt;/p&gt; &#xA;&lt;img width=&#34;1621&#34; alt=&#34;Screenshot 2025-01-24 at 10 08 22 PM&#34; src=&#34;https://github.com/user-attachments/assets/4de6bd89-4f3b-424c-a9cb-70ebd3d45c5f&#34;&gt; &#xA;&lt;h3&gt;Model Compatibility Note&lt;/h3&gt; &#xA;&lt;p&gt;When selecting a local LLM, set steps use structured JSON output. Some models may have difficulty with this requirement, and the assistant has fallback mechanisms to handle this. As an example, the &lt;a href=&#34;https://ollama.com/library/deepseek-llm:7b&#34;&gt;DeepSeek R1 (7B)&lt;/a&gt; and &lt;a href=&#34;https://ollama.com/library/deepseek-r1:1.5b&#34;&gt;DeepSeek R1 (1.5B)&lt;/a&gt; models have difficulty producing required JSON output, and the assistant will use a fallback mechanism to handle this.&lt;/p&gt; &#xA;&lt;h3&gt;Browser Compatibility Note&lt;/h3&gt; &#xA;&lt;p&gt;When accessing the LangGraph Studio UI:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Firefox is recommended for the best experience&lt;/li&gt; &#xA; &lt;li&gt;Safari users may encounter security warnings due to mixed content (HTTPS/HTTP)&lt;/li&gt; &#xA; &lt;li&gt;If you encounter issues, try: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Using Firefox or another browser&lt;/li&gt; &#xA;   &lt;li&gt;Disabling ad-blocking extensions&lt;/li&gt; &#xA;   &lt;li&gt;Checking browser console for specific error messages&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;Local Deep Researcher is inspired by &lt;a href=&#34;https://arxiv.org/html/2410.04343v1#:~:text=To%20tackle%20this%20issue%2C%20we,used%20to%20generate%20intermediate%20answers.&#34;&gt;IterDRAG&lt;/a&gt;. This approach will decompose a query into sub-queries, retrieve documents for each one, answer the sub-query, and then build on the answer by retrieving docs for the second sub-query. Here, we do similar:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Given a user-provided topic, use a local LLM (via &lt;a href=&#34;https://ollama.com/search&#34;&gt;Ollama&lt;/a&gt; or &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LMStudio&lt;/a&gt;) to generate a web search query&lt;/li&gt; &#xA; &lt;li&gt;Uses a search engine / tool to find relevant sources&lt;/li&gt; &#xA; &lt;li&gt;Uses LLM to summarize the findings from web search related to the user-provided research topic&lt;/li&gt; &#xA; &lt;li&gt;Then, it uses the LLM to reflect on the summary, identifying knowledge gaps&lt;/li&gt; &#xA; &lt;li&gt;It generates a new search query to address the knowledge gaps&lt;/li&gt; &#xA; &lt;li&gt;The process repeats, with the summary being iteratively updated with new information from web search&lt;/li&gt; &#xA; &lt;li&gt;Runs for a configurable number of iterations (see &lt;code&gt;configuration&lt;/code&gt; tab)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Outputs&lt;/h2&gt; &#xA;&lt;p&gt;The output of the graph is a markdown file containing the research summary, with citations to the sources used. All sources gathered during research are saved to the graph state. You can visualize them in the graph state, which is visible in LangGraph Studio:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/e8ac1c0b-9acb-4a75-8c15-4e677e92f6cb&#34; alt=&#34;Screenshot 2024-12-05 at 4 08 59 PM&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The final summary is saved to the graph state as well:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/f6d997d5-9de5-495f-8556-7d3891f6bc96&#34; alt=&#34;Screenshot 2024-12-05 at 4 10 11 PM&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Deployment Options&lt;/h2&gt; &#xA;&lt;p&gt;There are &lt;a href=&#34;https://langchain-ai.github.io/langgraph/concepts/#deployment-options&#34;&gt;various ways&lt;/a&gt; to deploy this graph. See &lt;a href=&#34;https://github.com/langchain-ai/langchain-academy/tree/main/module-6&#34;&gt;Module 6&lt;/a&gt; of LangChain Academy for a detailed walkthrough of deployment options with LangGraph.&lt;/p&gt; &#xA;&lt;h2&gt;TypeScript Implementation&lt;/h2&gt; &#xA;&lt;p&gt;A TypeScript port of this project (without Perplexity search) is available at: &lt;a href=&#34;https://github.com/PacoVK/ollama-deep-researcher-ts&#34;&gt;https://github.com/PacoVK/ollama-deep-researcher-ts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running as a Docker container&lt;/h2&gt; &#xA;&lt;p&gt;The included &lt;code&gt;Dockerfile&lt;/code&gt; only runs LangChain Studio with ollama-deep-researcher as a service, but does not include Ollama as a dependant service. You must run Ollama separately and configure the &lt;code&gt;OLLAMA_BASE_URL&lt;/code&gt; environment variable. Optionally you can also specify the Ollama model to use by providing the &lt;code&gt;OLLAMA_MODEL&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;p&gt;Clone the repo and build an image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker build -t ollama-deep-researcher .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker run --rm -it -p 2024:2024 \&#xA;  -e SEARCH_API=&#34;tavily&#34; \ &#xA;  -e TAVILY_API_KEY=&#34;tvly-***YOUR_KEY_HERE***&#34; \&#xA;  -e OLLAMA_BASE_URL=&#34;http://host.docker.internal:11434/&#34; \&#xA;  -e OLLAMA_MODEL=&#34;llama3.2&#34; \  &#xA;  ollama-deep-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: You will see log message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;2025-02-10T13:45:04.784915Z [info     ] ğŸ¨ Opening Studio in your browser... [browser_opener] api_variant=local_dev message=ğŸ¨ Opening Studio in your browser...&#xA;URL: https://smith.langchain.com/studio/?baseUrl=http://0.0.0.0:2024&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...but the browser will not launch from the container.&lt;/p&gt; &#xA;&lt;p&gt;Instead, visit this link with the correct baseUrl IP address: &lt;a href=&#34;https://smith.langchain.com/studio/thread?baseUrl=http://127.0.0.1:2024&#34;&gt;&lt;code&gt;https://smith.langchain.com/studio/thread?baseUrl=http://127.0.0.1:2024&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jiji262/douyin-downloader</title>
    <updated>2025-03-23T01:48:19Z</updated>
    <id>tag:github.com,2025-03-23:/jiji262/douyin-downloader</id>
    <link href="https://github.com/jiji262/douyin-downloader" rel="alternate"></link>
    <summary type="html">&lt;p&gt;æŠ–éŸ³æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œå»æ°´å°ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€åˆé›†ã€éŸ³ä¹(åŸå£°)ã€‚å…è´¹ï¼å…è´¹ï¼å…è´¹ï¼&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DouYin Downloader&lt;/h1&gt; &#xA;&lt;p&gt;DouYin Downloader æ˜¯ä¸€ä¸ªç”¨äºæ‰¹é‡ä¸‹è½½æŠ–éŸ³å†…å®¹çš„å·¥å…·ã€‚åŸºäºæŠ–éŸ³ API å®ç°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°æˆ– YAML é…ç½®æ–‡ä»¶æ–¹å¼è¿è¡Œï¼Œå¯æ»¡è¶³å¤§éƒ¨åˆ†æŠ–éŸ³å†…å®¹çš„ä¸‹è½½éœ€æ±‚ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;âœ¨ ç‰¹æ€§&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¤šç§å†…å®¹æ”¯æŒ&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;è§†é¢‘ã€å›¾é›†ã€éŸ³ä¹ã€ç›´æ’­ä¿¡æ¯ä¸‹è½½&lt;/li&gt; &#xA;   &lt;li&gt;æ”¯æŒä¸ªäººä¸»é¡µã€ä½œå“åˆ†äº«ã€ç›´æ’­ã€åˆé›†ã€éŸ³ä¹é›†åˆç­‰å¤šç§é“¾æ¥&lt;/li&gt; &#xA;   &lt;li&gt;æ”¯æŒå»æ°´å°ä¸‹è½½&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ‰¹é‡ä¸‹è½½èƒ½åŠ›&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½&lt;/li&gt; &#xA;   &lt;li&gt;æ”¯æŒå¤šé“¾æ¥æ‰¹é‡ä¸‹è½½&lt;/li&gt; &#xA;   &lt;li&gt;è‡ªåŠ¨è·³è¿‡å·²ä¸‹è½½å†…å®¹&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;çµæ´»é…ç½®&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶ä¸¤ç§æ–¹å¼&lt;/li&gt; &#xA;   &lt;li&gt;å¯è‡ªå®šä¹‰ä¸‹è½½è·¯å¾„ã€çº¿ç¨‹æ•°ç­‰&lt;/li&gt; &#xA;   &lt;li&gt;æ”¯æŒä¸‹è½½æ•°é‡é™åˆ¶&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¢é‡æ›´æ–°&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ”¯æŒä¸»é¡µä½œå“å¢é‡æ›´æ–°&lt;/li&gt; &#xA;   &lt;li&gt;æ”¯æŒæ•°æ®æŒä¹…åŒ–åˆ°æ•°æ®åº“&lt;/li&gt; &#xA;   &lt;li&gt;å¯æ ¹æ®æ—¶é—´èŒƒå›´è¿‡æ»¤&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; &#xA;&lt;h3&gt;å®‰è£…&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å®‰è£… Python ä¾èµ–ï¼š&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;å¤åˆ¶é…ç½®æ–‡ä»¶ï¼š&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp config.example.yml config.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;é…ç½®&lt;/h3&gt; &#xA;&lt;p&gt;ç¼–è¾‘ &lt;code&gt;config.yml&lt;/code&gt; æ–‡ä»¶ï¼Œè®¾ç½®ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä¸‹è½½é“¾æ¥&lt;/li&gt; &#xA; &lt;li&gt;ä¿å­˜è·¯å¾„&lt;/li&gt; &#xA; &lt;li&gt;Cookie ä¿¡æ¯ï¼ˆä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·è·å–ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;å…¶ä»–ä¸‹è½½é€‰é¡¹&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;è¿è¡Œ&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ–¹å¼ä¸€ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python DouYinCommand.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ–¹å¼äºŒï¼šä½¿ç”¨å‘½ä»¤è¡Œ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python DouYinCommand.py -C True -l &#34;æŠ–éŸ³åˆ†äº«é“¾æ¥&#34; -p &#34;ä¸‹è½½è·¯å¾„&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ä½¿ç”¨äº¤æµç¾¤&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/fuye.png&#34; alt=&#34;fuye&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ä½¿ç”¨æˆªå›¾&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/DouYinCommand1.png&#34; alt=&#34;DouYinCommand1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/DouYinCommand2.png&#34; alt=&#34;DouYinCommand2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/DouYinCommanddownload.jpg&#34; alt=&#34;DouYinCommand download&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/DouYinCommanddownloaddetail.jpg&#34; alt=&#34;DouYinCommand download detail&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“ æ”¯æŒçš„é“¾æ¥ç±»å‹&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä½œå“åˆ†äº«é“¾æ¥ï¼š&lt;code&gt;https://v.douyin.com/xxx/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ä¸ªäººä¸»é¡µï¼š&lt;code&gt;https://www.douyin.com/user/xxx&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;å•ä¸ªè§†é¢‘ï¼š&lt;code&gt;https://www.douyin.com/video/xxx&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;å›¾é›†ï¼š&lt;code&gt;https://www.douyin.com/note/xxx&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;åˆé›†ï¼š&lt;code&gt;https://www.douyin.com/collection/xxx&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;éŸ³ä¹åŸå£°ï¼š&lt;code&gt;https://www.douyin.com/music/xxx&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ç›´æ’­ï¼š&lt;code&gt;https://live.douyin.com/xxx&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ› ï¸ é«˜çº§ç”¨æ³•&lt;/h2&gt; &#xA;&lt;h3&gt;å‘½ä»¤è¡Œå‚æ•°&lt;/h3&gt; &#xA;&lt;p&gt;åŸºç¡€å‚æ•°ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;-C, --cmd            ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼&#xA;-l, --link          ä¸‹è½½é“¾æ¥&#xA;-p, --path          ä¿å­˜è·¯å¾„&#xA;-t, --thread        çº¿ç¨‹æ•°ï¼ˆé»˜è®¤5ï¼‰&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ä¸‹è½½é€‰é¡¹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;-m, --music         ä¸‹è½½éŸ³ä¹ï¼ˆé»˜è®¤Trueï¼‰&#xA;-c, --cover         ä¸‹è½½å°é¢ï¼ˆé»˜è®¤Trueï¼‰&#xA;-a, --avatar        ä¸‹è½½å¤´åƒï¼ˆé»˜è®¤Trueï¼‰&#xA;-j, --json          ä¿å­˜JSONæ•°æ®ï¼ˆé»˜è®¤Trueï¼‰&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;æ›´å¤šå‚æ•°è¯´æ˜è¯·ä½¿ç”¨ &lt;code&gt;-h&lt;/code&gt; æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;ç¤ºä¾‹å‘½ä»¤&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ä¸‹è½½å•ä¸ªè§†é¢‘ï¼š&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python DouYinCommand.py -C True -l &#34;https://v.douyin.com/xxx/&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;ä¸‹è½½ä¸»é¡µä½œå“ï¼š&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python DouYinCommand.py -C True -l &#34;https://v.douyin.com/xxx/&#34; -M post&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;æ‰¹é‡ä¸‹è½½ï¼š&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python DouYinCommand.py -C True -l &#34;é“¾æ¥1&#34; -l &#34;é“¾æ¥2&#34; -p &#34;./downloads&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;æ›´å¤šç¤ºä¾‹è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/docs/examples.md&#34;&gt;ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“‹ æ³¨æ„äº‹é¡¹&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨&lt;/li&gt; &#xA; &lt;li&gt;ä½¿ç”¨å‰è¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–&lt;/li&gt; &#xA; &lt;li&gt;Cookie ä¿¡æ¯éœ€è¦è‡ªè¡Œè·å–&lt;/li&gt; &#xA; &lt;li&gt;å»ºè®®é€‚å½“è°ƒæ•´çº¿ç¨‹æ•°ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸ¤ è´¡çŒ®&lt;/h2&gt; &#xA;&lt;p&gt;æ¬¢è¿æäº¤ Issue å’Œ Pull Requestã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“œ è®¸å¯è¯&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®é‡‡ç”¨ &lt;a href=&#34;https://raw.githubusercontent.com/jiji262/douyin-downloader/main/LICENSE&#34;&gt;MIT&lt;/a&gt; è®¸å¯è¯ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ™ é¸£è°¢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Johnserf-Seed/TikTokDownload&#34;&gt;TikTokDownload&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®ä½¿ç”¨äº† ChatGPT è¾…åŠ©å¼€å‘ï¼Œå¦‚æœ‰é—®é¢˜è¯·æ Issue&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“Š Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#jiji262/douyin-downloader&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=jiji262/douyin-downloader&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>comet-ml/opik</title>
    <updated>2025-03-23T01:48:19Z</updated>
    <id>tag:github.com,2025-03-23:/comet-ml/opik</id>
    <link href="https://github.com/comet-ml/opik" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34; style=&#34;border-bottom: none&#34;&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://www.comet.com/site/products/opik/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=header_img&amp;amp;utm_campaign=opik&#34;&gt;&#xA;   &lt;picture&gt; &#xA;    &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/logo-dark-mode.svg&#34;&gt; &#xA;    &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg&#34;&gt; &#xA;    &lt;img alt=&#34;Comet Opik logo&#34; src=&#34;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg?sanitize=true&#34; width=&#34;200&#34;&gt; &#xA;   &lt;/picture&gt;&lt;/a&gt; &#xA;  &lt;br&gt; Opik &#xA; &lt;/div&gt; Open source LLM evaluation framework&lt;br&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; From RAG chatbots to code assistants to complex agentic pipelines and beyond, build LLM systems that run better, faster, and cheaper with tracing, evaluations, and dashboards. &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/opik/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/opik&#34; alt=&#34;Python SDK&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/comet-ml/opik/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/comet-ml/opik&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/comet-ml/opik/actions/workflows/build_apps.yml&#34;&gt;&lt;img src=&#34;https://github.com/comet-ml/opik/actions/workflows/build_apps.yml/badge.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/opik_quickstart.ipynb&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/opik_quickstart.ipynb&#34;&gt; &#xA;  &lt;!-- &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open Quickstart In Colab&#34;/&gt; --&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.comet.com/site/products/opik/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=website_button&amp;amp;utm_campaign=opik&#34;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; â€¢ &lt;a href=&#34;https://chat.comet.com&#34;&gt;&lt;b&gt;Slack community&lt;/b&gt;&lt;/a&gt; â€¢ &lt;a href=&#34;https://x.com/Cometml&#34;&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; â€¢ &lt;a href=&#34;https://www.comet.com/docs/opik/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=docs_button&amp;amp;utm_campaign=opik&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/comet-ml/opik/main/readme-thumbnail.png&#34; alt=&#34;Opik thumbnail&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸš€ What is Opik?&lt;/h2&gt; &#xA;&lt;p&gt;Opik is an open-source platform for evaluating, testing and monitoring LLM applications. Built by &lt;a href=&#34;https://www.comet.com?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=what_is_opik_link&amp;amp;utm_campaign=opik&#34;&gt;Comet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;You can use Opik for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Development:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tracing:&lt;/strong&gt; Track all LLM calls and traces during development and production (&lt;a href=&#34;https://www.comet.com/docs/opik/quickstart/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=quickstart_link&amp;amp;utm_campaign=opik&#34;&gt;Quickstart&lt;/a&gt;, &lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/overview/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=integrations_link&amp;amp;utm_campaign=opik&#34;&gt;Integrations&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Annotations:&lt;/strong&gt; Annotate your LLM calls by logging feedback scores using the &lt;a href=&#34;https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-and-spans-using-the-sdk?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=sdk_link&amp;amp;utm_campaign=opik&#34;&gt;Python SDK&lt;/a&gt; or the &lt;a href=&#34;https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-through-the-ui?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=ui_link&amp;amp;utm_campaign=opik&#34;&gt;UI&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Playground:&lt;/strong&gt;: Try out different prompts and models in the &lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/playground/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=playground_link&amp;amp;utm_campaign=opik&#34;&gt;prompt playground&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: Automate the evaluation process of your LLM application:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Datasets and Experiments&lt;/strong&gt;: Store test cases and run experiments (&lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=datasets_link&amp;amp;utm_campaign=opik&#34;&gt;Datasets&lt;/a&gt;, &lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=eval_link&amp;amp;utm_campaign=opik&#34;&gt;Evaluate your LLM Application&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM as a judge metrics&lt;/strong&gt;: Use Opik&#39;s LLM as a judge metric for complex issues like &lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/metrics/hallucination/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=hallucination_link&amp;amp;utm_campaign=opik&#34;&gt;hallucination detection&lt;/a&gt;, &lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/metrics/moderation/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=moderation_link&amp;amp;utm_campaign=opik&#34;&gt;moderation&lt;/a&gt; and RAG evaluation (&lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/metrics/answer_relevance/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=alex_link&amp;amp;utm_campaign=opik&#34;&gt;Answer Relevance&lt;/a&gt;, &lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/metrics/context_precision/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=context_link&amp;amp;utm_campaign=opik&#34;&gt;Context Precision&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;CI/CD integration&lt;/strong&gt;: Run evaluations as part of your CI/CD pipeline using our &lt;a href=&#34;https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=pytest_link&amp;amp;utm_campaign=opik&#34;&gt;PyTest integration&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Production Monitoring&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Log all your production traces&lt;/strong&gt;: Opik has been designed to support high volumes of traces, making it easy to monitor your production applications. Even small deployments can ingest more than 40 million traces per day!&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Monitoring dashboards&lt;/strong&gt;: Review your feedback scores, trace count and tokens over time in the &lt;a href=&#34;https://www.comet.com/docs/opik/self-host/opik_dashboard/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=dashboard_link&amp;amp;utm_campaign=opik&#34;&gt;Opik Dashboard&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Online evaluation metrics&lt;/strong&gt;: Easily score all your production traces using LLM as a Judge metrics and identify any issues with your production LLM application thanks to &lt;a href=&#34;https://www.comet.com/docs/opik/production/rules/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=dashboard_link&amp;amp;utm_campaign=opik&#34;&gt;Opik&#39;s online evaluation metrics&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;br&gt; If you are looking for features that Opik doesn&#39;t have today, please raise a new &lt;a href=&#34;https://github.com/comet-ml/opik/issues/new/choose&#34;&gt;Feature request&lt;/a&gt; ğŸš€&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;ğŸ› ï¸ Installation&lt;/h2&gt; &#xA;&lt;p&gt;Opik is available as a fully open source local installation or using Comet.com as a hosted solution. The easiest way to get started with Opik is by creating a free Comet account at &lt;a href=&#34;https://www.comet.com/signup?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=install&amp;amp;utm_campaign=opik&#34;&gt;comet.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to self-host Opik, you can do so by cloning the repository and starting the platform using Docker Compose:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the Opik repository&#xA;git clone https://github.com/comet-ml/opik.git&#xA;&#xA;# Navigate to the opik/deployment/docker-compose directory&#xA;cd opik/deployment/docker-compose&#xA;&#xA;# Optionally, you can force a pull of the latest images&#xA;docker compose pull&#xA;&#xA;# Start the Opik platform&#xA;docker compose up --detach&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now visit &lt;a href=&#34;http://localhost:5173&#34;&gt;localhost:5173&lt;/a&gt; on your browser!&lt;/p&gt; &#xA;&lt;p&gt;For more information about the different deployment options, please see our deployment guides:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Installation methods&lt;/th&gt; &#xA;   &lt;th&gt;Docs link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Local instance&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/self-host/local_deployment?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=self_host_link&amp;amp;utm_campaign=opik&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Local%20Deployments-%232496ED?style=flat&amp;amp;logo=docker&amp;amp;logoColor=white&#34; alt=&#34;Local Deployment&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kubernetes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=kubernetes_link&amp;amp;utm_campaign=opik&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Kubernetes-%23326ce5.svg?&amp;amp;logo=kubernetes&amp;amp;logoColor=white&#34; alt=&#34;Kubernetes&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ğŸ Get Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started, you will need to first install the Python SDK:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install opik&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the SDK is installed, you can configure it by running the &lt;code&gt;opik configure&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;opik configure&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will allow you to configure Opik locally by setting the correct local server address or if you&#39;re using the Cloud platform by setting the API Key&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;br&gt; You can also call the &lt;code&gt;opik.configure(use_local=True)&lt;/code&gt; method from your Python code to configure the SDK to run on the local installation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You are now ready to start logging traces using the &lt;a href=&#34;https://www.comet.com/docs/opik/python-sdk-reference/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=sdk_link2&amp;amp;utm_campaign=opik&#34;&gt;Python SDK&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ“ Logging Traces&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to get started is to use one of our integrations. Opik supports:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Integration&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Documentation&lt;/th&gt; &#xA;   &lt;th&gt;Try in Colab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all OpenAI LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/openai/?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=openai_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/openai.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LiteLLM&lt;/td&gt; &#xA;   &lt;td&gt;Call any LLM model using the OpenAI format&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/litellm/?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=openai_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/litellm.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all LangChain LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/langchain/?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=langchain_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/langchain.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Haystack&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all Haystack calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/haystack/?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=haystack_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/haystack.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Anthropic&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all Anthropic LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/anthropic?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=anthropic_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/anthropic.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bedrock&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all Bedrock LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/bedrock?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=bedrock_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/bedrock.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CrewAI&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all CrewAI calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/crewai?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=crewai_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/crewai.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all DeepSeek LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/deepseek?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=deepseek_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DSPy&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all DSPy runs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/dspy?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=dspy_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/dspy.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemini&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all Gemini LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/gemini?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=gemini_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/gemini.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Groq&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all Groq LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/groq?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=groq_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/groq.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Guardrails&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all Guardrails validations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/guardrails/?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=guardrails_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/guardrails-ai.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Instructor&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all LLM calls made with Instructor&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/instructor/?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=instructor_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/instructor.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LangGraph&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all LangGraph executions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/langgraph/?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=langchain_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/langgraph.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LlamaIndex&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all LlamaIndex LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/llama_index?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=llama_index_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/llama-index.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ollama&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all Ollama LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/ollama?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=ollama_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/ollama.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Predibase&lt;/td&gt; &#xA;   &lt;td&gt;Fine-tune and serve open-source Large Language Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/predibase?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=predibase_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/predibase.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ragas&lt;/td&gt; &#xA;   &lt;td&gt;Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/ragas?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=ragas_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/ragas.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;watsonx&lt;/td&gt; &#xA;   &lt;td&gt;Log traces for all watsonx LLM calls&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.comet.com/docs/opik/tracing/integrations/watsonx?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=watsonx_link&amp;amp;utm_campaign=opik&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/watsonx.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Quickstart In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;br&gt; If the framework you are using is not listed above, feel free to &lt;a href=&#34;https://github.com/comet-ml/opik/issues&#34;&gt;open an issue&lt;/a&gt; or submit a PR with the integration.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you are not using any of the frameworks above, you can also use the &lt;code&gt;track&lt;/code&gt; function decorator to &lt;a href=&#34;https://www.comet.com/docs/opik/tracing/log_traces/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=traces_link&amp;amp;utm_campaign=opik&#34;&gt;log traces&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import opik&#xA;&#xA;opik.configure(use_local=True) # Run locally&#xA;&#xA;@opik.track&#xA;def my_llm_function(user_question: str) -&amp;gt; str:&#xA;    # Your LLM code here&#xA;&#xA;    return &#34;Hello&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;br&gt; The track decorator can be used in conjunction with any of our integrations and can also be used to track nested function calls.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;ğŸ§‘â€âš–ï¸ LLM as a Judge metrics&lt;/h3&gt; &#xA;&lt;p&gt;The Python Opik SDK includes a number of LLM as a judge metrics to help you evaluate your LLM application. Learn more about it in the &lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/metrics/overview/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=metrics_2_link&amp;amp;utm_campaign=opik&#34;&gt;metrics documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use them, simply import the relevant metric and use the &lt;code&gt;score&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from opik.evaluation.metrics import Hallucination&#xA;&#xA;metric = Hallucination()&#xA;score = metric.score(&#xA;    input=&#34;What is the capital of France?&#34;,&#xA;    output=&#34;Paris&#34;,&#xA;    context=[&#34;France is a country in Europe.&#34;]&#xA;)&#xA;print(score)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Opik also includes a number of pre-built heuristic metrics as well as the ability to create your own. Learn more about it in the &lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/metrics/overview?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=metrics_3_link&amp;amp;utm_campaign=opik&#34;&gt;metrics documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ” Evaluating your LLM Application&lt;/h3&gt; &#xA;&lt;p&gt;Opik allows you to evaluate your LLM application during development through &lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=datasets_2_link&amp;amp;utm_campaign=opik&#34;&gt;Datasets&lt;/a&gt; and &lt;a href=&#34;https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=experiments_link&amp;amp;utm_campaign=opik&#34;&gt;Experiments&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also run evaluations as part of your CI/CD pipeline using our &lt;a href=&#34;https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=pytest_2_link&amp;amp;utm_campaign=opik&#34;&gt;PyTest integration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;â­ Star Us on GitHub&lt;/h2&gt; &#xA;&lt;p&gt;If you find Opik useful, please consider giving us a star! Your support helps us grow our community and continue improving the product.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/user-attachments/assets/ffc208bb-3dc0-40d8-9a20-8513b5e4a59d&#34; alt=&#34;Opik GitHub Star History&#34; width=&#34;600&#34;&gt; &#xA;&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;There are many ways to contribute to Opik:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit &lt;a href=&#34;https://github.com/comet-ml/opik/issues&#34;&gt;bug reports&lt;/a&gt; and &lt;a href=&#34;https://github.com/comet-ml/opik/issues&#34;&gt;feature requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Review the documentation and submit &lt;a href=&#34;https://github.com/comet-ml/opik/pulls&#34;&gt;Pull Requests&lt;/a&gt; to improve it&lt;/li&gt; &#xA; &lt;li&gt;Speaking or writing about Opik and &lt;a href=&#34;https://chat.comet.com&#34;&gt;letting us know&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Upvoting &lt;a href=&#34;https://github.com/comet-ml/opik/issues?q=is%3Aissue+is%3Aopen+label%3A%22enhancement%22&#34;&gt;popular feature requests&lt;/a&gt; to show your support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about how to contribute to Opik, please see our &lt;a href=&#34;https://raw.githubusercontent.com/comet-ml/opik/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>