<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-22T02:00:28Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mikel-brostrom/yolov8_tracking</title>
    <updated>2023-01-22T02:00:28Z</updated>
    <id>tag:github.com,2023-01-22:/mikel-brostrom/yolov8_tracking</id>
    <link href="https://github.com/mikel-brostrom/yolov8_tracking" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-time multi-object tracking and segmentation using YOLOv8&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SOTA real-time multi-object tracking and segmentation&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mikel-brostrom/yolov8_tracking/master/trackers/strongsort/results/track_all_seg_1280_025conf.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/actions&#34;&gt;&lt;img src=&#34;https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/workflows/CI%20CPU%20testing/badge.svg?sanitize=true&#34; alt=&#34;CI CPU testing&#34;&gt;&lt;/a&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://doi.org/10.5281/zenodo.7452874&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.7452874.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains a highly configurable two-stage-tracker that adjusts to different deployment scenarios. It can jointly perform multiple object tracking and instance segmentation (MOTS). The detections generated by &lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;YOLOv8&lt;/a&gt;, a family of object detection architectures and models pretrained on the &lt;a href=&#34;https://arxiv.org/abs/1405.0312&#34;&gt;COCO&lt;/a&gt; dataset, are passed to the tracker of your choice. Supported ones at the moment are: &lt;a href=&#34;https://github.com/dyhBUPT/StrongSORT&#34;&gt;StrongSORT&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.13514&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KaiyangZhou/deep-person-reid&#34;&gt;OSNet&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.00953&#34;&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/noahcao/OC_SORT&#34;&gt;OCSORT&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.14360&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ifzhang/ByteTrack&#34;&gt;ByteTrack&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.06864&#34;&gt;&lt;/a&gt;. They can track any object that your Yolov8 model was trained to detect.&lt;/p&gt; &#xA;&lt;h2&gt;Why using this tracking toolbox?&lt;/h2&gt; &#xA;&lt;p&gt;Everything is designed with simplicity and flexibility in mind. We don&#39;t hyperfocus on results on a single dataset, we prioritize real-world results. If you don&#39;t get good tracking results on your custom dataset with the out-of-the-box tracker configurations, use the &lt;code&gt;evolve.py&lt;/code&gt; script for tracker hyperparameter tuning.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules https://github.com/mikel-brostrom/yolov8_tracking.git  # clone recursively&#xA;cd yolov8_tracking&#xA;pip install -r requirements.txt  # install dependencies&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tutorials&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data&#34;&gt;Yolov5 training (link to external repository)&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://kaiyangzhou.github.io/deep-person-reid/user_guide.html&#34;&gt;Deep appearance descriptor training (link to external repository)&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/ReID-multi-framework-model-export&#34;&gt;ReID model export to ONNX, OpenVINO, TensorRT and TorchScript&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/How-to-evaluate-on-custom-tracking-dataset&#34;&gt;Evaluation on custom tracking dataset&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Inference acceleration with Nebullvm&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1J6dl90-zOjNNtcwhw7Yuuxqg5oWp_YJa?usp=sharing&#34;&gt;Yolov5&lt;/a&gt;&amp;nbsp;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1APUZ1ijCiQFBR9xD0gUvFUOC8yOJIvHm?usp=sharing&#34;&gt;ReID&lt;/a&gt;&amp;nbsp;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt;&#xA; &lt;/ul&gt;&#xA;&lt;/details&gt;   &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Experiments&lt;/summary&gt; &#xA; &lt;p&gt;In inverse chronological order:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Evaluation-of-the-params-evolved-for-first-half-of-MOT17-on-the-complete-MOT17&#34;&gt;Evaluation of the params evolved for first half of MOT17 on the complete MOT17&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Segmentation-model-vs-object-detetion-model-on-MOT-metrics&#34;&gt;Segmentation model vs object detetion model on MOT metrics&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Masked-detection-crops-vs-regular-detection-crops-for-ReID-feature-extraction&#34;&gt;Effect of masking objects before feature extraction&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/conf-thres-vs-MOT-metrics&#34;&gt;conf-thres vs HOTA, MOTA and IDF1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Effect-of-KF-updates-ahead-for-tracks-with-no-associations,-on-MOT17&#34;&gt;Effect of KF updates ahead for tracks with no associations on MOT17&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Effect-of-passing-full-image-input-vs-1280-re-scaled-to-StrongSORT-on-MOT17&#34;&gt;Effect of full images vs 1280 input to StrongSORT on MOT17&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/OSNet-architecture-performances-on-MOT16&#34;&gt;Effect of different OSNet architectures on MOT16&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/StrongSORT-vs-BoTSORT-vs-OCSORT&#34;&gt;Yolov5 StrongSORT vs BoTSORT vs OCSORT&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Yolov5 &lt;a href=&#34;https://arxiv.org/abs/2206.14651&#34;&gt;BoTSORT&lt;/a&gt; branch: &lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/tree/botsort&#34;&gt;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/tree/botsort&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/MOT-17-evaluation-(private-detector)&#34;&gt;Yolov5 StrongSORT OSNet vs other trackers MOT17&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Yolov5DeepSORTwithOSNet-vs-Yolov5StrongSORTwithOSNet-ablation-study-on-MOT16&#34;&gt;StrongSORT MOT16 ablation study&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/MOT-16-evaluation&#34;&gt;Yolov5 StrongSORT OSNet vs other trackers MOT16 (deprecated)&lt;/a&gt;&amp;nbsp;&lt;/p&gt; &lt;/li&gt;&#xA; &lt;/ul&gt;&#xA;&lt;/details&gt;   &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Custom object detection architecture&lt;/summary&gt; &#xA; &lt;p&gt;The trackers provided in this repo can be used with other object detectors than Yolov5. Make sure that the output of your detector has the following format:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(x1,y1, x2, y2, obj, cls0, cls1, ..., clsn)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;pass this directly to the tracker here:&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/raw/a4bc0c38c33023fab9e5481861d9520eb81e28bc/track.py#L189&#34;&gt;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/blob/a4bc0c38c33023fab9e5481861d9520eb81e28bc/track.py#L189&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Tracking&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python track.py --yolo-weights yolov8n.pt     # bboxes only&#xA;                                 yolov8-seg.pt  # bboxes + segmentation masks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tracking methods&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python track.py --tracking-method strongsort&#xA;                                    ocsort&#xA;                                    bytetrack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tracking sources&lt;/summary&gt; &#xA; &lt;p&gt;Tracking can be run on most video formats&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python track.py --source 0  # webcam&#xA;                           img.jpg  # image&#xA;                           vid.mp4  # video&#xA;                           path/  # directory&#xA;                           path/*.jpg  # glob&#xA;                           &#39;https://youtu.be/Zgi9g1ksQHc&#39;  # YouTube&#xA;                           &#39;rtsp://example.com/media.mp4&#39;  # RTSP, RTMP, HTTP stream&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Select Yolov8 model&lt;/summary&gt; &#xA; &lt;p&gt;There is a clear trade-off between model inference speed and overall performance. In order to make it possible to fulfill your inference speed/accuracy needs you can select a Yolov5 family model for automatic download. These model can be further optimized for you needs by the &lt;a href=&#34;https://github.com/ultralytics/yolov5/raw/master/export.py&#34;&gt;export.py&lt;/a&gt; script&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;&#xA;$ python track.py --source 0 --yolo-weights yolov8n.pt --img 640&#xA;                                            yolov8s.tflite&#xA;                                            yolov8m.pt&#xA;                                            yolov8l.onnx &#xA;                                            yolov8x.pt --img 1280&#xA;                                            ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Select ReID model&lt;/summary&gt; &#xA; &lt;p&gt;Some tracking methods combine appearance description and motion in the process of tracking. For those which use appearance, you can choose a ReID model based on your needs from this &lt;a href=&#34;https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO&#34;&gt;ReID model zoo&lt;/a&gt;. These model can be further optimized for you needs by the &lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/raw/master/reid_export.py&#34;&gt;reid_export.py&lt;/a&gt; script&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;&#xA;$ python track.py --source 0 --reid-weights osnet_x0_25_market1501.pt&#xA;                                            mobilenetv2_x1_4_msmt17.engine&#xA;                                            resnet50_msmt17.onnx&#xA;                                            osnet_x1_0_msmt17.pt&#xA;                                            ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Filter tracked classes&lt;/summary&gt; &#xA; &lt;p&gt;By default the tracker tracks all MS COCO classes.&lt;/p&gt; &#xA; &lt;p&gt;If you want to track a subset of the classes that you model predicts, add their corresponding index after the classes flag,&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python track.py --source 0 --yolo-weights yolov8s.pt --classes 16 17  # COCO yolov8 model. Track cats and dogs, only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/&#34;&gt;Here&lt;/a&gt; is a list of all the possible objects that a Yolov8 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Updates with predicted-ahead bbox in StrongSORT&lt;/summary&gt; &#xA; &lt;p&gt;If your use-case contains many occlussions and the motion trajectiories are not too complex, you will most certainly benefit from updating the Kalman Filter by its own predicted state. Select the number of predictions that suits your needs here:&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/raw/b1da64717ef50e1f60df2f1d51e1ff91d3b31ed4/trackers/strong_sort/configs/strong_sort.yaml#L7&#34;&gt;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/blob/b1da64717ef50e1f60df2f1d51e1ff91d3b31ed4/trackers/strong_sort/configs/strong_sort.yaml#L7&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Save the trajectories to you video by:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python track.py --source ... --save-trajectories --save-vid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mikel-brostrom/yolov8_tracking/master/trackers/strong_sort/results/preds_example.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;MOT compliant results&lt;/summary&gt; &#xA; &lt;p&gt;Can be saved to your experiment folder &lt;code&gt;runs/track/&amp;lt;yolo_model&amp;gt;_&amp;lt;deep_sort_model&amp;gt;/&lt;/code&gt; by&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python track.py --source ... --save-txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tracker hyperparameter tuning&lt;/summary&gt; &#xA; &lt;p&gt;We use a fast and elitist multiobjective genetic algorithm for tracker hyperparameter tuning. By default the objectives are: HOTA, MOTA, IDF1. Run it by&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python evolve.py --tracking-method strongsort --benchmark MOT17 --n-trials 100  # tune strongsort for MOT17&#xA;                   --tracking-method ocsort     --benchmark &amp;lt;your-custom-dataset&amp;gt; --objective HOTA, # tune ocsort for maximizing HOTA on your custom tracking dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The set of hyperparameters leading to the best HOTA result are written to the tracker&#39;s config file.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For Yolov5 StrongSORT OSNet bugs and feature requests please visit &lt;a href=&#34;https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/issues&#34;&gt;GitHub Issues&lt;/a&gt;. For business inquiries or professional support requests please send an email to: &lt;a href=&#34;mailto:yolov5.deepsort.pytorch@gmail.com&#34;&gt;yolov5.deepsort.pytorch@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmyolo</title>
    <updated>2023-01-22T02:00:28Z</updated>
    <id>tag:github.com,2023-01-22:/open-mmlab/mmyolo</id>
    <link href="https://github.com/open-mmlab/mmyolo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab YOLO series toolbox and benchmark. Implemented RTMDet, YOLOv5, YOLOv6, YOLOv7, YOLOv8,YOLOX, PPYOLOE, etc.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;100%&#34; src=&#34;https://user-images.githubusercontent.com/27466624/213130448-1f8529fd-2247-4ac4-851c-acd0148a49b9.png&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/mmyolo&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mmyolo&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmyolo/workflows/deploy/badge.svg?sanitize=true&#34; alt=&#34;deploy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmyolo&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmyolo/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmyolo.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmyolo.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmyolo.svg?sanitize=true&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/&#34;&gt;📘Documentation&lt;/a&gt; | &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/get_started.html&#34;&gt;🛠️Installation&lt;/a&gt; | &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/model_zoo.html&#34;&gt;👀Model Zoo&lt;/a&gt; | &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/notes/changelog.html&#34;&gt;🆕Update News&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/issues/new/choose&#34;&gt;🤔Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/README_zh-CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;📄 Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#--whats-new-&#34;&gt;🥳 🚀 What&#39;s New&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-highlight-&#34;&gt;✨ Highlight&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-introduction-&#34;&gt;📖 Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#%EF%B8%8F-installation-&#34;&gt;🛠️ Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-tutorial-&#34;&gt;👨‍🏫 Tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-overview-of-benchmark-and-model-zoo-&#34;&gt;📊 Overview of Benchmark and Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-faq-&#34;&gt;❓ FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-contributing-&#34;&gt;🙌 Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-acknowledgement-&#34;&gt;🤝 Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#%EF%B8%8F-citation-&#34;&gt;🖊️ Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-license-&#34;&gt;🎫 License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#%EF%B8%8F-projects-in-openmmlab-&#34;&gt;🏗️ Projects in OpenMMLab&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🥳 🚀 What&#39;s New &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;💎 &lt;strong&gt;v0.4.0&lt;/strong&gt; was released on 18/1/2023:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Implemented &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/configs/yolov8/README.md&#34;&gt;YOLOv8&lt;/a&gt; object detection model, and supports model deployment in &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/projects/easydeploy&#34;&gt;projects/easydeploy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Added Chinese and English versions of &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/docs/en/algorithm_descriptions/yolov8_description.md&#34;&gt;Algorithm principles and implementation with YOLOv8&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For release history and update details, please refer to &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/notes/changelog.html&#34;&gt;changelog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;✨ Highlight &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We are excited to announce our latest work on real-time object recognition tasks, &lt;strong&gt;RTMDet&lt;/strong&gt;, a family of fully convolutional single-stage detectors. RTMDet not only achieves the best parameter-accuracy trade-off on object detection from tiny to extra-large model sizes but also obtains new state-of-the-art performance on instance segmentation and rotated object detection tasks. Details can be found in the &lt;a href=&#34;https://arxiv.org/abs/2212.07784&#34;&gt;technical report&lt;/a&gt;. Pre-trained models are &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/rtmdet&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/real-time-instance-segmentation-on-mscoco?p=rtmdet-an-empirical-study-of-designing-real&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rtmdet-an-empirical-study-of-designing-real/real-time-instance-segmentation-on-mscoco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/object-detection-in-aerial-images-on-dota-1?p=rtmdet-an-empirical-study-of-designing-real&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rtmdet-an-empirical-study-of-designing-real/object-detection-in-aerial-images-on-dota-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/object-detection-in-aerial-images-on-hrsc2016?p=rtmdet-an-empirical-study-of-designing-real&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rtmdet-an-empirical-study-of-designing-real/object-detection-in-aerial-images-on-hrsc2016&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;AP&lt;/th&gt; &#xA;   &lt;th&gt;FPS(TRT FP16 BS1 3090)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Object Detection&lt;/td&gt; &#xA;   &lt;td&gt;COCO&lt;/td&gt; &#xA;   &lt;td&gt;52.8&lt;/td&gt; &#xA;   &lt;td&gt;322&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Instance Segmentation&lt;/td&gt; &#xA;   &lt;td&gt;COCO&lt;/td&gt; &#xA;   &lt;td&gt;44.6&lt;/td&gt; &#xA;   &lt;td&gt;188&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rotated Object Detection&lt;/td&gt; &#xA;   &lt;td&gt;DOTA&lt;/td&gt; &#xA;   &lt;td&gt;78.9(single-scale)/81.3(multi-scale)&lt;/td&gt; &#xA;   &lt;td&gt;121&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/12907710/208044554-1e8de6b5-48d8-44e4-a7b5-75076c7ebb71.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;MMYOLO currently only implements the object detection algorithm, but it has a significant training acceleration compared to the MMDeteciton version. The training speed is 2.6 times faster than the previous version.&lt;/p&gt; &#xA;&lt;h2&gt;📖 Introduction &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO is an open source toolbox for YOLO series algorithms based on PyTorch and &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;. It is a part of the &lt;a href=&#34;https://openmmlab.com/&#34;&gt;OpenMMLab&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;The master branch works with &lt;strong&gt;PyTorch 1.6+&lt;/strong&gt;. &lt;img src=&#34;https://user-images.githubusercontent.com/45811724/190993591-bd3f1f11-1c30-4b93-b5f4-05c9ff64ff7f.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Major features&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;🕹️ &lt;strong&gt;Unified and convenient benchmark&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMYOLO unifies the implementation of modules in various YOLO algorithms and provides a unified benchmark. Users can compare and analyze in a fair and convenient way.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;📚 &lt;strong&gt;Rich and detailed documentation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMYOLO provides rich documentation for getting started, model deployment, advanced usages, and algorithm analysis, making it easy for users at different levels to get started and make extensions quickly.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;🧩 &lt;strong&gt;Modular Design&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMYOLO decomposes the framework into different components where users can easily customize a model by combining different modules with various training and testing strategies.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/27466624/199999337-0544a4cb-3cbd-4f3e-be26-bcd9e74db7ff.jpg&#34; alt=&#34;BaseModule-P5&#34;&gt; The figure above is contributed by RangeKing@GitHub, thank you very much! &#xA; &lt;p&gt;And the figure of P6 model is in &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/model_design.md&#34;&gt;model_design.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;🛠️ Installation &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO relies on PyTorch, MMCV, MMEngine, and MMDetection. Below are quick steps for installation. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/get_started.md&#34;&gt;Install Guide&lt;/a&gt; for more detailed instructions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n open-mmlab python=3.8 pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y&#xA;conda activate open-mmlab&#xA;pip install openmim&#xA;mim install &#34;mmengine&amp;gt;=0.3.1&#34;&#xA;mim install &#34;mmcv&amp;gt;=2.0.0rc1,&amp;lt;2.1.0&#34;&#xA;mim install &#34;mmdet&amp;gt;=3.0.0rc5,&amp;lt;3.1.0&#34;&#xA;git clone https://github.com/open-mmlab/mmyolo.git&#xA;cd mmyolo&#xA;# Install albumentations&#xA;pip install -r requirements/albu.txt&#xA;# Install MMYOLO&#xA;mim install -v -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;👨‍🏫 Tutorial &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO is based on MMDetection and adopts the same code structure and design approach. To get better use of this, please read &lt;a href=&#34;https://mmdetection.readthedocs.io/en/latest/get_started.html&#34;&gt;MMDetection Overview&lt;/a&gt; for the first understanding of MMDetection.&lt;/p&gt; &#xA;&lt;p&gt;The usage of MMYOLO is almost identical to MMDetection and all tutorials are straightforward to use, you can also learn about &lt;a href=&#34;https://mmdetection.readthedocs.io/en/3.x/&#34;&gt;MMDetection User Guide and Advanced Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For different parts from MMDetection, we have also prepared user guides and advanced guides, please read our &lt;a href=&#34;https://mmyolo.readthedocs.io/zenh_CN/latest/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;User Guides&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/user_guides/index.html#train-test&#34;&gt;Train &amp;amp; Test&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/config.md&#34;&gt;Learn about Configs with YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/user_guides/index.html#get-started-to-deployment&#34;&gt;From getting started to deployment&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/custom_dataset.md&#34;&gt;Custom Dataset&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/yolov5_tutorial.md&#34;&gt;From getting started to deployment with YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmdetection.readthedocs.io/en/latest/user_guides/index.html#useful-tools&#34;&gt;Useful Tools&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/visualization.md&#34;&gt;Visualization&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/useful_tools.md&#34;&gt;Useful Tools&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Algorithm description&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/algorithm_descriptions/index.html#essential-basics&#34;&gt;Essential Basics&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/model_design.md&#34;&gt;Model design-related instructions&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/algorithm_descriptions/index.html#algorithm-principles-and-implementation&#34;&gt;Algorithm principles and implementation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/yolov5_description.md&#34;&gt;Algorithm principles and implementation with YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/rtmdet_description.md&#34;&gt;Algorithm principles and implementation with RTMDet&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/yolov8_description.md&#34;&gt;Algorithm principles and implementation with YOLOv8&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deployment Guides&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/deploy/index.html#basic-deployment-guide&#34;&gt;Basic Deployment Guide&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/deploy/basic_deployment_guide.md&#34;&gt;Basic Deployment Guide&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/deploy/index.html#deployment-tutorial&#34;&gt;Deployment Tutorial&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/deploy/yolov5_deployment.md&#34;&gt;YOLOv5 Deployment&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Advanced Guides&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/advanced_guides/data_flow.md&#34;&gt;Data flow&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/advanced_guides/how_to.md&#34;&gt;How to&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/advanced_guides/plugins.md&#34;&gt;Plugins&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📊 Overview of Benchmark and Model Zoo &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Results and models are available in the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/model_zoo.md&#34;&gt;model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;b&gt;Supported Algorithms&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov5&#34;&gt;YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolox&#34;&gt;YOLOX&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/rtmdet&#34;&gt;RTMDet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov6&#34;&gt;YOLOv6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov7&#34;&gt;YOLOv7&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/ppyoloe&#34;&gt;PPYOLOE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov8&#34;&gt;YOLOv8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;Module Components&lt;/b&gt; &#xA; &lt;/div&gt; &#xA; &lt;table align=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;    &lt;td&gt; &lt;b&gt;Backbones&lt;/b&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;b&gt;Necks&lt;/b&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;b&gt;Loss&lt;/b&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;b&gt;Common&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr valign=&#34;top&#34;&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;YOLOv5CSPDarknet&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv8CSPDarknet&lt;/li&gt; &#xA;      &lt;li&gt;YOLOXCSPDarknet&lt;/li&gt; &#xA;      &lt;li&gt;EfficientRep&lt;/li&gt; &#xA;      &lt;li&gt;CSPNeXt&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv7Backbone&lt;/li&gt; &#xA;      &lt;li&gt;PPYOLOECSPResNet&lt;/li&gt; &#xA;      &lt;li&gt;mmdet backbone&lt;/li&gt; &#xA;      &lt;li&gt;mmcls backbone&lt;/li&gt; &#xA;      &lt;li&gt;timm&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;YOLOv5PAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv8PAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv6RepPAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOXPAFPN&lt;/li&gt; &#xA;      &lt;li&gt;CSPNeXtPAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv7PAFPN&lt;/li&gt; &#xA;      &lt;li&gt;PPYOLOECSPPAFPN&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;IoULoss&lt;/li&gt; &#xA;      &lt;li&gt;mmdet loss&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt;   &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;❓ FAQ &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/notes/faq.md&#34;&gt;FAQ&lt;/a&gt; for frequently asked questions.&lt;/p&gt; &#xA;&lt;h2&gt;🙌 Contributing &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improving MMYOLO. Ongoing projects can be found in our &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/projects&#34;&gt;GitHub Projects&lt;/a&gt;. Welcome community users to participate in these projects. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;🤝 Acknowledgement &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedback. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to re-implement existing methods and develop their own new detectors.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=open-mmlab/mmyolo&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🖊️ Citation &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{mmyolo2022,&#xA;    title={{MMYOLO: OpenMMLab YOLO} series toolbox and benchmark},&#xA;    author={MMYOLO Contributors},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmyolo}},&#xA;    year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🎫 License &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/LICENSE&#34;&gt;GPL 3.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🏗️ Projects in OpenMMLab &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/#-table-of-contents&#34;&gt;🔝&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt;: OpenMMLab foundational library for training deep learning models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmclassification&#34;&gt;MMClassification&lt;/a&gt;: OpenMMLab image classification toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmyolo&#34;&gt;MMYOLO&lt;/a&gt;: OpenMMLab YOLO series toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt;: OpenMMLab image and video editing toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;: OpenMMLab image and video generative models toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmeval&#34;&gt;MMEval&lt;/a&gt;: OpenMMLab machine learning evaluation library.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>innnky/so-vits-svc</title>
    <updated>2023-01-22T02:00:28Z</updated>
    <id>tag:github.com,2023-01-22:/innnky/so-vits-svc</id>
    <link href="https://github.com/innnky/so-vits-svc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;基于vits与softvc的歌声音色转换模型&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SoftVC VITS Singing Voice Conversion&lt;/h1&gt; &#xA;&lt;h2&gt;English docs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/Eng_docs.md&#34;&gt;英语资料&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;据不完全统计，多说话人似乎会导致&lt;strong&gt;音色泄漏加重&lt;/strong&gt;，不建议训练超过5人的模型，目前的建议是如果想炼出来更像目标音色，&lt;strong&gt;尽可能炼单说话人的&lt;/strong&gt;&lt;br&gt; 断音问题已解决，音质提升了不少&lt;br&gt; 2.0版本已经移至 sovits_2.0分支&lt;br&gt; 3.0版本使用FreeVC的代码结构，与旧版本不通用&lt;br&gt; 与&lt;a href=&#34;https://github.com/prophesier/diff-svc&#34;&gt;DiffSVC&lt;/a&gt; 相比，在训练数据质量非常高时diffsvc有着更好的表现，对于质量差一些的数据集，本仓库可能会有更好的表现，此外，本仓库推理速度上比diffsvc快很多&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;模型简介&lt;/h2&gt; &#xA;&lt;p&gt;歌声音色转换模型，通过SoftVC内容编码器提取源音频语音特征，与F0同时输入VITS替换原本的文本输入达到歌声转换的效果。同时，更换声码器为 &lt;a href=&#34;https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan&#34;&gt;NSF HiFiGAN&lt;/a&gt; 解决断音问题&lt;/p&gt; &#xA;&lt;h2&gt;注意&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;当前分支是32khz版本的分支，32khz模型推理更快，显存占用大幅减小，数据集所占硬盘空间也大幅降低，推荐训练该版本模型&lt;/li&gt; &#xA; &lt;li&gt;如果要训练48khz的模型请切换到&lt;a href=&#34;https://github.com/innnky/so-vits-svc/tree/main&#34;&gt;main分支&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;预先下载的模型文件&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;soft vc hubert：&lt;a href=&#34;https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt&#34;&gt;hubert-soft-0d54a1f4.pt&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;放在hubert目录下&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;预训练底模文件 &lt;a href=&#34;https://huggingface.co/innnky/sovits_pretrained/resolve/main/G_0.pth&#34;&gt;G_0.pth&lt;/a&gt; 与 &lt;a href=&#34;https://huggingface.co/innnky/sovits_pretrained/resolve/main/D_0.pth&#34;&gt;D_0.pth&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;放在logs/32k 目录下&lt;/li&gt; &#xA;   &lt;li&gt;预训练底模为必选项，因为据测试从零开始训练有概率不收敛，同时底模也能加快训练速度&lt;/li&gt; &#xA;   &lt;li&gt;预训练底模训练数据集包含云灏 即霜 辉宇·星AI 派蒙 绫地宁宁，覆盖男女生常见音域，可以认为是相对通用的底模&lt;/li&gt; &#xA;   &lt;li&gt;底模删除了optimizer speaker_embedding 等无关权重, 只可以用于初始化训练，无法用于推理&lt;/li&gt; &#xA;   &lt;li&gt;该底模和48khz底模通用&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 一键下载&#xA;# hubert&#xA;wget -P hubert/ https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt&#xA;# G与D预训练模型&#xA;wget -P logs/32k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/G_0.pth&#xA;wget -P logs/32k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/D_0.pth&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;colab一键数据集制作、训练脚本&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1_-gh9i-wCPNlRZw6pYF-9UufetcVrGBX?usp=sharing&#34;&gt;一键colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;数据集准备&lt;/h2&gt; &#xA;&lt;p&gt;仅需要以以下文件结构将数据集放入dataset_raw目录即可&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dataset_raw&#xA;├───speaker0&#xA;│   ├───xxx1-xxx1.wav&#xA;│   ├───...&#xA;│   └───Lxx-0xx8.wav&#xA;└───speaker1&#xA;    ├───xx2-0xxx2.wav&#xA;    ├───...&#xA;    └───xxx7-xxx007.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;数据预处理&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;重采样至 32khz&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python resample.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;自动划分训练集 验证集 测试集 以及自动生成配置文件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python preprocess_flist_config.py&#xA;# 注意&#xA;# 自动生成的配置文件中，说话人数量n_speakers会自动按照数据集中的人数而定&#xA;# 为了给之后添加说话人留下一定空间，n_speakers自动设置为 当前数据集人数乘2&#xA;# 如果想多留一些空位可以在此步骤后 自行修改生成的config.json中n_speakers数量&#xA;# 一旦模型开始训练后此项不可再更改&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;生成hubert与f0&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python preprocess_hubert_f0.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;执行完以上步骤后 dataset 目录便是预处理完成的数据，可以删除dataset_raw文件夹了&lt;/p&gt; &#xA;&lt;h2&gt;训练&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py -c configs/config.json -m 32k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;推理&lt;/h2&gt; &#xA;&lt;p&gt;使用 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/inference_main.py&#34;&gt;inference_main.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更改model_path为你自己训练的最新模型记录点&lt;/li&gt; &#xA; &lt;li&gt;将待转换的音频放在raw文件夹下&lt;/li&gt; &#xA; &lt;li&gt;clean_names 写待转换的音频名称&lt;/li&gt; &#xA; &lt;li&gt;trans 填写变调半音数量&lt;/li&gt; &#xA; &lt;li&gt;spk_list 填写合成的说话人名称&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Onnx导出&lt;/h2&gt; &#xA;&lt;p&gt;使用 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/onnx_export.py&#34;&gt;onnx_export.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;新建文件夹：checkpoints 并打开&lt;/li&gt; &#xA; &lt;li&gt;在checkpoints文件夹中新建一个文件夹作为项目文件夹，文件夹名为你的项目名称&lt;/li&gt; &#xA; &lt;li&gt;将你的模型更名为model.pth，配置文件更名为config.json，并放置到刚才创建的文件夹下&lt;/li&gt; &#xA; &lt;li&gt;将 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/onnx_export.py&#34;&gt;onnx_export.py&lt;/a&gt; 中path = &#34;NyaruTaffy&#34; 的 &#34;NyaruTaffy&#34; 修改为你的项目名称&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/onnx_export.py&#34;&gt;onnx_export.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;等待执行完毕，在你的项目文件夹下会生成一个model.onnx，即为导出的模型 &lt;h3&gt;Onnx模型支持的UI&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NaruseMioShirakana/MoeSS&#34;&gt;MoeSS&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Gradio（WebUI）&lt;/h2&gt; &#xA;&lt;p&gt;使用 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/sovits_gradio.py&#34;&gt;sovits_gradio.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;新建文件夹：checkpoints 并打开&lt;/li&gt; &#xA; &lt;li&gt;在checkpoints文件夹中新建一个文件夹作为项目文件夹，文件夹名为你的项目名称&lt;/li&gt; &#xA; &lt;li&gt;将你的模型更名为model.pth，配置文件更名为config.json，并放置到刚才创建的文件夹下&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/sovits_gradio.py&#34;&gt;sovits_gradio.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>