<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-17T01:51:43Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>danielmiessler/fabric</title>
    <updated>2024-03-17T01:51:43Z</updated>
    <id>tag:github.com,2024-03-17:/danielmiessler/fabric</id>
    <link href="https://github.com/danielmiessler/fabric" rel="alternate"></link>
    <summary type="html">&lt;p&gt;fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/images/fabric-logo-gif.gif&#34; alt=&#34;fabriclogo&#34; width=&#34;400&#34; height=&#34;400&#34;&gt; &#xA; &lt;h1&gt;&lt;code&gt;fabric&lt;/code&gt;&lt;/h1&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/mission-human_flourishing_via_AI_augmentation-purple&#34; alt=&#34;Static Badge&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://img.shields.io/github/languages/top/danielmiessler/fabric&#34; alt=&#34;GitHub top language&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/danielmiessler/fabric&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p class=&#34;align center&#34;&gt; &lt;/p&gt;&#xA; &lt;h4&gt;&lt;code&gt;fabric&lt;/code&gt; is an open-source framework for augmenting humans using AI.&lt;/h4&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#introduction-video&#34;&gt;Introduction Video&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#whatandwhy&#34;&gt;What and Why&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#philosophy&#34;&gt;Philosophy&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#structure&#34;&gt;Structure&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#examples&#34;&gt;Examples&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#meta&#34;&gt;Meta&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Navigation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#introduction-videos&#34;&gt;Introduction Videos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#what-and-why&#34;&gt;What and Why&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#philosophy&#34;&gt;Philosophy&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#breaking-problems-into-components&#34;&gt;Breaking problems into components&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#too-many-prompts&#34;&gt;Too many prompts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#our-approach-to-prompting&#34;&gt;The Fabric approach to prompting&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#setting-up-the-fabric-commands&#34;&gt;Setting up the fabric commands&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#using-the-fabric-client&#34;&gt;Using the fabric client&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#just-use-the-patterns&#34;&gt;Just use the Patterns&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#create-your-own-fabric-mill&#34;&gt;Create your own Fabric Mill&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#structure&#34;&gt;Structure&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#components&#34;&gt;Components&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#cli-native&#34;&gt;CLI-native&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#directly-calling-patterns&#34;&gt;Directly calling Patterns&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#meta&#34;&gt;Meta&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#primary-contributors&#34;&gt;Primary contributors&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; We are adding functionality to the project so often that you should update often as well. That means: &lt;code&gt;git pull; pipx upgrade fabric; fabric --update; source ~/.zshrc (or ~/.bashrc)&lt;/code&gt; in the main directory!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;March 13, 2024&lt;/strong&gt; — We just added &lt;code&gt;pipx&lt;/code&gt; install support, which makes it way easier to install Fabric, support for Claude, local models via Ollama, and a number of new Patterns. Be sure to update and check &lt;code&gt;fabric -h&lt;/code&gt; for the latest!&lt;/p&gt; &#xA;&lt;h2&gt;Introduction videos&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; These videos use the &lt;code&gt;./setup.sh&lt;/code&gt; install method, which is now replaced with the easier &lt;code&gt;pipx install .&lt;/code&gt; method. Other than that everything else is still the same.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://youtu.be/wPEyyigh10g&#34;&gt; &lt;img width=&#34;972&#34; alt=&#34;fabric_intro_video&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/1eb1b9be-0bab-4c77-8ed2-ed265e8a3435&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA; &lt;a href=&#34;http://www.youtube.com/watch?feature=player_embedded&amp;amp;v=lEXd6TXPw7E target=&#34; _blank&#34;&gt; &lt;img src=&#34;http://img.youtube.com/vi/lEXd6TXPw7E/mqdefault.jpg&#34; alt=&#34;Watch the video&#34; width=&#34;972&#34; &#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;What and why&lt;/h2&gt; &#xA;&lt;p&gt;Since the start of 2023 and GenAI we&#39;ve seen a massive number of AI applications for accomplishing tasks. It&#39;s powerful, but &lt;em&gt;it&#39;s not easy to integrate this functionality into our lives.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt;In other words, AI doesn&#39;t have a capabilities problem—it has an &lt;em&gt;integration&lt;/em&gt; problem.&lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Fabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.&lt;/p&gt; &#xA;&lt;h2&gt;Philosophy&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;AI isn&#39;t a thing; it&#39;s a &lt;em&gt;magnifier&lt;/em&gt; of a thing. And that thing is &lt;strong&gt;human creativity&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the &lt;strong&gt;human&lt;/strong&gt; problems we want to solve.&lt;/p&gt; &#xA;&lt;h3&gt;Breaking problems into components&lt;/h3&gt; &#xA;&lt;p&gt;Our approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.&lt;/p&gt; &#xA;&lt;img width=&#34;2078&#34; alt=&#34;augmented_challenges&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06&#34;&gt; &#xA;&lt;h3&gt;Too many prompts&lt;/h3&gt; &#xA;&lt;p&gt;Prompts are good for this, but the biggest challenge I faced in 2023——which still exists today—is &lt;strong&gt;the sheer number of AI prompts out there&lt;/strong&gt;. We all have prompts that are useful, but it&#39;s hard to discover new ones, know if they are good or not, &lt;em&gt;and manage different versions of the ones we like&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;One of &lt;code&gt;fabric&lt;/code&gt;&#39;s primary features is helping people collect and integrate prompts, which we call &lt;em&gt;Patterns&lt;/em&gt;, into various parts of their lives.&lt;/p&gt; &#xA;&lt;p&gt;Fabric has Patterns for all sorts of life and work activities, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extracting the most interesting parts of YouTube videos and podcasts&lt;/li&gt; &#xA; &lt;li&gt;Writing an essay in your own voice with just an idea as an input&lt;/li&gt; &#xA; &lt;li&gt;Summarizing opaque academic papers&lt;/li&gt; &#xA; &lt;li&gt;Creating perfectly matched AI art prompts for a piece of writing&lt;/li&gt; &#xA; &lt;li&gt;Rating the quality of content to see if you want to read/watch the whole thing&lt;/li&gt; &#xA; &lt;li&gt;Getting summaries of long, boring content&lt;/li&gt; &#xA; &lt;li&gt;Explaining code to you&lt;/li&gt; &#xA; &lt;li&gt;Turning bad documentation into usable documentation&lt;/li&gt; &#xA; &lt;li&gt;Creating social media posts from any content input&lt;/li&gt; &#xA; &lt;li&gt;And a million more…&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Our approach to prompting&lt;/h3&gt; &#xA;&lt;p&gt;Fabric &lt;em&gt;Patterns&lt;/em&gt; are different than most prompts you&#39;ll see.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;First, we use &lt;code&gt;Markdown&lt;/code&gt; to help ensure maximum readability and editability&lt;/strong&gt;. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. &lt;em&gt;Importantly, this also includes the AI you&#39;re sending it to!&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s an example of a Fabric Pattern.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;1461&#34; alt=&#34;pattern-example&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/b910c551-9263-405f-9735-71ca69bbab6d&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next, we are extremely clear in our instructions&lt;/strong&gt;, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;And finally, we tend to use the System section of the prompt almost exclusively&lt;/strong&gt;. In over a year of being heads-down with this stuff, we&#39;ve just seen more efficacy from doing that. If that changes, or we&#39;re shown data that says otherwise, we will adjust.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;The most feature-rich way to use Fabric is to use the &lt;code&gt;fabric&lt;/code&gt; client, which can be found under &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main/client&#34;&gt;&lt;code&gt;/client&lt;/code&gt;&lt;/a&gt; directory in this repository.&lt;/p&gt; &#xA;&lt;h3&gt;Setting up the fabric commands&lt;/h3&gt; &#xA;&lt;p&gt;Follow these steps to get all fabric related apps installed and configured.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to where you want the Fabric project to live on your system in a semi-permanent place on your computer.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Find a home for Fabric&#xA;cd /where/you/keep/code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Clone the project to your computer.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone Fabric to your computer&#xA;git clone https://github.com/danielmiessler/fabric.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Enter Fabric&#39;s main directory&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Enter the project folder (where you cloned it)&#xA;cd fabric&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Install pipx:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;macOS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install pipx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install pipx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows:&lt;/p&gt; &#xA;&lt;p&gt;Use WSL and follow the Linux instructions.&lt;/p&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Install fabric&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Run setup:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fabric --setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Restart your shell to reload everything.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now you are up and running! You can test by running the help.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Making sure the paths are set up correctly&#xA;fabric --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; If you&#39;re using the &lt;code&gt;server&lt;/code&gt; functions, &lt;code&gt;fabric-api&lt;/code&gt; and &lt;code&gt;fabric-webui&lt;/code&gt; need to be run in distinct terminal windows.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Using the &lt;code&gt;fabric&lt;/code&gt; client&lt;/h3&gt; &#xA;&lt;p&gt;Once you have it all set up, here&#39;s how to use it.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Check out the options &lt;code&gt;fabric -h&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;us the results in&#xA;                        realtime. NOTE: You will not be able to pipe the&#xA;                        output into another command.&#xA;  --list, -l            List available patterns&#xA;  --clear               Clears your persistent model choice so that you can&#xA;                        once again use the --model flag&#xA;  --update, -u          Update patterns. NOTE: This will revert the default&#xA;                        model to gpt4-turbo. please run --changeDefaultModel&#xA;                        to once again set default model&#xA;  --pattern PATTERN, -p PATTERN&#xA;                        The pattern (prompt) to use&#xA;  --setup               Set up your fabric instance&#xA;  --changeDefaultModel CHANGEDEFAULTMODEL&#xA;                        Change the default model. For a list of available&#xA;                        models, use the --listmodels flag.&#xA;  --model MODEL, -m MODEL&#xA;                        Select the model to use. NOTE: Will not work if you&#xA;                        have set a default model. please use --clear to clear&#xA;                        persistence before using this flag&#xA;  --listmodels          List all available models&#xA;  --remoteOllamaServer REMOTEOLLAMASERVER&#xA;                        The URL of the remote ollamaserver to use. ONLY USE&#xA;                        THIS if you are using a local ollama server in an non-&#xA;                        deault location or port&#xA;  --context, -c         Use Context file (context.md) to add context to your&#xA;                        pattern&#xA;age: fabric [-h] [--text TEXT] [--copy] [--agents {trip_planner,ApiKeys}]&#xA;              [--output [OUTPUT]] [--stream] [--list] [--clear] [--update]&#xA;              [--pattern PATTERN] [--setup]&#xA;              [--changeDefaultModel CHANGEDEFAULTMODEL] [--model MODEL]&#xA;              [--listmodels] [--remoteOllamaServer REMOTEOLLAMASERVER]&#xA;              [--context]&#xA;&#xA;An open source framework for augmenting humans using AI.&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --text TEXT, -t TEXT  Text to extract summary from&#xA;  --copy, -C            Copy the response to the clipboard&#xA;  --agents {trip_planner,ApiKeys}, -a {trip_planner,ApiKeys}&#xA;                        Use an AI agent to help you with a task. Acceptable&#xA;                        values are &#39;trip_planner&#39; or &#39;ApiKeys&#39;. This option&#xA;                        cannot be used with any other flag.&#xA;  --output [OUTPUT], -o [OUTPUT]&#xA;                        Save the response to a file&#xA;  --stream, -s          Use this option if you want to see&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example commands&lt;/h4&gt; &#xA;&lt;p&gt;The client, by default, runs Fabric patterns without needing a server (the Patterns were downloaded during setup). This means the client connects directly to OpenAI using the input given and the Fabric pattern used.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;summarize&lt;/code&gt; Pattern based on input from &lt;code&gt;stdin&lt;/code&gt;. In this case, the body of an article.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pbpaste | fabric --pattern summarize&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;analyze_claims&lt;/code&gt; Pattern with the &lt;code&gt;--stream&lt;/code&gt; option to get immediate and streaming results.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pbpaste | fabric --stream --pattern analyze_claims&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;new&lt;/strong&gt; All of the patterns have been added as aliases to your bash (or zsh) config file&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pbpaste | analyze_claims --stream&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; More examples coming in the next few days, including a demo video!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Just use the Patterns&lt;/h3&gt; &#xA;&lt;img width=&#34;1173&#34; alt=&#34;fabric-patterns-screenshot&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;If you&#39;re not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main/patterns&#34;&gt;&lt;code&gt;/patterns&lt;/code&gt;&lt;/a&gt; directory and start exploring!&lt;/p&gt; &#xA;&lt;p&gt;We hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.&lt;/p&gt; &#xA;&lt;p&gt;You can use any of the Patterns you see there in any AI application that you have, whether that&#39;s ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we&#39;ve published, and they will be way better than ours.&lt;/p&gt; &#xA;&lt;p&gt;The wisdom of crowds for the win.&lt;/p&gt; &#xA;&lt;h3&gt;Create your own Fabric Mill&lt;/h3&gt; &#xA;&lt;img width=&#34;2070&#34; alt=&#34;fabric_mill_architecture&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/ec3bd9b5-d285-483d-9003-7a8e6d842584&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;But we go beyond just providing Patterns. We provide code for you to build your very own Fabric server and personal AI infrastructure!&lt;/p&gt; &#xA;&lt;h2&gt;Structure&lt;/h2&gt; &#xA;&lt;p&gt;Fabric is themed off of, well… &lt;em&gt;fabric&lt;/em&gt;—as in…woven materials. So, think blankets, quilts, patterns, etc. Here&#39;s the concept and structure:&lt;/p&gt; &#xA;&lt;h3&gt;Components&lt;/h3&gt; &#xA;&lt;p&gt;The Fabric ecosystem has three primary components, all named within this textile theme.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;strong&gt;Mill&lt;/strong&gt; is the (optional) server that makes &lt;strong&gt;Patterns&lt;/strong&gt; available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Patterns&lt;/strong&gt; are the actual granular AI use cases (prompts).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stitches&lt;/strong&gt; are chained together &lt;em&gt;Patterns&lt;/em&gt; that create advanced functionality (see below).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Looms&lt;/strong&gt; are the client-side apps that call a specific &lt;strong&gt;Pattern&lt;/strong&gt; hosted by a &lt;strong&gt;Mill&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CLI-native&lt;/h3&gt; &#xA;&lt;p&gt;One of the coolest parts of the project is that it&#39;s &lt;strong&gt;command-line native&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;p&gt;Each Pattern you see in the &lt;code&gt;/patterns&lt;/code&gt; directory can be used in any AI application you use, but you can also set up your own server using the &lt;code&gt;/server&lt;/code&gt; code and then call APIs directly!&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;re set up, you can do things like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Take any idea from `stdin` and send it to the `/write_essay` API!&#xA;echo &#34;An idea that coding is like speaking with rules.&#34; | write_essay&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Directly calling Patterns&lt;/h3&gt; &#xA;&lt;p&gt;One key feature of &lt;code&gt;fabric&lt;/code&gt; and its Markdown-based format is the ability to _ directly reference_ (and edit) individual &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main#naming&#34;&gt;patterns&lt;/a&gt; directly—on their own—without surrounding code.&lt;/p&gt; &#xA;&lt;p&gt;As an example, here&#39;s how to call &lt;em&gt;the direct location&lt;/em&gt; of the &lt;code&gt;extract_wisdom&lt;/code&gt; pattern.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This means you can cleanly, and directly reference any pattern for use in a web-based AI app, your own code, or wherever!&lt;/p&gt; &#xA;&lt;p&gt;Even better, you can also have your &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main#naming&#34;&gt;Mill&lt;/a&gt; functionality directly call &lt;em&gt;system&lt;/em&gt; and &lt;em&gt;user&lt;/em&gt; prompts from &lt;code&gt;fabric&lt;/code&gt;, meaning you can have your personal AI ecosystem automatically kept up to date with the latest version of your favorite &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main#naming&#34;&gt;Patterns&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s what that looks like in code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://github.com/danielmiessler/fabric/blob/main/server/fabric_api_server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# /extwis&#xA;@app.route(&#34;/extwis&#34;, methods=[&#34;POST&#34;])&#xA;@auth_required  # Require authentication&#xA;def extwis():&#xA;    data = request.get_json()&#xA;&#xA;    # Warn if there&#39;s no input&#xA;    if &#34;input&#34; not in data:&#xA;        return jsonify({&#34;error&#34;: &#34;Missing input parameter&#34;}), 400&#xA;&#xA;    # Get data from client&#xA;    input_data = data[&#34;input&#34;]&#xA;&#xA;    # Set the system and user URLs&#xA;    system_url = &#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/patterns/extract_wisdom/system.md&#34;&#xA;    user_url = &#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/patterns/extract_wisdom/user.md&#34;&#xA;&#xA;    # Fetch the prompt content&#xA;    system_content = fetch_content_from_url(system_url)&#xA;    user_file_content = fetch_content_from_url(user_url)&#xA;&#xA;    # Build the API call&#xA;    system_message = {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: system_content}&#xA;    user_message = {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_file_content + &#34;\n&#34; + input_data}&#xA;    messages = [system_message, user_message]&#xA;    try:&#xA;        response = openai.chat.completions.create(&#xA;            model=&#34;gpt-4-1106-preview&#34;,&#xA;            messages=messages,&#xA;            temperature=0.0,&#xA;            top_p=1,&#xA;            frequency_penalty=0.1,&#xA;            presence_penalty=0.1,&#xA;        )&#xA;        assistant_message = response.choices[0].message.content&#xA;        return jsonify({&#34;response&#34;: assistant_message})&#xA;    except Exception as e:&#xA;        return jsonify({&#34;error&#34;: str(e)}), 500&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s an abridged output example from the &lt;a href=&#34;https://github.com/danielmiessler/fabric/raw/main/patterns/extract_wisdom/system.md&#34;&gt;&lt;code&gt;extract_wisdom&lt;/code&gt;&lt;/a&gt; pattern (limited to only 10 items per section).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Paste in the transcript of a YouTube video of Riva Tez on David Perrel&#39;s podcast&#xA;pbpaste | extract_wisdom&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;## SUMMARY:&#xA;&#xA;The content features a conversation between two individuals discussing various topics, including the decline of Western culture, the importance of beauty and subtlety in life, the impact of technology and AI, the resonance of Rilke&#39;s poetry, the value of deep reading and revisiting texts, the captivating nature of Ayn Rand&#39;s writing, the role of philosophy in understanding the world, and the influence of drugs on society. They also touch upon creativity, attention spans, and the importance of introspection.&#xA;&#xA;## IDEAS:&#xA;&#xA;1. Western culture is perceived to be declining due to a loss of values and an embrace of mediocrity.&#xA;2. Mass media and technology have contributed to shorter attention spans and a need for constant stimulation.&#xA;3. Rilke&#39;s poetry resonates due to its focus on beauty and ecstasy in everyday objects.&#xA;4. Subtlety is often overlooked in modern society due to sensory overload.&#xA;5. The role of technology in shaping music and performance art is significant.&#xA;6. Reading habits have shifted from deep, repetitive reading to consuming large quantities of new material.&#xA;7. Revisiting influential books as one ages can lead to new insights based on accumulated wisdom and experiences.&#xA;8. Fiction can vividly illustrate philosophical concepts through characters and narratives.&#xA;9. Many influential thinkers have backgrounds in philosophy, highlighting its importance in shaping reasoning skills.&#xA;10. Philosophy is seen as a bridge between theology and science, asking questions that both fields seek to answer.&#xA;&#xA;## QUOTES:&#xA;&#xA;1. &#34;You can&#39;t necessarily think yourself into the answers. You have to create space for the answers to come to you.&#34;&#xA;2. &#34;The West is dying and we are killing her.&#34;&#xA;3. &#34;The American Dream has been replaced by mass packaged mediocrity porn, encouraging us to revel like happy pigs in our own meekness.&#34;&#xA;4. &#34;There&#39;s just not that many people who have the courage to reach beyond consensus and go explore new ideas.&#34;&#xA;5. &#34;I&#39;ll start watching Netflix when I&#39;ve read the whole of human history.&#34;&#xA;6. &#34;Rilke saw beauty in everything... He sees it&#39;s in one little thing, a representation of all things that are beautiful.&#34;&#xA;7. &#34;Vanilla is a very subtle flavor... it speaks to sort of the sensory overload of the modern age.&#34;&#xA;8. &#34;When you memorize chapters [of the Bible], it takes a few months, but you really understand how things are structured.&#34;&#xA;9. &#34;As you get older, if there&#39;s books that moved you when you were younger, it&#39;s worth going back and rereading them.&#34;&#xA;10. &#34;She [Ayn Rand] took complicated philosophy and embodied it in a way that anybody could resonate with.&#34;&#xA;&#xA;## HABITS:&#xA;&#xA;1. Avoiding mainstream media consumption for deeper engagement with historical texts and personal research.&#xA;2. Regularly revisiting influential books from youth to gain new insights with age.&#xA;3. Engaging in deep reading practices rather than skimming or speed-reading material.&#xA;4. Memorizing entire chapters or passages from significant texts for better understanding.&#xA;5. Disengaging from social media and fast-paced news cycles for more focused thought processes.&#xA;6. Walking long distances as a form of meditation and reflection.&#xA;7. Creating space for thoughts to solidify through introspection and stillness.&#xA;8. Embracing emotions such as grief or anger fully rather than suppressing them.&#xA;9. Seeking out varied experiences across different careers and lifestyles.&#xA;10. Prioritizing curiosity-driven research without specific goals or constraints.&#xA;&#xA;## FACTS:&#xA;&#xA;1. The West is perceived as declining due to cultural shifts away from traditional values.&#xA;2. Attention spans have shortened due to technological advancements and media consumption habits.&#xA;3. Rilke&#39;s poetry emphasizes finding beauty in everyday objects through detailed observation.&#xA;4. Modern society often overlooks subtlety due to sensory overload from various stimuli.&#xA;5. Reading habits have evolved from deep engagement with texts to consuming large quantities quickly.&#xA;6. Revisiting influential books can lead to new insights based on accumulated life experiences.&#xA;7. Fiction can effectively illustrate philosophical concepts through character development and narrative arcs.&#xA;8. Philosophy plays a significant role in shaping reasoning skills and understanding complex ideas.&#xA;9. Creativity may be stifled by cultural nihilism and protectionist attitudes within society.&#xA;10. Short-term thinking undermines efforts to create lasting works of beauty or significance.&#xA;&#xA;## REFERENCES:&#xA;&#xA;1. Rainer Maria Rilke&#39;s poetry&#xA;2. Netflix&#xA;3. Underworld concert&#xA;4. Katy Perry&#39;s theatrical performances&#xA;5. Taylor Swift&#39;s performances&#xA;6. Bible study&#xA;7. Atlas Shrugged by Ayn Rand&#xA;8. Robert Pirsig&#39;s writings&#xA;9. Bertrand Russell&#39;s definition of philosophy&#xA;10. Nietzsche&#39;s walks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Meta&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; Special thanks to the following people for their inspiration and contributions!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Caleb Sima&lt;/em&gt; for pushing me over the edge of whether to make this a public project or not.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Joel Parish&lt;/em&gt; for super useful input on the project&#39;s Github directory structure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Jonathan Dunn&lt;/em&gt; for spectacular work on the soon-to-be-released universal client.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Joseph Thacker&lt;/em&gt; for the idea of a &lt;code&gt;-c&lt;/code&gt; context flag that adds pre-created context in the &lt;code&gt;./config/fabric/&lt;/code&gt; directory to all Pattern queries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Jason Haddix&lt;/em&gt; for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using &lt;code&gt;llama2&lt;/code&gt; before sending on to &lt;code&gt;gpt-4&lt;/code&gt; for analysis.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Dani Goland&lt;/em&gt; for enhancing the Fabric Server (Mill) infrastructure by migrating to FastAPI, breaking the server into discrete pieces, and Dockerizing the entire thing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Andre Guerra&lt;/em&gt; for simplifying installation by getting us onto Poetry for virtual environment and dependency management.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Primary contributors&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/danielmiessler&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/50654?v=4&#34; title=&#34;Daniel Miessler&#34; width=&#34;50&#34; height=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xssdoctor&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/9218431?v=4&#34; title=&#34;Jonathan Dunn&#34; width=&#34;50&#34; height=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sbehrens&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/688589?v=4&#34; title=&#34;Scott Behrens&#34; width=&#34;50&#34; height=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/agu3rra&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/10410523?v=4&#34; title=&#34;Andre Guerra&#34; width=&#34;50&#34; height=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;fabric&lt;/code&gt; was created by &lt;a href=&#34;https://danielmiessler.com/subscribe&#34; target=&#34;_blank&#34;&gt;Daniel Miessler&lt;/a&gt; in January of 2024. &lt;br&gt;&lt;br&gt; &lt;a href=&#34;https://twitter.com/intent/user?screen_name=danielmiessler&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/danielmiessler&#34; alt=&#34;X (formerly Twitter) Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jiaweizzhao/GaLore</title>
    <updated>2024-03-17T01:51:43Z</updated>
    <id>tag:github.com,2024-03-17:/jiaweizzhao/GaLore</id>
    <link href="https://github.com/jiaweizzhao/GaLore" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GaLore&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains the pre-release version of GaLore algorithm, proposed by &lt;a href=&#34;https://arxiv.org/abs/2403.03507&#34;&gt;GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Gradient Low-Rank Projection (GaLore) is a memory-efficient low-rank training strategy that allows &lt;em&gt;full-parameter&lt;/em&gt; learning but is more &lt;em&gt;memory-efficient&lt;/em&gt; than common low-rank adaptation methods, such as LoRA. As a gradient projection method, GaLore is independent of the choice of optimizers and can be easily plugged into existing ones with only two lines of code, as shown in Algorithm 1 below.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/jiaweizzhao/GaLore/master/imgs/galore_code_box.png&#34; alt=&#34;Image 2&#34; style=&#34;width: 550px; margin: 0 auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;Thanks everyone for the interest in GaLore!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We are working on the offical release of GaLore.&lt;/strong&gt; In the meanwhile, please feel free to try the pre-release version and provide feedback to us. Currently, the pre-release version (e.g., GaLore optimizers) should provide a decent memory reduction and accurate simulation of GaLore algorithm.&lt;/p&gt; &#xA;&lt;p&gt;The official release of GaLore will include:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Per-layer weight updates for multi-GPU training (DDP and FSDP) (working with &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Memory-efficient low-rank gradient accumulation (working with &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Optimized &lt;code&gt;GaLoreAdamW8bit&lt;/code&gt; (working with &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We would like to express our gratitude to the community members who have been actively working on integrating GaLore into different platforms, including &lt;a href=&#34;https://github.com/huggingface/transformers/pull/29588&#34;&gt;HuggingFace&lt;/a&gt;, &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;&gt;LLaMA-Factory&lt;/a&gt;, and &lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl/pull/1370&#34;&gt;Axolotl&lt;/a&gt;. Join our Slack workspace &lt;a href=&#34;https://join.slack.com/t/galore-social/shared_invite/zt-2ev152px0-DguuQ5WRTLQjtq2C88HBvQ&#34;&gt;GaLore-Social&lt;/a&gt; to engage in discussions with us.&lt;/p&gt; &#xA;&lt;h2&gt;Discussion &lt;a href=&#34;https://join.slack.com/t/galore-social/shared_invite/zt-2ev152px0-DguuQ5WRTLQjtq2C88HBvQ&#34;&gt;(GaLore-Social)&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We welcome any discussions, questions, and feedback on GaLore. Please join our Slack workspace &lt;a href=&#34;https://join.slack.com/t/galore-social/shared_invite/zt-2ev152px0-DguuQ5WRTLQjtq2C88HBvQ&#34;&gt;GaLore-Social&lt;/a&gt; to discuss with us and the community.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install GaLore optimizer&lt;/h3&gt; &#xA;&lt;p&gt;Install from pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install galore-torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or if you want to install from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:jiaweizzhao/GaLore.git&#xA;cd GaLore&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install experiment dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r exp_requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Save optimizer memory using GaLore optimizers&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from galore_torch import GaLoreAdamW, GaLoreAdamW8bit, GaLoreAdafactor&#xA;# define param groups as galore_params and non_galore_params&#xA;param_groups = [{&#39;params&#39;: non_galore_params}, &#xA;                {&#39;params&#39;: galore_params, &#39;rank&#39;: 128, &#39;update_proj_gap&#39;: 200, &#39;scale&#39;: 0.25, &#39;proj_type&#39;: &#39;std&#39;}]&#xA;optimizer = GaLoreAdamW(param_groups, lr=0.01)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Save weight gradient memory using per-layer weight updates&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;code&gt;register_post_accumulate_grad_hook&lt;/code&gt; provided by &lt;a href=&#34;https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html&#34;&gt;PyTorch&lt;/a&gt; to enable per-layer weight updates. An example is shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# define an optimizer for each parameter p, and store them in optimizer_dict&#xA;for p in model.parameters():&#xA;    if p.requires_grad:&#xA;        optimizer_dict[p] = GaLoreAdamW([{&#39;params&#39;: p, &#39;rank&#39;: 128, &#39;update_proj_gap&#39;: 200, &#39;scale&#39;: 0.25, &#39;proj_type&#39;: &#39;std&#39;}], lr=0.01)&#xA;&#xA;# define a hook function to update the parameter p during the backward pass&#xA;def optimizer_hook(p):&#xA;    if p.grad is None: &#xA;        return&#xA;    optimizer_dict[p].step()&#xA;    optimizer_dict[p].zero_grad()&#xA;&#xA;# Register the hook onto every parameter&#xA;for p in model.parameters():&#xA;    if p.requires_grad:&#xA;        p.register_post_accumulate_grad_hook(optimizer_hook)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details can be found in &lt;a href=&#34;https://github.com/jiaweizzhao/GaLore/raw/a6bc1650984b1c090a4e108d7c0e3109ee7ad844/torchrun_main.py#L334&#34;&gt;torchrun_main.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark 1: Pre-Training LLaMA on C4 dataset&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;torchrun_main.py&lt;/code&gt; is the main script for training LLaMA models on C4 with GaLore. Our benchmark scripts for various sizes of models are in &lt;code&gt;scripts/benchmark_c4&lt;/code&gt; folder. For example, to train a 60m model on C4, do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# LLaMA-60M, GaLore-Adam, 1 A100, 1 Node&#xA;torchrun --standalone --nproc_per_node 1 torchrun_main.py \&#xA;    --model_config configs/llama_60m.json \&#xA;    --lr 0.01 \&#xA;    --galore_scale 0.25 \&#xA;    --rank 128 \&#xA;    --update_proj_gap 200 \&#xA;    --batch_size 256 \&#xA;    --total_batch_size 512 \&#xA;    --num_training_steps 10000 \&#xA;    --warmup_steps 1000 \&#xA;    --weight_decay 0 \&#xA;    --dtype bfloat16 \&#xA;    --eval_every 1000 \&#xA;    --optimizer galore_adamw &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train 7B model with a single GPU with 24GB memory&lt;/h3&gt; &#xA;&lt;p&gt;To train a 7B model with a single GPU such as NVIDIA RTX 4090, all you need to do is to specify &lt;code&gt;--optimizer=galore_adamw8bit_per_layer&lt;/code&gt;, which enables &lt;code&gt;GaLoreAdamW8bit&lt;/code&gt; with per-layer weight updates. With activation checkpointing, you can maintain a batch size of 16 tested on NVIDIA RTX 4090.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# LLaMA-7B, 8-bit GaLore-Adam, single GPU, activation checkpointing&#xA;# bsz=16, 22.8G, &#xA;torchrun --standalone --nproc_per_node 1 torchrun_main.py \&#xA;    --model_config configs/llama_7b.json \&#xA;    --lr 0.005 \&#xA;    --galore_scale 0.25 \&#xA;    --rank 1024 \&#xA;    --update_proj_gap 500 \&#xA;    --batch_size 16 \&#xA;    --total_batch_size 512 \&#xA;    --activation_checkpointing \&#xA;    --num_training_steps 150000 \&#xA;    --warmup_steps 15000 \&#xA;    --weight_decay 0 \&#xA;    --grad_clipping 1.0 \&#xA;    --dtype bfloat16 \&#xA;    --eval_every 1000 \&#xA;    --single_gpu \&#xA;    --optimizer galore_adamw8bit_per_layer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently per-layer weight updates technique is only supported for single GPU training (&lt;code&gt;--single_gpu&lt;/code&gt;) without using &lt;code&gt;nn.parallel.DistributedDataParallel&lt;/code&gt;. We are working on supporting multi-GPU training with per-layer weight updates.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark 2: Fine-Tuning RoBERTa on GLUE tasks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;run_glue.py&lt;/code&gt; is the main script for fine-tuning RoBERTa models on GLUE tasks with GaLore. An example script is shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run_glue.py \&#xA;    --model_name_or_path roberta-base \&#xA;    --task_name mrpc \&#xA;    --enable_galore \&#xA;    --lora_all_modules \&#xA;    --max_length 512 \&#xA;    --seed=1234 \&#xA;    --lora_r 4 \&#xA;    --galore_scale 4 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --update_proj_gap 500 \&#xA;    --learning_rate 3e-5 \&#xA;    --num_train_epochs 30 \&#xA;    --output_dir results/ft/roberta_base/mrpc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhao2024galore,&#xA;      title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, &#xA;      author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},&#xA;      year={2024},&#xA;      eprint={2403.03507},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>VAST-AI-Research/TripoSR</title>
    <updated>2024-03-17T01:51:43Z</updated>
    <id>tag:github.com,2024-03-17:/VAST-AI-Research/TripoSR</id>
    <link href="https://github.com/VAST-AI-Research/TripoSR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TripoSR &lt;a href=&#34;https://huggingface.co/stabilityai/TripoSR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Model_Card-Huggingface-orange&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/stabilityai/TripoSR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Gradio%20Demo-Huggingface-orange&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2403.02151&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Paper-Huggingface-orange&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.02151&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2403.02151-B31B1B.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/mvS9mCfMnQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/teaser800.gif&#34; alt=&#34;Teaser Video&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This is the official codebase for &lt;strong&gt;TripoSR&lt;/strong&gt;, a state-of-the-art open-source model for &lt;strong&gt;fast&lt;/strong&gt; feedforward 3D reconstruction from a single image, collaboratively developed by &lt;a href=&#34;https://www.tripo3d.ai/&#34;&gt;Tripo AI&lt;/a&gt; and &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt;. &lt;br&gt;&lt;br&gt; Leveraging the principles of the &lt;a href=&#34;https://yiconghong.me/LRM/&#34;&gt;Large Reconstruction Model (LRM)&lt;/a&gt;, TripoSR brings to the table key advancements that significantly boost both the speed and quality of 3D reconstruction. Our model is distinguished by its ability to rapidly process inputs, generating high-quality 3D models in less than 0.5 seconds on an NVIDIA A100 GPU. TripoSR has exhibited superior performance in both qualitative and quantitative evaluations, outperforming other open-source alternatives across multiple public datasets. The figures below illustrate visual comparisons and metrics showcasing TripoSR&#39;s performance relative to other leading models. Details about the model architecture, training process, and comparisons can be found in this &lt;a href=&#34;https://arxiv.org/abs/2403.02151&#34;&gt;technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!--&#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  &lt;img src=&#34;figures/comparison800.gif&#34; alt=&#34;Teaser Video&#34;&gt;&#xA;&lt;/div&gt;&#xA;--&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;800&#34; src=&#34;https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/visual_comparisons.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;450&#34; src=&#34;https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/scatter-comparison.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The model is released under the MIT license, which includes the source code, pretrained models, and an interactive online demo. Our goal is to empower researchers, developers, and creatives to push the boundaries of what&#39;s possible in 3D generative AI and 3D content creation.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8&lt;/li&gt; &#xA; &lt;li&gt;Install CUDA if available&lt;/li&gt; &#xA; &lt;li&gt;Install PyTorch according to your platform: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt; &lt;strong&gt;[Please make sure that the locally-installed CUDA major version matches the PyTorch-shipped CUDA major version. For example if you have CUDA 11.x installed, make sure to install PyTorch compiled with CUDA 11.x.]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update setuptools by &lt;code&gt;pip install --upgrade setuptools&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install other dependencies by &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Manual Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python run.py examples/chair.png --output-dir output/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will save the reconstructed 3D model to &lt;code&gt;output/&lt;/code&gt;. You can also specify more than one image path separated by spaces. The default options takes about &lt;strong&gt;6GB VRAM&lt;/strong&gt; for a single image input.&lt;/p&gt; &#xA;&lt;p&gt;For detailed usage of this script, use &lt;code&gt;python run.py --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Local Gradio App&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;AttributeError: module &#39;torchmcubes_module&#39; has no attribute &#39;mcubes_cuda&#39;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;torchmcubes was not compiled with CUDA support, use CPU version instead.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This is because &lt;code&gt;torchmcubes&lt;/code&gt; is compiled without CUDA support. Please make sure that&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The locally-installed CUDA major version matches the PyTorch-shipped CUDA major version. For example if you have CUDA 11.x installed, make sure to install PyTorch compiled with CUDA 11.x.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;setuptools&amp;gt;=49.6.0&lt;/code&gt;. If not, upgrade by &lt;code&gt;pip install --upgrade setuptools&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then re-install &lt;code&gt;torchmcubes&lt;/code&gt; by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip uninstall torchmcubes&#xA;pip install git+https://github.com/tatsy/torchmcubes.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{TripoSR2024,&#xA;  title={TripoSR: Fast 3D Object Reconstruction from a Single Image},&#xA;  author={Tochilkin, Dmitry and Pankratz, David and Liu, Zexiang and Huang, Zixuan and and Letts, Adam and Li, Yangguang and Liang, Ding and Laforte, Christian and Jampani, Varun and Cao, Yan-Pei},&#xA;  journal={arXiv preprint arXiv:2403.02151},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>