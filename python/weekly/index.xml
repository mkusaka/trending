<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-16T01:50:17Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>volcengine/verl</title>
    <updated>2025-02-16T01:50:17Z</updated>
    <id>tag:github.com,2025-02-16:/volcengine/verl</id>
    <link href="https://github.com/volcengine/verl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;verl: Volcano Engine Reinforcement Learning for LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1 style=&#34;text-align: center;&#34;&gt;verl: Volcano Engine Reinforcement Learning for LLM&lt;/h1&gt; &#xA;&lt;p&gt;verl is a flexible, efficient and production-ready RL training library for large language models (LLMs).&lt;/p&gt; &#xA;&lt;p&gt;verl is the open-source version of &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.19256v2&#34;&gt;HybridFlow: A Flexible and Efficient RLHF Framework&lt;/a&gt;&lt;/strong&gt; paper.&lt;/p&gt; &#xA;&lt;p&gt;verl is flexible and easy to use with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy extension of diverse RL algorithms&lt;/strong&gt;: The Hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Seamless integration of existing LLM infra with modular APIs&lt;/strong&gt;: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM and vLLM. Moreover, users can easily extend to other LLM training and inference frameworks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible device mapping&lt;/strong&gt;: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Readily integration with popular HuggingFace models&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;verl is fast with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;State-of-the-art throughput&lt;/strong&gt;: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient actor model resharding with 3D-HybridEngine&lt;/strong&gt;: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://verl.readthedocs.io/en/latest/index.html&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2409.19256v2&#34;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA&#34;&gt;&lt;b&gt;Slack&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG&#34;&gt;&lt;b&gt;Wechat&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://x.com/verl_project&#34;&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;&#34;&gt;&lt;b&gt;Slides&lt;/b&gt;&lt;/a&gt; | --&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2025/2] We will present verl in the &lt;a href=&#34;https://lu.ma/ji7atxux&#34;&gt;Bytedance/NVIDIA/Anyscale Ray Meetup&lt;/a&gt; in bay area on Feb 13th. Come join us in person!&lt;/li&gt; &#xA; &lt;li&gt;[2025/1] &lt;a href=&#34;https://team.doubao.com/zh/special/doubao_1_5_pro&#34;&gt;Doubao-1.5-pro&lt;/a&gt; is released with SOTA-level performance on LLM &amp;amp; VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).&lt;/li&gt; &#xA; &lt;li&gt;[2024/12] The team presented &lt;a href=&#34;https://neurips.cc/Expo/Conferences/2024/workshop/100677&#34;&gt;Post-training LLMs: From Algorithms to Infrastructure&lt;/a&gt; at NeurIPS 2024. &lt;a href=&#34;https://github.com/eric-haibin-lin/verl-data/tree/neurips&#34;&gt;Slides&lt;/a&gt; and &lt;a href=&#34;https://neurips.cc/Expo/Conferences/2024/workshop/100677&#34;&gt;video&lt;/a&gt; available.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] verl is presented at Ray Summit. &lt;a href=&#34;https://www.youtube.com/watch?v=MrhMcXkXvJU&amp;amp;list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&amp;amp;index=37&#34;&gt;Youtube video&lt;/a&gt; available.&lt;/li&gt; &#xA; &lt;li&gt;[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;FSDP&lt;/strong&gt; and &lt;strong&gt;Megatron-LM&lt;/strong&gt; for training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt; and &lt;strong&gt;TGI&lt;/strong&gt; for rollout generation, &lt;strong&gt;SGLang&lt;/strong&gt; support coming soon.&lt;/li&gt; &#xA; &lt;li&gt;huggingface models support&lt;/li&gt; &#xA; &lt;li&gt;Supervised fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;Reinforcement learning from human feedback with &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/ppo_trainer&#34;&gt;PPO&lt;/a&gt;, &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/grpo_trainer&#34;&gt;GRPO&lt;/a&gt;, and &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/remax_trainer&#34;&gt;ReMax&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support model-based reward and function-based reward (verifiable reward)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;flash-attention, &lt;a href=&#34;https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh&#34;&gt;sequence packing&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh&#34;&gt;long context&lt;/a&gt; support via DeepSpeed Ulysses, &lt;a href=&#34;https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh&#34;&gt;Liger-kernel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;scales up to 70B models and hundreds of GPUs&lt;/li&gt; &#xA; &lt;li&gt;experiment tracking with wandb, swanlab and mlflow&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Upcoming Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reward model training&lt;/li&gt; &#xA; &lt;li&gt;DPO training&lt;/li&gt; &#xA; &lt;li&gt;DeepSeek integration with Megatron backend&lt;/li&gt; &#xA; &lt;li&gt;SGLang integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Checkout this &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/ppo_trainer/verl_getting_started.ipynb&#34;&gt;Jupyter Notebook&lt;/a&gt; to get started with PPO training with a single 24GB L4 GPU (&lt;strong&gt;FREE&lt;/strong&gt; GPU quota provided by &lt;a href=&#34;https://lightning.ai/hlin-verl/studios/verl-getting-started&#34;&gt;Lighting Studio&lt;/a&gt;)!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quickstart:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/start/install.html&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/start/quickstart.html&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/hybrid_flow.html&#34;&gt;Programming Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running a PPO example step-by-step:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data and Reward Preparation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/preparation/prepare_data.html&#34;&gt;Prepare Data for Post-Training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/preparation/reward_function.html&#34;&gt;Implement Reward Function for Dataset&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Understanding the PPO Example &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html&#34;&gt;PPO Example Architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/examples/config.html&#34;&gt;Config Explanation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/examples/gsm8k_example.html&#34;&gt;Run GSM8K Example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reproducible algorithm baselines:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/experiment/ppo.html&#34;&gt;PPO and GRPO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;For code explanation and advance usage (extension):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PPO Trainer and Workers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/workers/ray_trainer.html&#34;&gt;PPO Ray Trainer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html&#34;&gt;PyTorch FSDP Backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/index.html&#34;&gt;Megatron-LM Backend&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Advance Usage and Extension &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/placement.html&#34;&gt;Ray API design tutorial&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/dpo_extension.html&#34;&gt;Extend to Other RL(HF) algorithms&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html&#34;&gt;Add Models with the FSDP Backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/megatron_extension.html&#34;&gt;Add Models with the Megatron-LM Backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/split_placement&#34;&gt;Deployment using Separate GPU Resources&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance Tuning Guide&lt;/h2&gt; &#xA;&lt;p&gt;The performance is essential for on-policy RL algorithm. We write a detailed performance tuning guide to allow people tune the performance. See &lt;a href=&#34;https://verl.readthedocs.io/en/latest/perf/perf_tuning.html&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;vLLM v0.7 testing version&lt;/h2&gt; &#xA;&lt;p&gt;We have released a testing version of veRL that supports vLLM&amp;gt;=0.7.0. Please refer to &lt;a href=&#34;https://github.com/volcengine/verl/docs/README_vllm0.7.md&#34;&gt;this document&lt;/a&gt; for installation guide and more information.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution Guide&lt;/h2&gt; &#xA;&lt;p&gt;Contributions from the community are welcome!&lt;/p&gt; &#xA;&lt;h3&gt;Code formatting&lt;/h3&gt; &#xA;&lt;p&gt;We use yapf (Google style) to enforce strict code formatting when reviewing PRs. To reformat you code locally, make sure you installed &lt;strong&gt;latest&lt;/strong&gt; &lt;code&gt;yapf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install yapf --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, make sure you are at top level of verl repo and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/format.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation and acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;If you find the project helpful, please cite:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.19256v2&#34;&gt;HybridFlow: A Flexible and Efficient RLHF Framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf&#34;&gt;A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@article{sheng2024hybridflow,&#xA;  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},&#xA;  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},&#xA;  year    = {2024},&#xA;  journal = {arXiv preprint arXiv: 2409.19256}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and supported by Anyscale, Bytedance, LMSys.org, Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, and University of Hong Kong.&lt;/p&gt; &#xA;&lt;h2&gt;Awesome work using verl&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.09302&#34;&gt;Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.21236&#34;&gt;Flaming-hot Initiation with Regular Execution Sampling for Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PRIME-RL/PRIME/&#34;&gt;Process Reinforcement Through Implicit Rewards&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jiayi-Pan/TinyZero&#34;&gt;TinyZero&lt;/a&gt;: a reproduction of DeepSeek R1 Zero recipe for reasoning tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZihanWang314/ragen&#34;&gt;RAGEN&lt;/a&gt;: a general-purpose reasoning agent training framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Unakar/Logic-RL&#34;&gt;Logic R1&lt;/a&gt;: a reproduced DeepSeek R1 Zero on 2K Tiny Logic Puzzle Dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/agentica-project/deepscaler&#34;&gt;deepscaler&lt;/a&gt;: iterative context scaling with GRPO&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HKUNLP/critic-rl&#34;&gt;critic-rl&lt;/a&gt;: Teaching Language Models to Critique via Reinforcement Learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are HIRING! Send us an &lt;a href=&#34;mailto:haibin.lin@bytedance.com&#34;&gt;email&lt;/a&gt; if you are interested in internship/FTE opportunities in MLSys/LLM reasoning/multimodal alignment.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Huanshere/VideoLingo</title>
    <updated>2025-02-16T01:50:17Z</updated>
    <id>tag:github.com,2025-02-16:/Huanshere/VideoLingo</id>
    <link href="https://github.com/Huanshere/VideoLingo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | Netflix级字幕切割、翻译、对齐、甚至加上配音，一键全自动视频搬运AI字幕组&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/logo.png&#34; alt=&#34;VideoLingo Logo&#34; height=&#34;140&#34;&gt; &#xA; &lt;h1&gt;Connect the World, Frame by Frame&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/12200&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/12200&#34; alt=&#34;Huanshere%2FVideoLingo | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/README.md&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.zh.md&#34;&gt;&lt;strong&gt;简体中文&lt;/strong&gt;&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.zh-TW.md&#34;&gt;&lt;strong&gt;繁體中文&lt;/strong&gt;&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.ja.md&#34;&gt;&lt;strong&gt;日本語&lt;/strong&gt;&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.es.md&#34;&gt;&lt;strong&gt;Español&lt;/strong&gt;&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.ru.md&#34;&gt;&lt;strong&gt;Русский&lt;/strong&gt;&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/translations/README.fr.md&#34;&gt;&lt;strong&gt;Français&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🌟 Overview (&lt;a href=&#34;https://videolingo.io&#34;&gt;Try VL Now!&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;VideoLingo is an all-in-one video translation, localization, and dubbing tool aimed at generating Netflix-quality subtitles. It eliminates stiff machine translations and multi-line subtitles while adding high-quality dubbing, enabling global knowledge sharing across language barriers.&lt;/p&gt; &#xA;&lt;p&gt;Key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🎥 YouTube video download via yt-dlp&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;🎙️ Word-level and Low-illusion subtitle recognition with WhisperX&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;📝 NLP and AI-powered subtitle segmentation&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;📚 Custom + AI-generated terminology for coherent translation&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔄 3-step Translate-Reflect-Adaptation for cinematic quality&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;✅ Netflix-standard, Single-line subtitles Only&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;🗣️ Dubbing with GPT-SoVITS, Azure, OpenAI, and more&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🚀 One-click startup and processing in Streamlit&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🌍 Multi-language support in Streamlit UI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;📝 Detailed logging with progress resumption&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Difference from similar projects: &lt;strong&gt;Single-line subtitles only, superior translation quality, seamless dubbing experience&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🎥 Demo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;Russian Translation&lt;/h3&gt; &#xA;    &lt;hr&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/25264b5b-6931-4d39-948c-5a1e4ce42fa7&#34;&gt;https://github.com/user-attachments/assets/25264b5b-6931-4d39-948c-5a1e4ce42fa7&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;GPT-SoVITS Dubbing&lt;/h3&gt; &#xA;    &lt;hr&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c&#34;&gt;https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Language Support&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Input Language Support(more to come):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;🇺🇸 English 🤩 | 🇷🇺 Russian 😊 | 🇫🇷 French 🤩 | 🇩🇪 German 🤩 | 🇮🇹 Italian 🤩 | 🇪🇸 Spanish 🤩 | 🇯🇵 Japanese 😐 | 🇨🇳 Chinese* 😊&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;*Chinese uses a separate punctuation-enhanced whisper model, for now...&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Translation supports all languages, while dubbing language depends on the chosen TTS method.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You don&#39;t have to read the whole docs, &lt;a href=&#34;https://share.fastgpt.in/chat/share?shareId=066w11n3r9aq6879r4z0v9rh&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt; is an online AI agent to help you.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For Windows users with NVIDIA GPU, follow these steps before installation:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.76_windows.exe&#34;&gt;CUDA Toolkit 12.6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://developer.download.nvidia.com/compute/cudnn/9.3.0/local_installers/cudnn_9.3.0_windows.exe&#34;&gt;CUDNN 9.3.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Add &lt;code&gt;C:\Program Files\NVIDIA\CUDNN\v9.3\bin\12.6&lt;/code&gt; to your system PATH&lt;/li&gt; &#xA;  &lt;li&gt;Restart your computer&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; FFmpeg is required. Please install it via package managers:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Windows: &lt;code&gt;choco install ffmpeg&lt;/code&gt; (via &lt;a href=&#34;https://chocolatey.org/&#34;&gt;Chocolatey&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;macOS: &lt;code&gt;brew install ffmpeg&lt;/code&gt; (via &lt;a href=&#34;https://brew.sh/&#34;&gt;Homebrew&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;Linux: &lt;code&gt;sudo apt install ffmpeg&lt;/code&gt; (Debian/Ubuntu)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Huanshere/VideoLingo.git&#xA;cd VideoLingo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install dependencies(requires &lt;code&gt;python=3.10&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n videolingo python=3.10.0 -y&#xA;conda activate videolingo&#xA;python install.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Start the application&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;streamlit run st.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, you can use Docker (requires CUDA 12.4 and NVIDIA Driver version &amp;gt;550), see &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/docker.en-US.md&#34;&gt;Docker docs&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t videolingo .&#xA;docker run -d -p 8501:8501 --gpus all videolingo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;APIs&lt;/h2&gt; &#xA;&lt;p&gt;VideoLingo supports OpenAI-Like API format and various TTS interfaces:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLM: &lt;code&gt;claude-3-5-sonnet-20240620&lt;/code&gt;, &lt;code&gt;deepseek-chat(v3)&lt;/code&gt;, &lt;code&gt;gemini-2.0-flash-exp&lt;/code&gt;, &lt;code&gt;gpt-4o&lt;/code&gt;, ... (sorted by performance)&lt;/li&gt; &#xA; &lt;li&gt;WhisperX: Run whisperX locally or use 302.ai API&lt;/li&gt; &#xA; &lt;li&gt;TTS: &lt;code&gt;azure-tts&lt;/code&gt;, &lt;code&gt;openai-tts&lt;/code&gt;, &lt;code&gt;siliconflow-fishtts&lt;/code&gt;, &lt;strong&gt;&lt;code&gt;fish-tts&lt;/code&gt;&lt;/strong&gt;, &lt;code&gt;GPT-SoVITS&lt;/code&gt;, &lt;code&gt;edge-tts&lt;/code&gt;, &lt;code&gt;*custom-tts&lt;/code&gt;(You can modify your own TTS in custom_tts.py!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; VideoLingo works with &lt;strong&gt;&lt;a href=&#34;https://gpt302.saaslink.net/C2oHR9&#34;&gt;302.ai&lt;/a&gt;&lt;/strong&gt; - one API key for all services (LLM, WhisperX, TTS). Or run locally with Ollama and Edge-TTS for free, no API needed!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For detailed installation, API configuration, and batch mode instructions, please refer to the documentation: &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/start.en-US.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/start.zh-CN.md&#34;&gt;中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Current Limitations&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;WhisperX transcription performance may be affected by video background noise, as it uses wav2vac model for alignment. For videos with loud background music, please enable Voice Separation Enhancement. Additionally, subtitles ending with numbers or special characters may be truncated early due to wav2vac&#39;s inability to map numeric characters (e.g., &#34;1&#34;) to their spoken form (&#34;one&#34;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Using weaker models can lead to errors during intermediate processes due to strict JSON format requirements for responses. If this error occurs, please delete the &lt;code&gt;output&lt;/code&gt; folder and retry with a different LLM, otherwise repeated execution will read the previous erroneous response causing the same error.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The dubbing feature may not be 100% perfect due to differences in speech rates and intonation between languages, as well as the impact of the translation step. However, this project has implemented extensive engineering processing for speech rates to ensure the best possible dubbing results.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multilingual video transcription recognition will only retain the main language&lt;/strong&gt;. This is because whisperX uses a specialized model for a single language when forcibly aligning word-level subtitles, and will delete unrecognized languages.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cannot dub multiple characters separately&lt;/strong&gt;, as whisperX&#39;s speaker distinction capability is not sufficiently reliable.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;📄 License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. Special thanks to the following open source projects for their contributions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/m-bain/whisperX&#34;&gt;whisperX&lt;/a&gt;, &lt;a href=&#34;https://github.com/yt-dlp/yt-dlp&#34;&gt;yt-dlp&lt;/a&gt;, &lt;a href=&#34;https://github.com/mangiucugna/json_repair&#34;&gt;json_repair&lt;/a&gt;, &lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📬 Contact Me&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit &lt;a href=&#34;https://github.com/Huanshere/VideoLingo/issues&#34;&gt;Issues&lt;/a&gt; or &lt;a href=&#34;https://github.com/Huanshere/VideoLingo/pulls&#34;&gt;Pull Requests&lt;/a&gt; on GitHub&lt;/li&gt; &#xA; &lt;li&gt;DM me on Twitter: &lt;a href=&#34;https://twitter.com/Huanshere&#34;&gt;@Huanshere&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Email me at: &lt;a href=&#34;mailto:team@videolingo.io&#34;&gt;team@videolingo.io&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⭐ Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Huanshere/VideoLingo&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Huanshere/VideoLingo&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt;If you find VideoLingo helpful, please give me a ⭐️!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>RockChinQ/LangBot</title>
    <updated>2025-02-16T01:50:17Z</updated>
    <id>tag:github.com,2025-02-16:/RockChinQ/LangBot</id>
    <link href="https://github.com/RockChinQ/LangBot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;😎丰富生态、🧩支持扩展、🦄多模态 - 大模型原生即时通信机器人平台 | 适配 QQ / 微信（企业微信、个人微信）/ 飞书 / 钉钉 / Discord / Telegram 等消息平台 | 支持 OpenAI GPT、ChatGPT、DeepSeek、Dify、Claude、Gemini、Ollama、LM Studio、SiliconFlow、Qwen、Moonshot、ChatGLM 等 LLM 的机器人 / Agent | LLM-based instant messaging bots platform, supports Discord, Telegram, WeChat, Lark, DingTalk, QQ, OpenAI ChatGPT, DeepSeek&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://langbot.app&#34;&gt; &lt;img src=&#34;https://docs.langbot.app/social.png&#34; alt=&#34;LangBot&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/12901&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/12901&#34; alt=&#34;RockChinQ%2FLangBot | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://docs.langbot.app&#34;&gt;项目主页&lt;/a&gt; ｜ &lt;a href=&#34;https://docs.langbot.app/insight/intro.htmll&#34;&gt;功能介绍&lt;/a&gt; ｜ &lt;a href=&#34;https://docs.langbot.app/insight/guide.html&#34;&gt;部署文档&lt;/a&gt; ｜ &lt;a href=&#34;https://docs.langbot.app/usage/faq.html&#34;&gt;常见问题&lt;/a&gt; ｜ &lt;a href=&#34;https://docs.langbot.app/plugin/plugin-intro.html&#34;&gt;插件介绍&lt;/a&gt; ｜ &lt;a href=&#34;https://github.com/RockChinQ/LangBot/issues/new?assignees=&amp;amp;labels=%E7%8B%AC%E7%AB%8B%E6%8F%92%E4%BB%B6&amp;amp;projects=&amp;amp;template=submit-plugin.yml&amp;amp;title=%5BPlugin%5D%3A+%E8%AF%B7%E6%B1%82%E7%99%BB%E8%AE%B0%E6%96%B0%E6%8F%92%E4%BB%B6&#34;&gt;提交插件&lt;/a&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt;&#xA;   😎高稳定、🧩支持扩展、🦄多模态 - 大模型原生即时通信机器人平台🤖 &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/wdNEHETs87&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1335141740050649118?logo=discord&amp;amp;labelColor=%20%235462eb&amp;amp;logoColor=%20%23f5f5f5&amp;amp;color=%20%235462eb&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://qm.qq.com/q/PF9OuQCCcM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E7%A4%BE%E5%8C%BAQQ%E7%BE%A4-1030838208-blue&#34; alt=&#34;QQ Group&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RockChinQ/LangBot/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/RockChinQ/LangBot&#34; alt=&#34;GitHub release (latest by date)&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.qchatgpt.rockchin.top%2Fapi%2Fv2%2Fview%2Frealtime%2Fcount_query%3Fminute%3D10080&amp;amp;query=%24.data.count&amp;amp;label=%E4%BD%BF%E7%94%A8%E9%87%8F%EF%BC%887%E6%97%A5%EF%BC%89&#34; alt=&#34;Dynamic JSON Badge&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12-blue.svg?sanitize=true&#34; alt=&#34;python&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RockChinQ/LangBot/master/README.md&#34;&gt;简体中文&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/RockChinQ/LangBot/master/README_EN.md&#34;&gt;English&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/RockChinQ/LangBot/master/README_JP.md&#34;&gt;日本語&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;✨ 特性&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;💬 大模型对话、Agent：支持多种大模型，适配群聊和私聊；具有多轮对话、工具调用、多模态能力，并深度适配 &lt;a href=&#34;https://dify.ai&#34;&gt;Dify&lt;/a&gt;。目前支持 QQ、QQ频道、企业微信、个人微信、飞书、Discord、Telegram 等平台。&lt;/li&gt; &#xA; &lt;li&gt;🛠️ 高稳定性、功能完备：原生支持访问控制、限速、敏感词过滤等机制；配置简单，支持多种部署方式。&lt;/li&gt; &#xA; &lt;li&gt;🧩 插件扩展、活跃社区：支持事件驱动、组件扩展等插件机制；丰富生态，目前已有数十个&lt;a href=&#34;https://docs.langbot.app/plugin/plugin-intro.html&#34;&gt;插件&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;😻 [New] Web 管理面板：支持通过浏览器管理 LangBot 实例，具体支持功能，查看&lt;a href=&#34;https://docs.langbot.app/webui/intro.html&#34;&gt;文档&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📦 开始使用&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;p&gt;在您开始任何方式部署之前，请务必阅读&lt;a href=&#34;https://docs.langbot.app/insight/guide.html&#34;&gt;新手指引&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Docker Compose 部署&lt;/h4&gt; &#xA;&lt;p&gt;适合熟悉 Docker 的用户，查看文档&lt;a href=&#34;https://docs.langbot.app/deploy/langbot/docker.html&#34;&gt;Docker 部署&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;宝塔面板部署&lt;/h4&gt; &#xA;&lt;p&gt;已上架宝塔面板，若您已安装宝塔面板，可以根据&lt;a href=&#34;https://docs.langbot.app/deploy/langbot/one-click/bt.html&#34;&gt;文档&lt;/a&gt;使用。&lt;/p&gt; &#xA;&lt;h4&gt;Zeabur 云部署&lt;/h4&gt; &#xA;&lt;p&gt;社区贡献的 Zeabur 模板。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zeabur.com/zh-CN/templates/ZKTBDH&#34;&gt;&lt;img src=&#34;https://zeabur.com/button.svg?sanitize=true&#34; alt=&#34;Deploy on Zeabur&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Railway 云部署&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://railway.app/template/yRrAyL?referralCode=vogKPF&#34;&gt;&lt;img src=&#34;https://railway.com/button.svg?sanitize=true&#34; alt=&#34;Deploy on Railway&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;手动部署&lt;/h4&gt; &#xA;&lt;p&gt;直接使用发行版运行，查看文档&lt;a href=&#34;https://docs.langbot.app/deploy/langbot/manual.html&#34;&gt;手动部署&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;📸 效果展示&lt;/h2&gt; &#xA;&lt;img alt=&#34;回复效果（带有联网插件）&#34; src=&#34;https://docs.langbot.app/QChatGPT-0516.png&#34; width=&#34;500px&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;WebUI Demo: &lt;a href=&#34;https://demo.langbot.dev/&#34;&gt;https://demo.langbot.dev/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;登录信息：邮箱：&lt;code&gt;demo@langbot.app&lt;/code&gt; 密码：&lt;code&gt;langbot123456&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;注意：仅展示webui效果，公开环境，请不要在其中填入您的任何敏感信息。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔌 组件兼容性&lt;/h2&gt; &#xA;&lt;h3&gt;消息平台&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;平台&lt;/th&gt; &#xA;   &lt;th&gt;状态&lt;/th&gt; &#xA;   &lt;th&gt;备注&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QQ 个人号&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;QQ 个人号私聊、群聊&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QQ 官方机器人&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;QQ 官方机器人，支持频道、私聊、群聊&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;企业微信&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;个人微信&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;使用 &lt;a href=&#34;https://github.com/Devo919/Gewechat&#34;&gt;Gewechat&lt;/a&gt; 接入&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;微信公众号&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;飞书&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;钉钉&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Discord&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Telegram&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WhatsApp&lt;/td&gt; &#xA;   &lt;td&gt;🚧&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;🚧: 正在开发中&lt;/p&gt; &#xA;&lt;h3&gt;大模型&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;模型&lt;/th&gt; &#xA;   &lt;th&gt;状态&lt;/th&gt; &#xA;   &lt;th&gt;备注&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://platform.openai.com/&#34;&gt;OpenAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;可接入任何 OpenAI 接口格式模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepseek.com/&#34;&gt;DeepSeek&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.moonshot.cn/&#34;&gt;Moonshot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.anthropic.com/&#34;&gt;Anthropic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://x.ai/&#34;&gt;xAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://open.bigmodel.cn/&#34;&gt;智谱AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dify.ai&#34;&gt;Dify&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;LLMOps 平台&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;本地大模型运行平台&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LMStudio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;本地大模型运行平台&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.gitee.com/&#34;&gt;GiteeAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;大模型接口聚合平台&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://siliconflow.cn/&#34;&gt;SiliconFlow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;大模型聚合平台&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bailian.console.aliyun.com/&#34;&gt;阿里云百炼&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;大模型聚合平台, LLMOps 平台&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;😘 社区贡献&lt;/h2&gt; &#xA;&lt;p&gt;LangBot 离不开以下贡献者和社区内所有人的贡献，我们欢迎任何形式的贡献和反馈。&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/RockChinQ/LangBot/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=RockChinQ/LangBot&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>