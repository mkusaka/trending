<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-09T01:46:10Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>skypilot-org/skypilot</title>
    <updated>2024-06-09T01:46:10Z</updated>
    <id>tag:github.com,2024-06-09:/skypilot-org/skypilot</id>
    <link href="https://github.com/skypilot-org/skypilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SkyPilot: Run LLMs, AI, and Batch jobs on any cloud. Get maximum savings, highest GPU availability, and managed execution—all with a simple interface.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-dark-1k.png&#34;&gt; &#xA;  &lt;img alt=&#34;SkyPilot&#34; src=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png&#34; width=&#34;55%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/&#34;&gt; &lt;img alt=&#34;Documentation&#34; src=&#34;https://readthedocs.org/projects/skypilot/badge/?version=latest&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/skypilot-org/skypilot/releases&#34;&gt; &lt;img alt=&#34;GitHub Release&#34; src=&#34;https://img.shields.io/github/release/skypilot-org/skypilot.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;http://slack.skypilot.co&#34;&gt; &lt;img alt=&#34;Join Slack&#34; src=&#34;https://img.shields.io/badge/SkyPilot-Join%20Slack-blue?logo=slack&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Run LLMs and AI on Any Cloud &lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;span&gt;🔥&lt;/span&gt; &lt;em&gt;News&lt;/em&gt; &lt;span&gt;🔥&lt;/span&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Jun, 2024] Reproduce &lt;strong&gt;GPT&lt;/strong&gt; with &lt;a href=&#34;https://github.com/karpathy/llm.c/discussions/481&#34;&gt;llm.c&lt;/a&gt; on any cloud: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/gpt-2/&#34;&gt;&lt;strong&gt;guide&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Apr, 2024] Serve and finetune &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/gallery/llms/llama-3.html&#34;&gt;&lt;strong&gt;Llama 3&lt;/strong&gt;&lt;/a&gt; on any cloud or Kubernetes: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/llama-3/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Apr, 2024] Serve &lt;a href=&#34;https://qwenlm.github.io/blog/qwen1.5-110b/&#34;&gt;&lt;strong&gt;Qwen-110B&lt;/strong&gt;&lt;/a&gt; on your infra: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/qwen/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Apr, 2024] Using &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/a&gt; to deploy quantized LLMs on CPUs and GPUs: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/ollama/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Feb, 2024] Deploying and scaling &lt;a href=&#34;https://blog.google/technology/developers/gemma-open-models/&#34;&gt;&lt;strong&gt;Gemma&lt;/strong&gt;&lt;/a&gt; with SkyServe: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/gemma/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Feb, 2024] Serving &lt;a href=&#34;https://ai.meta.com/blog/code-llama-large-language-model-coding/&#34;&gt;&lt;strong&gt;Code Llama 70B&lt;/strong&gt;&lt;/a&gt; with vLLM and SkyServe: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/codellama/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Dec, 2023] &lt;a href=&#34;https://mistral.ai/news/mixtral-of-experts/&#34;&gt;&lt;strong&gt;Mixtral 8x7B&lt;/strong&gt;&lt;/a&gt;, a high quality sparse mixture-of-experts model, was released by Mistral AI! Deploy via SkyPilot on any cloud: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/mixtral/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Nov, 2023] Using &lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl&#34;&gt;&lt;strong&gt;Axolotl&lt;/strong&gt;&lt;/a&gt; to finetune Mistral 7B on the cloud (on-demand and spot): &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/axolotl/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Sep, 2023] Case study: &lt;a href=&#34;https://covariant.ai/&#34;&gt;&lt;strong&gt;Covariant&lt;/strong&gt;&lt;/a&gt; transformed AI development on the cloud using SkyPilot, delivering models 4x faster cost-effectively: &lt;a href=&#34;https://blog.skypilot.co/covariant/&#34;&gt;&lt;strong&gt;read the case study&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Aug, 2023] &lt;strong&gt;Finetuning Cookbook&lt;/strong&gt;: Finetuning Llama 2 in your own cloud environment, privately: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/vicuna-llama-2/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://blog.skypilot.co/finetuning-llama2-operational-guide/&#34;&gt;&lt;strong&gt;blog post&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[June, 2023] Serving LLM 24x Faster On the Cloud &lt;a href=&#34;https://vllm.ai/&#34;&gt;&lt;strong&gt;with vLLM&lt;/strong&gt;&lt;/a&gt; and SkyPilot: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/vllm/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/&#34;&gt;&lt;strong&gt;blog post&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Archived&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[Mar, 2024] Serve and deploy &lt;a href=&#34;https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm&#34;&gt;&lt;strong&gt;Databricks DBRX&lt;/strong&gt;&lt;/a&gt; on your infra: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/dbrx/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;[Feb, 2024] Speed up your LLM deployments with &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;&lt;strong&gt;SGLang&lt;/strong&gt;&lt;/a&gt; for 5x throughput on SkyServe: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/sglang/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;[Dec, 2023] Using &lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt;&lt;strong&gt;LoRAX&lt;/strong&gt;&lt;/a&gt; to serve 1000s of finetuned LLMs on a single instance in the cloud: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/lorax/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;[Sep, 2023] &lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;&lt;strong&gt;Mistral 7B&lt;/strong&gt;&lt;/a&gt;, a high-quality open LLM, was released! Deploy via SkyPilot on any cloud: &lt;a href=&#34;https://docs.mistral.ai/self-deployment/skypilot&#34;&gt;&lt;strong&gt;Mistral docs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;[July, 2023] Self-Hosted &lt;strong&gt;Llama-2 Chatbot&lt;/strong&gt; on Any Cloud: &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/llama-2/&#34;&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;[April, 2023] &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/vicuna/&#34;&gt;SkyPilot YAMLs&lt;/a&gt; for finetuning &amp;amp; serving the &lt;a href=&#34;https://lmsys.org/blog/2023-03-30-vicuna/&#34;&gt;Vicuna LLM&lt;/a&gt; with a single command!&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;SkyPilot is a framework for running LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.&lt;/p&gt; &#xA;&lt;p&gt;SkyPilot &lt;strong&gt;abstracts away cloud infra burdens&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Launch jobs &amp;amp; clusters on any cloud&lt;/li&gt; &#xA; &lt;li&gt;Easy scale-out: queue and run many jobs, automatically managed&lt;/li&gt; &#xA; &lt;li&gt;Easy access to object stores (S3, GCS, R2)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;SkyPilot &lt;strong&gt;maximizes GPU availability for your jobs&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provision in all zones/regions/clouds you have access to (&lt;a href=&#34;https://arxiv.org/abs/2205.07147&#34;&gt;the &lt;em&gt;Sky&lt;/em&gt;&lt;/a&gt;), with automatic failover&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;SkyPilot &lt;strong&gt;cuts your cloud costs&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html&#34;&gt;Managed Spot&lt;/a&gt;: 3-6x cost savings using spot VMs, with auto-recovery from preemptions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/examples/auto-failover.html&#34;&gt;Optimizer&lt;/a&gt;: 2x cost savings by auto-picking the cheapest VM/zone/region/cloud&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/reference/auto-stop.html&#34;&gt;Autostop&lt;/a&gt;: hands-free cleanup of idle clusters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;SkyPilot supports your existing GPU, TPU, and CPU workloads, with no code changes.&lt;/p&gt; &#xA;&lt;p&gt;Install with pip (we recommend the nightly build for the latest features or &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/getting-started/installation.html&#34;&gt;from source&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;skypilot-nightly[aws,gcp,azure,oci,lambda,runpod,fluidstack,paperspace,cudo,ibm,scp,kubernetes]&#34;  # choose your clouds&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get the last release, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U &#34;skypilot[aws,gcp,azure,oci,lambda,runpod,fluidstack,paperspace,cudo,ibm,scp,kubernetes]&#34;  # choose your clouds&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Current supported providers (AWS, Azure, GCP, OCI, Lambda Cloud, RunPod, Fluidstack, Paperspace, Cudo, IBM, Samsung, Cloudflare, any Kubernetes cluster):&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-dark.png&#34;&gt; &#xA;  &lt;img alt=&#34;SkyPilot&#34; src=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-light.png&#34; width=&#34;85%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;You can find our documentation &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/getting-started/installation.html&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/getting-started/quickstart.html&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/reference/cli.html&#34;&gt;CLI reference&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;SkyPilot in 1 Minute&lt;/h2&gt; &#xA;&lt;p&gt;A SkyPilot task specifies: resource requirements, data to be synced, setup commands, and the task commands.&lt;/p&gt; &#xA;&lt;p&gt;Once written in this &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html&#34;&gt;&lt;strong&gt;unified interface&lt;/strong&gt;&lt;/a&gt; (YAML or Python API), the task can be launched on any available cloud. This avoids vendor lock-in, and allows easily moving jobs to a different provider.&lt;/p&gt; &#xA;&lt;p&gt;Paste the following into a file &lt;code&gt;my_task.yaml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;resources:&#xA;  accelerators: V100:1  # 1x NVIDIA V100 GPU&#xA;&#xA;num_nodes: 1  # Number of VMs to launch&#xA;&#xA;# Working directory (optional) containing the project codebase.&#xA;# Its contents are synced to ~/sky_workdir/ on the cluster.&#xA;workdir: ~/torch_examples&#xA;&#xA;# Commands to be run before executing the job.&#xA;# Typical use: pip install -r requirements.txt, git clone, etc.&#xA;setup: |&#xA;  pip install &#34;torch&amp;lt;2.2&#34; torchvision --index-url https://download.pytorch.org/whl/cu121&#xA;&#xA;# Commands to run as a job.&#xA;# Typical use: launch the main program.&#xA;run: |&#xA;  cd mnist&#xA;  python main.py --epochs 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Prepare the workdir by cloning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/pytorch/examples.git ~/torch_examples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch with &lt;code&gt;sky launch&lt;/code&gt; (note: &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/cloud-setup/quota.html&#34;&gt;access to GPU instances&lt;/a&gt; is needed for this example):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sky launch my_task.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;SkyPilot then performs the heavy-lifting for you, including:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find the lowest priced VM instance type across different clouds&lt;/li&gt; &#xA; &lt;li&gt;Provision the VM, with auto-failover if the cloud returned capacity errors&lt;/li&gt; &#xA; &lt;li&gt;Sync the local &lt;code&gt;workdir&lt;/code&gt; to the VM&lt;/li&gt; &#xA; &lt;li&gt;Run the task&#39;s &lt;code&gt;setup&lt;/code&gt; commands to prepare the VM for running the task&lt;/li&gt; &#xA; &lt;li&gt;Run the task&#39;s &lt;code&gt;run&lt;/code&gt; commands&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://i.imgur.com/TgamzZ2.gif&#34; alt=&#34;SkyPilot Demo&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/getting-started/quickstart.html&#34;&gt;Quickstart&lt;/a&gt; to get started with SkyPilot.&lt;/p&gt; &#xA;&lt;h2&gt;More Information&lt;/h2&gt; &#xA;&lt;p&gt;To learn more, see our &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/&#34;&gt;Documentation&lt;/a&gt; and &lt;a href=&#34;https://github.com/skypilot-org/skypilot-tutorial&#34;&gt;Tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- Keep this section in sync with index.rst in SkyPilot Docs --&gt; &#xA;&lt;p&gt;Runnable examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLMs on SkyPilot &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/gpt-2/&#34;&gt;GPT-2 via &lt;code&gt;llm.c&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/llama-3/&#34;&gt;Llama 3&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/qwen/&#34;&gt;Qwen&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/dbrx/&#34;&gt;Databricks DBRX&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/gemma/&#34;&gt;Gemma&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/mixtral/&#34;&gt;Mixtral 8x7B&lt;/a&gt;; &lt;a href=&#34;https://docs.mistral.ai/self-deployment/skypilot/&#34;&gt;Mistral 7B&lt;/a&gt; (from official Mistral team)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/codellama/&#34;&gt;Code Llama&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/vllm/&#34;&gt;vLLM: Serving LLM 24x Faster On the Cloud&lt;/a&gt; (from official vLLM team)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/sglang/&#34;&gt;SGLang: Fast and Expressive LLM Serving On the Cloud&lt;/a&gt; (from official SGLang team)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/vicuna/&#34;&gt;Vicuna chatbots: Training &amp;amp; Serving&lt;/a&gt; (from official Vicuna team)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/vicuna-llama-2/&#34;&gt;Train your own Vicuna on Llama-2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/llama-2/&#34;&gt;Self-Hosted Llama-2 Chatbot&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/ollama/&#34;&gt;Ollama: Quantized LLMs on CPUs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/lorax/&#34;&gt;LoRAX&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/artidoro/qlora/pull/132&#34;&gt;QLoRA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/zetavg/LLaMA-LoRA-Tuner#run-on-a-cloud-service-via-skypilot&#34;&gt;LLaMA-LoRA-Tuner&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/TabbyML/tabby/raw/bed723fcedb44a6b867ce22a7b1f03d2f3531c1e/experimental/eval/skypilot.yaml&#34;&gt;Tabby: Self-hosted AI coding assistant&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/localgpt&#34;&gt;LocalGPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/falcon&#34;&gt;Falcon&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Add yours here &amp;amp; see more in &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm&#34;&gt;&lt;code&gt;llm/&lt;/code&gt;&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Framework examples: &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/resnet_distributed_torch.yaml&#34;&gt;PyTorch DDP&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/examples/deepspeed-multinode/sky.yaml&#34;&gt;DeepSpeed&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/tpu/tpuvm_mnist.yaml&#34;&gt;JAX/Flax on TPU&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/tree/master/examples/stable_diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/detectron2_docker.yaml&#34;&gt;Detectron2&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/resnet_distributed_tf_app.py&#34;&gt;Distributed&lt;/a&gt; &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/resnet_app_storage.yaml&#34;&gt;TensorFlow&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/examples/distributed_ray_train/ray_train.yaml&#34;&gt;Ray Train&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/nemo/nemo.yaml&#34;&gt;NeMo&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/huggingface_glue_imdb_grid_search_app.py&#34;&gt;programmatic grid search&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/docker/echo_app.yaml&#34;&gt;Docker&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/cog/&#34;&gt;Cog&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/examples/unsloth/unsloth.yaml&#34;&gt;Unsloth&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/raw/master/llm/ollama&#34;&gt;Ollama&lt;/a&gt;, &lt;a href=&#34;https://github.com/skypilot-org/skypilot/tree/master/llm/gpt-2&#34;&gt;llm.c&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/examples&#34;&gt;many more (&lt;code&gt;examples/&lt;/code&gt;)&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Follow updates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/skypilot_org&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://slack.skypilot.co&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.skypilot.co/&#34;&gt;SkyPilot Blog&lt;/a&gt; (&lt;a href=&#34;https://blog.skypilot.co/introducing-skypilot/&#34;&gt;Introductory blog post&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Read the research:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.usenix.org/system/files/nsdi23-yang-zongheng.pdf&#34;&gt;SkyPilot paper&lt;/a&gt; and &lt;a href=&#34;https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng&#34;&gt;talk&lt;/a&gt; (NSDI 2023)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.07147&#34;&gt;Sky Computing whitepaper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s02-stoica.pdf&#34;&gt;Sky Computing vision paper&lt;/a&gt; (HotOS 2021)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.usenix.org/conference/nsdi24/presentation/wu-zhanghao&#34;&gt;Policy for Managed Spot Jobs&lt;/a&gt; (NSDI 2024)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support and Questions&lt;/h2&gt; &#xA;&lt;p&gt;We are excited to hear your feedback!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For issues and feature requests, please &lt;a href=&#34;https://github.com/skypilot-org/skypilot/issues/new&#34;&gt;open a GitHub issue&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For questions, please use &lt;a href=&#34;https://github.com/skypilot-org/skypilot/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For general discussions, join us on the &lt;a href=&#34;http://slack.skypilot.co&#34;&gt;SkyPilot Slack&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome and value all contributions to the project! Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/skypilot-org/skypilot/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; for how to get involved.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenBMB/MiniCPM-V</title>
    <updated>2024-06-09T01:46:10Z</updated>
    <id>tag:github.com,2024-06-09:/OpenBMB/MiniCPM-V</id>
    <link href="https://github.com/OpenBMB/MiniCPM-V" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MiniCPM-Llama3-V 2.5: A GPT-4V Level Multimodal LLM on Your Phone&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv.png&#34; width=&#34;300em&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;A GPT-4V Level Multimodal LLM on Your Phone&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README_zh.md&#34;&gt;中文&lt;/a&gt; | English&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Join our &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/wechat.md&#34; target=&#34;_blank&#34;&gt; 💬 WeChat&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; MiniCPM-Llama3-V 2.5 &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/&#34;&gt;🤗&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5&#34;&gt;🤖&lt;/a&gt; | MiniCPM-V 2.0 &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2/&#34;&gt;🤗&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-V-2&#34;&gt;🤖&lt;/a&gt; | &lt;a href=&#34;https://openbmb.vercel.app/minicpm-v-2-en&#34;&gt; Technical Blog &lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-V&lt;/strong&gt; is a series of end-side multimodal LLMs (MLLMs) designed for vision-language understanding. The models take image and text as inputs and provide high-quality text outputs. Since February 2024, we have released 4 versions of the model, aiming to achieve &lt;strong&gt;strong performance and efficient deployment&lt;/strong&gt;. The most notable models in this series currently include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-Llama3-V 2.5&lt;/strong&gt;: 🔥🔥🔥 The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, the model &lt;strong&gt;surpasses proprietary models such as GPT-4V-1106, Gemini Pro, Qwen-VL-Max and Claude 3&lt;/strong&gt; in overall performance. Equipped with the enhanced OCR and instruction-following capability, the model can also support multimodal conversation for &lt;strong&gt;over 30 languages&lt;/strong&gt; including English, Chinese, French, Spanish, German etc. With help of quantization, compilation optimizations, and several efficient inference techniques on CPUs and NPUs, MiniCPM-Llama3-V 2.5 can be &lt;strong&gt;efficiently deployed on end-side devices&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 2.0&lt;/strong&gt;: The lightest model in the MiniCPM-V series. With 2B parameters, it surpasses larger models such as Yi-VL 34B, CogVLM-Chat 17B, and Qwen-VL-Chat 10B in overall performance. It can accept image inputs of any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), achieving comparable performance with Gemini Pro in understanding scene-text and matches GPT-4V in low hallucination rates.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;📌 Pinned&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024.05.28] 🚀🚀🚀 MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and ollama! Please pull the latest code &lt;strong&gt;of our provided forks&lt;/strong&gt; (&lt;a href=&#34;https://github.com/OpenBMB/llama.cpp/raw/minicpm-v2.5/examples/minicpmv/README.md&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5&#34;&gt;ollama&lt;/a&gt;). GGUF models in various sizes are available &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main&#34;&gt;here&lt;/a&gt;. We are working hard to merge PRs into official repositories. Please stay tuned!&lt;/li&gt; &#xA; &lt;li&gt;[2024.05.28] 💫 We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024.05.23] 🔍 We&#39;ve released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmarks evaluations, multilingual capabilities, and inference efficiency 🌟📊🌍🚀. Click &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/compare_with_phi-3_vision.md&#34;&gt;here&lt;/a&gt; to view more details.&lt;/li&gt; &#xA; &lt;li&gt;[2024.05.23] 🔥🔥🔥 MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradio’s official account, is available &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5&#34;&gt;here&lt;/a&gt;. Come and try it out!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model&#39;s layers across multiple GPUs. For more details, Check this &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md&#34;&gt;link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage&#34;&gt;here&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[2024.05.24] We release the MiniCPM-Llama3-V 2.5 &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf&#34;&gt;gguf&lt;/a&gt;, which supports &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-llamacpp&#34;&gt;llama.cpp&lt;/a&gt; inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!&lt;/li&gt; &#xA; &lt;li&gt;[2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#deployment-on-mobile-phone&#34;&gt;efficient inference&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md&#34;&gt;simple fine-tuning&lt;/a&gt;. Try it now!&lt;/li&gt; &#xA; &lt;li&gt;[2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#vllm&#34;&gt;here&lt;/a&gt; to view more details.&lt;/li&gt; &#xA; &lt;li&gt;[2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-V-2&#34;&gt;here&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[2024.04.17] MiniCPM-V-2.0 supports deploying &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#webui-demo&#34;&gt;WebUI Demo&lt;/a&gt; now!&lt;/li&gt; &#xA; &lt;li&gt;[2024.04.15] MiniCPM-V-2.0 now also supports &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/li&gt; &#xA; &lt;li&gt;[2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href=&#34;https://rank.opencompass.org.cn/leaderboard-multimodal&#34;&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href=&#34;https://openbmb.vercel.app/minicpm-v-2&#34;&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.&lt;/li&gt; &#xA; &lt;li&gt;[2024.03.14] MiniCPM-V now supports &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;fine-tuning&lt;/a&gt; with the SWIFT framework. Thanks to &lt;a href=&#34;https://github.com/Jintao-Huang&#34;&gt;Jintao&lt;/a&gt; for the contribution！&lt;/li&gt; &#xA; &lt;li&gt;[2024.03.01] MiniCPM-V now can be deployed on Mac!&lt;/li&gt; &#xA; &lt;li&gt;[2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-llama3-v-25&#34;&gt;MiniCPM-Llama3-V 2.5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v-20&#34;&gt;MiniCPM-V 2.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio&#34;&gt;Chat with Our Demo on Gradio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference&#34;&gt;Inference&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#model-zoo&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multi-turn-conversation&#34;&gt;Multi-turn Conversation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-mac&#34;&gt;Inference on Mac&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#deployment-on-mobile-phone&#34;&gt;Deployment on Mobile Phone&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-llamacpp&#34;&gt;Inference with llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm&#34;&gt;Inference with vLLM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#todo&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#-star-history&#34;&gt;🌟 Star History&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MiniCPM-Llama3-V 2.5&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-Llama3-V 2.5&lt;/strong&gt; is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Llama3-8B-Instruct with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.0. Notable features of MiniCPM-Llama3-V 2.5 include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;Leading Performance.&lt;/strong&gt; MiniCPM-Llama3-V 2.5 has achieved an average score of 65.1 on OpenCompass, a comprehensive evaluation over 11 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4V-1106, Gemini Pro, Claude 3 and Qwen-VL-Max&lt;/strong&gt; and greatly outperforms other Llama 3-based MLLMs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;💪 &lt;strong&gt;Strong OCR Capabilities.&lt;/strong&gt; MiniCPM-Llama3-V 2.5 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), achieving a &lt;strong&gt;700+ score on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V-0409, Qwen-VL-Max and Gemini Pro&lt;/strong&gt;. Based on recent user feedback, MiniCPM-Llama3-V 2.5 has now enhanced full-text OCR extraction, table-to-markdown conversion, and other high-utility capabilities, and has further strengthened its instruction-following and complex reasoning abilities, enhancing multimodal interaction experiences.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🏆 &lt;strong&gt;Trustworthy Behavior.&lt;/strong&gt; Leveraging the latest &lt;a href=&#34;https://github.com/RLHF-V/RLAIF-V/&#34;&gt;RLAIF-V&lt;/a&gt; method (the newest technique in the &lt;a href=&#34;https://github.com/RLHF-V&#34;&gt;RLHF-V&lt;/a&gt; [CVPR&#39;24] series), MiniCPM-Llama3-V 2.5 exhibits more trustworthy behavior. It achieves a &lt;strong&gt;10.3%&lt;/strong&gt; hallucination rate on Object HalBench, lower than GPT-4V-1106 (13.6%), achieving the best-level performance within the open-source community. &lt;a href=&#34;https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset&#34;&gt;Data released&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🌏 &lt;strong&gt;Multilingual Support.&lt;/strong&gt; Thanks to the strong multilingual capabilities of Llama 3 and the cross-lingual generalization technique from &lt;a href=&#34;https://github.com/OpenBMB/VisCPM&#34;&gt;VisCPM&lt;/a&gt;, MiniCPM-Llama3-V 2.5 extends its bilingual (Chinese-English) multimodal capabilities to &lt;strong&gt;over 30 languages including German, French, Spanish, Italian, Korean etc.&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-llama-v-2-5_languages.md&#34;&gt;All Supported Languages&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🚀 &lt;strong&gt;Efficient Deployment.&lt;/strong&gt; MiniCPM-Llama3-V 2.5 systematically employs &lt;strong&gt;model quantization, CPU optimizations, NPU optimizations and compilation optimizations&lt;/strong&gt;, achieving high-efficiency deployment on end-side devices. For mobile phones with Qualcomm chips, we have integrated the NPU acceleration framework QNN into llama.cpp for the first time. After systematic optimization, MiniCPM-Llama3-V 2.5 has realized a &lt;strong&gt;150x acceleration in end-side MLLM image encoding&lt;/strong&gt; and a &lt;strong&gt;3x speedup in language decoding&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;💫 &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-Llama3-V 2.5 can be easily used in various ways: (1) &lt;a href=&#34;https://github.com/OpenBMB/llama.cpp/raw/minicpm-v2.5/examples/minicpmv/README.md&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5&#34;&gt;ollama&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf&#34;&gt;GGUF&lt;/a&gt; format quantized models in 16 sizes, (3) efficient &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#lora-finetuning&#34;&gt;LoRA&lt;/a&gt; fine-tuning with only 2 V100 GPUs, (4) &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage&#34;&gt;streaming output&lt;/a&gt;, (5) quick local WebUI demo setup with &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V/raw/main/web_demo_2.5.py&#34;&gt;Gradio&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V/raw/main/web_demo_streamlit-2_5.py&#34;&gt;Streamlit&lt;/a&gt;, and (6) interactive demos on &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5&#34;&gt;HuggingFace Spaces&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evaluation &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/MiniCPM-Llama3-V-2.5-peformance.png&#34; width=&#34;66%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view results on TextVQA, DocVQA, OCRBench, OpenCompass, MME, MMBench, MMMU, MathVista, LLaVA Bench, RealWorld QA, Object HalBench. &lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;Size&lt;/th&gt; &#xA;     &lt;th&gt;OCRBench&lt;/th&gt; &#xA;     &lt;th&gt;TextVQA val&lt;/th&gt; &#xA;     &lt;th&gt;DocVQA test&lt;/th&gt; &#xA;     &lt;th&gt;Open-Compass&lt;/th&gt; &#xA;     &lt;th&gt;MME&lt;/th&gt; &#xA;     &lt;th&gt;MMB test (en)&lt;/th&gt; &#xA;     &lt;th&gt;MMB test (cn)&lt;/th&gt; &#xA;     &lt;th&gt;MMMU val&lt;/th&gt; &#xA;     &lt;th&gt;Math-Vista&lt;/th&gt; &#xA;     &lt;th&gt;LLaVA Bench&lt;/th&gt; &#xA;     &lt;th&gt;RealWorld QA&lt;/th&gt; &#xA;     &lt;th&gt;Object HalBench&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody align=&#34;center&#34;&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;14&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Gemini Pro&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;680&lt;/td&gt; &#xA;     &lt;td&gt;74.6&lt;/td&gt; &#xA;     &lt;td&gt;88.1&lt;/td&gt; &#xA;     &lt;td&gt;62.9&lt;/td&gt; &#xA;     &lt;td&gt;2148.9&lt;/td&gt; &#xA;     &lt;td&gt;73.6&lt;/td&gt; &#xA;     &lt;td&gt;74.3&lt;/td&gt; &#xA;     &lt;td&gt;48.9&lt;/td&gt; &#xA;     &lt;td&gt;45.8&lt;/td&gt; &#xA;     &lt;td&gt;79.9&lt;/td&gt; &#xA;     &lt;td&gt;60.4&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT-4V (2023.11.06)&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;645&lt;/td&gt; &#xA;     &lt;td&gt;78.0&lt;/td&gt; &#xA;     &lt;td&gt;88.4&lt;/td&gt; &#xA;     &lt;td&gt;63.5&lt;/td&gt; &#xA;     &lt;td&gt;1771.5&lt;/td&gt; &#xA;     &lt;td&gt;77.0&lt;/td&gt; &#xA;     &lt;td&gt;74.4&lt;/td&gt; &#xA;     &lt;td&gt;53.8&lt;/td&gt; &#xA;     &lt;td&gt;47.8&lt;/td&gt; &#xA;     &lt;td&gt;93.1&lt;/td&gt; &#xA;     &lt;td&gt;63.0&lt;/td&gt; &#xA;     &lt;td&gt;86.4&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;14&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Mini-Gemini&lt;/td&gt; &#xA;     &lt;td&gt;2.2B&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;56.2&lt;/td&gt; &#xA;     &lt;td&gt;34.2*&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1653.0&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;31.7&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;     &lt;td&gt;9.6B&lt;/td&gt; &#xA;     &lt;td&gt;488&lt;/td&gt; &#xA;     &lt;td&gt;61.5&lt;/td&gt; &#xA;     &lt;td&gt;62.6&lt;/td&gt; &#xA;     &lt;td&gt;51.6&lt;/td&gt; &#xA;     &lt;td&gt;1860.0&lt;/td&gt; &#xA;     &lt;td&gt;61.8&lt;/td&gt; &#xA;     &lt;td&gt;56.3&lt;/td&gt; &#xA;     &lt;td&gt;37.0&lt;/td&gt; &#xA;     &lt;td&gt;33.8&lt;/td&gt; &#xA;     &lt;td&gt;67.7&lt;/td&gt; &#xA;     &lt;td&gt;49.3&lt;/td&gt; &#xA;     &lt;td&gt;56.2&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;DeepSeek-VL-7B&lt;/td&gt; &#xA;     &lt;td&gt;7.3B&lt;/td&gt; &#xA;     &lt;td&gt;435&lt;/td&gt; &#xA;     &lt;td&gt;64.7*&lt;/td&gt; &#xA;     &lt;td&gt;47.0*&lt;/td&gt; &#xA;     &lt;td&gt;54.6&lt;/td&gt; &#xA;     &lt;td&gt;1765.4&lt;/td&gt; &#xA;     &lt;td&gt;73.8&lt;/td&gt; &#xA;     &lt;td&gt;71.4&lt;/td&gt; &#xA;     &lt;td&gt;38.3&lt;/td&gt; &#xA;     &lt;td&gt;36.8&lt;/td&gt; &#xA;     &lt;td&gt;77.8&lt;/td&gt; &#xA;     &lt;td&gt;54.2&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Yi-VL-34B&lt;/td&gt; &#xA;     &lt;td&gt;34B&lt;/td&gt; &#xA;     &lt;td&gt;290&lt;/td&gt; &#xA;     &lt;td&gt;43.4*&lt;/td&gt; &#xA;     &lt;td&gt;16.9*&lt;/td&gt; &#xA;     &lt;td&gt;52.2&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;2050.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;72.4&lt;/td&gt; &#xA;     &lt;td&gt;70.7&lt;/td&gt; &#xA;     &lt;td&gt;45.1&lt;/td&gt; &#xA;     &lt;td&gt;30.7&lt;/td&gt; &#xA;     &lt;td&gt;62.3&lt;/td&gt; &#xA;     &lt;td&gt;54.8&lt;/td&gt; &#xA;     &lt;td&gt;79.3&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;CogVLM-Chat&lt;/td&gt; &#xA;     &lt;td&gt;17.4B&lt;/td&gt; &#xA;     &lt;td&gt;590&lt;/td&gt; &#xA;     &lt;td&gt;70.4&lt;/td&gt; &#xA;     &lt;td&gt;33.3*&lt;/td&gt; &#xA;     &lt;td&gt;54.2&lt;/td&gt; &#xA;     &lt;td&gt;1736.6&lt;/td&gt; &#xA;     &lt;td&gt;65.8&lt;/td&gt; &#xA;     &lt;td&gt;55.9&lt;/td&gt; &#xA;     &lt;td&gt;37.3&lt;/td&gt; &#xA;     &lt;td&gt;34.7&lt;/td&gt; &#xA;     &lt;td&gt;73.9&lt;/td&gt; &#xA;     &lt;td&gt;60.3&lt;/td&gt; &#xA;     &lt;td&gt;73.6&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;TextMonkey&lt;/td&gt; &#xA;     &lt;td&gt;9.7B&lt;/td&gt; &#xA;     &lt;td&gt;558&lt;/td&gt; &#xA;     &lt;td&gt;64.3&lt;/td&gt; &#xA;     &lt;td&gt;66.7&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Idefics2&lt;/td&gt; &#xA;     &lt;td&gt;8.0B&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;73.0&lt;/td&gt; &#xA;     &lt;td&gt;74.0&lt;/td&gt; &#xA;     &lt;td&gt;57.2&lt;/td&gt; &#xA;     &lt;td&gt;1847.6&lt;/td&gt; &#xA;     &lt;td&gt;75.7&lt;/td&gt; &#xA;     &lt;td&gt;68.6&lt;/td&gt; &#xA;     &lt;td&gt;45.2&lt;/td&gt; &#xA;     &lt;td&gt;52.2&lt;/td&gt; &#xA;     &lt;td&gt;49.1&lt;/td&gt; &#xA;     &lt;td&gt;60.7&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Bunny-LLama-3-8B&lt;/td&gt; &#xA;     &lt;td&gt;8.4B&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;54.3&lt;/td&gt; &#xA;     &lt;td&gt;1920.3&lt;/td&gt; &#xA;     &lt;td&gt;77.0&lt;/td&gt; &#xA;     &lt;td&gt;73.9&lt;/td&gt; &#xA;     &lt;td&gt;41.3&lt;/td&gt; &#xA;     &lt;td&gt;31.5&lt;/td&gt; &#xA;     &lt;td&gt;61.2&lt;/td&gt; &#xA;     &lt;td&gt;58.8&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-NeXT Llama-3-8B&lt;/td&gt; &#xA;     &lt;td&gt;8.4B&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;78.2&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1971.5&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;41.7&lt;/td&gt; &#xA;     &lt;td&gt;37.5&lt;/td&gt; &#xA;     &lt;td&gt;80.1&lt;/td&gt; &#xA;     &lt;td&gt;60.0&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Phi-3-vision-128k-instruct&lt;/td&gt; &#xA;     &lt;td&gt;4.2B&lt;/td&gt; &#xA;     &lt;td&gt;639*&lt;/td&gt; &#xA;     &lt;td&gt;70.9&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1537.5*&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;40.4&lt;/td&gt; &#xA;     &lt;td&gt;44.5&lt;/td&gt; &#xA;     &lt;td&gt;64.2*&lt;/td&gt; &#xA;     &lt;td&gt;58.8*&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr style=&#34;background-color: #e6f2ff;&#34;&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V 1.0&lt;/td&gt; &#xA;     &lt;td&gt;2.8B&lt;/td&gt; &#xA;     &lt;td&gt;366&lt;/td&gt; &#xA;     &lt;td&gt;60.6&lt;/td&gt; &#xA;     &lt;td&gt;38.2&lt;/td&gt; &#xA;     &lt;td&gt;47.5&lt;/td&gt; &#xA;     &lt;td&gt;1650.2&lt;/td&gt; &#xA;     &lt;td&gt;64.1&lt;/td&gt; &#xA;     &lt;td&gt;62.6&lt;/td&gt; &#xA;     &lt;td&gt;38.3&lt;/td&gt; &#xA;     &lt;td&gt;28.9&lt;/td&gt; &#xA;     &lt;td&gt;51.3&lt;/td&gt; &#xA;     &lt;td&gt;51.2&lt;/td&gt; &#xA;     &lt;td&gt;78.4&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr style=&#34;background-color: #e6f2ff;&#34;&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V 2.0&lt;/td&gt; &#xA;     &lt;td&gt;2.8B&lt;/td&gt; &#xA;     &lt;td&gt;605&lt;/td&gt; &#xA;     &lt;td&gt;74.1&lt;/td&gt; &#xA;     &lt;td&gt;71.9&lt;/td&gt; &#xA;     &lt;td&gt;54.5&lt;/td&gt; &#xA;     &lt;td&gt;1808.6&lt;/td&gt; &#xA;     &lt;td&gt;69.1&lt;/td&gt; &#xA;     &lt;td&gt;66.5&lt;/td&gt; &#xA;     &lt;td&gt;38.2&lt;/td&gt; &#xA;     &lt;td&gt;38.7&lt;/td&gt; &#xA;     &lt;td&gt;69.2&lt;/td&gt; &#xA;     &lt;td&gt;55.8&lt;/td&gt; &#xA;     &lt;td&gt;85.5&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr style=&#34;background-color: #e6f2ff;&#34;&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-Llama3-V 2.5&lt;/td&gt; &#xA;     &lt;td&gt;8.5B&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;725&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;76.6&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;84.8&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;65.1&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;2024.6&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;77.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;74.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;45.8&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;54.3&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;86.7&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;63.5&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;89.7&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt; * We evaluate the officially released checkpoint by ourselves. &#xA;&lt;/details&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/llavabench_compare_3.png&#34; width=&#34;100%&#34;&gt; &#xA; &lt;br&gt; Evaluation results of multilingual LLaVA Bench &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Examples &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv-llama3-v2.5/cases_all.png&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We deploy MiniCPM-Llama3-V 2.5 on end devices. The demo video is the raw screen recording on a Xiaomi 14 Pro without edition.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/ticket.gif&#34; width=&#34;32%/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/meal_plan.gif&#34; width=&#34;32%/&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/1-4.gif&#34; width=&#34;64%/&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;MiniCPM-V 2.0&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view more details of MiniCPM-V 2.0&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;MiniCPM-V 2.0&lt;/strong&gt; is an efficient version with promising performance for deployment. The model is built based on SigLip-400M and &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/&#34;&gt;MiniCPM-2.4B&lt;/a&gt;, connected by a perceiver resampler. Our latest version, MiniCPM-V 2.0 has several notable features.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;State-of-the-art Performance.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; on multiple benchmarks (including OCRBench, TextVQA, MME, MMB, MathVista, etc) among models under 7B parameters. It even &lt;strong&gt;outperforms strong Qwen-VL-Chat 9.6B, CogVLM-Chat 17.4B, and Yi-VL 34B on OpenCompass, a comprehensive evaluation over 11 popular benchmarks&lt;/strong&gt;. Notably, MiniCPM-V 2.0 shows &lt;strong&gt;strong OCR capability&lt;/strong&gt;, achieving &lt;strong&gt;comparable performance to Gemini Pro in scene-text understanding&lt;/strong&gt;, and &lt;strong&gt;state-of-the-art performance on OCRBench&lt;/strong&gt; among open-source models.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;🏆 &lt;strong&gt;Trustworthy Behavior.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LMMs are known for suffering from hallucination, often generating text not factually grounded in images. MiniCPM-V 2.0 is &lt;strong&gt;the first end-side LMM aligned via multimodal RLHF for trustworthy behavior&lt;/strong&gt; (using the recent &lt;a href=&#34;https://rlhf-v.github.io/&#34;&gt;RLHF-V&lt;/a&gt; [CVPR&#39;24] series technique). This allows the model to &lt;strong&gt;match GPT-4V in preventing hallucinations&lt;/strong&gt; on Object HalBench.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;🌟 &lt;strong&gt;High-Resolution Images at Any Aspect Raito.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 can accept &lt;strong&gt;1.8 million pixels (e.g., 1344x1344) images at any aspect ratio&lt;/strong&gt;. This enables better perception of fine-grained visual information such as small objects and optical characters, which is achieved via a recent technique from &lt;a href=&#34;https://arxiv.org/pdf/2403.11703.pdf&#34;&gt;LLaVA-UHD&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;⚡️ &lt;strong&gt;High Efficiency.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 can be &lt;strong&gt;efficiently deployed on most GPU cards and personal computers&lt;/strong&gt;, and &lt;strong&gt;even on end devices such as mobile phones&lt;/strong&gt;. For visual encoding, we compress the image representations into much fewer tokens via a perceiver resampler. This allows MiniCPM-V 2.0 to operate with &lt;strong&gt;favorable memory cost and speed during inference even when dealing with high-resolution images&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;🙌 &lt;strong&gt;Bilingual Support.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 &lt;strong&gt;supports strong bilingual multimodal capabilities in both English and Chinese&lt;/strong&gt;. This is enabled by generalizing multimodal capabilities across languages, a technique from &lt;a href=&#34;https://arxiv.org/abs/2308.12038&#34;&gt;VisCPM&lt;/a&gt; [ICLR&#39;24].&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Examples &#xA;  &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv2-cases_2.png&#34; width=&#34;95%/&#34;&gt; &lt;/p&gt;&#xA; &lt;table align=&#34;center&#34;&gt;  &#xA; &lt;/table&gt; &#xA; &lt;p&gt;We deploy MiniCPM-V 2.0 on end devices. The demo video is the raw screen recording on a Xiaomi 14 Pro without edition.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/station.gif&#34; width=&#34;36%/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/london_car.gif&#34; width=&#34;36%/&#34;&gt; &lt;/p&gt;&#xA; &lt;table align=&#34;center&#34;&gt;  &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Legacy Models &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Introduction and Guidance&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/minicpm_v1.md&#34;&gt;Document&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OmniLMM-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/omnilmm_en.md&#34;&gt;Document&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Chat with Our Demo on Gradio&lt;/h2&gt; &#xA;&lt;p&gt;We provide online and local demos powered by HuggingFace &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;, the most popular model deployment framework nowadays. It supports streaming outputs, progress bars, queuing, alerts, and other useful features.&lt;/p&gt; &#xA;&lt;h3&gt;Online Demo &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Click here to try out the online demo of &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5&#34;&gt;MiniCPM-Llama3-V 2.5&lt;/a&gt; ｜ &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-V-2&#34;&gt;MiniCPM-V 2.0&lt;/a&gt; on HuggingFace Spaces.&lt;/p&gt; &#xA;&lt;h3&gt;Local WebUI Demo &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can easily build your own local WebUI demo with Gradio using the following commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# For NVIDIA GPUs, run:&#xA;python web_demo_2.5.py --device cuda&#xA;&#xA;# For Mac with MPS (Apple silicon or AMD GPUs), run:&#xA;PYTORCH_ENABLE_MPS_FALLBACK=1 python web_demo_2.5.py --device mps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to the source folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/OpenBMB/MiniCPM-V.git&#xA;cd MiniCPM-V&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create conda environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n MiniCPM-V python=3.10 -y&#xA;conda activate MiniCPM-V&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Model Zoo&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Device&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Memory&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;         Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-Llama3-V 2.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The lastest version, achieving state-of-the end-side multimodal performance.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-Llama3-V 2.5 gguf&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5-gguf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-Llama3-V 2.5 int4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The int4 quantized version，lower GPU memory usage.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4/&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-Llama3-V-2_5-int4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Light version, balance the performance the computation cost.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V-2&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Lightest version, achieving the fastest inference.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Multi-turn Conversation&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to the following codes to run.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/airplane.jpeg&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from chat import MiniCPMVChat, img2base64&#xA;import torch&#xA;import json&#xA;&#xA;torch.manual_seed(0)&#xA;&#xA;chat_model = MiniCPMVChat(&#39;openbmb/MiniCPM-Llama3-V-2_5&#39;)&#xA;&#xA;im_64 = img2base64(&#39;./assets/airplane.jpeg&#39;)&#xA;&#xA;# First round chat &#xA;msgs = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me the model of this aircraft.&#34;}]&#xA;&#xA;inputs = {&#34;image&#34;: im_64, &#34;question&#34;: json.dumps(msgs)}&#xA;answer = chat_model.chat(inputs)&#xA;print(answer)&#xA;&#xA;# Second round chat &#xA;# pass history context of multi-turn conversation&#xA;msgs.append({&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: answer})&#xA;msgs.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Introduce something about Airbus A380.&#34;})&#xA;&#xA;inputs = {&#34;image&#34;: im_64, &#34;question&#34;: json.dumps(msgs)}&#xA;answer = chat_model.chat(inputs)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will get the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;The aircraft in the image is an Airbus A380, which can be identified by its large size, double-deck structure, and the distinctive shape of its wings and engines. The A380 is a wide-body aircraft known for being the world&#39;s largest passenger airliner, designed for long-haul flights. It has four engines, which are characteristic of large commercial aircraft. The registration number on the aircraft can also provide specific information about the model if looked up in an aviation database.&#34;&#xA;&#xA;&#34;The Airbus A380 is a double-deck, wide-body, four-engine jet airliner made by Airbus. It is the world&#39;s largest passenger airliner and is known for its long-haul capabilities. The aircraft was developed to improve efficiency and comfort for passengers traveling over long distances. It has two full-length passenger decks, which can accommodate more passengers than a typical single-aisle airplane. The A380 has been operated by airlines such as Lufthansa, Singapore Airlines, and Emirates, among others. It is widely recognized for its unique design and significant impact on the aviation industry.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference on Mac&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view an example, to run MiniCPM-Llama3-V 2.5 on 💻 Mac with MPS (Apple silicon or AMD GPUs). &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# test.py  Need more than 16GB memory.&#xA;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-Llama3-V-2_5&#39;, trust_remote_code=True, low_cpu_mem_usage=True)&#xA;model = model.to(device=&#39;mps&#39;)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-Llama3-V-2_5&#39;, trust_remote_code=True)&#xA;model.eval()&#xA;&#xA;image = Image.open(&#39;./assets/hk_OCR.jpg&#39;).convert(&#39;RGB&#39;)&#xA;question = &#39;Where is this photo taken?&#39;&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: question}]&#xA;&#xA;answer, context, _ = model.chat(&#xA;    image=image,&#xA;    msgs=msgs,&#xA;    context=None,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Run with command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Deployment on Mobile Phone&lt;/h3&gt; &#xA;&lt;p&gt;MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0 can be deployed on mobile phones with Android operating systems. 🚀 Click &lt;a href=&#34;http://minicpm.modelbest.cn/android/modelbest-release-20240528_182155.apk&#34;&gt;MiniCPM-Llama3-V 2.5&lt;/a&gt; / &lt;a href=&#34;https://github.com/OpenBMB/mlc-MiniCPM&#34;&gt;MiniCPM-V 2.0&lt;/a&gt; to install apk.&lt;/p&gt; &#xA;&lt;h3&gt;Inference with llama.cpp&lt;a id=&#34;inference-with-llamacpp&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;MiniCPM-Llama3-V 2.5 can run with llama.cpp now! See our fork of &lt;a href=&#34;https://github.com/OpenBMB/llama.cpp/tree/minicpm-v2.5/examples/minicpmv&#34;&gt;llama.cpp&lt;/a&gt; for more detail. This implementation supports smooth inference of 6~8 token/s on mobile phones (test environment：Xiaomi 14 pro + Snapdragon 8 Gen 3).&lt;/p&gt; &#xA;&lt;h3&gt;Inference with vLLM&lt;a id=&#34;vllm&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see how to inference MiniCPM-V 2.0 with vLLM (MiniCPM-Llama3-V 2.5 coming soon) &lt;/summary&gt; Because our pull request to vLLM is still waiting for reviewing, we fork this repository to build and test our vLLM demo. Here are the steps: &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Clone our version of vLLM:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/OpenBMB/vllm.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Install vLLM:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd vllm&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Install timm:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install timm=0.9.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Run our demo:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python examples/minicpmv_example.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;h3&gt;Simple Fine-tuning &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We support simple fine-tuning with Hugging Face for MiniCPM-V 2.0 and MiniCPM-Llama3-V 2.5.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md&#34;&gt;Reference Document&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;With the SWIFT Framework &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We now support MiniCPM-V series fine-tuning with the SWIFT framework. SWIFT supports training, inference, evaluation and deployment of nearly 200 LLMs and MLLMs . It supports the lightweight training solutions provided by PEFT and a complete Adapters Library including techniques such as NEFTune, LoRA+ and LLaMA-PRO.&lt;/p&gt; &#xA;&lt;p&gt;Best Practices：&lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;MiniCPM-V 1.0&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;MiniCPM-V 2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; MiniCPM-V fine-tuning support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Code release for real-time interactive assistant&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model License &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;This repository is released under the &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/raw/main/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt; License.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The usage of MiniCPM-V model weights must strictly follow &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md&#34;&gt;MiniCPM Model License.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The models and weights of MiniCPM are completely free for academic research. after filling out a &lt;a href=&#34;https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g&#34;&gt;&#34;questionnaire&#34;&lt;/a&gt; for registration, are also available for free commercial use.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Statement &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;As LMMs, MiniCPM-V models (including OmniLMM) generate contents by learning a large amount of multimodal corpora, but they cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V models does not represent the views and positions of the model developers&lt;/p&gt; &#xA;&lt;p&gt;We will not be liable for any problems arising from the use of MiniCPMV-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.&lt;/p&gt; &#xA;&lt;h2&gt;Institutions &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://nlp.csai.tsinghua.edu.cn/&#34;&gt;THUNLP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://modelbest.cn/&#34;&gt;ModelBest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/zhihu.webp&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://www.zhihu.com/&#34;&gt;Zhihu&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other Multimodal Projects from Our Team &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;👏 Welcome to explore other multimodal projects of our team:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenBMB/VisCPM/tree/main&#34;&gt;VisCPM&lt;/a&gt; | &lt;a href=&#34;https://github.com/RLHF-V/RLHF-V&#34;&gt;RLHF-V&lt;/a&gt; | &lt;a href=&#34;https://github.com/thunlp/LLaVA-UHD&#34;&gt;LLaVA-UHD&lt;/a&gt; | &lt;a href=&#34;https://github.com/RLHF-V/RLAIF-V&#34;&gt;RLAIF-V&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🌟 Star History&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/star_history.svg?sanitize=true&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;!-- &lt;picture&gt;&#xA;  &lt;source&#xA;    media=&#34;(prefers-color-scheme: dark)&#34;&#xA;    srcset=&#34;&#xA;      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&amp;type=Date&amp;theme=dark&#xA;    &#34;&#xA;  /&gt;&#xA;  &lt;source&#xA;    media=&#34;(prefers-color-scheme: light)&#34;&#xA;    srcset=&#34;&#xA;      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&amp;type=Date&#xA;    &#34;&#xA;  /&gt;&#xA;  &lt;img&#xA;    alt=&#34;Star History Chart&#34;&#xA;    src=&#34;https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-V&amp;type=Date&#34;&#xA;  /&gt;&#xA;&lt;/picture&gt; --&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our model/code/paper helpful, please consider cite our papers 📝 and star us ⭐️！&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@article{yu2023rlhf,&#xA;  title={Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},&#xA;  author={Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},&#xA;  journal={arXiv preprint arXiv:2312.00849},&#xA;  year={2023}&#xA;}&#xA;@article{viscpm,&#xA;    title={Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages}, &#xA;    author={Jinyi Hu and Yuan Yao and Chongyi Wang and Shan Wang and Yinxu Pan and Qianyu Chen and Tianyu Yu and Hanghao Wu and Yue Zhao and Haoye Zhang and Xu Han and Yankai Lin and Jiao Xue and Dahai Li and Zhiyuan Liu and Maosong Sun},&#xA;    journal={arXiv preprint arXiv:2308.12038},&#xA;    year={2023}&#xA;}&#xA;@article{xu2024llava-uhd,&#xA;  title={{LLaVA-UHD}: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},&#xA;  author={Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Huang, Gao},&#xA;  journal={arXiv preprint arXiv:2403.11703},&#xA;  year={2024}&#xA;}&#xA;@article{yu2024rlaifv,&#xA;  title={RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness}, &#xA;  author={Yu, Tianyu and Zhang, Haoye and Yao, Yuan and Dang, Yunkai and Chen, Da and Lu, Xiaoman and Cui, Ganqu and He, Taiwen and Liu, Zhiyuan and Chua, Tat-Seng and Sun, Maosong},&#xA;  journal={arXiv preprint arXiv:2405.17220},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>kholia/OSX-KVM</title>
    <updated>2024-06-09T01:46:10Z</updated>
    <id>tag:github.com,2024-06-09:/kholia/OSX-KVM</id>
    <link href="https://github.com/kholia/OSX-KVM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run macOS on QEMU/KVM. With OpenCore + Monterey + Ventura + Sonoma support now! Only commercial (paid) support is available now to avoid spammy issues. No Mac system is required.&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Note&lt;/h3&gt; &#xA;&lt;p&gt;This &lt;code&gt;README.md&lt;/code&gt; documents the process of creating a &lt;code&gt;Virtual Hackintosh&lt;/code&gt; system.&lt;/p&gt; &#xA;&lt;p&gt;Note: All blobs and resources included in this repository are re-derivable (all instructions are included!).&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;💚&lt;/span&gt; Looking for &lt;strong&gt;commercial&lt;/strong&gt; support with this stuff? I am &lt;a href=&#34;mailto:dhiru.kholia@gmail.com?subject=%5BGitHub%5D%20OSX-KVM%20Commercial%20Support%20Request&amp;amp;body=Hi%20-%20We%20are%20interested%20in%20purchasing%20commercial%20support%20options%20for%20your%20project.&#34;&gt;available over email&lt;/a&gt; for a chat for &lt;strong&gt;commercial support options only&lt;/strong&gt;. Note: Project sponsors get access to the &lt;code&gt;Private OSX-KVM&lt;/code&gt; repository, and direct support.&lt;/p&gt; &#xA;&lt;p&gt;Struggling with &lt;code&gt;Content Caching&lt;/code&gt; stuff? We can help.&lt;/p&gt; &#xA;&lt;p&gt;Working with &lt;code&gt;Proxmox&lt;/code&gt; and macOS? See &lt;a href=&#34;https://www.nicksherlock.com/&#34;&gt;Nick&#39;s blog for sure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Yes, we support offline macOS installations now - see &lt;a href=&#34;https://raw.githubusercontent.com/kholia/OSX-KVM/master/run_offline.md&#34;&gt;this document&lt;/a&gt; 🎉&lt;/p&gt; &#xA;&lt;h3&gt;Contributing Back&lt;/h3&gt; &#xA;&lt;p&gt;This project can always use your help, time and attention. I am looking for help (pull-requests!) with the following work items:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Documentation around running macOS on popular cloud providers (Hetzner, GCP, AWS). See the &lt;code&gt;Is This Legal?&lt;/code&gt; section and associated references.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Document (share) how you use this project to build + test open-source projects / get your stuff done.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Document how to use this project for XNU kernel debugging and development.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Document the process to launch a bunch of headless macOS VMs (build farm).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Document usage of &lt;a href=&#34;https://github.com/munki/munki&#34;&gt;munki&lt;/a&gt; to deploy software to such a &lt;code&gt;build farm&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enable VNC + SSH support out of the box or more easily.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Robustness improvements are always welcome!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Not so) crazy idea - automate the macOS installation via OpenCV.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;A modern Linux distribution. E.g. Ubuntu 22.04 LTS 64-bit or later.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;QEMU &amp;gt;= 6.2.0&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A CPU with Intel VT-x / AMD SVM support is required (&lt;code&gt;grep -e vmx -e svm /proc/cpuinfo&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A CPU with SSE4.1 support is required for &amp;gt;= macOS Sierra&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A CPU with AVX2 support is required for &amp;gt;= macOS Mojave&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: Older AMD CPU(s) are known to be problematic but modern AMD Ryzen processors work just fine (even for macOS Sonoma).&lt;/p&gt; &#xA;&lt;h3&gt;Installation Preparation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Install QEMU and other packages.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt-get install qemu uml-utilities virt-manager git \&#xA;    wget libguestfs-tools p7zip-full make dmg2img tesseract-ocr \&#xA;    tesseract-ocr-eng genisoimage vim net-tools screen -y&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This step may need to be adapted for your Linux distribution.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this repository on your QEMU system. Files from this repository are used in the following steps.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd ~&#xA;&#xA;git clone --depth 1 --recursive https://github.com/kholia/OSX-KVM.git&#xA;&#xA;cd OSX-KVM&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Repository updates can be pulled via the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git pull --rebase&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This repository uses rebase based workflows heavily.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;KVM may need the following tweak on the host machine to work.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo modprobe kvm; echo 1 | sudo tee /sys/module/kvm/parameters/ignore_msrs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To make this change permanent, you may use the following command.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo cp kvm.conf /etc/modprobe.d/kvm.conf  # for intel boxes only&#xA;&#xA;sudo cp kvm_amd.conf /etc/modprobe.d/kvm.conf  # for amd boxes only&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add user to the &lt;code&gt;kvm&lt;/code&gt; and &lt;code&gt;libvirt&lt;/code&gt; groups (might be needed).&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo usermod -aG kvm $(whoami)&#xA;sudo usermod -aG libvirt $(whoami)&#xA;sudo usermod -aG input $(whoami)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: Re-login after executing this command.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fetch macOS installer.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./fetch-macOS-v2.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can choose your desired macOS version here. After executing this step, you should have the &lt;code&gt;BaseSystem.dmg&lt;/code&gt; file in the current folder.&lt;/p&gt; &lt;p&gt;ATTENTION: Let &lt;code&gt;&amp;gt;= Big Sur&lt;/code&gt; setup sit at the &lt;code&gt;Country Selection&lt;/code&gt; screen, and other similar places for a while if things are being slow. The initial macOS setup wizard will eventually succeed.&lt;/p&gt; &lt;p&gt;Sample run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ ./fetch-macOS-v2.py&#xA;1. High Sierra (10.13)&#xA;2. Mojave (10.14)&#xA;3. Catalina (10.15)&#xA;4. Big Sur (11.7)&#xA;5. Monterey (12.6)&#xA;6. Ventura (13) - RECOMMENDED&#xA;7. Sonoma (14)&#xA;&#xA;Choose a product to download (1-6): 6&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: Modern NVIDIA GPUs are supported on HighSierra but not on later versions of macOS.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Convert the downloaded &lt;code&gt;BaseSystem.dmg&lt;/code&gt; file into the &lt;code&gt;BaseSystem.img&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dmg2img -i BaseSystem.dmg BaseSystem.img&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a virtual HDD image where macOS will be installed. If you change the name of the disk image from &lt;code&gt;mac_hdd_ng.img&lt;/code&gt; to something else, the boot scripts will need to be updated to point to the new image name.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;qemu-img create -f qcow2 mac_hdd_ng.img 256G&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NOTE: Create this HDD image file on a fast SSD/NVMe disk for best results.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now you are ready to install macOS 🚀&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;CLI method (primary). Just run the &lt;code&gt;OpenCore-Boot.sh&lt;/code&gt; script to start the installation process.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./OpenCore-Boot.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: This same script works for all recent macOS versions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the &lt;code&gt;Disk Utility&lt;/code&gt; tool within the macOS installer to partition, and format the virtual disk attached to the macOS VM. Use &lt;code&gt;APFS&lt;/code&gt; (the default) for modern macOS versions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go ahead, and install macOS 🙌&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(OPTIONAL) Use this macOS VM disk with libvirt (virt-manager / virsh stuff).&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Edit &lt;code&gt;macOS-libvirt-Catalina.xml&lt;/code&gt; file and change the various file paths (search for &lt;code&gt;CHANGEME&lt;/code&gt; strings in that file). The following command should do the trick usually.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sed &#34;s/CHANGEME/$USER/g&#34; macOS-libvirt-Catalina.xml &amp;gt; macOS.xml&#xA;&#xA;virt-xml-validate macOS.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Create a VM by running the following command.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;virsh --connect qemu:///system define macOS.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;If needed, grant necessary permissions to libvirt-qemu user,&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo setfacl -m u:libvirt-qemu:rx /home/$USER&#xA;sudo setfacl -R -m u:libvirt-qemu:rx /home/$USER/OSX-KVM&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Launch &lt;code&gt;virt-manager&lt;/code&gt; and start the &lt;code&gt;macOS&lt;/code&gt; virtual machine.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Headless macOS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the provided &lt;a href=&#34;https://raw.githubusercontent.com/kholia/OSX-KVM/master/boot-macOS-headless.sh&#34;&gt;boot-macOS-headless.sh&lt;/a&gt; script.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./boot-macOS-headless.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Setting Expectations Right&lt;/h3&gt; &#xA;&lt;p&gt;Nice job on setting up a &lt;code&gt;Virtual Hackintosh&lt;/code&gt; system! Such a system can be used for a variety of purposes (e.g. software builds, testing, reversing work), and it may be all you need, along with some tweaks documented in this repository.&lt;/p&gt; &#xA;&lt;p&gt;However, such a system lacks graphical acceleration, a reliable sound sub-system, USB 3 functionality and other similar things. To enable these things, take a look at our &lt;a href=&#34;https://raw.githubusercontent.com/kholia/OSX-KVM/master/notes.md&#34;&gt;notes&lt;/a&gt;. We would like to resume our testing and documentation work around this area. Please &lt;a href=&#34;mailto:dhiru.kholia@gmail.com?subject=%5BGitHub%5D%20OSX-KVM%20Funding%20Support&#34;&gt;reach out to us&lt;/a&gt; if you are able to fund this area of work.&lt;/p&gt; &#xA;&lt;p&gt;It is possible to have &#39;beyond-native-apple-hw&#39; performance but it does require work, patience, and a bit of luck (perhaps?).&lt;/p&gt; &#xA;&lt;h3&gt;Post-Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/kholia/OSX-KVM/master/networking-qemu-kvm-howto.txt&#34;&gt;networking notes&lt;/a&gt; on how to setup networking in your VM, outbound and also inbound for remote access to your VM via SSH, VNC, etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To passthrough GPUs and other devices, see &lt;a href=&#34;https://raw.githubusercontent.com/kholia/OSX-KVM/master/notes.md#gpu-passthrough-notes&#34;&gt;these notes&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Need a different resolution? Check out the &lt;a href=&#34;https://raw.githubusercontent.com/kholia/OSX-KVM/master/notes.md#change-resolution-in-opencore&#34;&gt;notes&lt;/a&gt; included in this repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Trouble with iMessage? Check out the &lt;a href=&#34;https://raw.githubusercontent.com/kholia/OSX-KVM/master/notes.md#trouble-with-imessage&#34;&gt;notes&lt;/a&gt; included in this repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Highly recommended macOS tweaks - &lt;a href=&#34;https://github.com/sickcodes/osx-optimizer&#34;&gt;https://github.com/sickcodes/osx-optimizer&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Is This Legal?&lt;/h3&gt; &#xA;&lt;p&gt;The &#34;secret&#34; Apple OSK string is widely available on the Internet. It is also included in a public court document &lt;a href=&#34;http://www.rcfp.org/sites/default/files/docs/20120105_202426_apple_sealing.pdf&#34;&gt;available here&lt;/a&gt;. I am not a lawyer but it seems that Apple&#39;s attempt(s) to get the OSK string treated as a trade secret did not work out. Due to these reasons, the OSK string is freely included in this repository.&lt;/p&gt; &#xA;&lt;p&gt;Please review the &lt;a href=&#34;https://dortania.github.io/OpenCore-Install-Guide/why-oc.html#legality-of-hackintoshing&#34;&gt;&#39;Legality of Hackintoshing&#39; documentation bits from Dortania&#39;s OpenCore Install Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Gabriel Somlo also has &lt;a href=&#34;http://www.contrib.andrew.cmu.edu/~somlo/OSXKVM/&#34;&gt;some thoughts&lt;/a&gt; on the legal aspects involved in running macOS under QEMU/KVM.&lt;/p&gt; &#xA;&lt;p&gt;You may also find &lt;a href=&#34;https://aws.amazon.com/about-aws/whats-new/2020/11/announcing-amazon-ec2-mac-instances-for-macos/&#34;&gt;this &#39;Announcing Amazon EC2 Mac instances for macOS&#39; article&lt;/a&gt; interesting.&lt;/p&gt; &#xA;&lt;p&gt;Note: It is your responsibility to understand, and accept (or not accept) the Apple EULA.&lt;/p&gt; &#xA;&lt;p&gt;Note: This is not legal advice, so please make the proper assessments yourself and discuss with your lawyers if you have any concerns (Text credit: Dortania)&lt;/p&gt; &#xA;&lt;h3&gt;Motivation&lt;/h3&gt; &#xA;&lt;p&gt;My aim is to enable macOS based educational tasks, builds + testing, kernel debugging, reversing, and macOS security research in an easy, reproducible manner without getting &#39;invested&#39; in Apple&#39;s closed ecosystem (too heavily).&lt;/p&gt; &#xA;&lt;p&gt;These &lt;code&gt;Virtual Hackintosh&lt;/code&gt; systems are not intended to replace the genuine physical macOS systems.&lt;/p&gt; &#xA;&lt;p&gt;Personally speaking, this repository has been a way for me to &#39;exit&#39; the Apple ecosystem. It has helped me to test and compare the interoperability of &lt;code&gt;Canon CanoScan LiDE 120&lt;/code&gt; scanner, and &lt;code&gt;Brother HL-2250DN&lt;/code&gt; laser printer. And these devices now work decently enough on modern versions of Ubuntu (Yay for free software). Also, a long time back, I had to completely wipe my (then) brand new &lt;code&gt;MacBook Pro (Retina, 15-inch, Late 2013)&lt;/code&gt; and install Xubuntu on it - as the &lt;code&gt;OS X&lt;/code&gt; kernel kept crashing on it!&lt;/p&gt; &#xA;&lt;p&gt;Backstory: I was a (poor) student in Canada in a previous life and Apple made &lt;a href=&#34;https://github.com/openwall/john/raw/bleeding-jumbo/src/keychain_fmt_plug.c&#34;&gt;my work on cracking Apple Keychains&lt;/a&gt; a lot harder than it needed to be. This is how I got interested in Hackintosh systems.&lt;/p&gt;</summary>
  </entry>
</feed>