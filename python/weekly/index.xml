<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-07T02:02:01Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tiangolo/typer</title>
    <updated>2024-04-07T02:02:01Z</updated>
    <id>tag:github.com,2024-04-07:/tiangolo/typer</id>
    <link href="https://github.com/tiangolo/typer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Typer, build great CLIs. Easy to code. Based on Python type hints.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://typer.tiangolo.com&#34;&gt;&lt;img src=&#34;https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg?sanitize=true&#34; alt=&#34;Typer&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;em&gt;Typer, build great CLIs. Easy to code. Based on Python type hints.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/tiangolo/typer/actions?query=workflow%3ATest&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/tiangolo/typer/workflows/Test/badge.svg?sanitize=true&#34; alt=&#34;Test&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/tiangolo/typer/actions?query=workflow%3APublish&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/tiangolo/typer/workflows/Publish/badge.svg?sanitize=true&#34; alt=&#34;Publish&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://coverage-badge.samuelcolvin.workers.dev/redirect/tiangolo/typer&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://coverage-badge.samuelcolvin.workers.dev/tiangolo/typer.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://pypi.org/project/typer&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/typer?color=%2334D058&amp;amp;label=pypi%20package&#34; alt=&#34;Package version&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href=&#34;https://typer.tiangolo.com&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://typer.tiangolo.com&#34;&gt;https://typer.tiangolo.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Source Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/tiangolo/typer&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/tiangolo/typer&#34;&gt;https://github.com/tiangolo/typer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Typer is a library for building &lt;abbr title=&#34;command line interface, programs executed from a terminal&#34;&gt;CLI&lt;/abbr&gt; applications that users will &lt;strong&gt;love using&lt;/strong&gt; and developers will &lt;strong&gt;love creating&lt;/strong&gt;. Based on Python type hints.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s also a command line tool to run scripts, automatically converting them to CLI applications.&lt;/p&gt; &#xA;&lt;p&gt;The key features are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intuitive to write&lt;/strong&gt;: Great editor support. &lt;abbr title=&#34;also known as auto-complete, autocompletion, IntelliSense&#34;&gt;Completion&lt;/abbr&gt; everywhere. Less time debugging. Designed to be easy to use and learn. Less time reading docs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy to use&lt;/strong&gt;: It&#39;s easy to use for the final users. Automatic help, and automatic completion for all shells.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Short&lt;/strong&gt;: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Start simple&lt;/strong&gt;: The simplest example adds only 2 lines of code to your app: &lt;strong&gt;1 import, 1 function call&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Grow large&lt;/strong&gt;: Grow in complexity as much as you want, create arbitrarily complex trees of commands and groups of subcommands, with options and arguments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run scripts&lt;/strong&gt;: Typer includes a &lt;code&gt;typer&lt;/code&gt; command/program that you can use to run scripts, automatically converting them to CLIs, even if they don&#39;t use Typer internally.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FastAPI of CLIs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Typer&lt;/strong&gt; is &lt;a href=&#34;https://fastapi.tiangolo.com&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;FastAPI&lt;/a&gt;&#39;s little sibling, it&#39;s the FastAPI of CLIs.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;div class=&#34;termy&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ pip install typer&#xA;---&amp;gt; 100%&#xA;Successfully installed typer rich shellingham&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;h3&gt;The absolute minimum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a file &lt;code&gt;main.py&lt;/code&gt; with:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;def main(name: str):&#xA;    print(f&#34;Hello {name}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script doesn&#39;t even use Typer internally. But you can use the &lt;code&gt;typer&lt;/code&gt; command to run it as a CLI application.&lt;/p&gt; &#xA;&lt;h3&gt;Run it&lt;/h3&gt; &#xA;&lt;p&gt;Run your application with the &lt;code&gt;typer&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;div class=&#34;termy&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;// Run your application&#xA;$ typer main.py run&#xA;&#xA;// You get a nice error, you are missing NAME&#xA;Usage: typer [PATH_OR_MODULE] run [OPTIONS] NAME&#xA;Try &#39;typer [PATH_OR_MODULE] run --help&#39; for help.&#xA;╭─ Error ───────────────────────────────────────────╮&#xA;│ Missing argument &#39;NAME&#39;.                          │&#xA;╰───────────────────────────────────────────────────╯&#xA;&#xA;&#xA;// You get a --help for free&#xA;$ typer main.py run --help&#xA;&#xA;Usage: typer [PATH_OR_MODULE] run [OPTIONS] NAME&#xA;&#xA;Run the provided Typer app.&#xA;&#xA;╭─ Arguments ───────────────────────────────────────╮&#xA;│ *    name      TEXT  [default: None] [required]   |&#xA;╰───────────────────────────────────────────────────╯&#xA;╭─ Options ─────────────────────────────────────────╮&#xA;│ --help          Show this message and exit.       │&#xA;╰───────────────────────────────────────────────────╯&#xA;&#xA;// Now pass the NAME argument&#xA;$ typer main.py run Camila&#xA;&#xA;Hello Camila&#xA;&#xA;// It works! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This is the simplest use case, not even using Typer internally, but it can already be quite useful for simple scripts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: auto-completion works when you create a Python package and run it with &lt;code&gt;--install-completion&lt;/code&gt; or when you use the &lt;code&gt;typer&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;h2&gt;Use Typer in your code&lt;/h2&gt; &#xA;&lt;p&gt;Now let&#39;s start using Typer in your own code, update &lt;code&gt;main.py&lt;/code&gt; with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import typer&#xA;&#xA;&#xA;def main(name: str):&#xA;    print(f&#34;Hello {name}&#34;)&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    typer.run(main)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you could run it with Python directly:&lt;/p&gt; &#xA;&lt;div class=&#34;termy&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;// Run your application&#xA;$ python main.py&#xA;&#xA;// You get a nice error, you are missing NAME&#xA;Usage: main.py [OPTIONS] NAME&#xA;Try &#39;main.py --help&#39; for help.&#xA;╭─ Error ───────────────────────────────────────────╮&#xA;│ Missing argument &#39;NAME&#39;.                          │&#xA;╰───────────────────────────────────────────────────╯&#xA;&#xA;&#xA;// You get a --help for free&#xA;$ python main.py --help&#xA;&#xA;Usage: main.py [OPTIONS] NAME&#xA;&#xA;╭─ Arguments ───────────────────────────────────────╮&#xA;│ *    name      TEXT  [default: None] [required]   |&#xA;╰───────────────────────────────────────────────────╯&#xA;╭─ Options ─────────────────────────────────────────╮&#xA;│ --help          Show this message and exit.       │&#xA;╰───────────────────────────────────────────────────╯&#xA;&#xA;// Now pass the NAME argument&#xA;$ python main.py Camila&#xA;&#xA;Hello Camila&#xA;&#xA;// It works! 🎉&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: you can also call this same script with the &lt;code&gt;typer&lt;/code&gt; command, but you don&#39;t need to.&lt;/p&gt; &#xA;&lt;h2&gt;Example upgrade&lt;/h2&gt; &#xA;&lt;p&gt;This was the simplest example possible.&lt;/p&gt; &#xA;&lt;p&gt;Now let&#39;s see one a bit more complex.&lt;/p&gt; &#xA;&lt;h3&gt;An example with two subcommands&lt;/h3&gt; &#xA;&lt;p&gt;Modify the file &lt;code&gt;main.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;typer.Typer()&lt;/code&gt; app, and create two subcommands with their parameters.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import typer&#xA;&#xA;app = typer.Typer()&#xA;&#xA;&#xA;@app.command()&#xA;def hello(name: str):&#xA;    print(f&#34;Hello {name}&#34;)&#xA;&#xA;&#xA;@app.command()&#xA;def goodbye(name: str, formal: bool = False):&#xA;    if formal:&#xA;        print(f&#34;Goodbye Ms. {name}. Have a good day.&#34;)&#xA;    else:&#xA;        print(f&#34;Bye {name}!&#34;)&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    app()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And that will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Explicitly create a &lt;code&gt;typer.Typer&lt;/code&gt; app. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The previous &lt;code&gt;typer.run&lt;/code&gt; actually creates one implicitly for you.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Add two subcommands with &lt;code&gt;@app.command()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Execute the &lt;code&gt;app()&lt;/code&gt; itself, as if it was a function (instead of &lt;code&gt;typer.run&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run the upgraded example&lt;/h3&gt; &#xA;&lt;p&gt;Check the new help:&lt;/p&gt; &#xA;&lt;div class=&#34;termy&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ python main.py --help&#xA;&#xA; Usage: main.py [OPTIONS] COMMAND [ARGS]...&#xA;&#xA;╭─ Options ─────────────────────────────────────────╮&#xA;│ --install-completion          Install completion  │&#xA;│                               for the current     │&#xA;│                               shell.              │&#xA;│ --show-completion             Show completion for │&#xA;│                               the current shell,  │&#xA;│                               to copy it or       │&#xA;│                               customize the       │&#xA;│                               installation.       │&#xA;│ --help                        Show this message   │&#xA;│                               and exit.           │&#xA;╰───────────────────────────────────────────────────╯&#xA;╭─ Commands ────────────────────────────────────────╮&#xA;│ goodbye                                           │&#xA;│ hello                                             │&#xA;╰───────────────────────────────────────────────────╯&#xA;&#xA;// When you create a package you get ✨ auto-completion ✨ for free, installed with --install-completion&#xA;&#xA;// You have 2 subcommands (the 2 functions): goodbye and hello&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Now check the help for the &lt;code&gt;hello&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;div class=&#34;termy&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ python main.py hello --help&#xA;&#xA; Usage: main.py hello [OPTIONS] NAME&#xA;&#xA;╭─ Arguments ───────────────────────────────────────╮&#xA;│ *    name      TEXT  [default: None] [required]   │&#xA;╰───────────────────────────────────────────────────╯&#xA;╭─ Options ─────────────────────────────────────────╮&#xA;│ --help          Show this message and exit.       │&#xA;╰───────────────────────────────────────────────────╯&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;And now check the help for the &lt;code&gt;goodbye&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;div class=&#34;termy&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ python main.py goodbye --help&#xA;&#xA; Usage: main.py goodbye [OPTIONS] NAME&#xA;&#xA;╭─ Arguments ───────────────────────────────────────╮&#xA;│ *    name      TEXT  [default: None] [required]   │&#xA;╰───────────────────────────────────────────────────╯&#xA;╭─ Options ─────────────────────────────────────────╮&#xA;│ --formal    --no-formal      [default: no-formal] │&#xA;│ --help                       Show this message    │&#xA;│                              and exit.            │&#xA;╰───────────────────────────────────────────────────╯&#xA;&#xA;// Automatic --formal and --no-formal for the bool option 🎉&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Now you can try out the new command line application:&lt;/p&gt; &#xA;&lt;div class=&#34;termy&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;// Use it with the hello command&#xA;&#xA;$ python main.py hello Camila&#xA;&#xA;Hello Camila&#xA;&#xA;// And with the goodbye command&#xA;&#xA;$ python main.py goodbye Camila&#xA;&#xA;Bye Camila!&#xA;&#xA;// And with --formal&#xA;&#xA;$ python main.py goodbye --formal Camila&#xA;&#xA;Goodbye Ms. Camila. Have a good day.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Recap&lt;/h3&gt; &#xA;&lt;p&gt;In summary, you declare &lt;strong&gt;once&lt;/strong&gt; the types of parameters (&lt;em&gt;CLI arguments&lt;/em&gt; and &lt;em&gt;CLI options&lt;/em&gt;) as function parameters.&lt;/p&gt; &#xA;&lt;p&gt;You do that with standard modern Python types.&lt;/p&gt; &#xA;&lt;p&gt;You don&#39;t have to learn a new syntax, the methods or classes of a specific library, etc.&lt;/p&gt; &#xA;&lt;p&gt;Just standard &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, for an &lt;code&gt;int&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;total: int&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for a &lt;code&gt;bool&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;force: bool&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And similarly for &lt;strong&gt;files&lt;/strong&gt;, &lt;strong&gt;paths&lt;/strong&gt;, &lt;strong&gt;enums&lt;/strong&gt; (choices), etc. And there are tools to create &lt;strong&gt;groups of subcommands&lt;/strong&gt;, add metadata, extra &lt;strong&gt;validation&lt;/strong&gt;, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;You get&lt;/strong&gt;: great editor support, including &lt;strong&gt;completion&lt;/strong&gt; and &lt;strong&gt;type checks&lt;/strong&gt; everywhere.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your users get&lt;/strong&gt;: automatic &lt;strong&gt;&lt;code&gt;--help&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;auto-completion&lt;/strong&gt; in their terminal (Bash, Zsh, Fish, PowerShell) when they install your package or when using the &lt;code&gt;typer&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;p&gt;For a more complete example including more features, see the &lt;a href=&#34;https://typer.tiangolo.com/tutorial/&#34;&gt;Tutorial - User Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Typer&lt;/strong&gt; stands on the shoulders of a giant. Its only internal required dependency is &lt;a href=&#34;https://click.palletsprojects.com/&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;Click&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By default it also comes with extra standard dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rich.readthedocs.io/en/stable/index.html&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;rich&lt;/code&gt;&lt;/a&gt;: to show nicely formatted errors automatically.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sarugaku/shellingham&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;shellingham&lt;/code&gt;&lt;/a&gt;: to automatically detect the current shell when installing completion. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;With &lt;code&gt;shellingham&lt;/code&gt; you can just use &lt;code&gt;--install-completion&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Without &lt;code&gt;shellingham&lt;/code&gt;, you have to pass the name of the shell to install completion for, e.g. &lt;code&gt;--install-completion bash&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;code&gt;typer-slim&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t want the extra standard optional dependencies, install &lt;code&gt;typer-slim&lt;/code&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;When you install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install typer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...it includes the same code and dependencies as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;typer-slim[standard]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;standard&lt;/code&gt; extra dependencies are &lt;code&gt;rich&lt;/code&gt; and &lt;code&gt;shellingham&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;typer&lt;/code&gt; command is only included in the &lt;code&gt;typer&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the terms of the MIT license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/dbrx</title>
    <updated>2024-04-07T02:02:01Z</updated>
    <id>tag:github.com,2024-04-07:/databricks/dbrx</id>
    <link href="https://github.com/databricks/dbrx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code examples and resources for DBRX, a large language model developed by Databricks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DBRX&lt;/h1&gt; &#xA;&lt;p&gt;DBRX is a large language model trained by Databricks, and made available under an open license. This repository contains the minimal code and examples to run inference, as well as a collection of resources and links for using DBRX.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.databricks.com/blog/announcing-dbrx-new-standard-efficient-open-source-customizable-llms&#34;&gt;Founder&#39;s Blog&lt;/a&gt;, &lt;a href=&#34;https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm&#34;&gt;DBRX Technical Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face: &lt;a href=&#34;https://huggingface.co/collections/databricks/&#34;&gt;https://huggingface.co/collections/databricks/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLM Foundry: &lt;a href=&#34;https://github.com/mosaicml/llm-foundry&#34;&gt;https://github.com/mosaicml/llm-foundry&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A reference model code can be found in this repository at &lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/model/modeling_dbrx.py&#34;&gt;modeling_dbrx.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; this model code is supplied for references purposes only, please see the &lt;a href=&#34;https://huggingface.co/collections/databricks/&#34;&gt;Hugging Face&lt;/a&gt; repository for the official supported version.&lt;/p&gt; &#xA;&lt;h2&gt;Model details&lt;/h2&gt; &#xA;&lt;p&gt;DBRX is a Mixture-of-Experts (MoE) model with 132B total parameters and 36B live parameters. We use 16 experts, of which 4 are active during training or inference. DBRX was pre-trained for 12T tokens of text. DBRX has a context length of 32K tokens.&lt;/p&gt; &#xA;&lt;p&gt;The following models are open-sourced:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/databricks/dbrx-base&#34;&gt;DBRX Base&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pre-trained base model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/databricks/dbrx-instruct&#34;&gt;DBRX Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Finetuned model for instruction following&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The model was trained using optimized versions of our open source libraries &lt;a href=&#34;https://www.github.com/mosaicml/composer&#34;&gt;Composer&lt;/a&gt;, &lt;a href=&#34;https://www.github.com/mosaicml/llm-foundry&#34;&gt;LLM Foundry&lt;/a&gt;, &lt;a href=&#34;https://github.com/databricks/megablocks&#34;&gt;MegaBlocks&lt;/a&gt; and &lt;a href=&#34;https://github.com/mosaicml/streaming&#34;&gt;Streaming&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the instruct model, we used the ChatML format. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/MODEL_CARD_dbrx_instruct.md&#34;&gt;DBRX Instruct model card&lt;/a&gt; for more information on this.&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;To download the weights and tokenizer, please first visit the DBRX Hugging Face page and accept the license. Note: access to the Base model requires manual approval.&lt;/p&gt; &#xA;&lt;p&gt;We recommend having at least 320GB of memory to run the model.&lt;/p&gt; &#xA;&lt;p&gt;Then, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt # Or requirements-gpu.txt to use flash attention on GPU(s)&#xA;huggingface-cli login           # Add your Hugging Face token in order to access the model&#xA;python generate.py              # See generate.py to change the prompt and other settings&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more advanced usage, please see LLM Foundry (&lt;a href=&#34;https://github.com/mosaicml/llm-foundry/raw/main/scripts/inference/hf_chat.py&#34;&gt;chat script&lt;/a&gt;, &lt;a href=&#34;https://github.com/mosaicml/llm-foundry/raw/main/scripts/inference/hf_generate.py&#34;&gt;batch generation script&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;If you have any package installation issues, we recommend using our Docker image: &lt;a href=&#34;https://github.com/mosaicml/llm-foundry?tab=readme-ov-file#mosaicml-docker-images&#34;&gt;&lt;code&gt;mosaicml/llm-foundry:2.2.1_cu121_flash2-latest&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Both TensorRT-LLM and vLLM can be used to run optimized inference with DBRX. We have tested both libraries on NVIDIA A100 and H100 systems. To run inference with 16-bit precision, a minimum of 4 x 80GB multi-GPU system is required.&lt;/p&gt; &#xA;&lt;h3&gt;TensorRT-LLM&lt;/h3&gt; &#xA;&lt;p&gt;DBRX support is being added to TensorRT-LLM library: &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/pull/1363&#34;&gt;Pending PR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After merging, instructions to build and run DBRX TensorRT engines will be found at: &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/raw/main/examples/dbrx/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;vLLM&lt;/h3&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://docs.vllm.ai/en/latest/&#34;&gt;vLLM docs&lt;/a&gt; for instructions on how to run DBRX with the vLLM engine.&lt;/p&gt; &#xA;&lt;h3&gt;MLX&lt;/h3&gt; &#xA;&lt;p&gt;If you have an Apple laptop with a sufficiently powerful M-series chip, quantized version of DBRX can be run with MLX. See instructions for running DBRX on MLX &lt;a href=&#34;https://huggingface.co/mlx-community/dbrx-instruct-4bit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Finetune&lt;/h2&gt; &#xA;&lt;p&gt;To finetune DBRX with our open source library &lt;a href=&#34;https://www.github.com/mosaicml/llm-foundry&#34;&gt;LLM Foundry&lt;/a&gt;, please see the instructions in our training script (found &lt;a href=&#34;https://github.com/mosaicml/llm-foundry/tree/main/scripts/train&#34;&gt;here&lt;/a&gt;). We have finetuning support for both:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full parameter finetuning, see the yaml config &lt;a href=&#34;https://github.com/mosaicml/llm-foundry/raw/main/scripts/train/yamls/finetune/dbrx-full-ft.yaml&#34;&gt;dbrx-full-ft.yaml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LoRA finetuning, see the yaml config &lt;a href=&#34;https://github.com/mosaicml/llm-foundry/raw/main/scripts/train/yamls/finetune/dbrx-lora-ft.yaml&#34;&gt;dbrx-lora-ft.yaml&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: LoRA support currently cannot finetune the experts, since the experts are fused. Stay tuned for more.&lt;/p&gt; &#xA;&lt;h2&gt;Model card&lt;/h2&gt; &#xA;&lt;p&gt;The model cards can be found at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/MODEL_CARD_dbrx_base.md&#34;&gt;DBRX Base&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/MODEL_CARD_dbrx_instruct.md&#34;&gt;DBRX Instruct&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;p&gt;DBRX is available on the Databricks platform through:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/machine-learning/foundation-models/supported-models.html#dbrx-instruct&#34;&gt;Mosaic AI Model Serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/large-language-models/ai-playground.html&#34;&gt;Mosaic AI Playground&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other providers have recently added support for DBRX:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://you.com/&#34;&gt;You.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://labs.perplexity.ai/&#34;&gt;Perplexity Labs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The same tools used to train high quality MoE models such as DBRX are available for Databricks customers. Please reach out to us at &lt;a href=&#34;https://www.databricks.com/company/contact&#34;&gt;https://www.databricks.com/company/contact&lt;/a&gt; if you are interested in pre-training, finetuning, or deploying your own DBRX models!&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;For issues with model output, or community discussion, please use the Hugging Face community forum (&lt;a href=&#34;https://huggingface.co/databricks/dbrx-instruct&#34;&gt;instruct&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/databricks/dbrx-base&#34;&gt;base&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;For issues with LLM Foundry, or any of the underlying training libraries, please open an issue on the relevant GitHub repository.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our model weights and code are licensed for both researchers and commercial entities. The &lt;a href=&#34;https://www.databricks.com/legal/open-model-license&#34;&gt;Databricks Open Source License&lt;/a&gt; can be found at &lt;a href=&#34;https://raw.githubusercontent.com/databricks/dbrx/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;, and our Acceptable Use Policy can be found &lt;a href=&#34;https://www.databricks.com/legal/acceptable-use-policy-open-model&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>agiresearch/AIOS</title>
    <updated>2024-04-07T02:02:01Z</updated>
    <id>tag:github.com,2024-04-07:/agiresearch/AIOS</id>
    <link href="https://github.com/agiresearch/AIOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AIOS: LLM Agent Operating System&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AIOS: LLM Agent Operating System&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.16971&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.03815&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/agiresearch/AIOS/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-MIT-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;AIOS, a Large Language Model (LLM) Agent operating system, embeds large language model into Operating Systems (OS) as the brain of the OS, enabling an operating system &#34;with soul&#34; -- an important step towards AGI. AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, maintain access control for agents, and provide a rich set of toolkits for LLM Agent developers.&lt;/p&gt; &#xA;&lt;h2&gt;🏠 Architecture of AIOS&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/agiresearch/AIOS/main/images/AIOS-Architecture.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;📰 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-04-05]&lt;/strong&gt; 🛠️ AIOS codebase has been updated to add shell simulator, rapid API calls, and pre-commit test cases. Please see &lt;a href=&#34;https://github.com/agiresearch/AIOS/raw/main/CONTRIBUTE.md&#34;&gt;CONTRIBUTE&lt;/a&gt; for how to test your contributions and create pull requests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-04-02]&lt;/strong&gt; 🌟 AIOS &lt;a href=&#34;https://discord.gg/aUg3b2Kd&#34;&gt;Discord Community&lt;/a&gt; is up. Welcome to join the community for discussions, brainstorming, development, or just random chats!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-03-25]&lt;/strong&gt; ✈️ Our paper &lt;a href=&#34;https://arxiv.org/abs/2403.16971&#34;&gt;AIOS: LLM Agent Operating System&lt;/a&gt; is released and AIOS repository is officially launched!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023-12-06]&lt;/strong&gt; 📋 After several months of working, our perspective paper &lt;a href=&#34;https://arxiv.org/abs/2312.03815&#34;&gt;LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem&lt;/a&gt; is officially released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✈️ Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/agiresearch/AIOS.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make sure you have Python &amp;gt;= 3.9 and &amp;lt;= 3.11&lt;/strong&gt; Install the required packages using pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;If you use open-sourced models from huggingface, you need to setup your &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;Hugging Face token&lt;/a&gt; and cache directory&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export HUGGING_FACE_HUB_TOKEN=&amp;lt;YOUR READ TOKEN&amp;gt;&#xA;export HF_HOME=&amp;lt;YOUR CACHE DIRECTORY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use LLM APIs like Gemini-pro, you need to setup your &lt;a href=&#34;https://aistudio.google.com/app/apikey&#34;&gt;Gemini API Key&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GEMINI_API_KEY=&amp;lt;YOUR GEMINI API KEY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we provide two modes to run the AIOS: interactive mode and deployment mode&lt;/p&gt; &#xA;&lt;h4&gt;Interactive Mode&lt;/h4&gt; &#xA;&lt;p&gt;In the interactive mode, you can interact with AIOS to see the output of each step in running multiple agents&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use Gemma-2b-it, replace the max_gpu_memory and eval_device with your own and run&#xA;python main.py --llm_name gemma-2b-it --max_gpu_memory &#39;{&#34;0&#34;: &#34;24GB&#34;}&#39; --eval_device &#34;cuda:0&#34; --max_new_tokens 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use Mixtral-8x7b-it, replace the max_gpu_memory and eval_device with your own and run&#xA;python main.py --llm_name mixtral-8x7b-it --max_gpu_memory &#39;{&#34;0&#34;: &#34;48GB&#34;, &#34;1&#34;: &#34;48GB&#34;, &#34;2&#34;: &#34;48GB&#34;}&#39; --eval_device &#34;cuda:0&#34; --max_new_tokens 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use Gemini-pro, run with Gemini-pro&#xA;python main.py --llm_name gemini-pro&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Deployment Mode&lt;/h4&gt; &#xA;&lt;p&gt;In the deployment mode, the outputs of running agents are stored in files. And in this mode, you are provided with multiple commands to run agents and see resource usage of agents (e.g., run &amp;lt;xxxAgent&amp;gt;: &amp;lt;YOUR TASK&amp;gt;, print agent)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use Gemma-2b-it, replace the max_gpu_memory and eval_device with your own and run&#xA;python simulator.py --llm_name gemma-2b-it --max_gpu_memory &#39;{&#34;0&#34;: &#34;24GB&#34;}&#39; --eval_device &#34;cuda:0&#34; --max_new_tokens 256 --scheduler_log_mode file --agent_log_mode file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use Mixtral-8x7b-it&#xA;python simulator.py --llm_name mixtral-8x7b-it --max_gpu_memory &#39;{&#34;0&#34;: &#34;48GB&#34;, &#34;1&#34;: &#34;48GB&#34;, &#34;2&#34;: &#34;48GB&#34;}&#39; --eval_device &#34;cuda:0&#34; --max_new_tokens 256 --scheduler_log_mode file --agent_log_mode file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use Gemini-pro&#xA;python simulator.py --llm_name gemini-pro --scheduler_log_mode file --agent_log_mode file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🖋️ References&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{mei2024aios,&#xA;  title={AIOS: LLM Agent Operating System},&#xA;  author={Mei, Kai and Li, Zelong and Xu, Shuyuan and Ye, Ruosong and Ge, Yingqiang and Zhang, Yongfeng}&#xA;  journal={arXiv:2403.16971},&#xA;  year={2024}&#xA;}&#xA;@article{ge2023llm,&#xA;  title={LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem},&#xA;  author={Ge, Yingqiang and Ren, Yujie and Hua, Wenyue and Xu, Shuyuan and Tan, Juntao and Zhang, Yongfeng},&#xA;  journal={arXiv:2312.03815},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🚀 Contributions&lt;/h2&gt; &#xA;&lt;p&gt;AIOS is dedicated to facilitating LLM agents&#39; development and deployment in a systematic way, collaborators and contributions are always welcome to foster a cohesive, effective and efficient AIOS-Agent ecosystem!&lt;/p&gt; &#xA;&lt;p&gt;For detailed information on how to contribute, see &lt;a href=&#34;https://github.com/agiresearch/AIOS/raw/main/CONTRIBUTE.md&#34;&gt;CONTRIBUTE&lt;/a&gt;. If you would like to contribute to the codebase, &lt;a href=&#34;https://github.com/agiresearch/AIOS/issues&#34;&gt;issues&lt;/a&gt; or &lt;a href=&#34;https://github.com/agiresearch/AIOS/pulls&#34;&gt;pull requests&lt;/a&gt; are always welcome!&lt;/p&gt; &#xA;&lt;h2&gt;🌟 Discord Channel&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to join the community, ask questions, chat with fellows, learn about or propose new features, and participate in future developments, join our &lt;a href=&#34;https://discord.gg/aUg3b2Kd&#34;&gt;Discord Community&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;📪 Contact&lt;/h2&gt; &#xA;&lt;p&gt;For issues related to AIOS development, we encourage submtting &lt;a href=&#34;https://github.com/agiresearch/AIOS/issues&#34;&gt;issues&lt;/a&gt;, &lt;a href=&#34;https://github.com/agiresearch/AIOS/pulls&#34;&gt;pull requests&lt;/a&gt;, or initiating discussions in the AIOS &lt;a href=&#34;https://discord.gg/aUg3b2Kd&#34;&gt;Discord Channel&lt;/a&gt;. For other issues please feel free to contact Kai Mei (&lt;a href=&#34;mailto:marknju2018@gmail.com&#34;&gt;marknju2018@gmail.com&lt;/a&gt;) and Yongfeng Zhang (&lt;a href=&#34;mailto:yongfeng@email.com&#34;&gt;yongfeng@email.com&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;🌍 AIOS Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/agiresearch/AIOS/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=agiresearch/AIOS&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>