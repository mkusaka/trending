<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-22T01:59:39Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>THUDM/CogVLM</title>
    <updated>2023-10-22T01:59:39Z</updated>
    <id>tag:github.com,2023-10-22:/THUDM/CogVLM</id>
    <link href="https://github.com/THUDM/CogVLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;a state-of-the-art-level open visual language model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CogVLM&lt;/h1&gt; &#xA;&lt;p&gt;📖 &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/cogvlm-paper.pdf&#34;&gt;Paper（论文）&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🌐 &lt;a href=&#34;http://36.103.203.44:7861/&#34;&gt;web demo（测试网址）&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM is a powerful &lt;strong&gt;open-source visual language model&lt;/strong&gt; (&lt;strong&gt;VLM&lt;/strong&gt;). CogVLM-17B has 10 billion vision parameters and 7 billion language parameters.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and rank the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., &lt;strong&gt;surpassing or matching PaLI-X 55B&lt;/strong&gt;. CogVLM can also &lt;a href=&#34;http://36.103.203.44:7861&#34;&gt;chat with you&lt;/a&gt; about images.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Chinese brief introduction: CogVLM 是一个强大的开源视觉语言模型，利用视觉专家模块深度整合语言编码和视觉编码，在 10 项权威跨模态基准上取得了SOTA性能。目前仅支持英文，后续会提供中英双语版本支持，欢迎持续关注！&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/metrics-min.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;!-- CogVLM is powerful for answering various types of visual questions, including **Detailed Description &amp; Visual Question Answering**,  **Complex Counting**, **Visual Math Problem Solving**, **OCR-Free Reasonging**, **OCR-Free Visual Question Answering**, **World Knowledge**, **Referring Expression Comprehension**, **Programming with Visual Input**, **Grounding with Caption**, **Grounding Visual Question Answering**, etc. --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM can accurately describe images in details with &lt;strong&gt;very few hallucinations&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Click for comparison with LLAVA-1.5 and MiniGPT-4.&lt;/summary&gt; &#xA;   &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/llava-comparison-min.png&#34; alt=&#34;LLAVA Comparision&#34;&gt;&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CogVLM can understand and answer various types of questions, and has a &lt;strong&gt;visual grounding&lt;/strong&gt; version.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/pear_grounding.png&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CogVLM sometimes captures more detailed content than GPT-4V(ision).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/compare-min.png&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ![compare](assets/compare.png) --&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to expand more examples.&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/chat.png&#34; alt=&#34;Chat Examples&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a &lt;strong&gt;visual expert module&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/cogvlm-paper.pdf&#34;&gt;Paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/method-min.png&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;We support two GUIs for model inference, &lt;strong&gt;web demo&lt;/strong&gt; and &lt;strong&gt;CLI&lt;/strong&gt;. If you want to use it in your python code, it is easy to modify the CLI scripts for your case.&lt;/p&gt; &#xA;&lt;p&gt;First, we need to install the dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;python -m spacy download en_core_web_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Hardware requirement&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Model Inference: 1 * A100(80G) or 2 * RTX 3090(24G).&lt;/li&gt; &#xA; &lt;li&gt;Finetuning: 4 * A100(80G) &lt;em&gt;[Recommend]&lt;/em&gt; or 8* RTX 3090(24G).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ### Online Web Demo&#xA;We provide a [web demo](http://36.103.203.44:7861/) based on [Gradio](https://gradio.app). --&gt; &#xA;&lt;h3&gt;Web Demo&lt;/h3&gt; &#xA;&lt;p&gt;We also offer a local web demo based on Gradio. First, install Gradio by running: &lt;code&gt;pip install gradio&lt;/code&gt;. Then download and enter this repository and run &lt;code&gt;web_demo.py&lt;/code&gt;. See the next section for detailed usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;python web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --english --bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The GUI of the web demo looks like:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/web_demo-min.png&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;CLI&lt;/h3&gt; &#xA;&lt;p&gt;We open-source different checkpoints for different downstreaming tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;cogvlm-chat&lt;/code&gt; The model after SFT for alignment, which supports chat like GPT-4V.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cogvlm-base-224&lt;/code&gt; The original checkpoint after text-image pretraining.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cogvlm-base-490&lt;/code&gt; The finetuned version on &lt;code&gt;490px&lt;/code&gt; resolution from &lt;code&gt;cogvlm-base-224&lt;/code&gt;. The finetuning data includes the training sets of VQA datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cogvlm-grounding-generalist&lt;/code&gt;. This checkpoint supports different visual grounding tasks, e.g. REC, Grounding Captioning, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Run CLI demo via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py --from_pretrained cogvlm-base-224 --version base --english --bf16 --no_prompt&#xA;python cli_demo.py --from_pretrained cogvlm-base-490 --version base --english --bf16 --no_prompt&#xA;python cli_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;python cli_demo.py --from_pretrained cogvlm-grounding-generalist --version base --english --bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The program will automatically download the sat model and interact in the command line. You can generate replies by entering instructions and pressing enter. Enter &lt;code&gt;clear&lt;/code&gt; to clear the conversation history and &lt;code&gt;stop&lt;/code&gt; to stop the program.&lt;/p&gt; &#xA;&lt;h4&gt;Multi-GPU inference&lt;/h4&gt; &#xA;&lt;p&gt;We also support model parallel inference, which splits model to multiple (2/4/8) GPUs. &lt;code&gt;--nproc-per-node=[n]&lt;/code&gt; in the following command controls the number of used GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have trouble in accessing huggingface.co, you can add &lt;code&gt;--local_tokenizer /path/to/vicuna-7b-v1.5&lt;/code&gt; to load the tokenizer.&lt;/li&gt; &#xA; &lt;li&gt;If you have trouble in automatically downloading model with 🔨&lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt;, try downloading from 🤖&lt;a href=&#34;https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary&#34;&gt;modelscope&lt;/a&gt; or 🤗&lt;a href=&#34;https://huggingface.co/THUDM/CogVLM&#34;&gt;huggingface&lt;/a&gt; manually.&lt;/li&gt; &#xA; &lt;li&gt;Download model using 🔨&lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt;, the model will be saved to the default location &lt;code&gt;~/.sat_models&lt;/code&gt;. Change the default location by setting the environment variable &lt;code&gt;SAT_HOME&lt;/code&gt;. For example, if you want to save the model to &lt;code&gt;/path/to/my/models&lt;/code&gt;, you can run &lt;code&gt;export SAT_HOME=/path/to/my/models&lt;/code&gt; before running the python command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The program provides the following hyperparameters to control the generation process:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: cli_demo.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE] [--english]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --max_length MAX_LENGTH&#xA;                        max length of the total sequence&#xA;  --top_p TOP_P         top p for nucleus sampling&#xA;  --top_k TOP_K         top k for top k sampling&#xA;  --temperature TEMPERATURE&#xA;                        temperature for sampling&#xA;  --english             only output English&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;You may want to use CogVLM in your own task, which needs a &lt;strong&gt;different output style or domain knowledge&lt;/strong&gt;. We here provide a finetuning example for &lt;strong&gt;Captcha Recognition&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Start by downloading the &lt;a href=&#34;https://www.kaggle.com/datasets/aadhavvignesh/captcha-images&#34;&gt;Captcha Images dataset&lt;/a&gt;. Once downloaded, extract the contents of the ZIP file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To create a train/validation/test split in the ratio of 80/5/15, execute the following:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/split_dataset.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start the fine-tuning process with this command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/finetune_(224/490)_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Merge the model to &lt;code&gt;model_parallel_size=1&lt;/code&gt;: (replace the 4 below with your training &lt;code&gt;MP_SIZE&lt;/code&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --standalone --nnodes=1 --nproc-per-node=4 merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(224/490)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Evaluate the performance of your model.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/evaluate_(224/490).sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;It is recommended to use the &lt;code&gt;490px&lt;/code&gt; version. However, if you have limited GPU resources (such as only one node with 8* RTX 3090), you can try &lt;code&gt;224px&lt;/code&gt; version with model parallel.&lt;/p&gt; &#xA;&lt;p&gt;The anticipated result of this script is around &lt;code&gt;95%&lt;/code&gt; accuracy on test set.&lt;/p&gt; &#xA;&lt;p&gt;It is worth noting that the fine-tuning examples only tune limited parameters. (Expert only) If you want to get &lt;code&gt;&amp;gt;98%&lt;/code&gt; accuracy, you need to increase the trainable parameters in &lt;code&gt;finetune_demo.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository is open source under the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/LICENSE&#34;&gt;Apache-2.0 license&lt;/a&gt;, while the use of the CogVLM model weights must comply with the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation &amp;amp; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please consider citing the following papers&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Yes, you can help us!!!&#xA;The paper (ArXiv ID 5148899) has been &#34;on hold&#34; by arXiv for more than two weeks without clear reason. &#xA;If you happen to know the moderators (cs.CV), please help to accelarate the process. Thank you!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the instruction fine-tuning phase of the CogVLM, there are some English image-text data from the &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT-4&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLAVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/FuxiaoLiu/LRV-Instruction&#34;&gt;LRV-Instruction&lt;/a&gt;, &lt;a href=&#34;https://github.com/SALT-NLP/LLaVAR&#34;&gt;LLaVAR&lt;/a&gt; and &lt;a href=&#34;https://github.com/shikras/shikra&#34;&gt;Shikra&lt;/a&gt; projects, as well as many classic cross-modal work datasets. We sincerely thank them for their contributions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mindsdb/mindsdb</title>
    <updated>2023-10-22T01:59:39Z</updated>
    <id>tag:github.com,2023-10-22:/mindsdb/mindsdb</id>
    <link href="https://github.com/mindsdb/mindsdb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MindsDB connects AI models to datasources.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img width=&#34;300&#34; src=&#34;https://github.com/mindsdb/mindsdb_native/raw/stable/assets/MindsDBColorPurp@3x.png?raw=true&#34; alt=&#34;MindsDB&#34;&gt; &lt;br&gt; &lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://runacap.com/ross-index/annual-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;/a&gt; &lt;img style=&#34;width: 260px; height: 56px&#34; src=&#34;https://runacap.com/wp-content/uploads/2023/02/Annual_ROSS_badge_white_2022.svg?sanitize=true&#34; alt=&#34;ROSS Index - Fastest Growing Open-Source Startups | Runa Capital&#34; width=&#34;260&#34; height=&#34;56&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt; &lt;a href=&#34;https://github.com/mindsdb/mindsdb/actions&#34;&gt;&lt;img src=&#34;https://github.com/mindsdb/mindsdb/actions/workflows/release.yml/badge.svg?sanitize=true&#34; alt=&#34;MindsDB Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.8.x%7C%203.9.x%7C%203.10.x%7C%203.11.x-brightgreen.svg?sanitize=true&#34; alt=&#34;Python supported&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/MindsDB/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/MindsDB.svg?sanitize=true&#34; alt=&#34;PyPi Version&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;img alt=&#34;PyPI - Downloads&#34; src=&#34;https://img.shields.io/pypi/dm/Mindsdb&#34;&gt; &lt;a href=&#34;https://hub.docker.com/u/mindsdb&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/mindsdb/mindsdb&#34; alt=&#34;Docker pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ossrank.com/p/630&#34;&gt;&lt;img src=&#34;https://shields.io/endpoint?url=https://ossrank.com/shield/630&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.mindsdb.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/website?url=https%3A%2F%2Fwww.mindsdb.com%2F&#34; alt=&#34;MindsDB Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mindsdb.com/joincommunity&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-@mindsdbcommunity-brightgreen.svg?logo=slack &#34; alt=&#34;MindsDB Community&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://deepnote.com/project/Machine-Learning-With-SQL-8GDF7bc7SzKlhBLorqoIcw/%2Fmindsdb_demo.ipynb&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote-white.svg?sanitize=true&#34; alt=&#34;Launch in Deepnote&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://gitpod.io/#https://github.com/mindsdb/mindsdb&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;Open in Gitpod&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;h3 align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;Website&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href=&#34;https://docs.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;Docs&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href=&#34;https://mindsdb.com/joincommunity&#34;&gt;Community Slack&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href=&#34;https://github.com/mindsdb/mindsdb/projects?type=classic&#34;&gt;Contribute&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href=&#34;https://cloud.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;Demo&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href=&#34;https://mindsdb.com/hackerminds-ai-app-challenge&#34;&gt;Hackathon&lt;/a&gt; &lt;/h3&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;MindsDB&#39;s&lt;/a&gt; &lt;strong&gt;AI Virtual Database&lt;/strong&gt; empowers developers to connect any AI/ML model to any datasource. This includes relational and non-relational databases, data warehouses and SaaS applications. &lt;a href=&#34;https://twitter.com/intent/tweet?text=Build%20AI-Centered%20Applications%20&amp;amp;url=https://www.mindsdb.com&amp;amp;via=mindsdb&amp;amp;hashtags=ai,ml,nlp,machine_learning,neural_networks,databases,gpt3&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&#34; alt=&#34;Tweet&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MindsDB offers two primary benefits to its users.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Hook AI models to run automatically as new data is observed and plug the output into any of our integrations.&lt;/li&gt; &#xA; &lt;li&gt;Automate training and finetuning AI models from data contained in any of the 130+ datasources we support.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img width=&#34;1089&#34; alt=&#34;image&#34; src=&#34;https://github.com/mindsdb/mindsdb/assets/5898506/5451fe7e-a854-4c53-b34b-769b6c7c9863&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mindsdb/mindsdb/staging/#Installation&#34;&gt;Installation&lt;/a&gt;- &lt;a href=&#34;https://raw.githubusercontent.com/mindsdb/mindsdb/staging/#Howitworks&#34;&gt;How it works&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/mindsdb/mindsdb/staging/#DatabaseIntegrations&#34;&gt;DatabaseIntegrations&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/mindsdb/mindsdb/staging/#Documentation&#34;&gt;Documentation&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/mindsdb/mindsdb/staging/#Support&#34;&gt;Support&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/mindsdb/mindsdb/staging/#Contributing&#34;&gt;Contributing&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/mindsdb/mindsdb/staging/#Currentcontributors&#34;&gt;Current contributors&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/mindsdb/mindsdb/staging/#License&#34;&gt;License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;You can try MindsDB using our &lt;a href=&#34;https://cloud.mindsdb.com/?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;demo environment&lt;/a&gt; with sample data for the most popular use cases.&lt;/p&gt; &#xA;&lt;h2&gt;Installation &lt;a name=&#34;Installation&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The preferred way is to use MindsDB Cloud &lt;a href=&#34;https://cloud.mindsdb.com/home&#34;&gt;free demo instance&lt;/a&gt; or use a &lt;a href=&#34;https://cloud.mindsdb.com/home&#34;&gt;dedicated instance&lt;/a&gt;. If you want to move to production, use &lt;a href=&#34;https://aws.amazon.com/marketplace/seller-profile?id=03a65520-86ca-4ab8-a394-c11eb54573a9&#34;&gt;the AWS AMI image&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To install locally or on-premise, pull the latest Docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker pull mindsdb/mindsdb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How it works &lt;a name=&#34;How it works&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Connect:&lt;/strong&gt; Link MindsDB to your data platform. With support for hundreds of integrations and counting, we&#39;re constantly expanding our list.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create Model:&lt;/strong&gt; Select an AI Engine to learn from your data. Models are provisioned and deployed instantly for inference. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Choose from pre-trained models like OpenAI&#39;s GPT, Hugging Face, LangChain, etc., for NLP and generative AI.&lt;/li&gt; &#xA;   &lt;li&gt;Or select from a range of state-of-the-art engines for classic ML tasks (regression, classification, time-series).&lt;/li&gt; &#xA;   &lt;li&gt;Even &lt;a href=&#34;https://docs.mindsdb.com/custom-model/byom&#34;&gt;import custom models&lt;/a&gt; built with any ML framework.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Query Models:&lt;/strong&gt; Use &lt;a href=&#34;https://docs.mindsdb.com/sql/api/select&#34;&gt;SELECT statements&lt;/a&gt;, &lt;a href=&#34;https://docs.mindsdb.com/rest/usage&#34;&gt;API calls&lt;/a&gt;, or &lt;a href=&#34;https://docs.mindsdb.com/sql/api/join&#34;&gt;JOIN commands&lt;/a&gt; to make predictions on thousands or millions of data points simultaneously.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fine-Tune Models:&lt;/strong&gt; Experiment and &lt;a href=&#34;https://docs.mindsdb.com/sql/api/finetune&#34;&gt;Fine-Tune&lt;/a&gt; to achieve optimal results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automate Workflows:&lt;/strong&gt; Streamline operations with &lt;a href=&#34;https://docs.mindsdb.com/sql/create/jobs&#34;&gt;JOBS&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://docs.mindsdb.com/quickstart?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;quickstart guide&lt;/a&gt; with sample data to get on-boarded as fast as possible.&lt;/p&gt; &#xA;&lt;h2&gt;Data Integrations &lt;a name=&#34;DatabaseIntegrations&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MindsDB works with most SQL, NoSQL databases, data warehouses, and popular applications. You can find the list of all supported integrations &lt;a href=&#34;https://docs.mindsdb.com/data-integrations/all-data-integrations&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mindsdb/mindsdb/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature-mindsdb-request.yaml&#34;&gt;&lt;span&gt;❓&lt;/span&gt; &lt;span&gt;👋&lt;/span&gt; Missing integration?&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation &lt;a name=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;You can find the complete documentation of MindsDB at &lt;a href=&#34;https://docs.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;docs.mindsdb.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support &lt;a name=&#34;Support&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If you found a bug, please submit an &lt;a href=&#34;https://github.com/mindsdb/mindsdb/issues/new/choose&#34;&gt;issue on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To get community support, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Post a question at MindsDB &lt;a href=&#34;https://mindsdb.com/joincommunity&#34;&gt;Slack community&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Ask for help at our &lt;a href=&#34;https://github.com/mindsdb/mindsdb/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Ask a question at &lt;a href=&#34;https://stackoverflow.com/questions/tagged/mindsdb&#34;&gt;Stackoverflow&lt;/a&gt; with a MindsDB tag.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you need commercial support, please &lt;a href=&#34;https://mindsdb.com/contact/?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;contact&lt;/a&gt; MindsDB team.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing &lt;a name=&#34;Contributing&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;A great place to start contributing to MindsDB is to check our GitHub projects &lt;span&gt;🏁&lt;/span&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Community contributor&#39;s &lt;a href=&#34;https://github.com/mindsdb/mindsdb/projects/8&#34;&gt;dashboard tasks&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mindsdb/mindsdb/issues?q=is%3Aissue+is%3Aopen+label%3Afirst-timers-only&#34;&gt;First timers only issues&lt;/a&gt;, if this is your first time contributing to an open source project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are always open to suggestions, so feel free to open new issues with your ideas, and we can guide you!&lt;/p&gt; &#xA;&lt;p&gt;Being part of the core team is accessible to anyone who is motivated and wants to be part of that journey! If you&#39;d like to contribute to the project, refer to the &lt;a href=&#34;https://docs.mindsdb.com/contribute/?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;contributing documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project is released with a &lt;a href=&#34;https://github.com/mindsdb/mindsdb/raw/stable/CODE_OF_CONDUCT.md&#34;&gt;Contributor Code of Conduct&lt;/a&gt;. By participating in this project, you agree to follow its terms.&lt;/p&gt; &#xA;&lt;p&gt;Also, check out the &lt;a href=&#34;https://mindsdb.com/community?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;rewards and community programs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Current contributors &lt;a name=&#34;Current contributors&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;a href=&#34;https://github.com/mindsdb/mindsdb/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contributors-img.web.app&#34;&gt;contributors-img&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Subscribe to updates&lt;/h2&gt; &#xA;&lt;p&gt;Join our &lt;a href=&#34;https://mindsdb.com/joincommunity&#34;&gt;Slack community&lt;/a&gt; and subscribe to the monthly &lt;a href=&#34;https://mindsdb.com/newsletter/?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo&#34;&gt;Developer Newsletter&lt;/a&gt; to get product updates, information about MindsDB events and contests, and useful content, like tutorials.&lt;/p&gt; &#xA;&lt;h2&gt;License &lt;a name=&#34;License&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MindsDB is licensed under &lt;a href=&#34;https://github.com/mindsdb/mindsdb/raw/master/LICENSE&#34;&gt;GNU General Public License v3.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Audio-AGI/AudioSep</title>
    <updated>2023-10-22T01:59:39Z</updated>
    <id>tag:github.com,2023-10-22:/Audio-AGI/AudioSep</id>
    <link href="https://github.com/Audio-AGI/AudioSep" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of &#34;Separate Anything You Describe&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Separate Anything You Describe&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.05037&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Audio-AGI/AudioSep/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Audio-AGI/AudioSep?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://audio-agi.github.io/Separate-Anything-You-Describe&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&amp;amp;style=flat-square&#34; alt=&#34;githubio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Audio-AGI/AudioSep/blob/main/AudioSep_Colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Audio-AGI/AudioSep&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/cjwbw/audiosep&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/audiosep/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the official implementation of &lt;a href=&#34;https://audio-agi.github.io/Separate-Anything-You-Describe/AudioSep_arXiv.pdf&#34;&gt;&#34;Separate Anything You Describe&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We introduce AudioSep, a foundation model for open-domain sound separation with natural language queries. AudioSep demonstrates strong separation performance and impressive zero-shot generalization ability on numerous tasks such as audio event separation, musical instrument separation, and speech enhancement. Check the separated audio examples in the &lt;a href=&#34;https://audio-agi.github.io/Separate-Anything-You-Describe/&#34;&gt;Demo Page&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;middle&#34; width=&#34;800&#34; src=&#34;https://raw.githubusercontent.com/Audio-AGI/AudioSep/main/assets/results.png&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AudioSep training &amp;amp; finetuning code release.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AudioSep base model checkpoint release.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Evaluation benchmark release.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository and setup the conda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/Audio-AGI/AudioSep.git &amp;amp;&amp;amp; \&#xA;cd AudioSep &amp;amp;&amp;amp; \ &#xA;conda env create -f environment.yml &amp;amp;&amp;amp; \&#xA;conda activate AudioSep&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://huggingface.co/spaces/Audio-AGI/AudioSep/tree/main/checkpoint&#34;&gt;model weights&lt;/a&gt; at &lt;code&gt;checkpoint/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pipeline import build_audiosep, inference&#xA;import torch&#xA;&#xA;device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)&#xA;&#xA;model = build_audiosep(&#xA;      config_yaml=&#39;config/audiosep_base.yaml&#39;, &#xA;      checkpoint_path=&#39;checkpoint/audiosep_base_4M_steps.ckpt&#39;, &#xA;      device=device)&#xA;&#xA;audio_file = &#39;path_to_audio_file&#39;&#xA;text = &#39;textual_description&#39;&#xA;output_file=&#39;separated_audio.wav&#39;&#xA;&#xA;# AudioSep processes the audio at 32 kHz sampling rate  &#xA;inference(model, audio_file, text, output_file, device)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;To load directly from Hugging Face, you can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from models.audiosep import AudioSep&#xA;from utils import get_ss_model&#xA;import torch&#xA;&#xA;device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)&#xA;&#xA;ss_model = get_ss_model(&#39;config/audiosep_base.yaml&#39;)&#xA;&#xA;model = AudioSep.from_pretrained(&#34;nielsr/audiosep-demo&#34;, ss_model=ss_model)&#xA;&#xA;audio_file = &#39;path_to_audio_file&#39;&#xA;text = &#39;textual_description&#39;&#xA;output_file=&#39;separated_audio.wav&#39;&#xA;&#xA;# AudioSep processes the audio at 32 kHz sampling rate  &#xA;inference(model, audio_file, text, output_file, device)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Use chunk-based inference to save memory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;inference(model, audio_file, text, output_file, device, use_chunk=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To utilize your audio-text paired dataset:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Format your dataset to match our JSON structure. Refer to the provided template at &lt;code&gt;datafiles/template.json&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Update the &lt;code&gt;config/audiosep_base.yaml&lt;/code&gt; file by listing your formatted JSON data files under &lt;code&gt;datafiles&lt;/code&gt;. For example:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;data:&#xA;    datafiles:&#xA;        - &#39;datafiles/your_datafile_1.json&#39;&#xA;        - &#39;datafiles/your_datafile_2.json&#39;&#xA;        ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train AudioSep from scatch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python train.py --workspace workspace/AudioSep --config_yaml config/audiosep_base.yaml --resume_checkpoint_path checkpoint/ &#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finetune AudioSep from pretrained checkpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python train.py --workspace workspace/AudioSep --config_yaml config/audiosep_base.yaml --resume_checkpoint_path path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://drive.google.com/drive/folders/1PbCsuvdrzwAZZ_fwIzF0PeVGZkTk0-kL?usp=sharing&#34;&gt;evaluation data&lt;/a&gt; under the &lt;code&gt;evaluation/data&lt;/code&gt; folder. The data should be organized as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;evaluation:&#xA;    data:&#xA;        - audioset/&#xA;        - audiocaps/&#xA;        - vggsound/&#xA;        - music/&#xA;        - clotho/&#xA;        - esc50/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run benchmark inference script, the results will be saved at &lt;code&gt;eval_logs/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python benchmark.py --checkpoint_path audiosep_base_4M_steps.ckpt&#xA;&#xA;&#34;&#34;&#34;&#xA;Evaluation Results:&#xA;&#xA;VGGSound Avg SDRi: 9.144, SISDR: 9.043&#xA;MUSIC Avg SDRi: 10.508, SISDR: 9.425&#xA;ESC-50 Avg SDRi: 10.040, SISDR: 8.810&#xA;AudioSet Avg SDRi: 7.739, SISDR: 6.903&#xA;AudioCaps Avg SDRi: 8.220, SISDR: 7.189&#xA;Clotho Avg SDRi: 6.850, SISDR: 5.242&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cite this work&lt;/h2&gt; &#xA;&lt;p&gt;If you found this tool useful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023separate,&#xA;  title={Separate Anything You Describe},&#xA;  author={Liu, Xubo and Kong, Qiuqiang and Zhao, Yan and Liu, Haohe and Yuan, Yi and Liu, Yuzhuo and Xia, Rui and Wang, Yuxuan and Plumbley, Mark D and Wang, Wenwu},&#xA;  journal={arXiv preprint arXiv:2308.05037},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{liu22w_interspeech,&#xA;  title={Separate What You Describe: Language-Queried Audio Source Separation},&#xA;  author={Liu, Xubo and Liu, Haohe and Kong, Qiuqiang and Mei, Xinhao and Zhao, Jinzheng and Huang, Qiushi and Plumbley, Mark D and Wang, Wenwu},&#xA;  year=2022,&#xA;  booktitle={Proc. Interspeech},&#xA;  pages={1801--1805},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>