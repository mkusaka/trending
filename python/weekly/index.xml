<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-14T01:40:30Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pytube/pytube</title>
    <updated>2024-07-14T01:40:30Z</updated>
    <id>tag:github.com,2024-07-14:/pytube/pytube</id>
    <link href="https://github.com/pytube/pytube" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A lightweight, dependency-free Python library (and command-line utility) for downloading YouTube Videos.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytube/pytube/master/#&#34;&gt;&lt;img src=&#34;https://assets.nickficano.com/gh-pytube.min.svg?sanitize=true&#34; width=&#34;456&#34; height=&#34;143&#34; alt=&#34;pytube logo&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/pytube/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/pytube?style=flat-square&#34; alt=&#34;pypi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytube.io/en/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/python-pytube/badge/?version=latest&amp;amp;style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pytube/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pytube?style=flat-square&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Actively soliciting contributors!&lt;/h3&gt; &#xA;&lt;p&gt;Have ideas for how pytube can be improved? Feel free to open an issue or a pull request!&lt;/p&gt; &#xA;&lt;h1&gt;pytube&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;pytube&lt;/em&gt; is a genuine, lightweight, dependency-free Python library (and command-line utility) for downloading YouTube videos.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Detailed documentation about the usage of the library can be found at &lt;a href=&#34;https://pytube.io&#34;&gt;pytube.io&lt;/a&gt;. This is recommended for most cases. If you want to hastily download a single video, the &lt;a href=&#34;https://raw.githubusercontent.com/pytube/pytube/master/#Quickstart&#34;&gt;quick start&lt;/a&gt; guide below might be what you&#39;re looking for.&lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;YouTube is the most popular video-sharing platform in the world and as a hacker, you may encounter a situation where you want to script something to download videos. For this, I present to you: &lt;em&gt;pytube&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;pytube&lt;/em&gt; is a lightweight library written in Python. It has no third-party dependencies and aims to be highly reliable.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;pytube&lt;/em&gt; also makes pipelining easy, allowing you to specify callback functions for different download events, such as &lt;code&gt;on progress&lt;/code&gt; or &lt;code&gt;on complete&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Furthermore, &lt;em&gt;pytube&lt;/em&gt; includes a command-line utility, allowing you to download videos right from the terminal.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for both progressive &amp;amp; DASH streams&lt;/li&gt; &#xA; &lt;li&gt;Support for downloading the complete playlist&lt;/li&gt; &#xA; &lt;li&gt;Easily register &lt;code&gt;on_download_progress&lt;/code&gt; &amp;amp; &lt;code&gt;on_download_complete&lt;/code&gt; callbacks&lt;/li&gt; &#xA; &lt;li&gt;Command-line interfaced included&lt;/li&gt; &#xA; &lt;li&gt;Caption track support&lt;/li&gt; &#xA; &lt;li&gt;Outputs caption tracks to .srt format (SubRip Subtitle)&lt;/li&gt; &#xA; &lt;li&gt;Ability to capture thumbnail URL&lt;/li&gt; &#xA; &lt;li&gt;Extensively documented source code&lt;/li&gt; &#xA; &lt;li&gt;No third-party dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;This guide covers the most basic usage of the library. For more detailed information, please refer to &lt;a href=&#34;https://pytube.io&#34;&gt;pytube.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Pytube requires an installation of Python 3.6 or greater, as well as pip. (Pip is typically bundled with Python &lt;a href=&#34;https://python.org/downloads&#34;&gt;installations&lt;/a&gt;.)&lt;/p&gt; &#xA;&lt;p&gt;To install from PyPI with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python -m pip install pytube&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sometimes, the PyPI release becomes slightly outdated. To install from the source with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python -m pip install git+https://github.com/pytube/pytube&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using pytube in a Python script&lt;/h3&gt; &#xA;&lt;p&gt;To download a video using the library in a script, you&#39;ll need to import the YouTube class from the library and pass an argument of the video URL. From there, you can access the streams and download them.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; &amp;gt;&amp;gt;&amp;gt; from pytube import YouTube&#xA; &amp;gt;&amp;gt;&amp;gt; YouTube(&#39;https://youtu.be/2lAe1cqCOXo&#39;).streams.first().download()&#xA; &amp;gt;&amp;gt;&amp;gt; yt = YouTube(&#39;http://youtube.com/watch?v=2lAe1cqCOXo&#39;)&#xA; &amp;gt;&amp;gt;&amp;gt; yt.streams&#xA;  ... .filter(progressive=True, file_extension=&#39;mp4&#39;)&#xA;  ... .order_by(&#39;resolution&#39;)&#xA;  ... .desc()&#xA;  ... .first()&#xA;  ... .download()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the command-line interface&lt;/h3&gt; &#xA;&lt;p&gt;Using the CLI is remarkably straightforward as well. To download a video at the highest progressive quality, you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pytube https://youtube.com/watch?v=2lAe1cqCOXo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also do the same for a playlist:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pytube https://www.youtube.com/playlist?list=PLS1QulWo1RIaJECMeUT4LFwJ-ghgoSH6n&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>stanford-oval/storm</title>
    <updated>2024-07-14T01:40:30Z</updated>
    <id>tag:github.com,2024-07-14:/stanford-oval/storm</id>
    <link href="https://github.com/stanford-oval/storm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/logo.svg?sanitize=true&#34; style=&#34;width: 25%; height: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;http://storm.genie.stanford.edu&#34;&gt;&lt;b&gt;Research preview&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://storm-project.stanford.edu/&#34;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Latest News&lt;/strong&gt; üî•&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/07] We add &lt;code&gt;VectorRM&lt;/code&gt; to support grounding on user-provided documents, complementing existing support of search engines (&lt;code&gt;YouRM&lt;/code&gt;, &lt;code&gt;BingSearch&lt;/code&gt;). (check out &lt;a href=&#34;https://github.com/stanford-oval/storm/pull/58&#34;&gt;#58&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[2024/07] We release demo light for developers a minimal user interface built with streamlit framework in Python, handy for local development and demo hosting (checkout &lt;a href=&#34;https://github.com/stanford-oval/storm/pull/54&#34;&gt;#54&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[2024/06] We will present STORM at NAACL 2024! Find us at Poster Session 2 on June 17 or check our &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/storm_naacl2024_slides.pdf&#34;&gt;presentation material&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] We add Bing Search support in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/rm.py&#34;&gt;rm.py&lt;/a&gt;. Test STORM with &lt;code&gt;GPT-4o&lt;/code&gt; - we now configure the article generation part in our demo using &lt;code&gt;GPT-4o&lt;/code&gt; model.&lt;/li&gt; &#xA; &lt;li&gt;[2024/04] We release refactored version of STORM codebase! We define &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/interface.py&#34;&gt;interface&lt;/a&gt; for STORM pipeline and reimplement STORM-wiki (check out &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/storm_wiki&#34;&gt;&lt;code&gt;src/storm_wiki&lt;/code&gt;&lt;/a&gt;) to demonstrate how to instantiate the pipeline. We provide API to support customization of different language models and retrieval/search integration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;(Try STORM now!)&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/overview.svg?sanitize=true&#34; style=&#34;width: 90%; height: auto;&#34;&gt; &lt;/p&gt; STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. &#xA;&lt;p&gt;While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Try out our &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;live research preview&lt;/a&gt; to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system üôè!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How STORM works&lt;/h2&gt; &#xA;&lt;p&gt;STORM breaks down generating long articles with citations into two steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-writing stage&lt;/strong&gt;: The system conducts Internet-based research to collect references and generates an outline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Writing stage&lt;/strong&gt;: The system uses the outline and references to generate the full-length article with citations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/two_stages.jpg&#34; style=&#34;width: 60%; height: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Perspective-Guided Question Asking&lt;/strong&gt;: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simulated Conversation&lt;/strong&gt;: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Based on the separation of the two stages, STORM is implemented in a highly modular way using &lt;a href=&#34;https://github.com/stanfordnlp/dspy&#34;&gt;dspy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;1. Setup&lt;/h3&gt; &#xA;&lt;p&gt;Below, we provide a quick start guide to run STORM locally.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the git repository.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/stanford-oval/storm.git&#xA;cd storm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required packages.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n storm python=3.11&#xA;conda activate storm&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up OpenAI API key (if you want to use OpenAI models to power STORM) and &lt;a href=&#34;https://api.you.com/&#34;&gt;You.com search API&lt;/a&gt; key. Create a file &lt;code&gt;secrets.toml&lt;/code&gt; under the root directory and add the following content:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Set up OpenAI API key.&#xA;OPENAI_API_KEY=&#34;your_openai_api_key&#34;&#xA;# If you are using the API service provided by OpenAI, include the following line:&#xA;OPENAI_API_TYPE=&#34;openai&#34;&#xA;# If you are using the API service provided by Microsoft Azure, include the following lines:&#xA;OPENAI_API_TYPE=&#34;azure&#34;&#xA;AZURE_API_BASE=&#34;your_azure_api_base_url&#34;&#xA;AZURE_API_VERSION=&#34;your_azure_api_version&#34;&#xA;# Set up You.com search API key.&#xA;YDC_API_KEY=&#34;your_youcom_api_key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2. Running STORM-wiki locally&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;To run STORM with &lt;code&gt;gpt&lt;/code&gt; family models with default configurations&lt;/strong&gt;: Make sure you have set up the OpenAI API key and run the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/run_storm_wiki_gpt.py \&#xA;    --output-dir $OUTPUT_DIR \&#xA;    --retriever you \&#xA;    --do-research \&#xA;    --do-generate-outline \&#xA;    --do-generate-article \&#xA;    --do-polish-article&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-research&lt;/code&gt;: if True, simulate conversation to research the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-generate-outline&lt;/code&gt;: If True, generate an outline for the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-generate-article&lt;/code&gt;: If True, generate an article for the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-polish-article&lt;/code&gt;: If True, polish the article by adding a summarization section and (optionally) removing duplicate content.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We provide more example scripts under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; to demonstrate how you can run STORM using your favorite language models or grounding on your own corpus.&lt;/p&gt; &#xA;&lt;h2&gt;Customize STORM&lt;/h2&gt; &#xA;&lt;h3&gt;Customization of the Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;Besides running scripts in &lt;code&gt;examples&lt;/code&gt;, you can customize STORM based on your own use case. STORM engine consists of 4 modules:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Knowledge Curation Module: Collects a broad coverage of information about the given topic.&lt;/li&gt; &#xA; &lt;li&gt;Outline Generation Module: Organizes the collected information by generating a hierarchical outline for the curated knowledge.&lt;/li&gt; &#xA; &lt;li&gt;Article Generation Module: Populates the generated outline with the collected information.&lt;/li&gt; &#xA; &lt;li&gt;Article Polishing Module: Refines and enhances the written article for better presentation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The interface for each module is defined in &lt;code&gt;src/interface.py&lt;/code&gt;, while their implementations are instantiated in &lt;code&gt;src/storm_wiki/modules/*&lt;/code&gt;. These modules can be customized according to your specific requirements (e.g., generating sections in bullet point format instead of full paragraphs).&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üåü&lt;/span&gt; &lt;strong&gt;You can share your customization of &lt;code&gt;Engine&lt;/code&gt; by making PRs to this repo!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Customization of Retriever Module&lt;/h3&gt; &#xA;&lt;p&gt;As a knowledge curation engine, STORM grabs information from the Retriever module. The Retriever modules are implemented in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/rm.py&#34;&gt;&lt;code&gt;src/rm.py&lt;/code&gt;&lt;/a&gt;. Currently, STORM supports the following retrievers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;YouRM&lt;/code&gt;: You.com search engine API&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;BingSearch&lt;/code&gt;: Bing Search API&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;VectorRM&lt;/code&gt;: a retrieval model that retrieves information from user provide corpus&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;üåü&lt;/span&gt; &lt;strong&gt;PRs for integrating more search engines/retrievers are highly appreciated!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Customization of Language Models&lt;/h3&gt; &#xA;&lt;p&gt;STORM provides the following language model implementations in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/lm.py&#34;&gt;&lt;code&gt;src/lm.py&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OpenAIModel&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ClaudeModel&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;VLLMClient&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TGIClient&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TogetherClient&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;üåü&lt;/span&gt; &lt;strong&gt;PRs for integrating more language model clients are highly appreciated!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üí°&lt;/span&gt; &lt;strong&gt;For a good practice,&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;choose a cheaper/faster model for &lt;code&gt;conv_simulator_lm&lt;/code&gt; which is used to split queries, synthesize answers in the conversation.&lt;/li&gt; &#xA; &lt;li&gt;if you need to conduct the actual writing step, choose a more powerful model for &lt;code&gt;article_gen_lm&lt;/code&gt;. Based on our experiments, weak models are bad at generating text with citations.&lt;/li&gt; &#xA; &lt;li&gt;for open models, adding one-shot example can help it better follow instructions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to the scripts in the &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory for concrete guidance on customizing the language model used in the pipeline.&lt;/p&gt; &#xA;&lt;h2&gt;Replicate NAACL2024 result&lt;/h2&gt; &#xA;&lt;p&gt;Please switch to the branch &lt;code&gt;NAACL-2024-code-backup&lt;/code&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show me instructions&lt;/summary&gt; &#xA; &lt;h3&gt;Paper Experiments&lt;/h3&gt; &#xA; &lt;p&gt;The FreshWiki dataset used in our experiments can be found in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/FreshWiki&#34;&gt;./FreshWiki&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Run the following commands under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src&#34;&gt;./src&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h4&gt;Pre-writing Stage&lt;/h4&gt; &#xA; &lt;p&gt;For batch experiment on FreshWiki dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_prewriting --input-source file --input-path ../FreshWiki/topic_list.csv  --engine gpt-4 --do-research --max-conv-turn 5 --max-perspective 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--engine&lt;/code&gt; (choices=[&lt;code&gt;gpt-4&lt;/code&gt;, &lt;code&gt;gpt-35-turbo&lt;/code&gt;]): the LLM engine used for generating the outline&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--do-research&lt;/code&gt;: if True, simulate conversation to research the topic; otherwise, load the results.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--max-conv-turn&lt;/code&gt;: the maximum number of questions for each information-seeking conversation&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--max-perspective&lt;/code&gt;: the maximum number of perspectives to be considered, each perspective corresponds to an information-seeking conversation. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;STORM also uses a general conversation to collect basic information about the topic. So, the maximum number of QA pairs is &lt;code&gt;max_turn * (max_perspective + 1)&lt;/code&gt;. &lt;span&gt;üí°&lt;/span&gt; Reducing &lt;code&gt;max_turn&lt;/code&gt; or &lt;code&gt;max_perspective&lt;/code&gt; can speed up the process and reduce the cost but may result in less comprehensive outline.&lt;/li&gt; &#xA;    &lt;li&gt;The parameter will not have any effect if &lt;code&gt;--disable-perspective&lt;/code&gt; is set (the perspective-driven question asking is disabled).&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;To run the experiment on a single topic:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_prewriting --input-source console --engine gpt-4 --max-conv-turn 5 --max-perspective 5 --do-research&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;The script will ask you to enter the &lt;code&gt;Topic&lt;/code&gt; and the &lt;code&gt;Ground truth url&lt;/code&gt; that will be excluded. If you do not have any url to exclude, leave that field empty.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The generated outline will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_outline.txt&lt;/code&gt; and the collected references will be saved in &lt;code&gt;{output_dir}/{topic}/raw_search_results.json&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;h4&gt;Writing Stage&lt;/h4&gt; &#xA; &lt;p&gt;For batch experiment on FreshWiki dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_writing --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-polish-article --remove-duplicate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--do-polish-article&lt;/code&gt;: if True, polish the article by adding a summarization section and removing duplicate content if &lt;code&gt;--remove-duplicate&lt;/code&gt; is set True.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;To run the experiment on a single topic:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_writing --input-source console --engine gpt-4 --do-polish-article --remove-duplicate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;The script will ask you to enter the &lt;code&gt;Topic&lt;/code&gt;. Please enter the same topic as the one used in the pre-writing stage.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The generated article will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_article.txt&lt;/code&gt; and the references corresponding to citation index will be saved in &lt;code&gt;{output_dir}/{topic}/url_to_info.json&lt;/code&gt;. If &lt;code&gt;--do-polish-article&lt;/code&gt; is set, the polished article will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_article_polished.txt&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Customize the STORM Configurations&lt;/h3&gt; &#xA; &lt;p&gt;We set up the default LLM configuration in &lt;code&gt;LLMConfigs&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/modules/utils.py&#34;&gt;src/modules/utils.py&lt;/a&gt;. You can use &lt;code&gt;set_conv_simulator_lm()&lt;/code&gt;,&lt;code&gt;set_question_asker_lm()&lt;/code&gt;, &lt;code&gt;set_outline_gen_lm()&lt;/code&gt;, &lt;code&gt;set_article_gen_lm()&lt;/code&gt;, &lt;code&gt;set_article_polish_lm()&lt;/code&gt; to override the default configuration. These functions take in an instance from &lt;code&gt;dspy.dsp.LM&lt;/code&gt; or &lt;code&gt;dspy.dsp.HFModel&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Automatic Evaluation&lt;/h3&gt; &#xA; &lt;p&gt;In our paper, we break down the evaluation into two parts: outline quality and full-length article quality.&lt;/p&gt; &#xA; &lt;h4&gt;Outline Quality&lt;/h4&gt; &#xA; &lt;p&gt;We introduce &lt;em&gt;heading soft recall&lt;/em&gt; and &lt;em&gt;heading entity recall&lt;/em&gt; to evaluate the outline quality. This makes it easier to prototype methods for pre-writing.&lt;/p&gt; &#xA; &lt;p&gt;Run the following command under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval&#34;&gt;./eval&lt;/a&gt; to compute the metrics on FreshWiki dataset:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_outline_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --pred-file-name storm_gen_outline.txt --result-output-path ../results/storm_outline_quality.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Full-length Article Quality&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/eval_article_quality.py&#34;&gt;eval/eval_article_quality.py&lt;/a&gt; provides the entry point of evaluating full-length article quality using ROUGE, entity recall, and rubric grading. Run the following command under &lt;code&gt;eval&lt;/code&gt; to compute the metrics:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_article_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --gt-dir ../FreshWiki --output-dir ../results/storm_article_eval_results --pred-file-name storm_gen_article_polished.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Use the Metric Yourself&lt;/h4&gt; &#xA; &lt;p&gt;The similarity-based metrics (i.e., ROUGE, entity recall, and heading entity recall) are implemented in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/metrics.py&#34;&gt;eval/metrics.py&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For rubric grading, we use the &lt;a href=&#34;https://huggingface.co/prometheus-eval/prometheus-13b-v1.0&#34;&gt;prometheus-13b-v1.0&lt;/a&gt; introduced in &lt;a href=&#34;https://arxiv.org/abs/2310.08491&#34;&gt;this paper&lt;/a&gt;. &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/evaluation_prometheus.py&#34;&gt;eval/evaluation_prometheus.py&lt;/a&gt; provides the entry point of using the metric.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!&lt;/p&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;mailto:shaoyj@stanford.edu&#34;&gt;Yijia Shao&lt;/a&gt; and &lt;a href=&#34;mailto:yuchengj@stanford.edu&#34;&gt;Yucheng Jiang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank Wikipedia for their excellent open-source content. The FreshWiki dataset is sourced from Wikipedia, licensed under the Creative Commons Attribution-ShareAlike (CC BY-SA) license.&lt;/p&gt; &#xA;&lt;p&gt;We are very grateful to &lt;a href=&#34;https://michelle123lam.github.io/&#34;&gt;Michelle Lam&lt;/a&gt; for designing the logo for this project and &lt;a href=&#34;https://dekun.me&#34;&gt;Dekun Ma&lt;/a&gt; for leading the UI development.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper if you use this code or part of it in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{shao2024assisting,&#xA;      title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, &#xA;      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},&#xA;      year={2024},&#xA;      booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Lightning-AI/litgpt</title>
    <updated>2024-07-14T01:40:30Z</updated>
    <id>tag:github.com,2024-07-14:/Lightning-AI/litgpt</id>
    <link href="https://github.com/Lightning-AI/litgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;20+ high-performance LLMs with recipes to pretrain, finetune and deploy at scale.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;‚ö° LitGPT&lt;/h1&gt; &#xA; &lt;p&gt;&lt;strong&gt;20+ high-performance LLMs with recipes to pretrain, finetune, deploy at scale.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&#xA;‚úÖ From scratch implementations     ‚úÖ No abstractions    ‚úÖ Beginner friendly   &#xA;‚úÖ Flash attention                  ‚úÖ FSDP               ‚úÖ LoRA, QLoRA, Adapter&#xA;‚úÖ Reduce GPU memory (fp4/8/16/32)  ‚úÖ 1-1000+ GPUs/TPUs  ‚úÖ 20+ LLMs            &#xA;&lt;/pre&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pytorch-lightning&#34; alt=&#34;PyPI - Python Version&#34;&gt; &lt;img src=&#34;https://github.com/lightning-AI/lit-stablelm/actions/workflows/cpu-tests.yml/badge.svg?sanitize=true&#34; alt=&#34;cpu-tests&#34;&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lit-stablelm/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/VptPCZkGNa&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1077906959069626439&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://lightning.ai/&#34;&gt;Lightning AI&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#quick-start&#34;&gt;Quick start&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#choose-from-20-llms&#34;&gt;Models&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#finetune-an-llm&#34;&gt;Finetune&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#deploy-an-llm&#34;&gt;Deploy&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#all-workflows&#34;&gt;All workflows&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#state-of-the-art-features&#34;&gt;Features&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#training-recipes&#34;&gt;Recipes (YAML)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#tutorials&#34;&gt;Tutorials&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://lightning.ai/lightning-ai/studios/litgpt-quick-start&#34;&gt; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/get-started-badge.svg?sanitize=true&#34; height=&#34;36px&#34; alt=&#34;Get started&#34;&gt; &lt;/a&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Use, finetune, pretrain, deploy LLMs Lightning fast ‚ö°‚ö°&lt;/h1&gt; &#xA;&lt;p&gt;Every LLM is implemented from scratch with &lt;strong&gt;no abstractions&lt;/strong&gt; and &lt;strong&gt;full control&lt;/strong&gt;, making them blazing fast, minimal, and performant at enterprise scale.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &lt;strong&gt;Enterprise ready -&lt;/strong&gt; Apache 2.0 for unlimited enterprise use.&lt;br&gt; ‚úÖ &lt;strong&gt;Developer friendly -&lt;/strong&gt; Easy debugging with no abstraction layers and single file implementations.&lt;br&gt; ‚úÖ &lt;strong&gt;Optimized performance -&lt;/strong&gt; Models designed to maximize performance, reduce costs, and speed up training.&lt;br&gt; ‚úÖ &lt;strong&gt;Proven recipes -&lt;/strong&gt; Highly-optimized training/finetuning recipes tested at enterprise scale.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;p&gt;Install LitGPT&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install &#39;litgpt[all]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Load and use any of the &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#choose-from-20-llms&#34;&gt;20+ LLMs&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litgpt import LLM&#xA;&#xA;llm = LLM.load(&#34;microsoft/phi-2&#34;)&#xA;text = llm.generate(&#34;Fix the spelling: Every fall, the familly goes to the mountains.&#34;)&#xA;print(text)&#xA;# Corrected Sentence: Every fall, the family goes to the mountains.       &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ Optimized for fast inference&lt;br&gt; ‚úÖ Quantization&lt;br&gt; ‚úÖ Runs on low-memory GPUs&lt;br&gt; ‚úÖ No layers of internal abstractions&lt;br&gt; ‚úÖ Optimized for production scale&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Advanced install options&lt;/summary&gt; &#xA; &lt;p&gt;Install from source:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Lightning-AI/litgpt&#xA;cd litgpt&#xA;pip install -e &#39;.[all]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/python-api.md&#34;&gt;Explore the full Python API docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Choose from 20+ LLMs&lt;/h1&gt; &#xA;&lt;p&gt;Every model is written from scratch to maximize performance and remove layers of abstraction:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Model size&lt;/th&gt; &#xA;   &lt;th&gt;Author&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3&lt;/td&gt; &#xA;   &lt;td&gt;8B, 70B&lt;/td&gt; &#xA;   &lt;td&gt;Meta AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/meta-llama/llama3&#34;&gt;Meta AI 2024&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 70B&lt;/td&gt; &#xA;   &lt;td&gt;Meta AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Touvron et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Code Llama&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 34B, 70B&lt;/td&gt; &#xA;   &lt;td&gt;Meta AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.12950&#34;&gt;Rozi√®re et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral MoE&lt;/td&gt; &#xA;   &lt;td&gt;8x7B&lt;/td&gt; &#xA;   &lt;td&gt;Mistral AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mistral.ai/news/mixtral-of-experts/&#34;&gt;Mistral AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;Mistral AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;Mistral AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeGemma&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.google.dev/gemma/docs/codegemma&#34;&gt;Google Team, Google Deepmind&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;See full list of 20+ LLMs&lt;/summary&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;h4&gt;All models&lt;/h4&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Model size&lt;/th&gt; &#xA;    &lt;th&gt;Author&lt;/th&gt; &#xA;    &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;CodeGemma&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;Google&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://ai.google.dev/gemma/docs/codegemma&#34;&gt;Google Team, Google Deepmind&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code Llama&lt;/td&gt; &#xA;    &lt;td&gt;7B, 13B, 34B, 70B&lt;/td&gt; &#xA;    &lt;td&gt;Meta AI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.12950&#34;&gt;Rozi√®re et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Danube2&lt;/td&gt; &#xA;    &lt;td&gt;1.8B&lt;/td&gt; &#xA;    &lt;td&gt;H2O.ai&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://h2o.ai/platform/danube-1-8b/&#34;&gt;H2O.ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Dolly&lt;/td&gt; &#xA;    &lt;td&gt;3B, 7B, 12B&lt;/td&gt; &#xA;    &lt;td&gt;Databricks&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm&#34;&gt;Conover et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Falcon&lt;/td&gt; &#xA;    &lt;td&gt;7B, 40B, 180B&lt;/td&gt; &#xA;    &lt;td&gt;TII UAE&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://falconllm.tii.ae&#34;&gt;TII 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;FreeWilly2 (Stable Beluga 2)&lt;/td&gt; &#xA;    &lt;td&gt;70B&lt;/td&gt; &#xA;    &lt;td&gt;Stability AI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models&#34;&gt;Stability AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Function Calling Llama 2&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;Trelis&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2&#34;&gt;Trelis et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Gemma&lt;/td&gt; &#xA;    &lt;td&gt;2B, 7B&lt;/td&gt; &#xA;    &lt;td&gt;Google&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf&#34;&gt;Google Team, Google Deepmind&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Llama 2&lt;/td&gt; &#xA;    &lt;td&gt;7B, 13B, 70B&lt;/td&gt; &#xA;    &lt;td&gt;Meta AI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Touvron et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Llama 3&lt;/td&gt; &#xA;    &lt;td&gt;8B, 70B&lt;/td&gt; &#xA;    &lt;td&gt;Meta AI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/meta-llama/llama3&#34;&gt;Meta AI 2024&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;LongChat&lt;/td&gt; &#xA;    &lt;td&gt;7B, 13B&lt;/td&gt; &#xA;    &lt;td&gt;LMSYS&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://lmsys.org/blog/2023-06-29-longchat/&#34;&gt;LongChat Team 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MicroLlama&lt;/td&gt; &#xA;    &lt;td&gt;300M&lt;/td&gt; &#xA;    &lt;td&gt;Ken Wang&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/keeeeenw/MicroLlama&#34;&gt;MicroLlama repo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mixtral MoE&lt;/td&gt; &#xA;    &lt;td&gt;8x7B&lt;/td&gt; &#xA;    &lt;td&gt;Mistral AI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://mistral.ai/news/mixtral-of-experts/&#34;&gt;Mistral AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mistral&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;Mistral AI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;Mistral AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Nous-Hermes&lt;/td&gt; &#xA;    &lt;td&gt;7B, 13B, 70B&lt;/td&gt; &#xA;    &lt;td&gt;NousResearch&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/NousResearch&#34;&gt;Org page&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OpenLLaMA&lt;/td&gt; &#xA;    &lt;td&gt;3B, 7B, 13B&lt;/td&gt; &#xA;    &lt;td&gt;OpenLM Research&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/openlm-research/open_llama&#34;&gt;Geng &amp;amp; Liu 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Phi 1.5 &amp;amp; 2&lt;/td&gt; &#xA;    &lt;td&gt;1.3B, 2.7B&lt;/td&gt; &#xA;    &lt;td&gt;Microsoft Research&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.05463&#34;&gt;Li et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Phi 3&lt;/td&gt; &#xA;    &lt;td&gt;3.8B&lt;/td&gt; &#xA;    &lt;td&gt;Microsoft Research&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.14219&#34;&gt;Abdin et al. 2024&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Platypus&lt;/td&gt; &#xA;    &lt;td&gt;7B, 13B, 70B&lt;/td&gt; &#xA;    &lt;td&gt;Lee et al.&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.07317&#34;&gt;Lee, Hunter, and Ruiz 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Pythia&lt;/td&gt; &#xA;    &lt;td&gt;{14,31,70,160,410}M, {1,1.4,2.8,6.9,12}B&lt;/td&gt; &#xA;    &lt;td&gt;EleutherAI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.01373&#34;&gt;Biderman et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;RedPajama-INCITE&lt;/td&gt; &#xA;    &lt;td&gt;3B, 7B&lt;/td&gt; &#xA;    &lt;td&gt;Together&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://together.ai/blog/redpajama-models-v1&#34;&gt;Together 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;StableCode&lt;/td&gt; &#xA;    &lt;td&gt;3B&lt;/td&gt; &#xA;    &lt;td&gt;Stability AI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://stability.ai/blog/stablecode-llm-generative-ai-coding&#34;&gt;Stability AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;StableLM&lt;/td&gt; &#xA;    &lt;td&gt;3B, 7B&lt;/td&gt; &#xA;    &lt;td&gt;Stability AI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/Stability-AI/StableLM&#34;&gt;Stability AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;StableLM Zephyr&lt;/td&gt; &#xA;    &lt;td&gt;3B&lt;/td&gt; &#xA;    &lt;td&gt;Stability AI&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://stability.ai/blog/stablecode-llm-generative-ai-coding&#34;&gt;Stability AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;TinyLlama&lt;/td&gt; &#xA;    &lt;td&gt;1.1B&lt;/td&gt; &#xA;    &lt;td&gt;Zhang et al.&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/jzhang38/TinyLlama&#34;&gt;Zhang et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Vicuna&lt;/td&gt; &#xA;    &lt;td&gt;7B, 13B, 33B&lt;/td&gt; &#xA;    &lt;td&gt;LMSYS&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://lmsys.org/blog/2023-03-30-vicuna/&#34;&gt;Li et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can list all available models by running the &lt;code&gt;litgpt download list&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Workflows&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#finetune-an-llm&#34;&gt;Finetune&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#pretrain-an-llm&#34;&gt;Pretrain&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#continue-pretraining-an-llm&#34;&gt;Continued pretraining&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#evaluate-an-llm&#34;&gt;Evaluate&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#deploy-an-llm&#34;&gt;Deploy&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#test-an-llm&#34;&gt;Test&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;Use the command line interface to run advanced workflows such as pretraining or finetuning on your own data.&lt;/p&gt; &#xA;&lt;h2&gt;All workflows&lt;/h2&gt; &#xA;&lt;p&gt;After installing LitGPT, select the model and workflow to run (finetune, pretrain, evaluate, deploy, etc...):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ligpt [action] [model]&#xA;litgpt  serve     meta-llama/Meta-Llama-3-8B-Instruct&#xA;litgpt  finetune  meta-llama/Meta-Llama-3-8B-Instruct&#xA;litgpt  pretrain  meta-llama/Meta-Llama-3-8B-Instruct&#xA;litgpt  chat      meta-llama/Meta-Llama-3-8B-Instruct&#xA;litgpt  evaluate  meta-llama/Meta-Llama-3-8B-Instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Finetune an LLM&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://lightning.ai/lightning-ai/studios/litgpt-finetune&#34;&gt; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/run-on-studio.svg?sanitize=true&#34; height=&#34;36px&#34; alt=&#34;Run on Studios&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;Finetuning is the process of taking a pretrained AI model and further training it on a smaller, specialized dataset tailored to a specific task or application.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 0) setup your dataset&#xA;curl -L https://huggingface.co/datasets/ksaw008/finance_alpaca/resolve/main/finance_alpaca.json -o my_custom_dataset.json&#xA;&#xA;# 1) Finetune a model (auto downloads weights)&#xA;litgpt finetune microsoft/phi-2 \&#xA;  --data JSON \&#xA;  --data.json_path my_custom_dataset.json \&#xA;  --data.val_split_fraction 0.1 \&#xA;  --out_dir out/custom-model&#xA;&#xA;# 2) Test the model&#xA;litgpt chat out/custom-model/final&#xA;&#xA;# 3) Deploy the model&#xA;litgpt serve out/custom-model/final&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune.md&#34;&gt;Read the full finetuning docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Deploy an LLM&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://lightning.ai/lightning-ai/studios/litgpt-serve&#34;&gt; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/deploy-on-studios.svg?sanitize=true&#34; height=&#34;36px&#34; alt=&#34;Deploy on Studios&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;Deploy a pretrained or finetune LLM to use it in real-world applications. Deploy, automatically sets up a web server that can be accessed by a website or app.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# deploy an out-of-the-box LLM&#xA;litgpt serve microsoft/phi-2&#xA;&#xA;# deploy your own trained model&#xA;litgpt serve path/to/microsoft/phi-2/checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show code to query server:&lt;/summary&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;p&gt;Test the server in a separate terminal and integrate the model API into your AI product:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 3) Use the server (in a separate Python session)&#xA;import requests, json&#xA;response = requests.post(&#xA;    &#34;http://127.0.0.1:8000/predict&#34;,&#xA;    json={&#34;prompt&#34;: &#34;Fix typos in the following sentence: Exampel input&#34;}&#xA;)&#xA;print(response.json()[&#34;output&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/deploy.md&#34;&gt;Read the full deploy docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Evaluate an LLM&lt;/h2&gt; &#xA;&lt;p&gt;Evaluate an LLM to test its performance on various tasks to see how well it understands and generates text. Simply put, we can evaluate things like how well would it do in college-level chemistry, coding, etc... (MMLU, Truthful QA, etc...)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;litgpt evaluate microsoft/phi-2 --tasks &#39;truthfulqa_mc2,mmlu&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/evaluation.md&#34;&gt;Read the full evaluation docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Test an LLM&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://lightning.ai/lightning-ai/studios/litgpt-chat&#34;&gt; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/run-on-studio.svg?sanitize=true&#34; height=&#34;36px&#34; alt=&#34;Run on Studios&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;Test how well the model works via an interactive chat. Use the &lt;code&gt;chat&lt;/code&gt; command to chat, extract embeddings, etc...&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example showing how to use the Phi-2 LLM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;litgpt chat microsoft/phi-2&#xA;&#xA;&amp;gt;&amp;gt; Prompt: What do Llamas eat?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Full code:&lt;/summary&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1) List all supported LLMs&#xA;litgpt download list&#xA;&#xA;# 2) Use a model (auto downloads weights)&#xA;litgpt chat microsoft/phi-2&#xA;&#xA;&amp;gt;&amp;gt; Prompt: What do Llamas eat?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The download of certain models requires an additional access token. You can read more about this in the &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/download_model_weights.md#specific-models-and-access-tokens&#34;&gt;download&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/inference.md&#34;&gt;Read the full chat docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Pretrain an LLM&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://lightning.ai/lightning-ai/studios/litgpt-pretrain&#34;&gt; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/run-on-studio.svg?sanitize=true&#34; height=&#34;36px&#34; alt=&#34;Run on Studios&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;Pretraining is the process of teaching an AI model by exposing it to a large amount of data before it is fine-tuned for specific tasks.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show code:&lt;/summary&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p custom_texts&#xA;curl https://www.gutenberg.org/cache/epub/24440/pg24440.txt --output custom_texts/book1.txt&#xA;curl https://www.gutenberg.org/cache/epub/26393/pg26393.txt --output custom_texts/book2.txt&#xA;&#xA;# 1) Download a tokenizer&#xA;litgpt download EleutherAI/pythia-160m \&#xA;  --tokenizer_only True&#xA;&#xA;# 2) Pretrain the model&#xA;litgpt pretrain EleutherAI/pythia-160m \&#xA;  --tokenizer_dir EleutherAI/pythia-160m \&#xA;  --data TextFiles \&#xA;  --data.train_data_path &#34;custom_texts/&#34; \&#xA;  --train.max_tokens 10_000_000 \&#xA;  --out_dir out/custom-model&#xA;&#xA;# 3) Test the model&#xA;litgpt chat out/custom-model/final&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain.md&#34;&gt;Read the full pretraining docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Continue pretraining an LLM&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://lightning.ai/lightning-ai/studios/litgpt-continue-pretraining&#34;&gt; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/run-on-studio.svg?sanitize=true&#34; height=&#34;36px&#34; alt=&#34;Run on Studios&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;Continued pretraining is another way of finetuning that specializes an already pretrained model by training on custom data:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show code:&lt;/summary&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p custom_texts&#xA;curl https://www.gutenberg.org/cache/epub/24440/pg24440.txt --output custom_texts/book1.txt&#xA;curl https://www.gutenberg.org/cache/epub/26393/pg26393.txt --output custom_texts/book2.txt&#xA;&#xA;# 1) Continue pretraining a model (auto downloads weights)&#xA;litgpt pretrain EleutherAI/pythia-160m \&#xA;  --tokenizer_dir EleutherAI/pythia-160m \&#xA;  --initial_checkpoint_dir EleutherAI/pythia-160m \&#xA;  --data TextFiles \&#xA;  --data.train_data_path &#34;custom_texts/&#34; \&#xA;  --train.max_tokens 10_000_000 \&#xA;  --out_dir out/custom-model&#xA;&#xA;# 2) Test the model&#xA;litgpt chat out/custom-model/final&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain.md#continued-pretraining-on-custom-data&#34;&gt;Read the full continued pretraining docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;State-of-the-art features&lt;/h1&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;State-of-the-art optimizations: Flash Attention v2, multi-GPU support via fully-sharded data parallelism, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/oom.md#do-sharding-across-multiple-gpus&#34;&gt;optional CPU offloading&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/extensions/xla&#34;&gt;TPU and XLA support&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain.md&#34;&gt;Pretrain&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune.md&#34;&gt;finetune&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/inference.md&#34;&gt;deploy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Reduce compute requirements with low-precision settings: FP16, BF16, and FP16/FP32 mixed.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Lower memory requirements with &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/quantize.md&#34;&gt;quantization&lt;/a&gt;: 4-bit floats, 8-bit integers, and double quantization.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub&#34;&gt;Configuration files&lt;/a&gt; for great out-of-the-box performance.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Parameter-efficient finetuning: &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_lora.md&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_lora.md&#34;&gt;QLoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_adapter.md&#34;&gt;Adapter&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_adapter.md&#34;&gt;Adapter v2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/convert_lit_models.md&#34;&gt;Exporting&lt;/a&gt; to other popular model weight formats.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Many popular datasets for &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain.md&#34;&gt;pretraining&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/prepare_dataset.md&#34;&gt;finetuning&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/prepare_dataset.md#preparing-custom-datasets-for-instruction-finetuning&#34;&gt;support for custom datasets&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Readable and easy-to-modify code to experiment with the latest research ideas.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Training recipes&lt;/h1&gt; &#xA;&lt;p&gt;LitGPT comes with validated recipes (YAML configs) to train models under different conditions. We&#39;ve generated these recipes based on the parameters we found to perform the best for different training conditions.&lt;/p&gt; &#xA;&lt;p&gt;Browse all training recipes &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;litgpt finetune \&#xA;  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚úÖ Use configs to customize training&lt;/summary&gt; &#xA; &lt;p&gt;Configs let you customize training for all granular parameters like:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# The path to the base model&#39;s checkpoint directory to load for finetuning. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: checkpoints/stabilityai/stablelm-base-alpha-3b)&#xA;checkpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf&#xA;&#xA;# Directory in which to save checkpoints and logs. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: out/lora)&#xA;out_dir: out/finetune/qlora-llama2-7b&#xA;&#xA;# The precision to use for finetuning. Possible choices: &#34;bf16-true&#34;, &#34;bf16-mixed&#34;, &#34;32-true&#34;. (type: Optional[str], default: null)&#xA;precision: bf16-true&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚úÖ Example: LoRA finetuning config&lt;/summary&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# The path to the base model&#39;s checkpoint directory to load for finetuning. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: checkpoints/stabilityai/stablelm-base-alpha-3b)&#xA;checkpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf&#xA;&#xA;# Directory in which to save checkpoints and logs. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: out/lora)&#xA;out_dir: out/finetune/qlora-llama2-7b&#xA;&#xA;# The precision to use for finetuning. Possible choices: &#34;bf16-true&#34;, &#34;bf16-mixed&#34;, &#34;32-true&#34;. (type: Optional[str], default: null)&#xA;precision: bf16-true&#xA;&#xA;# If set, quantize the model with this algorithm. See ``tutorials/quantize.md`` for more information. (type: Optional[Literal[&#39;nf4&#39;, &#39;nf4-dq&#39;, &#39;fp4&#39;, &#39;fp4-dq&#39;, &#39;int8-training&#39;]], default: null)&#xA;quantize: bnb.nf4&#xA;&#xA;# How many devices/GPUs to use. (type: Union[int, str], default: 1)&#xA;devices: 1&#xA;&#xA;# How many nodes to use. (type: int, default: 1)&#xA;num_nodes: 1&#xA;&#xA;# The LoRA rank. (type: int, default: 8)&#xA;lora_r: 32&#xA;&#xA;# The LoRA alpha. (type: int, default: 16)&#xA;lora_alpha: 16&#xA;&#xA;# The LoRA dropout value. (type: float, default: 0.05)&#xA;lora_dropout: 0.05&#xA;&#xA;# Whether to apply LoRA to the query weights in attention. (type: bool, default: True)&#xA;lora_query: true&#xA;&#xA;# Whether to apply LoRA to the key weights in attention. (type: bool, default: False)&#xA;lora_key: false&#xA;&#xA;# Whether to apply LoRA to the value weights in attention. (type: bool, default: True)&#xA;lora_value: true&#xA;&#xA;# Whether to apply LoRA to the output projection in the attention block. (type: bool, default: False)&#xA;lora_projection: false&#xA;&#xA;# Whether to apply LoRA to the weights of the MLP in the attention block. (type: bool, default: False)&#xA;lora_mlp: false&#xA;&#xA;# Whether to apply LoRA to output head in GPT. (type: bool, default: False)&#xA;lora_head: false&#xA;&#xA;# Data-related arguments. If not provided, the default is ``litgpt.data.Alpaca``.&#xA;data:&#xA;  class_path: litgpt.data.Alpaca2k&#xA;  init_args:&#xA;    mask_prompt: false&#xA;    val_split_fraction: 0.05&#xA;    prompt_style: alpaca&#xA;    ignore_index: -100&#xA;    seed: 42&#xA;    num_workers: 4&#xA;    download_dir: data/alpaca2k&#xA;&#xA;# Training-related arguments. See ``litgpt.args.TrainArgs`` for details&#xA;train:&#xA;&#xA;  # Number of optimizer steps between saving checkpoints (type: Optional[int], default: 1000)&#xA;  save_interval: 200&#xA;&#xA;  # Number of iterations between logging calls (type: int, default: 1)&#xA;  log_interval: 1&#xA;&#xA;  # Number of samples between optimizer steps across data-parallel ranks (type: int, default: 128)&#xA;  global_batch_size: 8&#xA;&#xA;  # Number of samples per data-parallel rank (type: int, default: 4)&#xA;  micro_batch_size: 2&#xA;&#xA;  # Number of iterations with learning rate warmup active (type: int, default: 100)&#xA;  lr_warmup_steps: 10&#xA;&#xA;  # Number of epochs to train on (type: Optional[int], default: 5)&#xA;  epochs: 4&#xA;&#xA;  # Total number of tokens to train on (type: Optional[int], default: null)&#xA;  max_tokens:&#xA;&#xA;  # Limits the number of optimizer steps to run (type: Optional[int], default: null)&#xA;  max_steps:&#xA;&#xA;  # Limits the length of samples (type: Optional[int], default: null)&#xA;  max_seq_length: 512&#xA;&#xA;  # Whether to tie the embedding weights with the language modeling head weights (type: Optional[bool], default: null)&#xA;  tie_embeddings:&#xA;&#xA;  #   (type: float, default: 0.0003)&#xA;  learning_rate: 0.0002&#xA;&#xA;  #   (type: float, default: 0.02)&#xA;  weight_decay: 0.0&#xA;&#xA;  #   (type: float, default: 0.9)&#xA;  beta1: 0.9&#xA;&#xA;  #   (type: float, default: 0.95)&#xA;  beta2: 0.95&#xA;&#xA;  #   (type: Optional[float], default: null)&#xA;  max_norm:&#xA;&#xA;  #   (type: float, default: 6e-05)&#xA;  min_lr: 6.0e-05&#xA;&#xA;# Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details&#xA;eval:&#xA;&#xA;  # Number of optimizer steps between evaluation calls (type: int, default: 100)&#xA;  interval: 100&#xA;&#xA;  # Number of tokens to generate (type: Optional[int], default: 100)&#xA;  max_new_tokens: 100&#xA;&#xA;  # Number of iterations (type: int, default: 100)&#xA;  max_iters: 100&#xA;&#xA;# The name of the logger to send metrics to. (type: Literal[&#39;wandb&#39;, &#39;tensorboard&#39;, &#39;csv&#39;], default: csv)&#xA;logger_name: csv&#xA;&#xA;# The random seed to use for reproducibility. (type: int, default: 1337)&#xA;seed: 1337&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚úÖ Override any parameter in the CLI:&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;litgpt finetune \&#xA;  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml \&#xA;  --lora_r 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Project highlights&lt;/h1&gt; &#xA;&lt;p&gt;LitGPT powers many great AI projects, initiatives, challenges and of course enterprises. Please submit a pull request to be considered for a feature.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üìä SAMBA: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling&lt;/summary&gt; &#xA; &lt;p&gt;The &lt;a href=&#34;https://github.com/microsoft/Samba&#34;&gt;Samba&lt;/a&gt; project by researchers at Microsoft is built on top of the LitGPT code base and combines state space models with sliding window attention, which outperforms pure state space models.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üèÜ NeurIPS 2023 Large Language Model Efficiency Challenge: 1 LLM + 1 GPU + 1 Day&lt;/summary&gt; &#xA; &lt;p&gt;The LitGPT repository was the official starter kit for the &lt;a href=&#34;https://llm-efficiency-challenge.github.io&#34;&gt;NeurIPS 2023 LLM Efficiency Challenge&lt;/a&gt;, which is a competition focused on finetuning an existing non-instruction tuned LLM for 24 hours on a single GPU.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;ü¶ô TinyLlama: An Open-Source Small Language Model&lt;/summary&gt; &#xA; &lt;p&gt;LitGPT powered the &lt;a href=&#34;https://github.com/jzhang38/TinyLlama&#34;&gt;TinyLlama project&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2401.02385&#34;&gt;TinyLlama: An Open-Source Small Language Model&lt;/a&gt; research paper.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üç™ MicroLlama: MicroLlama-300M&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/keeeeenw/MicroLlama&#34;&gt;MicroLlama&lt;/a&gt; is a 300M Llama model pretrained on 50B tokens powered by TinyLlama and LitGPT.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üî¨ Pre-training Small Base LMs with Fewer Tokens&lt;/summary&gt; &#xA; &lt;p&gt;The research paper &lt;a href=&#34;https://arxiv.org/abs/2404.08634&#34;&gt;&#34;Pre-training Small Base LMs with Fewer Tokens&#34;&lt;/a&gt;, which utilizes LitGPT, develops smaller base language models by inheriting a few transformer blocks from larger models and training on a tiny fraction of the data used by the larger models. It demonstrates that these smaller models can perform comparably to larger models despite using significantly less training data and resources.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;p&gt;We welcome all individual contributors, regardless of their level of experience or hardware. Your contributions are valuable, and we are excited to see what you can accomplish in this collaborative and supportive environment.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lightning-AI/litgpt/issues&#34;&gt;Request a feature&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning.ai/pages/community/tutorial/how-to-contribute-to-litgpt/&#34;&gt;Submit your first contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/VptPCZkGNa&#34;&gt;Join our Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h1&gt;Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;üöÄ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/0_to_litgpt.md&#34;&gt;Get started&lt;/a&gt;&lt;br&gt; ‚ö°Ô∏è &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune.md&#34;&gt;Finetuning, incl. LoRA, QLoRA, and Adapters&lt;/a&gt;&lt;br&gt; ü§ñ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain.md&#34;&gt;Pretraining&lt;/a&gt;&lt;br&gt; üí¨ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/evaluation.md&#34;&gt;Model evaluation&lt;/a&gt;&lt;br&gt; üìò &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/prepare_dataset.md&#34;&gt;Supported and custom datasets&lt;/a&gt;&lt;br&gt; üßπ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/quantize.md&#34;&gt;Quantization&lt;/a&gt;&lt;br&gt; ü§Ø &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/oom.md&#34;&gt;Tips for dealing with out-of-memory (OOM) errors&lt;/a&gt;&lt;br&gt; üßëüèΩ‚Äçüíª &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/extensions/xla&#34;&gt;Using cloud TPUs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;p&gt;This implementation extends on &lt;a href=&#34;https://github.com/lightning-AI/lit-llama&#34;&gt;Lit-LLaMA&lt;/a&gt; and &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;, and it&#39;s &lt;strong&gt;powered by &lt;a href=&#34;https://lightning.ai/docs/fabric/stable/&#34;&gt;Lightning Fabric&lt;/a&gt; ‚ö°&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy&#34;&gt;@karpathy&lt;/a&gt; for &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI&#34;&gt;@EleutherAI&lt;/a&gt; for &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX&lt;/a&gt; and the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;Evaluation Harness&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TimDettmers&#34;&gt;@TimDettmers&lt;/a&gt; for &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft&#34;&gt;@Microsoft&lt;/a&gt; for &lt;a href=&#34;https://github.com/microsoft/LoRA&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tridao&#34;&gt;@tridao&lt;/a&gt; for &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;Flash Attention 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;LitGPT is released under the &lt;a href=&#34;https://github.com/Lightning-AI/litgpt/raw/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;If you use LitGPT in your research, please cite the following work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{litgpt-2023,&#xA;  author       = {Lightning AI},&#xA;  title        = {LitGPT},&#xA;  howpublished = {\url{https://github.com/Lightning-AI/litgpt}},&#xA;  year         = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</summary>
  </entry>
</feed>