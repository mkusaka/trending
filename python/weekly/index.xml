<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-20T01:45:50Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Alibaba-NLP/WebAgent</title>
    <updated>2025-07-20T01:45:50Z</updated>
    <id>tag:github.com,2025-07-20:/Alibaba-NLP/WebAgent</id>
    <link href="https://github.com/Alibaba-NLP/WebAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üåê WebAgent for Information Seeking built by Tongyi Lab: WebWalker &amp; WebDancer &amp; WebSailor &amp; WebShaper https://arxiv.org/pdf/2507.02592&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;WebAgent for Information Seeking built by Tongyi Lab, Alibaba Group &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png&#34; width=&#34;30px&#34; style=&#34;display:inline;&#34;&gt;&lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/14217&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/14217&#34; alt=&#34;Alibaba-NLP%2FWebAgent | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebSailor-3B&#34; target=&#34;_blank&#34;&gt;WebSailor-3B&lt;/a&gt; ÔΩú &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png&#34; width=&#34;14px&#34; style=&#34;display:inline;&#34;&gt; &lt;a href=&#34;https://modelscope.cn/models/iic/WebSailor-3B&#34; target=&#34;_blank&#34;&gt;ModelScope WebSailor-3B&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebDancer-32B&#34; target=&#34;_blank&#34;&gt;WebDancer-QwQ-32B&lt;/a&gt; | &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png&#34; width=&#34;14px&#34; style=&#34;display:inline;&#34;&gt; &lt;a href=&#34;https://modelscope.cn/models/iic/WebDancer-32B&#34; target=&#34;_blank&#34;&gt;ModelScope WebDancer-QwQ-32B&lt;/a&gt; | ü§ó &lt;a href=&#34;https://huggingface.co/datasets/callanwu/WebWalkerQA&#34; target=&#34;_blank&#34;&gt;WebWalkerQA&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/roadmap.png&#34; width=&#34;100%&#34; height=&#34;400%&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can check the paper of &lt;a href=&#34;https://arxiv.org/pdf/2505.22648&#34;&gt;WebDancer&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2501.07572&#34;&gt;WebWalker&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2507.02592&#34;&gt;WebSailor&lt;/a&gt; and &lt;a href=&#34;&#34;&gt;WebShaper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üí• üí• üí• Stay tuned for more updates! We are working on building native agentic model based on the Browser and more open-domain environments!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebShaper&#34;&gt;&lt;strong&gt;WebShaper&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebSailor&#34;&gt;&lt;strong&gt;WebSailor&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer&#34;&gt;&lt;strong&gt;WebDancer&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebDancer: Towards Autonomous Information Seeking Agency&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebWalker&#34;&gt;&lt;strong&gt;WebWalker&lt;/strong&gt;&lt;/a&gt; (ACL 2025) - WebWalker: Benchmarking LLMs in Web Traversal&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì∞ News and Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.07.19&lt;/code&gt; üî•üî•üî•&lt;strong&gt;WebShaper&lt;/strong&gt; is coming soon.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.07.11&lt;/code&gt; üî•üî•üî•&lt;strong&gt;WebSailor-3B&lt;/strong&gt; is &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebSailor-3B&#34;&gt;released&lt;/a&gt;. You can deploy it with one click using &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/aliyun.png&#34; width=&#34;14px&#34; style=&#34;display:inline;&#34;&gt; &lt;a href=&#34;https://functionai.console.aliyun.com/template-detail?template=Alibaba-NLP-WebSailor-3B&#34;&gt;Alibaba Cloud&#39;s FunctionAI&lt;/a&gt; in ten minutes!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.07.03&lt;/code&gt; üî•üî•üî•We release &lt;strong&gt;WebSailor&lt;/strong&gt;, an agentic search model specialized in performing extremely complex information seeking tasks, achieving open-source SOTA on some of the most difficult browsing benchmarks. &lt;strong&gt;WebSailor&lt;/strong&gt; topped the HuggingFace &lt;a href=&#34;https://huggingface.co/papers/2507.02592&#34;&gt;daily papers&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.06.23&lt;/code&gt; üî•üî•üî•The model, interactive demo, and some of the data of &lt;strong&gt;WebDancer&lt;/strong&gt; have been open-sourced. You&#39;re welcome to try them out!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.05.29&lt;/code&gt; üî•üî•üî•We release &lt;strong&gt;WebDancer&lt;/strong&gt;, a native agentic search model towards autonomous information seeking agency and &lt;em&gt;Deep Research&lt;/em&gt;-like model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.05.15&lt;/code&gt; &lt;strong&gt;WebWalker&lt;/strong&gt; is accepted by ACL 2025 main conference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.01.14&lt;/code&gt; We release &lt;strong&gt;WebWalker&lt;/strong&gt;, a benchmark for LLMs in web traversal and a multi-agent framework for information seeking.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíé Results Showcase&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/webagent-gaia.png&#34; width=&#34;800%&#34; height=&#34;400%&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/webagent-bc.png&#34; width=&#34;800%&#34; height=&#34;400%&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;‚õµÔ∏è Features for WebSailor&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A complete post-training methodology enabling models to engage in extended thinking and information seeking, ultimately allowing them to successfully complete extremely complex tasks previously considered unsolvable.&lt;/li&gt; &#xA; &lt;li&gt;Introduces &lt;strong&gt;SailorFog-QA&lt;/strong&gt;, a scalable QA benchmark with high uncertainty and difficulty, curated with a novel data synthesis method through graph sampling and information obfuscation. Example SailorFog-QA data samples can be found at: &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebSailor/dataset/sailorfog-QA.jsonl&#34;&gt;&lt;code&gt;WebSailor/dataset/sailorfog-QA.jsonl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Effective post-training pipeline consisting of (1) high-quality reconstruction of concise reasoning from expert trajectories for clean supervision, (2) a two-stage training process involving an RFT cold start stage, followed by &lt;strong&gt;Duplicating Sampling Policy Optimization (DUPO)&lt;/strong&gt;, an efficient agentic RL algorithm excelling in effectiveness and efficiency.&lt;/li&gt; &#xA; &lt;li&gt;WebSailor-72B significantly outperforms all open-source agents and frameworks while closing the performance gap with leading proprietary systems, achieving a score of &lt;strong&gt;12.0%&lt;/strong&gt; on BrowseComp-en, &lt;strong&gt;30.1%&lt;/strong&gt; on BrowseComp-zh, and &lt;strong&gt;55.4%&lt;/strong&gt; on GAIA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The checkpoint is coming soon.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåê Features for WebDancer&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Native agentic search reasoning model using ReAct framework towards autonomous information seeking agency and &lt;em&gt;Deep Research&lt;/em&gt;-like model.&lt;/li&gt; &#xA; &lt;li&gt;We introduce a four-stage training paradigm comprising &lt;strong&gt;browsing data construction, trajectory sampling, supervised fine-tuning for effective cold start, and reinforcement learning for improved generalization&lt;/strong&gt;, enabling the agent to autonomously acquire autonomous search and reasoning skills.&lt;/li&gt; &#xA; &lt;li&gt;Our data-centric approach integrates trajectory-level supervision fine-tuning and reinforcement learning (DAPO) to develop a scalable pipeline for &lt;strong&gt;training agentic systems&lt;/strong&gt; via SFT or RL.&lt;/li&gt; &#xA; &lt;li&gt;WebDancer achieves a Pass@3 score of 64.1% on GAIA and 62.0% on WebWalkerQA.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You need to enter the &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer&#34;&gt;&lt;code&gt;WebDancer&lt;/code&gt;&lt;/a&gt; folder for the following commands.&lt;/p&gt; &#xA;&lt;h3&gt;Step 0: Set Up the Environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n webdancer python=3.12&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 1: Deploy the Model&lt;/h3&gt; &#xA;&lt;p&gt;Download the WebDancer model from &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebDancer-32B&#34;&gt;ü§ó HuggingFace&lt;/a&gt; and deploy it using the provided scripts with &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;sglang&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd scripts&#xA;bash deploy_model.sh WebDancer_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Replace &lt;code&gt;WebDancer_PATH&lt;/code&gt; with the actual path to the downloaded model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Step 2: Run the Demo&lt;/h3&gt; &#xA;&lt;p&gt;Edit the following keys in &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer/scripts/run_demo.sh&#34;&gt;&lt;code&gt;WebDancer/scripts/run_demo.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;GOOGLE_SEARCH_KEY&lt;/code&gt;, you can get it from &lt;a href=&#34;https://serper.dev/&#34;&gt;serper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;JINA_API_KEY&lt;/code&gt;, you can get it from &lt;a href=&#34;https://jina.ai/api-dashboard/&#34;&gt;jina&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt;, you can get it from &lt;a href=&#34;https://dashscope.aliyun.com/&#34;&gt;dashscope&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then, launch the demo with Gradio to interact with the WebDancer model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd scripts&#xA;bash run_demo.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üé• WebSailor Demos&lt;/h2&gt; &#xA;&lt;p&gt;We provide demos for BrowseComp-en, BrowseComp-zh and Daily Use. Our model can complete highly difficult and uncertain tasks requiring massive information acquisition and complex reasoning.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;BrowseComp-en&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/2dc0b03a-c241-4f70-bf11-92fda28020fa&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;BrowseComp-zh&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/f9aed746-ffc8-4b76-b135-715ec0eab544&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;Daily Use&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/1299c5a8-cee3-4a70-b68b-c5d227cf8055&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üé• WebDancer Demos&lt;/h2&gt; &#xA;&lt;p&gt;We provide demos for WebWalkerQA, GAIA and Daily Use. Our model can execute the long-horizon tasks with &lt;strong&gt;multiple steps&lt;/strong&gt; and &lt;strong&gt;complex reasoning&lt;/strong&gt;, such as web traversal, information seeking and question answering.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;WebWalkerQA&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/0bbaf55b-897e-4c57-967d-a6e8bbd2167e&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;GAIA&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/935c668e-6169-4712-9c04-ac80f0531872&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;Daily Use&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/d1d5b533-4009-478b-bd87-96b86389327d&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üìÉ License&lt;/h2&gt; &#xA;&lt;p&gt;The content of this project itself is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üö© Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bigquery&#34;&gt;@misc{li2025websailor,&#xA;      title={WebSailor: Navigating Super-human Reasoning for Web Agent},&#xA;      author={Kuan Li and Zhongwang Zhang and Huifeng Yin and Liwen Zhang and Litu Ou and Jialong Wu and Wenbiao Yin and Baixuan Li and Zhengwei Tao and Xinyu Wang and Weizhou Shen and Junkai Zhang and Dingchu Zhang and Xixi Wu and Yong Jiang and Ming Yan and Pengjun Xie and Fei Huang and Jingren Zhou},&#xA;      year={2025},&#xA;      eprint={2507.02592},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2507.02592},&#xA;}&#xA;@misc{wu2025webdancer,&#xA;      title={WebDancer: Towards Autonomous Information Seeking Agency},&#xA;      author={Jialong Wu and Baixuan Li and Runnan Fang and Wenbiao Yin and Liwen Zhang and Zhengwei Tao and Dingchu Zhang and Zekun Xi and Yong Jiang and Pengjun Xie and Fei Huang and Jingren Zhou},&#xA;      year={2025},&#xA;      eprint={2505.22648},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2505.22648},&#xA;}&#xA;@misc{wu2025webwalker,&#xA;      title={WebWalker: Benchmarking LLMs in Web Traversal},&#xA;      author={Jialong Wu and Wenbiao Yin and Yong Jiang and Zhenglin Wang and Zekun Xi and Runnan Fang and Deyu Zhou and Pengjun Xie and Fei Huang},&#xA;      year={2025},&#xA;      eprint={2501.07572},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2501.07572},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üåü Misc&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#Alibaba-NLP/WebAgent&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Alibaba-NLP/WebAgent&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üö© Talent Recruitment&lt;/h2&gt; &#xA;&lt;p&gt;üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)&lt;/p&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;Research Area&lt;/strong&gt;ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; &#xA;&lt;p&gt;‚òéÔ∏è &lt;strong&gt;Contact&lt;/strong&gt;Ôºö&lt;a href=&#34;&#34;&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact Information&lt;/h2&gt; &#xA;&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href=&#34;mailto:yongjiang.jy@alibaba-inc.com&#34;&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/markitdown</title>
    <updated>2025-07-20T01:45:50Z</updated>
    <id>tag:github.com,2025-07-20:/microsoft/markitdown</id>
    <link href="https://github.com/microsoft/markitdown" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/markitdown/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/markitdown.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/dd/markitdown&#34; alt=&#34;PyPI - Downloads&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/autogen&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue&#34; alt=&#34;Built by AutoGen Team&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href=&#34;https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp&#34;&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install &#39;markitdown[all]&#39;&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; &#xA;  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; &#xA;  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href=&#34;https://github.com/deanmalmgren/textract&#34;&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; &#xA;&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PDF&lt;/li&gt; &#xA; &lt;li&gt;PowerPoint&lt;/li&gt; &#xA; &lt;li&gt;Word&lt;/li&gt; &#xA; &lt;li&gt;Excel&lt;/li&gt; &#xA; &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; &#xA; &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; &#xA; &lt;li&gt;HTML&lt;/li&gt; &#xA; &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; &#xA; &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; &#xA; &lt;li&gt;Youtube URLs&lt;/li&gt; &#xA; &lt;li&gt;EPubs&lt;/li&gt; &#xA; &lt;li&gt;... and more!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why Markdown?&lt;/h2&gt; &#xA;&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI&#39;s GPT-4o, natively &#34;&lt;em&gt;speak&lt;/em&gt;&#34; Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; &#xA;&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .venv&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv venv --python=3.12 .venv&#xA;source .venv/bin/activate&#xA;# NOTE: Be sure to use &#39;uv pip install&#39; rather than just &#39;pip install&#39; to install packages in this virtual environment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n markitdown python=3.12&#xA;conda activate markitdown&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install &#39;markitdown[all]&#39;&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:microsoft/markitdown.git&#xA;cd markitdown&#xA;pip install -e &#39;packages/markitdown[all]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Command-Line&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown path-to-file.pdf &amp;gt; document.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown path-to-file.pdf -o document.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also pipe content:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat path-to-file.pdf | markitdown&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Optional Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#39;markitdown[pdf, docx, pptx]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; &#xA;&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Plugins&lt;/h3&gt; &#xA;&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown --list-plugins&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable plugins use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown --use-plugins path-to-file.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; &#xA;&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown path-to-file.pdf -o document.md -d -e &#34;&amp;lt;document_intelligence_endpoint&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;p&gt;Basic usage in Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from markitdown import MarkItDown&#xA;&#xA;md = MarkItDown(enable_plugins=False) # Set to True to enable plugins&#xA;result = md.convert(&#34;test.xlsx&#34;)&#xA;print(result.text_content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from markitdown import MarkItDown&#xA;&#xA;md = MarkItDown(docintel_endpoint=&#34;&amp;lt;document_intelligence_endpoint&amp;gt;&#34;)&#xA;result = md.convert(&#34;test.pdf&#34;)&#xA;print(result.text_content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use Large Language Models for image descriptions, provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from markitdown import MarkItDown&#xA;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;md = MarkItDown(llm_client=client, llm_model=&#34;gpt-4o&#34;)&#xA;result = md.convert(&#34;example.jpg&#34;)&#xA;print(result.text_content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker build -t markitdown:latest .&#xA;docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h3&gt;How to Contribute&lt;/h3&gt; &#xA;&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#39;open for contribution&#39; and &#39;open for reviewing&#39; to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;/th&gt; &#xA;    &lt;th&gt;All&lt;/th&gt; &#xA;    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/markitdown/issues&#34;&gt;All Issues&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22&#34;&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/markitdown/pulls&#34;&gt;All PRs&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22&#34;&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd packages/markitdown&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/&#xA;hatch shell&#xA;hatch test&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Reopen the project in Devcontainer and run:&#xA;hatch test&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; &#xA;&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PromtEngineer/localGPT</title>
    <updated>2025-07-20T01:45:50Z</updated>
    <id>tag:github.com,2025-07-20:/PromtEngineer/localGPT</id>
    <link href="https://github.com/PromtEngineer/localGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with your documents on your local device using GPT models. No data leaves your device and 100% private.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LocalGPT: Secure, Local Conversations with Your Documents üåê&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/2947&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/2947&#34; alt=&#34;PromtEngineer%2FlocalGPT | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/PromtEngineer/localGPT/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PromtEngineer/localGPT?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PromtEngineer/localGPT/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/PromtEngineer/localGPT?style=social&#34; alt=&#34;GitHub Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PromtEngineer/localGPT/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PromtEngineer/localGPT&#34; alt=&#34;GitHub Issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PromtEngineer/localGPT/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/PromtEngineer/localGPT&#34; alt=&#34;GitHub Pull Requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PromtEngineer/localGPT/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/PromtEngineer/localGPT&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üö®üö® NEW VERSION OF LOCALGPT IS OUT ON THE &lt;a href=&#34;https://github.com/PromtEngineer/localGPT/tree/localgpt-v2&#34;&gt;LOCALGPT-V2&lt;/a&gt; BRANCH.&lt;/p&gt; &#xA;&lt;p&gt;You can run localGPT on a pre-configured &lt;a href=&#34;https://bit.ly/localGPT&#34;&gt;Virtual Machine&lt;/a&gt;. Make sure to use the code: PromptEngineering to get 50% off. I will get a small commision!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LocalGPT&lt;/strong&gt; is an open-source initiative that allows you to converse with your documents without compromising your privacy. With everything running locally, you can be assured that no data ever leaves your computer. Dive into the world of secure, local document interactions with LocalGPT.&lt;/p&gt; &#xA;&lt;h2&gt;Features üåü&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Utmost Privacy&lt;/strong&gt;: Your data remains on your computer, ensuring 100% security.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Versatile Model Support&lt;/strong&gt;: Seamlessly integrate a variety of open-source models, including HF, GPTQ, GGML, and GGUF.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diverse Embeddings&lt;/strong&gt;: Choose from a range of open-source embeddings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reuse Your LLM&lt;/strong&gt;: Once downloaded, reuse your LLM without the need for repeated downloads.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat History&lt;/strong&gt;: Remembers your previous conversations (in a session).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt;: LocalGPT has an API that you can use for building RAG Applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Graphical Interface&lt;/strong&gt;: LocalGPT comes with two GUIs, one uses the API and the other is standalone (based on streamlit).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPU, CPU, HPU &amp;amp; MPS Support&lt;/strong&gt;: Supports multiple platforms out of the box, Chat with your data using &lt;code&gt;CUDA&lt;/code&gt;, &lt;code&gt;CPU&lt;/code&gt;, &lt;code&gt;HPU (Intel¬Æ Gaudi¬Æ)&lt;/code&gt; or &lt;code&gt;MPS&lt;/code&gt; and more!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dive Deeper with Our Videos üé•&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/MlyoObdIHyo&#34;&gt;Detailed code-walkthrough&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/lbFmceo4D5E&#34;&gt;Llama-2 with LocalGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/d7otIM_MCZs&#34;&gt;Adding Chat History&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/G_prHSKX9d4&#34;&gt;LocalGPT - Updated (09/17/2023)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Technical Details üõ†Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;By selecting the right local models and the power of &lt;code&gt;LangChain&lt;/code&gt; you can run the entire RAG pipeline locally, without any data leaving your environment, and with reasonable performance.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;ingest.py&lt;/code&gt; uses &lt;code&gt;LangChain&lt;/code&gt; tools to parse the document and create embeddings locally using &lt;code&gt;InstructorEmbeddings&lt;/code&gt;. It then stores the result in a local vector database using &lt;code&gt;Chroma&lt;/code&gt; vector store.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;run_localGPT.py&lt;/code&gt; uses a local LLM to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the docs.&lt;/li&gt; &#xA; &lt;li&gt;You can replace this local LLM with any other LLM from the HuggingFace. Make sure whatever LLM you select is in the HF format.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This project was inspired by the original &lt;a href=&#34;https://github.com/imartinez/privateGPT&#34;&gt;privateGPT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Built Using üß©&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models&#34;&gt;HuggingFace LLMs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://instructor-embedding.github.io/&#34;&gt;InstructorEmbeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abetlen/llama-cpp-python&#34;&gt;LLAMACPP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.trychroma.com/&#34;&gt;ChromaDB&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://streamlit.io/&#34;&gt;Streamlit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Environment Setup üåç&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üì• Clone the repo using git:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/PromtEngineer/localGPT.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;üêç Install &lt;a href=&#34;https://www.anaconda.com/download&#34;&gt;conda&lt;/a&gt; for virtual environment management. Create and activate a new virtual environment.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n localGPT python=3.10.0&#xA;conda activate localGPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;üõ†Ô∏è Install the dependencies using pip&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To set up your environment to run the code, first install all requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Installing LLAMA-CPP :&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;LocalGPT uses &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python&#34;&gt;LlamaCpp-Python&lt;/a&gt; for GGML (you will need llama-cpp-python &amp;lt;=0.1.76) and GGUF (llama-cpp-python &amp;gt;=0.1.83) models.&lt;/p&gt; &#xA;&lt;p&gt;To run the quantized Llama3 model, ensure you have llama-cpp-python version 0.2.62 or higher installed.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use BLAS or Metal with &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal&#34;&gt;llama-cpp&lt;/a&gt; you can set appropriate flags:&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;NVIDIA&lt;/code&gt; GPUs support, use &lt;code&gt;cuBLAS&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Example: cuBLAS&#xA;CMAKE_ARGS=&#34;-DLLAMA_CUBLAS=on&#34; FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Apple Metal (&lt;code&gt;M1/M2&lt;/code&gt;) support, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Example: METAL&#xA;CMAKE_ARGS=&#34;-DLLAMA_METAL=on&#34;  FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details, please refer to &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal&#34;&gt;llama-cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Docker üê≥&lt;/h2&gt; &#xA;&lt;p&gt;Installing the required packages for GPU inference on NVIDIA GPUs, like gcc 11 and CUDA 11, may cause conflicts with other packages in your system. As an alternative to Conda, you can use Docker with the provided Dockerfile. It includes CUDA, your system just needs Docker, BuildKit, your NVIDIA GPU driver and the NVIDIA container toolkit. Build as &lt;code&gt;docker build -t localgpt .&lt;/code&gt;, requires BuildKit. Docker BuildKit does not support GPU during &lt;em&gt;docker build&lt;/em&gt; time right now, only during &lt;em&gt;docker run&lt;/em&gt;. Run as &lt;code&gt;docker run -it --mount src=&#34;$HOME/.cache&#34;,target=/root/.cache,type=bind --gpus=all localgpt&lt;/code&gt;. For running the code on Intel¬Æ Gaudi¬Æ HPU, use the following Dockerfile - &lt;code&gt;Dockerfile_hpu&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Test dataset&lt;/h2&gt; &#xA;&lt;p&gt;For testing, this repository comes with &lt;a href=&#34;https://constitutioncenter.org/media/files/constitution.pdf&#34;&gt;Constitution of USA&lt;/a&gt; as an example file to use.&lt;/p&gt; &#xA;&lt;h2&gt;Ingesting your OWN Data.&lt;/h2&gt; &#xA;&lt;p&gt;Put your files in the &lt;code&gt;SOURCE_DOCUMENTS&lt;/code&gt; folder. You can put multiple folders within the &lt;code&gt;SOURCE_DOCUMENTS&lt;/code&gt; folder and the code will recursively read your files.&lt;/p&gt; &#xA;&lt;h3&gt;Support file formats:&lt;/h3&gt; &#xA;&lt;p&gt;LocalGPT currently supports the following file formats. LocalGPT uses &lt;code&gt;LangChain&lt;/code&gt; for loading these file formats. The code in &lt;code&gt;constants.py&lt;/code&gt; uses a &lt;code&gt;DOCUMENT_MAP&lt;/code&gt; dictionary to map a file format to the corresponding loader. In order to add support for another file format, simply add this dictionary with the file format and the corresponding loader from &lt;a href=&#34;https://python.langchain.com/docs/modules/data_connection/document_loaders/&#34;&gt;LangChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;DOCUMENT_MAP = {&#xA;    &#34;.txt&#34;: TextLoader,&#xA;    &#34;.md&#34;: TextLoader,&#xA;    &#34;.py&#34;: TextLoader,&#xA;    &#34;.pdf&#34;: PDFMinerLoader,&#xA;    &#34;.csv&#34;: CSVLoader,&#xA;    &#34;.xls&#34;: UnstructuredExcelLoader,&#xA;    &#34;.xlsx&#34;: UnstructuredExcelLoader,&#xA;    &#34;.docx&#34;: Docx2txtLoader,&#xA;    &#34;.doc&#34;: Docx2txtLoader,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Ingest&lt;/h3&gt; &#xA;&lt;p&gt;Run the following command to ingest all the data.&lt;/p&gt; &#xA;&lt;p&gt;If you have &lt;code&gt;cuda&lt;/code&gt; setup on your system.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python ingest.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will see an output like this: &lt;img width=&#34;1110&#34; alt=&#34;Screenshot 2023-09-14 at 3 36 27 PM&#34; src=&#34;https://github.com/PromtEngineer/localGPT/assets/134474669/c9274e9a-842c-49b9-8d95-606c3d80011f&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use the device type argument to specify a given device. To run on &lt;code&gt;cpu&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python ingest.py --device_type cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run on &lt;code&gt;M1/M2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python ingest.py --device_type mps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use help for a full list of supported devices.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python ingest.py --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a new folder called &lt;code&gt;DB&lt;/code&gt; and use it for the newly created vector store. You can ingest as many documents as you want, and all will be accumulated in the local embeddings database. If you want to start from an empty database, delete the &lt;code&gt;DB&lt;/code&gt; and reingest your documents.&lt;/p&gt; &#xA;&lt;p&gt;Note: When you run this for the first time, it will need internet access to download the embedding model (default: &lt;code&gt;Instructor Embedding&lt;/code&gt;). In the subsequent runs, no data will leave your local environment and you can ingest data without internet connection.&lt;/p&gt; &#xA;&lt;h2&gt;Ask questions to your documents, locally!&lt;/h2&gt; &#xA;&lt;p&gt;In order to chat with your documents, run the following command (by default, it will run on &lt;code&gt;cuda&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_localGPT.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify the device type just like &lt;code&gt;ingest.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_localGPT.py --device_type mps # to run on Apple silicon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# To run on Intel¬Æ Gaudi¬Æ hpu&#xA;MODEL_ID = &#34;mistralai/Mistral-7B-Instruct-v0.2&#34; # in constants.py&#xA;python run_localGPT.py --device_type hpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will load the ingested vector store and embedding model. You will be presented with a prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;gt; Enter a query:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After typing your question, hit enter. LocalGPT will take some time based on your hardware. You will get a response like this below. &lt;img width=&#34;1312&#34; alt=&#34;Screenshot 2023-09-14 at 3 33 19 PM&#34; src=&#34;https://github.com/PromtEngineer/localGPT/assets/134474669/a7268de9-ade0-420b-a00b-ed12207dbe41&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once the answer is generated, you can then ask another question without re-running the script, just wait for the prompt again.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/em&gt; When you run this for the first time, it will need internet connection to download the LLM (default: &lt;code&gt;TheBloke/Llama-2-7b-Chat-GGUF&lt;/code&gt;). After that you can turn off your internet connection, and the script inference would still work. No data gets out of your local environment.&lt;/p&gt; &#xA;&lt;p&gt;Type &lt;code&gt;exit&lt;/code&gt; to finish the script.&lt;/p&gt; &#xA;&lt;h3&gt;Extra Options with run_localGPT.py&lt;/h3&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;--show_sources&lt;/code&gt; flag with &lt;code&gt;run_localGPT.py&lt;/code&gt; to show which chunks were retrieved by the embedding model. By default, it will show 4 different sources/chunks. You can change the number of sources/chunks&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_localGPT.py --show_sources&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another option is to enable chat history. &lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/em&gt;: This is disabled by default and can be enabled by using the &lt;code&gt;--use_history&lt;/code&gt; flag. The context window is limited so keep in mind enabling history will use it and might overflow.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_localGPT.py --use_history&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can store user questions and model responses with flag &lt;code&gt;--save_qa&lt;/code&gt; into a csv file &lt;code&gt;/local_chat_history/qa_log.csv&lt;/code&gt;. Every interaction will be stored.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_localGPT.py --save_qa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Run the Graphical User Interface&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open &lt;code&gt;constants.py&lt;/code&gt; in an editor of your choice and depending on choice add the LLM you want to use. By default, the following model will be used:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;MODEL_ID = &#34;TheBloke/Llama-2-7b-Chat-GGUF&#34;&#xA;MODEL_BASENAME = &#34;llama-2-7b-chat.Q4_K_M.gguf&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open up a terminal and activate your python environment that contains the dependencies installed from requirements.txt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the &lt;code&gt;/LOCALGPT&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following command &lt;code&gt;python run_localGPT_API.py&lt;/code&gt;. The API should being to run.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Wait until everything has loaded in. You should see something like &lt;code&gt;INFO:werkzeug:Press CTRL+C to quit&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open up a second terminal and activate the same python environment.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the &lt;code&gt;/LOCALGPT/localGPTUI&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the command &lt;code&gt;python localGPTUI.py&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open up a web browser and go the address &lt;code&gt;http://localhost:5111/&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;How to select different LLM models?&lt;/h1&gt; &#xA;&lt;p&gt;To change the models you will need to set both &lt;code&gt;MODEL_ID&lt;/code&gt; and &lt;code&gt;MODEL_BASENAME&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open up &lt;code&gt;constants.py&lt;/code&gt; in the editor of your choice.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Change the &lt;code&gt;MODEL_ID&lt;/code&gt; and &lt;code&gt;MODEL_BASENAME&lt;/code&gt;. If you are using a quantized model (&lt;code&gt;GGML&lt;/code&gt;, &lt;code&gt;GPTQ&lt;/code&gt;, &lt;code&gt;GGUF&lt;/code&gt;), you will need to provide &lt;code&gt;MODEL_BASENAME&lt;/code&gt;. For unquantized models, set &lt;code&gt;MODEL_BASENAME&lt;/code&gt; to &lt;code&gt;NONE&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;There are a number of example models from HuggingFace that have already been tested to be run with the original trained model (ending with HF or have a .bin in its &#34;Files and versions&#34;), and quantized models (ending with GPTQ or have a .no-act-order or .safetensors in its &#34;Files and versions&#34;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For models that end with HF or have a .bin inside its &#34;Files and versions&#34; on its HuggingFace page.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure you have a &lt;code&gt;MODEL_ID&lt;/code&gt; selected. For example -&amp;gt; &lt;code&gt;MODEL_ID = &#34;TheBloke/guanaco-7B-HF&#34;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Go to the &lt;a href=&#34;https://huggingface.co/TheBloke/guanaco-7B-HF&#34;&gt;HuggingFace Repo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For models that contain GPTQ in its name and or have a .no-act-order or .safetensors extension inside its &#34;Files and versions on its HuggingFace page.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure you have a &lt;code&gt;MODEL_ID&lt;/code&gt; selected. For example -&amp;gt; model_id = &lt;code&gt;&#34;TheBloke/wizardLM-7B-GPTQ&#34;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Got to the corresponding &lt;a href=&#34;https://huggingface.co/TheBloke/wizardLM-7B-GPTQ&#34;&gt;HuggingFace Repo&lt;/a&gt; and select &#34;Files and versions&#34;.&lt;/li&gt; &#xA;   &lt;li&gt;Pick one of the model names and set it as &lt;code&gt;MODEL_BASENAME&lt;/code&gt;. For example -&amp;gt; &lt;code&gt;MODEL_BASENAME = &#34;wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow the same steps for &lt;code&gt;GGUF&lt;/code&gt; and &lt;code&gt;GGML&lt;/code&gt; models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;GPU and VRAM Requirements&lt;/h1&gt; &#xA;&lt;p&gt;Below is the VRAM requirement for different models depending on their size (Billions of parameters). The estimates in the table does not include VRAM used by the Embedding models - which use an additional 2GB-7GB of VRAM depending on the model.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Mode Size (B)&lt;/th&gt; &#xA;   &lt;th&gt;float32&lt;/th&gt; &#xA;   &lt;th&gt;float16&lt;/th&gt; &#xA;   &lt;th&gt;GPTQ 8bit&lt;/th&gt; &#xA;   &lt;th&gt;GPTQ 4bit&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;28 GB&lt;/td&gt; &#xA;   &lt;td&gt;14 GB&lt;/td&gt; &#xA;   &lt;td&gt;7 GB - 9 GB&lt;/td&gt; &#xA;   &lt;td&gt;3.5 GB - 5 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;52 GB&lt;/td&gt; &#xA;   &lt;td&gt;26 GB&lt;/td&gt; &#xA;   &lt;td&gt;13 GB - 15 GB&lt;/td&gt; &#xA;   &lt;td&gt;6.5 GB - 8 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;32B&lt;/td&gt; &#xA;   &lt;td&gt;130 GB&lt;/td&gt; &#xA;   &lt;td&gt;65 GB&lt;/td&gt; &#xA;   &lt;td&gt;32.5 GB - 35 GB&lt;/td&gt; &#xA;   &lt;td&gt;16.25 GB - 19 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;260.8 GB&lt;/td&gt; &#xA;   &lt;td&gt;130.4 GB&lt;/td&gt; &#xA;   &lt;td&gt;65.2 GB - 67 GB&lt;/td&gt; &#xA;   &lt;td&gt;32.6 GB - 35 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;System Requirements&lt;/h1&gt; &#xA;&lt;h2&gt;Python Version&lt;/h2&gt; &#xA;&lt;p&gt;To use this software, you must have Python 3.10 or later installed. Earlier versions of Python will not compile.&lt;/p&gt; &#xA;&lt;h2&gt;C++ Compiler&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter an error while building a wheel during the &lt;code&gt;pip install&lt;/code&gt; process, you may need to install a C++ compiler on your computer.&lt;/p&gt; &#xA;&lt;h3&gt;For Windows 10/11&lt;/h3&gt; &#xA;&lt;p&gt;To install a C++ compiler on Windows 10/11, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Visual Studio 2022.&lt;/li&gt; &#xA; &lt;li&gt;Make sure the following components are selected: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Universal Windows Platform development&lt;/li&gt; &#xA;   &lt;li&gt;C++ CMake tools for Windows&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Download the MinGW installer from the &lt;a href=&#34;https://sourceforge.net/projects/mingw/&#34;&gt;MinGW website&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run the installer and select the &#34;gcc&#34; component.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;NVIDIA Driver&#39;s Issues:&lt;/h3&gt; &#xA;&lt;p&gt;Follow this &lt;a href=&#34;https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04&#34;&gt;page&lt;/a&gt; to install NVIDIA Drivers.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#PromtEngineer/localGPT&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=PromtEngineer/localGPT&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This is a test project to validate the feasibility of a fully local solution for question answering using LLMs and Vector embeddings. It is not production ready, and it is not meant to be used in production. Vicuna-7B is based on the Llama model so that has the original Llama license.&lt;/p&gt; &#xA;&lt;h1&gt;Common Errors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch/issues/30664&#34;&gt;Torch not compatible with CUDA enabled&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Get CUDA version &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;nvcc --version&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Try installing PyTorch depending on your CUDA version &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   conda install -c pytorch torchvision cudatoolkit=10.1 pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;If it doesn&#39;t work, try reinstalling &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   pip uninstall torch&#xA;   pip cache purge&#xA;   pip install torch -f https://download.pytorch.org/whl/torch_stable.html&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/72672196/error-pips-dependency-resolver-does-not-currently-take-into-account-all-the-pa/76604141#76604141&#34;&gt;ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   pip install h5py&#xA;   pip install typing-extensions&#xA;   pip install wheel&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/issues/11262&#34;&gt;Failed to import transformers&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Try re-install &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   conda uninstall tokenizers, transformers&#xA;   pip install transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>