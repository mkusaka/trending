<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-24T01:57:46Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chatanywhere/GPT_API_free</title>
    <updated>2023-12-24T01:57:46Z</updated>
    <id>tag:github.com,2023-12-24:/chatanywhere/GPT_API_free</id>
    <link href="https://github.com/chatanywhere/GPT_API_free" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free ChatGPT API Key，免费ChatGPT API，支持GPT4 API（免费），ChatGPT国内可用免费转发API，直连无需代理。可以搭配ChatBox等软件/插件使用，极大降低接口使用成本。国内即可无限制畅快聊天。&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/logo.png&#34; alt=&#34;icon&#34; width=&#34;50px&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;GPT-API-free&lt;/h1&gt; &#xA; &lt;p&gt;支持 &lt;strong&gt;GPT-4&lt;/strong&gt; / GPT-3.5-Turbo / GPT-3.5-Turbo-16K / embeddings / DALL·E / whisper / text-davinci&lt;/p&gt; &#xA; &lt;p&gt;国内动态加速 直连无需代理&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8&#34;&gt;快速开始&lt;/a&gt; / &lt;a href=&#34;https://chatanywhere.apifox.cn/&#34;&gt;API文档&lt;/a&gt; / &lt;a href=&#34;https://api.chatanywhere.org/v1/oauth/free/github/render&#34;&gt;申请内测免费Key&lt;/a&gt; / &lt;a href=&#34;https://peiqi.shop/&#34;&gt;支持付费Key&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://qm.qq.com/cgi-bin/qm/qr?k=OFhxu4Z3qI-c-76QJfC2LLXfKGr0g-57&amp;amp;jump_from=webapi&amp;amp;authKey=Kzuf7g4fsE0ZAM7RN+7XvivEANxgDVqDbUs3WI6cB98pt4pFzq/3L8NMiMOy+mo1&#34;&gt;QQ群: 780366686&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;隐私声明&lt;/h2&gt; &#xA;&lt;p&gt;该项目高度重视隐私，致力于保护其用户的隐私。该项目不会以任何方式收集、记录或存储用户输入的任何文本或由 OpenAI 服务器返回的任何文本。该项目不会向 OpenAI 或任何第三方提供有关 API 调用者的身份的任何信息，包括但不限于 IP 地址和用户代理字符串。&lt;/p&gt; &#xA;&lt;p&gt;但OpenAI官方会根据其&lt;a href=&#34;https://platform.openai.com/docs/data-usage-policies&#34;&gt;数据使用政策&lt;/a&gt;保留 30 天的数据。&lt;/p&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年6月14日&lt;/strong&gt; 适配GPT-3.5-Turbo-16K，免费key也支持16k模型；付费key跟随官方价格降低收费。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年6月15日&lt;/strong&gt; 适配0613版本新增的functions。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年6月18日&lt;/strong&gt; 新增对语音转文字模型Whisper支持。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年8月4日&lt;/strong&gt; 免费Key不再支持gpt-3.5-turbo-16k模型调用。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年9月7日&lt;/strong&gt; chatapi.chatanywhere.cn镜像站不再向国内用户提供服务，不影响API的正常使用。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年11月8日&lt;/strong&gt; 支持1106版本各模型，支持TTS文本转语音模型。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年11月19日&lt;/strong&gt; 支持gpt-4-1106-preview模型，价格仅原先gpt-4模型的三分之一到二分之一。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023年11月29日&lt;/strong&gt; 开放免费API的gpt-4权限，每天可以免费使用10次。（不保证能长期提供）&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;特点&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;支持Models, Embedding, text-davinci, GPT-3.5-Turbo, GPT-3.5-Turbo-16K(免费版不支持), &lt;em&gt;&lt;strong&gt;GPT-4&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;DALLE&lt;/strong&gt;&lt;/em&gt;(免费版不支持), &lt;em&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/em&gt;(免费版不支持)。（免费版就可以支持AutoGPT, gpt_academic, langchain等）&lt;/li&gt; &#xA; &lt;li&gt;免费版支持GPT-4，一天10次。（免费版gpt-4相对慢一些，付费版更稳定）&lt;/li&gt; &#xA; &lt;li&gt;与官方完全一致的接口标准，兼容各种软件/插件。&lt;/li&gt; &#xA; &lt;li&gt;支持流式响应。&lt;/li&gt; &#xA; &lt;li&gt;国内线路使用动态加速，体验远优于使用代理连接官方。&lt;/li&gt; &#xA; &lt;li&gt;无需科学上网，国内环境直接可用。&lt;/li&gt; &#xA; &lt;li&gt;个人完全免费使用。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🚩注意事项&lt;/h2&gt; &#xA;&lt;p&gt;❗️&lt;em&gt;近期OpenAI频繁出错，如果遇到无回复，报错等情况，可以查看 status.openai.com ，很大可能是OpenAI官方服务问题。&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;❗️&lt;strong&gt;免费API Key仅可用于个人非商业用途，教育，非营利性科研工作中。严禁商用，严禁大规模训练商用模型！训练科研用模型请提前加群联系我们。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;❗️我们将不定期对被滥用的Key进行封禁，如发现自己的key被误封请通过QQ群联系我们。&lt;/p&gt; &#xA;&lt;p&gt;❗️我们的系统仅供内部评估测试使用，商用或面向大众使用请自行承担风险。&lt;/p&gt; &#xA;&lt;p&gt;为了该项目长久发展，免费API Key限制&lt;strong&gt;60请求/小时/IP&amp;amp;Key&lt;/strong&gt;调用频率，也就是说你如果在一个IP下使用多个Key，所有Key的每小时请求数总和不能超过60；同理，你如果将一个Key用于多个IP，这个Key的每小时请求数也不能超过60。(&lt;strong&gt;付费版API没有这个限制&lt;/strong&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;免费使用&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;🚀&lt;a href=&#34;https://api.chatanywhere.org/v1/oauth/free/github/render&#34;&gt;申请领取内测免费API Key&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;免费版支持gpt-3.5-turbo, embedding, gpt-4。其中gpt-4由于价格过高，每24小时限制10次调用，且不支持流式传输。需要更稳定快速的gpt-4请使用付费版。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;转发Host1: &lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt; (国内中转，延时更低，host1和host2二选一)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;转发Host2: &lt;code&gt;https://api.chatanywhere.com.cn&lt;/code&gt; (国内中转，延时更低，host1和host2二选一)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;转发Host3: &lt;code&gt;https://api.chatanywhere.cn&lt;/code&gt; (国外使用,国内需要全局代理)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;我们会定期根据使用量进行相应的扩容，只要不被官方制裁我们会一直提供免费API，如果该项目对你有帮助，还请为我们点一个&lt;em&gt;&lt;strong&gt;Star&lt;/strong&gt;&lt;/em&gt;。如果遇到问题可以在&lt;a href=&#34;https://github.com/chatanywhere/GPT_API_free/issues&#34;&gt;Issues&lt;/a&gt;中反馈，有空会解答。&lt;/p&gt; &#xA;&lt;p&gt;该API Key用于转发API，需要将Host改为&lt;code&gt;api.chatanywhere.tech&lt;/code&gt;(国内首选)或者&lt;code&gt;api.chatanywhere.cn&lt;/code&gt;(国外使用，国内需要全局代理)。&lt;/p&gt; &#xA;&lt;h2&gt;付费版API&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;纯公益提供免费Key显然不是能持久运营下去的方案，所以我们引入付费API Key维持项目的日常开销，以促进项目的良性循环，还望大家理解。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://peiqi.shop/&#34;&gt;购买低价付费Key&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;支持&lt;strong&gt;更稳定更快速的GPT4 API&lt;/strong&gt;，GPT4体验更好，无限使用，价格仅官方价格85折。&lt;/li&gt; &#xA; &lt;li&gt;性价比高，除了GPT4的其他模型价格相当于官网价格七分之一。&lt;/li&gt; &#xA; &lt;li&gt;同官网计费策略，流式问答使用tiktoken库准确计算Tokens，非流式问答直接使用官方返回Tokens用量计费。&lt;/li&gt; &#xA; &lt;li&gt;余额不会过期，永久有效。根据用户反馈30块钱个人中度使用GPT3.5估计能用一年。&lt;/li&gt; &#xA; &lt;li&gt;所有的接口都保证转发自OpenAI官方接口，非peo、plus等不稳定方案，无水分，不掺假，保证稳定性。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;如何使用&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;由于频繁的恶意请求，我们不再直接提供公共的免费Key，现在需要你使用你的Github账号绑定来领取你自己的免费Key。&lt;/li&gt; &#xA; &lt;li&gt;🚀&lt;a href=&#34;https://api.chatanywhere.org/v1/oauth/free/github/render&#34;&gt;申请领取内测免费API Key&lt;/a&gt; 或 &lt;a href=&#34;https://peiqi.shop/&#34;&gt;购买内测付费API Key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;转发Host1: &lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt; (国内中转，延时更低，host1和host2二选一)&lt;/li&gt; &#xA; &lt;li&gt;转发Host2: &lt;code&gt;https://api.chatanywhere.com.cn&lt;/code&gt; (国内中转，延时更低，host1和host2二选一)&lt;/li&gt; &#xA; &lt;li&gt;转发Host3: &lt;code&gt;https://api.chatanywhere.cn&lt;/code&gt; (国外使用,国内需要全局代理)&lt;/li&gt; &#xA; &lt;li&gt;余额和使用记录查询（通知公告也会发在这里）: &lt;a href=&#34;https://api.chatanywhere.tech/&#34;&gt;余额查询及公告&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;转发API无法直接向官方接口api.openai.com发起请求，需要将请求地址改为api.chatanywhere.tech才可以使用，大部分插件和软件都可以修改。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;常见软件/插件使用方法&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;python openai官方库（使用AutoGPT，langchain等）&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;示例代码请参考&lt;a href=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/demo.py&#34;&gt;demo.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;方法一&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_base = &#34;https://api.chatanywhere.tech/v1&#34;&#xA;# openai.api_base = &#34;https://api.chatanywhere.cn/v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;方法二（方法一不起作用用这个）&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;修改环境变量OPENAI_API_BASE，各个系统怎么改环境变量请自行搜索，修改环境变量后不起作用请重启系统。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_BASE=https://api.chatanywhere.tech/v1&#xA;或 OPENAI_API_BASE=https://api.chatanywhere.cn/v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;开源gpt_academic&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;找到&lt;code&gt;config.py&lt;/code&gt;文件中的&lt;code&gt;API_URL_REDIRECT&lt;/code&gt;配置并修改为以下内容：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;API_URL_REDIRECT = {&#34;https://api.openai.com/v1/chat/completions&#34;: &#34;https://api.chatanywhere.tech/v1/chat/completions&#34;}&#xA;# API_URL_REDIRECT = {&#34;https://api.openai.com/v1/chat/completions&#34;: &#34;https://api.chatanywhere.cn/v1/chat/completions&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;BotGem(AMA)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;ChatGPT桌面应用，支持全平台，&lt;em&gt;&lt;strong&gt;支持gpt-4-vision&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt; &#xA;&lt;p&gt;下载链接：&lt;a href=&#34;https://bytemyth.com/ama&#34;&gt;https://bytemyth.com/ama&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;使用方法：下载安装后在设置中如图设置，并点击更新。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/botgem.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;ChatBox&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;ChatGPT开源桌面应用，支持全部桌面平台。&lt;/p&gt; &#xA;&lt;p&gt;下载链接：&lt;a href=&#34;https://github.com/Bin-Huang/chatbox/releases&#34;&gt;https://github.com/Bin-Huang/chatbox/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;使用方法：如图在设置中填入购买的密钥，并将代理设置为&lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt;即可&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/chatbox.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Zotero插件&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;pdf阅读插件zotero-gpt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;下载链接：&lt;a href=&#34;https://github.com/MuiseDestiny/zotero-gpt/releases&#34;&gt;https://github.com/MuiseDestiny/zotero-gpt/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;安装好插件后使用以下命令设置，还是不会可以去b站搜教程。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/api https://api.chatanywhere.tech&#xA;&#xA;/secretKey 购买的转发key 记住别忘记带sk-&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/zotero-gpt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;翻译插件zotero-pdf-translate&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;下载链接：&lt;a href=&#34;https://github.com/windingwind/zotero-pdf-translate/releases&#34;&gt;https://github.com/windingwind/zotero-pdf-translate/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;接口地址填写: &lt;a href=&#34;https://api.chatanywhere.tech/v1/chat/completions&#34;&gt;https://api.chatanywhere.tech/v1/chat/completions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;不用管状态是否显示可用 填上之后就可以了&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/zotero-pdf-translate.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;浏览器插件ChatGPT Sidebar&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;官网链接：&lt;a href=&#34;https://chatgpt-sidebar.com/&#34;&gt;https://chatgpt-sidebar.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;安装好插件后进入设置页面，如图所示修改设置，将url修改为 &lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt; 。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/sidebar.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Jetbrains插件ChatGPT - Easycode&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/jet1.png&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;安装好插件后在Settings &amp;gt; Tools &amp;gt; OpenAI &amp;gt; GPT 3.5 Turbo中如图所示配置好插件，重点要将Server Settings 修改为 &lt;code&gt;https://api.chatanywhere.tech/v1/chat/completions&lt;/code&gt; 。并勾选Customize Server。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/jet2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Raycast 插件 ChatGPT&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;在 Raycast Store 中找到 ChatGPT 插件，并按照提示安装： &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;安装完成后在该插件配置中的 &lt;code&gt;API Key&lt;/code&gt; 中填入我们的API Key，以及选中 &lt;code&gt;Change API Endpoint&lt;/code&gt;，并在 &lt;code&gt;API Endpoint&lt;/code&gt; 中填入 &lt;code&gt;https://api.chatanywhere.tech/v1&lt;/code&gt; &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast2.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🍺 enjoy it~ &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast4.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;API报错说明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Overload错误&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;具体错误信息：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &#34;error&#34;: {&#xA;    &#34;message&#34;: &#34;That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID xxxxxxxxxxxx in your message.)&#34;,&#xA;    &#34;type&#34;: &#34;server_error&#34;,&#xA;    &#34;param&#34;: null,&#xA;    &#34;code&#34;: null&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;该错误由于OpenAI官方服务器负载高引起，与转发服务器负载无关。一般一段时间后恢复，可以等几秒后再试。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#chatanywhere/GPT_API_free&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=chatanywhere/GPT_API_free&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>d2l-ai/d2l-zh</title>
    <updated>2023-12-24T01:57:46Z</updated>
    <id>tag:github.com,2023-12-24:/d2l-ai/d2l-zh</id>
    <link href="https://github.com/d2l-ai/d2l-zh" rel="alternate"></link>
    <summary type="html">&lt;p&gt;《动手学深度学习》：面向中文读者、能运行、可讨论。中英文版被70多个国家的500多所大学用于教学。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;动手学深度学习（Dive into Deep Learning，D2L.ai）&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zh.d2l.ai&#34;&gt;第二版：zh.D2L.ai&lt;/a&gt; | &lt;a href=&#34;https://zh-v1.d2l.ai/&#34;&gt;第一版：zh-v1.D2L.ai&lt;/a&gt; | 安装和使用书中源代码： &lt;a href=&#34;https://zh.d2l.ai/chapter_installation/index.html&#34;&gt;第二版&lt;/a&gt; &lt;a href=&#34;https://zh-v1.d2l.ai/chapter_prerequisite/install.html&#34;&gt;第一版&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt;&lt;i&gt;理解深度学习的最佳方法是学以致用。&lt;/i&gt;&lt;/h5&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/eq.jpg&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/figure.jpg&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/code.jpg&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/notebook.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;本开源项目代表了我们的一种尝试：我们将教给读者概念、背景知识和代码；我们将在同一个地方阐述剖析问题所需的批判性思维、解决问题所需的数学知识，以及实现解决方案所需的工程技能。&lt;/p&gt; &#xA;&lt;p&gt;我们的目标是创建一个为实现以下目标的统一资源：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;所有人均可在网上免费获取；&lt;/li&gt; &#xA; &lt;li&gt;提供足够的技术深度，从而帮助读者实际成为深度学习应用科学家：既理解数学原理，又能够实现并不断改进方法；&lt;/li&gt; &#xA; &lt;li&gt;包含可运行的代码，为读者展示如何在实际中解决问题。这样不仅直接将数学公式对应成实际代码，而且可以修改代码、观察结果并及时获取经验；&lt;/li&gt; &#xA; &lt;li&gt;允许我们和整个社区不断快速迭代内容，从而紧跟仍在高速发展的深度学习领域；&lt;/li&gt; &#xA; &lt;li&gt;由包含有关技术细节问答的论坛作为补充，使大家可以相互答疑并交换经验。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt;将本书（中英文版）用作教材或参考书的大学&lt;/h5&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;400&#34; src=&#34;https://d2l.ai/_images/map.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;如果本书对你有帮助，请Star (★) 本仓库或引用本书的英文版：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{zhang2023dive,&#xA;    title={Dive into Deep Learning},&#xA;    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},&#xA;    publisher={Cambridge University Press},&#xA;    note={\url{https://D2L.ai}},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;本书的英文版&lt;/h2&gt; &#xA;&lt;p&gt;虽然纸质书已出版，但深度学习领域依然在迅速发展。为了得到来自更广泛的英文开源社区的帮助，从而提升本书质量，本书的新版将继续用英文编写，并搬回中文版。&lt;/p&gt; &#xA;&lt;p&gt;欢迎关注本书的&lt;a href=&#34;https://github.com/d2l-ai/d2l-en&#34;&gt;英文开源项目&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;中英文教学资源&lt;/h2&gt; &#xA;&lt;p&gt;加州大学伯克利分校 2019 年春学期 &lt;a href=&#34;http://courses.d2l.ai/berkeley-stat-157/index.html&#34;&gt;&lt;em&gt;Introduction to Deep Learning&lt;/em&gt; 课程&lt;/a&gt;教材（同时提供含教学视频地址的&lt;a href=&#34;https://github.com/d2l-ai/berkeley-stat-157/tree/master/slides-zh&#34;&gt;中文版课件&lt;/a&gt;）。&lt;/p&gt; &#xA;&lt;h2&gt;学术界推荐&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;Dive into this book if you want to dive into deep learning!&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 韩家炜，ACM 院士、IEEE 院士，美国伊利诺伊大学香槟分校计算机系 Michael Aiken Chair 教授&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;This is a highly welcome addition to the machine learning literature.&#34;&lt;/p&gt; &#xA; &lt;b&gt;— Bernhard Schölkopf，ACM 院士、德国国家科学院院士，德国马克斯•普朗克研究所智能系统院院长&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;书中代码可谓‘所学即所用’。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 周志华，ACM 院士、IEEE 院士、AAAS 院士，南京大学计算机科学与技术系主任&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;这本书可以帮助深度学习实践者快速提升自己的能力。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 张潼，ASA 院士、IMS 院士，香港科技大学计算机系和数学系教授&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;工业界推荐&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;一本优秀的深度学习教材，值得任何想了解深度学习何以引爆人工智能革命的人关注。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 黄仁勋，NVIDIA创始人 &amp;amp; CEO&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;《动手学深度学习》是最适合工业界研发工程师学习的。我毫无保留地向广大的读者们强烈推荐。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 余凯，地平线公司创始人 &amp;amp; CEO&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;强烈推荐这本书！我特别赞赏这种手脑一体的学习方式。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 漆远，复旦大学“浩清”教授、人工智能创新与产业研究院院长&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;《动手学深度学习》是一本很容易让学习者上瘾的书。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 沈强，将门创投创始合伙人&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;贡献&lt;/h2&gt; &#xA;&lt;p&gt;感谢&lt;a href=&#34;https://github.com/d2l-ai/d2l-zh/graphs/contributors&#34;&gt;社区贡献者们&lt;/a&gt;为每一位读者改进这本开源书。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html&#34;&gt;如何贡献&lt;/a&gt; | &lt;a href=&#34;https://zh.d2l.ai/chapter_preface/index.html&#34;&gt;致谢&lt;/a&gt; | &lt;a href=&#34;https://discuss.d2l.ai/c/chinese-version/16&#34;&gt;讨论或报告问题&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/INFO.md&#34;&gt;其他&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/promptbase</title>
    <updated>2023-12-24T01:57:46Z</updated>
    <id>tag:github.com,2023-12-24:/microsoft/promptbase</id>
    <link href="https://github.com/microsoft/promptbase" rel="alternate"></link>
    <summary type="html">&lt;p&gt;All things prompt engineering&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;promptbase&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;promptbase&lt;/code&gt; is an evolving collection of resources, best practices, and example scripts for eliciting the best performance from foundation models like &lt;code&gt;GPT-4&lt;/code&gt;. We currently host scripts demonstrating the &lt;a href=&#34;https://arxiv.org/abs/2311.16452&#34;&gt;&lt;code&gt;Medprompt&lt;/code&gt; methodology&lt;/a&gt;, including examples of how we further extended this collection of prompting techniques (&#34;&lt;code&gt;Medprompt+&lt;/code&gt;&#34;) into non-medical domains:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Benchmark&lt;/th&gt; &#xA;   &lt;th&gt;GPT-4 Prompt&lt;/th&gt; &#xA;   &lt;th&gt;GPT-4 Results&lt;/th&gt; &#xA;   &lt;th&gt;Gemini Ultra Results&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU&lt;/td&gt; &#xA;   &lt;td&gt;Medprompt+&lt;/td&gt; &#xA;   &lt;td&gt;90.10%&lt;/td&gt; &#xA;   &lt;td&gt;90.04%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GSM8K&lt;/td&gt; &#xA;   &lt;td&gt;Zero-shot&lt;/td&gt; &#xA;   &lt;td&gt;95.3%&lt;/td&gt; &#xA;   &lt;td&gt;94.4%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MATH&lt;/td&gt; &#xA;   &lt;td&gt;Zero-shot&lt;/td&gt; &#xA;   &lt;td&gt;68.4%&lt;/td&gt; &#xA;   &lt;td&gt;53.2%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HumanEval&lt;/td&gt; &#xA;   &lt;td&gt;Zero-shot&lt;/td&gt; &#xA;   &lt;td&gt;87.8%&lt;/td&gt; &#xA;   &lt;td&gt;74.4%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BIG-Bench-Hard&lt;/td&gt; &#xA;   &lt;td&gt;Few-shot + CoT&lt;/td&gt; &#xA;   &lt;td&gt;89.0%&lt;/td&gt; &#xA;   &lt;td&gt;83.6%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DROP&lt;/td&gt; &#xA;   &lt;td&gt;Zero-shot + CoT&lt;/td&gt; &#xA;   &lt;td&gt;83.7%&lt;/td&gt; &#xA;   &lt;td&gt;82.4%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HellaSwag&lt;/td&gt; &#xA;   &lt;td&gt;10-shot&lt;/td&gt; &#xA;   &lt;td&gt;95.3%&lt;/td&gt; &#xA;   &lt;td&gt;87.8%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;In the near future, &lt;code&gt;promptbase&lt;/code&gt; will also offer further case studies and structured interviews around the scientific process we take behind prompt engineering. We&#39;ll also offer specialized deep dives into specialized tooling that accentuates the prompt engineering process. Stay tuned!&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;Medprompt&lt;/code&gt; and The Power of Prompting&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;em&gt;&#34;Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine&#34; (H. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King, J. Larson, Y. Li, W. Liu, R. Luo, S. M. McKinney, R. O. Ness, H. Poon, T. Qin, N. Usuyama, C. White, E. Horvitz 2023)&lt;/em&gt; &lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&#xA;&lt;p&gt;@article{nori2023can,&#xA;title={Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine},&#xA;author={Nori, Harsha and Lee, Yin Tat and Zhang, Sheng and Carignan, Dean and Edgar, Richard and Fusi, Nicolo and King, Nicholas and Larson, Jonathan and Li, Yuanzhi and Liu, Weishung and others},&#xA;journal={arXiv preprint arXiv:2311.16452},&#xA;year={2023}&#xA;}&#xA;&lt;/p&gt;&lt;/pre&gt; &#xA; &lt;a href=&#34;https://arxiv.org/pdf/1909.09223.pdf&#34;&gt;Paper link&lt;/a&gt;&#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/promptbase/main/images/medprompt_radar.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In a recent &lt;a href=&#34;https://arxiv.org/abs/2311.16452&#34;&gt;study&lt;/a&gt;, we showed how the composition of several prompting strategies into a method that we refer to as &lt;code&gt;Medprompt&lt;/code&gt; can efficiently steer generalist models like GPT-4 to achieve top performance, even when compared to models specifically finetuned for medicine. &lt;code&gt;Medprompt&lt;/code&gt; composes three distinct strategies together -- including dynamic few-shot selection, self-generated chain of thought, and choice-shuffle ensembling -- to elicit specialist level performance from GPT-4. We briefly describe these strategies here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/promptbase/main/images/medprompt_sa_graphic.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dynamic Few Shots&lt;/strong&gt;: Few-shot learning -- providing several examples of the task and response to a foundation model -- enables models quickly adapt to a specific domain and learn to follow the task format. For simplicity and efficiency, the few-shot examples applied in prompting for a particular task are typically fixed; they are unchanged across test examples. This necessitates that the few-shot examples selected are broadly representative and relevant to a wide distribution of text examples. One approach to meeting these requirements is to have domain experts carefully hand-craft exemplars. Even so, this approach cannot guarantee that the curated, fixed few-shot examples will be appropriately representative of every test example. However, with enough available data, we can select &lt;em&gt;different&lt;/em&gt; few-shot examples for different task inputs. We refer to this approach as employing dynamic few-shot examples. The method makes use of a mechanism to identify examples based on their similarity to the case at hand. For Medprompt, we did the following to identify representative few shot examples: Given a test example, we choose k training examples that are semantically similar using a k-NN clustering in the embedding space. Specifically, we first use OpenAI&#39;s &lt;code&gt;text-embedding-ada-002&lt;/code&gt; model to embed candidate exemplars for few-shot learning. Then, for each test question x, we retrieve its nearest k neighbors x1, x2, ..., xk from the training set (according to distance in the embedding space of text-embedding-ada-002). These examples -- the ones most similar in embedding space to the test question -- are ultimately registered in the prompt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Self-Generated Chain of Thought (CoT)&lt;/strong&gt;: Chain-of-thought (CoT) uses natural language statements, such as “Let’s think step by step,” to explicitly encourage the model to generate a series of intermediate reasoning steps. The approach has been found to significantly improve the ability of foundation models to perform complex reasoning. Most approaches to chain-of-thought center on the use of experts to manually compose few-shot examples with chains of thought for prompting. Rather than rely on human experts, we pursued a mechanism to automate the creation of chain-of-thought examples. We found that we could simply ask GPT-4 to generate chain-of-thought for the training examples, with appropriate guardrails for reducing risk of hallucination via incorrect reasoning chains.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Majority Vote Ensembling&lt;/strong&gt;: &lt;a href=&#34;https://en.wikipedia.org/wiki/Ensemble_learning&#34;&gt;Ensembling&lt;/a&gt; refers to combining the output of several algorithms together to yield better predictive performance than any individual algorithm. Frontier models like &lt;code&gt;GPT-4&lt;/code&gt; benefit from ensembling of their own outputs. A simple technique is to have a variety of prompts, or a single prompt with varied &lt;code&gt;temperature&lt;/code&gt;, and report the most frequent answer amongst the ensemble constituents. For multiple choice questions, we employ a further trick that increases the diversity of the ensemble called &lt;code&gt;choice-shuffling&lt;/code&gt;, where we shuffle the relative order of the answer choices before generating each reasoning path. We then select the most consistent answer, i.e., the one that is least sensitive to choice shuffling, which increases the robustness of the answer.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The combination of these three techniques led to breakthrough performance in Medprompt for medical challenge questions. Implementation details of these techniques can be found here: &lt;a href=&#34;https://github.com/microsoft/promptbase/tree/main/src/promptbase/mmlu&#34;&gt;https://github.com/microsoft/promptbase/tree/main/src/promptbase/mmlu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;Medprompt+&lt;/code&gt; | Extending the power of prompting&lt;/h2&gt; &#xA;&lt;p&gt;Here we provide some intuitive details on how we extended the &lt;code&gt;medprompt&lt;/code&gt; prompting framework to elicit even stronger out-of-domain performance on the MMLU (Measuring Massive Multitask Language Understanding) benchmark. MMLU was established as a test of general knowledge and reasoning powers of large language models. The complete MMLU benchmark contains tens of thousands of challenge problems of different forms across 57 areas from basic mathematics to United States history, law, computer science, engineering, medicine, and more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/promptbase/main/images/mmlu_accuracy_ablation.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We found that applying Medprompt without modification to the whole MMLU achieved a score of 89.1%. Not bad for a single policy working across a great diversity of problems! But could we push Medprompt to do better? Simply scaling-up MedPrompt can yield further benefits. As a first step, we increased the number of ensembled calls from five to 20. This boosted performance to 89.56%.&lt;/p&gt; &#xA;&lt;p&gt;On working to push further with refinement of Medprompt, we noticed that performance was relatively poor for specific topics of the MMLU. MMLU contains a great diversity of types of questions, depending on the discipline and specific benchmark at hand. How might we push GPT-4 to perform even better on MMLU given the diversity of problems?&lt;/p&gt; &#xA;&lt;p&gt;We focused on extension to a portfolio approach based on the observation that some topical areas tend to ask questions that would require multiple steps of reasoning and perhaps a scratch pad to keep track of multiple parts of a solution. Other areas seek factual answers that follow more directly from questions. Medprompt employs “chain-of-thought” (CoT) reasoning, resonating with multi-step solving. We wondered if the sophisticated Medprompt-classic approach might do less well on very simple questions and if the system might do better if a simpler method were used for the factual queries.&lt;/p&gt; &#xA;&lt;p&gt;Following this argument, we found that we could boost the performance on MMLU by extending MedPrompt with a simple two-method prompt portfolio. We add to the classic Medprompt a set of 10 simple, direct few-shot prompts soliciting an answer directly without Chain of Thought. We then ask GPT-4 for help with deciding on the best strategy for each topic area and question. As a screening call, for each question we first ask GPT-4:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Question&#xA;{{ question }}&#xA; &#xA;# Task&#xA;Does answering the question above require a scratch-pad?&#xA;A. Yes&#xA;B. No&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If GPT-4 thinks the question does require a scratch-pad, then the contribution of the Chain-of-Thought component of the ensemble is doubled. If it doesn&#39;t, we halve that contribution (and let the ensemble instead depend more on the direct few-shot prompts). Dynamically leveraging the appropriate prompting technique in the ensemble led to a further +0.5% performance improvement across the MMLU.&lt;/p&gt; &#xA;&lt;p&gt;We note that Medprompt+ relies on accessing confidence scores (logprobs) from GPT-4. These are not publicly available via the current API but will be enabled for all in the near future.&lt;/p&gt; &#xA;&lt;h2&gt;Running Scripts&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Some scripts hosted here are published for reference on methodology, but may not be immediately executable against public APIs. We&#39;re working hard on making the pipelines easier to run &#34;out of the box&#34; over the next few days, and appreciate your patience in the interim!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;First, clone the repo and install the promptbase package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd src&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, decide which tests you&#39;d like to run. You can choose from:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;bigbench&lt;/li&gt; &#xA; &lt;li&gt;drop&lt;/li&gt; &#xA; &lt;li&gt;gsm8k&lt;/li&gt; &#xA; &lt;li&gt;humaneval&lt;/li&gt; &#xA; &lt;li&gt;math&lt;/li&gt; &#xA; &lt;li&gt;mmlu&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Before running the tests, you will need to download the datasets from the original sources (see below) and place them in the &lt;code&gt;src/promptbase/datasets&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;After downloading datasets and installing the promptbase package, you can run a test with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python -m promptbase dataset_name&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python -m promptbase gsm8k&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dataset Links&lt;/h2&gt; &#xA;&lt;p&gt;To run evaluations, download these datasets and add them to /src/promptbase/datasets/&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MMLU: &lt;a href=&#34;https://github.com/hendrycks/test&#34;&gt;https://github.com/hendrycks/test&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;HumanEval: &lt;a href=&#34;https://huggingface.co/datasets/openai_humaneval&#34;&gt;https://huggingface.co/datasets/openai_humaneval&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DROP: &lt;a href=&#34;https://allenai.org/data/drop&#34;&gt;https://allenai.org/data/drop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GSM8K: &lt;a href=&#34;https://github.com/openai/grade-school-math&#34;&gt;https://github.com/openai/grade-school-math&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MATH: &lt;a href=&#34;https://huggingface.co/datasets/hendrycks/competition_math&#34;&gt;https://huggingface.co/datasets/hendrycks/competition_math&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Big-Bench-Hard: &lt;a href=&#34;https://github.com/suzgunmirac/BIG-Bench-Hard&#34;&gt;https://github.com/suzgunmirac/BIG-Bench-Hard&lt;/a&gt; The contents of this repo need to be put into a directory called &lt;code&gt;BigBench&lt;/code&gt; in the &lt;code&gt;datasets&lt;/code&gt; directory&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other Resources:&lt;/h2&gt; &#xA;&lt;p&gt;Medprompt Blog: &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/&#34;&gt;https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Medprompt Research Paper: &lt;a href=&#34;https://arxiv.org/abs/2311.16452&#34;&gt;https://arxiv.org/abs/2311.16452&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Medprompt+: &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/&#34;&gt;https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Microsoft Introduction to Prompt Engineering: &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering&#34;&gt;https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Microsoft Advanced Prompt Engineering Guide: &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions&#34;&gt;https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>