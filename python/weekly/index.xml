<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-02T01:53:59Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vladmandic/automatic</title>
    <updated>2024-06-02T01:53:59Z</updated>
    <id>tag:github.com,2024-06-02:/vladmandic/automatic</id>
    <link href="https://github.com/vladmandic/automatic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SD.Next: Advanced Implementation of Stable Diffusion and other Diffusion-based generative image models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/vladmandic/automatic/raw/dev/html/favicon.png&#34; width=&#34;200&#34; alt=&#34;SD.Next&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;Stable Diffusion implementation with advanced features&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/sponsors/vladmandic&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;color=%23fe8e86&#34; alt=&#34;Sponsors&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/vladmandic/automatic?svg=true&#34; alt=&#34;Last Commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/vladmandic/automatic?svg=true&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://discord.gg/VjvR2tabEX&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1101998836328697867?logo=Discord&amp;amp;svg=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/wiki&#34;&gt;Wiki&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/VjvR2tabEX&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/CHANGELOG.md&#34;&gt;Changelog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#sdnext-features&#34;&gt;SD.Next Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#backend-support&#34;&gt;Backend support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#model-support&#34;&gt;Model support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#platform-support&#34;&gt;Platform support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#notes&#34;&gt;Notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;SD.Next Features&lt;/h2&gt; &#xA;&lt;p&gt;All individual features are not listed here, instead check &lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/CHANGELOG.md&#34;&gt;ChangeLog&lt;/a&gt; for full list of changes&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multiple backends!&lt;br&gt; ‚ñπ &lt;strong&gt;Diffusers | Original&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multiple UIs!&lt;br&gt; ‚ñπ &lt;strong&gt;Standard | Modern&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multiple diffusion models!&lt;br&gt; ‚ñπ &lt;strong&gt;Stable Diffusion 1.5/2.1 | SD-XL | LCM | Segmind | Kandinsky | Pixart-Œ± | Pixart-Œ£ | Stable Cascade | W√ºrstchen | aMUSEd | DeepFloyd IF | UniDiffusion | SD-Distilled | BLiP Diffusion | KOALA | SDXS | Hyper-SD | etc.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Built-in Control for Text, Image, Batch and video processing!&lt;br&gt; ‚ñπ &lt;strong&gt;ControlNet | ControlNet XS | Control LLLite | T2I Adapters | IP Adapters&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multiplatform!&lt;br&gt; ‚ñπ &lt;strong&gt;Windows | Linux | MacOS with CPU | nVidia | AMD | IntelArc/IPEX | DirectML | OpenVINO | ONNX+Olive | ZLUDA&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Platform specific autodetection and tuning performed on install&lt;/li&gt; &#xA; &lt;li&gt;Optimized processing with latest &lt;code&gt;torch&lt;/code&gt; developments with built-in support for &lt;code&gt;torch.compile&lt;/code&gt;&lt;br&gt; and multiple compile backends: &lt;em&gt;Triton, ZLUDA, StableFast, DeepCache, OpenVINO, NNCF, IPEX, OneDiff&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improved prompt parser&lt;/li&gt; &#xA; &lt;li&gt;Enhanced &lt;em&gt;Lora&lt;/em&gt;/&lt;em&gt;LoCon&lt;/em&gt;/&lt;em&gt;Lyco&lt;/em&gt; code supporting latest trends in training&lt;/li&gt; &#xA; &lt;li&gt;Built-in queue management&lt;/li&gt; &#xA; &lt;li&gt;Enterprise level logging and hardened API&lt;/li&gt; &#xA; &lt;li&gt;Built in installer with automatic updates and dependency management&lt;/li&gt; &#xA; &lt;li&gt;Modernized UI with theme support and number of built-in themes &lt;em&gt;(dark and light)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mobile compatible&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;Main interface using &lt;strong&gt;StandardUI&lt;/strong&gt;&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-text2image.jpg&#34; alt=&#34;Screenshot-Dark&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Main interface using &lt;strong&gt;ModernUI&lt;/strong&gt;&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-modernui.jpg&#34; alt=&#34;Screenshot-Dark&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For screenshots and informations on other available themes, see &lt;a href=&#34;https://github.com/vladmandic/automatic/wiki/Themes&#34;&gt;Themes Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Backend support&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;SD.Next&lt;/strong&gt; supports two main backends: &lt;em&gt;Diffusers&lt;/em&gt; and &lt;em&gt;Original&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusers&lt;/strong&gt;: Based on new &lt;a href=&#34;https://huggingface.co/docs/diffusers/index&#34;&gt;Huggingface Diffusers&lt;/a&gt; implementation&lt;br&gt; Supports &lt;em&gt;all&lt;/em&gt; models listed below&lt;br&gt; This backend is set as default for new installations&lt;br&gt; See &lt;a href=&#34;https://github.com/vladmandic/automatic/wiki/Diffusers&#34;&gt;wiki article&lt;/a&gt; for more information&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Original&lt;/strong&gt;: Based on &lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;LDM&lt;/a&gt; reference implementation and significantly expanded on by &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;A1111&lt;/a&gt;&lt;br&gt; This backend and is fully compatible with most existing functionality and extensions written for &lt;em&gt;A1111 SDWebUI&lt;/em&gt;&lt;br&gt; Supports &lt;strong&gt;SD 1.x&lt;/strong&gt; and &lt;strong&gt;SD 2.x&lt;/strong&gt; models&lt;br&gt; All other model types such as &lt;em&gt;SD-XL, LCM, Stable Cascade, PixArt, Playground, Segmind, Kandinsky, etc.&lt;/em&gt; require backend &lt;strong&gt;Diffusers&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model support&lt;/h2&gt; &#xA;&lt;p&gt;Additional models will be added as they become available and there is public interest in them&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/stablediffusion/&#34;&gt;RunwayML Stable Diffusion&lt;/a&gt; 1.x and 2.x &lt;em&gt;(all variants)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;StabilityAI Stable Diffusion XL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-video-diffusion-img2vid&#34;&gt;StabilityAI Stable Video Diffusion&lt;/a&gt; Base, XT 1.0, XT 1.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/consistency_models&#34;&gt;LCM: Latent Consistency Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/playgroundai/playground-v2-256px-base&#34;&gt;Playground&lt;/a&gt; &lt;em&gt;v1, v2 256, v2 512, v2 1024 and latest v2.5&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/StableCascade&#34;&gt;Stable Cascade&lt;/a&gt; &lt;em&gt;Full&lt;/em&gt; and &lt;em&gt;Lite&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/amused/amused-256&#34;&gt;aMUSEd 256&lt;/a&gt; 256 and 512&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/segmind/Segmind-Vega&#34;&gt;Segmind Vega&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/segmind/SSD-1B&#34;&gt;Segmind SSD-1B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/segmind/segmoe&#34;&gt;Segmind SegMoE&lt;/a&gt; &lt;em&gt;SD and SD-XL&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ai-forever/Kandinsky-2&#34;&gt;Kandinsky&lt;/a&gt; &lt;em&gt;2.1 and 2.2 and latest 3.0&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-Œ± XL 2&lt;/a&gt; &lt;em&gt;Medium and Large&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-sigma&#34;&gt;PixArt-Œ£&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/wuertschen&#34;&gt;Warp Wuerstchen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thu-ml/unidiffuser&#34;&gt;Tsinghua UniDiffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deep-floyd/IF&#34;&gt;DeepFloyd IF&lt;/a&gt; &lt;em&gt;Medium and Large&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/damo-vilab/text-to-video-ms-1.7b&#34;&gt;ModelScope T2V&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/sd_distillation&#34;&gt;Segmind SD Distilled&lt;/a&gt; &lt;em&gt;(all variants)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dxli94.github.io/BLIP-Diffusion-website/&#34;&gt;BLIP-Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/youngwanLEE/sdxl-koala&#34;&gt;KOALA 700M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ali-vilab/i2vgen-xl&#34;&gt;VGen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDKiro/sdxs&#34;&gt;SDXS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ByteDance/Hyper-SD&#34;&gt;Hyper-SD&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also supported are modifiers such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LCM&lt;/strong&gt; and &lt;strong&gt;Turbo&lt;/strong&gt; (&lt;em&gt;adversarial diffusion distillation&lt;/em&gt;) networks&lt;/li&gt; &#xA; &lt;li&gt;All &lt;strong&gt;LoRA&lt;/strong&gt; types such as LoCon, LyCORIS, HADA, IA3, Lokr, OFT&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;IP-Adapters&lt;/strong&gt; for SD 1.5 and SD-XL&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;InstantID&lt;/strong&gt;, &lt;strong&gt;FaceSwap&lt;/strong&gt;, &lt;strong&gt;FaceID&lt;/strong&gt;, &lt;strong&gt;PhotoMerge&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AnimateDiff&lt;/strong&gt; for SD 1.5&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Platform support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;nVidia&lt;/em&gt; GPUs using &lt;strong&gt;CUDA&lt;/strong&gt; libraries on both &lt;em&gt;Windows and Linux&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;AMD&lt;/em&gt; GPUs using &lt;strong&gt;ROCm&lt;/strong&gt; libraries on &lt;em&gt;Linux&lt;/em&gt;&lt;br&gt; Support will be extended to &lt;em&gt;Windows&lt;/em&gt; once AMD releases ROCm for Windows&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Intel Arc&lt;/em&gt; GPUs using &lt;strong&gt;OneAPI&lt;/strong&gt; with &lt;em&gt;IPEX XPU&lt;/em&gt; libraries on both &lt;em&gt;Windows and Linux&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Any GPU compatible with &lt;em&gt;DirectX&lt;/em&gt; on &lt;em&gt;Windows&lt;/em&gt; using &lt;strong&gt;DirectML&lt;/strong&gt; libraries&lt;br&gt; This includes support for AMD GPUs that are not supported by native ROCm libraries&lt;/li&gt; &#xA; &lt;li&gt;Any GPU or device compatible with &lt;strong&gt;OpenVINO&lt;/strong&gt; libraries on both &lt;em&gt;Windows and Linux&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Apple M1/M2&lt;/em&gt; on &lt;em&gt;OSX&lt;/em&gt; using built-in support in Torch with &lt;strong&gt;MPS&lt;/strong&gt; optimizations&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;ONNX/Olive&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;IP Adapters&lt;/em&gt;: &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-ipadapter.jpg&#34; alt=&#34;Screenshot-IPAdapter&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Color grading&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-color.jpg&#34; alt=&#34;Screenshot-Color&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;InstantID&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-instantid.jpg&#34; alt=&#34;Screenshot-InstantID&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Loading any model other than standard SD 1.x / SD 2.x requires use of backend &lt;strong&gt;Diffusers&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Loading any other models using &lt;strong&gt;Original&lt;/strong&gt; backend is not supported&lt;/li&gt; &#xA;  &lt;li&gt;Loading manually download model &lt;code&gt;.safetensors&lt;/code&gt; files is supported for specified models only (typically SD 1.x / SD 2.x / SD-XL models only)&lt;/li&gt; &#xA;  &lt;li&gt;For all other model types, use backend &lt;strong&gt;Diffusers&lt;/strong&gt; and use built in Model downloader or&lt;br&gt; select model from Networks -&amp;gt; Models -&amp;gt; Reference list in which case it will be auto-downloaded and loaded&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/wiki/Installation&#34;&gt;Step-by-step install guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/wiki/Advanced-Install&#34;&gt;Advanced install notes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nWTnTyFTuAs&#34;&gt;Video: install and use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/discussions/1627&#34;&gt;Common installation errors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/discussions/1011&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;If you can&#39;t run SD.Next locally, try cloud deployment using &lt;a href=&#34;https://rundiffusion.com?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=SDNext&#34;&gt;RunDiffusion&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;Server can run with or without virtual environment,&lt;br&gt; Recommended to use &lt;code&gt;VENV&lt;/code&gt; to avoid library version conflicts with other applications&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;nVidia/CUDA&lt;/strong&gt; / &lt;strong&gt;AMD/ROCm&lt;/strong&gt; / &lt;strong&gt;Intel/OneAPI&lt;/strong&gt; are auto-detected if present and available,&lt;br&gt; For any other use case such as &lt;strong&gt;DirectML&lt;/strong&gt;, &lt;strong&gt;ONNX/Olive&lt;/strong&gt;, &lt;strong&gt;OpenVINO&lt;/strong&gt; specify required parameter explicitly&lt;br&gt; or wrong packages may be installed as installer will assume CPU-only environment&lt;/li&gt; &#xA;  &lt;li&gt;Full startup sequence is logged in &lt;code&gt;sdnext.log&lt;/code&gt;,&lt;br&gt; so if you encounter any issues, please check it first&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;p&gt;Once SD.Next is installed, simply run &lt;code&gt;webui.ps1&lt;/code&gt; or &lt;code&gt;webui.bat&lt;/code&gt; (&lt;em&gt;Windows&lt;/em&gt;) or &lt;code&gt;webui.sh&lt;/code&gt; (&lt;em&gt;Linux or MacOS&lt;/em&gt;)&lt;/p&gt; &#xA;&lt;p&gt;List of available parameters, run &lt;code&gt;webui --help&lt;/code&gt; for the full &amp;amp; up-to-date list:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Server options:&#xA;  --config CONFIG                                    Use specific server configuration file, default: config.json&#xA;  --ui-config UI_CONFIG                              Use specific UI configuration file, default: ui-config.json&#xA;  --medvram                                          Split model stages and keep only active part in VRAM, default: False&#xA;  --lowvram                                          Split model components and keep only active part in VRAM, default: False&#xA;  --ckpt CKPT                                        Path to model checkpoint to load immediately, default: None&#xA;  --vae VAE                                          Path to VAE checkpoint to load immediately, default: None&#xA;  --data-dir DATA_DIR                                Base path where all user data is stored, default:&#xA;  --models-dir MODELS_DIR                            Base path where all models are stored, default: models&#xA;  --allow-code                                       Allow custom script execution, default: False&#xA;  --share                                            Enable UI accessible through Gradio site, default: False&#xA;  --insecure                                         Enable extensions tab regardless of other options, default: False&#xA;  --use-cpu USE_CPU [USE_CPU ...]                    Force use CPU for specified modules, default: []&#xA;  --listen                                           Launch web server using public IP address, default: False&#xA;  --port PORT                                        Launch web server with given server port, default: 7860&#xA;  --freeze                                           Disable editing settings&#xA;  --auth AUTH                                        Set access authentication like &#34;user:pwd,user:pwd&#34;&#34;&#xA;  --auth-file AUTH_FILE                              Set access authentication using file, default: None&#xA;  --autolaunch                                       Open the UI URL in the system&#39;s default browser upon launch&#xA;  --docs                                             Mount API docs, default: False&#xA;  --api-only                                         Run in API only mode without starting UI&#xA;  --api-log                                          Enable logging of all API requests, default: False&#xA;  --device-id DEVICE_ID                              Select the default CUDA device to use, default: None&#xA;  --cors-origins CORS_ORIGINS                        Allowed CORS origins as comma-separated list, default: None&#xA;  --cors-regex CORS_REGEX                            Allowed CORS origins as regular expression, default: None&#xA;  --tls-keyfile TLS_KEYFILE                          Enable TLS and specify key file, default: None&#xA;  --tls-certfile TLS_CERTFILE                        Enable TLS and specify cert file, default: None&#xA;  --tls-selfsign                                     Enable TLS with self-signed certificates, default: False&#xA;  --server-name SERVER_NAME                          Sets hostname of server, default: None&#xA;  --no-hashing                                       Disable hashing of checkpoints, default: False&#xA;  --no-metadata                                      Disable reading of metadata from models, default: False&#xA;  --disable-queue                                    Disable queues, default: False&#xA;  --subpath SUBPATH                                  Customize the URL subpath for usage with reverse proxy&#xA;  --backend {original,diffusers}                     force model pipeline type&#xA;  --allowed-paths ALLOWED_PATHS [ALLOWED_PATHS ...]  add additional paths to paths allowed for web access&#xA;&#xA;Setup options:&#xA;  --reset                                            Reset main repository to latest version, default: False&#xA;  --upgrade                                          Upgrade main repository to latest version, default: False&#xA;  --requirements                                     Force re-check of requirements, default: False&#xA;  --quick                                            Bypass version checks, default: False&#xA;  --use-directml                                     Use DirectML if no compatible GPU is detected, default: False&#xA;  --use-openvino                                     Use Intel OpenVINO backend, default: False&#xA;  --use-ipex                                         Force use Intel OneAPI XPU backend, default: False&#xA;  --use-cuda                                         Force use nVidia CUDA backend, default: False&#xA;  --use-rocm                                         Force use AMD ROCm backend, default: False&#xA;  --use-zluda                                        Force use ZLUDA, AMD GPUs only, default: False&#xA;  --use-xformers                                     Force use xFormers cross-optimization, default: False&#xA;  --skip-requirements                                Skips checking and installing requirements, default: False&#xA;  --skip-extensions                                  Skips running individual extension installers, default: False&#xA;  --skip-git                                         Skips running all GIT operations, default: False&#xA;  --skip-torch                                       Skips running Torch checks, default: False&#xA;  --skip-all                                         Skips running all checks, default: False&#xA;  --skip-env                                         Skips setting of env variables during startup, default: False&#xA;  --experimental                                     Allow unsupported versions of libraries, default: False&#xA;  --reinstall                                        Force reinstallation of all requirements, default: False&#xA;  --test                                             Run test only and exit&#xA;  --version                                          Print version information&#xA;  --ignore                                           Ignore any errors and attempt to continue&#xA;  --safe                                             Run in safe mode with no user extensions&#xA;&#xA;Logging options:&#xA;  --log LOG                                          Set log file, default: None&#xA;  --debug                                            Run installer with debug logging, default: False&#xA;  --profile                                          Run profiler, default: False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] If you don&#39;t want to use built-in &lt;code&gt;venv&lt;/code&gt; support and prefer to run SD.Next in your own environment such as &lt;em&gt;Docker&lt;/em&gt; container, &lt;em&gt;Conda&lt;/em&gt; environment or any other virtual environment, you can skip &lt;code&gt;venv&lt;/code&gt; create/activate and launch SD.Next directly using &lt;code&gt;python launch.py&lt;/code&gt; (command line flags noted above still apply).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Control&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;SD.Next&lt;/strong&gt; comes with built-in control for all types of text2image, image2image, video2video and batch processing&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Control interface&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-control.jpg&#34; alt=&#34;Screenshot-Control&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Control processors&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-processors.jpg&#34; alt=&#34;Screenshot-Process&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Masking&lt;/em&gt;: &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-mask.jpg&#34; alt=&#34;Screenshot-Mask&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Extensions&lt;/h3&gt; &#xA;&lt;p&gt;SD.Next comes with several extensions pre-installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/sd-extension-system-info&#34;&gt;System Info&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/sd-extension-chainner&#34;&gt;chaiNNer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/sd-extension-rembg&#34;&gt;RemBg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ArtVentureX/sd-webui-agent-scheduler&#34;&gt;Agent Scheduler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BinaryQuantumSoul/sdnext-modernui&#34;&gt;Modern UI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Collab&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We&#39;d love to have additional maintainers (with comes with full repo rights). If you&#39;re interested, ping us!&lt;/li&gt; &#xA; &lt;li&gt;In addition to general cross-platform code, desire is to have a lead for each of the main platforms&lt;br&gt; This should be fully cross-platform, but we&#39;d really love to have additional contributors and/or maintainers to join and help lead the efforts on different platforms&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Credits&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Main credit goes to &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;Automatic1111 WebUI&lt;/a&gt; for original codebase&lt;/li&gt; &#xA; &lt;li&gt;Additional credits are listed in &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/#credits&#34;&gt;Credits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Licenses for modules are listed in &lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/licenses.html&#34;&gt;Licenses&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evolution&lt;/h3&gt; &#xA;&lt;a href=&#34;https://star-history.com/#vladmandic/automatic&amp;amp;Date&#34;&gt; &#xA; &lt;picture width=&#34;640&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=vladmandic/automatic&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;img src=&#34;https://api.star-history.com/svg?repos=vladmandic/automatic&amp;amp;type=Date&#34; alt=&#34;starts&#34; width=&#34;320&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ossinsight.io/analyze/vladmandic/automatic#overview&#34;&gt;OSS Stats&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docs&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re unsure how to use a feature, best place to start is &lt;a href=&#34;https://github.com/vladmandic/automatic/wiki&#34;&gt;Wiki&lt;/a&gt; and if its not there,&lt;br&gt; check &lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/CHANGELOG.md&#34;&gt;ChangeLog&lt;/a&gt; for when feature was first introduced as it will always have a short note on how to use it&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/wiki&#34;&gt;Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/README.md&#34;&gt;ReadMe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/TODO.md&#34;&gt;ToDo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/CHANGELOG.md&#34;&gt;ChangeLog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/cli/README.md&#34;&gt;CLI Tools&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Sponsors&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- sponsors --&gt;&#xA; &lt;a href=&#34;https://github.com/Tillerz&#34;&gt;&lt;img src=&#34;https://github.com/Tillerz.png&#34; width=&#34;60px&#34; alt=&#34;Tillerz&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/allangrant&#34;&gt;&lt;img src=&#34;https://github.com/allangrant.png&#34; width=&#34;60px&#34; alt=&#34;Allan Grant&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/BrentOzar&#34;&gt;&lt;img src=&#34;https://github.com/BrentOzar.png&#34; width=&#34;60px&#34; alt=&#34;Brent Ozar&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/inktomi&#34;&gt;&lt;img src=&#34;https://github.com/inktomi.png&#34; width=&#34;60px&#34; alt=&#34;Matthew Runo&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/4joeknight4&#34;&gt;&lt;img src=&#34;https://github.com/4joeknight4.png&#34; width=&#34;60px&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/SaladTechnologies&#34;&gt;&lt;img src=&#34;https://github.com/SaladTechnologies.png&#34; width=&#34;60px&#34; alt=&#34;Salad Technologies&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/mantzaris&#34;&gt;&lt;img src=&#34;https://github.com/mantzaris.png&#34; width=&#34;60px&#34; alt=&#34;a.v.mantzaris&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/CurseWave&#34;&gt;&lt;img src=&#34;https://github.com/CurseWave.png&#34; width=&#34;60px&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&#xA; &lt;!-- sponsors --&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>openai/whisper</title>
    <updated>2024-06-02T01:53:59Z</updated>
    <id>tag:github.com,2024-06-02:/openai/whisper</id>
    <link href="https://github.com/openai/whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Robust Speech Recognition via Large-Scale Weak Supervision&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Whisper&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.com/blog/whisper&#34;&gt;[Blog]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.04356&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/openai/whisper/raw/main/model-card.md&#34;&gt;[Model card]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb&#34;&gt;[Colab example]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.&lt;/p&gt; &#xA;&lt;h2&gt;Approach&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/whisper/main/approach.png&#34; alt=&#34;Approach&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We used Python 3.9.9 and &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably &lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;OpenAI&#39;s tiktoken&lt;/a&gt; for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U openai-whisper&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/openai/whisper.git &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update the package to the latest version of this repository, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It also requires the command-line tool &lt;a href=&#34;https://ffmpeg.org/&#34;&gt;&lt;code&gt;ffmpeg&lt;/code&gt;&lt;/a&gt; to be installed on your system, which is available from most package managers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# on Ubuntu or Debian&#xA;sudo apt update &amp;amp;&amp;amp; sudo apt install ffmpeg&#xA;&#xA;# on Arch Linux&#xA;sudo pacman -S ffmpeg&#xA;&#xA;# on MacOS using Homebrew (https://brew.sh/)&#xA;brew install ffmpeg&#xA;&#xA;# on Windows using Chocolatey (https://chocolatey.org/)&#xA;choco install ffmpeg&#xA;&#xA;# on Windows using Scoop (https://scoop.sh/)&#xA;scoop install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may need &lt;a href=&#34;http://rust-lang.org&#34;&gt;&lt;code&gt;rust&lt;/code&gt;&lt;/a&gt; installed as well, in case &lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;tiktoken&lt;/a&gt; does not provide a pre-built wheel for your platform. If you see installation errors during the &lt;code&gt;pip install&lt;/code&gt; command above, please follow the &lt;a href=&#34;https://www.rust-lang.org/learn/get-started&#34;&gt;Getting started page&lt;/a&gt; to install Rust development environment. Additionally, you may need to configure the &lt;code&gt;PATH&lt;/code&gt; environment variable, e.g. &lt;code&gt;export PATH=&#34;$HOME/.cargo/bin:$PATH&#34;&lt;/code&gt;. If the installation fails with &lt;code&gt;No module named &#39;setuptools_rust&#39;&lt;/code&gt;, you need to install &lt;code&gt;setuptools_rust&lt;/code&gt;, e.g. by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install setuptools-rust&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Available models and languages&lt;/h2&gt; &#xA;&lt;p&gt;There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and inference speed relative to the large model; actual speed may vary depending on many factors including the available hardware.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;English-only model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Multilingual model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Required VRAM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Relative speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tiny&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;tiny.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;tiny&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~32x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;base.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~16x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;small&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;244 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;small.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~2 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~6x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;medium&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;769 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;medium.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;medium&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~5 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~2x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1550 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;large&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~10 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The &lt;code&gt;.en&lt;/code&gt; models for English-only applications tend to perform better, especially for the &lt;code&gt;tiny.en&lt;/code&gt; and &lt;code&gt;base.en&lt;/code&gt; models. We observed that the difference becomes less significant for the &lt;code&gt;small.en&lt;/code&gt; and &lt;code&gt;medium.en&lt;/code&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;Whisper&#39;s performance varies widely depending on the language. The figure below shows a performance breakdown of &lt;code&gt;large-v3&lt;/code&gt; and &lt;code&gt;large-v2&lt;/code&gt; models by language, using WERs (word error rates) or CER (character error rates, shown in &lt;em&gt;Italic&lt;/em&gt;) evaluated on the Common Voice 15 and Fleurs datasets. Additional WER/CER metrics corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4 of &lt;a href=&#34;https://arxiv.org/abs/2212.04356&#34;&gt;the paper&lt;/a&gt;, as well as the BLEU (Bilingual Evaluation Understudy) scores for translation in Appendix D.3.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62&#34; alt=&#34;WER breakdown by language&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Command-line usage&lt;/h2&gt; &#xA;&lt;p&gt;The following command will transcribe speech in audio files, using the &lt;code&gt;medium&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper audio.flac audio.mp3 audio.wav --model medium&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default setting (which selects the &lt;code&gt;small&lt;/code&gt; model) works well for transcribing English. To transcribe an audio file containing non-English speech, you can specify the language using the &lt;code&gt;--language&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper japanese.wav --language Japanese&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adding &lt;code&gt;--task translate&lt;/code&gt; will translate the speech into English:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper japanese.wav --language Japanese --task translate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the following to view all available options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/openai/whisper/raw/main/whisper/tokenizer.py&#34;&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/p&gt; &#xA;&lt;h2&gt;Python usage&lt;/h2&gt; &#xA;&lt;p&gt;Transcription can also be performed within Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import whisper&#xA;&#xA;model = whisper.load_model(&#34;base&#34;)&#xA;result = model.transcribe(&#34;audio.mp3&#34;)&#xA;print(result[&#34;text&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Internally, the &lt;code&gt;transcribe()&lt;/code&gt; method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.&lt;/p&gt; &#xA;&lt;p&gt;Below is an example usage of &lt;code&gt;whisper.detect_language()&lt;/code&gt; and &lt;code&gt;whisper.decode()&lt;/code&gt; which provide lower-level access to the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import whisper&#xA;&#xA;model = whisper.load_model(&#34;base&#34;)&#xA;&#xA;# load audio and pad/trim it to fit 30 seconds&#xA;audio = whisper.load_audio(&#34;audio.mp3&#34;)&#xA;audio = whisper.pad_or_trim(audio)&#xA;&#xA;# make log-Mel spectrogram and move to the same device as the model&#xA;mel = whisper.log_mel_spectrogram(audio).to(model.device)&#xA;&#xA;# detect the spoken language&#xA;_, probs = model.detect_language(mel)&#xA;print(f&#34;Detected language: {max(probs, key=probs.get)}&#34;)&#xA;&#xA;# decode the audio&#xA;options = whisper.DecodingOptions()&#xA;result = whisper.decode(model, mel, options)&#xA;&#xA;# print the recognized text&#xA;print(result.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;Please use the &lt;a href=&#34;https://github.com/openai/whisper/discussions/categories/show-and-tell&#34;&gt;üôå Show and tell&lt;/a&gt; category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Whisper&#39;s code and model weights are released under the MIT License. See &lt;a href=&#34;https://github.com/openai/whisper/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for further details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>geekcomputers/Python</title>
    <updated>2024-06-02T01:53:59Z</updated>
    <id>tag:github.com,2024-06-02:/geekcomputers/Python</id>
    <link href="https://github.com/geekcomputers/Python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;My Python Examples&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;My Python Eggs üêç üòÑ&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;I do not consider myself as a programmer. I create these little programs as experiments to play with Python, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me: &lt;a href=&#34;mailto:craig@geekcomputers.co.uk&#34;&gt;craig@geekcomputers.co.uk&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This repository contains a collection of Python scripts that are designed to reduce human workload and serve as educational examples for beginners to get started with Python. The code documentation is aligned correctly for viewing in &lt;a href=&#34;https://notepad-plus-plus.org/&#34;&gt;Notepad++&lt;/a&gt; &lt;span&gt;üóí&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;Feel free to explore the scripts and use them for your learning and automation needs!&lt;/p&gt; &#xA;&lt;h2&gt;List of Scripts:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/batch_file_rename.py&#34;&gt;batch_file_rename.py&lt;/a&gt; - Batch rename a group of files in a specified directory, changing their extensions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/create_dir_if_not_there.py&#34;&gt;create_dir_if_not_there.py&lt;/a&gt; - Check if a directory exists in the user&#39;s home directory. Create it if it doesn&#39;t exist.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/youtubedownloader.py&#34;&gt;Fast Youtube Downloader&lt;/a&gt; - Download YouTube videos quickly with parallel threads using aria2c.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/tree/master/Google_Image_Downloader&#34;&gt;Google Image Downloader&lt;/a&gt; - Query a given term and retrieve images from the Google Image database.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/dir_test.py&#34;&gt;dir_test.py&lt;/a&gt; - Test if the directory &lt;code&gt;testdir&lt;/code&gt; exists. If not, create it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/env_check.py&#34;&gt;env_check.py&lt;/a&gt; - Check if all the required environment variables are set.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ratna04priya/Python/raw/master/BlackJack_game/blackjack.py&#34;&gt;blackjack.py&lt;/a&gt; - Casino Blackjack-21 game in Python.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/fileinfo.py&#34;&gt;fileinfo.py&lt;/a&gt; - Show file information for a given file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/folder_size.py&#34;&gt;folder_size.py&lt;/a&gt; - Scan the current directory and all subdirectories and display their sizes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/logs.py&#34;&gt;logs.py&lt;/a&gt; - Search for all &lt;code&gt;*.log&lt;/code&gt; files in a directory, zip them using the specified program, and date stamp them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/move_files_over_x_days.py&#34;&gt;move_files_over_x_days.py&lt;/a&gt; - Move all files over a specified age (in days) from the source directory to the destination directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/nslookup_check.py&#34;&gt;nslookup_check.py&lt;/a&gt; - Open the file &lt;code&gt;server_list.txt&lt;/code&gt; and perform nslookup for each server to check the DNS entry.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/osinfo.py&#34;&gt;osinfo.py&lt;/a&gt; - Display information about the operating system on which the script is running.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/ping_servers.py&#34;&gt;ping_servers.py&lt;/a&gt; - Ping the servers associated with the specified application group.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/ping_subnet.py&#34;&gt;ping_subnet.py&lt;/a&gt; - Scan the final range of a given IP subnet for available addresses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/powerdown_startup.py&#34;&gt;powerdown_startup.py&lt;/a&gt; - Ping machines in the server list. Load the putty session if the machine is up, or notify if it is not.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/puttylogs.py&#34;&gt;puttylogs.py&lt;/a&gt; - Zip all the logs in the given directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/script_count.py&#34;&gt;script_count.py&lt;/a&gt; - Scan the scripts directory and count the different types of scripts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/get_youtube_view.py&#34;&gt;get_youtube_view.py&lt;/a&gt; - Get more views for YouTube videos and repeat songs on YouTube.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/script_listing.py&#34;&gt;script_listing.py&lt;/a&gt; - List all files in a given directory and its subdirectories.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/testlines.py&#34;&gt;testlines.py&lt;/a&gt; - Open a file and print out 100 lines of the set line variable.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/tweeter.py&#34;&gt;tweeter.py&lt;/a&gt; - Tweet text or a picture from the terminal.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/serial_scanner.py&#34;&gt;serial_scanner.py&lt;/a&gt; - List available serial ports in use on Linux and Windows systems.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/get_youtube_view.py&#34;&gt;get_youtube_view.py&lt;/a&gt; - Get more views for YouTube videos and repeat songs on YouTube.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/CountMillionCharacter.py&#34;&gt;CountMillionCharacter.py&lt;/a&gt; and &lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/CountMillionCharacters-2.0.py&#34;&gt;CountMillionCharacter2.0&lt;/a&gt; - Get character count of a text file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/xkcd_downloader.py&#34;&gt;xkcd_downloader.py&lt;/a&gt; - Download the latest XKCD comic and place them in a new folder called &#34;comics&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/timymodule.py&#34;&gt;timymodule.py&lt;/a&gt; - An alternative to Python&#39;s &#39;timeit&#39; module and easier to use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/calculator.py&#34;&gt;calculator.py&lt;/a&gt; - Implement a calculator using Python&#39;s eval() function.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/Google_News.py&#34;&gt;Google_News.py&lt;/a&gt; - Use BeautifulSoup to provide latest news headlines along with news links.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/Cricket_score.py&#34;&gt;cricket_live_score&lt;/a&gt; - Use BeautifulSoup to provide live cricket scores.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/youtube.py&#34;&gt;youtube.py&lt;/a&gt; - Take a song name as input and fetch the YouTube URL of the best matching song and play it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/site_health.py&#34;&gt;site_health.py&lt;/a&gt; - Check the health of a remote server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/SimpleStopWatch.py&#34;&gt;SimpleStopWatch.py&lt;/a&gt; - Simple stop watch implementation using Python&#39;s time module.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/changemac.py&#34;&gt;Changemac.py&lt;/a&gt; - Change your MAC address, generate a random MAC address, or enter input as a new MAC address on Linux (Successfully Tested in Ubuntu 18.04).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/raw/master/whatsapp-monitor.py&#34;&gt;whatsapp-monitor.py&lt;/a&gt; - Use Selenium to give online status updates about your contacts in WhatsApp on the terminal.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/subahanii/whatsapp-Chat-Analyzer&#34;&gt;whatsapp-chat-analyzer.py&lt;/a&gt; - WhatsApp group/individual chat analyzer that visualizes chat activity using matplotlib.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://git.io/fjH8m&#34;&gt;JARVIS.py&lt;/a&gt; - Control Windows programs with your voice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://git.io/JvnJh&#34;&gt;Images Downloader&lt;/a&gt; - Download images from webpages on Unix-based systems.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/meezan-mallick/space_invader_game&#34;&gt;space_invader.py.py&lt;/a&gt; - Classical 2D space invader game to recall your childhood memories.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tanmay-901/test-case-generator/raw/master/test_case.py&#34;&gt;Test Case Generator&lt;/a&gt; - Generate different types of test cases with a clean and friendly UI, used in competitive programming and software testing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekcomputers/Python/tree/ExtractThumbnailFromVideo&#34;&gt;Extract Thumbnail From Video&lt;/a&gt; - Extract Thumbnail from video files&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=v2X51AVgl3o&#34;&gt;How to begin the journey of open source (first contribution)&lt;/a&gt; - First Contribution of open source&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;: The content in this repository belongs to the respective authors and creators. I&#39;m just providing a formatted README.md for better presentation.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
</feed>