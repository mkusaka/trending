<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-06T01:46:49Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>meta-llama/llama-stack</title>
    <updated>2024-10-06T01:46:49Z</updated>
    <id>tag:github.com,2024-10-06:/meta-llama/llama-stack</id>
    <link href="https://github.com/meta-llama/llama-stack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Model components of the Llama Stack APIs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama Stack&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/llama_stack/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/llama_stack.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/llama-stack/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/llama-stack&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/llama-stack&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1257833999603335178&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the Llama Stack API specifications as well as API Providers and Llama Stack Distributions.&lt;/p&gt; &#xA;&lt;p&gt;The Llama Stack defines and standardizes the building blocks needed to bring generative AI applications to market. These blocks span the entire development lifecycle: from model training and fine-tuning, through product evaluation, to building and running AI agents in production. Beyond definition, we are building providers for the Llama Stack APIs. These were developing open-source versions and partnering with providers, ensuring developers can assemble AI solutions using consistent, interlocking pieces across platforms. The ultimate goal is to accelerate innovation in the AI space.&lt;/p&gt; &#xA;&lt;p&gt;The Stack APIs are rapidly improving, but still very much work in progress and we invite feedback as well as direct contributions.&lt;/p&gt; &#xA;&lt;h2&gt;APIs&lt;/h2&gt; &#xA;&lt;p&gt;The Llama Stack consists of the following set of APIs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference&lt;/li&gt; &#xA; &lt;li&gt;Safety&lt;/li&gt; &#xA; &lt;li&gt;Memory&lt;/li&gt; &#xA; &lt;li&gt;Agentic System&lt;/li&gt; &#xA; &lt;li&gt;Evaluation&lt;/li&gt; &#xA; &lt;li&gt;Post Training&lt;/li&gt; &#xA; &lt;li&gt;Synthetic Data Generation&lt;/li&gt; &#xA; &lt;li&gt;Reward Scoring&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each of the APIs themselves is a collection of REST endpoints.&lt;/p&gt; &#xA;&lt;h2&gt;API Providers&lt;/h2&gt; &#xA;&lt;p&gt;A Provider is what makes the API real -- they provide the actual implementation backing the API.&lt;/p&gt; &#xA;&lt;p&gt;As an example, for Inference, we could have the implementation be backed by open source libraries like &lt;code&gt;[ torch | vLLM | TensorRT ]&lt;/code&gt; as possible options.&lt;/p&gt; &#xA;&lt;p&gt;A provider can also be just a pointer to a remote REST service -- for example, cloud providers or dedicated inference providers could serve these APIs.&lt;/p&gt; &#xA;&lt;h2&gt;Llama Stack Distribution&lt;/h2&gt; &#xA;&lt;p&gt;A Distribution is where APIs and Providers are assembled together to provide a consistent whole to the end application developer. You can mix-and-match providers -- some could be backed by local code and some could be remote. As a hobbyist, you can serve a small model locally, but can choose a cloud provider for a large model. Regardless, the higher level APIs your app needs to work with don&#39;t need to change at all. You can even imagine moving across the server / mobile-device boundary as well always using the same uniform set of APIs for developing Generative AI applications.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Llama Stack Implementations&lt;/h2&gt; &#xA;&lt;h3&gt;API Providers&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;API Provider Builder&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Environments&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Safety&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Telemetry&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta Reference&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fireworks&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AWS Bedrock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Together&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ollama&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TGI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted and Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chroma&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PG Vector&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PyTorch ExecuTorch&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;On-device iOS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Distributions&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Distribution Provider&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Safety&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Telemetry&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta Reference&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-gpu/general&#34;&gt;Local GPU&lt;/a&gt;, &lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-cpu/general&#34;&gt;Local CPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dell-TGI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-tgi-chroma/general&#34;&gt;Local TGI + Chroma&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install this repository as a &lt;a href=&#34;https://pypi.org/project/llama-stack/&#34;&gt;package&lt;/a&gt; with &lt;code&gt;pip install llama-stack&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to install from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/local&#xA;cd ~/local&#xA;git clone git@github.com:meta-llama/llama-stack.git&#xA;&#xA;conda create -n stack python=3.10&#xA;conda activate stack&#xA;&#xA;cd llama-stack&#xA;$CONDA_PREFIX/bin/pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;The Llama CLI&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;llama&lt;/code&gt; CLI makes it easy to work with the Llama Stack set of tools, including installing and running Distributions, downloading models, studying model prompt formats, etc. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-stack/main/docs/cli_reference.md&#34;&gt;CLI reference&lt;/a&gt; for details. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-stack/main/docs/getting_started.md&#34;&gt;Getting Started&lt;/a&gt; guide for running a Llama Stack server.&lt;/p&gt; &#xA;&lt;h2&gt;Llama Stack Client SDK&lt;/h2&gt; &#xA;&lt;p&gt;Check out our client SDKs for connecting to Llama Stack server in your preferred language, you can choose from &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-python&#34;&gt;python&lt;/a&gt;, &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-node&#34;&gt;node&lt;/a&gt;, &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-swift&#34;&gt;swift&lt;/a&gt;, and &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-kotlin&#34;&gt;kotlin&lt;/a&gt; programming languages to quickly build your applications.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>exo-explore/exo</title>
    <updated>2024-10-06T01:46:49Z</updated>
    <id>tag:github.com,2024-10-06:/exo-explore/exo</id>
    <link href="https://github.com/exo-explore/exo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;/docs/exo-logo-black-bg.jpg&#34;&gt; &#xA;  &lt;img alt=&#34;exo logo&#34; src=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href=&#34;https://x.com/exolabs&#34;&gt;exo labs&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt; &lt;p&gt;&lt;a href=&#34;https://discord.gg/EUnjGpsmWw&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://t.me/+Kh-KqHTzFYg3MGNk&#34;&gt;Telegram&lt;/a&gt; | &lt;a href=&#34;https://x.com/exolabs&#34;&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/exo-explore/exo/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/exo-explore/exo&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main&#34;&gt;&lt;img src=&#34;https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true&#34; alt=&#34;License: GPL v3&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Forget expensive NVIDIA GPUs, unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, Linux, pretty much any device!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;Update: exo is hiring. See &lt;a href=&#34;https://exolabs.net&#34;&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Get Involved&lt;/h2&gt; &#xA;&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href=&#34;https://x.com/exolabs&#34;&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; &#xA;&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing&#34;&gt;this sheet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Wide Model Support&lt;/h3&gt; &#xA;&lt;p&gt;exo supports different models including LLaMA (&lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py&#34;&gt;MLX&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py&#34;&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen and Deepseek.&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; &#xA;&lt;p&gt;exo &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py&#34;&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; &#xA;&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; &#xA;&lt;p&gt;exo will &lt;a href=&#34;https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/standard_node.py#L154&#34;&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; &#xA;&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; &#xA;&lt;p&gt;exo provides a &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py&#34;&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It&#39;s a &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh&#34;&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; &#xA;&lt;h3&gt;Device Equality&lt;/h3&gt; &#xA;&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href=&#34;https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/standard_node.py#L161&#34;&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; &#xA;&lt;p&gt;Exo supports different &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py&#34;&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py&#34;&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; &#xA;&lt;p&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;ring topology&#34; src=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/docs/ring-topology.png&#34; width=&#34;30%&#34; height=&#34;30%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href=&#34;https://github.com/exo-explore/exo/issues/5&#34;&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; &#xA; &lt;li&gt;Linux (with NVIDIA card): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;NVIDIA driver (test with &lt;code&gt;nvidia-smi&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;CUDA (&lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation&#34;&gt;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation&lt;/a&gt;) (test with &lt;code&gt;nvcc --version&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;cuDNN (&lt;a href=&#34;https://developer.nvidia.com/cudnn-downloads&#34;&gt;https://developer.nvidia.com/cudnn-downloads&lt;/a&gt;) (test with &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older&#34;&gt;link&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Hardware Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; &#xA;   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; &#xA;   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;From source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/exo-explore/exo.git&#xA;cd exo&#xA;pip install -e .&#xA;# alternatively, with venv&#xA;source install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If running on Mac, MLX has an &lt;a href=&#34;https://ml-explore.github.io/mlx/build/html/install.html&#34;&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Upgrade to the latest version of MacOS 15.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Example Usage on Multiple MacOS Devices&lt;/h3&gt; &#xA;&lt;h4&gt;Device 1:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Device 2:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; &#xA;&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href=&#34;https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat&#34;&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href=&#34;http://localhost:8000/v1/chat/completions&#34;&gt;http://localhost:8000/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; &#xA;&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;     &#34;model&#34;: &#34;llama-3.2-3b&#34;,&#xA;     &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the meaning of exo?&#34;}],&#xA;     &#34;temperature&#34;: 0.7&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;     &#34;model&#34;: &#34;llama-3.1-405b&#34;,&#xA;     &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the meaning of exo?&#34;}],&#xA;     &#34;temperature&#34;: 0.7&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;     &#34;model&#34;: &#34;llava-1.5-7b-hf&#34;,&#xA;     &#34;messages&#34;: [&#xA;      {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;          {&#xA;            &#34;type&#34;: &#34;text&#34;,&#xA;            &#34;text&#34;: &#34;What are these?&#34;&#xA;          },&#xA;          {&#xA;            &#34;type&#34;: &#34;image_url&#34;,&#xA;            &#34;image_url&#34;: {&#xA;              &#34;url&#34;: &#34;http://images.cocodataset.org/val2017/000000039769.jpg&#34;&#xA;            }&#xA;          }&#xA;        ]&#xA;      }&#xA;    ],&#xA;     &#34;temperature&#34;: 0.0&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (MacOS + Linux)&lt;/h3&gt; &#xA;&lt;h4&gt;Device 1 (MacOS):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo --inference-engine tinygrad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; &#xA;&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; &#xA;&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href=&#34;https://docs.tinygrad.org/env_vars/&#34;&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Example Usage on a single device with &#34;exo run&#34; command&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo run llama-3.2-3b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With a custom prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo run llama-3.2-3b --prompt &#34;What is the meaning of exo?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Debugging&lt;/h2&gt; &#xA;&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;DEBUG=9 exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;TINYGRAD_DEBUG=2 exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On some versions of MacOS/Python, certificates are not installed properly which can lead to SSL errors (e.g. SSL error with huggingface.co). To fix this, run the Install Certificates command, usually:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/Applications/Python 3.x/Install Certificates.command&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöß As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it&#39;s ready. If you would like access to the iOS implementation now, please email &lt;a href=&#34;mailto:alex@exolabs.net&#34;&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Inference Engines&lt;/h2&gt; &#xA;&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py&#34;&gt;MLX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py&#34;&gt;tinygrad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://github.com/exo-explore/exo/pull/139&#34;&gt;PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://github.com/exo-explore/exo/issues/167&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Networking Modules&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc&#34;&gt;GRPC&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/TODO&#34;&gt;Radio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/TODO&#34;&gt;Bluetooth&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/RD-Agent</title>
    <updated>2024-10-06T01:46:49Z</updated>
    <id>tag:github.com,2024-10-06:/microsoft/RD-Agent</id>
    <link href="https://github.com/microsoft/RD-Agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Research and development (R&amp;D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&amp;D are mainly focused on data and models. We are committed to automate these high-value generic R&amp;D processes through our open source R&amp;D automation tool RD-Agent, which let AI drive data-driven AI.&lt;/p&gt;&lt;hr&gt;&lt;h4 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/logo.png&#34; alt=&#34;RA-Agent logo&#34; style=&#34;width:70%; &#34;&gt; &lt;p&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net&#34; target=&#34;_blank&#34;&gt;üñ•Ô∏è Live Demo&lt;/a&gt; | &lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop&#34; target=&#34;_blank&#34;&gt;üé• Demo Video&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR&#34; target=&#34;_blank&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt; | &lt;a href=&#34;https://rdagent.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34;&gt;üìñ Documentation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#-paperwork-list&#34;&gt; üìÉ Papers &lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg?sanitize=true&#34; alt=&#34;CodeQL&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg?sanitize=true&#34; alt=&#34;Dependabot Updates&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg?sanitize=true&#34; alt=&#34;Lint PR Title&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/release.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg?sanitize=true&#34; alt=&#34;Release.yml&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/rdagent/#files&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/platform-Linux-blue&#34; alt=&#34;Platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/rdagent/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/rdagent&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/rdagent/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/rdagent&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/microsoft/RD-Agent&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/RD-Agent&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pre-commit/pre-commit&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&#34; alt=&#34;pre-commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://mypy-lang.org/&#34;&gt;&lt;img src=&#34;https://www.mypy-lang.org/static/mypy_badge.svg?sanitize=true&#34; alt=&#34;Checked with mypy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/astral-sh/ruff&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&#34; alt=&#34;Ruff&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/ybQ97B6Jjy&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-discord-blue&#34; alt=&#34;Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg?sanitize=true&#34; alt=&#34;Readthedocs Preview&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üì∞ News&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;üóûÔ∏è News&lt;/th&gt; &#xA;   &lt;th&gt;üìù Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Official WeChat group release&lt;/td&gt; &#xA;   &lt;td&gt;We created a WeChat group, welcome to join! (üó™&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/WeChat_QR_code.jpg&#34;&gt;QR Code&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Official Discord release&lt;/td&gt; &#xA;   &lt;td&gt;We launch our first chatting channel in Discord (üó™&lt;a href=&#34;https://discord.gg/ybQ97B6Jjy&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-discord-blue&#34; alt=&#34;Chat&#34;&gt;&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;First release&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;RDAgent&lt;/strong&gt; is released on GitHub&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;üåü Introduction&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/scen.png&#34; alt=&#34;Our focused scenario&#34; style=&#34;width:80%; &#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;RDAgent aims to automate the most critical and valuable aspects of the industrial R&amp;amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. Methodologically, we have identified a framework with two key components: &#39;R&#39; for proposing new ideas and &#39;D&#39; for implementing them. We believe that the automatic evolution of R&amp;amp;D will lead to solutions of significant industrial value.&lt;/p&gt; &#xA;&lt;!-- Tag Cloud --&gt; &#xA;&lt;p&gt;R&amp;amp;D is a very general scenario. The advent of RDAgent can be your&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí∞ &lt;strong&gt;Automatic Quant Factory&lt;/strong&gt; (&lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop&#34;&gt;üé•Demo Video&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ &lt;strong&gt;Data Mining Agent:&lt;/strong&gt; Iteratively proposing data &amp;amp; models (&lt;a href=&#34;https://rdagent.azurewebsites.net/model_loop&#34;&gt;üé•Demo Video 1&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) (&lt;a href=&#34;https://rdagent.azurewebsites.net/dmm&#34;&gt;üé•Demo Video 2&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=VIaSTZuoZg4&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) and implementing them by gaining knowledge from data.&lt;/li&gt; &#xA; &lt;li&gt;ü¶æ &lt;strong&gt;Research Copilot:&lt;/strong&gt; Auto read research papers (&lt;a href=&#34;https://rdagent.azurewebsites.net/report_model&#34;&gt;üé•Demo Video&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=BiA2SfdKQ7o&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) / financial reports (&lt;a href=&#34;https://rdagent.azurewebsites.net/report_factor&#34;&gt;üé•Demo Video&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=ECLTXVcSx-c&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) and implement model structures or building datasets.&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can click the links above to view the demo. We&#39;re continuously adding more methods and scenarios to the project to enhance your R&amp;amp;D processes and boost productivity.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, you can take a closer look at the examples in our &lt;strong&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/&#34;&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://rdagent.azurewebsites.net/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/demo.png&#34; alt=&#34;Watch the demo&#34; width=&#34;80%&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;‚ö° Quick start&lt;/h1&gt; &#xA;&lt;p&gt;You can try above demos by running the following command:&lt;/p&gt; &#xA;&lt;h3&gt;üê≥ Docker installation.&lt;/h3&gt; &#xA;&lt;p&gt;Users must ensure Docker is installed before attempting most scenarios. Please refer to the &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;official üê≥Docker page&lt;/a&gt; for installation instructions.&lt;/p&gt; &#xA;&lt;h3&gt;üêç Create a Conda Environment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI): &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n rdagent python=3.10&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Activate the environment: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda activate rdagent&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üõ†Ô∏è Install the RDAgent&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can directly install the RDAgent package from PyPI: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install rdagent&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;‚öôÔ∏è Configuration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You have to config your GPT model in the &lt;code&gt;.env&lt;/code&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env&#xA;OPENAI_API_KEY=&amp;lt;your_api_key&amp;gt;&#xA;# EMBEDDING_MODEL=text-embedding-3-small&#xA;CHAT_MODEL=gpt-4-turbo&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üöÄ Run the Application&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;strong&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/&#34;&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/strong&gt; is implemented by the following commands(each item represents one demo, you can select the one you prefer):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Factors Evolution&lt;/strong&gt;: &lt;a href=&#34;http://github.com/microsoft/qlib&#34;&gt;Qlib&lt;/a&gt; self-loop factor proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rdagent fin_factor&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Model Evolution&lt;/strong&gt;: &lt;a href=&#34;http://github.com/microsoft/qlib&#34;&gt;Qlib&lt;/a&gt; self-loop model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rdagent fin_model&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Medical Prediction Model Evolution&lt;/strong&gt;: Medical self-loop model proposal and implementation application&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;(1) Apply for an account at &lt;a href=&#34;https://physionet.org/&#34;&gt;PhysioNet&lt;/a&gt;. &lt;br&gt; (2) Request access to FIDDLE preprocessed data: &lt;a href=&#34;https://physionet.org/content/mimic-eicu-fiddle-feature/1.0.0/&#34;&gt;FIDDLE Dataset&lt;/a&gt;. &lt;br&gt; (3) Place your username and password in &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt;&amp;gt; .env&#xA;DM_USERNAME=&amp;lt;your_username&amp;gt;&#xA;DM_PASSWORD=&amp;lt;your_password&amp;gt;&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rdagent med_model&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Factors Extraction from Financial Reports&lt;/strong&gt;: Run the &lt;a href=&#34;http://github.com/microsoft/qlib&#34;&gt;Qlib&lt;/a&gt; factor extraction and implementation application based on financial reports&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 1. Generally, you can run this scenario using the following command:&#xA;rdagent fin_factor_report --report_folder=&amp;lt;Your financial reports folder path&amp;gt;&#xA;&#xA;# 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:&#xA;wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip&#xA;unzip all_reports.zip -d git_ignore_folder/reports&#xA;rdagent fin_factor_report --report_folder=git_ignore_folder/reports&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Model Research &amp;amp; Development Copilot&lt;/strong&gt;: model extraction and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 1. Generally, you can run your own papers/reports with the following command:&#xA;rdagent general_model &amp;lt;Your paper URL&amp;gt;&#xA;&#xA;# 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:&#xA;rdagent general_model  &#34;https://arxiv.org/pdf/2210.09789&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üñ•Ô∏è Monitor the Application Results&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can serve our demo app to monitor the RD loop by running the following command: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rdagent ui --port 80 --log_dir &amp;lt;your log folder like &#34;log/&#34;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üè≠ Scenarios&lt;/h1&gt; &#xA;&lt;p&gt;We have applied RD-Agent to multiple valuable data-driven industrial scenarios.&lt;/p&gt; &#xA;&lt;h2&gt;üéØ Goal: Agent for Data-driven R&amp;amp;D&lt;/h2&gt; &#xA;&lt;p&gt;In this project, we are aiming to build an Agent to automate Data-Driven R&amp;amp;D that can&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìÑ Read real-world material (reports, papers, etc.) and &lt;strong&gt;extract&lt;/strong&gt; key formulas, descriptions of interested &lt;strong&gt;features&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt;, which are the key components of data-driven R&amp;amp;D .&lt;/li&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Implement&lt;/strong&gt; the extracted formulas (e.g., features, factors, and models) in runnable codes. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üí° Propose &lt;strong&gt;new ideas&lt;/strong&gt; based on current knowledge and observations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ![Data-Centric R&amp;D Overview](docs/_static/overview.png) --&gt; &#xA;&lt;h2&gt;üìà Scenarios/Demos&lt;/h2&gt; &#xA;&lt;p&gt;In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ü¶æCopilot and ü§ñAgent.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The ü¶æCopilot follows human instructions to automate repetitive tasks.&lt;/li&gt; &#xA; &lt;li&gt;The ü§ñAgent, being more autonomous, actively proposes ideas for better results in the future.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The supported scenarios are listed below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scenario/Target&lt;/th&gt; &#xA;   &lt;th&gt;Model Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Data Building&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üíπ Finance&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ñ &lt;a href=&#34;https://rdagent.azurewebsites.net/model_loop&#34;&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ñ &lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop&#34;&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt; &lt;br&gt; ü¶æ &lt;a href=&#34;https://rdagent.azurewebsites.net/report_factor&#34;&gt;Auto reports reading &amp;amp; implementation&lt;/a&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ECLTXVcSx-c&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ü©∫ Medical&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ñ &lt;a href=&#34;https://rdagent.azurewebsites.net/dmm&#34;&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VIaSTZuoZg4&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üè≠ General&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü¶æ &lt;a href=&#34;https://rdagent.azurewebsites.net/report_model&#34;&gt;Auto paper reading &amp;amp; implementation&lt;/a&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BiA2SfdKQ7o&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.&lt;/p&gt; &#xA;&lt;p&gt;Here is a gallery of &lt;a href=&#34;https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip&#34;&gt;successful explorations&lt;/a&gt; (5 traces showed in &lt;strong&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/&#34;&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/strong&gt;). You can download and view the execution trace using the command below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rdagent ui --port 80 --log_dir ./demo_traces&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;strong&gt;&lt;a href=&#34;https://rdagent.readthedocs.io/en/latest/scens/catalog.html&#34;&gt;üìñreadthedocs_scen&lt;/a&gt;&lt;/strong&gt; for more details of the scenarios.&lt;/p&gt; &#xA;&lt;h1&gt;‚öôÔ∏è Framework&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/Framework-RDAgent.png&#34; alt=&#34;Framework-RDAgent&#34; width=&#34;85%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Automating the R&amp;amp;D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.&lt;/p&gt; &#xA;&lt;p&gt;The research questions within this framework can be divided into three main categories:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Research Area&lt;/th&gt; &#xA;   &lt;th&gt;Paper/Work List&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Benchmark the R&amp;amp;D abilities&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Idea proposal:&lt;/strong&gt; Explore new ideas or refine existing ones&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#research&#34;&gt;Research&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Ability to realize ideas:&lt;/strong&gt; Implement and execute ideas&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#development&#34;&gt;Development&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We believe that the key to delivering high-quality solutions lies in the ability to evolve R&amp;amp;D capabilities. Agents should learn like human experts, continuously improving their R&amp;amp;D skills.&lt;/p&gt; &#xA;&lt;p&gt;More documents can be found in the &lt;strong&gt;&lt;a href=&#34;https://rdagent.readthedocs.io/&#34;&gt;üìñ readthedocs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;üìÉ Paper/Work list&lt;/h1&gt; &#xA;&lt;h2&gt;üìä Benchmark&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.11276&#34;&gt;Towards Data-Centric Automatic R&amp;amp;D&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{chen2024datacentric,&#xA;    title={Towards Data-Centric Automatic R&amp;amp;D},&#xA;    author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},&#xA;    year={2024},&#xA;    eprint={2404.11276},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/494f55d3-de9e-4e73-ba3d-a787e8f9e841&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üîç Research&lt;/h2&gt; &#xA;&lt;p&gt;In a data mining expert&#39;s daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.&lt;/p&gt; &#xA;&lt;p&gt;Based on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.&lt;/p&gt; &#xA;&lt;p&gt;For more detail, please refer to our &lt;strong&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net&#34;&gt;üñ•Ô∏è Live Demo page&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Development&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.18690&#34;&gt;Collaborative Evolving Strategy for Automatic Data-Centric Development&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{yang2024collaborative,&#xA;    title={Collaborative Evolving Strategy for Automatic Data-Centric Development},&#xA;    author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},&#xA;    year={2024},&#xA;    eprint={2407.18690},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/75d9769b-0edd-4caf-9d45-57d1e577054b&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ü§ù Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;üìù Guidelines&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Contributing to this project is straightforward and rewarding. Whether it&#39;s solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve RDAgent.&lt;/p&gt; &#xA;&lt;p&gt;To get started, you can explore the issues list, or search for &lt;code&gt;TODO:&lt;/code&gt; comments in the codebase by running the command &lt;code&gt;grep -r &#34;TODO:&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://img.shields.io/github/contributors-anon/microsoft/RD-Agent&#34;&gt; &#xA;&lt;a href=&#34;https://github.com/microsoft/RD-Agent/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=microsoft/RD-Agent&amp;amp;max=100&amp;amp;columns=15&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Before we released RD-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.&lt;/p&gt; &#xA;&lt;h1&gt;‚öñÔ∏è Legal disclaimer&lt;/h1&gt; &#xA;&lt;p style=&#34;line-height: 1; font-style: italic;&#34;&gt;The RD-agent is provided ‚Äúas is‚Äù, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.&lt;/p&gt;</summary>
  </entry>
</feed>