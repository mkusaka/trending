<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-09T01:45:44Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lkwq007/stablediffusion-infinity</title>
    <updated>2022-10-09T01:45:44Z</updated>
    <id>tag:github.com,2022-10-09:/lkwq007/stablediffusion-infinity</id>
    <link href="https://github.com/lkwq007/stablediffusion-infinity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Outpainting with Stable Diffusion on an infinite canvas&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;stablediffusion-infinity&lt;/h1&gt; &#xA;&lt;p&gt;Outpainting with Stable Diffusion on an infinite canvas.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/lkwq007/stablediffusion-infinity/blob/master/stablediffusion_infinity_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/lnyan/stablediffusion-infinity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Start with init_image (updated demo in Gradio):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1665437/193394123-d202efc8-24a7-41b3-a5cf-6b2e0b60db28.mp4&#34;&gt;https://user-images.githubusercontent.com/1665437/193394123-d202efc8-24a7-41b3-a5cf-6b2e0b60db28.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Start with text2img (&lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity/tree/ipycanvas&#34;&gt;ipycanvas&lt;/a&gt; version):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1665437/190212025-f4a82c46-0ff1-4ca2-b79b-6c81601e3eed.mp4&#34;&gt;https://user-images.githubusercontent.com/1665437/190212025-f4a82c46-0ff1-4ca2-b79b-6c81601e3eed.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The web app might work on Windows (see this issue &lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity/issues/12&#34;&gt;https://github.com/lkwq007/stablediffusion-infinity/issues/12&lt;/a&gt; for more information) and Apple Silicon devices (untested, check guide here: &lt;a href=&#34;https://huggingface.co/docs/diffusers/optimization/mps&#34;&gt;https://huggingface.co/docs/diffusers/optimization/mps&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project mainly works as a proof of concept. In that case, &lt;del&gt;the UI design is relatively weak&lt;/del&gt;, and the quality of results is not guaranteed. You may need to do prompt engineering or change the size of the selection box to get better outpainting results.&lt;/p&gt; &#xA;&lt;p&gt;The project now becomes a web app based on PyScript and Gradio. For Jupyter Notebook version, please check out the &lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity/tree/ipycanvas&#34;&gt;ipycanvas&lt;/a&gt; branch.&lt;/p&gt; &#xA;&lt;p&gt;Pull requests are welcome for better UI control, ideas to achieve better results, or any other improvements.&lt;/p&gt; &#xA;&lt;p&gt;Update: the project also supports &lt;a href=&#34;https://github.com/Jack000/glid-3-xl-stable&#34;&gt;glid-3-xl-stable&lt;/a&gt; as inpainting/outpainting model. Note that you have to restart the &lt;code&gt;app.py&lt;/code&gt; to change model. (not supported on colab)&lt;/p&gt; &#xA;&lt;p&gt;Update: the project add photometric correction to suppress seams, to use this feature, you need to install &lt;a href=&#34;https://github.com/Trinkle23897/Fast-Poisson-Image-Editing&#34;&gt;fpie&lt;/a&gt;: &lt;code&gt;pip install fpie&lt;/code&gt; (Linux/MacOS only)&lt;/p&gt; &#xA;&lt;h2&gt;Setup environment&lt;/h2&gt; &#xA;&lt;p&gt;setup with &lt;code&gt;environment.yml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules https://github.com/lkwq007/stablediffusion-infinity&#xA;cd stablediffusion-infinity&#xA;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if the &lt;code&gt;environment.yml&lt;/code&gt; doesn&#39;t work for you, you may install dependencies manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n sd-inf python=3.10&#xA;conda activate sd-inf&#xA;conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch&#xA;conda install scipy scikit-image&#xA;conda install -c conda-forge diffusers transformers ftfy&#xA;pip install opencv-python&#xA;pip install gradio==3.4.0&#xA;pip install pytorch-lightning==1.7.7 einops==0.4.1 omegaconf==2.2.3&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For windows, you may need to replace &lt;code&gt;pip install opencv-python&lt;/code&gt; with &lt;code&gt;conda install -c conda-forge opencv&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CPP library (optional)&lt;/h2&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;opencv&lt;/code&gt; library (e.g. &lt;code&gt;libopencv-dev&lt;/code&gt;/&lt;code&gt;opencv-devel&lt;/code&gt;, the package name may differ on different distributions) is required for &lt;code&gt;PyPatchMatch&lt;/code&gt;. You may need to install &lt;code&gt;opencv&lt;/code&gt; by yourself. If no &lt;code&gt;opencv&lt;/code&gt; installed, the &lt;code&gt;patch_match&lt;/code&gt; option (usually better quality) won&#39;t work.&lt;/p&gt; &#xA;&lt;h2&gt;How-to&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate sd-inf&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running with Docker on Windows or Linux with NVIDIA GPU&lt;/h2&gt; &#xA;&lt;p&gt;On Windows 10 or 11 you can follow this guide to setting up Docker with WSL2 &lt;a href=&#34;https://www.youtube.com/watch?v=PB7zM3JrgkI&#34;&gt;https://www.youtube.com/watch?v=PB7zM3JrgkI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Native Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd stablediffusion-infinity/docker&#xA;./docker-run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows 10,11 with WSL2 shell:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;open windows Command Prompt, type &#34;bash&#34;&lt;/li&gt; &#xA; &lt;li&gt;once in bash, type:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd /mnt/c/PATH-TO-YOUR/stablediffusion-infinity/docker&#xA;./docker-run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open &#34;&lt;a href=&#34;http://localhost:8888&#34;&gt;http://localhost:8888&lt;/a&gt;&#34; in your browser ( even though the log says &lt;a href=&#34;http://0.0.0.0:8888&#34;&gt;http://0.0.0.0:8888&lt;/a&gt; )&lt;/p&gt; &#xA;&lt;h2&gt;FAQs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Troubleshooting on Windows (outdated): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity/issues/12&#34;&gt;https://github.com/lkwq007/stablediffusion-infinity/issues/12&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The result is a black square: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;False positive rate of safety checker is relatively high, you may disable the safety_checker&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What is the init_mode &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;init_mode indicates how to fill the empty/masked region, usually &lt;code&gt;patch_match&lt;/code&gt; is better than others&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Why not use &lt;code&gt;postMessage&lt;/code&gt; for iframe interaction &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The iframe and the gradio are in the same origin. For &lt;code&gt;postMessage&lt;/code&gt; version, check out &lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity/tree/gradio-space&#34;&gt;gradio-space&lt;/a&gt; version&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;p&gt;The code of &lt;code&gt;perlin2d.py&lt;/code&gt; is from &lt;a href=&#34;https://stackoverflow.com/questions/42147776/producing-2d-perlin-noise-with-numpy/42154921#42154921&#34;&gt;https://stackoverflow.com/questions/42147776/producing-2d-perlin-noise-with-numpy/42154921#42154921&lt;/a&gt; and is &lt;strong&gt;not&lt;/strong&gt; included in the scope of LICENSE used in this repo.&lt;/p&gt; &#xA;&lt;p&gt;The submodule &lt;code&gt;glid_3_xl_stable&lt;/code&gt; is based on &lt;a href=&#34;https://github.com/Jack000/glid-3-xl-stable&#34;&gt;https://github.com/Jack000/glid-3-xl-stable&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The submodule &lt;code&gt;PyPatchMatch&lt;/code&gt; is based on &lt;a href=&#34;https://github.com/vacancy/PyPatchMatch&#34;&gt;https://github.com/vacancy/PyPatchMatch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The code of &lt;code&gt;postprocess.py&lt;/code&gt; and &lt;code&gt;process.py&lt;/code&gt; is modified based on &lt;a href=&#34;https://github.com/Trinkle23897/Fast-Poisson-Image-Editing&#34;&gt;https://github.com/Trinkle23897/Fast-Poisson-Image-Editing&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>psf/black</title>
    <updated>2022-10-09T01:45:44Z</updated>
    <id>tag:github.com,2022-10-09:/psf/black</id>
    <link href="https://github.com/psf/black" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The uncompromising Python code formatter&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/psf/black/main/docs/_static/logo2-readme.png&#34; alt=&#34;Black Logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;The Uncompromising Code Formatter&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/psf/black/actions&#34;&gt;&lt;img alt=&#34;Actions Status&#34; src=&#34;https://github.com/psf/black/workflows/Test/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://black.readthedocs.io/en/stable/?badge=stable&#34;&gt;&lt;img alt=&#34;Documentation Status&#34; src=&#34;https://readthedocs.org/projects/black/badge/?version=stable&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://coveralls.io/github/psf/black?branch=main&#34;&gt;&lt;img alt=&#34;Coverage Status&#34; src=&#34;https://coveralls.io/repos/github/psf/black/badge.svg?branch=main&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black/raw/main/LICENSE&#34;&gt;&lt;img alt=&#34;License: MIT&#34; src=&#34;https://black.readthedocs.io/en/stable/_static/license.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/black/&#34;&gt;&lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/black&#34;&gt;&lt;img alt=&#34;Downloads&#34; src=&#34;https://pepy.tech/badge/black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/black/&#34;&gt;&lt;img alt=&#34;conda-forge&#34; src=&#34;https://img.shields.io/conda/dn/conda-forge/black.svg?label=conda-forge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img alt=&#34;Code style: black&#34; src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;“Any color you like.”&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return, &lt;em&gt;Black&lt;/em&gt; gives you speed, determinism, and freedom from &lt;code&gt;pycodestyle&lt;/code&gt; nagging about formatting. You will save time and mental energy for more important matters.&lt;/p&gt; &#xA;&lt;p&gt;Blackened code looks the same regardless of the project you&#39;re reading. Formatting becomes transparent after a while and you can focus on the content instead.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; makes code review faster by producing the smallest diffs possible.&lt;/p&gt; &#xA;&lt;p&gt;Try it out now using the &lt;a href=&#34;https://black.vercel.app&#34;&gt;Black Playground&lt;/a&gt;. Watch the &lt;a href=&#34;https://youtu.be/esZLCuWs_2Y&#34;&gt;PyCon 2019 talk&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable&#34;&gt;Read the documentation on ReadTheDocs!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation and usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; can be installed by running &lt;code&gt;pip install black&lt;/code&gt;. It requires Python 3.7+ to run. If you want to format Jupyter Notebooks, install with &lt;code&gt;pip install &#39;black[jupyter]&#39;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you can&#39;t wait for the latest &lt;em&gt;hotness&lt;/em&gt; and want to install from GitHub, use:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install git+https://github.com/psf/black&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;To get started right away with sensible defaults:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;black {source_file_or_directory}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can run &lt;em&gt;Black&lt;/em&gt; as a package if running it as a script doesn&#39;t work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m black {source_file_or_directory}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Further information can be found in our docs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/usage_and_configuration/index.html&#34;&gt;Usage and Configuration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is already &lt;a href=&#34;https://github.com/psf/black#used-by&#34;&gt;successfully used&lt;/a&gt; by many projects, small and big. &lt;em&gt;Black&lt;/em&gt; has a comprehensive test suite, with efficient parallel tests, and our own auto formatting and parallel Continuous Integration runner. Now that we have become stable, you should not expect large formatting to changes in the future. Stylistic changes will mostly be responses to bug reports and support for new Python syntax. For more information please refer to the &lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/index.html&#34;&gt;The Black Code Style&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Also, as a safety measure which slows down processing, &lt;em&gt;Black&lt;/em&gt; will check that the reformatted code still produces a valid AST that is effectively equivalent to the original (see the &lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#ast-before-and-after-formatting&#34;&gt;Pragmatism&lt;/a&gt; section for details). If you&#39;re feeling confident, use &lt;code&gt;--fast&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The &lt;em&gt;Black&lt;/em&gt; code style&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is a PEP 8 compliant opinionated formatter. &lt;em&gt;Black&lt;/em&gt; reformats entire files in place. Style configuration options are deliberately limited and rarely added. It doesn&#39;t take previous formatting into account (see &lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism&#34;&gt;Pragmatism&lt;/a&gt; for exceptions).&lt;/p&gt; &#xA;&lt;p&gt;Our documentation covers the current &lt;em&gt;Black&lt;/em&gt; code style, but planned changes to it are also documented. They&#39;re both worth taking a look:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html&#34;&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Current style&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/future_style.html&#34;&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Future style&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Changes to the &lt;em&gt;Black&lt;/em&gt; code style are bound by the Stability Policy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/index.html#stability-policy&#34;&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Stability Policy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to this document before submitting an issue. What seems like a bug might be intended behaviour.&lt;/p&gt; &#xA;&lt;h3&gt;Pragmatism&lt;/h3&gt; &#xA;&lt;p&gt;Early versions of &lt;em&gt;Black&lt;/em&gt; used to be absolutist in some respects. They took after its initial author. This was fine at the time as it made the implementation simpler and there were not many users anyway. Not many edge cases were reported. As a mature tool, &lt;em&gt;Black&lt;/em&gt; does make some exceptions to rules it otherwise holds.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism&#34;&gt;The &lt;em&gt;Black&lt;/em&gt; code style: Pragmatism&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to this document before submitting an issue just like with the document above. What seems like a bug might be intended behaviour.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is able to read project-specific default values for its command line options from a &lt;code&gt;pyproject.toml&lt;/code&gt; file. This is especially useful for specifying custom &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt;/&lt;code&gt;--force-exclude&lt;/code&gt;/&lt;code&gt;--extend-exclude&lt;/code&gt; patterns for your project.&lt;/p&gt; &#xA;&lt;p&gt;You can find more details in our documentation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/usage_and_configuration/the_basics.html#configuration-via-a-file&#34;&gt;The basics: Configuration via a file&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And if you&#39;re looking for more general configuration documentation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/usage_and_configuration/index.html&#34;&gt;Usage and Configuration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pro-tip&lt;/strong&gt;: If you&#39;re asking yourself &#34;Do I need to configure anything?&#34; the answer is &#34;No&#34;. &lt;em&gt;Black&lt;/em&gt; is all about sensible defaults. Applying those defaults will have your code in compliance with many other &lt;em&gt;Black&lt;/em&gt; formatted projects.&lt;/p&gt; &#xA;&lt;h2&gt;Used by&lt;/h2&gt; &#xA;&lt;p&gt;The following notable open-source projects trust &lt;em&gt;Black&lt;/em&gt; with enforcing a consistent code style: pytest, tox, Pyramid, Django, Django Channels, Hypothesis, attrs, SQLAlchemy, Poetry, PyPA applications (Warehouse, Bandersnatch, Pipenv, virtualenv), pandas, Pillow, Twisted, LocalStack, every Datadog Agent Integration, Home Assistant, Zulip, Kedro, OpenOA, FLORIS, ORBIT, WOMBAT, and many more.&lt;/p&gt; &#xA;&lt;p&gt;The following organizations use &lt;em&gt;Black&lt;/em&gt;: Facebook, Dropbox, KeepTruckin, Mozilla, Quora, Duolingo, QuantumBlack, Tesla.&lt;/p&gt; &#xA;&lt;p&gt;Are we missing anyone? Let us know.&lt;/p&gt; &#xA;&lt;h2&gt;Testimonials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mike Bayer&lt;/strong&gt;, &lt;a href=&#34;https://www.sqlalchemy.org/&#34;&gt;author of &lt;code&gt;SQLAlchemy&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;I can&#39;t think of any single tool in my entire programming career that has given me a bigger productivity increase by its introduction. I can now do refactorings in about 1% of the keystrokes that it would have taken me previously when we had no way for code to format itself.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dusty Phillips&lt;/strong&gt;, &lt;a href=&#34;https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;amp;field-keywords=dusty+phillips&#34;&gt;writer&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is opinionated so you don&#39;t have to be.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hynek Schlawack&lt;/strong&gt;, &lt;a href=&#34;https://www.attrs.org/&#34;&gt;creator of &lt;code&gt;attrs&lt;/code&gt;&lt;/a&gt;, core developer of Twisted and CPython:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;An auto-formatter that doesn&#39;t suck is all I want for Xmas!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Carl Meyer&lt;/strong&gt;, &lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;Django&lt;/a&gt; core developer:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;At least the name is good.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kenneth Reitz&lt;/strong&gt;, creator of &lt;a href=&#34;https://requests.readthedocs.io/en/latest/&#34;&gt;&lt;code&gt;requests&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://readthedocs.org/projects/pipenv/&#34;&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This vastly improves the formatting of our code. Thanks a ton!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Show your style&lt;/h2&gt; &#xA;&lt;p&gt;Use the badge in your project&#39;s README.md:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using the badge in README.rst:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.. image:: https://img.shields.io/badge/code%20style-black-000000.svg&#xA;    :target: https://github.com/psf/black&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Looks like this: &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Welcome! Happy to see you willing to make the project better. You can get started by reading this:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/latest/contributing/the_basics.html&#34;&gt;Contributing: The basics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also take a look at the rest of the contributing docs or talk with the developers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/latest/contributing/index.html&#34;&gt;Contributing documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/RtVdv86PrH&#34;&gt;Chat on Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Change log&lt;/h2&gt; &#xA;&lt;p&gt;The log has become rather long. It moved to its own file.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://black.readthedocs.io/en/latest/change_log.html&#34;&gt;CHANGES&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;p&gt;The author list is quite long nowadays, so it lives in its own file.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/psf/black/main/AUTHORS.md&#34;&gt;AUTHORS.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;Everyone participating in the &lt;em&gt;Black&lt;/em&gt; project, and in particular in the issue tracker, pull requests, and social media activity, is expected to treat other people with respect and more generally to follow the guidelines articulated in the &lt;a href=&#34;https://www.python.org/psf/codeofconduct/&#34;&gt;Python Community Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, humor is encouraged. In fact, basic familiarity with Monty Python&#39;s Flying Circus is expected. We are not savages.&lt;/p&gt; &#xA;&lt;p&gt;And if you &lt;em&gt;really&lt;/em&gt; need to slap somebody, do it with a fish while dancing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nv-tlabs/GET3D</title>
    <updated>2022-10-09T01:45:44Z</updated>
    <id>tag:github.com,2022-10-09:/nv-tlabs/GET3D</id>
    <link href="https://github.com/nv-tlabs/GET3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images (NeurIPS 2022)&lt;br&gt;&lt;sub&gt;Official PyTorch implementation &lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/docs/assets/get3d_model.png&#34; alt=&#34;Teaser image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;http://www.cs.toronto.edu/~jungao/&#34;&gt;Jun Gao&lt;/a&gt; , &lt;a href=&#34;http://www.cs.toronto.edu/~shenti11/&#34;&gt;Tianchang Shen&lt;/a&gt; , &lt;a href=&#34;http://www.cs.toronto.edu/~zianwang/&#34;&gt;Zian Wang&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~wenzheng/&#34;&gt;Wenzheng Chen&lt;/a&gt;, &lt;a href=&#34;https://kangxue.org/&#34;&gt;Kangxue Yin&lt;/a&gt; , &lt;a href=&#34;https://scholar.google.ca/citations?user=8q2ISMIAAAAJ&amp;amp;hl=en&#34;&gt;Daiqing Li&lt;/a&gt;, &lt;a href=&#34;https://orlitany.github.io/&#34;&gt;Or Litany&lt;/a&gt;, &lt;a href=&#34;https://zgojcic.github.io/&#34;&gt;Zan Gojcic&lt;/a&gt;, &lt;a href=&#34;https://www.cs.toronto.edu/~fidler/&#34;&gt;Sanja Fidler&lt;/a&gt; &lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://nv-tlabs.github.io/GET3D/assets/paper.pdf&#34;&gt;Paper&lt;/a&gt; , &lt;a href=&#34;https://nv-tlabs.github.io/GET3D/&#34;&gt;Project Page&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Abstract: &lt;em&gt;As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/docs/assets/teaser_result.jpg&#34; alt=&#34;Teaser Results&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For business inquiries, please visit our website and submit the form: &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA Research Licensing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2022-09-29: Code released!&lt;/li&gt; &#xA; &lt;li&gt;2022-09-22: Code will be uploaded next week!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We recommend Linux for performance and compatibility reasons.&lt;/li&gt; &#xA; &lt;li&gt;8 high-end NVIDIA GPUs. We have done all testing and development using V100 or A100 GPUs.&lt;/li&gt; &#xA; &lt;li&gt;64-bit Python 3.8 and PyTorch 1.9.0. See &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt; for PyTorch install instructions.&lt;/li&gt; &#xA; &lt;li&gt;CUDA toolkit 11.1 or later. (Why is a separate CUDA toolkit installation required? We use the custom CUDA extensions from the StyleGAN3 repo. Please see &lt;a href=&#34;https://github.com/NVlabs/stylegan3/raw/main/docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary&#34;&gt;Troubleshooting&lt;/a&gt;) .&lt;/li&gt; &#xA; &lt;li&gt;We also recommend to install Nvdiffrast following instructions from &lt;a href=&#34;https://github.com/NVlabs/nvdiffrast&#34;&gt;official repo&lt;/a&gt;, and install &lt;a href=&#34;https://github.com/NVIDIAGameWorks/kaolin&#34;&gt;Kaolin&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/install_get3d.sh&#34;&gt;script&lt;/a&gt; to install packages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Server usage through Docker&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build Docker image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker&#xA;chmod +x make_image.sh&#xA;./make_image.sh get3d:v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start an interactive docker container: &lt;code&gt;docker run --gpus device=all -it --rm -v YOUR_LOCAL_FOLDER:MOUNT_FOLDER -it get3d:v1 bash&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preparing datasets&lt;/h2&gt; &#xA;&lt;p&gt;GET3D is trained on synthetic dataset. We provide rendering scripts for Shapenet. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/render_shapenet_data/README.md&#34;&gt;readme&lt;/a&gt; to download shapenet dataset and render it.&lt;/p&gt; &#xA;&lt;h2&gt;Train the model&lt;/h2&gt; &#xA;&lt;h4&gt;Clone the gitlab code and necessary files:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd YOUR_CODE_PARH&#xA;git clone git@github.com:nv-tlabs/GET3D.git&#xA;cd GET3D; mkdir cache; cd cache&#xA;wget https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Train the model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd YOUR_CODE_PATH &#xA;export PYTHONPATH=$PWD:$PYTHONPATH&#xA;export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train on the unified generator on cars, motorbikes or chair (Improved generator in Appendix):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0&#xA;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=80 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0&#xA;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=400 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If want to train on seperate generators (main Figure in the paper):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 0&#xA;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=80 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 0&#xA;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=3200 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If want to debug the model first, reduce the number of gpus to 1 and batch size to 4 via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--gpus=1 --batch=4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Inference on a pretrained model for visualization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference could operate on a single GPU with 16 GB memory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_3d.py --outdir=save_inference_results/shapenet_car  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH&#xA;python train_3d.py --outdir=save_inference_results/shapenet_chair  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH&#xA;python train_3d.py --outdir=save_inference_results/shapenet_motorbike  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To generate mesh with textures, add one option to the inference command: &lt;code&gt;--inference_to_generate_textured_mesh 1&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To generate the results with latent code interpolation, add one option to the inference command: &lt;code&gt;--inference_save_interpolation 1&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evluation metrics&lt;/h3&gt; &#xA;&lt;h5&gt;Compute FID&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To evaluate the model with FID metric, add one option to the inference command: &lt;code&gt;--inference_compute_fid 1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Compute COV &amp;amp; MMD scores for LFD &amp;amp; CD&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First generate 3D objects for evaluation, add one option to the inference command: &lt;code&gt;--inference_generate_geo 1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Following &lt;a href=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/evaluation_scripts/README.md&#34;&gt;README&lt;/a&gt; to compute metrics.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright © 2022, NVIDIA Corporation &amp;amp; affiliates. All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;This work is made available under the &lt;a href=&#34;https://github.com/nv-tlabs/GET3D/raw/master/LICENSE.txt&#34;&gt;Nvidia Source Code License&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;h2&gt;Broader Information&lt;/h2&gt; &#xA;&lt;p&gt;GET3D builds upon several previous works:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nv-tlabs.github.io/DefTet/&#34;&gt;Learning Deformable Tetrahedral Meshes for 3D Reconstruction (NeurIPS 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nv-tlabs.github.io/DMTet/&#34;&gt;Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis (NeurIPS 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nvlabs.github.io/nvdiffrec/&#34;&gt;Extracting Triangular 3D Models, Materials, and Lighting From Images (CVPR 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nv-tlabs.github.io/DIBRPlus/&#34;&gt;DIB-R++: Learning to Predict Lighting and Material with a Hybrid Differentiable Renderer (NeurIPS 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nvlabs.github.io/nvdiffrast/&#34;&gt;Nvdiffrast – Modular Primitives for High-Performance Differentiable Rendering (SIGRAPH Asia 2020)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@inproceedings{gao2022get3d,&#xA;title={GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images},&#xA;author={Jun Gao and Tianchang Shen and Zian Wang and Wenzheng Chen and Kangxue Yin&#xA;and Daiqing Li and Or Litany and Zan Gojcic and Sanja Fidler},&#xA;booktitle={Advances In Neural Information Processing Systems},&#xA;year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>