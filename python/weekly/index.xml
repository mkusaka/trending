<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-16T02:02:50Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yoheinakajima/babyagi</title>
    <updated>2023-04-16T02:02:50Z</updated>
    <id>tag:github.com,2023-04-16:/yoheinakajima/babyagi</id>
    <link href="https://github.com/yoheinakajima/babyagi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Translations:&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-fr.md&#34;&gt;&lt;img title=&#34;Français&#34; alt=&#34;Français&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/fr.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-pt-br.md&#34;&gt;&lt;img title=&#34;Portuguese&#34; alt=&#34;Portuguese&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/br.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-ro.md&#34;&gt;&lt;img title=&#34;Romanian&#34; alt=&#34;Romanian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ro.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-ru.md&#34;&gt;&lt;img title=&#34;Russian&#34; alt=&#34;Russian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ru.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-si.md&#34;&gt;&lt;img title=&#34;Slovenian&#34; alt=&#34;Slovenian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/si.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-es.md&#34;&gt;&lt;img title=&#34;Spanish&#34; alt=&#34;Spanish&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/es.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-tr.md&#34;&gt;&lt;img title=&#34;Turkish&#34; alt=&#34;Turkish&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/tr.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-ua.md&#34;&gt;&lt;img title=&#34;Ukrainian&#34; alt=&#34;Ukrainian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ua.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-cn.md&#34;&gt;&lt;img title=&#34;简体中文&#34; alt=&#34;Simplified Chinese&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/cn.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-zh-tw.md&#34;&gt;&lt;img title=&#34;繁體中文 (Traditional Chinese)&#34; alt=&#34;繁體中文 (Traditional Chinese)&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/tw.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Objective&lt;/h1&gt; &#xA;&lt;p&gt;This Python script is an example of an AI-powered task management system. The system uses OpenAI and Pinecone APIs to create, prioritize, and execute tasks. The main idea behind this system is that it creates tasks based on the result of previous tasks and a predefined objective. The script then uses OpenAI&#39;s natural language processing (NLP) capabilities to create new tasks based on the objective, and Pinecone to store and retrieve task results for context. This is a pared-down version of the original &lt;a href=&#34;https://twitter.com/yoheinakajima/status/1640934493489070080?s=20&#34;&gt;Task-Driven Autonomous Agent&lt;/a&gt; (Mar 28, 2023).&lt;/p&gt; &#xA;&lt;p&gt;This README will cover the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#how-it-works&#34;&gt;How the script works&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#how-to-use&#34;&gt;How to use the script&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#continous-script-warning&#34;&gt;Warning about running the script continuously&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How It Works&lt;a name=&#34;how-it-works&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;The script works by running an infinite loop that does the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pulls the first task from the task list.&lt;/li&gt; &#xA; &lt;li&gt;Sends the task to the execution agent, which uses OpenAI&#39;s API to complete the task based on the context.&lt;/li&gt; &#xA; &lt;li&gt;Enriches the result and stores it in Pinecone.&lt;/li&gt; &#xA; &lt;li&gt;Creates new tasks and reprioritizes the task list based on the objective and the result of the previous task. &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The execution_agent() function is where the OpenAI API is used. It takes two parameters: the objective and the task. It then sends a prompt to OpenAI&#39;s API, which returns the result of the task. The prompt consists of a description of the AI system&#39;s task, the objective, and the task itself. The result is then returned as a string. &lt;br&gt; The task_creation_agent() function is where OpenAI&#39;s API is used to create new tasks based on the objective and the result of the previous task. The function takes four parameters: the objective, the result of the previous task, the task description, and the current task list. It then sends a prompt to OpenAI&#39;s API, which returns a list of new tasks as strings. The function then returns the new tasks as a list of dictionaries, where each dictionary contains the name of the task. &lt;br&gt; The prioritization_agent() function is where OpenAI&#39;s API is used to reprioritize the task list. The function takes one parameter, the ID of the current task. It sends a prompt to OpenAI&#39;s API, which returns the reprioritized task list as a numbered list.&lt;/p&gt; &#xA;&lt;p&gt;Finally, the script uses Pinecone to store and retrieve task results for context. The script creates a Pinecone index based on the table name specified in the YOUR_TABLE_NAME variable. Pinecone is then used to store the results of the task in the index, along with the task name and any additional metadata.&lt;/p&gt; &#xA;&lt;h1&gt;How to Use&lt;a name=&#34;how-to-use&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;To use the script, you will need to follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository via &lt;code&gt;git clone https://github.com/yoheinakajima/babyagi.git&lt;/code&gt; and &lt;code&gt;cd&lt;/code&gt; into the cloned repository.&lt;/li&gt; &#xA; &lt;li&gt;Install the required packages: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy the .env.example file to .env: &lt;code&gt;cp .env.example .env&lt;/code&gt;. This is where you will set the following variables.&lt;/li&gt; &#xA; &lt;li&gt;Set your OpenAI and Pinecone API keys in the OPENAI_API_KEY, OPENAPI_API_MODEL, and PINECONE_API_KEY variables.&lt;/li&gt; &#xA; &lt;li&gt;Set the Pinecone environment in the PINECONE_ENVIRONMENT variable.&lt;/li&gt; &#xA; &lt;li&gt;Set the name of the table where the task results will be stored in the TABLE_NAME variable.&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Set the objective of the task management system in the OBJECTIVE variable.&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Set the first task of the system in the INITIAL_TASK variable.&lt;/li&gt; &#xA; &lt;li&gt;Run the script.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;All optional values above can also be specified on the command line.&lt;/p&gt; &#xA;&lt;h1&gt;Running inside a docker container&lt;/h1&gt; &#xA;&lt;p&gt;As a prerequisite, you will need docker and docker-compose installed. Docker desktop is the simplest option &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run the system inside a docker container, setup your .env file as per steps above and then run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Supported Models&lt;a name=&#34;supported-models&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This script works with all OpenAI models, as well as Llama through Llama.cpp. Default model is &lt;strong&gt;gpt-3.5-turbo&lt;/strong&gt;. To use a different model, specify it through OPENAI_API_MODEL or use the command line.&lt;/p&gt; &#xA;&lt;h2&gt;Llama&lt;/h2&gt; &#xA;&lt;p&gt;Download the latest version of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;Llama.cpp&lt;/a&gt; and follow instructions to make it. You will also need the Llama model weights.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Under no circumstances share IPFS, magnet links, or any other links to model downloads anywhere in this repository, including in issues, discussions or pull requests. They will be immediately deleted.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After that link &lt;code&gt;llama/main&lt;/code&gt; to llama.cpp/main and &lt;code&gt;models&lt;/code&gt; to the folder where you have the Llama model weights. Then run the script with &lt;code&gt;OPENAI_API_MODEL=llama&lt;/code&gt; or &lt;code&gt;-l&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;h1&gt;Warning&lt;a name=&#34;continous-script-warning&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This script is designed to be run continuously as part of a task management system. Running this script continuously can result in high API usage, so please use it responsibly. Additionally, the script requires the OpenAI and Pinecone APIs to be set up correctly, so make sure you have set up the APIs before running the script.&lt;/p&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Needless to say, BabyAGI is still in its infancy and thus we are still determining its direction and the steps to get there. Currently, a key design goal for BabyAGI is to be &lt;em&gt;simple&lt;/em&gt; such that it&#39;s easy to understand and build upon. To maintain this simplicity, we kindly request that you adhere to the following guidelines when submitting PRs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Focus on small, modular modifications rather than extensive refactoring.&lt;/li&gt; &#xA; &lt;li&gt;When introducing new features, provide a detailed description of the specific use case you are addressing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A note from @yoheinakajima (Apr 5th, 2023):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;I know there are a growing number of PRs, appreciate your patience - as I am both new to GitHub/OpenSource, and did not plan my time availability accordingly this week. Re:direction, I&#39;ve been torn on keeping it simple vs expanding - currently leaning towards keeping a core Baby AGI simple, and using this as a platform to support and promote different approaches to expanding this (eg. BabyAGIxLangchain as one direction). I believe there are various opinionated approaches that are worth exploring, and I see value in having a central place to compare and discuss. More updates coming shortly.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;I am new to GitHub and open source, so please be patient as I learn to manage this project properly. I run a VC firm by day, so I will generally be checking PRs and issues at night after I get my kids down - which may not be every night. Open to the idea of bringing in support, will be updating this section soon (expectations, visions, etc). Talking to lots of people and learning - hang tight for updates!&lt;/p&gt; &#xA;&lt;h1&gt;Backstory&lt;/h1&gt; &#xA;&lt;p&gt;BabyAGI is a pared-down version of the original &lt;a href=&#34;https://twitter.com/yoheinakajima/status/1640934493489070080?s=20&#34;&gt;Task-Driven Autonomous Agent&lt;/a&gt; (Mar 28, 2023) shared on Twitter. This version is down to 140 lines: 13 comments, 22 blanks, and 105 code. The name of the repo came up in the reaction to the original autonomous agent - the author does not mean to imply that this is AGI.&lt;/p&gt; &#xA;&lt;p&gt;Made with love by &lt;a href=&#34;https://twitter.com/yoheinakajima&#34;&gt;@yoheinakajima&lt;/a&gt;, who happens to be a VC (would love to see what you&#39;re building!)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chroma-core/chroma</title>
    <updated>2023-04-16T02:02:50Z</updated>
    <id>tag:github.com,2023-04-16:/chroma-core/chroma</id>
    <link href="https://github.com/chroma-core/chroma" rel="alternate"></link>
    <summary type="html">&lt;p&gt;the AI-native open-source embedding database&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trychroma.com&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png&#34; alt=&#34;Chroma logo&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Chroma - the open-source embedding database&lt;/b&gt;. &lt;br&gt; The fastest way to build Python or JavaScript LLM apps with memory! &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/MMeYNTmh3x&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/1073293645303795742&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; | &lt;a href=&#34;https://github.com/chroma-core/chroma/raw/master/LICENSE&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=license&amp;amp;message=Apache 2.0&amp;amp;color=white&#34; alt=&#34;License&#34;&gt; &lt;/a&gt; | &lt;a href=&#34;https://docs.trychroma.com/&#34; target=&#34;_blank&#34;&gt; Docs &lt;/a&gt; | &lt;a href=&#34;https://www.trychroma.com/&#34; target=&#34;_blank&#34;&gt; Homepage &lt;/a&gt; &lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install chromadb # python client&#xA;# for javascript, npm install chromadb!&#xA;# for client-server mode, docker-compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The core API is only 4 functions (run our &lt;a href=&#34;https://colab.research.google.com/drive/1QEzFyqnoFxq7LUGyP1vzR4iLt9PpCDXv?usp=sharing&#34;&gt;💡 Google Colab&lt;/a&gt; or &lt;a href=&#34;https://replit.com/@swyx/BasicChromaStarter?v=1&#34;&gt;Replit template&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import chromadb&#xA;# setup Chroma in-memory, for easy prototyping. Can add persistence easily!&#xA;client = chromadb.Client()&#xA;&#xA;# Create collection. get_collection, get_or_create_collection, delete_collection also available!&#xA;collection = client.create_collection(&#34;all-my-documents&#34;) &#xA;&#xA;# Add docs to the collection. Can also update and delete. Row-based API coming soon!&#xA;collection.add(&#xA;    documents=[&#34;This is document1&#34;, &#34;This is document2&#34;], # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well&#xA;    metadatas=[{&#34;source&#34;: &#34;notion&#34;}, {&#34;source&#34;: &#34;google-docs&#34;}], # filter on these!&#xA;    ids=[&#34;doc1&#34;, &#34;doc2&#34;], # unique for each doc &#xA;)&#xA;&#xA;# Query/search 2 most similar results. You can also .get by id&#xA;results = collection.query(&#xA;    query_texts=[&#34;This is a query document&#34;],&#xA;    n_results=2,&#xA;    # where={&#34;metadata_field&#34;: &#34;is_equal_to_this&#34;}, # optional filter&#xA;    # where_document={&#34;$contains&#34;:&#34;search_string&#34;}  # optional filter&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: Fully-typed, fully-tested, fully-documented == happiness&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt;: &lt;a href=&#34;https://blog.langchain.dev/langchain-chroma/&#34;&gt;&lt;code&gt;🦜️🔗 LangChain&lt;/code&gt;&lt;/a&gt; (python and js), &lt;a href=&#34;https://twitter.com/atroyn/status/1628557389762007040&#34;&gt;&lt;code&gt;🦙 LlamaIndex&lt;/code&gt;&lt;/a&gt; and more soon&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dev, Test, Prod&lt;/strong&gt;: the same API that runs in your python notebook, scales to your cluster&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feature-rich&lt;/strong&gt;: Queries, filtering, density estimation and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Free &amp;amp; Open Source&lt;/strong&gt;: Apache 2.0 Licensed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Use case: ChatGPT for ______&lt;/h2&gt; &#xA;&lt;p&gt;For example, the &lt;code&gt;&#34;Chat your data&#34;&lt;/code&gt; use case:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add documents to your database. You can pass in your own embeddings, embedding function, or let Chroma embed them for you.&lt;/li&gt; &#xA; &lt;li&gt;Query relevant documents with natural language.&lt;/li&gt; &#xA; &lt;li&gt;Compose documents into the context window of an LLM like &lt;code&gt;GPT3&lt;/code&gt; for additional summarization or analysis.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Embeddings?&lt;/h2&gt; &#xA;&lt;p&gt;What are embeddings?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings/what-are-embeddings&#34;&gt;Read the guide from OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Literal&lt;/strong&gt;: Embedding something turns it from image/text/audio into a list of numbers. 🖼️ or 📄 =&amp;gt; &lt;code&gt;[1.2, 2.1, ....]&lt;/code&gt;. This process makes documents &#34;understandable&#34; to a machine learning model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;By analogy&lt;/strong&gt;: An embedding represents the essence of a document. This enables documents and queries with the same essence to be &#34;near&#34; each other and therefore easy to find.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Technical&lt;/strong&gt;: An embedding is the latent-space position of a document at a layer of a deep neural network. For models trained specifically to embed data, this is the last layer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A small example&lt;/strong&gt;: If you search your photos for &#34;famous bridge in San Francisco&#34;. By embedding this query and comparing it to the embeddings of your photos and their metadata - it should return photos of the Golden Gate Bridge.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Embeddings databases (also known as &lt;strong&gt;vector databases&lt;/strong&gt;) store embeddings and allow you to search by nearest neighbors rather than by substrings like a traditional database. By default, Chroma uses &lt;a href=&#34;https://docs.trychroma.com/embeddings#default-sentence-transformers&#34;&gt;Sentence Transformers&lt;/a&gt; to embed for you but you can also use OpenAI embeddings, Cohere (multilingual) embeddings, or your own.&lt;/p&gt; &#xA;&lt;h2&gt;Get involved&lt;/h2&gt; &#xA;&lt;p&gt;Chroma is a rapidly developing project. We welcome PR contributors and ideas for how to improve the project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/MMeYNTmh3x&#34;&gt;Join the conversation on Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chroma-core/chroma/wiki/Roadmap&#34;&gt;Review the roadmap and contribute your ideas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chroma-core/chroma/issues&#34;&gt;Grab an issue and open a PR&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chroma-core/chroma/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TabbyML/tabby</title>
    <updated>2023-04-16T02:02:50Z</updated>
    <id>tag:github.com,2023-04-16:/TabbyML/tabby</id>
    <link href="https://github.com/TabbyML/tabby" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Self-hosted AI coding assistant&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;🐾 Tabby&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TabbyML/tabby/actions/workflows/docker.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/TabbyML/tabby/docker.yml?label=docker%20image%20build&#34; alt=&#34;Docker build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/tabbyml/tabby&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/tabbyml/tabby&#34; alt=&#34;Docker pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/388154/229353706-230d70e1-7d09-48e2-a884-4da768bccf6f.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Self-hosted AI coding assistant. An opensource / on-prem alternative to GitHub Copilot.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; Tabby is still in the alpha phase&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Self-contained, with no need for a DBMS or cloud service&lt;/li&gt; &#xA; &lt;li&gt;Web UI for visualizing and configuration models and MLOps.&lt;/li&gt; &#xA; &lt;li&gt;OpenAPI interface, easy to integrate with existing infrastructure (e.g Cloud IDE).&lt;/li&gt; &#xA; &lt;li&gt;Consumer level GPU supports (FP-16 weight loading with various optimization).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/spaces/TabbyML/tabby&#34;&gt;&lt;img alt=&#34;Open in Spaces&#34; src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-md.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Demo&#34; src=&#34;https://user-images.githubusercontent.com/388154/230440226-9bc01d05-9f57-478b-b04d-81184eba14ca.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Get started: Server&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Tabby requires &lt;a href=&#34;https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/&#34;&gt;Pascal or newer&lt;/a&gt; NVIDIA GPU.&lt;/p&gt; &#xA;&lt;p&gt;Before running Tabby, ensure the installation of the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;. We suggest using NVIDIA drivers that are compatible with CUDA version 11.8 or higher.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create data dir and grant owner to 1000 (Tabby run as uid 1000 in container)&#xA;mkdir -p data/hf_cache &amp;amp;&amp;amp; chown -R 1000 data&#xA;&#xA;docker run \&#xA;  --gpus all \&#xA;  -it --rm \&#xA;  -v &#34;/$(pwd)/data:/data&#34; \&#xA;  -v &#34;/$(pwd)/data/hf_cache:/home/app/.cache/huggingface&#34; \&#xA;  -p 5000:5000 \&#xA;  -e MODEL_NAME=TabbyML/J-350M \&#xA;  -e MODEL_BACKEND=triton \&#xA;  --name=tabby \&#xA;  tabbyml/tabby&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then query the server using &lt;code&gt;/v1/completions&lt;/code&gt; endpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST http://localhost:5000/v1/completions -H &#39;Content-Type: application/json&#39; --data &#39;{&#xA;    &#34;prompt&#34;: &#34;def binarySearch(arr, left, right, x):\n    mid = (left +&#34;&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provides an interactive playground in admin panel &lt;a href=&#34;http://localhost:5000/_admin&#34;&gt;localhost:5000/_admin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Skypilot&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/TabbyML/tabby/main/deployment/skypilot/README.md&#34;&gt;deployment/skypilot/README.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started: Client&lt;/h2&gt; &#xA;&lt;p&gt;We offer multiple methods to connect to Tabby Server, including using OpenAPI and editor extensions.&lt;/p&gt; &#xA;&lt;h3&gt;API&lt;/h3&gt; &#xA;&lt;p&gt;Tabby has opened a FastAPI server at &lt;a href=&#34;https://localhost:5000&#34;&gt;localhost:5000&lt;/a&gt;, which includes an OpenAPI documentation of the HTTP API. The same API documentation is also hosted at &lt;a href=&#34;https://tabbyml.github.io/tabby&#34;&gt;https://tabbyml.github.io/tabby&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Editor Extensions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TabbyML/tabby/main/clients/vscode&#34;&gt;VSCode Extension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TabbyML/tabby/main/clients/vim&#34;&gt;VIM Extension&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Go to &lt;code&gt;development&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make dev-triton # Turn on triton backend (for cuda env developers)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>