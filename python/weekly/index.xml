<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-12T01:42:58Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>VikParuchuri/surya</title>
    <updated>2024-05-12T01:42:58Z</updated>
    <id>tag:github.com,2024-05-12:/VikParuchuri/surya</id>
    <link href="https://github.com/VikParuchuri/surya" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OCR, layout analysis, reading order, line detection in 90+ languages&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Surya&lt;/h1&gt; &#xA;&lt;p&gt;Surya is a document OCR toolkit that does:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OCR in 90+ languages that benchmarks favorably vs cloud services&lt;/li&gt; &#xA; &lt;li&gt;Line-level text detection in any language&lt;/li&gt; &#xA; &lt;li&gt;Layout analysis (table, image, header, etc detection)&lt;/li&gt; &#xA; &lt;li&gt;Reading order detection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It works on a range of documents (see &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/#usage&#34;&gt;usage&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/#benchmarks&#34;&gt;benchmarks&lt;/a&gt; for more details).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Detection&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;OCR&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt.png&#34; alt=&#34;New York Times Article Detection&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt_text.png&#34; alt=&#34;New York Times Article Recognition&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Layout&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reading Order&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt_layout.png&#34; alt=&#34;New York Times Article Layout&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt_reading.jpg&#34; alt=&#34;New York Times Article Reading Order&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Surya is named for the &lt;a href=&#34;https://en.wikipedia.org/wiki/Surya&#34;&gt;Hindu sun god&lt;/a&gt;, who has universal vision.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg//KuZwXNGnfH&#34;&gt;Discord&lt;/a&gt; is where we discuss future development.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Detection&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;OCR&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Layout&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Order&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Japanese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hindi&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arabic&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/arabic.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/arabic_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/arabic_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/arabic_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese + Hindi&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chi_hind.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chi_hind_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chi_hind_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chi_hind_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Presentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scientific Paper&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scanned Document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;New York Times&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt_order.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scanned Form&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Textbook&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/textbook.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/textbook_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/textbook_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/textbook_order.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Commercial usage&lt;/h1&gt; &#xA;&lt;p&gt;I want surya to be as widely accessible as possible, while still funding my development/training costs. Research and personal usage is always okay, but there are some restrictions on commercial usage.&lt;/p&gt; &#xA;&lt;p&gt;The weights for the models are licensed &lt;code&gt;cc-by-nc-sa-4.0&lt;/code&gt;, but I will waive that for any organization under $5M USD in gross revenue in the most recent 12-month period AND under $5M in lifetime VC/angel funding raised. If you want to remove the GPL license requirements (dual-license) and/or use the weights commercially over the revenue limit, check out the options &lt;a href=&#34;https://www.datalab.to&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;You&#39;ll need python 3.9+ and PyTorch. You may need to install the CPU version of torch first if you&#39;re not using a Mac or a GPU machine. See &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install surya-ocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weights will automatically download the first time you run surya. Note that this does not work with the latest version of transformers &lt;code&gt;4.37+&lt;/code&gt; &lt;a href=&#34;https://github.com/huggingface/transformers/issues/28846#issuecomment-1926109135&#34;&gt;yet&lt;/a&gt;, so you will need to keep &lt;code&gt;4.36.2&lt;/code&gt;, which is installed with surya.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspect the settings in &lt;code&gt;surya/settings.py&lt;/code&gt;. You can override any settings with environment variables.&lt;/li&gt; &#xA; &lt;li&gt;Your torch device will be automatically detected, but you can override this. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt;. For text detection, the &lt;code&gt;mps&lt;/code&gt; device has a bug (on the &lt;a href=&#34;https://github.com/pytorch/pytorch/issues/84936&#34;&gt;Apple side&lt;/a&gt;) that may prevent it from working properly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Interactive App&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;ve included a streamlit app that lets you interactively try Surya on images or PDF files. Run it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install streamlit&#xA;surya_gui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pass the &lt;code&gt;--math&lt;/code&gt; command line argument to use the math text detection model instead of the default model. This will detect math better, but will be worse at everything else.&lt;/p&gt; &#xA;&lt;h2&gt;OCR (text recognition)&lt;/h2&gt; &#xA;&lt;p&gt;This command will write out a json file with the detected text and bboxes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;surya_ocr DATA_PATH --images --langs hi,en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--langs&lt;/code&gt; specifies the language(s) to use for OCR. You can comma separate multiple languages (I don&#39;t recommend using more than &lt;code&gt;4&lt;/code&gt;). Use the language name or two-letter ISO code from &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes&#34;&gt;here&lt;/a&gt;. Surya supports the 90+ languages found in &lt;code&gt;surya/languages.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--lang_file&lt;/code&gt; if you want to use a different language for different PDFs/images, you can specify languages here. The format is a JSON dict with the keys being filenames and the values as a list, like &lt;code&gt;{&#34;file1.pdf&#34;: [&#34;en&#34;, &#34;hi&#34;], &#34;file2.pdf&#34;: [&#34;en&#34;]}&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--start_page&lt;/code&gt; specifies the page number to start processing from&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;text_lines&lt;/code&gt; - the detected text and bounding boxes for each line &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;text&lt;/code&gt; - the text in the line&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;confidence&lt;/code&gt; - the confidence of the model in the detected text (0-1)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;polygon&lt;/code&gt; - the polygon for the text line in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;languages&lt;/code&gt; - the languages specified for the page&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page&lt;/code&gt; - the page number in the file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_bbox&lt;/code&gt; - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;RECOGNITION_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;50MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;256&lt;/code&gt;, which will use about 12.8GB of VRAM. Depending on your CPU core count, it may help, too - the default CPU batch size is &lt;code&gt;32&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from surya.ocr import run_ocr&#xA;from surya.model.detection import segformer&#xA;from surya.model.recognition.model import load_model&#xA;from surya.model.recognition.processor import load_processor&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;langs = [&#34;en&#34;] # Replace with your languages&#xA;det_processor, det_model = segformer.load_processor(), segformer.load_model()&#xA;rec_model, rec_processor = load_model(), load_processor()&#xA;&#xA;predictions = run_ocr([image], [langs], det_model, det_processor, rec_model, rec_processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text line detection&lt;/h2&gt; &#xA;&lt;p&gt;This command will write out a json file with the detected bboxes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;surya_detect DATA_PATH --images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--math&lt;/code&gt; uses a specialized math detection model instead of the default model. This will be better at math, but worse at everything else.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bboxes&lt;/code&gt; - detected bounding boxes for text &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;polygon&lt;/code&gt; - the polygon for the text line in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;confidence&lt;/code&gt; - the confidence of the model in the detected text (0-1)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vertical_lines&lt;/code&gt; - vertical lines detected in the document &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned line coordinates.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;horizontal_lines&lt;/code&gt; - horizontal lines detected in the document &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned line coordinates.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page&lt;/code&gt; - the page number in the file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_bbox&lt;/code&gt; - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;DETECTOR_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;280MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;32&lt;/code&gt;, which will use about 9GB of VRAM. Depending on your CPU core count, it might help, too - the default CPU batch size is &lt;code&gt;2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from surya.detection import batch_text_detection&#xA;from surya.model.detection.segformer import load_model, load_processor&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;model, processor = load_model(), load_processor()&#xA;&#xA;# predictions is a list of dicts, one per image&#xA;predictions = batch_text_detection([image], model, processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Layout analysis&lt;/h2&gt; &#xA;&lt;p&gt;This command will write out a json file with the detected layout.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;surya_layout DATA_PATH --images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bboxes&lt;/code&gt; - detected bounding boxes for text &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;polygon&lt;/code&gt; - the polygon for the text line in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;confidence&lt;/code&gt; - the confidence of the model in the detected text (0-1). This is currently not very reliable.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;label&lt;/code&gt; - the label for the bbox. One of &lt;code&gt;Caption&lt;/code&gt;, &lt;code&gt;Footnote&lt;/code&gt;, &lt;code&gt;Formula&lt;/code&gt;, &lt;code&gt;List-item&lt;/code&gt;, &lt;code&gt;Page-footer&lt;/code&gt;, &lt;code&gt;Page-header&lt;/code&gt;, &lt;code&gt;Picture&lt;/code&gt;, &lt;code&gt;Figure&lt;/code&gt;, &lt;code&gt;Section-header&lt;/code&gt;, &lt;code&gt;Table&lt;/code&gt;, &lt;code&gt;Text&lt;/code&gt;, &lt;code&gt;Title&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page&lt;/code&gt; - the page number in the file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_bbox&lt;/code&gt; - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;DETECTOR_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;280MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;32&lt;/code&gt;, which will use about 9GB of VRAM. Depending on your CPU core count, it might help, too - the default CPU batch size is &lt;code&gt;2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from surya.detection import batch_text_detection&#xA;from surya.layout import batch_layout_detection&#xA;from surya.model.detection.segformer import load_model, load_processor&#xA;from surya.settings import settings&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;model = load_model(checkpoint=settings.LAYOUT_MODEL_CHECKPOINT)&#xA;processor = load_processor(checkpoint=settings.LAYOUT_MODEL_CHECKPOINT)&#xA;det_model = load_model()&#xA;det_processor = load_processor()&#xA;&#xA;# layout_predictions is a list of dicts, one per image&#xA;line_predictions = batch_text_detection([image], det_model, det_processor)&#xA;layout_predictions = batch_layout_detection([image], model, processor, line_predictions)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reading order&lt;/h2&gt; &#xA;&lt;p&gt;This command will write out a json file with the detected reading order and layout.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;surya_order DATA_PATH --images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bboxes&lt;/code&gt; - detected bounding boxes for text &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;position&lt;/code&gt; - the position in the reading order of the bbox, starting from 0.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;label&lt;/code&gt; - the label for the bbox. See the layout section of the documentation for a list of potential labels.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page&lt;/code&gt; - the page number in the file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_bbox&lt;/code&gt; - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;ORDER_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;360MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;32&lt;/code&gt;, which will use about 11GB of VRAM. Depending on your CPU core count, it might help, too - the default CPU batch size is &lt;code&gt;4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from surya.ordering import batch_ordering&#xA;from surya.model.ordering.processor import load_processor&#xA;from surya.model.ordering.model import load_model&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;# bboxes should be a list of lists with layout bboxes for the image in [x1,y1,x2,y2] format&#xA;# You can get this from the layout model, see above for usage&#xA;bboxes = [bbox1, bbox2, ...]&#xA;&#xA;model = load_model()&#xA;processor = load_processor()&#xA;&#xA;# order_predictions will be a list of dicts, one per image&#xA;order_predictions = batch_ordering([image], [bboxes], model, processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is specialized for document OCR. It will likely not work on photos or other images.&lt;/li&gt; &#xA; &lt;li&gt;It is for printed text, not handwriting (though it may work on some handwriting).&lt;/li&gt; &#xA; &lt;li&gt;The text detection model has trained itself to ignore advertisements.&lt;/li&gt; &#xA; &lt;li&gt;You can find language support for OCR in &lt;code&gt;surya/languages.py&lt;/code&gt;. Text detection, layout analysis, and reading order will work with any language.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;If OCR isn&#39;t working properly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try increasing resolution of the image so the text is bigger. If the resolution is already very high, try decreasing it to no more than a &lt;code&gt;2048px&lt;/code&gt; width.&lt;/li&gt; &#xA; &lt;li&gt;Preprocessing the image (binarizing, deskewing, etc) can help with very old/blurry images.&lt;/li&gt; &#xA; &lt;li&gt;You can adjust &lt;code&gt;DETECTOR_BLANK_THRESHOLD&lt;/code&gt; and &lt;code&gt;DETECTOR_TEXT_THRESHOLD&lt;/code&gt; if you don&#39;t get good results. &lt;code&gt;DETECTOR_BLANK_THRESHOLD&lt;/code&gt; controls the space between lines - any prediction below this number will be considered blank space. &lt;code&gt;DETECTOR_TEXT_THRESHOLD&lt;/code&gt; controls how text is joined - any number above this is considered text. &lt;code&gt;DETECTOR_TEXT_THRESHOLD&lt;/code&gt; should always be higher than &lt;code&gt;DETECTOR_BLANK_THRESHOLD&lt;/code&gt;, and both should be in the 0-1 range. Looking at the heatmap from the debug output of the detector can tell you how to adjust these (if you see faint things that look like boxes, lower the thresholds, and if you see bboxes being joined together, raise the thresholds).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Manual install&lt;/h1&gt; &#xA;&lt;p&gt;If you want to develop surya, you can install it manually:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/VikParuchuri/surya.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd surya&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry install&lt;/code&gt; - installs main and dev dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry shell&lt;/code&gt; - activates the virtual environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;h2&gt;OCR&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_rec_chart.png&#34; alt=&#34;Benchmark chart tesseract&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Time per page (s)&lt;/th&gt; &#xA;   &lt;th&gt;Avg similarity (⬆)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;surya&lt;/td&gt; &#xA;   &lt;td&gt;.62&lt;/td&gt; &#xA;   &lt;td&gt;0.97&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tesseract&lt;/td&gt; &#xA;   &lt;td&gt;.45&lt;/td&gt; &#xA;   &lt;td&gt;0.88&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/rec_acc_table.png&#34;&gt;Full language results&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tesseract is CPU-based, and surya is CPU or GPU. I tried to cost-match the resources used, so I used a 1xA6000 (48GB VRAM) for surya, and 28 CPU cores for Tesseract (same price on Lambda Labs/DigitalOcean).&lt;/p&gt; &#xA;&lt;h3&gt;Google Cloud Vision&lt;/h3&gt; &#xA;&lt;p&gt;I benchmarked OCR against Google Cloud vision since it has similar language coverage to Surya.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/gcloud_rec_bench.png&#34; alt=&#34;Benchmark chart google cloud&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/gcloud_full_langs.png&#34;&gt;Full language results&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I measured normalized sentence similarity (0-1, higher is better) based on a set of real-world and synthetic pdfs. I sampled PDFs from common crawl, then filtered out the ones with bad OCR. I couldn&#39;t find PDFs for some languages, so I also generated simple synthetic PDFs for those.&lt;/p&gt; &#xA;&lt;p&gt;I used the reference line bboxes from the PDFs with both tesseract and surya, to just evaluate the OCR quality.&lt;/p&gt; &#xA;&lt;p&gt;For Google Cloud, I aligned the output from Google Cloud with the ground truth. I had to skip RTL languages since they didn&#39;t align well.&lt;/p&gt; &#xA;&lt;h2&gt;Text line detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_chart_small.png&#34; alt=&#34;Benchmark chart&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Time (s)&lt;/th&gt; &#xA;   &lt;th&gt;Time per page (s)&lt;/th&gt; &#xA;   &lt;th&gt;precision&lt;/th&gt; &#xA;   &lt;th&gt;recall&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;surya&lt;/td&gt; &#xA;   &lt;td&gt;52.6892&lt;/td&gt; &#xA;   &lt;td&gt;0.205817&lt;/td&gt; &#xA;   &lt;td&gt;0.844426&lt;/td&gt; &#xA;   &lt;td&gt;0.937818&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tesseract&lt;/td&gt; &#xA;   &lt;td&gt;74.4546&lt;/td&gt; &#xA;   &lt;td&gt;0.290838&lt;/td&gt; &#xA;   &lt;td&gt;0.631498&lt;/td&gt; &#xA;   &lt;td&gt;0.997694&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Tesseract is CPU-based, and surya is CPU or GPU. I ran the benchmarks on a system with an A6000 GPU, and a 32 core CPU. This was the resource usage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;tesseract - 32 CPU cores, or 8 workers using 4 cores each&lt;/li&gt; &#xA; &lt;li&gt;surya - 32 batch size, for 9GB VRAM usage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Surya predicts line-level bboxes, while tesseract and others predict word-level or character-level. It&#39;s hard to find 100% correct datasets with line-level annotations. Merging bboxes can be noisy, so I chose not to use IoU as the metric for evaluation.&lt;/p&gt; &#xA;&lt;p&gt;I instead used coverage, which calculates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Precision - how well the predicted bboxes cover ground truth bboxes&lt;/li&gt; &#xA; &lt;li&gt;Recall - how well ground truth bboxes cover predicted bboxes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;First calculate coverage for each bbox, then add a small penalty for double coverage, since we want the detection to have non-overlapping bboxes. Anything with a coverage of 0.5 or higher is considered a match.&lt;/p&gt; &#xA;&lt;p&gt;Then we calculate precision and recall for the whole dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Layout analysis&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_layout_chart.png&#34; alt=&#34;Benchmark chart&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Layout Type&lt;/th&gt; &#xA;   &lt;th&gt;precision&lt;/th&gt; &#xA;   &lt;th&gt;recall&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image&lt;/td&gt; &#xA;   &lt;td&gt;0.95&lt;/td&gt; &#xA;   &lt;td&gt;0.99&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Table&lt;/td&gt; &#xA;   &lt;td&gt;0.95&lt;/td&gt; &#xA;   &lt;td&gt;0.96&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;0.89&lt;/td&gt; &#xA;   &lt;td&gt;0.95&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Title&lt;/td&gt; &#xA;   &lt;td&gt;0.92&lt;/td&gt; &#xA;   &lt;td&gt;0.89&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Time per image - .79 seconds on GPU (A6000).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I benchmarked the layout analysis on &lt;a href=&#34;https://github.com/ibm-aur-nlp/PubLayNet&#34;&gt;Publaynet&lt;/a&gt;, which was not in the training data. I had to align publaynet labels with the surya layout labels. I was then able to find coverage for each layout type:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Precision - how well the predicted bboxes cover ground truth bboxes&lt;/li&gt; &#xA; &lt;li&gt;Recall - how well ground truth bboxes cover predicted bboxes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reading Order&lt;/h2&gt; &#xA;&lt;p&gt;75% mean accuracy, and .14 seconds per image on an A6000 GPU. See methodology for notes - this benchmark is not perfect measure of accuracy, and is more useful as a sanity check.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I benchmarked the layout analysis on the layout dataset from &lt;a href=&#34;https://www.icst.pku.edu.cn/cpdp/sjzy/&#34;&gt;here&lt;/a&gt;, which was not in the training data. Unfortunately, this dataset is fairly noisy, and not all the labels are correct. It was very hard to find a dataset annotated with reading order and also layout information. I wanted to avoid using a cloud service for the ground truth.&lt;/p&gt; &#xA;&lt;p&gt;The accuracy is computed by finding if each pair of layout boxes is in the correct order, then taking the % that are correct.&lt;/p&gt; &#xA;&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;You can benchmark the performance of surya on your machine.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the manual install instructions above.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry install --group dev&lt;/code&gt; - installs dev dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text line detection&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will evaluate tesseract and surya for text line detection across a randomly sampled set of images from &lt;a href=&#34;https://huggingface.co/datasets/vikp/doclaynet_bench&#34;&gt;doclaynet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/detection.py --max 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt; will render images and detected bboxes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--pdf_path&lt;/code&gt; will let you specify a pdf to benchmark instead of the default data&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text recognition&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will evaluate surya and optionally tesseract on multilingual pdfs from common crawl (with synthetic data for missing languages).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/recognition.py --tesseract&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug 2&lt;/code&gt; will render images with detected text&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--tesseract&lt;/code&gt; will run the benchmark with tesseract. You have to run &lt;code&gt;sudo apt-get install tesseract-ocr-all&lt;/code&gt; to install all tesseract data, and set &lt;code&gt;TESSDATA_PREFIX&lt;/code&gt; to the path to the tesseract data folder.&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;RECOGNITION_BATCH_SIZE=864&lt;/code&gt; to use the same batch size as the benchmark.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Layout analysis&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will evaluate surya on the publaynet dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/layout.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt; will render images with detected text&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reading Order&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/ordering.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt; will render images with detected text&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;p&gt;Text detection was trained on 4x A6000s for 3 days. It used a diverse set of images as training data. It was trained from scratch using a modified segformer architecture that reduces inference RAM requirements.&lt;/p&gt; &#xA;&lt;p&gt;Text recognition was trained on 4x A6000s for 2 weeks. It was trained using a modified donut model (GQA, MoE layer, UTF-16 decoding, layer config changes).&lt;/p&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;p&gt;This work would not have been possible without amazing open source AI work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.15203.pdf&#34;&gt;Segformer&lt;/a&gt; from NVIDIA&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/clovaai/donut&#34;&gt;Donut&lt;/a&gt; from Naver&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; from huggingface&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/clovaai/CRAFT-pytorch&#34;&gt;CRAFT&lt;/a&gt;, a great scene text detection model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thank you to everyone who makes open source AI possible.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>JoeanAmier/XHS-Downloader</title>
    <updated>2024-05-12T01:42:58Z</updated>
    <id>tag:github.com,2024-05-12:/JoeanAmier/XHS-Downloader</id>
    <link href="https://github.com/JoeanAmier/XHS-Downloader" rel="alternate"></link>
    <summary type="html">&lt;p&gt;小红书链接提取/作品采集工具：提取账号发布、收藏、点赞作品链接；提取搜索结果作品、用户链接；采集小红书作品信息；提取小红书作品下载地址；下载小红书无水印作品文件！&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/XHS-Downloader.png&#34; alt=&#34;&#34; height=&#34;256&#34; width=&#34;256&#34;&gt;&#xA; &lt;br&gt; &#xA; &lt;h1&gt;XHS-Downloader&lt;/h1&gt; &#xA; &lt;p&gt;简体中文 | &lt;a href=&#34;https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/README_EN.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=ff7a45&#34;&gt; &#xA; &lt;img alt=&#34;GitHub forks&#34; src=&#34;https://img.shields.io/github/forks/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=9254de&#34;&gt; &#xA; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=ff7875&#34;&gt; &#xA; &lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/UserScript-ffec3d?style=for-the-badge&amp;amp;logo=tampermonkey&amp;amp;logoColor=%2300485B&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img alt=&#34;GitHub code size in bytes&#34; src=&#34;https://img.shields.io/github/languages/code-size/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=73d13d&#34;&gt; &#xA; &lt;img alt=&#34;GitHub release (with filter)&#34; src=&#34;https://img.shields.io/github/v/release/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=40a9ff&#34;&gt; &#xA; &lt;img alt=&#34;GitHub all releases&#34; src=&#34;https://img.shields.io/github/downloads/JoeanAmier/XHS-Downloader/total?style=for-the-badge&amp;amp;color=f759ab&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;🔥 &lt;b&gt;小红书链接提取/作品采集工具&lt;/b&gt;：提取账号发布、收藏、点赞作品链接；提取搜索结果作品链接、用户链接；采集小红书作品信息；提取小红书作品下载地址；下载小红书无水印作品文件！&lt;/p&gt; &#xA;&lt;h1&gt;📑 项目功能&lt;/h1&gt; &#xA;&lt;ul&gt;&#xA; &lt;b&gt;程序功能&lt;/b&gt; &#xA; &lt;li&gt;✅ 采集小红书作品信息&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取小红书作品下载地址&lt;/li&gt; &#xA; &lt;li&gt;✅ 下载小红书无水印作品文件&lt;/li&gt; &#xA; &lt;li&gt;✅ 自动跳过已下载的作品文件&lt;/li&gt; &#xA; &lt;li&gt;✅ 作品文件完整性处理机制&lt;/li&gt; &#xA; &lt;li&gt;✅ 自定义图文作品文件下载格式&lt;/li&gt; &#xA; &lt;li&gt;✅ 持久化储存作品信息至文件&lt;/li&gt; &#xA; &lt;li&gt;✅ 作品文件储存至单独文件夹&lt;/li&gt; &#xA; &lt;li&gt;✅ 后台监听剪贴板下载作品&lt;/li&gt; &#xA; &lt;li&gt;✅ 记录已下载作品 ID&lt;/li&gt; &#xA; &lt;li&gt;✅ 支持命令行下载作品文件&lt;/li&gt; &#xA; &lt;li&gt;✅ 从浏览器读取 Cookie&lt;/li&gt; &#xA; &lt;li&gt;✅ 自定义文件名称格式&lt;/li&gt; &#xA; &lt;li&gt;☑️ 支持 API 调用功能&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt;&#xA; &lt;b&gt;脚本功能&lt;/b&gt; &#xA; &lt;li&gt;✅ 下载小红书无水印作品文件&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取发现页面作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取账号发布作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取账号收藏作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取账号点赞作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取搜索结果作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取搜索结果用户链接&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;📸 程序截图&lt;/h1&gt; &#xA;&lt;p&gt;&lt;b&gt;🎥 点击图片观看演示视频&lt;/b&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1PJ4m1Y7Jt/&#34;&gt;&lt;img src=&#34;static/screenshot/程序运行截图CN1.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1PJ4m1Y7Jt/&#34;&gt;&lt;img src=&#34;static/screenshot/程序运行截图CN2.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1PJ4m1Y7Jt/&#34;&gt;&lt;img src=&#34;static/screenshot/程序运行截图CN3.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;h1&gt;🔗 支持链接&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;https://www.xiaohongshu.com/explore/作品ID&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;https://www.xiaohongshu.com/discovery/item/作品ID&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;https://xhslink.com/分享码&lt;/code&gt;&lt;/li&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;b&gt;支持单次输入多个作品链接，链接之间使用空格分隔。&lt;/b&gt;&lt;/p&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;🪟 关于终端&lt;/h1&gt; &#xA;&lt;p&gt;⭐ 推荐使用 &lt;a href=&#34;https://learn.microsoft.com/zh-cn/windows/terminal/install&#34;&gt;Windows 终端&lt;/a&gt; （Windows 11 默认终端）运行程序以便获得最佳显示效果！&lt;/p&gt; &#xA;&lt;h1&gt;🥣 使用方法&lt;/h1&gt; &#xA;&lt;p&gt;如果仅需下载无水印作品文件，建议选择 &lt;b&gt;程序运行&lt;/b&gt;；如果有其他需求，建议选择 &lt;b&gt;源码运行&lt;/b&gt;！&lt;/p&gt; &#xA;&lt;p&gt;建议自行设置 &lt;code&gt;cookie&lt;/code&gt; 参数，若不设置该参数，程序功能可能无法正常使用！&lt;/p&gt; &#xA;&lt;h2&gt;🖱 程序运行&lt;/h2&gt; &#xA;&lt;p&gt;Windows 10 及以上用户可前往 &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/releases/latest&#34;&gt;Releases&lt;/a&gt; 下载程序压缩包，解压后打开程序文件夹，双击运行 &lt;code&gt;main.exe&lt;/code&gt; 即可使用。&lt;/p&gt; &#xA;&lt;p&gt;若通过此方式使用程序，文件默认下载路径为：&lt;code&gt;.\_internal\Download&lt;/code&gt;；配置文件路径为：&lt;code&gt;.\_internal\settings.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;⌨️ 源码运行&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;安装版本号不低于 &lt;code&gt;3.12&lt;/code&gt; 的 Python 解释器&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;code&gt;pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt&lt;/code&gt; 命令安装程序所需模块&lt;/li&gt; &#xA; &lt;li&gt;下载本项目最新的源码或 &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/releases/latest&#34;&gt;Releases&lt;/a&gt; 发布的源码至本地&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;code&gt;main.py&lt;/code&gt; 即可使用&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;🛠 命令行模式&lt;/h1&gt; &#xA;&lt;p&gt;项目支持命令行运行模式，若想要下载图文作品的部分图片，可以使用此模式传入需要下载的图片序号！&lt;/p&gt; &#xA;&lt;p&gt;可以使用命令行从浏览器读取 Cookie 并写入配置文件！注意需要关闭对应浏览器才能读取数据！&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;bool&lt;/code&gt; 类型参数支持使用 &lt;code&gt;true&lt;/code&gt;、&lt;code&gt;false&lt;/code&gt;、&lt;code&gt;1&lt;/code&gt;、&lt;code&gt;0&lt;/code&gt;、&lt;code&gt;yes&lt;/code&gt;、&lt;code&gt;no&lt;/code&gt;、&lt;code&gt;on&lt;/code&gt; 或 &lt;code&gt;off&lt;/code&gt;（不区分大小写）来设置。&lt;/p&gt; &#xA;&lt;p&gt;命令示例：&lt;code&gt;python .\main.py --browser_cookie Chrome --update_settings&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;img src=&#34;static/screenshot/命令行模式截图1.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;img src=&#34;static/screenshot/命令行模式截图2.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;h1&gt;🕹 用户脚本&lt;/h1&gt; &#xA;&lt;img src=&#34;static/screenshot/用户脚本截图1.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;img src=&#34;static/screenshot/用户脚本截图2.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;p&gt;如果您的浏览器安装了 &lt;a href=&#34;https://www.tampermonkey.net/&#34;&gt;Tampermonkey&lt;/a&gt; 浏览器扩展程序，可以添加 &lt;a href=&#34;https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/XHS-Downloader.js&#34;&gt;用户脚本&lt;/a&gt;，无需下载安装即可体验项目功能！&lt;/p&gt; &#xA;&lt;p&gt;提示：使用 XHS-Downloader 用户脚本批量提取作品链接，搭配 XHS-Downloader 程序可以实现批量下载无水印作品文件！&lt;/p&gt; &#xA;&lt;h1&gt;💻 二次开发&lt;/h1&gt; &#xA;&lt;p&gt;如果有其他需求，可以根据 &lt;code&gt;main.py&lt;/code&gt; 的注释提示进行代码调用或修改！&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;async def example():&#xA;    &#34;&#34;&#34;通过代码设置参数，适合二次开发&#34;&#34;&#34;&#xA;    # 示例链接&#xA;    error_link = &#34;https://github.com/JoeanAmier/XHS_Downloader&#34;&#xA;    demo_link = &#34;https://www.xiaohongshu.com/explore/xxxxxxxxxx&#34;&#xA;    multiple_links = f&#34;{demo_link} {demo_link} {demo_link}&#34;&#xA;    # 实例对象&#xA;    work_path = &#34;D:\\&#34;  # 作品数据/文件保存根路径，默认值：项目根路径&#xA;    folder_name = &#34;Download&#34;  # 作品文件储存文件夹名称（自动创建），默认值：Download&#xA;    user_agent = &#34;&#34;  # 请求头 User-Agent，可选参数&#xA;    cookie = &#34;&#34;  # 小红书网页版 Cookie，无需登录，必需参数&#xA;    proxy = None  # 网络代理&#xA;    timeout = 5  # 请求数据超时限制，单位：秒，默认值：10&#xA;    chunk = 1024 * 1024 * 10  # 下载文件时，每次从服务器获取的数据块大小，单位：字节&#xA;    max_retry = 2  # 请求数据失败时，重试的最大次数，单位：秒，默认值：5&#xA;    record_data = False  # 是否保存作品数据至文件&#xA;    image_format = &#34;WEBP&#34;  # 图文作品文件下载格式，支持：PNG、WEBP&#xA;    folder_mode = False  # 是否将每个作品的文件储存至单独的文件夹&#xA;    async with XHS() as xhs:&#xA;        pass  # 使用默认参数&#xA;    async with XHS(work_path=work_path,&#xA;                   folder_name=folder_name,&#xA;                   user_agent=user_agent,&#xA;                   cookie=cookie,&#xA;                   proxy=proxy,&#xA;                   timeout=timeout,&#xA;                   chunk=chunk,&#xA;                   max_retry=max_retry,&#xA;                   record_data=record_data,&#xA;                   image_format=image_format,&#xA;                   folder_mode=folder_mode,&#xA;                   ) as xhs:  # 使用自定义参数&#xA;        download = True  # 是否下载作品文件，默认值：False&#xA;        # 返回作品详细信息，包括下载地址&#xA;        # 获取数据失败时返回空字典&#xA;        print(await xhs.extract(error_link, download, ))&#xA;        print(await xhs.extract(demo_link, download, ))&#xA;        # 支持传入多个作品链接&#xA;        print(await xhs.extract(multiple_links, download, ))&#xA;&lt;/pre&gt; &#xA;&lt;h1&gt;⚙️ 配置文件&lt;/h1&gt; &#xA;&lt;p&gt;项目根目录下的 &lt;code&gt;settings.json&lt;/code&gt; 文件，首次运行自动生成，可以自定义部分运行参数。&lt;/p&gt; &#xA;&lt;p&gt;如果设置了无效的参数值，程序将会使用参数默认值！&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;含义&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;默认值&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;work_path&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;作品数据 / 文件保存根路径&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;项目根路径&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;folder_name&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;作品文件储存文件夹名称&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Download&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;name_format&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;作品文件名称格式，字段之间使用空格分隔，支持字段：&lt;code&gt;收藏数量&lt;/code&gt;、&lt;code&gt;评论数量&lt;/code&gt;、&lt;code&gt;分享数量&lt;/code&gt;、&lt;code&gt;点赞数量&lt;/code&gt;、&lt;code&gt;作品标签&lt;/code&gt;、&lt;code&gt;作品ID&lt;/code&gt;、&lt;code&gt;作品标题&lt;/code&gt;、&lt;code&gt;作品描述&lt;/code&gt;、&lt;code&gt;作品类型&lt;/code&gt;、&lt;code&gt;发布时间&lt;/code&gt;、&lt;code&gt;最后更新时间&lt;/code&gt;、&lt;code&gt;作者昵称&lt;/code&gt;、&lt;code&gt;作者ID&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;发布时间 作者昵称 作品标题&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;user_agent&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;请求头 User-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;默认 UA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;cookie&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;小红书网页版 Cookie，&lt;b&gt;无需登录&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;无&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;proxy&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;设置程序代理&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;null&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;timeout&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;int&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;请求数据超时限制，单位：秒&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;chunk&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;int&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;下载文件时，每次从服务器获取的数据块大小，单位：字节&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1048576(1 MB)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;max_retry&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;int&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;请求数据失败时，重试的最大次数，单位：秒&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;record_data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;是否保存作品数据至文件，保存格式：&lt;code&gt;SQLite&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;false&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;image_format&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;图文作品文件下载格式，支持：&lt;code&gt;PNG&lt;/code&gt;、&lt;code&gt;WEBP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PNG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;image_download&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;图文作品文件下载开关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;true&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;video_download&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;视频作品文件下载开关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;true&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;folder_mode&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;是否将每个作品的文件储存至单独的文件夹；文件夹名称与文件名称保持一致&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;false&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;language&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;设置程序语言，目前支持：&lt;code&gt;zh_CN&lt;/code&gt;、&lt;code&gt;en_GB&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zh_CN&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;🌐 Cookie&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;打开浏览器（可选无痕模式启动），访问 &lt;code&gt;https://www.xiaohongshu.com/explore&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;按下 &lt;code&gt;F12&lt;/code&gt; 打开开发人员工具&lt;/li&gt; &#xA; &lt;li&gt;选择 &lt;code&gt;网络&lt;/code&gt; 选项卡&lt;/li&gt; &#xA; &lt;li&gt;勾选 &lt;code&gt;保留日志&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;在 &lt;code&gt;过滤&lt;/code&gt; 输入框输入 &lt;code&gt;cookie-name:web_session&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;选择 &lt;code&gt;Fetch/XHR&lt;/code&gt; 筛选器&lt;/li&gt; &#xA; &lt;li&gt;点击小红书页面任意作品&lt;/li&gt; &#xA; &lt;li&gt;在 &lt;code&gt;网络&lt;/code&gt; 选项卡选择任意数据包（如果无数据包，重复步骤7）&lt;/li&gt; &#xA; &lt;li&gt;全选复制 Cookie 写入程序或配置文件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;static/screenshot/获取Cookie示意图.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;h1&gt;🗳 下载记录&lt;/h1&gt; &#xA;&lt;p&gt;XHS-Downloader 会将下载过的作品 ID 储存至数据库，当重复下载相同的作品时，XHS-Downloader 会自动跳过该作品的文件下载（即使作品文件不存在），如果想要重新下载作品文件，请先删除数据库中对应的作品 ID，再使用 XHS-Downloader 下载作品文件！&lt;/p&gt; &#xA;&lt;h1&gt;♥️ 支持项目&lt;/h1&gt; &#xA;&lt;p&gt;如果 &lt;b&gt;XHS-Downloader&lt;/b&gt; 对您有帮助，请考虑为它点个 &lt;b&gt;Star&lt;/b&gt; ⭐，感谢您的支持！&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;微信(WeChat)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;支付宝(Alipay)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;./static/微信赞助二维码.png&#34; alt=&#34;微信赞助二维码&#34; height=&#34;200&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;./static/支付宝赞助二维码.png&#34; alt=&#34;支付宝赞助二维码&#34; height=&#34;200&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;如果您愿意，可以考虑提供资助为 &lt;b&gt;XHS-Downloader&lt;/b&gt; 提供额外的支持！&lt;/p&gt; &#xA;&lt;h1&gt;✉️ 联系作者&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;微信(其他事务): Downloader_Tools&lt;/li&gt; &#xA; &lt;li&gt;微信公众号(问题解答): Downloader Tools&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;Discord 社区&lt;/b&gt;: &lt;a href=&#34;https://discord.com/invite/ZYtmgKud9Y&#34;&gt;点击加入社区&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;QQ 群聊(使用交流): &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/raw/master/static/QQ%E7%BE%A4%E8%81%8A%E4%BA%8C%E7%BB%B4%E7%A0%81.png&#34;&gt;扫码加入群聊&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;b&gt;说明：&lt;/b&gt;QQ 群聊仅限于讨论项目使用问题，严禁发布任何广告，严禁讨论任何账号交易、账号流量、流量变现、灰色产业等相关的内容！&lt;/p&gt; &#xA;&lt;p&gt;✨ &lt;b&gt;作者的其他开源项目：&lt;/b&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;TikTokDownloader（抖音 / TikTok）&lt;/b&gt;：&lt;a href=&#34;https://github.com/JoeanAmier/TikTokDownloader&#34;&gt;https://github.com/JoeanAmier/TikTokDownloader&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;KS-Downloader（快手）&lt;/b&gt;：&lt;a href=&#34;https://github.com/JoeanAmier/KS-Downloader&#34;&gt;https://github.com/JoeanAmier/KS-Downloader&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;⚠️ 免责声明&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;使用者对本项目的使用由使用者自行决定，并自行承担风险。作者对使用者使用本项目所产生的任何损失、责任、或风险概不负责。&lt;/li&gt; &#xA; &lt;li&gt;本项目的作者提供的代码和功能是基于现有知识和技术的开发成果。作者尽力确保代码的正确性和安全性，但不保证代码完全没有错误或缺陷。&lt;/li&gt; &#xA; &lt;li&gt;使用者在使用本项目时必须严格遵守 &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/raw/master/LICENSE&#34;&gt;GNU General Public License v3.0&lt;/a&gt; 的要求，并在适当的地方注明使用了 &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/raw/master/LICENSE&#34;&gt;GNU General Public License v3.0&lt;/a&gt; 的代码。 &lt;/li&gt; &#xA; &lt;li&gt;使用者在任何情况下均不得将本项目的作者、贡献者或其他相关方与使用者的使用行为联系起来，或要求其对使用者使用本项目所产生的任何损失或损害负责。&lt;/li&gt; &#xA; &lt;li&gt;使用者在使用本项目的代码和功能时，必须自行研究相关法律法规，并确保其使用行为合法合规。任何因违反法律法规而导致的法律责任和风险，均由使用者自行承担。&lt;/li&gt; &#xA; &lt;li&gt;本项目的作者不会提供 XHS-Downloader 项目的付费版本，也不会提供与 XHS-Downloader 项目相关的任何商业服务。&lt;/li&gt; &#xA; &lt;li&gt;基于本项目进行的任何二次开发、修改或编译的程序与原创作者无关，原创作者不承担与二次开发行为或其结果相关的任何责任，使用者应自行对因二次开发可能带来的各种情况负全部责任。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;b&gt;在使用本项目的代码和功能之前，请您认真考虑并接受以上免责声明。如果您对上述声明有任何疑问或不同意，请不要使用本项目的代码和功能。如果您使用了本项目的代码和功能，则视为您已完全理解并接受上述免责声明，并自愿承担使用本项目的一切风险和后果。&lt;/b&gt; &#xA;&lt;h1&gt;💡 代码参考&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.aiohttp.org/en/stable/&#34;&gt;https://docs.aiohttp.org/en/stable/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://textual.textualize.io/&#34;&gt;https://textual.textualize.io/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiosqlite.omnilib.dev/en/stable/&#34;&gt;https://aiosqlite.omnilib.dev/en/stable/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://click.palletsprojects.com/en/8.1.x/&#34;&gt;https://click.palletsprojects.com/en/8.1.x/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/borisbabic/browser_cookie3&#34;&gt;https://github.com/borisbabic/browser_cookie3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>roboflow/supervision</title>
    <updated>2024-05-12T01:42:58Z</updated>
    <id>tag:github.com,2024-05-12:/roboflow/supervision</id>
    <link href="https://github.com/roboflow/supervision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;We write your reusable computer vision tools. 💜&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a align=&#34;center&#34; href=&#34;&#34; target=&#34;https://supervision.roboflow.com&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/notebooks&#34;&gt;notebooks&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/inference&#34;&gt;inference&lt;/a&gt; | &lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;autodistill&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/multimodal-maestro&#34;&gt;maestro&lt;/a&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/supervision.svg?sanitize=true&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/supervision&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/supervision&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/supervision&#34; alt=&#34;python-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/Annotators&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;gradio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/GbfgXGJ8Bk&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1159501506232451173&#34; alt=&#34;discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://squidfunk.github.io/mkdocs-material/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&amp;amp;logoColor=white&#34; alt=&#34;built-with-material-for-mkdocs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;👋 hello&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We write your reusable computer vision tools.&lt;/strong&gt; Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! 🤝&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/orgs/roboflow/projects/10&#34;&gt;&lt;img src=&#34;https://github.com/roboflow/supervision/assets/26109316/c05cc954-b9a6-4ed5-9a52-d0b4b619ff65&#34; alt=&#34;supervision-hackfest&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💻 install&lt;/h2&gt; &#xA;&lt;p&gt;Pip install the supervision package in a &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install supervision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Read more about conda, mamba, and installing from source in our &lt;a href=&#34;https://roboflow.github.io/supervision/&#34;&gt;guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🔥 quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;models&lt;/h3&gt; &#xA;&lt;p&gt;Supervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created &lt;a href=&#34;https://supervision.roboflow.com/latest/detection/core/#detections&#34;&gt;connectors&lt;/a&gt; for the most popular libraries like Ultralytics, Transformers, or MMDetection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;from ultralytics import YOLO&#xA;&#xA;image = cv2.imread(...)&#xA;model = YOLO(&#39;yolov8s.pt&#39;)&#xA;result = model(image)[0]&#xA;detections = sv.Detections.from_ultralytics(result)&#xA;&#xA;len(detections)&#xA;# 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;👉 more model connectors&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;inference&lt;/p&gt; &lt;p&gt;Running with &lt;a href=&#34;https://github.com/roboflow/inference&#34;&gt;Inference&lt;/a&gt; requires a &lt;a href=&#34;https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key&#34;&gt;Roboflow API KEY&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;from inference import get_model&#xA;&#xA;image = cv2.imread(...)&#xA;model = get_model(model_id=&#34;yolov8s-640&#34;, api_key=&amp;lt;ROBOFLOW API KEY&amp;gt;)&#xA;result = model.infer(image)[0]&#xA;detections = sv.Detections.from_inference(result)&#xA;&#xA;len(detections)&#xA;# 5&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;annotators&lt;/h3&gt; &#xA;&lt;p&gt;Supervision offers a wide range of highly customizable &lt;a href=&#34;https://supervision.roboflow.com/latest/annotators/&#34;&gt;annotators&lt;/a&gt;, allowing you to compose the perfect visualization for your use case.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;&#xA;image = cv2.imread(...)&#xA;detections = sv.Detections(...)&#xA;&#xA;bounding_box_annotator = sv.BoundingBoxAnnotator()&#xA;annotated_frame = bounding_box_annotator.annotate(&#xA;    scene=image.copy(),&#xA;    detections=detections&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;datasets&lt;/h3&gt; &#xA;&lt;p&gt;Supervision provides a set of &lt;a href=&#34;https://supervision.roboflow.com/latest/datasets/&#34;&gt;utils&lt;/a&gt; that allow you to load, split, merge, and save datasets in one of the supported formats.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import supervision as sv&#xA;&#xA;dataset = sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset.classes&#xA;[&#39;dog&#39;, &#39;person&#39;]&#xA;&#xA;len(dataset)&#xA;#&amp;nbsp;1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;👉 more dataset utils&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;load&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset = sv.DetectionDataset.from_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&#xA;dataset = sv.DetectionDataset.from_coco(&#xA;    images_directory_path=...,&#xA;    annotations_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;split&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_dataset, test_dataset = dataset.split(split_ratio=0.7)&#xA;test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)&#xA;&#xA;len(train_dataset), len(test_dataset), len(valid_dataset)&#xA;#&amp;nbsp;(700, 150, 150)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;merge&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds_1 = sv.DetectionDataset(...)&#xA;len(ds_1)&#xA;#&amp;nbsp;100&#xA;ds_1.classes&#xA;#&amp;nbsp;[&#39;dog&#39;, &#39;person&#39;]&#xA;&#xA;ds_2 = sv.DetectionDataset(...)&#xA;len(ds_2)&#xA;# 200&#xA;ds_2.classes&#xA;#&amp;nbsp;[&#39;cat&#39;]&#xA;&#xA;ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])&#xA;len(ds_merged)&#xA;#&amp;nbsp;300&#xA;ds_merged.classes&#xA;#&amp;nbsp;[&#39;cat&#39;, &#39;dog&#39;, &#39;person&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;save&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset.as_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset.as_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&#xA;dataset.as_coco(&#xA;    images_directory_path=...,&#xA;    annotations_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;convert&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;).as_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;🎬 tutorials&lt;/h2&gt; &#xA;&lt;p&gt;Want to learn how to use Supervision? Explore our &lt;a href=&#34;https://supervision.roboflow.com/develop/how_to/detect_and_annotate/&#34;&gt;how-to guides&lt;/a&gt;, &lt;a href=&#34;https://github.com/roboflow/supervision/tree/develop/examples&#34;&gt;end-to-end examples&lt;/a&gt;, and &lt;a href=&#34;https://supervision.roboflow.com/develop/cookbooks/&#34;&gt;cookbooks&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/hAWpsIuem10&#34; title=&#34;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/a742823d-c158-407d-b30f-063a5d11b4e1&#34; alt=&#34;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/hAWpsIuem10&#34; title=&#34;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&#34;&gt;&lt;strong&gt;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 5 Apr 2024&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt;Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios.&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/uWP6UjDeZvY&#34; title=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91&#34; alt=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/uWP6UjDeZvY&#34; title=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34;&gt;&lt;strong&gt;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 11 Jan 2024&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt;Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💜 built with supervision&lt;/h2&gt; &#xA;&lt;p&gt;Did you build something cool using supervision? &lt;a href=&#34;https://github.com/roboflow/supervision/discussions/categories/built-with-supervision&#34;&gt;Let us know!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&#34;&gt;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📚 documentation&lt;/h2&gt; &#xA;&lt;p&gt;Visit our &lt;a href=&#34;https://roboflow.github.io/supervision&#34;&gt;documentation&lt;/a&gt; page to learn how supervision can help you build computer vision applications faster and more reliably.&lt;/p&gt; &#xA;&lt;h2&gt;🏆 contribution&lt;/h2&gt; &#xA;&lt;p&gt;We love your input! Please see our &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started. Thank you 🙏 to all our contributors!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/roboflow/supervision/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=roboflow/supervision&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://youtube.com/roboflow&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.linkedin.com/company/roboflow-ai/&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://disuss.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584&#34; width=&#34;3%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;&#xA;  &lt;a href=&#34;https://blog.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;  &#xA; &lt;/div&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>