<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-17T02:06:20Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>coleam00/Archon</title>
    <updated>2025-08-17T02:06:20Z</updated>
    <id>tag:github.com,2025-08-17:/coleam00/Archon</id>
    <link href="https://github.com/coleam00/Archon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/archon-ui-main/public/archon-main-graphic.png&#34; alt=&#34;Archon Main Graphic&#34; width=&#34;853&#34; height=&#34;422&#34; /&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;em&gt;Power up your AI coding assistants with your own custom knowledge base and task management as an MCP server&lt;/em&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt; â€¢ &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/#whats-included&#34;&gt;What&#39;s Included&lt;/a&gt; â€¢ &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/#architecture&#34;&gt;Architecture&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;ğŸ¯ What is Archon?&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Archon is currently in beta! Expect things to not work 100%, and please feel free to share any feedback and contribute with fixes/new features!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Archon is the &lt;strong&gt;command center&lt;/strong&gt; for AI coding assistants. For you, it&#39;s a sleek interface to manage knowledge, context, and tasks for your projects. For the AI coding assistant(s), it&#39;s a &lt;strong&gt;Model Context Protocol (MCP) server&lt;/strong&gt; to collaborate on and leverage the same knowledge, context, and tasks. Connect Claude Code, Kiro, Cursor, Windsurf, etc. to give your AI agents access to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Your documentation&lt;/strong&gt; (crawled websites, uploaded PDFs/docs)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart search capabilities&lt;/strong&gt; with advanced RAG strategies&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task management&lt;/strong&gt; integrated with your knowledge base&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-time updates&lt;/strong&gt; as you add new content and collaborate with your coding assistant on tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Much more&lt;/strong&gt; coming soon to build Archon into an integrated environment for all context engineering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This new vision for Archon replaces the old one (the agenteer). Archon used to be the AI agent that builds other agents, and now you can use Archon to do that and more.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;It doesn&#39;t matter what you&#39;re building or if it&#39;s a new/existing codebase - Archon&#39;s knowledge and task management capabilities will improve the output of &lt;strong&gt;any&lt;/strong&gt; AI driven coding.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ğŸ”— Important Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/coleam00/Archon/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/strong&gt; - Join the conversation and share ideas about Archon&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; - How to get involved and contribute to Archon&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://youtu.be/8pRc_s2VQIo&#34;&gt;Introduction Video&lt;/a&gt;&lt;/strong&gt; - Getting Started Guide and Vision for Archon&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://dynamous.ai&#34;&gt;Dynamous AI Mastery&lt;/a&gt;&lt;/strong&gt; - The birthplace of Archon - come join a vibrant community of other early AI adopters all helping each other transform their careers and businesses!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://supabase.com/&#34;&gt;Supabase&lt;/a&gt; account (free tier or local Supabase both work)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/api-keys&#34;&gt;OpenAI API key&lt;/a&gt; (Gemini and Ollama are supported too!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Setup Instructions&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone Repository&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/coleam00/archon.git&#xA;cd archon&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment Configuration&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env&#xA;# Edit .env and add your Supabase credentials:&#xA;# SUPABASE_URL=https://your-project.supabase.co&#xA;# SUPABASE_SERVICE_KEY=your-service-key-here&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NOTE: Supabase introduced a new type of service key but use the legacy one (the longer one).&lt;/p&gt; &lt;p&gt;OPTIONAL: If you want to enable the reranking RAG strategy, uncomment lines 20-22 in &lt;code&gt;python\requirements.server.txt&lt;/code&gt;. This will significantly increase the size of the Archon Server container which is why it&#39;s off by default.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Setup&lt;/strong&gt;: In your &lt;a href=&#34;https://supabase.com/dashboard&#34;&gt;Supabase project&lt;/a&gt; SQL Editor, copy, paste, and execute the contents of &lt;code&gt;migration/complete_setup.sql&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up --build -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This starts the core microservices:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Core API and business logic (Port: 8181)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;: Protocol interface for AI clients (Port: 8051)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Agents (coming soon!)&lt;/strong&gt;: AI operations and streaming (Port: 8052)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: Web interface (Port: 3737)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Ports are configurable in your .env as well!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure API Keys&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open &lt;a href=&#34;http://localhost:3737&#34;&gt;http://localhost:3737&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Go to &lt;strong&gt;Settings&lt;/strong&gt; â†’ Select your LLM/embedding provider and set the API key (OpenAI is default)&lt;/li&gt; &#xA;   &lt;li&gt;Test by uploading a document or crawling a website&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸ”„ Database Reset (Start Fresh if Needed)&lt;/h2&gt; &#xA;&lt;p&gt;If you need to completely reset your database and start fresh:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;âš ï¸ &lt;strong&gt;Reset Database - This will delete ALL data for Archon!&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Reset Script&lt;/strong&gt;: In your Supabase SQL Editor, run the contents of &lt;code&gt;migration/RESET_DB.sql&lt;/code&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ WARNING: This will delete all Archon specific tables and data! Nothing else will be touched in your DB though.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rebuild Database&lt;/strong&gt;: After reset, run &lt;code&gt;migration/complete_setup.sql&lt;/code&gt; to create all the tables again.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Restart Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconfigure&lt;/strong&gt;:&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Select your LLM/embedding provider and set the API key again&lt;/li&gt; &#xA;    &lt;li&gt;Re-upload any documents or re-crawl websites&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;The reset script safely removes all tables, functions, triggers, and policies with proper dependency handling.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;âš¡ Quick Test&lt;/h2&gt; &#xA;&lt;p&gt;Once everything is running:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test Web Crawling&lt;/strong&gt;: Go to &lt;a href=&#34;http://localhost:3737&#34;&gt;http://localhost:3737&lt;/a&gt; â†’ Knowledge Base â†’ &#34;Crawl Website&#34; â†’ Enter a doc URL (such as &lt;a href=&#34;https://ai.pydantic.dev/llms-full.txt&#34;&gt;https://ai.pydantic.dev/llms-full.txt&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test Document Upload&lt;/strong&gt;: Knowledge Base â†’ Upload a PDF&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test Projects&lt;/strong&gt;: Projects â†’ Create a new project and add tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integrate with your AI coding assistant&lt;/strong&gt;: MCP Dashboard â†’ Copy connection config for your AI coding assistant&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Core Services&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Service&lt;/th&gt; &#xA;   &lt;th&gt;Container Name&lt;/th&gt; &#xA;   &lt;th&gt;Default URL&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;archon-ui&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:3737&#34;&gt;http://localhost:3737&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main dashboard and controls&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;API Service&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;archon-server&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:8181&#34;&gt;http://localhost:8181&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Web crawling, document processing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;archon-mcp&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:8051&#34;&gt;http://localhost:8051&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Model Context Protocol interface&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Agents Service&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;archon-agents&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://localhost:8052&#34;&gt;http://localhost:8052&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AI/ML operations, reranking&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;What&#39;s Included&lt;/h2&gt; &#xA;&lt;h3&gt;ğŸ§  Knowledge Management&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart Web Crawling&lt;/strong&gt;: Automatically detects and crawls entire documentation sites, sitemaps, and individual pages&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Document Processing&lt;/strong&gt;: Upload and process PDFs, Word docs, markdown files, and text documents with intelligent chunking&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code Example Extraction&lt;/strong&gt;: Automatically identifies and indexes code examples from documentation for enhanced search&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vector Search&lt;/strong&gt;: Advanced semantic search with contextual embeddings for precise knowledge retrieval&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Source Management&lt;/strong&gt;: Organize knowledge by source, type, and tags for easy filtering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ğŸ¤– AI Integration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: Connect any MCP-compatible client (Claude Code, Cursor, even non-AI coding assistants like Claude Desktop)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;10 MCP Tools&lt;/strong&gt;: Comprehensive yet simple set of tools for RAG queries, task management, and project operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-LLM Support&lt;/strong&gt;: Works with OpenAI, Ollama, and Google Gemini models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RAG Strategies&lt;/strong&gt;: Hybrid search, contextual embeddings, and result reranking for optimal AI responses&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-time Streaming&lt;/strong&gt;: Live responses from AI agents with progress tracking&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ğŸ“‹ Project &amp;amp; Task Management&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical Projects&lt;/strong&gt;: Organize work with projects, features, and tasks in a structured workflow&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI-Assisted Creation&lt;/strong&gt;: Generate project requirements and tasks using integrated AI agents&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Document Management&lt;/strong&gt;: Version-controlled documents with collaborative editing capabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Real-time updates and status management across all project activities&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ğŸ”„ Real-time Collaboration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;WebSocket Updates&lt;/strong&gt;: Live progress tracking for crawling, processing, and AI operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-user Support&lt;/strong&gt;: Collaborative knowledge building and project management&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt;: Asynchronous operations that don&#39;t block the user interface&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Health Monitoring&lt;/strong&gt;: Built-in service health checks and automatic reconnection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;h3&gt;Microservices Structure&lt;/h3&gt; &#xA;&lt;p&gt;Archon uses true microservices architecture with clear separation of concerns:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;â”‚   Frontend UI   â”‚    â”‚  Server (API)   â”‚    â”‚   MCP Server    â”‚    â”‚ Agents Service  â”‚&#xA;â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚&#xA;â”‚  React + Vite   â”‚â—„â”€â”€â–ºâ”‚    FastAPI +    â”‚â—„â”€â”€â–ºâ”‚    Lightweight  â”‚â—„â”€â”€â–ºâ”‚   PydanticAI    â”‚&#xA;â”‚  Port 3737      â”‚    â”‚    SocketIO     â”‚    â”‚    HTTP Wrapper â”‚    â”‚   Port 8052     â”‚&#xA;â”‚                 â”‚    â”‚    Port 8181    â”‚    â”‚    Port 8051    â”‚    â”‚                 â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;         â”‚                        â”‚                        â”‚                        â”‚&#xA;         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                                  â”‚                        â”‚&#xA;                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚&#xA;                         â”‚    Database     â”‚               â”‚&#xA;                         â”‚                 â”‚               â”‚&#xA;                         â”‚    Supabase     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                         â”‚    PostgreSQL   â”‚&#xA;                         â”‚    PGVector     â”‚&#xA;                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Service Responsibilities&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Service&lt;/th&gt; &#xA;   &lt;th&gt;Location&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;   &lt;th&gt;Key Features&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Frontend&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;archon-ui-main/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Web interface and dashboard&lt;/td&gt; &#xA;   &lt;td&gt;React, TypeScript, TailwindCSS, Socket.IO client&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Server&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python/src/server/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Core business logic and APIs&lt;/td&gt; &#xA;   &lt;td&gt;FastAPI, service layer, Socket.IO broadcasts, all ML/AI operations&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python/src/mcp/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MCP protocol interface&lt;/td&gt; &#xA;   &lt;td&gt;Lightweight HTTP wrapper, 10 MCP tools, session management&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python/src/agents/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PydanticAI agent hosting&lt;/td&gt; &#xA;   &lt;td&gt;Document and RAG agents, streaming responses&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Communication Patterns&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;HTTP-based&lt;/strong&gt;: All inter-service communication uses HTTP APIs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Socket.IO&lt;/strong&gt;: Real-time updates from Server to Frontend&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MCP Protocol&lt;/strong&gt;: AI clients connect to MCP Server via SSE or stdio&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;No Direct Imports&lt;/strong&gt;: Services are truly independent with no shared code dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Key Architectural Benefits&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lightweight Containers&lt;/strong&gt;: Each service contains only required dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Independent Scaling&lt;/strong&gt;: Services can be scaled independently based on load&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Development Flexibility&lt;/strong&gt;: Teams can work on different services without conflicts&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Technology Diversity&lt;/strong&gt;: Each service uses the best tools for its specific purpose&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ”§ Configuring Custom Ports &amp;amp; Hostname&lt;/h2&gt; &#xA;&lt;p&gt;By default, Archon services run on the following ports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Archon-UI&lt;/strong&gt;: 3737&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Archon-Server&lt;/strong&gt;: 8181&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Archon-MCP&lt;/strong&gt;: 8051&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Archon-Agents&lt;/strong&gt;: 8052&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Archon-Docs&lt;/strong&gt;: 3838 (optional)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Changing Ports&lt;/h3&gt; &#xA;&lt;p&gt;To use custom ports, add these variables to your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Service Ports Configuration&#xA;ARCHON_UI_PORT=3737&#xA;ARCHON_SERVER_PORT=8181&#xA;ARCHON_MCP_PORT=8051&#xA;ARCHON_AGENTS_PORT=8052&#xA;ARCHON_DOCS_PORT=3838&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example: Running on different ports:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ARCHON_SERVER_PORT=8282&#xA;ARCHON_MCP_PORT=8151&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuring Hostname&lt;/h3&gt; &#xA;&lt;p&gt;By default, Archon uses &lt;code&gt;localhost&lt;/code&gt; as the hostname. You can configure a custom hostname or IP address by setting the &lt;code&gt;HOST&lt;/code&gt; variable in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Hostname Configuration&#xA;HOST=localhost  # Default&#xA;&#xA;# Examples of custom hostnames:&#xA;HOST=192.168.1.100     # Use specific IP address&#xA;HOST=archon.local      # Use custom domain&#xA;HOST=myserver.com      # Use public domain&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is useful when:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Running Archon on a different machine and accessing it remotely&lt;/li&gt; &#xA; &lt;li&gt;Using a custom domain name for your installation&lt;/li&gt; &#xA; &lt;li&gt;Deploying in a network environment where &lt;code&gt;localhost&lt;/code&gt; isn&#39;t accessible&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After changing hostname or ports:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Restart Docker containers: &lt;code&gt;docker-compose down &amp;amp;&amp;amp; docker-compose up -d&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Access the UI at: &lt;code&gt;http://${HOST}:${ARCHON_UI_PORT}&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update your AI client configuration with the new hostname and MCP port&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸ”§ Development&lt;/h2&gt; &#xA;&lt;p&gt;For development with hot reload:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Backend services (with auto-reload)&#xA;docker-compose up archon-server archon-mcp archon-agents --build&#xA;&#xA;# Frontend (with hot reload) &#xA;cd archon-ui-main &amp;amp;&amp;amp; npm run dev&#xA;&#xA;# Documentation (with hot reload)&#xA;cd docs &amp;amp;&amp;amp; npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The backend services are configured with &lt;code&gt;--reload&lt;/code&gt; flag in their uvicorn commands and have source code mounted as volumes for automatic hot reloading when you make changes.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; &#xA;&lt;p&gt;Archon Community License (ACL) v1.2 - see &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Archon is free, open, and hackable. Run it, fork it, share it - just don&#39;t sell it as-a-service without permission.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bytedance/Dolphin</title>
    <updated>2025-08-17T02:06:20Z</updated>
    <id>tag:github.com,2025-08-17:/bytedance/Dolphin</id>
    <link href="https://github.com/bytedance/Dolphin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repo for â€œDolphin: Document Image Parsing via Heterogeneous Anchor Promptingâ€, ACL, 2025.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/dolphin.png&#34; width=&#34;300&#34; /&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2505.14059&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Paper-arXiv-red&#34; /&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/ByteDance/Dolphin&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/HuggingFace-Dolphin-yellow&#34; /&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://modelscope.cn/models/ByteDance/Dolphin&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/ModelScope-Dolphin-purple&#34; /&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;http://115.190.42.15:8888/dolphin/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Demo-Dolphin-blue&#34; /&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/bytedance/Dolphin&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Code-Github-green&#34; /&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-MIT-lightgray&#34; /&gt; &lt;/a&gt; &#xA; &lt;br /&gt; &#xA;&lt;/div&gt; &#xA;&lt;br /&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/demo.gif&#34; width=&#34;800&#34; /&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting&lt;/h1&gt; &#xA;&lt;p&gt;Dolphin (&lt;strong&gt;Do&lt;/strong&gt;cument Image &lt;strong&gt;P&lt;/strong&gt;arsing via &lt;strong&gt;H&lt;/strong&gt;eterogeneous Anchor Prompt&lt;strong&gt;in&lt;/strong&gt;g) is a novel multimodal document image parsing model following an analyze-then-parse paradigm. This repository contains the demo code and pre-trained models for Dolphin.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“‘ Overview&lt;/h2&gt; &#xA;&lt;p&gt;Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Dolphin addresses these challenges through a two-stage approach:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;ğŸ” Stage 1&lt;/strong&gt;: Comprehensive page-level layout analysis by generating element sequence in natural reading order&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ğŸ§© Stage 2&lt;/strong&gt;: Efficient parallel parsing of document elements using heterogeneous anchors and task-specific prompts&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/framework.png&#34; width=&#34;680&#34; /&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Dolphin achieves promising performance across diverse page-level and element-level parsing tasks while ensuring superior efficiency through its lightweight architecture and parallel parsing mechanism.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸš€ Demo&lt;/h2&gt; &#xA;&lt;p&gt;Try our demo on &lt;a href=&#34;http://115.190.42.15:8888/dolphin/&#34;&gt;Demo-Dolphin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“… Changelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.07.10&lt;/strong&gt; Released the &lt;em&gt;Fox-Page Benchmark&lt;/em&gt;, a manually refined subset of the original &lt;a href=&#34;https://github.com/ucaslcl/Fox&#34;&gt;Fox dataset&lt;/a&gt;. Download via: &lt;a href=&#34;https://pan.baidu.com/share/init?surl=t746ULp6iU5bUraVrPlMSw&amp;amp;pwd=fox1&#34;&gt;Baidu Yun&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/file/d/1yZQZqI34QCqvhB4Tmdl3X_XEvYvQyP0q/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.30&lt;/strong&gt; Added &lt;a href=&#34;https://github.com/bytedance/Dolphin/raw/master/deployment/tensorrt_llm/ReadMe.md&#34;&gt;TensorRT-LLM support&lt;/a&gt; for accelerated inferenceï¼&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.27&lt;/strong&gt; Added &lt;a href=&#34;https://github.com/bytedance/Dolphin/raw/master/deployment/vllm/ReadMe.md&#34;&gt;vLLM support&lt;/a&gt; for accelerated inferenceï¼&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.13&lt;/strong&gt; Added multi-page PDF document parsing capability.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.21&lt;/strong&gt; Our demo is released at &lt;a href=&#34;http://115.190.42.15:8888/dolphin/&#34;&gt;link&lt;/a&gt;. Check it out!&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.20&lt;/strong&gt; The pretrained model and inference code of Dolphin are released.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.16&lt;/strong&gt; Our paper has been accepted by ACL 2025. Paper link: &lt;a href=&#34;https://arxiv.org/abs/2505.14059&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ› ï¸ Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ByteDance/Dolphin.git&#xA;cd Dolphin&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the pre-trained models using one of the following options:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A: Original Model Format (config-based)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download from &lt;a href=&#34;https://pan.baidu.com/s/15zcARoX0CTOHKbW8bFZovQ?pwd=9rpx&#34;&gt;Baidu Yun&lt;/a&gt; or &lt;a href=&#34;https://drive.google.com/drive/folders/1PQJ3UutepXvunizZEw-uGaQ0BCzf-mie?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; and put them in the &lt;code&gt;./checkpoints&lt;/code&gt; folder.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option B: Hugging Face Model Format&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Visit our Huggingface &lt;a href=&#34;https://huggingface.co/ByteDance/Dolphin&#34;&gt;model card&lt;/a&gt;, or download model by:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download the model from Hugging Face Hub&#xA;git lfs install&#xA;git clone https://huggingface.co/ByteDance/Dolphin ./hf_model&#xA;# Or use the Hugging Face CLI&#xA;pip install huggingface_hub&#xA;huggingface-cli download ByteDance/Dolphin --local-dir ./hf_model&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;âš¡ Inference&lt;/h2&gt; &#xA;&lt;p&gt;Dolphin provides two inference frameworks with support for two parsing granularities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Page-level Parsing&lt;/strong&gt;: Parse the entire document page into a structured JSON and Markdown format&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Element-level Parsing&lt;/strong&gt;: Parse individual document elements (text, table, formula)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ğŸ“„ Page-level Parsing&lt;/h3&gt; &#xA;&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Process a single document image&#xA;python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results&#xA;&#xA;# Process a single document pdf&#xA;python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results&#xA;&#xA;# Process all documents in a directory&#xA;python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results&#xA;&#xA;# Process with custom batch size for parallel element decoding&#xA;python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Process a single document image&#xA;python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results&#xA;&#xA;# Process a single document pdf&#xA;python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results&#xA;&#xA;# Process all documents in a directory&#xA;python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results&#xA;&#xA;# Process with custom batch size for parallel element decoding&#xA;python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ğŸ§© Element-level Parsing&lt;/h3&gt; &#xA;&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Process a single table image&#xA;python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/table_1.jpeg --element_type table&#xA;&#xA;# Process a single formula image&#xA;python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula&#xA;&#xA;# Process a single text paragraph image&#xA;python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/para_1.jpg --element_type text&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Process a single table image&#xA;python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/table_1.jpeg --element_type table&#xA;&#xA;# Process a single formula image&#xA;python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula&#xA;&#xA;# Process a single text paragraph image&#xA;python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/para_1.jpg --element_type text&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸŒŸ Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ”„ Two-stage analyze-then-parse approach based on a single VLM&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“Š Promising performance on document parsing tasks&lt;/li&gt; &#xA; &lt;li&gt;ğŸ” Natural reading order element sequence generation&lt;/li&gt; &#xA; &lt;li&gt;ğŸ§© Heterogeneous anchor prompting for different document elements&lt;/li&gt; &#xA; &lt;li&gt;â±ï¸ Efficient parallel parsing mechanism&lt;/li&gt; &#xA; &lt;li&gt;ğŸ¤— Support for Hugging Face Transformers for easier integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“® Notice&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Call for Bad Cases:&lt;/strong&gt; If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ’– Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to acknowledge the following open-source projects that provided inspiration and reference for this work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/clovaai/donut/&#34;&gt;Donut&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/nougat&#34;&gt;Nougat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ucas-HaoranWei/GOT-OCR2.0&#34;&gt;GOT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendatalab/MinerU/tree/master&#34;&gt;MinerU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/Swin-Transformer&#34;&gt;Swin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Hugging Face Transformers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code useful for your research, please use the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{feng2025dolphin,&#xA;  title={Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting},&#xA;  author={Feng, Hao and Wei, Shu and Fei, Xiang and Shi, Wei and Han, Yingdong and Liao, Lei and Lu, Jinghui and Wu, Binghong and Liu, Qi and Lin, Chunhui and others},&#xA;  journal={arXiv preprint arXiv:2505.14059},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#bytedance/Dolphin&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=bytedance/Dolphin&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34; /&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LeCAR-Lab/ASAP</title>
    <updated>2025-08-17T02:06:20Z</updated>
    <id>tag:github.com,2025-08-17:/LeCAR-Lab/ASAP</id>
    <link href="https://github.com/LeCAR-Lab/ASAP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of [RSS 2025] &#34;ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; ASAP: Aligning Simulation and Real-World Physics for &lt;p&gt;Learning Agile Humanoid Whole-Body Skills &lt;/p&gt;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;Robotics: Science and Systems (RSS) 2025&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://agile.human2humanoid.com/&#34;&gt;[Website]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2502.01143&#34;&gt;[Arxiv]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU&#34;&gt;[Video]&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/CMU-NV-logo-crop-png.png&#34; height=&#34;50&amp;quot;&#34; /&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/isaac-gym&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IsaacGym-Preview4-b.svg?sanitize=true&#34; alt=&#34;IsaacGym&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IsaacSim-4.2.0-b.svg?sanitize=true&#34; alt=&#34;IsaacSim&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Genesis-0.2.1-b.svg?sanitize=true&#34; alt=&#34;IsaacSim&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://ubuntu.com/blog/tag/22-04-lts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Platform-linux--64-orange.svg?sanitize=true&#34; alt=&#34;Linux platform&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img src=&#34;https://agile.human2humanoid.com/static/images/asap-preview-gif-480P.gif&#34; width=&#34;400px&#34; /&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- # Table of Contents --&gt; &#xA;&lt;h2&gt;ğŸ“š Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Links: &lt;a href=&#34;https://agile.human2humanoid.com/&#34;&gt;Website&lt;/a&gt; â€¢ &lt;a href=&#34;https://arxiv.org/pdf/2502.01143&#34;&gt;Arxiv&lt;/a&gt; â€¢ &lt;a href=&#34;https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU&#34;&gt;Video&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#installation&#34;&gt;Installation &amp;amp; Setup&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 2.1 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaacgym-conda-env&#34;&gt;Base Frameworks&lt;/a&gt;&lt;br /&gt; 2.2 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-isaacgym&#34;&gt;IsaacGym Setup&lt;/a&gt;&lt;br /&gt; 2.3 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-humanoidverse&#34;&gt;HumanoidVerse Setup&lt;/a&gt;&lt;br /&gt; 2.4 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaaclab-environment&#34;&gt;IsaacSim + IsaacLab Setup&lt;/a&gt;&lt;br /&gt; 2.5 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#genesis-environment&#34;&gt;Genesis Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training&#34;&gt;Training Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 3.1 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training&#34;&gt;Phase-Based Motion Tracking&lt;/a&gt;&lt;br /&gt; 3.2 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#asap-delta-action-model-training&#34;&gt;ASAP Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#train-delta-action-model&#34;&gt;Train Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#use-delta-action-model-for-policy-finetuning&#34;&gt;Finetune Policy with Delta Action Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-retargeting-to-any-humanoid&#34;&gt;Motion Retargeting to Any Humanoid&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 4.1 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#1-smpl-shape-preparation&#34;&gt;Step 1: SMPL Shape Preparation&lt;/a&gt;&lt;br /&gt; 4.2 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#2-smpl-motion-preparation-amass&#34;&gt;Step 2: SMPL Motion Preparation (AMASS)&lt;/a&gt;&lt;br /&gt; 4.3 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#3-robot-xml-and-motion-config-preparation&#34;&gt;Step 3: Robot XML &amp;amp; Motion Config&lt;/a&gt;&lt;br /&gt; 4.4 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#4-humanoid-smpl-shape-fitting&#34;&gt;Step 4: Humanoid-SMPL Shape Fitting&lt;/a&gt;&lt;br /&gt; 4.5 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#5-humanoid-smpl-motion-retargeting&#34;&gt;Step 5: Humanoid-SMPL Motion Retargeting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2simsim2real&#34;&gt;Deployment: Sim2Sim &amp;amp; Sim2Real&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 5.1 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#environment-setup&#34;&gt;Environment Setup&lt;/a&gt;&lt;br /&gt; 5.2 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2sim&#34;&gt;Sim2Sim Deployment&lt;/a&gt;&lt;br /&gt; 5.3 &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2real&#34;&gt;Sim2Real Deployment&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#license&#34;&gt;License&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled /&gt; Release code backbone&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled /&gt; Release phase-based motion tracking training pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled /&gt; Release ASAP motion datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled /&gt; Release motion retargeting pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled /&gt; Release sim2sim in MuJoCo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled /&gt; Release sim2real with UnitreeSDK&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled /&gt; Release ASAP delta action model training pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;ASAP codebase is built on top of &lt;a href=&#34;https://github.com/LeCAR-Lab/HumanoidVerse&#34;&gt;HumanoidVerse&lt;/a&gt; (a multi-simulator framework for humanoid learning) and &lt;a href=&#34;https://github.com/LeCAR-Lab/human2humanoid&#34;&gt;Human2Humanoid&lt;/a&gt; (our prior work on humanoid whole-body tracking).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/LeCAR-Lab/HumanoidVerse&#34;&gt;HumanoidVerse&lt;/a&gt; allows you to train humanoid skills in multiple simulators, including IsaacGym, IsaacSim, and Genesis. Its key design logic is the separation and modularization of simulators, tasks, and algorithms, which enables smooth transfers between different simulators and the real world with minimum effort (just one line of code change). We leverage this framework to develop &lt;a href=&#34;https://agile.human2humanoid.com/&#34;&gt;ASAP&lt;/a&gt; and study how to best transfer policies across simulators and the real world.&lt;/p&gt; &#xA;&lt;h2&gt;IsaacGym Conda Env&lt;/h2&gt; &#xA;&lt;p&gt;Create mamba/conda environment, in the following we use conda for example, but you can use mamba as well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n hvgym python=3.8&#xA;conda activate hvgym&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install IsaacGym&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://developer.nvidia.com/isaac-gym/download&#34;&gt;IsaacGym&lt;/a&gt; and extract:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://developer.nvidia.com/isaac-gym-preview-4&#xA;tar -xvzf isaac-gym-preview-4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install IsaacGym Python API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e isaacgym/python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Test installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python 1080_balls_of_solitude.py  # or&#xA;python joint_monkey.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For libpython error:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check conda path: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda info -e&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set LD_LIBRARY_PATH: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LD_LIBRARY_PATH=&amp;lt;/path/to/conda/envs/your_env/lib&amp;gt;:$LD_LIBRARY_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install HumanoidVerse&lt;/h3&gt; &#xA;&lt;p&gt;Install dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;pip install -e isaac_utils&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Test with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \&#xA;+simulator=isaacgym \&#xA;+exp=locomotion \&#xA;+domain_rand=NO_domain_rand \&#xA;+rewards=loco/reward_g1_locomotion \&#xA;+robot=g1/g1_29dof_anneal_23dof \&#xA;+terrain=terrain_locomotion_plane \&#xA;+obs=loco/leggedloco_obs_singlestep_withlinvel \&#xA;num_envs=1 \&#xA;project_name=TestIsaacGymInstallation \&#xA;experiment_name=G123dof_loco \&#xA;headless=False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Note:&lt;/summary&gt; This is ONLY for testing, NOT how we train the locomotion policy in the ASAP paper. But still, you can train a locomotion policy by: &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \&#xA;+simulator=isaacgym \&#xA;+exp=locomotion \&#xA;+domain_rand=NO_domain_rand \&#xA;+rewards=loco/reward_g1_locomotion \&#xA;+robot=g1/g1_29dof_anneal_23dof \&#xA;+terrain=terrain_locomotion_plane \&#xA;+obs=loco/leggedloco_obs_singlestep_withlinvel \&#xA;num_envs=4096 \&#xA;project_name=TestIsaacGymInstallation \&#xA;experiment_name=G123dof_loco \&#xA;headless=True \&#xA;rewards.reward_penalty_curriculum=True \&#xA;rewards.reward_initial_penalty_scale=0.1 \&#xA;rewards.reward_penalty_degree=0.00003 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;IsaacLab Environment&lt;/h2&gt; &#xA;&lt;h3&gt;Install IsaacSim&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download Omniverse Launcher&lt;/li&gt; &#xA; &lt;li&gt;Install Isaac Sim through launcher&lt;/li&gt; &#xA; &lt;li&gt;Set environment variables:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export ISAACSIM_PATH=&#34;${HOME}/.local/share/ov/pkg/isaac-sim-4.2.0&#34;&#xA;export ISAACSIM_PYTHON_EXE=&#34;${ISAACSIM_PATH}/python.sh&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install IsaacLab&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/isaac-sim/IsaacLab.git&#xA;cd IsaacLab &amp;amp;&amp;amp; ./isaaclab.sh --conda hvlab&#xA;mamba activate hvlab&#xA;sudo apt install cmake build-essential&#xA;./isaaclab.sh --install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setup HumanoidVerse&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;pip install -e isaac_utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Genesis Environment&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mamba create -n hvgen python=3.10&#xA;mamba activate hvgen&#xA;pip install genesis-world torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;pip install -e isaac_utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Motion Tracking Training&lt;/h1&gt; &#xA;&lt;p&gt;Train a phase-based motion tracking policy to imitate Cristiano Ronaldo&#39;s signature Siuuu move&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python humanoidverse/train_agent.py \&#xA;+simulator=isaacgym \&#xA;+exp=motion_tracking \&#xA;+domain_rand=NO_domain_rand \&#xA;+rewards=motion_tracking/reward_motion_tracking_dm_2real \&#xA;+robot=g1/g1_29dof_anneal_23dof \&#xA;+terrain=terrain_locomotion_plane \&#xA;+obs=motion_tracking/deepmimic_a2c_nolinvel_LARGEnoise_history \&#xA;num_envs=4096 \&#xA;project_name=MotionTracking \&#xA;experiment_name=MotionTracking_CR7 \&#xA;robot.motion.motion_file=&#34;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-TairanTestbed_TairanTestbed_CR7_video_CR7_level1_filter_amass.pkl&#34; \&#xA;rewards.reward_penalty_curriculum=True \&#xA;rewards.reward_penalty_degree=0.00001 \&#xA;env.config.resample_motion_when_training=False \&#xA;env.config.termination.terminate_when_motion_far=True \&#xA;env.config.termination_curriculum.terminate_when_motion_far_curriculum=True \&#xA;env.config.termination_curriculum.terminate_when_motion_far_threshold_min=0.3 \&#xA;env.config.termination_curriculum.terminate_when_motion_far_curriculum_degree=0.000025 \&#xA;robot.asset.self_collisions=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After training, you can visualize the policy by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python humanoidverse/eval_agent.py \&#xA;+checkpoint=logs/MotionTracking/xxxxxxxx_xxxxxxx-MotionTracking_CR7-motion_tracking-g1_29dof_anneal_23dof/model_5800.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is the visualization of the policy after traning 5800 iters. The policy is able to imitate the motion of Cristiano Ronaldo&#39;s Siuuu move. With more training, the policy will be more accurate and smooth (see the video in the &lt;a href=&#34;https://arxiv.org/pdf/2502.01143&#34;&gt;paper&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/motion_tracking_5800.gif&#34; width=&#34;400px&#34; /&gt; &#xA;&lt;h1&gt;ASAP delta action model training&lt;/h1&gt; &#xA;&lt;p&gt;Note that the only difference between the delta action model training and naive motion tracking training is that delta action model needs a motion file with extra keyname &lt;code&gt;&#34;action&#34;&lt;/code&gt; in the motion file, so that the resulting RL policy we are training is able to use the delta action model to &lt;code&gt;&#34;control the robot&#34;&lt;/code&gt; to match the real-world/sim2sim motions.&lt;/p&gt; &#xA;&lt;h2&gt;Train delta action model&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python humanoidverse/train_agent.py \                                                                                   &#xA;  +simulator=isaacgym \&#xA;  +exp=train_delta_a_open_loop \&#xA;  +domain_rand=NO_domain_rand \&#xA;  +rewards=motion_tracking/delta_a/reward_delta_a_openloop \&#xA;  +robot=g1/g1_29dof_anneal_23dof \&#xA;  +terrain=terrain_locomotion_plane \&#xA;  +obs=delta_a/open_loop \&#xA;  num_envs=5000 \&#xA;  project_name=DeltaA_Training \&#xA;  experiment_name=openloopDeltaA_training \&#xA;  robot.motion.motion_file=&#34;&amp;lt;PATH_TO_YOUR_MOTION_FILE_WITH_ACTION_KEYNAME&amp;gt;&#34; \&#xA;  env.config.max_episode_length_s=1.0 \&#xA;  rewards.reward_scales.penalty_minimal_action_norm=-0.1 \&#xA;  +device=cuda:0 \&#xA;  env.config.resample_motion_when_training=True \&#xA;  env.config.resample_time_interval_s=10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Use delta action model for policy finetuning&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;HYDRA_FULL_ERROR=1 \&#xA;python humanoidverse/train_agent.py \&#xA;+simulator=isaacgym \&#xA;+exp=train_delta_a_closed_loop \&#xA;algo.config.policy_checkpoint=&#39;&amp;lt;PATH_TO_YOUR_DELTA_A_MODEL&amp;gt;&#39; \&#xA;+domain_rand=NO_domain_rand_finetune_with_deltaA \&#xA;+rewards=motion_tracking/reward_motion_tracking_dm_simfinetuning \&#xA;+robot=g1/g1_29dof_anneal_23dof \&#xA;+terrain=terrain_locomotion_plane \&#xA;+obs=delta_a/train_policy_with_delta_a \&#xA;num_envs=4096 \&#xA;project_name=DeltaA_Finetune \&#xA;experiment_name=finetune_with_deltaA \&#xA;robot.motion.motion_file=&#34;&amp;lt;PATH_TO_YOUR_MOTION_FILE&amp;gt;&#34; \&#xA;+opt=wandb \&#xA;env.config.add_extra_action=True \&#xA;+checkpoint=&#34;&amp;lt;PATH_TO_YOUR_POLICY_TO_BE_FINETUNED&amp;gt;&#34; \&#xA;domain_rand.push_robots=False \&#xA;env.config.noise_to_initial_level=1 \&#xA;rewards.reward_penalty_curriculum=True \&#xA;+device=cuda:0 \&#xA;algo.config.save_interval=5 \&#xA;algo.config.num_learning_iterations=1000 &#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Motion Retargeting to Any Humanoid&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Here we share a generic humanoid motion retargeting pipeline to any humanoid from &lt;a href=&#34;https://github.com/ZhengyiLuo/PHC&#34;&gt;PHC&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] We have provided all the SMPL motions (&lt;code&gt;ASAP/humanoidverse/data/motions/raw_tairantestbed_smpl&lt;/code&gt;) and retargtted G1 motions (&lt;code&gt;ASAP/humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles&lt;/code&gt;) used in the ASAP paper in this codebase. If you are interested in using these motions G1, you can ignore this section. If you are interested in retargeting other humanoids or other motions, you can follow the steps below to prepare the SMPL shapes and motions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;It has three steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;SMPL Shape preparation&lt;/li&gt; &#xA; &lt;li&gt;SMPL Motion preparation&lt;/li&gt; &#xA; &lt;li&gt;Robot XML and Motion Config preparation&lt;/li&gt; &#xA; &lt;li&gt;Humanoid-SMPL shape fitting&lt;/li&gt; &#xA; &lt;li&gt;Humanoid-SMPL motion retargeting&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;1. SMPL Shape preparation&lt;/h2&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://download.is.tue.mpg.de/download.php?domain=smpl&amp;amp;sfile=SMPL_python_v.1.1.0.zip&#34;&gt;v1.1.0 SMPL files with pkl format&lt;/a&gt; and put it under &lt;code&gt;humanoidverse/data/smpl/&lt;/code&gt;, and you should have:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;|-- ASAP&#xA;    |-- humanoidverse&#xA;        |-- data&#xA;            |-- smpl&#xA;                |-- SMPL_python_v.1.1.0.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then &lt;code&gt;cd ASAP/humanoidverse/data/smpl/&lt;/code&gt; and &lt;code&gt;unzip SMPL_python_v.1.1.0.zip&lt;/code&gt;, after some copying and moving, you should have:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;|-- ASAP&#xA;    |-- humanoidverse&#xA;        |-- data&#xA;            |-- smpl&#xA;                |-- SMPL_python_v.1.1.0&#xA;                |-- models&#xA;                    |-- basicmodel_f_lbs_10_207_0_v1.1.0.pkl&#xA;                    |-- basicmodel_m_lbs_10_207_0_v1.1.0.pkl&#xA;                    |-- basicmodel_neutral_lbs_10_207_0_v1.1.0.pkl&#xA;                |-- smpl_webuser&#xA;                |-- ...&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Rename these three pkl files and move it under smpl like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;|-- ASAP&#xA;    |-- humanoidverse&#xA;        |-- data&#xA;            |-- smpl&#xA;                |-- SMPL_FEMALE.pkl&#xA;                |-- SMPL_MALE.pkl&#xA;                |-- SMPL_NEUTRAL.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. SMPL Motion preparation (AMASS)&lt;/h2&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://amass.is.tue.mpg.de/index.html&#34;&gt;AMASS Dataset&lt;/a&gt; with &lt;code&gt;SMPL + H G format&lt;/code&gt; and put it under &lt;code&gt;humanoidverse/data/motions/AMASS/AMASS_Complete/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;|-- ASAP&#xA;    |-- humanoidverse&#xA;        |-- data&#xA;            |-- AMASS&#xA;                |-- AMASS_Complete&#xA;                    |-- ACCAD.tar.bz2&#xA;                    |-- BMLhandball.tar.bz2&#xA;                    |-- BMLmovi.tar.bz2&#xA;                    |-- BMLrub.tar&#xA;                    |-- CMU.tar.bz2&#xA;                    |-- ...&#xA;                    |-- Transitions.tar.bz2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then cd ASAP/humanoidverse/data/motions/AMASS/AMASS_Complete/ and extract all the motion files by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;for file in *.tar.bz2; do&#xA;    tar -xvjf &#34;$file&#34;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you should have:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;|-- ASAP&#xA;    |-- humanoidverse&#xA;        |-- data&#xA;            |-- AMASS&#xA;                |-- AMASS_Complete&#xA;                    |-- ACCAD&#xA;                    |-- BioMotionLab_NTroje&#xA;                    |-- BMLhandball&#xA;                    |-- BMLmovi&#xA;                    |-- CMU&#xA;                    |-- ...&#xA;                    |-- Transitions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Robot XML and Motion Config preparation&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you have robot xml and meshes ready at (G1 as example) &lt;code&gt;humanoidverse/data/robots/g1/g1_29dof_anneal_23dof_fitmotionONLY.xml&lt;/code&gt; And add your config for the robot motion in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; with like the following. Remember to link the xml path in the config.&lt;/p&gt; &#xA;&lt;h2&gt;4. Humanoid-SMPL shape fitting&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command to fit the SMPL shape to the humanoid.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/data_process/fit_smpl_shape.py +robot=g1/g1_29dof_anneal_23dof&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And you should have you shape file located at &lt;code&gt;humanoidverse/data/shape/g1_29dof_anneal_23dof/shape_optimized_v1.pkl&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to visualize the shape, you can run with flag &lt;code&gt;+vis=True&lt;/code&gt;, then you can have visualization of the fitted SMPL body shape and the humanoid body keypoints like this shape. The blue is the humanoid body keypoints and the orange is the fitted SMPL body keypoint. You can tune the &lt;code&gt;robot motion&lt;/code&gt; in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; to adjust the correspondence, extend links lengths to get better fitted SMPL shape.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_shape.png&#34; width=&#34;400px&#34; /&gt; &#xA;&lt;h2&gt;5. Humanoid-SMPL motion retargeting&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command to retarget the motion to the humanoid.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/data_process/fit_smpl_motion.py +robot=g1/g1_29dof_anneal_23dof&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualize motion&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file=&#34;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To test, and you should have you one single motion file located at &lt;code&gt;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to visualize the motion, you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file=&#34;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should have&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_motion.gif&#34; width=&#34;400px&#34; /&gt; &#xA;&lt;h1&gt;Sim2Sim/Sim2Real&lt;/h1&gt; &#xA;&lt;h2&gt;Environment Setup&lt;/h2&gt; &#xA;&lt;p&gt;Env Installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mamba create -n asap_deploy python=3.10&#xA;mamba activate asap_deploy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install ros2-python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# this adds the conda-forge channel to the new created environment configuration &#xA;conda config --env --add channels conda-forge&#xA;# and the robostack channel&#xA;conda config --env --add channels robostack-staging&#xA;# remove the defaults channel just in case, this might return an error if it is not in the list which is ok&#xA;conda config --env --remove channels defaults&#xA;# install the ros2-python package&#xA;conda install ros-humble-desktop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Test Ros2Installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rviz2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see the UI like this:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/rviz.png&#34; width=&#34;400px&#34; /&gt; &#xA;&lt;p&gt;Install Unitree SDK&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:unitreerobotics/unitree_sdk2_python.git&#xA;cd unitree_sdk2_python&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;minor issue to fix:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade numpy scipy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Sim2Sim&lt;/h2&gt; &#xA;&lt;p&gt;start the simulation in the sim2real folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sim_env/base_sim.py --config=config/g1_29dof_hist.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;in another terminal, start the policy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; &#xA; &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; &#xA; &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And you should be able to play around with some checkpoints from the ASAP paper. Have fun!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip0-ezgif.com-video-to-gif-converter.gif&#34; width=&#34;300px&#34; /&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip1-ezgif.com-video-to-gif-converter.gif&#34; width=&#34;300px&#34; /&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip3-ezgif.com-video-to-gif-converter.gif&#34; width=&#34;300px&#34; /&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip4-ezgif.com-video-to-gif-converter.gif&#34; width=&#34;300px&#34; /&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip5-ezgif.com-video-to-gif-converter.gif&#34; width=&#34;300px&#34; /&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip6-ezgif.com-video-to-gif-converter.gif&#34; width=&#34;300px&#34; /&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;!-- Replace gif1.gif ... gif6.gif with your actual gif filenames and optionally add captions below each if desired --&gt; &#xA;&lt;h2&gt;Sim2Real&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;Note from Tairan&lt;/code&gt;: make sure to make the G1 robot to 29dof following this &lt;a href=&#34;https://support.unitree.com/home/en/G1_developer/waist_fastener&#34;&gt;doc&lt;/a&gt; and restart the robot after waist unlocking. If you don&#39;t know how to log into the Unitree Explore APP, contact unitree support.&lt;/p&gt; &#xA;&lt;p&gt;Enter Low-Level for g1&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open humanoid and wait until the head blue light is constantly on&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;L2+R2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;L2+A&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;L2+B&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Connect PC to the G1 by ethernet cable and configure the network following &lt;a href=&#34;https://support.unitree.com/home/en/G1_developer/quick_development&#34;&gt;this document&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Before starting the policy, modify the &lt;code&gt;config/g1_29dof_hist.yaml&lt;/code&gt; to set &lt;code&gt;INTERFACE&lt;/code&gt; to &lt;code&gt;eth0&lt;/code&gt; (if you are using linux), basically the network interface that you are using to connect to the robot with your PC&#39;s IP shown as &lt;code&gt;192.168.123.xxx&lt;/code&gt; in &lt;code&gt;ifconfig&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;start the policy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; &#xA; &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; &#xA; &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; &#xA; &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;â€¼ï¸Alert &amp;amp; Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;Deploying these models on physical hardware can be hazardous. Unless you have deep simâ€‘toâ€‘real expertise and robust safety protocols, we strongly advise against running the model on real robots. These models are supplied for research use only, and we disclaim all responsibility for any harm, loss, or malfunction arising from their deployment.&lt;/p&gt; &#xA;&lt;h3&gt;Demo code to collect real-world data&lt;/h3&gt; &#xA;&lt;p&gt;We provide a demo code to collect real-world data in the &lt;code&gt;sim2real/rl_policy/listener_deltaa.py&lt;/code&gt; file. Since MoCap setup is hard to transfer across different robots/labs, we hope this code can help you to collect data for your own experiments. Contact us (&lt;a href=&#34;mailto:tairanh@andrew.cmu.edu&#34;&gt;tairanh@andrew.cmu.edu&lt;/a&gt;) if you have any questions.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find our work useful, please consider citing us!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{he2025asap,&#xA;  title={ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills},&#xA;  author={He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbabu, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi &#34;Jim&#34; and Zhu, Yuke and Liu, Changliu and Shi, Guanya},&#xA;  journal={arXiv preprint arXiv:2502.01143},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
</feed>