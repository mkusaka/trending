<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-24T01:52:02Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NanmiCoder/MediaCrawler</title>
    <updated>2024-03-24T01:52:02Z</updated>
    <id>tag:github.com,2024-03-24:/NanmiCoder/MediaCrawler</id>
    <link href="https://github.com/NanmiCoder/MediaCrawler" rel="alternate"></link>
    <summary type="html">&lt;p&gt;å°çº¢ä¹¦ç¬”è®° | è¯„è®ºçˆ¬è™«ã€æŠ–éŸ³è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€å¿«æ‰‹è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€B ç«™è§†é¢‘ ï½œ è¯„è®ºçˆ¬è™«ã€å¾®åšå¸–å­ ï½œ è¯„è®ºçˆ¬è™«&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;å£°æ˜ï¼šè¯·éµå®ˆç›¸å…³æ³•è§„æ³•æ¡&lt;/strong&gt; &lt;br&gt; å›½å†…çˆ¬è™«è¿è§„è¿æ³•æ¡ˆä»¶ï¼š&lt;a href=&#34;https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China&#34;&gt;https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;æœ€è¿‘è¿™ä¸ªä»“åº“å—åˆ°äº†å¤§é‡çš„å…³æ³¨ï¼Œæœ¬èº«æˆ‘å¼€æºå‡ºæ¥è¿™ä¸ªä»“åº“çš„ç›®çš„æ˜¯æä¾›ä¸€ç§æ–°æ€è·¯åˆ†äº«ç»™å¤§å®¶å­¦ä¹ å’Œäº†è§£çˆ¬è™«ã€‚&lt;br&gt; ä½†æ˜¯éšç€çƒ­åº¦çš„ä¸æ–­æš´æ¶¨ï¼Œéšä¹‹è€Œæ¥çš„æ˜¯è¯¥ä»“åº“å¯èƒ½ç»™å„å¹³å°æ–¹å¸¦æ¥ä¸å°‘çš„æœåŠ¡å™¨å‹åŠ›ï¼Œä»¥åŠä¸€äº›å…¶ä»–ä¸å¥½çš„å½±å“ï¼ˆè™½ç„¶æˆ‘æœ‰å…è´£å£°æ˜ï¼Œä½†æ˜¯ä¸èƒ½æ§åˆ¶ä¸€äº›ä½¿ç”¨è¯¥ä»“åº“çš„äººï¼‰&lt;br&gt; æ‰€ä»¥ç»¼åˆè€ƒè™‘ä¹‹ä¸‹ï¼Œå†³å®šæ¸…é™¤è¯¥ä»“åº“ã€‚æˆ‘ä¸æƒ³ç»™è‡ªå·±å¸¦æ¥ä¸€äº›ä¸å¿…è¦çš„éº»çƒ¦ï¼Œæœ¬æ¥å°±æ²¡ä»ä¸­å¾—åˆ°äº›ä»€ä¹ˆï¼Œé£é™©è¿˜å¾ˆé«˜&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Sponsor&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dashboard.ipcola.com/register?referral_code=vkybwyucyuidpne&#34;&gt;å…¨çƒipä»£ç†è¶…æ–°æ˜Ÿ&lt;/a&gt; &lt;a href=&#34;https://dashboard.ipcola.com/register?referral_code=vkybwyucyuidpne&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2024/03/18/LKJaWcIHQl92ip5.jpg&#34; alt=&#34;IPCola,  å…¨çƒipä»£ç†è¶…æ–°æ˜Ÿ-å®˜ç½‘å›¾&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://monica.im/invitation?c=4HCSQRYS&#34;&gt;ä½ ä¹Ÿå¯ä»¥é€šè¿‡æ³¨å†Œè¿™æ¬¾å…è´¹çš„ChatGPTäº§å“ï¼Œå¸®æˆ‘è·å–é¢å¤–çš„GPT-4é¢åº¦ä½œä¸ºæ”¯æŒï¼Œä¹Ÿæ˜¯æˆ‘æ¯å¤©éƒ½åœ¨ç”¨çš„ä¸€æ¬¾chromeæ•ˆç‡æ’ä»¶ï¼Œæ¨èç»™ä½ ï¼Œä½ ä¹Ÿèƒ½è·å¾—å…è´¹é¢åº¦ã€‚&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/NanmiCoder/MediaCrawler/issues/180&#34;&gt;æ•´æ•°æ™ºèƒ½ã€Šé«˜çº§çˆ¬è™«å·¥ç¨‹å¸ˆã€‹æ‹›è˜&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;æ•™ç¨‹&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ä»¥åæŠ€æœ¯æ•™ç¨‹åˆ†äº«ä¼šä¼˜å…ˆè€ƒè™‘è¿è§„é—®é¢˜ï¼Œæœ¬èº«æŠ€æœ¯ä¸åˆ†å¥½åï¼Œä½†æ˜¯æœ‰çš„äººä¼šæ‹¿å®ƒåšä¸€äº›ä¸å¥½çš„äº‹ï¼Œæ‰€ä»¥åšå†³ä¸åšä¸€äº›è®©è‡ªå·±é™·å…¥éº»çƒ¦çš„äº‹å„¿ï½ï¼ˆæˆ‘æ¯”è¾ƒè°¨æ…ï¼‰&lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;åç»­ä¼šé™†é™†ç»­ç»­ç»™å¤§å®¶å½•åˆ¶åˆ†äº«ä¸€äº›æŠ€æœ¯æ•™ç¨‹ï¼Œè¯·å…³æ³¨æˆ‘çš„&lt;a href=&#34;https://space.bilibili.com/434377496&#34;&gt;Bç«™è´¦å·&lt;/a&gt;ï¼Œæ›´æ–°åä¼šç¬¬ä¸€æ—¶é—´ç»™ä½ å‘é€šçŸ¥.&lt;br&gt; Bç«™ç©ºé—´åœ°å€ï¼š&lt;a href=&#34;https://space.bilibili.com/434377496&#34;&gt;https://space.bilibili.com/434377496&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;star è¶‹åŠ¿å›¾&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#NanmiCoder/MediaCrawler&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zylon-ai/private-gpt</title>
    <updated>2024-03-24T01:52:02Z</updated>
    <id>tag:github.com,2024-03-24:/zylon-ai/private-gpt</id>
    <link href="https://github.com/zylon-ai/private-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Interact with your documents using the power of GPT, 100% privately, no data leaks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ”’ PrivateGPT ğŸ“‘&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/imartinez/privateGPT/actions/workflows/tests.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/imartinez/privateGPT/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.privategpt.dev/&#34;&gt;&lt;img src=&#34;https://img.shields.io/website?up_message=check%20it&amp;amp;down_message=down&amp;amp;url=https%3A%2F%2Fdocs.privategpt.dev%2F&amp;amp;label=Documentation&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/bK6mRVpErU&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1164200432894234644?logo=discord&amp;amp;label=PrivateGPT&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/ZylonPrivateGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/ZylonPrivateGPT&#34; alt=&#34;X (formerly Twitter) Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Install &amp;amp; usage docs: &lt;a href=&#34;https://docs.privategpt.dev/&#34;&gt;https://docs.privategpt.dev/&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Join the community: &lt;a href=&#34;https://twitter.com/PrivateGPT_AI&#34;&gt;Twitter&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://discord.gg/bK6mRVpErU&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zylon-ai/private-gpt/main/fern/docs/assets/ui.png?raw=true&#34; alt=&#34;Gradio UI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PrivateGPT is a production-ready AI project that allows you to ask questions about your documents using the power of Large Language Models (LLMs), even in scenarios without an Internet connection. 100% private, no data leaves your execution environment at any point.&lt;/p&gt; &#xA;&lt;p&gt;The project provides an API offering all the primitives required to build private, context-aware AI applications. It follows and extends the &lt;a href=&#34;https://openai.com/blog/openai-api&#34;&gt;OpenAI API standard&lt;/a&gt;, and supports both normal and streaming responses.&lt;/p&gt; &#xA;&lt;p&gt;The API is divided into two logical blocks:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-level API&lt;/strong&gt;, which abstracts all the complexity of a RAG (Retrieval Augmented Generation) pipeline implementation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ingestion of documents: internally managing document parsing, splitting, metadata extraction, embedding generation and storage.&lt;/li&gt; &#xA; &lt;li&gt;Chat &amp;amp; Completions using context from ingested documents: abstracting the retrieval of context, the prompt engineering and the response generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Low-level API&lt;/strong&gt;, which allows advanced users to implement their own complex pipelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Embeddings generation: based on a piece of text.&lt;/li&gt; &#xA; &lt;li&gt;Contextual chunks retrieval: given a query, returns the most relevant chunks of text from the ingested documents.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition to this, a working &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio UI&lt;/a&gt; client is provided to test the API, together with a set of useful tools such as bulk model download script, ingestion script, documents folder watch, etc.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ğŸ‘‚ &lt;strong&gt;Need help applying PrivateGPT to your specific use case?&lt;/strong&gt; &lt;a href=&#34;https://forms.gle/4cSDmH13RZBHV9at7&#34;&gt;Let us know more about it&lt;/a&gt; and we&#39;ll try to help! We are refining PrivateGPT through your feedback.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ğŸï¸ Overview&lt;/h2&gt; &#xA;&lt;p&gt;DISCLAIMER: This README is not updated as frequently as the &lt;a href=&#34;https://docs.privategpt.dev/&#34;&gt;documentation&lt;/a&gt;. Please check it out for the latest updates!&lt;/p&gt; &#xA;&lt;h3&gt;Motivation behind PrivateGPT&lt;/h3&gt; &#xA;&lt;p&gt;Generative AI is a game changer for our society, but adoption in companies of all sizes and data-sensitive domains like healthcare or legal is limited by a clear concern: &lt;strong&gt;privacy&lt;/strong&gt;. Not being able to ensure that your data is fully under your control when using third-party AI tools is a risk those industries cannot take.&lt;/p&gt; &#xA;&lt;h3&gt;Primordial version&lt;/h3&gt; &#xA;&lt;p&gt;The first version of PrivateGPT was launched in May 2023 as a novel approach to address the privacy concerns by using LLMs in a complete offline way.&lt;/p&gt; &#xA;&lt;p&gt;That version, which rapidly became a go-to project for privacy-sensitive setups and served as the seed for thousands of local-focused generative AI projects, was the foundation of what PrivateGPT is becoming nowadays; thus a simpler and more educational implementation to understand the basic concepts required to build a fully local -and therefore, private- chatGPT-like tool.&lt;/p&gt; &#xA;&lt;p&gt;If you want to keep experimenting with it, we have saved it in the &lt;a href=&#34;https://github.com/imartinez/privateGPT/tree/primordial&#34;&gt;primordial branch&lt;/a&gt; of the project.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;It is strongly recommended to do a clean clone and install of this new version of PrivateGPT if you come from the previous, primordial version.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Present and Future of PrivateGPT&lt;/h3&gt; &#xA;&lt;p&gt;PrivateGPT is now evolving towards becoming a gateway to generative AI models and primitives, including completions, document ingestion, RAG pipelines and other low-level building blocks. We want to make it easier for any developer to build AI applications and experiences, as well as provide a suitable extensive architecture for the community to keep contributing.&lt;/p&gt; &#xA;&lt;p&gt;Stay tuned to our &lt;a href=&#34;https://github.com/imartinez/privateGPT/releases&#34;&gt;releases&lt;/a&gt; to check out all the new features and changes included.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“„ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Full documentation on installation, dependencies, configuration, running the server, deployment options, ingesting local documents, API details and UI features can be found here: &lt;a href=&#34;https://docs.privategpt.dev/&#34;&gt;https://docs.privategpt.dev/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ§© Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Conceptually, PrivateGPT is an API that wraps a RAG pipeline and exposes its primitives.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The API is built using &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; and follows &lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;&gt;OpenAI&#39;s API scheme&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The RAG pipeline is based on &lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The design of PrivateGPT allows to easily extend and adapt both the API and the RAG implementation. Some key architectural decisions are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dependency Injection, decoupling the different components and layers.&lt;/li&gt; &#xA; &lt;li&gt;Usage of LlamaIndex abstractions such as &lt;code&gt;LLM&lt;/code&gt;, &lt;code&gt;BaseEmbedding&lt;/code&gt; or &lt;code&gt;VectorStore&lt;/code&gt;, making it immediate to change the actual implementations of those abstractions.&lt;/li&gt; &#xA; &lt;li&gt;Simplicity, adding as few layers and new abstractions as possible.&lt;/li&gt; &#xA; &lt;li&gt;Ready to use, providing a full implementation of the API and RAG pipeline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Main building blocks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;APIs are defined in &lt;code&gt;private_gpt:server:&amp;lt;api&amp;gt;&lt;/code&gt;. Each package contains an &lt;code&gt;&amp;lt;api&amp;gt;_router.py&lt;/code&gt; (FastAPI layer) and an &lt;code&gt;&amp;lt;api&amp;gt;_service.py&lt;/code&gt; (the service implementation). Each &lt;em&gt;Service&lt;/em&gt; uses LlamaIndex base abstractions instead of specific implementations, decoupling the actual implementation from its usage.&lt;/li&gt; &#xA; &lt;li&gt;Components are placed in &lt;code&gt;private_gpt:components:&amp;lt;component&amp;gt;&lt;/code&gt;. Each &lt;em&gt;Component&lt;/em&gt; is in charge of providing actual implementations to the base abstractions used in the Services - for example &lt;code&gt;LLMComponent&lt;/code&gt; is in charge of providing an actual implementation of an &lt;code&gt;LLM&lt;/code&gt; (for example &lt;code&gt;LlamaCPP&lt;/code&gt; or &lt;code&gt;OpenAI&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ’¡ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcomed! To ensure code quality we have enabled several format and typing checks, just run &lt;code&gt;make check&lt;/code&gt; before committing to make sure your code is ok. Remember to test your code! You&#39;ll find a tests folder with helpers, and you can run tests using &lt;code&gt;make test&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t know what to contribute? Here is the public &lt;a href=&#34;https://github.com/users/imartinez/projects/3&#34;&gt;Project Board&lt;/a&gt; with several ideas.&lt;/p&gt; &#xA;&lt;p&gt;Head over to Discord #contributors channel and ask for write permissions on that GitHub project.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ’¬ Community&lt;/h2&gt; &#xA;&lt;p&gt;Join the conversation around PrivateGPT on our:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/PrivateGPT_AI&#34;&gt;Twitter (aka X)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/bK6mRVpErU&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“– Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use PrivateGPT in a paper, check out the &lt;a href=&#34;https://raw.githubusercontent.com/zylon-ai/private-gpt/main/CITATION.cff&#34;&gt;Citation file&lt;/a&gt; for the correct citation.&lt;br&gt; You can also use the &#34;Cite this repository&#34; button in this repo to get the citation in different formats.&lt;/p&gt; &#xA;&lt;p&gt;Here are a couple of examples:&lt;/p&gt; &#xA;&lt;h4&gt;BibTeX&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{Martinez_Toro_PrivateGPT_2023,&#xA;author = {MartÃ­nez Toro, IvÃ¡n and Gallego Vico, Daniel and Orgaz, Pablo},&#xA;license = {Apache-2.0},&#xA;month = may,&#xA;title = {{PrivateGPT}},&#xA;url = {https://github.com/imartinez/privateGPT},&#xA;year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;APA&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;MartÃ­nez Toro, I., Gallego Vico, D., &amp;amp; Orgaz, P. (2023). PrivateGPT [Computer software]. https://github.com/imartinez/privateGPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ¤— Partners &amp;amp; Supporters&lt;/h2&gt; &#xA;&lt;p&gt;PrivateGPT is actively supported by the teams behind:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qdrant.tech/&#34;&gt;Qdrant&lt;/a&gt;, providing the default vector database&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://buildwithfern.com/&#34;&gt;Fern&lt;/a&gt;, providing Documentation and SDKs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;, providing the base RAG framework and abstractions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This project has been strongly influenced and supported by other amazing projects like &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;GPT4All&lt;/a&gt;, &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;LlamaCpp&lt;/a&gt;, &lt;a href=&#34;https://www.trychroma.com/&#34;&gt;Chroma&lt;/a&gt; and &lt;a href=&#34;https://www.sbert.net/&#34;&gt;SentenceTransformers&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>princeton-nlp/SWE-bench</title>
    <updated>2024-03-24T01:52:02Z</updated>
    <id>tag:github.com,2024-03-24:/princeton-nlp/SWE-bench</id>
    <link href="https://github.com/princeton-nlp/SWE-bench" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICLR 2024] SWE-Bench: Can Language Models Resolve Real-world Github Issues?&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/princeton-nlp/Llamao&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/assets/swellama_banner.png&#34; width=&#34;50%&#34; alt=&#34;Kawi the SWE-Llama&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;| &lt;a href=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/docs/README_JP.md&#34;&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench&#34;&gt;English&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Code and data for our ICLR 2024 paper &lt;a href=&#34;http://swe-bench.github.io/paper.pdf&#34;&gt;SWE-bench: Can Language Models Resolve Real-World GitHub Issues?&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://www.python.org/&#34;&gt; &lt;img alt=&#34;Build&#34; src=&#34;https://img.shields.io/badge/Python-3.8+-1f425f.svg?color=purple&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://copyright.princeton.edu/policy&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/License-MIT-blue&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Please refer our &lt;a href=&#34;http://swe-bench.github.io&#34;&gt;website&lt;/a&gt; for the public leaderboard and the &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/CHANGELOG.md&#34;&gt;change log&lt;/a&gt; for information on the latest updates to the SWE-bench benchmark.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ‘‹ Overview&lt;/h2&gt; &#xA;&lt;p&gt;SWE-bench is a benchmark for evaluating large language models on real world software issues collected from GitHub. Given a &lt;em&gt;codebase&lt;/em&gt; and an &lt;em&gt;issue&lt;/em&gt;, a language model is tasked with generating a &lt;em&gt;patch&lt;/em&gt; that resolves the described problem.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/assets/teaser.png&#34;&gt; &#xA;&lt;h2&gt;ğŸš€ Set Up&lt;/h2&gt; &#xA;&lt;p&gt;To build SWE-bench from source, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository locally&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd&lt;/code&gt; into the repository.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;conda env create -f environment.yml&lt;/code&gt; to created a conda environment named &lt;code&gt;swe-bench&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Activate the environment with &lt;code&gt;conda activate swe-bench&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸ’½ Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can download the SWE-bench dataset directly (&lt;a href=&#34;https://drive.google.com/uc?export=download&amp;amp;id=1SbOxHiR0eXlq2azPSSOIDZz-Hva0ETpX&#34;&gt;dev&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/uc?export=download&amp;amp;id=164g55i3_B78F6EphCZGtgSrd2GneFyRM&#34;&gt;test&lt;/a&gt; sets) or from &lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use SWE-Bench, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train your own models on our pre-processed datasets&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/inference/&#34;&gt;inference&lt;/a&gt; on existing models (either models you have on-disk like LLaMA, or models you have access to through an API like GPT-4). The inference step is where you get a repo and an issue and have the model try to generate a fix for it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/harness/&#34;&gt;Evaluate&lt;/a&gt; models against SWE-bench. This is where you take a SWE-Bench task and a model-proposed solution and evaluate its correctness.&lt;/li&gt; &#xA; &lt;li&gt;Run SWE-bench&#39;s &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/collect/&#34;&gt;data collection procedure&lt;/a&gt; on your own repositories, to make new SWE-Bench tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;â¬‡ï¸ Downloads&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datasets&lt;/th&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench&#34;&gt;ğŸ¤— SWE-bench&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-13b&#34;&gt;ğŸ¦™ SWE-Llama 13b&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_oracle&#34;&gt;ğŸ¤— &#34;Oracle&#34; Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-13b-peft&#34;&gt;ğŸ¦™ SWE-Llama 13b (PEFT)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_13K&#34;&gt;ğŸ¤— BM25 Retrieval 13K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-7b&#34;&gt;ğŸ¦™ SWE-Llama 7b&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_27K&#34;&gt;ğŸ¤— BM25 Retrieval 27K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-7b-peft&#34;&gt;ğŸ¦™ SWE-Llama 7b (PEFT)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_40K&#34;&gt;ğŸ¤— BM25 Retrieval 40K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_50k_llama&#34;&gt;ğŸ¤— BM25 Retrieval 50K (Llama tokens)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ğŸ Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve also written the following blog posts on how to use different parts of SWE-bench. If you&#39;d like to see a post about a particular topic, please let us know via an issue.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Nov 1. 2023] Collecting Evaluation Tasks for SWE-Bench (&lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/tree/main/tutorials/collection.md&#34;&gt;ğŸ”—&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[Nov 6. 2023] Evaluating on SWE-bench (&lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/tree/main/tutorials/evaluation.md&#34;&gt;ğŸ”—&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ’« Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We would love to hear from the broader NLP, Machine Learning, and Software Engineering research communities, and we welcome any contributions, pull requests, or issues! To do so, please either file a new pull request or issue and fill in the corresponding templates accordingly. We&#39;ll be sure to follow up shortly!&lt;/p&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;http://www.carlosejimenez.com/&#34;&gt;Carlos E. Jimenez&lt;/a&gt; and &lt;a href=&#34;https://john-b-yang.github.io/&#34;&gt;John Yang&lt;/a&gt; (Email: {carlosej, jy1682}@princeton.edu).&lt;/p&gt; &#xA;&lt;h2&gt;âœï¸ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please use the following citations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{&#xA;    jimenez2024swebench,&#xA;    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},&#xA;    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},&#xA;    booktitle={The Twelfth International Conference on Learning Representations},&#xA;    year={2024},&#xA;    url={https://openreview.net/forum?id=VTF8yNQM66}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸªª License&lt;/h2&gt; &#xA;&lt;p&gt;MIT. Check &lt;code&gt;LICENSE.md&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>