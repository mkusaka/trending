<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-14T02:04:40Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ali-vilab/dreamtalk</title>
    <updated>2024-01-14T02:04:40Z</updated>
    <id>tag:github.com,2024-01-14:/ali-vilab/dreamtalk</id>
    <link href="https://github.com/ali-vilab/dreamtalk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementations for paper: DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;center&#34;&gt;DreamTalk: When Expressive Talking Head Generation &lt;br&gt; Meets Diffusion Probabilistic Models&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://dreamtalk-project.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.09767&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/VF4vlE6ZqWQ&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ali-vilab/dreamtalk/main/media/teaser.gif&#34; alt=&#34;teaser&#34; title=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;DreamTalk is a diffusion-based audio-driven expressive talking head generation framework that can produce high-quality talking head videos across diverse speaking styles. DreamTalk exhibits robust performance with a diverse array of inputs, including songs, speech in multiple languages, noisy audio, and out-of-domain portraits.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; Release inference code and pretrained checkpoint.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n dreamtalk python=3.7.0&#xA;conda activate dreamtalk&#xA;pip install -r requirements.txt&#xA;conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;conda update ffmpeg&#xA;&#xA;pip install urllib3==1.26.6&#xA;pip install transformers==4.28.1&#xA;pip install dlib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;In light of the social impact, we have ceased public download access to checkpoints. If you want to obtain the checkpoints, please request it by emailing &lt;a href=&#34;mailto:mayf18@mails.tsinghua.edu.cn&#34;&gt;mayf18@mails.tsinghua.edu.cn&lt;/a&gt; . It is important to note that sending this email implies your consent to use the provided method &lt;strong&gt;solely for academic research purposes&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Put the downloaded checkpoints into &lt;code&gt;checkpoints&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Run the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_for_demo_video.py \&#xA;--wav_path data/audio/acknowledgement_english.m4a \&#xA;--style_clip_path data/style_clip/3DMM/M030_front_neutral_level1_001.mat \&#xA;--pose_path data/pose/RichardShelby_front_neutral_level1_001.mat \&#xA;--image_path data/src_img/uncropped/male_face.png \&#xA;--cfg_scale 1.0 \&#xA;--max_gen_len 30 \&#xA;--output_name acknowledgement_english@M030_front_neutral_level1_001@male_face&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;wav_path&lt;/code&gt; specifies the input audio. The input audio file extensions such as wav, mp3, m4a, and mp4 (video with sound) should all be compatible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;style_clip_path&lt;/code&gt; specifies the reference speaking style and &lt;code&gt;pose_path&lt;/code&gt; specifies head pose. They are 3DMM parameter sequences extracted from reference videos. You can follow &lt;a href=&#34;https://github.com/RenYurui/PIRender&#34;&gt;PIRenderer&lt;/a&gt; to extract 3DMM parameters from your own videos. Note that the video frame rate should be 25 FPS. Besides, videos used for head pose reference should be first cropped to $256\times256$ using scripts in &lt;a href=&#34;https://github.com/AliaksandrSiarohin/video-preprocessing&#34;&gt;FOMM video preprocessing&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;image_path&lt;/code&gt; specifies the input portrait. Its resolution should be larger than $256\times256$. Frontal portraits, with the face directly facing forward and not tilted to one side, usually achieve satisfactory results. The input portrait will be cropped to $256\times256$. If your portrait is already cropped to $256\times256$ and you want to disable cropping, use option &lt;code&gt;--disable_img_crop&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_for_demo_video.py \&#xA;--wav_path data/audio/acknowledgement_chinese.m4a \&#xA;--style_clip_path data/style_clip/3DMM/M030_front_surprised_level3_001.mat \&#xA;--pose_path data/pose/RichardShelby_front_neutral_level1_001.mat \&#xA;--image_path data/src_img/cropped/zp1.png \&#xA;--disable_img_crop \&#xA;--cfg_scale 1.0 \&#xA;--max_gen_len 30 \&#xA;--output_name acknowledgement_chinese@M030_front_surprised_level3_001@zp1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;cfg_scale&lt;/code&gt; controls the scale of classifer-free guidance. It can adjust the intensity of speaking styles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;max_gen_len&lt;/code&gt; is the maximum video generation duration, measured in seconds. If the input audio exceeds this length, it will be truncated.&lt;/p&gt; &#xA;&lt;p&gt;The generated video will be named &lt;code&gt;$(output_name).mp4&lt;/code&gt; and put in the output_video folder. Intermediate results, including the cropped portrait, will be in the &lt;code&gt;tmp/$(output_name)&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Sample inputs are presented in &lt;code&gt;data&lt;/code&gt; folder. Due to copyright issues, we are unable to include the songs we have used in this folder.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run this program on CPU, please add &lt;code&gt;--device=cpu&lt;/code&gt; to the command line arguments. (Thank &lt;a href=&#34;https://github.com/lukevs&#34;&gt;lukevs&lt;/a&gt; for adding CPU support.)&lt;/p&gt; &#xA;&lt;h2&gt;Ad-hoc solutions to improve resolution&lt;/h2&gt; &#xA;&lt;p&gt;The main goal of this method is to achieve accurate lip-sync and produce vivid expressions across diverse speaking styles. The resolution was not considered in the initial design process. There are two ad-hoc solutions to improve resolution. The first option is to utilize &lt;a href=&#34;https://github.com/sczhou/CodeFormer&#34;&gt;CodeFormer&lt;/a&gt;, which can achieve a resolution of $1024\times1024$; however, it is relatively slow, processing only one frame per second on an A100 GPU, and suffers from issues with temporal inconsistency. The second option is to employ the Temporal Super-Resolution Model from &lt;a href=&#34;https://github.com/Meta-Portrait/MetaPortrait&#34;&gt;MetaPortrait&lt;/a&gt;, which attains a resolution of $512\times512$, offers a faster performance of 10 frames per second, and maintains temporal coherence. However, these super-resolution modules may reduce the intensity of facial emotions.&lt;/p&gt; &#xA;&lt;p&gt;The sample results after super-resolution processing are in the &lt;code&gt;output_video&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We extend our heartfelt thanks for the invaluable contributions made by preceding works to the development of DreamTalk. This includes, but is not limited to: &lt;a href=&#34;https://github.com/RenYurui/PIRender&#34;&gt;PIRenderer&lt;/a&gt; ,&lt;a href=&#34;https://github.com/FuxiVirtualHuman/AAAI22-one-shot-talking-face&#34;&gt;AVCT&lt;/a&gt; ,&lt;a href=&#34;https://github.com/FuxiVirtualHuman/styletalk&#34;&gt;StyleTalk&lt;/a&gt; ,&lt;a href=&#34;https://github.com/sicxu/Deep3DFaceRecon_pytorch&#34;&gt;Deep3DFaceRecon_pytorch&lt;/a&gt; ,&lt;a href=&#34;https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english&#34;&gt;Wav2vec2.0&lt;/a&gt; ,&lt;a href=&#34;https://github.com/luost26/diffusion-point-cloud&#34;&gt;diffusion-point-cloud&lt;/a&gt; ,&lt;a href=&#34;https://github.com/AliaksandrSiarohin/video-preprocessing&#34;&gt;FOMM video preprocessing&lt;/a&gt;. We are dedicated to advancing upon these foundational works with the utmost respect for their original contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this codebase useful for your research, please use the following entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{ma2023dreamtalk,&#xA;  title={DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models},&#xA;  author={Ma, Yifeng and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Zhang, Yingya and Deng, Zhidong},&#xA;  journal={arXiv preprint arXiv:2312.09767},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This method is intended for &lt;strong&gt;RESEARCH/NON-COMMERCIAL USE ONLY&lt;/strong&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MarkFzp/act-plus-plus</title>
    <updated>2024-01-14T02:04:40Z</updated>
    <id>tag:github.com,2024-01-14:/MarkFzp/act-plus-plus</id>
    <link href="https://github.com/MarkFzp/act-plus-plus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Imitation learning algorithms with Co-training for Mobile ALOHA: ACT, Diffusion Policy, VINN&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Imitation Learning algorithms and Co-training for Mobile ALOHA&lt;/h1&gt; &#xA;&lt;h4&gt;Project Website: &lt;a href=&#34;https://mobile-aloha.github.io/&#34;&gt;https://mobile-aloha.github.io/&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This repo contains the implementation of ACT, Diffusion Policy and VINN, together with 2 simulated environments: Transfer Cube and Bimanual Insertion. You can train and evaluate them in sim or real. For real, you would also need to install &lt;a href=&#34;https://github.com/MarkFzp/mobile-aloha&#34;&gt;Mobile ALOHA&lt;/a&gt;. This repo is forked from the &lt;a href=&#34;https://github.com/tonyzhaozh/act&#34;&gt;ACT repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Updates:&lt;/h3&gt; &#xA;&lt;p&gt;You can find all scripted/human demo for simulated environments &lt;a href=&#34;https://drive.google.com/drive/folders/1gPR03v05S1xiInoVJn7G7VJ9pDCnxq9O?usp=share_link&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Repo Structure&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;imitate_episodes.py&lt;/code&gt; Train and Evaluate ACT&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;policy.py&lt;/code&gt; An adaptor for ACT policy&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;detr&lt;/code&gt; Model definitions of ACT, modified from DETR&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sim_env.py&lt;/code&gt; Mujoco + DM_Control environments with joint space control&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ee_sim_env.py&lt;/code&gt; Mujoco + DM_Control environments with EE space control&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scripted_policy.py&lt;/code&gt; Scripted policies for sim environments&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;constants.py&lt;/code&gt; Constants shared across files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utils.py&lt;/code&gt; Utils such as data loading and helper functions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;visualize_episodes.py&lt;/code&gt; Save videos from a .hdf5 dataset&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n aloha python=3.8.10&#xA;conda activate aloha&#xA;pip install torchvision&#xA;pip install torch&#xA;pip install pyquaternion&#xA;pip install pyyaml&#xA;pip install rospkg&#xA;pip install pexpect&#xA;pip install mujoco==2.3.7&#xA;pip install dm_control==1.0.14&#xA;pip install opencv-python&#xA;pip install matplotlib&#xA;pip install einops&#xA;pip install packaging&#xA;pip install h5py&#xA;pip install ipython&#xA;cd act/detr &amp;amp;&amp;amp; pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;also need to install &lt;a href=&#34;https://github.com/ARISE-Initiative/robomimic/tree/r2d2&#34;&gt;https://github.com/ARISE-Initiative/robomimic/tree/r2d2&lt;/a&gt; (note the r2d2 branch) for Diffusion Policy by &lt;code&gt;pip install -e .&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example Usages&lt;/h3&gt; &#xA;&lt;p&gt;To set up a new terminal, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate aloha&#xA;cd &amp;lt;path to act repo&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Simulated experiments (LEGACY table-top ALOHA environments)&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;code&gt;sim_transfer_cube_scripted&lt;/code&gt; task in the examples below. Another option is &lt;code&gt;sim_insertion_scripted&lt;/code&gt;. To generated 50 episodes of scripted data, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 record_sim_episodes.py --task_name sim_transfer_cube_scripted --dataset_dir &amp;lt;data save dir&amp;gt; --num_episodes 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To can add the flag &lt;code&gt;--onscreen_render&lt;/code&gt; to see real-time rendering. To visualize the simulated episodes after it is collected, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 visualize_episodes.py --dataset_dir &amp;lt;data save dir&amp;gt; --episode_idx 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: to visualize data from the mobile-aloha hardware, use the visualize_episodes.py from &lt;a href=&#34;https://github.com/MarkFzp/mobile-aloha&#34;&gt;https://github.com/MarkFzp/mobile-aloha&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To train ACT:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Transfer Cube task&#xA;python3 imitate_episodes.py --task_name sim_transfer_cube_scripted --ckpt_dir &amp;lt;ckpt dir&amp;gt; --policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --batch_size 8 --dim_feedforward 3200 --num_epochs 2000  --lr 1e-5 --seed 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To evaluate the policy, run the same command but add &lt;code&gt;--eval&lt;/code&gt;. This loads the best validation checkpoint. The success rate should be around 90% for transfer cube, and around 50% for insertion. To enable temporal ensembling, add flag &lt;code&gt;--temporal_agg&lt;/code&gt;. Videos will be saved to &lt;code&gt;&amp;lt;ckpt_dir&amp;gt;&lt;/code&gt; for each rollout. You can also add &lt;code&gt;--onscreen_render&lt;/code&gt; to see real-time rendering during evaluation.&lt;/p&gt; &#xA;&lt;p&gt;For real-world data where things can be harder to model, train for at least 5000 epochs or 3-4 times the length after the loss has plateaued. Please refer to &lt;a href=&#34;https://docs.google.com/document/d/1FVIZfoALXg_ZkYKaYVh-qOlaXveq5CtvJHXkY25eYhs/edit?usp=sharing&#34;&gt;tuning tips&lt;/a&gt; for more info.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://docs.google.com/document/d/1FVIZfoALXg_ZkYKaYVh-qOlaXveq5CtvJHXkY25eYhs/edit?usp=sharing&#34;&gt;ACT tuning tips&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;TL;DR: if your ACT policy is jerky or pauses in the middle of an episode, just train for longer! Success rate and smoothness can improve way after loss plateaus.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>embedchain/embedchain</title>
    <updated>2024-01-14T02:04:40Z</updated>
    <id>tag:github.com,2024-01-14:/embedchain/embedchain</id>
    <link href="https://github.com/embedchain/embedchain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Open Source RAG framework&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/embedchain/embedchain/main/docs/logo/dark.svg?sanitize=true&#34; width=&#34;400px&#34; alt=&#34;Embedchain Logo&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://runacap.com/ross-index/q3-2023/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img style=&#34;width: 260px; height: 56px&#34; src=&#34;https://runacap.com/wp-content/uploads/2023/10/ROSS_badge_black_Q3_2023.svg?sanitize=true&#34; alt=&#34;ROSS Index - Fastest Growing Open-Source Startups in Q3 2023 | Runa Capital&#34; width=&#34;260&#34; height=&#34;56&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/embedchain/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/embedchain&#34; alt=&#34;PyPI&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/embedchain&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/badge/embedchain&#34; alt=&#34;Downloads&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://embedchain.ai/slack&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/slack-embedchain-brightgreen.svg?logo=slack&#34; alt=&#34;Slack&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://embedchain.ai/discord&#34;&gt; &lt;img src=&#34;https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/embedchain&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/embedchain&#34; alt=&#34;Twitter&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/138lMWhENGeEu7Q1-6lNbNTHGLZXBBz_B?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/embedchain/embedchain&#34;&gt; &lt;img src=&#34;https://codecov.io/gh/embedchain/embedchain/graph/badge.svg?token=EMRRHZXW1Q&#34; alt=&#34;codecov&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Checkout our latest &lt;a href=&#34;https://sadhguru-ai.streamlit.app/&#34;&gt;Sadhguru AI app&lt;/a&gt; built using Embedchain.&lt;/h3&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;What is Embedchain?&lt;/h2&gt; &#xA;&lt;p&gt;Embedchain is an Open Source RAG Framework that makes it easy to create and deploy AI apps. At its core, Embedchain follows the design principle of being &lt;em&gt;&#34;Conventional but Configurable&#34;&lt;/em&gt; to serve both software engineers and machine learning engineers.&lt;/p&gt; &#xA;&lt;p&gt;Embedchain streamlines the creation of Retrieval-Augmented Generation (RAG) applications, offering a seamless process for managing various types of unstructured data. It efficiently segments data into manageable chunks, generates relevant embeddings, and stores them in a vector database for optimized retrieval. With a suite of diverse APIs, it enables users to extract contextual information, find precise answers, or engage in interactive chat conversations, all tailored to their own data.&lt;/p&gt; &#xA;&lt;h2&gt;üîß Quick install&lt;/h2&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install embedchain&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚ú® Live demo&lt;/h2&gt; &#xA;&lt;p&gt;Checkout the &lt;a href=&#34;https://embedchain.ai/demo/chat-pdf&#34;&gt;Chat with PDF&lt;/a&gt; live demo we created using Embedchain. You can find the source code &lt;a href=&#34;https://github.com/embedchain/embedchain/tree/main/examples/chat-pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üîç Usage&lt;/h2&gt; &#xA;&lt;!-- Demo GIF or Image --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/embedchain/embedchain/main/docs/images/cover.gif&#34; width=&#34;900px&#34; alt=&#34;Embedchain Demo&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;For example, you can create an Elon Musk bot using the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from embedchain import Pipeline as App&#xA;&#xA;# Create a bot instance&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;YOUR API KEY&#34;&#xA;elon_bot = App()&#xA;&#xA;# Embed online resources&#xA;elon_bot.add(&#34;https://en.wikipedia.org/wiki/Elon_Musk&#34;)&#xA;elon_bot.add(&#34;https://www.forbes.com/profile/elon-musk&#34;)&#xA;&#xA;# Query the bot&#xA;elon_bot.query(&#34;How many companies does Elon Musk run and name those?&#34;)&#xA;# Answer: Elon Musk currently runs several companies. As of my knowledge, he is the CEO and lead designer of SpaceX, the CEO and product architect of Tesla, Inc., the CEO and founder of Neuralink, and the CEO and founder of The Boring Company. However, please note that this information may change over time, so it&#39;s always good to verify the latest updates.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also try it in your browser with Google Colab:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/17ON1LPonnXAtLaZEebnOktstB_1cJJmh?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Comprehensive guides and API documentation are available to help you get the most out of Embedchain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.embedchain.ai/get-started/introduction#what-is-embedchain&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.embedchain.ai/get-started/quickstart&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.embedchain.ai/examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.embedchain.ai/components/data-sources/overview&#34;&gt;Supported data types&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîó Join the Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Connect with fellow developers by joining our &lt;a href=&#34;https://embedchain.ai/slack&#34;&gt;Slack Community&lt;/a&gt; or &lt;a href=&#34;https://embedchain.ai/discord&#34;&gt;Discord Community&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Dive into &lt;a href=&#34;https://github.com/embedchain/embedchain/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;, ask questions, or share your experiences.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ù Schedule a 1-on-1 Session&lt;/h2&gt; &#xA;&lt;p&gt;Book a &lt;a href=&#34;https://cal.com/taranjeetio/ec&#34;&gt;1-on-1 Session&lt;/a&gt; with the founders, to discuss any issues, provide feedback, or explore how we can improve Embedchain for you.&lt;/p&gt; &#xA;&lt;h2&gt;üåê Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Please check out the issues on the repository, and feel free to open a pull request. For more information, please see the &lt;a href=&#34;https://raw.githubusercontent.com/embedchain/embedchain/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more reference, please go through &lt;a href=&#34;https://docs.embedchain.ai/contribution/dev&#34;&gt;Development Guide&lt;/a&gt; and &lt;a href=&#34;https://docs.embedchain.ai/contribution/docs&#34;&gt;Documentation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/embedchain/embedchain/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=embedchain/embedchain&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Anonymous Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;We collect anonymous usage metrics to enhance our package&#39;s quality and user experience. This includes data like feature usage frequency and system info, but never personal details. The data helps us prioritize improvements and ensure compatibility. If you wish to opt-out, set the environment variable &lt;code&gt;EC_TELEMETRY=false&lt;/code&gt;. We prioritize data security and don&#39;t share this data externally.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you utilize this repository, please consider citing it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{embedchain,&#xA;  author = {Taranjeet Singh, Deshraj Yadav},&#xA;  title = {Embedchain: The Open Source RAG Framework},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/embedchain/embedchain}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>