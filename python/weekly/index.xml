<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-12T01:58:22Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yihong0618/bilingual_book_maker</title>
    <updated>2023-03-12T01:58:22Z</updated>
    <id>tag:github.com,2023-03-12:/yihong0618/bilingual_book_maker</id>
    <link href="https://github.com/yihong0618/bilingual_book_maker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make bilingual epub books Using AI translate&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This forked added Google Translate support, only supported translate to &lt;code&gt;zh-CN&lt;/code&gt;. Usage: make sure to add &lt;code&gt;--model google&lt;/code&gt; in the command.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yihong0618/bilingual_book_maker/main/README-CN.md&#34;&gt;中文&lt;/a&gt; | English&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;bilingual_book_maker&lt;/h1&gt; &#xA;&lt;p&gt;The bilingual_book_maker is an AI translation tool that uses ChatGPT to assist users in creating multi-language versions of epub/txt files and books. This tool is exclusively designed for translating epub books that have entered the public domain and is not intended for copyrighted works. Before using this tool, please review the project&#39;s &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yihong0618/bilingual_book_maker/main/disclaimer.md&#34;&gt;disclaimer&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/15976103/222317531-a05317c5-4eee-49de-95cd-04063d9539d9.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Preparation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ChatGPT or OpenAI token &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;^token&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;epub/txt books&lt;/li&gt; &#xA; &lt;li&gt;Environment with internet access or proxy&lt;/li&gt; &#xA; &lt;li&gt;Python 3.8+&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Use&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use &lt;code&gt;--openai_key&lt;/code&gt; option to specify OpenAI API key. If you have multiple keys, separate them by commas (xxx,xxx,xxx) to reduce errors caused by API call limits.&lt;br&gt; Or, just set environment variable &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to ignore this option.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A sample book, &lt;code&gt;test_books/animal_farm.epub&lt;/code&gt;, is provided for testing purposes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The default underlying model is &lt;a href=&#34;https://openai.com/blog/introducing-chatgpt-and-whisper-apis&#34;&gt;GPT-3.5-turbo&lt;/a&gt;, which is used by ChatGPT currently. Use &lt;code&gt;--model gpt3&lt;/code&gt; to change the underlying model to &lt;code&gt;GPT3&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use &lt;code&gt;--test&lt;/code&gt; option to preview the result if you haven&#39;t paid for the service. Note that there is a limit and it may take some time.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the target language like &lt;code&gt;--language &#34;Simplified Chinese&#34;&lt;/code&gt;. Default target language is &lt;code&gt;&#34;Simplified Chinese&#34;&lt;/code&gt;.&lt;br&gt; Read available languages by helper message: &lt;code&gt;python make_book.py --help&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use &lt;code&gt;--proxy&lt;/code&gt; option to specify proxy server for internet access. Enter a string such as &lt;code&gt;http://127.0.0.1:7890&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use &lt;code&gt;--resume&lt;/code&gt; option to manually resume the process after an interruption.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;epub is made of html files. By default, we only translate contents in &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt;. Use &lt;code&gt;--translate-tags&lt;/code&gt; to specify tags need for translation. Use comma to seperate multiple tags. For example: &lt;code&gt;--translate-tags h1,h2,h3,p,div&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use &lt;code&gt;--book_from&lt;/code&gt; option to specify e-reader type (Now only &lt;code&gt;kobo&lt;/code&gt; is available), and use &lt;code&gt;--device_path&lt;/code&gt; to specify the mounting point.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to change api_base like using Cloudflare Workers, use &lt;code&gt;--api_base &amp;lt;URL&amp;gt;&lt;/code&gt; to support it.&lt;br&gt; &lt;strong&gt;Note: the api url should be &#39;&lt;code&gt;https://xxxx/v1&lt;/code&gt;&#39;. Quotation marks are required.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once the translation is complete, a bilingual book named &lt;code&gt;${book_name}_bilingual.epub&lt;/code&gt; would be generated.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If there are any errors or you wish to interrupt the translation by pressing &lt;code&gt;CTRL+C&lt;/code&gt;. A book named &lt;code&gt;${book_name}_bilingual_temp.epub&lt;/code&gt; would be generated. You can simply rename it to any desired name.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to translate strings in an e-book that aren&#39;t labeled with any tags, you can use the &lt;code&gt;--allow_navigable_strings&lt;/code&gt; parameter. This will add the strings to the translation queue. &lt;strong&gt;Note that it&#39;s best to look for e-books that are more standardized if possible.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To tweak the prompt, use the &lt;code&gt;--prompt&lt;/code&gt; parameter. The parameter can be a prompt template string or a path to the template &lt;code&gt;.txt&lt;/code&gt; file. Valid placeholders for the template include &lt;code&gt;{text}&lt;/code&gt; and &lt;code&gt;{language}&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once the translation is complete, a bilingual book named &lt;code&gt;${book_name}_bilingual.epub&lt;/code&gt; would be generated.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If there are any errors or you wish to interrupt the translation by pressing &lt;code&gt;CTRL+C&lt;/code&gt;. A book named &lt;code&gt;${book_name}_bilingual_temp.epub&lt;/code&gt; would be generated. You can simply rename it to any desired name.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to translate strings in an e-book that aren&#39;t labeled with any tags, you can use the &lt;code&gt;--allow_navigable_strings&lt;/code&gt; parameter. This will add the strings to the translation queue. &lt;strong&gt;Note that it&#39;s best to look for e-books that are more standardized if possible.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Eamples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Test quickly&#xA;python3 make_book.py --book_name test_books/animal_farm.epub --openai_key ${openai_key}  --test --language zh-hans&#xA;&#xA;# Or translate the whole book&#xA;python3 make_book.py --book_name test_books/animal_farm.epub --openai_key ${openai_key} --language zh-hans&#xA;&#xA;# Set env OPENAI_API_KEY to ignore option --openai_key&#xA;export OPENAI_API_KEY=${your_api_key}&#xA;&#xA;# Use the GPT-3 model with Japanese&#xA;python3 make_book.py --book_name test_books/animal_farm.epub --model gpt3 --language ja&#xA;&#xA;# Translate contents in &amp;lt;div&amp;gt; and &amp;lt;p&amp;gt;&#xA;python3 make_book.py --book_name test_books/animal_farm.epub --translate-tags div,p&#xA;&#xA;# Tweaking the prompt&#xA;python3 make_book.py --book_name test_books/animal_farm.epub --prompt prompt_template_sample.txt&#xA;# or&#xA;python3 make_book.py --book_name test_books/animal_farm.epub --prompt &#34;Please translate \`{text}\` to {language}&#34;&#xA;# Translate books download from Rakuten Kobo on kobo e-reader&#xA;python3 make_book.py --book_from kobo --device_path /tmp/kobo&#xA;&#xA;# translate txt file&#xA;python3 make_book.py --book_name test_books/the_little_prince.txt --test --language zh-hans&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More understandable example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 make_book.py --book_name &#39;animal_farm.epub&#39; --openai_key sk-XXXXX --api_base &#39;https://xxxxx/v1&#39;&#xA;&#xA;# Or python3 is not in your PATH&#xA;python make_book.py --book_name &#39;animal_farm.epub&#39; --openai_key sk-XXXXX --api_base &#39;https://xxxxx/v1&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;You can use &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; if you don&#39;t want to deal with setting up the environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Build image&#xA;docker build --tag bilingual_book_maker .&#xA;&#xA;# Run container&#xA;# &#34;$folder_path&#34; represents the folder where your book file locates. Also, it is where the processed file will be stored.&#xA;&#xA;# Windows PowerShell&#xA;$folder_path=your_folder_path # $folder_path=&#34;C:\Users\user\mybook\&#34;&#xA;$book_name=your_book_name # $book_name=&#34;animal_farm.epub&#34;&#xA;$openai_key=your_api_key # $openai_key=&#34;sk-xxx&#34;&#xA;$language=your_language # see utils.py&#xA;&#xA;docker run --rm --name bilingual_book_maker --mount type=bind,source=$folder_path,target=&#39;/app/test_books&#39; bilingual_book_maker --book_name &#34;/app/test_books/$book_name&#34; --openai_key $openai_key --language $language&#xA;&#xA;# Linux&#xA;export folder_path=${your_folder_path}&#xA;export book_name=${your_book_name}&#xA;export openai_key=${your_api_key}&#xA;export language=${your_language}&#xA;&#xA;docker run --rm --name bilingual_book_maker --mount type=bind,source=${folder_path},target=&#39;/app/test_books&#39; bilingual_book_maker --book_name &#34;/app/test_books/${book_name}&#34; --openai_key ${openai_key} --language &#34;${language}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Linux&#xA;docker run --rm --name bilingual_book_maker --mount type=bind,source=/home/user/my_books,target=&#39;/app/test_books&#39; bilingual_book_maker --book_name /app/test_books/animal_farm.epub --openai_key sk-XXX --test --test_num 1 --language zh-hant&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;API token from free trial has limit. If you want to speed up the process, consider paying for the service or use multiple OpenAI tokens&lt;/li&gt; &#xA; &lt;li&gt;PR is welcome&lt;/li&gt; &#xA; &lt;li&gt;The DeepL model will be updated later.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;@&lt;a href=&#34;https://github.com/yetone&#34;&gt;yetone&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Any issues or PRs are welcome.&lt;/li&gt; &#xA; &lt;li&gt;TODOs in the issue can also be selected.&lt;/li&gt; &#xA; &lt;li&gt;Please run &lt;code&gt;black make_book.py&lt;/code&gt;&lt;a href=&#34;https://github.com/psf/black&#34;&gt;^black&lt;/a&gt; before submitting the code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;p&gt;Thank you, that&#39;s enough.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/15976103/222407199-1ed8930c-13a8-402b-9993-aaac8ee84744.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zhayujie/bot-on-anything</title>
    <updated>2023-03-12T01:58:22Z</updated>
    <id>tag:github.com,2023-03-12:/zhayujie/bot-on-anything</id>
    <link href="https://github.com/zhayujie/bot-on-anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Connect AI models (like ChatGPT-3.5, GPT-3.0) to apps (like Wechat, public account, DingTalk, Telegram, QQ). 将 ChatGPT 等算法模型应用于各类平台，目前已完成命令行、个人微信、公众号、QQ、Telegram、Gmail邮箱、Slack、Web，计划继续接入企业微信、钉钉等。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;简介&lt;/h1&gt; &#xA;&lt;p&gt;将 &lt;strong&gt;AI模型&lt;/strong&gt; 接入各类 &lt;strong&gt;消息应用&lt;/strong&gt;，开发者通过轻量配置即可在二者之间选择一条连线，运行起一个智能对话机器人，在一个项目中轻松完成多条链路的切换。该架构扩展性强，每接入一个应用可复用已有的算法能力，同样每接入一个模型也可作用于所有应用之上。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;模型：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#1-chatgpt&#34;&gt;ChatGPT (gpt-3.5)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#2gpt-30&#34;&gt;GPT-3.0&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;应用：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#1%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%BB%88%E7%AB%AF&#34;&gt;终端&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#9web&#34;&gt;Web&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#2%E4%B8%AA%E4%BA%BA%E5%BE%AE%E4%BF%A1&#34;&gt;个人微信&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#3%E4%B8%AA%E4%BA%BA%E8%AE%A2%E9%98%85%E5%8F%B7&#34;&gt;订阅号&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#4%E4%BC%81%E4%B8%9A%E6%9C%8D%E5%8A%A1%E5%8F%B7&#34;&gt;服务号&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 企业微信&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#6telegram&#34;&gt;Telegram&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#5qq&#34;&gt;QQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 钉钉&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 飞书&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#7gmail&#34;&gt;Gmail&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything#8slack&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;快速开始&lt;/h1&gt; &#xA;&lt;h2&gt;一、准备&lt;/h2&gt; &#xA;&lt;h3&gt;1.运行环境&lt;/h3&gt; &#xA;&lt;p&gt;支持 Linux、MacOS、Windows 系统（Linux服务器上可长期运行)。同时需安装 Python，建议Python版本在 3.7.1~3.10 之间。&lt;/p&gt; &#xA;&lt;p&gt;项目代码克隆：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/zhayujie/bot-on-anything&#xA;cd bot-on-anything/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;或在 Realase 直接手动下载源码。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2.配置说明&lt;/h3&gt; &#xA;&lt;p&gt;核心配置文件为 &lt;code&gt;config.json&lt;/code&gt;，在项目中提供了模板文件 &lt;code&gt;config-template.json&lt;/code&gt; ，可以从模板复制生成最终生效的 &lt;code&gt;config.json&lt;/code&gt; 文件：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp config-template.json config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;每一个模型和应用都有自己的配置块，最终组成完整的配置文件，整体结构如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;{&#xA;  &#34;model&#34;: {&#xA;    &#34;type&#34; : &#34;chatgpt&#34;,             # 选用的算法模型&#xA;    &#34;openai&#34;: {&#xA;      # openAI配置&#xA;    }&#xA;  },&#xA;  &#34;channel&#34;: {&#xA;    &#34;type&#34;: &#34;wechat_mp&#34;,            # 需要接入的应用 &#xA;    &#34;wechat&#34;: {&#xA;        # 个人微信配置&#xA;    },&#xA;    &#34;wechat_mp&#34;: {&#xA;        # 公众号配置&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;配置文件在最外层分成 &lt;code&gt;model&lt;/code&gt; 和 &lt;code&gt;channel&lt;/code&gt; 两部分，model部分为模型配置，其中的 &lt;code&gt;type&lt;/code&gt; 指定了选用哪个模型；channel部分包含了应用渠道的配置，&lt;code&gt;type&lt;/code&gt; 字段指定了接入哪个应用。&lt;/p&gt; &#xA;&lt;p&gt;在使用时只需要更改 model 和 channel 配置块下的 type 字段，即可在任意模型和应用间完成切换，连接不同的通路。下面将依次介绍各个 模型 及 应用 的配置和运行过程。&lt;/p&gt; &#xA;&lt;h2&gt;二、选择模型&lt;/h2&gt; &#xA;&lt;h3&gt;1. ChatGPT&lt;/h3&gt; &#xA;&lt;p&gt;使用的模型是 &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;，详情参考&lt;a href=&#34;https://platform.openai.com/docs/guides/chat&#34;&gt;官方文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;(1) 注册 OpenAI 账号&lt;/h4&gt; &#xA;&lt;p&gt;前往 &lt;a href=&#34;https://beta.openai.com/signup&#34;&gt;OpenAI注册页面&lt;/a&gt; 创建账号，参考这篇 &lt;a href=&#34;https://www.cnblogs.com/damugua/p/16969508.html&#34;&gt;教程&lt;/a&gt; 可以通过虚拟手机号来接收验证码。创建完账号则前往 &lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;API管理页面&lt;/a&gt; 创建一个 API Key 并保存下来，后面需要在项目中配置这个key。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;项目中使用的对话模型是 davinci，计费方式是约每 750 字 (包含请求和回复) 消耗 $0.02，图片生成是每张消耗 $0.016，账号创建有免费的 $18 额度，使用完可以更换邮箱重新注册。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;(2) 安装依赖&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install --upgrade openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注： openai版本需要&lt;code&gt;0.27.0&lt;/code&gt;以上。如果安装失败可先升级pip，&lt;code&gt;pip3 install --upgrade pip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;(3) 配置项说明&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;{&#xA;  &#34;model&#34;: {&#xA;    &#34;type&#34; : &#34;chatgpt&#34;,&#xA;   &#xA;    &#34;openai&#34;: {&#xA;      &#34;api_key&#34;: &#34;YOUR API KEY&#34;,&#xA;      &#34;proxy&#34;: &#34;http://127.0.0.1:7890&#34;,&#xA;      &#34;character_desc&#34;: &#34;你是ChatGPT, 一个由OpenAI训练的大型语言模型, 你旨在回答并解决人们的任何问题，并且可以使用多种语言与人交流。&#34;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;api_key&lt;/code&gt;: 填入上面注册账号时创建的 &lt;code&gt;OpenAI API KEY&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;proxy&lt;/code&gt;: 代理客户端的地址，详情参考 &lt;a href=&#34;https://github.com/zhayujie/bot-on-anything/issues/56&#34;&gt;#56&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;character_desc&lt;/code&gt;: 配置中保存着你对chatgpt说的一段话，他会记住这段话并作为他的设定，你可以为他定制任何人格&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.GPT-3.0&lt;/h3&gt; &#xA;&lt;p&gt;使用的模型是 &lt;code&gt;text-davinci-003&lt;/code&gt;，详情参考&lt;a href=&#34;https://raw.githubusercontent.com/zhayujie/bot-on-anything/master/%5Bhttps://platform.openai.com/docs/guides/chat%5D(https://platform.openai.com/docs/guides/completion/introduction)&#34;&gt;官方文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;使用步骤和上述GPT-3.5基本相同：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;注册OpenAI账号并配置API KEY&lt;/li&gt; &#xA; &lt;li&gt;安装openai依赖，要求版本高于 &lt;code&gt;0.25.0&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;修改&lt;code&gt;config.json&lt;/code&gt;配置中的type字段为 &lt;code&gt;openai&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;{&#xA;  &#34;model&#34;: {&#xA;    &#34;type&#34; : &#34;openai&#34;,&#xA;   &#xA;    &#34;openai&#34;: {&#xA;      &#34;api_key&#34;: &#34;YOUR API KEY&#34;,&#xA;      &#34;character_desc&#34;: &#34;你是ChatGPT, 一个由OpenAI训练的大型语言模型, 你旨在回答并解决人们的任何问题，并且可以使用多种语言与人交流。&#34;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;三、选择应用&lt;/h2&gt; &#xA;&lt;h3&gt;1.命令行终端&lt;/h3&gt; &#xA;&lt;p&gt;配置模板中默认启动的应用即是终端，无需任何额外配置，直接在项目目录下通过命令行执行 &lt;code&gt;python3 app.py&lt;/code&gt; 便可启动程序。用户通过命令行的输入与对话模型交互，且支持流式响应效果。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zhayujie/bot-on-anything/master/docs/images/terminal_demo.png&#34; alt=&#34;terminal_demo.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2.个人微信&lt;/h3&gt; &#xA;&lt;p&gt;与项目 &lt;a href=&#34;https://github.com/zhayujie/chatgpt-on-wechat&#34;&gt;chatgpt-on-wechat&lt;/a&gt; 的使用方式相同，目前接入个人微信可能导致账号被限制，暂时不建议使用。&lt;/p&gt; &#xA;&lt;p&gt;配置项说明：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#34;channel&#34;: {&#xA;    &#34;type&#34;: &#34;wechat&#34;,&#xA;    &#xA;    &#34;single_chat_prefix&#34;: [&#34;bot&#34;, &#34;@bot&#34;],&#xA;    &#34;single_chat_reply_prefix&#34;: &#34;[bot] &#34;,&#xA;    &#34;group_chat_prefix&#34;: [&#34;@bot&#34;],&#xA;    &#34;group_name_white_list&#34;: [&#34;ChatGPT测试群&#34;],&#xA;    &#34;image_create_prefix&#34;: [&#34;画&#34;, &#34;看&#34;, &#34;找一张&#34;],&#xA;    &#xA;    &#34;wechat&#34;: {&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;个人微信的配置项放在和 &lt;code&gt;type&lt;/code&gt; 同级的层次，表示这些为公共配置，会复用于其他应用。配置加载时会优先使用模块内的配置，如果未找到便使用公共配置。&lt;/p&gt; &#xA;&lt;p&gt;在项目根目录下执行 &lt;code&gt;python3 app.py&lt;/code&gt; 即可启动程序，用手机扫码后完成登录，使用详情参考 &lt;a href=&#34;https://github.com/zhayujie/chatgpt-on-wechat&#34;&gt;chatgpt-on-wechat&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;3.个人订阅号&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;需要：&lt;/strong&gt; 一台服务器，一个订阅号&lt;/p&gt; &#xA;&lt;h4&gt;3.1 依赖安装&lt;/h4&gt; &#xA;&lt;p&gt;安装 &lt;a href=&#34;https://github.com/offu/WeRoBot&#34;&gt;werobot&lt;/a&gt; 依赖：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install werobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3.2 配置&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#34;channel&#34;: {&#xA;    &#34;type&#34;: &#34;wechat_mp&#34;,&#xA;        &#xA;    &#34;wechat_mp&#34;: {&#xA;      &#34;token&#34;: &#34;YOUR TOKEN&#34;,           # token值&#xA;      &#34;port&#34;: &#34;8088&#34;                   # 程序启动监听的端口&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3.3 运行程序&lt;/h4&gt; &#xA;&lt;p&gt;在项目目录下运行 &lt;code&gt;python3 app.py&lt;/code&gt;，终端显示如下则表示已成功运行：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[INFO][2023-02-16 01:39:53][app.py:12] - [INIT] load config: ...&#xA;[INFO][2023-02-16 01:39:53][wechat_mp_channel.py:25] - [WX_Public] Wechat Public account service start!&#xA;Bottle v0.12.23 server starting up (using AutoServer())...&#xA;Listening on http://127.0.0.1:8088/&#xA;Hit Ctrl-C to quit.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2.2 设置公众号回调地址&lt;/h4&gt; &#xA;&lt;p&gt;在 &lt;a href=&#34;https://mp.weixin.qq.com/&#34;&gt;微信公众平台&lt;/a&gt; 中进入个人订阅号，启用服务器配置：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zhayujie/bot-on-anything/master/docs/images/wx_mp_config.png&#34; alt=&#34;wx_mp_config.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;服务器地址 (URL) 配置&lt;/strong&gt;： 如果在浏览器上通过配置的URL 能够访问到服务器上的Python程序 (默认监听8088端口)，则说明配置有效。由于公众号只能配置 80/443端口，可以修改配置为直接监听 80 端口 (需要sudo权限)，或者使用反向代理进行转发 (如nginx)。 根据官方文档说明，此处填写公网ip或域名均可。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;令牌 (Token) 配置&lt;/strong&gt;：需和 &lt;code&gt;config.json&lt;/code&gt; 配置中的token一致。&lt;/p&gt; &#xA;&lt;p&gt;详细操作过程参考 &lt;a href=&#34;https://developers.weixin.qq.com/doc/offiaccount/Getting_Started/Getting_Started_Guide.html&#34;&gt;官方文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3 使用&lt;/h4&gt; &#xA;&lt;p&gt;用户关注订阅号后，发送消息即可。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注：用户发送消息后，微信后台会向配置的URL地址推送，但如果5s内未回复就会断开连接，同时重试3次，但往往请求openai接口不止5s。本项目中通过异步和缓存将5s超时限制优化至15s，但超出该时间仍无法正常回复。 同时每次5s连接断开时web框架会报错，待后续优化。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;4.企业服务号&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;需要：&lt;/strong&gt; 一个服务器、一个已微信认证的服务号&lt;/p&gt; &#xA;&lt;p&gt;在企业服务号中，通过先异步访问openai接口，再通过客服接口主动推送给用户的方式，解决了个人订阅号的15s超时问题。服务号的开发者模式配置和上述订阅号类似，详情参考 &lt;a href=&#34;https://developers.weixin.qq.com/doc/offiaccount/Getting_Started/Getting_Started_Guide.html&#34;&gt;官方文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;企业服务号的 &lt;code&gt;config.json&lt;/code&gt; 配置只需修改type为&lt;code&gt;wechat_mp_service&lt;/code&gt;，但配置块仍复用 &lt;code&gt;wechat_mp&lt;/code&gt;，在此基础上需要增加 &lt;code&gt;app_id&lt;/code&gt; 和 &lt;code&gt;app_secret&lt;/code&gt; 两个配置项。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#34;channel&#34;: {&#xA;    &#34;type&#34;: &#34;wechat_mp_service&#34;,&#xA;        &#xA;    &#34;wechat_mp&#34;: {&#xA;      &#34;token&#34;: &#34;YOUR TOKEN&#34;,            # token值&#xA;      &#34;port&#34;: &#34;8088&#34;,                   # 程序启动监听的端口&#xA;      &#34;app_id&#34;: &#34;YOUR APP ID&#34;,          # app ID&#xA;      &#34;app_secret&#34;: &#34;YOUR APP SECRET&#34;   # app secret&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注意：需将服务器ip地址配置在 &#34;IP白名单&#34; 内，否则用户将收不到主动推送的消息。&lt;/p&gt; &#xA;&lt;h3&gt;5.QQ&lt;/h3&gt; &#xA;&lt;p&gt;需要：一台PC或服务器 (国内网络)、一个QQ号&lt;/p&gt; &#xA;&lt;p&gt;运行qq机器人 需要额外运行一个&lt;code&gt;go-cqhttp&lt;/code&gt; 程序，cqhttp程序负责接收和发送qq消息， 我们的&lt;code&gt;bot-on-anything&lt;/code&gt;程序负责访问&lt;code&gt;openai&lt;/code&gt;生成对话内容。&lt;/p&gt; &#xA;&lt;h4&gt;5.1 下载 go-cqhttp&lt;/h4&gt; &#xA;&lt;p&gt;在 &lt;a href=&#34;https://github.com/Mrs4s/go-cqhttp/releases&#34;&gt;go-cqhttp的Release&lt;/a&gt; 中下载对应机器的程序，解压后将 &lt;code&gt;go-cqhttp&lt;/code&gt; 二进制文件放置在我们的 &lt;code&gt;bot-on-anything/channel/qq&lt;/code&gt; 目录下。 同时这里已经准备好了一个 &lt;code&gt;config.yml&lt;/code&gt; 配置文件，仅需要填写其中的 QQ 账号配置 (account-uin)。&lt;/p&gt; &#xA;&lt;h4&gt;5.2 安装 aiocqhttp&lt;/h4&gt; &#xA;&lt;p&gt;使用 &lt;a href=&#34;https://github.com/nonebot/aiocqhttp&#34;&gt;aiocqhttp&lt;/a&gt; 来与 go-cqhttp 交互， 执行以下语句安装依赖：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install aiocqhttp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;5.3 配置&lt;/h4&gt; &#xA;&lt;p&gt;只需修改 &lt;code&gt;config.json&lt;/code&gt; 配置文件 channel 块中的 type 为 &lt;code&gt;qq&lt;/code&gt;：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#34;channel&#34;: {&#xA;    &#34;type&#34;: &#34;qq&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;5.4 运行&lt;/h4&gt; &#xA;&lt;p&gt;首先进入 &lt;code&gt;bot-on-anything&lt;/code&gt; 项目根目录，在 终端1 运行：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 app.py    # 此时会监听8080端口&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;第二步打开 终端2，进入到放置 &lt;code&gt;cqhttp&lt;/code&gt; 的目录并运行：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd channel/qq&#xA;./go-cqhttp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注意：目前未设置任何 关键词匹配 及 群聊白名单，对所有私聊均会自动回复，在群聊中只要被@也会自动回复。&lt;/p&gt; &#xA;&lt;h3&gt;6.Telegram&lt;/h3&gt; &#xA;&lt;p&gt;Contributor: &lt;a href=&#34;https://github.com/brucelt1993&#34;&gt;brucelt1993&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;6.1 获取token&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;telegram 机器人申请可以自行谷歌下，很简单，重要的是获取机器人的token id。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;6.2 依赖安装&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;pip install pyTelegramBotAPI&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;6.3 配置&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#34;channel&#34;: {&#xA;    &#34;type&#34;: &#34;telegram&#34;,&#xA;    &#34;telegram&#34;:{&#xA;      &#34;bot_token&#34;: &#34;YOUR BOT TOKEN ID&#34;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;7.Gmail&lt;/h3&gt; &#xA;&lt;p&gt;需要: 一个服务器、一个Gmail account&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contributor:&lt;/strong&gt; &lt;a href=&#34;https://github.com/413675377&#34;&gt;Simon&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://support.google.com/mail/answer/185833?hl=en&#34;&gt;官方文档&lt;/a&gt; to create APP password for google account, config as below, then cheers!!!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#34;channel&#34;: {&#xA;    &#34;type&#34;: &#34;gmail&#34;,&#xA;    &#34;gmail&#34;: {&#xA;      &#34;subject_keyword&#34;: [&#34;bot&#34;, &#34;@bot&#34;],&#xA;      &#34;host_email&#34;: &#34;xxxx@gmail.com&#34;,&#xA;      &#34;host_password&#34;: &#34;GMAIL ACCESS KEY&#34;&#xA;    }&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;8.Slack&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;需要：&lt;/strong&gt; 服务器、 Slack 应用&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contributor:&lt;/strong&gt; &lt;a href=&#34;https://github.com/amaoo&#34;&gt;amao&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;依赖&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install slack_bolt flask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;配置&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#34;channel&#34;: {&#xA;    &#34;type&#34;: &#34;slack&#34;,&#xA;    &#34;slack&#34;: {&#xA;      &#34;slack_bot_token&#34;: &#34;xoxb-xxxx&#34;,&#xA;      &#34;slack_signing_secret&#34;: &#34;xxxx&#34;&#xA;    }&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;需要 80 端口,可以在 &lt;strong&gt;channel/slack/slack_channel.py:45&lt;/strong&gt; 修改相应端口&lt;/p&gt; &#xA;&lt;p&gt;将范围设置为机器人令牌范围 OAuth &amp;amp; Permission:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;app_mentions:read&#xA;channels:join&#xA;chat:write&#xA;im:history&#xA;im:read&#xA;im:writ&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;在事件订阅中设置范围 - Subscribe to bot events&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;app_mention&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;订阅 URL，如果端口是 80 ，可不填&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http:/你的固定公网ip或者域名:端口/slack/events&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;参考文档&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://slack.dev/bolt-python/tutorial/getting-started&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;9.Web&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contributor:&lt;/strong&gt; &lt;a href=&#34;https://github.com/RegimenArsenic&#34;&gt;RegimenArsenic&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;依赖&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install PyJWT flask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;配置&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#34;channel&#34;: {&#xA;    &#34;type&#34;: &#34;http&#34;,&#xA;    &#34;http&#34;: {&#xA;      &#34;http_auth_secret_key&#34;: &#34;6d25a684-9558-11e9-aa94-efccd7a0659b&#34;,    //jwt认证秘钥&#xA;      &#34;http_auth_password&#34;: &#34;6.67428e-11&#34;,        //认证密码,仅仅只是自用,最初步的防御别人扫描端口后DDOS浪费tokens&#xA;      &#34;port&#34;: &#34;80&#34;       //端口&#xA;    }&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;本地运行：&lt;code&gt;python3 app.py&lt;/code&gt;运行后访问 &lt;code&gt;http://127.0.0.1:80&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;服务器运行：部署后访问 &lt;code&gt;http://公网域名或IP:端口&lt;/code&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>oobabooga/text-generation-webui</title>
    <updated>2023-03-12T01:58:22Z</updated>
    <id>tag:github.com,2023-03-12:/oobabooga/text-generation-webui</id>
    <link href="https://github.com/oobabooga/text-generation-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Text generation web UI&lt;/h1&gt; &#xA;&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.&lt;/p&gt; &#xA;&lt;p&gt;Its goal is to become the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt; of text generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/oobabooga/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb&#34;&gt;[Try it on Google Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/qa.png&#34; alt=&#34;Image1&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/cai3.png&#34; alt=&#34;Image2&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/gpt4chan.png&#34; alt=&#34;Image3&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/galactica.png&#34; alt=&#34;Image4&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Switch between different models using a dropdown menu.&lt;/li&gt; &#xA; &lt;li&gt;Notebook mode that resembles OpenAI&#39;s playground.&lt;/li&gt; &#xA; &lt;li&gt;Chat mode for conversation and role playing.&lt;/li&gt; &#xA; &lt;li&gt;Generate nice HTML output for GPT-4chan.&lt;/li&gt; &#xA; &lt;li&gt;Generate Markdown output for &lt;a href=&#34;https://github.com/paperswithcode/galai&#34;&gt;GALACTICA&lt;/a&gt;, including LaTeX support.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;a href=&#34;https://huggingface.co/models?search=pygmalionai/pygmalion&#34;&gt;Pygmalion&lt;/a&gt; and custom characters in JSON or TavernAI Character Card formats (&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Pygmalion-chat-model-FAQ&#34;&gt;FAQ&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Advanced chat features (send images, get audio responses with TTS).&lt;/li&gt; &#xA; &lt;li&gt;Stream the text output in real time.&lt;/li&gt; &#xA; &lt;li&gt;Load parameter presets from text files.&lt;/li&gt; &#xA; &lt;li&gt;Load large models in 8-bit mode (see &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/147#issuecomment-1456040134&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/20#issuecomment-1411650652&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://www.reddit.com/r/PygmalionAI/comments/1115gom/running_pygmalion_6b_with_8gb_of_vram/&#34;&gt;here&lt;/a&gt; if you are on Windows).&lt;/li&gt; &#xA; &lt;li&gt;Split large models across your GPU(s), CPU, and disk.&lt;/li&gt; &#xA; &lt;li&gt;CPU mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/FlexGen&#34;&gt;FlexGen offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/DeepSpeed&#34;&gt;DeepSpeed ZeRO-3 offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Get responses via API, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example-streaming.py&#34;&gt;with&lt;/a&gt; or &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example.py&#34;&gt;without&lt;/a&gt; streaming.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model&#34;&gt;Supports the LLaMA model&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/RWKV-model&#34;&gt;Supports the RWKV model&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Supports softprompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Extensions&#34;&gt;Supports extensions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Running-on-Colab&#34;&gt;Works on Google Colab&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation option 1: conda&lt;/h2&gt; &#xA;&lt;p&gt;Open a terminal and copy and paste these commands one at a time (&lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;install conda&lt;/a&gt; first if you don&#39;t have it already):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n textgen&#xA;conda activate textgen&#xA;conda install torchvision torchaudio pytorch-cuda=11.7 git -c pytorch -c nvidia&#xA;git clone https://github.com/oobabooga/text-generation-webui&#xA;cd text-generation-webui&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The third line assumes that you have an NVIDIA GPU.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have an AMD GPU, replace the third command with this one:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you are running it in CPU mode, replace the third command with this one:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision torchaudio git -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation option 2: one-click installers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga-windows.zip&#34;&gt;oobabooga-windows.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga-linux.zip&#34;&gt;oobabooga-linux.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just download the zip above, extract it, and double click on &#34;install&#34;. The web UI and all its dependencies will be installed in the same folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To download a model, double click on &#34;download-model&#34;&lt;/li&gt; &#xA; &lt;li&gt;To start the web UI, double click on &#34;start-webui&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloading models&lt;/h2&gt; &#xA;&lt;p&gt;Models should be placed under &lt;code&gt;models/model-name&lt;/code&gt;. For instance, &lt;code&gt;models/gpt-j-6B&lt;/code&gt; for &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Hugging Face&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;Hugging Face&lt;/a&gt; is the main place to download models. These are some noteworthy examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&amp;amp;search=eleutherai+%2F+gpt-neo&#34;&gt;GPT-Neo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=eleutherai/pythia&#34;&gt;Pythia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/opt&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/galactica&#34;&gt;GALACTICA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=erebus&#34;&gt;*-Erebus&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=pygmalion&#34;&gt;Pygmalion&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can automatically download a model from HF using the script &lt;code&gt;download-model.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py organization/model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py facebook/opt-1.3b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to download a model manually, note that all you need are the json, txt, and pytorch*.bin (or model*.safetensors) files. The remaining files are not necessary.&lt;/p&gt; &#xA;&lt;h4&gt;GPT-4chan&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/ykilcher/gpt-4chan&#34;&gt;GPT-4chan&lt;/a&gt; has been shut down from Hugging Face, so you need to download it elsewhere. You have two options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Torrent: &lt;a href=&#34;https://archive.org/details/gpt4chan_model_float16&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://archive.org/details/gpt4chan_model&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Direct download: &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model_float16/&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model/&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The 32-bit version is only relevant if you intend to run the model in CPU mode. Otherwise, you should use the 16-bit version.&lt;/p&gt; &#xA;&lt;p&gt;After downloading the model, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Place the files under &lt;code&gt;models/gpt4chan_model_float16&lt;/code&gt; or &lt;code&gt;models/gpt4chan_model&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Place GPT-J 6B&#39;s config.json file in that same folder: &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/raw/main/config.json&#34;&gt;config.json&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download GPT-J 6B&#39;s tokenizer files (they will be automatically detected when you attempt to load GPT-4chan):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py EleutherAI/gpt-j-6B --text-only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the web UI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate textgen&#xA;python server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then browse to&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://localhost:7860/?__theme=dark&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Optionally, you can use the following command-line flags:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Flag&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;show this help message and exit&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model MODEL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Name of the model to load by default.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--notebook&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in notebook mode, where the output is written to the same text box as the input.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cai-chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode with a style similar to Character.AI&#39;s. If the file &lt;code&gt;img_bot.png&lt;/code&gt; or &lt;code&gt;img_bot.jpg&lt;/code&gt; exists in the same folder as server.py, this image will be used as the bot&#39;s profile picture. Similarly, &lt;code&gt;img_me.png&lt;/code&gt; or &lt;code&gt;img_me.jpg&lt;/code&gt; will be used as your profile picture.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use the CPU to generate text.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--load-in-8bit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with 8-bit precision.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--load-in-4bit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with 4-bit precision. Currently only works with LLaMA.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--bf16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--auto-devices&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Automatically split the model across the available GPU(s) and CPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;If the model is too large for your GPU(s) and CPU combined, send the remaining layers to the disk.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk-cache-dir DISK_CACHE_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory to save the disk cache to. Defaults to &lt;code&gt;cache/&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--gpu-memory GPU_MEMORY [GPU_MEMORY ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maxmimum GPU memory in GiB to be allocated per GPU. Example: &lt;code&gt;--gpu-memory 10&lt;/code&gt; for a single GPU, &lt;code&gt;--gpu-memory 10 5&lt;/code&gt; for two GPUs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu-memory CPU_MEMORY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maximum CPU memory in GiB to allocate for offloaded weights. Must be an integer number. Defaults to 99.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--flexgen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of FlexGen offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--percent PERCENT [PERCENT ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: allocation percentages. Must be 6 numbers separated by spaces (default: 0, 100, 100, 0, 100, 0).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--compress-weight&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: Whether to compress weight (default: False).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--pin-weight [PIN_WEIGHT]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: whether to pin weights (setting this to False reduces CPU memory by 20%).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--deepspeed&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of DeepSpeed ZeRO-3 for inference via the Transformers integration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--nvme-offload-dir NVME_OFFLOAD_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Directory to use for ZeRO-3 NVME offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--local_rank LOCAL_RANK&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Optional argument for distributed setups.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-strategy RWKV_STRATEGY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: The strategy to use while loading the model. Examples: &#34;cpu fp32&#34;, &#34;cuda fp16&#34;, &#34;cuda fp16i8&#34;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-cuda-on&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: Compile the CUDA kernel for better performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--no-stream&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Don&#39;t stream the text output in real time. This improves the text generation performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--settings SETTINGS_FILE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the default interface settings from this json file. See &lt;code&gt;settings-template.json&lt;/code&gt; for an example. If you create a file called &lt;code&gt;settings.json&lt;/code&gt;, this file will be loaded by default without the need to use the &lt;code&gt;--settings&lt;/code&gt; flag.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--extensions EXTENSIONS [EXTENSIONS ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The list of extensions to load. If you want to load more than one extension, write the names separated by spaces.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Make the web UI reachable from your local network.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen-port LISTEN_PORT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The listening port that the server will use.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--share&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Create a public URL. This is useful for running the web UI on Google Colab or similar.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--verbose&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Print the prompts to the terminal.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Out of memory errors? &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide&#34;&gt;Check this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Presets&lt;/h2&gt; &#xA;&lt;p&gt;Inference settings presets can be created under &lt;code&gt;presets/&lt;/code&gt; as text files. These files are detected automatically at startup.&lt;/p&gt; &#xA;&lt;p&gt;By default, 10 presets by NovelAI and KoboldAI are included. These were selected out of a sample of 43 presets after applying a K-Means clustering algorithm and selecting the elements closest to the average of each cluster.&lt;/p&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/System-requirements&#34;&gt;wiki&lt;/a&gt; for some examples of VRAM and RAM usage in both GPU and CPU mode.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests, suggestions, and issue reports are welcome.&lt;/p&gt; &#xA;&lt;p&gt;Before reporting a bug, make sure that you have created a conda environment and installed the dependencies exactly as in the &lt;em&gt;Installation&lt;/em&gt; section above.&lt;/p&gt; &#xA;&lt;p&gt;These issues are known:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8-bit doesn&#39;t work properly on Windows or older GPUs.&lt;/li&gt; &#xA; &lt;li&gt;DeepSpeed doesn&#39;t work properly on Windows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For these two, please try commenting on an existing issue instead of creating a new one.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradio dropdown menu refresh button: &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Verbose preset: Anonymous 4chan user.&lt;/li&gt; &#xA; &lt;li&gt;NovelAI and KoboldAI presets: &lt;a href=&#34;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&#34;&gt;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pygmalion preset, code for early stopping in chat mode, code for some of the sliders, --chat mode colors: &lt;a href=&#34;https://github.com/PygmalionAI/gradio-ui/&#34;&gt;https://github.com/PygmalionAI/gradio-ui/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>