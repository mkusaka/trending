<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-16T01:49:48Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openai/openai-python</title>
    <updated>2025-03-16T01:49:48Z</updated>
    <id>tag:github.com,2025-03-16:/openai/openai-python</id>
    <link href="https://github.com/openai/openai-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official Python library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Python API library&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/openai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openai.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by &lt;a href=&#34;https://github.com/encode/httpx&#34;&gt;httpx&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is generated from our &lt;a href=&#34;https://github.com/openai/openai-openapi&#34;&gt;OpenAPI specification&lt;/a&gt; with &lt;a href=&#34;https://stainlessapi.com/&#34;&gt;Stainless&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The REST API documentation can be found on &lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;&gt;platform.openai.com&lt;/a&gt;. The full API of this library can be found in &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-python/main/api.md&#34;&gt;api.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# install from PyPI&#xA;pip install openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The full API of this library can be found in &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-python/main/api.md&#34;&gt;api.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The primary API for interacting with OpenAI models is the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/responses&#34;&gt;Responses API&lt;/a&gt;. You can generate text from the model with the code below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from openai import OpenAI&#xA;&#xA;client = OpenAI(&#xA;    # This is the default and can be omitted&#xA;    api_key=os.environ.get(&#34;OPENAI_API_KEY&#34;),&#xA;)&#xA;&#xA;response = client.responses.create(&#xA;    model=&#34;gpt-4o&#34;,&#xA;    instructions=&#34;You are a coding assistant that talks like a pirate.&#34;,&#xA;    input=&#34;How do I check if a Python object is an instance of a class?&#34;,&#xA;)&#xA;&#xA;print(response.output_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The previous standard (supported indefinitely) for generating text is the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;Chat Completions API&lt;/a&gt;. You can use that API to generate text from the model with the code below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;&#xA;completion = client.chat.completions.create(&#xA;    model=&#34;gpt-4o&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;developer&#34;, &#34;content&#34;: &#34;Talk like a pirate.&#34;},&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;How do I check if a Python object is an instance of a class?&#34;,&#xA;        },&#xA;    ],&#xA;)&#xA;&#xA;print(completion.choices[0].message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;While you can provide an &lt;code&gt;api_key&lt;/code&gt; keyword argument, we recommend using &lt;a href=&#34;https://pypi.org/project/python-dotenv/&#34;&gt;python-dotenv&lt;/a&gt; to add &lt;code&gt;OPENAI_API_KEY=&#34;My API Key&#34;&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file so that your API key is not stored in source control. &lt;a href=&#34;https://platform.openai.com/settings/organization/api-keys&#34;&gt;Get an API key here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Vision&lt;/h3&gt; &#xA;&lt;p&gt;With an image URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#34;What is in this image?&#34;&#xA;img_url = &#34;https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg&#34;&#xA;&#xA;response = client.responses.create(&#xA;    model=&#34;gpt-4o-mini&#34;,&#xA;    input=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: [&#xA;                {&#34;type&#34;: &#34;input_text&#34;, &#34;text&#34;: prompt},&#xA;                {&#34;type&#34;: &#34;input_image&#34;, &#34;image_url&#34;: f&#34;{img_url}&#34;},&#xA;            ],&#xA;        }&#xA;    ],&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With the image as a base64 encoded string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import base64&#xA;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;&#xA;prompt = &#34;What is in this image?&#34;&#xA;with open(&#34;path/to/image.png&#34;, &#34;rb&#34;) as image_file:&#xA;    b64_image = base64.b64encode(image_file.read()).decode(&#34;utf-8&#34;)&#xA;&#xA;response = client.responses.create(&#xA;    model=&#34;gpt-4o-mini&#34;,&#xA;    input=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: [&#xA;                {&#34;type&#34;: &#34;input_text&#34;, &#34;text&#34;: prompt},&#xA;                {&#34;type&#34;: &#34;input_image&#34;, &#34;image_url&#34;: f&#34;data:image/png;base64,{b64_image}&#34;},&#xA;            ],&#xA;        }&#xA;    ],&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Async usage&lt;/h2&gt; &#xA;&lt;p&gt;Simply import &lt;code&gt;AsyncOpenAI&lt;/code&gt; instead of &lt;code&gt;OpenAI&lt;/code&gt; and use &lt;code&gt;await&lt;/code&gt; with each API call:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import asyncio&#xA;from openai import AsyncOpenAI&#xA;&#xA;client = AsyncOpenAI(&#xA;    # This is the default and can be omitted&#xA;    api_key=os.environ.get(&#34;OPENAI_API_KEY&#34;),&#xA;)&#xA;&#xA;&#xA;async def main() -&amp;gt; None:&#xA;    response = await client.responses.create(&#xA;        model=&#34;gpt-4o&#34;, input=&#34;Explain disestablishmentarianism to a smart five year old.&#34;&#xA;    )&#xA;    print(response.output_text)&#xA;&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Functionality between the synchronous and asynchronous clients is otherwise identical.&lt;/p&gt; &#xA;&lt;h2&gt;Streaming responses&lt;/h2&gt; &#xA;&lt;p&gt;We provide support for streaming responses using Server Side Events (SSE).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;&#xA;stream = client.responses.create(&#xA;    model=&#34;gpt-4o&#34;,&#xA;    input=&#34;Write a one-sentence bedtime story about a unicorn.&#34;,&#xA;    stream=True,&#xA;)&#xA;&#xA;for event in stream:&#xA;    print(event)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The async client uses the exact same interface.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from openai import AsyncOpenAI&#xA;&#xA;client = AsyncOpenAI()&#xA;&#xA;&#xA;async def main():&#xA;    stream = client.responses.create(&#xA;        model=&#34;gpt-4o&#34;,&#xA;        input=&#34;Write a one-sentence bedtime story about a unicorn.&#34;,&#xA;        stream=True,&#xA;    )&#xA;&#xA;    for event in stream:&#xA;        print(event)&#xA;&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Realtime API beta&lt;/h2&gt; &#xA;&lt;p&gt;The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling&#34;&gt;function calling&lt;/a&gt; through a WebSocket connection.&lt;/p&gt; &#xA;&lt;p&gt;Under the hood the SDK uses the &lt;a href=&#34;https://websockets.readthedocs.io/en/stable/&#34;&gt;&lt;code&gt;websockets&lt;/code&gt;&lt;/a&gt; library to manage connections.&lt;/p&gt; &#xA;&lt;p&gt;The Realtime API works through a combination of client-sent events and server-sent events. Clients can send events to do things like update session configuration or send text and audio inputs. Server events confirm when audio responses have completed, or when a text response from the model has been received. A full event reference can be found &lt;a href=&#34;https://platform.openai.com/docs/api-reference/realtime-client-events&#34;&gt;here&lt;/a&gt; and a guide can be found &lt;a href=&#34;https://platform.openai.com/docs/guides/realtime&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Basic text based example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import asyncio&#xA;from openai import AsyncOpenAI&#xA;&#xA;async def main():&#xA;    client = AsyncOpenAI()&#xA;&#xA;    async with client.beta.realtime.connect(model=&#34;gpt-4o-realtime-preview&#34;) as connection:&#xA;        await connection.session.update(session={&#39;modalities&#39;: [&#39;text&#39;]})&#xA;&#xA;        await connection.conversation.item.create(&#xA;            item={&#xA;                &#34;type&#34;: &#34;message&#34;,&#xA;                &#34;role&#34;: &#34;user&#34;,&#xA;                &#34;content&#34;: [{&#34;type&#34;: &#34;input_text&#34;, &#34;text&#34;: &#34;Say hello!&#34;}],&#xA;            }&#xA;        )&#xA;        await connection.response.create()&#xA;&#xA;        async for event in connection:&#xA;            if event.type == &#39;response.text.delta&#39;:&#xA;                print(event.delta, flush=True, end=&#34;&#34;)&#xA;&#xA;            elif event.type == &#39;response.text.done&#39;:&#xA;                print()&#xA;&#xA;            elif event.type == &#34;response.done&#34;:&#xA;                break&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However the real magic of the Realtime API is handling audio inputs / outputs, see this example &lt;a href=&#34;https://github.com/openai/openai-python/raw/main/examples/realtime/push_to_talk_app.py&#34;&gt;TUI script&lt;/a&gt; for a fully fledged example.&lt;/p&gt; &#xA;&lt;h3&gt;Realtime error handling&lt;/h3&gt; &#xA;&lt;p&gt;Whenever an error occurs, the Realtime API will send an &lt;a href=&#34;https://platform.openai.com/docs/guides/realtime-model-capabilities#error-handling&#34;&gt;&lt;code&gt;error&lt;/code&gt; event&lt;/a&gt; and the connection will stay open and remain usable. This means you need to handle it yourself, as &lt;em&gt;no errors are raised directly&lt;/em&gt; by the SDK when an &lt;code&gt;error&lt;/code&gt; event comes in.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;client = AsyncOpenAI()&#xA;&#xA;async with client.beta.realtime.connect(model=&#34;gpt-4o-realtime-preview&#34;) as connection:&#xA;    ...&#xA;    async for event in connection:&#xA;        if event.type == &#39;error&#39;:&#xA;            print(event.error.type)&#xA;            print(event.error.code)&#xA;            print(event.error.event_id)&#xA;            print(event.error.message)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using types&lt;/h2&gt; &#xA;&lt;p&gt;Nested request parameters are &lt;a href=&#34;https://docs.python.org/3/library/typing.html#typing.TypedDict&#34;&gt;TypedDicts&lt;/a&gt;. Responses are &lt;a href=&#34;https://docs.pydantic.dev&#34;&gt;Pydantic models&lt;/a&gt; which also provide helper methods for things like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Serializing back into JSON, &lt;code&gt;model.to_json()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Converting to a dictionary, &lt;code&gt;model.to_dict()&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set &lt;code&gt;python.analysis.typeCheckingMode&lt;/code&gt; to &lt;code&gt;basic&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Pagination&lt;/h2&gt; &#xA;&lt;p&gt;List methods in the OpenAI API are paginated.&lt;/p&gt; &#xA;&lt;p&gt;This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;&#xA;all_jobs = []&#xA;# Automatically fetches more pages as needed.&#xA;for job in client.fine_tuning.jobs.list(&#xA;    limit=20,&#xA;):&#xA;    # Do something with job here&#xA;    all_jobs.append(job)&#xA;print(all_jobs)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, asynchronously:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from openai import AsyncOpenAI&#xA;&#xA;client = AsyncOpenAI()&#xA;&#xA;&#xA;async def main() -&amp;gt; None:&#xA;    all_jobs = []&#xA;    # Iterate through items across all pages, issuing requests as needed.&#xA;    async for job in client.fine_tuning.jobs.list(&#xA;        limit=20,&#xA;    ):&#xA;        all_jobs.append(job)&#xA;    print(all_jobs)&#xA;&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can use the &lt;code&gt;.has_next_page()&lt;/code&gt;, &lt;code&gt;.next_page_info()&lt;/code&gt;, or &lt;code&gt;.get_next_page()&lt;/code&gt; methods for more granular control working with pages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;first_page = await client.fine_tuning.jobs.list(&#xA;    limit=20,&#xA;)&#xA;if first_page.has_next_page():&#xA;    print(f&#34;will fetch next page using these details: {first_page.next_page_info()}&#34;)&#xA;    next_page = await first_page.get_next_page()&#xA;    print(f&#34;number of items we just fetched: {len(next_page.data)}&#34;)&#xA;&#xA;# Remove `await` for non-async usage.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or just work directly with the returned data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;first_page = await client.fine_tuning.jobs.list(&#xA;    limit=20,&#xA;)&#xA;&#xA;print(f&#34;next page cursor: {first_page.after}&#34;)  # =&amp;gt; &#34;next page cursor: ...&#34;&#xA;for job in first_page.data:&#xA;    print(job.id)&#xA;&#xA;# Remove `await` for non-async usage.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Nested params&lt;/h2&gt; &#xA;&lt;p&gt;Nested parameters are dictionaries, typed using &lt;code&gt;TypedDict&lt;/code&gt;, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;&#xA;response = client.chat.responses.create(&#xA;    input=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;How much ?&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;gpt-4o&#34;,&#xA;    response_format={&#34;type&#34;: &#34;json_object&#34;},&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;File uploads&lt;/h2&gt; &#xA;&lt;p&gt;Request parameters that correspond to file uploads can be passed as &lt;code&gt;bytes&lt;/code&gt;, a &lt;a href=&#34;https://docs.python.org/3/library/os.html#os.PathLike&#34;&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance or a tuple of &lt;code&gt;(filename, contents, media type)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pathlib import Path&#xA;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;&#xA;client.files.create(&#xA;    file=Path(&#34;input.jsonl&#34;),&#xA;    purpose=&#34;fine-tune&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The async client uses the exact same interface. If you pass a &lt;a href=&#34;https://docs.python.org/3/library/os.html#os.PathLike&#34;&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance, the file contents will be read asynchronously automatically.&lt;/p&gt; &#xA;&lt;h2&gt;Handling errors&lt;/h2&gt; &#xA;&lt;p&gt;When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of &lt;code&gt;openai.APIConnectionError&lt;/code&gt; is raised.&lt;/p&gt; &#xA;&lt;p&gt;When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of &lt;code&gt;openai.APIStatusError&lt;/code&gt; is raised, containing &lt;code&gt;status_code&lt;/code&gt; and &lt;code&gt;response&lt;/code&gt; properties.&lt;/p&gt; &#xA;&lt;p&gt;All errors inherit from &lt;code&gt;openai.APIError&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;&#xA;try:&#xA;    client.fine_tuning.jobs.create(&#xA;        model=&#34;gpt-4o&#34;,&#xA;        training_file=&#34;file-abc123&#34;,&#xA;    )&#xA;except openai.APIConnectionError as e:&#xA;    print(&#34;The server could not be reached&#34;)&#xA;    print(e.__cause__)  # an underlying Exception, likely raised within httpx.&#xA;except openai.RateLimitError as e:&#xA;    print(&#34;A 429 status code was received; we should back off a bit.&#34;)&#xA;except openai.APIStatusError as e:&#xA;    print(&#34;Another non-200-range status code was received&#34;)&#xA;    print(e.status_code)&#xA;    print(e.response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Error codes are as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Status Code&lt;/th&gt; &#xA;   &lt;th&gt;Error Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;400&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;BadRequestError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;401&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;AuthenticationError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;403&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PermissionDeniedError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;404&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;NotFoundError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;422&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;UnprocessableEntityError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;429&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RateLimitError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;gt;=500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;InternalServerError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;APIConnectionError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Request IDs&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For more information on debugging requests, see &lt;a href=&#34;https://platform.openai.com/docs/api-reference/debugging-requests&#34;&gt;these docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;x-request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to OpenAI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = await client.responses.create(&#xA;    model=&#34;gpt-4o-mini&#34;,&#xA;    input=&#34;Say &#39;this is a test&#39;.&#34;,&#xA;)&#xA;print(response._request_id)  # req_123&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that unlike other properties that use an &lt;code&gt;_&lt;/code&gt; prefix, the &lt;code&gt;_request_id&lt;/code&gt; property &lt;em&gt;is&lt;/em&gt; public. Unless documented otherwise, &lt;em&gt;all&lt;/em&gt; other &lt;code&gt;_&lt;/code&gt; prefix properties, methods and modules are &lt;em&gt;private&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br&gt; If you need to access request IDs for failed requests you must catch the &lt;code&gt;APIStatusError&lt;/code&gt; exception&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;&#xA;try:&#xA;    completion = await client.chat.completions.create(&#xA;        messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Say this is a test&#34;}], model=&#34;gpt-4&#34;&#xA;    )&#xA;except openai.APIStatusError as exc:&#xA;    print(exc.request_id)  # req_123&#xA;    raise exc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Retries&lt;/h2&gt; &#xA;&lt;p&gt;Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors are all retried by default.&lt;/p&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;max_retries&lt;/code&gt; option to configure or disable retry settings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;# Configure the default for all requests:&#xA;client = OpenAI(&#xA;    # default is 2&#xA;    max_retries=0,&#xA;)&#xA;&#xA;# Or, configure per-request:&#xA;client.with_options(max_retries=5).chat.completions.create(&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;How can I get the name of the current day in JavaScript?&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;gpt-4o&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Timeouts&lt;/h2&gt; &#xA;&lt;p&gt;By default requests time out after 10 minutes. You can configure this with a &lt;code&gt;timeout&lt;/code&gt; option, which accepts a float or an &lt;a href=&#34;https://www.python-httpx.org/advanced/timeouts/#fine-tuning-the-configuration&#34;&gt;&lt;code&gt;httpx.Timeout&lt;/code&gt;&lt;/a&gt; object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;# Configure the default for all requests:&#xA;client = OpenAI(&#xA;    # 20 seconds (default is 10 minutes)&#xA;    timeout=20.0,&#xA;)&#xA;&#xA;# More granular control:&#xA;client = OpenAI(&#xA;    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),&#xA;)&#xA;&#xA;# Override per-request:&#xA;client.with_options(timeout=5.0).chat.completions.create(&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;How can I list all files in a directory using Python?&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;gpt-4o&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On timeout, an &lt;code&gt;APITimeoutError&lt;/code&gt; is thrown.&lt;/p&gt; &#xA;&lt;p&gt;Note that requests that time out are &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-python/main/#retries&#34;&gt;retried twice by default&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Advanced&lt;/h2&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;We use the standard library &lt;a href=&#34;https://docs.python.org/3/library/logging.html&#34;&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/a&gt; module.&lt;/p&gt; &#xA;&lt;p&gt;You can enable logging by setting the environment variable &lt;code&gt;OPENAI_LOG&lt;/code&gt; to &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ export OPENAI_LOG=info&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or to &lt;code&gt;debug&lt;/code&gt; for more verbose logging.&lt;/p&gt; &#xA;&lt;h3&gt;How to tell whether &lt;code&gt;None&lt;/code&gt; means &lt;code&gt;null&lt;/code&gt; or missing&lt;/h3&gt; &#xA;&lt;p&gt;In an API response, a field may be explicitly &lt;code&gt;null&lt;/code&gt;, or missing entirely; in either case, its value is &lt;code&gt;None&lt;/code&gt; in this library. You can differentiate the two cases with &lt;code&gt;.model_fields_set&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;if response.my_field is None:&#xA;  if &#39;my_field&#39; not in response.model_fields_set:&#xA;    print(&#39;Got json like {}, without a &#34;my_field&#34; key present at all.&#39;)&#xA;  else:&#xA;    print(&#39;Got json like {&#34;my_field&#34;: null}.&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Accessing raw response data (e.g. headers)&lt;/h3&gt; &#xA;&lt;p&gt;The &#34;raw&#34; Response object can be accessed by prefixing &lt;code&gt;.with_raw_response.&lt;/code&gt; to any HTTP method call, e.g.,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;response = client.chat.completions.with_raw_response.create(&#xA;    messages=[{&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;Say this is a test&#34;,&#xA;    }],&#xA;    model=&#34;gpt-4o&#34;,&#xA;)&#xA;print(response.headers.get(&#39;X-My-Header&#39;))&#xA;&#xA;completion = response.parse()  # get the object that `chat.completions.create()` would have returned&#xA;print(completion)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These methods return a &lt;a href=&#34;https://github.com/openai/openai-python/tree/main/src/openai/_legacy_response.py&#34;&gt;&lt;code&gt;LegacyAPIResponse&lt;/code&gt;&lt;/a&gt; object. This is a legacy class as we&#39;re changing it slightly in the next major version.&lt;/p&gt; &#xA;&lt;p&gt;For the sync client this will mostly be the same with the exception of &lt;code&gt;content&lt;/code&gt; &amp;amp; &lt;code&gt;text&lt;/code&gt; will be methods instead of properties. In the async client, all methods will be async.&lt;/p&gt; &#xA;&lt;p&gt;A migration script will be provided &amp;amp; the migration in general should be smooth.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;.with_streaming_response&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The above interface eagerly reads the full response body when you make the request, which may not always be what you want.&lt;/p&gt; &#xA;&lt;p&gt;To stream the response body, use &lt;code&gt;.with_streaming_response&lt;/code&gt; instead, which requires a context manager and only reads the response body once you call &lt;code&gt;.read()&lt;/code&gt;, &lt;code&gt;.text()&lt;/code&gt;, &lt;code&gt;.json()&lt;/code&gt;, &lt;code&gt;.iter_bytes()&lt;/code&gt;, &lt;code&gt;.iter_text()&lt;/code&gt;, &lt;code&gt;.iter_lines()&lt;/code&gt; or &lt;code&gt;.parse()&lt;/code&gt;. In the async client, these are async methods.&lt;/p&gt; &#xA;&lt;p&gt;As such, &lt;code&gt;.with_streaming_response&lt;/code&gt; methods return a different &lt;a href=&#34;https://github.com/openai/openai-python/tree/main/src/openai/_response.py&#34;&gt;&lt;code&gt;APIResponse&lt;/code&gt;&lt;/a&gt; object, and the async client returns an &lt;a href=&#34;https://github.com/openai/openai-python/tree/main/src/openai/_response.py&#34;&gt;&lt;code&gt;AsyncAPIResponse&lt;/code&gt;&lt;/a&gt; object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with client.chat.completions.with_streaming_response.create(&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Say this is a test&#34;,&#xA;        }&#xA;    ],&#xA;    model=&#34;gpt-4o&#34;,&#xA;) as response:&#xA;    print(response.headers.get(&#34;X-My-Header&#34;))&#xA;&#xA;    for line in response.iter_lines():&#xA;        print(line)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The context manager is required so that the response will reliably be closed.&lt;/p&gt; &#xA;&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; &#xA;&lt;p&gt;This library is typed for convenient access to the documented API.&lt;/p&gt; &#xA;&lt;p&gt;If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; &#xA;&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; &#xA;&lt;p&gt;To make requests to undocumented endpoints, you can make requests using &lt;code&gt;client.get&lt;/code&gt;, &lt;code&gt;client.post&lt;/code&gt;, and other http verbs. Options on the client will be respected (such as retries) when making this request.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import httpx&#xA;&#xA;response = client.post(&#xA;    &#34;/foo&#34;,&#xA;    cast_to=httpx.Response,&#xA;    body={&#34;my_param&#34;: True},&#xA;)&#xA;&#xA;print(response.headers.get(&#34;x-foo&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Undocumented request params&lt;/h4&gt; &#xA;&lt;p&gt;If you want to explicitly send an extra param, you can do so with the &lt;code&gt;extra_query&lt;/code&gt;, &lt;code&gt;extra_body&lt;/code&gt;, and &lt;code&gt;extra_headers&lt;/code&gt; request options.&lt;/p&gt; &#xA;&lt;h4&gt;Undocumented response properties&lt;/h4&gt; &#xA;&lt;p&gt;To access undocumented response properties, you can access the extra fields like &lt;code&gt;response.unknown_prop&lt;/code&gt;. You can also get all the extra fields on the Pydantic model as a dict with &lt;a href=&#34;https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_extra&#34;&gt;&lt;code&gt;response.model_extra&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Configuring the HTTP client&lt;/h3&gt; &#xA;&lt;p&gt;You can directly override the &lt;a href=&#34;https://www.python-httpx.org/api/#client&#34;&gt;httpx client&lt;/a&gt; to customize it for your use case, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for &lt;a href=&#34;https://www.python-httpx.org/advanced/proxies/&#34;&gt;proxies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Custom &lt;a href=&#34;https://www.python-httpx.org/advanced/transports/&#34;&gt;transports&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Additional &lt;a href=&#34;https://www.python-httpx.org/advanced/clients/&#34;&gt;advanced&lt;/a&gt; functionality&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import httpx&#xA;from openai import OpenAI, DefaultHttpxClient&#xA;&#xA;client = OpenAI(&#xA;    # Or use the `OPENAI_BASE_URL` env var&#xA;    base_url=&#34;http://my.test.server.example.com:8083/v1&#34;,&#xA;    http_client=DefaultHttpxClient(&#xA;        proxy=&#34;http://my.test.proxy.example.com&#34;,&#xA;        transport=httpx.HTTPTransport(local_address=&#34;0.0.0.0&#34;),&#xA;    ),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also customize the client on a per-request basis by using &lt;code&gt;with_options()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client.with_options(http_client=DefaultHttpxClient(...))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Managing HTTP resources&lt;/h3&gt; &#xA;&lt;p&gt;By default the library closes underlying HTTP connections whenever the client is &lt;a href=&#34;https://docs.python.org/3/reference/datamodel.html#object.__del__&#34;&gt;garbage collected&lt;/a&gt;. You can manually close the client using the &lt;code&gt;.close()&lt;/code&gt; method if desired, or with a context manager that closes when exiting.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from openai import OpenAI&#xA;&#xA;with OpenAI() as client:&#xA;  # make requests here&#xA;  ...&#xA;&#xA;# HTTP client is now closed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;To use this library with &lt;a href=&#34;https://learn.microsoft.com/azure/ai-services/openai/overview&#34;&gt;Azure OpenAI&lt;/a&gt;, use the &lt;code&gt;AzureOpenAI&lt;/code&gt; class instead of the &lt;code&gt;OpenAI&lt;/code&gt; class.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won&#39;t always be correct.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from openai import AzureOpenAI&#xA;&#xA;# gets the API Key from environment variable AZURE_OPENAI_API_KEY&#xA;client = AzureOpenAI(&#xA;    # https://learn.microsoft.com/azure/ai-services/openai/reference#rest-api-versioning&#xA;    api_version=&#34;2023-07-01-preview&#34;,&#xA;    # https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource&#xA;    azure_endpoint=&#34;https://example-endpoint.openai.azure.com&#34;,&#xA;)&#xA;&#xA;completion = client.chat.completions.create(&#xA;    model=&#34;deployment-name&#34;,  # e.g. gpt-35-instant&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;How do I output all files in a directory using Python?&#34;,&#xA;        },&#xA;    ],&#xA;)&#xA;print(completion.to_json())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition to the options provided in the base &lt;code&gt;OpenAI&lt;/code&gt; client, the following options are provided:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;azure_endpoint&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt; environment variable)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;azure_deployment&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;api_version&lt;/code&gt; (or the &lt;code&gt;OPENAI_API_VERSION&lt;/code&gt; environment variable)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;azure_ad_token&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_AD_TOKEN&lt;/code&gt; environment variable)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;azure_ad_token_provider&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An example of using the client with Microsoft Entra ID (formerly known as Azure Active Directory) can be found &lt;a href=&#34;https://github.com/openai/openai-python/raw/main/examples/azure_ad.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Versioning&lt;/h2&gt; &#xA;&lt;p&gt;This package generally follows &lt;a href=&#34;https://semver.org/spec/v2.0.0.html&#34;&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Changes that only affect static types, without breaking runtime behavior.&lt;/li&gt; &#xA; &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; &#xA;&lt;p&gt;We are keen for your feedback; please open an &lt;a href=&#34;https://www.github.com/openai/openai-python/issues&#34;&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; &#xA;&lt;h3&gt;Determining the installed version&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;ve upgraded to the latest version but aren&#39;t seeing any new features you were expecting then your python environment is likely still using an older version.&lt;/p&gt; &#xA;&lt;p&gt;You can determine the version that is being used at runtime with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import openai&#xA;print(openai.__version__)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Python 3.8 or higher.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-python/main/CONTRIBUTING.md&#34;&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>BerriAI/litellm</title>
    <updated>2025-03-16T01:49:48Z</updated>
    <id>tag:github.com,2025-03-16:/BerriAI/litellm</id>
    <link href="https://github.com/BerriAI/litellm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; ðŸš… LiteLLM &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://render.com/deploy?repo=https://github.com/BerriAI/litellm&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;img src=&#34;https://render.com/images/deploy-to-render-button.svg?sanitize=true&#34; alt=&#34;Deploy to Render&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://railway.app/template/HLP0Ub?referralCode=jch2ME&#34;&gt; &lt;img src=&#34;https://railway.app/button.svg?sanitize=true&#34; alt=&#34;Deploy on Railway&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.] &lt;br&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/simple_proxy&#34; target=&#34;_blank&#34;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href=&#34;https://docs.litellm.ai/docs/hosted&#34; target=&#34;_blank&#34;&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href=&#34;https://docs.litellm.ai/docs/enterprise&#34; target=&#34;_blank&#34;&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/litellm/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/litellm.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://dl.circleci.com/status-badge/redirect/gh/BerriAI/litellm/tree/main&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://dl.circleci.com/status-badge/img/gh/BerriAI/litellm/tree/main.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.ycombinator.com/companies/berriai&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square&#34; alt=&#34;Y Combinator W23&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://wa.link/huol9n&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=WhatsApp&amp;amp;color=success&amp;amp;logo=WhatsApp&amp;amp;style=flat-square&#34; alt=&#34;Whatsapp&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/wuPM9dRgDw&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=Discord&amp;amp;color=blue&amp;amp;logo=Discord&amp;amp;style=flat-square&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;p&gt;LiteLLM manages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Translate inputs to provider&#39;s &lt;code&gt;completion&lt;/code&gt;, &lt;code&gt;embedding&lt;/code&gt;, and &lt;code&gt;image_generation&lt;/code&gt; endpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/completion/output&#34;&gt;Consistent output&lt;/a&gt;, text responses will always be available at &lt;code&gt;[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - &lt;a href=&#34;https://docs.litellm.ai/docs/routing&#34;&gt;Router&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Set Budgets &amp;amp; Rate limits per project, api key, model &lt;a href=&#34;https://docs.litellm.ai/docs/simple_proxy&#34;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs&#34;&gt;&lt;strong&gt;Jump to LiteLLM Proxy (LLM Gateway) Docs&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs&#34;&gt;&lt;strong&gt;Jump to Supported LLM Providers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸš¨ &lt;strong&gt;Stable Release:&lt;/strong&gt; Use docker images with the &lt;code&gt;-stable&lt;/code&gt; tag. These have undergone 12 hour load tests, before being published. &lt;a href=&#34;https://docs.litellm.ai/docs/proxy/release_cycle&#34;&gt;More information about the release cycle here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Support for more providers. Missing a provider or LLM Platform, raise a &lt;a href=&#34;https://github.com/BerriAI/litellm/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;template=feature_request.yml&amp;amp;title=%5BFeature%5D%3A+&#34;&gt;feature request&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Usage (&lt;a href=&#34;https://docs.litellm.ai/docs/&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;)&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] LiteLLM v1.0.0 now requires &lt;code&gt;openai&amp;gt;=1.0.0&lt;/code&gt;. Migration guide &lt;a href=&#34;https://docs.litellm.ai/docs/migration&#34;&gt;here&lt;/a&gt;&lt;br&gt; LiteLLM v1.40.14+ now requires &lt;code&gt;pydantic&amp;gt;=2.0.0&lt;/code&gt;. No changes required.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install litellm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litellm import completion&#xA;import os&#xA;&#xA;## set ENV variables&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;your-openai-key&#34;&#xA;os.environ[&#34;ANTHROPIC_API_KEY&#34;] = &#34;your-anthropic-key&#34;&#xA;&#xA;messages = [{ &#34;content&#34;: &#34;Hello, how are you?&#34;,&#34;role&#34;: &#34;user&#34;}]&#xA;&#xA;# openai call&#xA;response = completion(model=&#34;openai/gpt-4o&#34;, messages=messages)&#xA;&#xA;# anthropic call&#xA;response = completion(model=&#34;anthropic/claude-3-sonnet-20240229&#34;, messages=messages)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Response (OpenAI Format)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;id&#34;: &#34;chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885&#34;,&#xA;    &#34;created&#34;: 1734366691,&#xA;    &#34;model&#34;: &#34;claude-3-sonnet-20240229&#34;,&#xA;    &#34;object&#34;: &#34;chat.completion&#34;,&#xA;    &#34;system_fingerprint&#34;: null,&#xA;    &#34;choices&#34;: [&#xA;        {&#xA;            &#34;finish_reason&#34;: &#34;stop&#34;,&#xA;            &#34;index&#34;: 0,&#xA;            &#34;message&#34;: {&#xA;                &#34;content&#34;: &#34;Hello! As an AI language model, I don&#39;t have feelings, but I&#39;m operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?&#34;,&#xA;                &#34;role&#34;: &#34;assistant&#34;,&#xA;                &#34;tool_calls&#34;: null,&#xA;                &#34;function_call&#34;: null&#xA;            }&#xA;        }&#xA;    ],&#xA;    &#34;usage&#34;: {&#xA;        &#34;completion_tokens&#34;: 43,&#xA;        &#34;prompt_tokens&#34;: 13,&#xA;        &#34;total_tokens&#34;: 56,&#xA;        &#34;completion_tokens_details&#34;: null,&#xA;        &#34;prompt_tokens_details&#34;: {&#xA;            &#34;audio_tokens&#34;: null,&#xA;            &#34;cached_tokens&#34;: 0&#xA;        },&#xA;        &#34;cache_creation_input_tokens&#34;: 0,&#xA;        &#34;cache_read_input_tokens&#34;: 0&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Call any model supported by a provider, with &lt;code&gt;model=&amp;lt;provider_name&amp;gt;/&amp;lt;model_name&amp;gt;&lt;/code&gt;. There might be provider-specific details here, so refer to &lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;provider docs for more information&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Async (&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream#async-completion&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litellm import acompletion&#xA;import asyncio&#xA;&#xA;async def test_get_response():&#xA;    user_message = &#34;Hello, how are you?&#34;&#xA;    messages = [{&#34;content&#34;: user_message, &#34;role&#34;: &#34;user&#34;}]&#xA;    response = await acompletion(model=&#34;openai/gpt-4o&#34;, messages=messages)&#xA;    return response&#xA;&#xA;response = asyncio.run(test_get_response())&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Streaming (&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;liteLLM supports streaming the model response back, pass &lt;code&gt;stream=True&lt;/code&gt; to get a streaming iterator in response.&lt;br&gt; Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litellm import completion&#xA;response = completion(model=&#34;openai/gpt-4o&#34;, messages=messages, stream=True)&#xA;for part in response:&#xA;    print(part.choices[0].delta.content or &#34;&#34;)&#xA;&#xA;# claude 2&#xA;response = completion(&#39;anthropic/claude-3-sonnet-20240229&#39;, messages, stream=True)&#xA;for part in response:&#xA;    print(part)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Response chunk (OpenAI Format)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;id&#34;: &#34;chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697&#34;,&#xA;    &#34;created&#34;: 1734366925,&#xA;    &#34;model&#34;: &#34;claude-3-sonnet-20240229&#34;,&#xA;    &#34;object&#34;: &#34;chat.completion.chunk&#34;,&#xA;    &#34;system_fingerprint&#34;: null,&#xA;    &#34;choices&#34;: [&#xA;        {&#xA;            &#34;finish_reason&#34;: null,&#xA;            &#34;index&#34;: 0,&#xA;            &#34;delta&#34;: {&#xA;                &#34;content&#34;: &#34;Hello&#34;,&#xA;                &#34;role&#34;: &#34;assistant&#34;,&#xA;                &#34;function_call&#34;: null,&#xA;                &#34;tool_calls&#34;: null,&#xA;                &#34;audio&#34;: null&#xA;            },&#xA;            &#34;logprobs&#34;: null&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Logging Observability (&lt;a href=&#34;https://docs.litellm.ai/docs/observability/callbacks&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from litellm import completion&#xA;&#xA;## set env variables for logging tools (when using MLflow, no API key set up is required)&#xA;os.environ[&#34;LUNARY_PUBLIC_KEY&#34;] = &#34;your-lunary-public-key&#34;&#xA;os.environ[&#34;HELICONE_API_KEY&#34;] = &#34;your-helicone-auth-key&#34;&#xA;os.environ[&#34;LANGFUSE_PUBLIC_KEY&#34;] = &#34;&#34;&#xA;os.environ[&#34;LANGFUSE_SECRET_KEY&#34;] = &#34;&#34;&#xA;os.environ[&#34;ATHINA_API_KEY&#34;] = &#34;your-athina-api-key&#34;&#xA;&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;your-openai-key&#34;&#xA;&#xA;# set callbacks&#xA;litellm.success_callback = [&#34;lunary&#34;, &#34;mlflow&#34;, &#34;langfuse&#34;, &#34;athina&#34;, &#34;helicone&#34;] # log input/output to lunary, langfuse, supabase, athina, helicone etc&#xA;&#xA;#openai call&#xA;response = completion(model=&#34;openai/gpt-4o&#34;, messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hi ðŸ‘‹ - i&#39;m openai&#34;}])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;LiteLLM Proxy Server (LLM Gateway) - (&lt;a href=&#34;https://docs.litellm.ai/docs/simple_proxy&#34;&gt;Docs&lt;/a&gt;)&lt;/h1&gt; &#xA;&lt;p&gt;Track spend + Load Balance across multiple projects&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/hosted&#34;&gt;Hosted Proxy (Preview)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The proxy provides:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth&#34;&gt;Hooks for auth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class&#34;&gt;Hooks for logging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend&#34;&gt;Cost tracking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/users#set-rate-limits&#34;&gt;Rate Limiting&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ðŸ“– Proxy Endpoints - &lt;a href=&#34;https://litellm-api.up.railway.app/&#34;&gt;Swagger Docs&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Quick Start Proxy - CLI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install &#39;litellm[proxy]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 1: Start litellm proxy&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ litellm --model huggingface/bigcode/starcoder&#xA;&#xA;#INFO: Proxy running on http://0.0.0.0:4000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Make ChatCompletions Request to Proxy&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] ðŸ’¡ &lt;a href=&#34;https://docs.litellm.ai/docs/proxy/user_keys&#34;&gt;Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai # openai v1.0.0+&#xA;client = openai.OpenAI(api_key=&#34;anything&#34;,base_url=&#34;http://0.0.0.0:4000&#34;) # set proxy to base_url&#xA;# request sent to model set on litellm proxy, `litellm --model`&#xA;response = client.chat.completions.create(model=&#34;gpt-3.5-turbo&#34;, messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;this is a test request, write a short poem&#34;&#xA;    }&#xA;])&#xA;&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Proxy Key Management (&lt;a href=&#34;https://docs.litellm.ai/docs/proxy/virtual_keys&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Connect the proxy with a Postgres DB to create proxy keys&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get the code&#xA;git clone https://github.com/BerriAI/litellm&#xA;&#xA;# Go to folder&#xA;cd litellm&#xA;&#xA;# Add the master key - you can change this after setup&#xA;echo &#39;LITELLM_MASTER_KEY=&#34;sk-1234&#34;&#39; &amp;gt; .env&#xA;&#xA;# Add the litellm salt key - you cannot change this after adding a model&#xA;# It is used to encrypt / decrypt your LLM API Key credentials&#xA;# We recommend - https://1password.com/password-generator/ &#xA;# password generator to get a random hash for litellm salt key&#xA;echo &#39;LITELLM_SALT_KEY=&#34;sk-1234&#34;&#39; &amp;gt; .env&#xA;&#xA;source .env&#xA;&#xA;# Start&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;UI on &lt;code&gt;/ui&lt;/code&gt; on your proxy server &lt;img src=&#34;https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033&#34; alt=&#34;ui_3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Set budgets and rate limits across multiple projects &lt;code&gt;POST /key/generate&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Request&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl &#39;http://0.0.0.0:4000/key/generate&#39; \&#xA;--header &#39;Authorization: Bearer sk-1234&#39; \&#xA;--header &#39;Content-Type: application/json&#39; \&#xA;--data-raw &#39;{&#34;models&#34;: [&#34;gpt-3.5-turbo&#34;, &#34;gpt-4&#34;, &#34;claude-2&#34;], &#34;duration&#34;: &#34;20m&#34;,&#34;metadata&#34;: {&#34;user&#34;: &#34;ishaan@berri.ai&#34;, &#34;team&#34;: &#34;core-infra&#34;}}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Expected Response&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;{&#xA;    &#34;key&#34;: &#34;sk-kdEXbIqZRwEeEiHwdg7sFA&#34;, # Bearer token&#xA;    &#34;expires&#34;: &#34;2023-11-19T01:38:25.838000+00:00&#34; # datetime object&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Supported Providers (&lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;Docs&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Provider&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/#basic-usage&#34;&gt;Completion&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream#streaming-responses&#34;&gt;Streaming&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream#async-completion&#34;&gt;Async Completion&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/completion/stream#async-streaming&#34;&gt;Async Streaming&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/embedding/supported_embedding&#34;&gt;Async Embedding&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/image_generation&#34;&gt;Async Image Generation&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/openai&#34;&gt;openai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/azure&#34;&gt;azure&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/aiml&#34;&gt;AI/ML API&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/aws_sagemaker&#34;&gt;aws - sagemaker&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/bedrock&#34;&gt;aws - bedrock&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/vertex&#34;&gt;google - vertex_ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/palm&#34;&gt;google - palm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/gemini&#34;&gt;google AI Studio - gemini&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/mistral&#34;&gt;mistral ai api&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/cloudflare_workers&#34;&gt;cloudflare AI Workers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/cohere&#34;&gt;cohere&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/anthropic&#34;&gt;anthropic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/empower&#34;&gt;empower&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/huggingface&#34;&gt;huggingface&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/replicate&#34;&gt;replicate&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/togetherai&#34;&gt;together_ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/openrouter&#34;&gt;openrouter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/ai21&#34;&gt;ai21&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/baseten&#34;&gt;baseten&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/vllm&#34;&gt;vllm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/nlp_cloud&#34;&gt;nlp_cloud&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/aleph_alpha&#34;&gt;aleph alpha&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/petals&#34;&gt;petals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/ollama&#34;&gt;ollama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/deepinfra&#34;&gt;deepinfra&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/perplexity&#34;&gt;perplexity-ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/groq&#34;&gt;Groq AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/deepseek&#34;&gt;Deepseek&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/anyscale&#34;&gt;anyscale&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/watsonx&#34;&gt;IBM - watsonx.ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/voyage&#34;&gt;voyage ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/xinference&#34;&gt;xinference [Xorbits Inference]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/friendliai&#34;&gt;FriendliAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/galadriel&#34;&gt;Galadriel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/&#34;&gt;&lt;strong&gt;Read the Docs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Interested in contributing? Contributions to LiteLLM Python SDK, Proxy Server, and contributing LLM integrations are both accepted and highly encouraged! &lt;a href=&#34;https://docs.litellm.ai/docs/extras/contributing_code&#34;&gt;See our Contribution Guide for more details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Enterprise&lt;/h1&gt; &#xA;&lt;p&gt;For companies that need better security, user management and professional support&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat&#34;&gt;Talk to founders&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This covers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;âœ… &lt;strong&gt;Features under the &lt;a href=&#34;https://docs.litellm.ai/docs/proxy/enterprise&#34;&gt;LiteLLM Commercial License&lt;/a&gt;:&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;âœ… &lt;strong&gt;Feature Prioritization&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;âœ… &lt;strong&gt;Custom Integrations&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;âœ… &lt;strong&gt;Professional Support - Dedicated discord + slack&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;âœ… &lt;strong&gt;Custom SLAs&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;âœ… &lt;strong&gt;Secure access with Single Sign-On&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Code Quality / Linting&lt;/h1&gt; &#xA;&lt;p&gt;LiteLLM follows the &lt;a href=&#34;https://google.github.io/styleguide/pyguide.html&#34;&gt;Google Python Style Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We run:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ruff for &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L320&#34;&gt;formatting and linting checks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mypy + Pyright for typing &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L90&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L4&#34;&gt;2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Black for &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L79&#34;&gt;formatting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;isort for &lt;a href=&#34;https://github.com/BerriAI/litellm/raw/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L10&#34;&gt;import sorting&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have suggestions on how to improve the code quality feel free to open an issue or a PR.&lt;/p&gt; &#xA;&lt;h1&gt;Support / talk with founders&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version&#34;&gt;Schedule Demo ðŸ‘‹&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/wuPM9dRgDw&#34;&gt;Community Discord ðŸ’­&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Our numbers ðŸ“ž +1 (770) 8783-106 / â€­+1 (412) 618-6238â€¬&lt;/li&gt; &#xA; &lt;li&gt;Our emails âœ‰ï¸ &lt;a href=&#34;mailto:ishaan@berri.ai&#34;&gt;ishaan@berri.ai&lt;/a&gt; / &lt;a href=&#34;mailto:krrish@berri.ai&#34;&gt;krrish@berri.ai&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Why did we build this&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Need for simplicity&lt;/strong&gt;: Our code started to get extremely complicated managing &amp;amp; translating calls between Azure, OpenAI and Cohere.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;!-- markdownlint-restore --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; &#xA;&lt;a href=&#34;https://github.com/BerriAI/litellm/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=BerriAI/litellm&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Run in Developer mode&lt;/h2&gt; &#xA;&lt;h3&gt;Services&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Setup .env file in root&lt;/li&gt; &#xA; &lt;li&gt;Run dependant services &lt;code&gt;docker-compose up db prometheus&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Backend&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(In root) create virtual environment &lt;code&gt;python -m venv .venv&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Activate virtual environment &lt;code&gt;source .venv/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies &lt;code&gt;pip install -e &#34;.[all]&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start proxy backend &lt;code&gt;uvicorn litellm.proxy.proxy_server:app --host localhost --port 4000 --reload&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Frontend&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to &lt;code&gt;ui/litellm-dashboard&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;npm run dev&lt;/code&gt; to start the dashboard&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>browser-use/browser-use</title>
    <updated>2025-03-16T01:49:48Z</updated>
    <id>tag:github.com,2025-03-16:/browser-use/browser-use</id>
    <link href="https://github.com/browser-use/browser-use" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make websites accessible for AI agents&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;./static/browser-use-dark.png&#34;&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;./static/browser-use.png&#34;&gt; &#xA; &lt;img alt=&#34;Shows a black Browser Use Logo in light color mode and a white one in dark color mode.&#34; src=&#34;https://raw.githubusercontent.com/browser-use/browser-use/main/static/browser-use.png&#34; width=&#34;full&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Enable AI to control your browser ðŸ¤–&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gregpr07/browser-use/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/gregpr07/browser-use?style=social&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cloud.browser-use.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-blue&#34; alt=&#34;Cloud&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.browser-use.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/gregpr07&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Gregor?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/mamagnus00&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Magnus?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&amp;amp;labelColor=#EC6341&#34; alt=&#34;Weave Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸŒ Browser-use is the easiest way to connect your AI agents with the browser.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ’¡ See what others are building and share your projects in our &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;Discord&lt;/a&gt;! Want Swag? Check out our &lt;a href=&#34;https://browsermerch.com&#34;&gt;Merch store&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ðŸŒ¤ï¸ Skip the setup - try our &lt;b&gt;hosted version&lt;/b&gt; for instant browser automation! &lt;b&gt;&lt;a href=&#34;https://cloud.browser-use.com&#34;&gt;Try the cloud â˜ï¸Ž&lt;/a&gt;&lt;/b&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;p&gt;With pip (Python&amp;gt;=3.11):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install browser-use&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;install playwright:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spin up your agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain_openai import ChatOpenAI&#xA;from browser_use import Agent&#xA;import asyncio&#xA;from dotenv import load_dotenv&#xA;load_dotenv()&#xA;&#xA;async def main():&#xA;    agent = Agent(&#xA;        task=&#34;Compare the price of gpt-4o and DeepSeek-V3&#34;,&#xA;        llm=ChatOpenAI(model=&#34;gpt-4o&#34;),&#xA;    )&#xA;    await agent.run()&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add your API keys for the provider you want to use to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other settings, models, and more, check out the &lt;a href=&#34;https://docs.browser-use.com&#34;&gt;documentation ðŸ“•&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Test with UI&lt;/h3&gt; &#xA;&lt;p&gt;You can test &lt;a href=&#34;https://github.com/browser-use/web-ui&#34;&gt;browser-use with a UI repository&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or simply run the gradio example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;uv pip install gradio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/ui/gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Demos&lt;/h1&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/use-cases/shopping.py&#34;&gt;Task&lt;/a&gt;: Add grocery items to cart, and checkout.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=L2Ya9PYNns8&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/d9359085-bde6-41d4-aa4e-6520d0221872&#34; alt=&#34;AI Did My Groceries&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: Add my latest LinkedIn follower to my leads in Salesforce.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/1440affc-a552-442e-b702-d0d3b277b0ae&#34; alt=&#34;LinkedIn to Salesforce&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/use-cases/find_and_apply_to_jobs.py&#34;&gt;Prompt&lt;/a&gt;: Read my CV &amp;amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.&#39;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&#34;&gt;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py&#34;&gt;Prompt&lt;/a&gt;: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa&#34; alt=&#34;Letter to Papa&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/custom-functions/save_to_file_hugging_face.py&#34;&gt;Prompt&lt;/a&gt;: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&#34;&gt;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;For more examples see the &lt;a href=&#34;https://raw.githubusercontent.com/browser-use/browser-use/main/examples&#34;&gt;examples&lt;/a&gt; folder or join the &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;Discord&lt;/a&gt; and show off your project.&lt;/p&gt; &#xA;&lt;h1&gt;Vision&lt;/h1&gt; &#xA;&lt;p&gt;Tell your computer what to do, and it gets it done.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;h3&gt;Agent&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve agent memory (summarize, compress, RAG, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enhance planning capabilities (load website specific context)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reduce token consumption (system prompt, DOM state)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;DOM Extraction&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve extraction for datepickers, dropdowns, special elements&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve state representation for UI elements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Rerunning tasks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; LLM as fallback&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make it easy to define workfows templates where LLM fills in the details&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Return playwright script from the agent&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create datasets for complex tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Benchmark various models against each other&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine-tuning models for specific tasks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;User Experience&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Human-in-the-loop execution&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve the generated GIF quality&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create various demos for tutorial execution, job application, QA testing, social media, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the &lt;code&gt;/docs&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Local Setup&lt;/h2&gt; &#xA;&lt;p&gt;To learn more about the library, check out the &lt;a href=&#34;https://docs.browser-use.com/development/local-setup&#34;&gt;local setup ðŸ“•&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Cooperations&lt;/h2&gt; &#xA;&lt;p&gt;We are forming a commission to define best practices for UI/UX design for browser agents. Together, we&#39;re exploring how software redesign improves the performance of AI agents and gives these companies a competitive advantage by designing their existing software to be at the forefront of the agent age.&lt;/p&gt; &#xA;&lt;p&gt;Email &lt;a href=&#34;mailto:tbiddle@loop11.com?subject=I%20want%20to%20join%20the%20UI/UX%20commission%20for%20AI%20agents&amp;amp;body=Hi%20Toby%2C%0A%0AI%20found%20you%20in%20the%20browser-use%20GitHub%20README.%0A%0A&#34;&gt;Toby&lt;/a&gt; to apply for a seat on the committee.&lt;/p&gt; &#xA;&lt;h2&gt;Swag&lt;/h2&gt; &#xA;&lt;p&gt;Want to show off your Browser-use swag? Check out our &lt;a href=&#34;https://browsermerch.com&#34;&gt;Merch store&lt;/a&gt;. Good contributors will receive swag for free ðŸ‘€.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Browser Use in your research or project, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{browser_use2024,&#xA;  author = {MÃ¼ller, Magnus and Å½uniÄ, Gregor},&#xA;  title = {Browser Use: Enable AI to control your browser},&#xA;  year = {2024},&#xA;  publisher = {GitHub},&#xA;  url = {https://github.com/browser-use/browser-use}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/user-attachments/assets/402b2129-b6ac-44d3-a217-01aea3277dce&#34; width=&#34;400&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://x.com/gregpr07&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Gregor?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/mamagnus00&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Magnus?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Made with â¤ï¸ in Zurich and San Francisco &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>