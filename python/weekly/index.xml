<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-19T02:00:35Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openai/DALL-E</title>
    <updated>2022-06-19T02:00:35Z</updated>
    <id>tag:github.com,2022-06-19:/openai/DALL-E</id>
    <link href="https://github.com/openai/DALL-E" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch package for the discrete VAE used for DALLÂ·E.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.com/blog/dall-e/&#34;&gt;[Blog]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2102.12092&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/openai/DALL-E/master/model_card.md&#34;&gt;[Model Card]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/openai/DALL-E/master/notebooks/usage.ipynb&#34;&gt;[Usage]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official PyTorch package for the discrete VAE used for DALLÂ·E. The transformer used to generate the images from the text is not part of this code release.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Before running &lt;a href=&#34;https://raw.githubusercontent.com/openai/DALL-E/master/notebooks/usage.ipynb&#34;&gt;the example notebook&lt;/a&gt;, you will need to install the package using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install DALL-E&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>google/jax</title>
    <updated>2022-06-19T02:00:35Z</updated>
    <id>tag:github.com,2022-06-19:/google/jax</id>
    <link href="https://github.com/google/jax" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png&#34; alt=&#34;logo&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;JAX: Autograd and XLA&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/google/jax/workflows/Continuous%20integration/badge.svg?sanitize=true&#34; alt=&#34;Continuous integration&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/jax&#34; alt=&#34;PyPI version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#quickstart-colab-in-the-cloud&#34;&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#transformations&#34;&gt;&lt;strong&gt;Transformations&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#installation&#34;&gt;&lt;strong&gt;Install guide&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#neural-network-libraries&#34;&gt;&lt;strong&gt;Neural net libraries&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://jax.readthedocs.io/en/latest/changelog.html&#34;&gt;&lt;strong&gt;Change logs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://jax.readthedocs.io/en/latest/&#34;&gt;&lt;strong&gt;Reference docs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is JAX?&lt;/h2&gt; &#xA;&lt;p&gt;JAX is &lt;a href=&#34;https://github.com/hips/autograd&#34;&gt;Autograd&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/xla&#34;&gt;XLA&lt;/a&gt;, brought together for high-performance machine learning research.&lt;/p&gt; &#xA;&lt;p&gt;With its updated version of &lt;a href=&#34;https://github.com/hips/autograd&#34;&gt;Autograd&lt;/a&gt;, JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation) via &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#automatic-differentiation-with-grad&#34;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; as well as forward-mode differentiation, and the two can be composed arbitrarily to any order.&lt;/p&gt; &#xA;&lt;p&gt;Whatâ€™s new is that JAX uses &lt;a href=&#34;https://www.tensorflow.org/xla&#34;&gt;XLA&lt;/a&gt; to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API, &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#compilation-with-jit&#34;&gt;&lt;code&gt;jit&lt;/code&gt;&lt;/a&gt;. Compilation and automatic differentiation can be composed arbitrarily, so you can express sophisticated algorithms and get maximal performance without leaving Python. You can even program multiple GPUs or TPU cores at once using &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#spmd-programming-with-pmap&#34;&gt;&lt;code&gt;pmap&lt;/code&gt;&lt;/a&gt;, and differentiate through the whole thing.&lt;/p&gt; &#xA;&lt;p&gt;Dig a little deeper, and you&#39;ll see that JAX is really an extensible system for &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#transformations&#34;&gt;composable function transformations&lt;/a&gt;. Both &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#automatic-differentiation-with-grad&#34;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#compilation-with-jit&#34;&gt;&lt;code&gt;jit&lt;/code&gt;&lt;/a&gt; are instances of such transformations. Others are &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#auto-vectorization-with-vmap&#34;&gt;&lt;code&gt;vmap&lt;/code&gt;&lt;/a&gt; for automatic vectorization and &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#spmd-programming-with-pmap&#34;&gt;&lt;code&gt;pmap&lt;/code&gt;&lt;/a&gt; for single-program multiple-data (SPMD) parallel programming of multiple accelerators, with more to come.&lt;/p&gt; &#xA;&lt;p&gt;This is a research project, not an official Google product. Expect bugs and &lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html&#34;&gt;sharp edges&lt;/a&gt;. Please help by trying it out, &lt;a href=&#34;https://github.com/google/jax/issues&#34;&gt;reporting bugs&lt;/a&gt;, and letting us know what you think!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jax.numpy as jnp&#xA;from jax import grad, jit, vmap&#xA;&#xA;def predict(params, inputs):&#xA;  for W, b in params:&#xA;    outputs = jnp.dot(inputs, W) + b&#xA;    inputs = jnp.tanh(outputs)  # inputs to the next layer&#xA;  return outputs                # no activation on last layer&#xA;&#xA;def loss(params, inputs, targets):&#xA;  preds = predict(params, inputs)&#xA;  return jnp.sum((preds - targets)**2)&#xA;&#xA;grad_loss = jit(grad(loss))  # compiled gradient evaluation function&#xA;perex_grads = jit(vmap(grad_loss, in_axes=(None, 0, 0)))  # fast per-example grads&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Contents&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#quickstart-colab-in-the-cloud&#34;&gt;Quickstart: Colab in the Cloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#transformations&#34;&gt;Transformations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#current-gotchas&#34;&gt;Current gotchas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#neural-network-libraries&#34;&gt;Neural net libraries&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#citing-jax&#34;&gt;Citing JAX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/#reference-documentation&#34;&gt;Reference documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart: Colab in the Cloud&lt;/h2&gt; &#xA;&lt;p&gt;Jump right in using a notebook in your browser, connected to a Google Cloud GPU. Here are some starter notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/quickstart.html&#34;&gt;The basics: NumPy on accelerators, &lt;code&gt;grad&lt;/code&gt; for differentiation, &lt;code&gt;jit&lt;/code&gt; for compilation, and &lt;code&gt;vmap&lt;/code&gt; for vectorization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb&#34;&gt;Training a Simple Neural Network, with TensorFlow Dataset Data Loading&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;JAX now runs on Cloud TPUs.&lt;/strong&gt; To try out the preview, see the &lt;a href=&#34;https://github.com/google/jax/tree/main/cloud_tpu_colabs&#34;&gt;Cloud TPU Colabs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a deeper dive into JAX:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html&#34;&gt;The Autodiff Cookbook, Part 1: easy and powerful automatic differentiation in JAX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html&#34;&gt;Common gotchas and sharp edges&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://github.com/google/jax/tree/main/docs/notebooks&#34;&gt;full list of notebooks&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also take a look at &lt;a href=&#34;https://github.com/google/jax/tree/main/jax/example_libraries/README.md&#34;&gt;the mini-libraries in &lt;code&gt;jax.example_libraries&lt;/code&gt;&lt;/a&gt;, like &lt;a href=&#34;https://github.com/google/jax/tree/main/jax/example_libraries/README.md#neural-net-building-with-stax&#34;&gt;&lt;code&gt;stax&lt;/code&gt; for building neural networks&lt;/a&gt; and &lt;a href=&#34;https://github.com/google/jax/tree/main/jax/example_libraries/README.md#first-order-optimization&#34;&gt;&lt;code&gt;optimizers&lt;/code&gt; for first-order stochastic optimization&lt;/a&gt;, or the &lt;a href=&#34;https://github.com/google/jax/tree/main/examples&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Transformations&lt;/h2&gt; &#xA;&lt;p&gt;At its core, JAX is an extensible system for transforming numerical functions. Here are four transformations of primary interest: &lt;code&gt;grad&lt;/code&gt;, &lt;code&gt;jit&lt;/code&gt;, &lt;code&gt;vmap&lt;/code&gt;, and &lt;code&gt;pmap&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Automatic differentiation with &lt;code&gt;grad&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;JAX has roughly the same API as &lt;a href=&#34;https://github.com/hips/autograd&#34;&gt;Autograd&lt;/a&gt;. The most popular function is &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.html#jax.grad&#34;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; for reverse-mode gradients:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from jax import grad&#xA;import jax.numpy as jnp&#xA;&#xA;def tanh(x):  # Define a function&#xA;  y = jnp.exp(-2.0 * x)&#xA;  return (1.0 - y) / (1.0 + y)&#xA;&#xA;grad_tanh = grad(tanh)  # Obtain its gradient function&#xA;print(grad_tanh(1.0))   # Evaluate it at x = 1.0&#xA;# prints 0.4199743&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can differentiate to any order with &lt;code&gt;grad&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(grad(grad(grad(tanh)))(1.0))&#xA;# prints 0.62162673&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more advanced autodiff, you can use &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.html#jax.vjp&#34;&gt;&lt;code&gt;jax.vjp&lt;/code&gt;&lt;/a&gt; for reverse-mode vector-Jacobian products and &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.html#jax.jvp&#34;&gt;&lt;code&gt;jax.jvp&lt;/code&gt;&lt;/a&gt; for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here&#39;s one way to compose those to make a function that efficiently computes &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.html#jax.hessian&#34;&gt;full Hessian matrices&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from jax import jit, jacfwd, jacrev&#xA;&#xA;def hessian(fun):&#xA;  return jit(jacfwd(jacrev(fun)))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As with &lt;a href=&#34;https://github.com/hips/autograd&#34;&gt;Autograd&lt;/a&gt;, you&#39;re free to use differentiation with Python control structures:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def abs_val(x):&#xA;  if x &amp;gt; 0:&#xA;    return x&#xA;  else:&#xA;    return -x&#xA;&#xA;abs_val_grad = grad(abs_val)&#xA;print(abs_val_grad(1.0))   # prints 1.0&#xA;print(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation&#34;&gt;reference docs on automatic differentiation&lt;/a&gt; and the &lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html&#34;&gt;JAX Autodiff Cookbook&lt;/a&gt; for more.&lt;/p&gt; &#xA;&lt;h3&gt;Compilation with &lt;code&gt;jit&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can use XLA to compile your functions end-to-end with &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit&#34;&gt;&lt;code&gt;jit&lt;/code&gt;&lt;/a&gt;, used either as an &lt;code&gt;@jit&lt;/code&gt; decorator or as a higher-order function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jax.numpy as jnp&#xA;from jax import jit&#xA;&#xA;def slow_f(x):&#xA;  # Element-wise ops see a large benefit from fusion&#xA;  return x * x + x * 2.0&#xA;&#xA;x = jnp.ones((5000, 5000))&#xA;fast_f = jit(slow_f)&#xA;%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X&#xA;%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can mix &lt;code&gt;jit&lt;/code&gt; and &lt;code&gt;grad&lt;/code&gt; and any other JAX transformation however you like.&lt;/p&gt; &#xA;&lt;p&gt;Using &lt;code&gt;jit&lt;/code&gt; puts constraints on the kind of Python control flow the function can use; see the &lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-+-JIT&#34;&gt;Gotchas Notebook&lt;/a&gt; for more.&lt;/p&gt; &#xA;&lt;h3&gt;Auto-vectorization with &lt;code&gt;vmap&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap&#34;&gt;&lt;code&gt;vmap&lt;/code&gt;&lt;/a&gt; is the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a functionâ€™s primitive operations for better performance.&lt;/p&gt; &#xA;&lt;p&gt;Using &lt;code&gt;vmap&lt;/code&gt; can save you from having to carry around batch dimensions in your code. For example, consider this simple &lt;em&gt;unbatched&lt;/em&gt; neural network prediction function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def predict(params, input_vec):&#xA;  assert input_vec.ndim == 1&#xA;  activations = input_vec&#xA;  for W, b in params:&#xA;    outputs = jnp.dot(W, activations) + b  # `activations` on the right-hand side!&#xA;    activations = jnp.tanh(outputs)        # inputs to the next layer&#xA;  return outputs                           # no activation on last layer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We often instead write &lt;code&gt;jnp.dot(activations, W)&lt;/code&gt; to allow for a batch dimension on the left side of &lt;code&gt;activations&lt;/code&gt;, but weâ€™ve written this particular prediction function to apply only to single input vectors. If we wanted to apply this function to a batch of inputs at once, semantically we could just write&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from functools import partial&#xA;predictions = jnp.stack(list(map(partial(predict, params), input_batch)))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;But pushing one example through the network at a time would be slow! Itâ€™s better to vectorize the computation, so that at every layer weâ€™re doing matrix-matrix multiplication rather than matrix-vector multiplication.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;vmap&lt;/code&gt; function does that transformation for us. That is, if we write&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from jax import vmap&#xA;predictions = vmap(partial(predict, params))(input_batch)&#xA;# or, alternatively&#xA;predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then the &lt;code&gt;vmap&lt;/code&gt; function will push the outer loop inside the function, and our machine will end up executing matrix-matrix multiplications exactly as if weâ€™d done the batching by hand.&lt;/p&gt; &#xA;&lt;p&gt;Itâ€™s easy enough to manually batch a simple neural network without &lt;code&gt;vmap&lt;/code&gt;, but in other cases manual vectorization can be impractical or impossible. Take the problem of efficiently computing per-example gradients: that is, for a fixed set of parameters, we want to compute the gradient of our loss function evaluated separately at each example in a batch. With &lt;code&gt;vmap&lt;/code&gt;, itâ€™s easy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;per_example_gradients = vmap(partial(grad(loss), params))(inputs, targets)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Of course, &lt;code&gt;vmap&lt;/code&gt; can be arbitrarily composed with &lt;code&gt;jit&lt;/code&gt;, &lt;code&gt;grad&lt;/code&gt;, and any other JAX transformation! We use &lt;code&gt;vmap&lt;/code&gt; with both forward- and reverse-mode automatic differentiation for fast Jacobian and Hessian matrix calculations in &lt;code&gt;jax.jacfwd&lt;/code&gt;, &lt;code&gt;jax.jacrev&lt;/code&gt;, and &lt;code&gt;jax.hessian&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SPMD programming with &lt;code&gt;pmap&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;For parallel programming of multiple accelerators, like multiple GPUs, use &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap&#34;&gt;&lt;code&gt;pmap&lt;/code&gt;&lt;/a&gt;. With &lt;code&gt;pmap&lt;/code&gt; you write single-program multiple-data (SPMD) programs, including fast parallel collective communication operations. Applying &lt;code&gt;pmap&lt;/code&gt; will mean that the function you write is compiled by XLA (similarly to &lt;code&gt;jit&lt;/code&gt;), then replicated and executed in parallel across devices.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example on an 8-GPU machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from jax import random, pmap&#xA;import jax.numpy as jnp&#xA;&#xA;# Create 8 random 5000 x 6000 matrices, one per GPU&#xA;keys = random.split(random.PRNGKey(0), 8)&#xA;mats = pmap(lambda key: random.normal(key, (5000, 6000)))(keys)&#xA;&#xA;# Run a local matmul on each device in parallel (no data transfer)&#xA;result = pmap(lambda x: jnp.dot(x, x.T))(mats)  # result.shape is (8, 5000, 5000)&#xA;&#xA;# Compute the mean on each device in parallel and print the result&#xA;print(pmap(jnp.mean)(result))&#xA;# prints [1.1566595 1.1805978 ... 1.2321935 1.2015157]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition to expressing pure maps, you can use fast &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators&#34;&gt;collective communication operations&lt;/a&gt; between devices:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from functools import partial&#xA;from jax import lax&#xA;&#xA;@partial(pmap, axis_name=&#39;i&#39;)&#xA;def normalize(x):&#xA;  return x / lax.psum(x, &#39;i&#39;)&#xA;&#xA;print(normalize(jnp.arange(4.)))&#xA;# prints [0.         0.16666667 0.33333334 0.5       ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can even &lt;a href=&#34;https://colab.research.google.com/github/google/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb#scrollTo=MdRscR5MONuN&#34;&gt;nest &lt;code&gt;pmap&lt;/code&gt; functions&lt;/a&gt; for more sophisticated communication patterns.&lt;/p&gt; &#xA;&lt;p&gt;It all composes, so you&#39;re free to differentiate through parallel computations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from jax import grad&#xA;&#xA;@pmap&#xA;def f(x):&#xA;  y = jnp.sin(x)&#xA;  @pmap&#xA;  def g(z):&#xA;    return jnp.cos(z) * jnp.tan(y.sum()) * jnp.tanh(x).sum()&#xA;  return grad(lambda w: jnp.sum(g(w)))(x)&#xA;&#xA;print(f(x))&#xA;# [[ 0.        , -0.7170853 ],&#xA;#  [-3.1085174 , -0.4824318 ],&#xA;#  [10.366636  , 13.135289  ],&#xA;#  [ 0.22163185, -0.52112055]]&#xA;&#xA;print(grad(lambda x: jnp.sum(f(x)))(x))&#xA;# [[ -3.2369726,  -1.6356447],&#xA;#  [  4.7572474,  11.606951 ],&#xA;#  [-98.524414 ,  42.76499  ],&#xA;#  [ -1.6007166,  -1.2568436]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When reverse-mode differentiating a &lt;code&gt;pmap&lt;/code&gt; function (e.g. with &lt;code&gt;grad&lt;/code&gt;), the backward pass of the computation is parallelized just like the forward pass.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://colab.research.google.com/github/google/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb&#34;&gt;SPMD Cookbook&lt;/a&gt; and the &lt;a href=&#34;https://github.com/google/jax/raw/main/examples/spmd_mnist_classifier_fromscratch.py&#34;&gt;SPMD MNIST classifier from scratch example&lt;/a&gt; for more.&lt;/p&gt; &#xA;&lt;h2&gt;Current gotchas&lt;/h2&gt; &#xA;&lt;p&gt;For a more thorough survey of current gotchas, with examples and explanations, we highly recommend reading the &lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html&#34;&gt;Gotchas Notebook&lt;/a&gt;. Some standouts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;JAX transformations only work on &lt;a href=&#34;https://en.wikipedia.org/wiki/Pure_function&#34;&gt;pure functions&lt;/a&gt;, which don&#39;t have side-effects and respect &lt;a href=&#34;https://en.wikipedia.org/wiki/Referential_transparency&#34;&gt;referential transparency&lt;/a&gt; (i.e. object identity testing with &lt;code&gt;is&lt;/code&gt; isn&#39;t preserved). If you use a JAX transformation on an impure Python function, you might see an error like &lt;code&gt;Exception: Can&#39;t lift Traced...&lt;/code&gt; or &lt;code&gt;Exception: Different traces at same level&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#in-place-updates&#34;&gt;In-place mutating updates of arrays&lt;/a&gt;, like &lt;code&gt;x[i] += y&lt;/code&gt;, aren&#39;t supported, but &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.ops.html&#34;&gt;there are functional alternatives&lt;/a&gt;. Under a &lt;code&gt;jit&lt;/code&gt;, those functional alternatives will reuse buffers in-place automatically.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#random-numbers&#34;&gt;Random numbers are different&lt;/a&gt;, but for &lt;a href=&#34;https://github.com/google/jax/raw/main/docs/design_notes/prng.md&#34;&gt;good reasons&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re looking for &lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/convolutions.html&#34;&gt;convolution operators&lt;/a&gt;, they&#39;re in the &lt;code&gt;jax.lax&lt;/code&gt; package.&lt;/li&gt; &#xA; &lt;li&gt;JAX enforces single-precision (32-bit, e.g. &lt;code&gt;float32&lt;/code&gt;) values by default, and &lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#double-64bit-precision&#34;&gt;to enable double-precision&lt;/a&gt; (64-bit, e.g. &lt;code&gt;float64&lt;/code&gt;) one needs to set the &lt;code&gt;jax_enable_x64&lt;/code&gt; variable at startup (or set the environment variable &lt;code&gt;JAX_ENABLE_X64=True&lt;/code&gt;). On TPU, JAX uses 32-bit values by default for everything &lt;em&gt;except&lt;/em&gt; internal temporary variables in &#39;matmul-like&#39; operations, such as &lt;code&gt;jax.numpy.dot&lt;/code&gt; and &lt;code&gt;lax.conv&lt;/code&gt;. Those ops have a &lt;code&gt;precision&lt;/code&gt; parameter which can be used to simulate true 32-bit, with a cost of possibly slower runtime.&lt;/li&gt; &#xA; &lt;li&gt;Some of NumPy&#39;s dtype promotion semantics involving a mix of Python scalars and NumPy types aren&#39;t preserved, namely &lt;code&gt;np.add(1, np.array([2], np.float32)).dtype&lt;/code&gt; is &lt;code&gt;float64&lt;/code&gt; rather than &lt;code&gt;float32&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Some transformations, like &lt;code&gt;jit&lt;/code&gt;, &lt;a href=&#34;https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow&#34;&gt;constrain how you can use Python control flow&lt;/a&gt;. You&#39;ll always get loud errors if something goes wrong. You might have to use &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit&#34;&gt;&lt;code&gt;jit&lt;/code&gt;&#39;s &lt;code&gt;static_argnums&lt;/code&gt; parameter&lt;/a&gt;, &lt;a href=&#34;https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators&#34;&gt;structured control flow primitives&lt;/a&gt; like &lt;a href=&#34;https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan&#34;&gt;&lt;code&gt;lax.scan&lt;/code&gt;&lt;/a&gt;, or just use &lt;code&gt;jit&lt;/code&gt; on smaller subfunctions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;JAX is written in pure Python, but it depends on XLA, which needs to be installed as the &lt;code&gt;jaxlib&lt;/code&gt; package. Use the following instructions to install a binary package with &lt;code&gt;pip&lt;/code&gt;, or to build JAX from source.&lt;/p&gt; &#xA;&lt;p&gt;We support installing or building &lt;code&gt;jaxlib&lt;/code&gt; on Linux (Ubuntu 16.04 or later) and macOS (10.12 or later) platforms. Windows users can use JAX on CPU and GPU via the &lt;a href=&#34;https://docs.microsoft.com/en-us/windows/wsl/about&#34;&gt;Windows Subsystem for Linux&lt;/a&gt;. There is some initial native Windows support, but since it is still somewhat immature, there are no binary releases and it must be &lt;a href=&#34;https://jax.readthedocs.io/en/latest/developer.html#additional-notes-for-building-jaxlib-from-source-on-windows&#34;&gt;built from source&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;pip installation: CPU&lt;/h3&gt; &#xA;&lt;p&gt;To install a CPU-only version of JAX, which might be useful for doing local development on a laptop, you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip&#xA;pip install --upgrade &#34;jax[cpu]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Linux, it is often necessary to first update &lt;code&gt;pip&lt;/code&gt; to a version that supports &lt;code&gt;manylinux2014&lt;/code&gt; wheels.&lt;/p&gt; &#xA;&lt;h3&gt;pip installation: GPU (CUDA)&lt;/h3&gt; &#xA;&lt;p&gt;If you want to install JAX with both CPU and NVidia GPU support, you must first install &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA&lt;/a&gt; and &lt;a href=&#34;https://developer.nvidia.com/CUDNN&#34;&gt;CuDNN&lt;/a&gt;, if they have not already been installed. Unlike some other popular deep learning systems, JAX does not bundle CUDA or CuDNN as part of the &lt;code&gt;pip&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;p&gt;JAX provides pre-built CUDA-compatible wheels for &lt;strong&gt;Linux only&lt;/strong&gt;, with CUDA 11.1 or newer, and CuDNN 8.0.5 or newer. Other combinations of operating system, CUDA, and CuDNN are possible, but require building from source.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA 11.1 or newer is &lt;em&gt;required&lt;/em&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You may be able to use older CUDA versions if you build from source, but there are known bugs in CUDA in all CUDA versions older than 11.1, so we do not ship prebuilt binaries for older CUDA versions.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The supported cuDNN versions for the prebuilt wheels are: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;cuDNN 8.2 or newer. We recommend using the cuDNN 8.2 wheel if your cuDNN installation is new enough, since it supports additional functionality.&lt;/li&gt; &#xA;   &lt;li&gt;cuDNN 8.0.5 or newer.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;You &lt;em&gt;must&lt;/em&gt; use an NVidia driver version that is at least as new as your &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-major-component-versions__table-cuda-toolkit-driver-versions&#34;&gt;CUDA toolkit&#39;s corresponding driver version&lt;/a&gt;. For example, if you have CUDA 11.4 update 4 installed, you must use NVidia driver 470.82.01 or newer if on Linux. This is a strict requirement that exists because JAX relies on JIT-compiling code; older drivers may lead to failures. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you need to use an newer CUDA toolkit with an older driver, for example on a cluster where you cannot update the NVidia driver easily, you may be able to use the &lt;a href=&#34;https://docs.nvidia.com/deploy/cuda-compatibility/&#34;&gt;CUDA forward compatibility packages&lt;/a&gt; that NVidia provides for this purpose.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Next, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip&#xA;# Installs the wheel compatible with CUDA 11 and cuDNN 8.2 or newer.&#xA;# Note: wheels only available on linux.&#xA;pip install --upgrade &#34;jax[cuda]&#34; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The jaxlib version must correspond to the version of the existing CUDA installation you want to use. You can specify a particular CUDA and CuDNN version for jaxlib explicitly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip&#xA;&#xA;# Installs the wheel compatible with Cuda &amp;gt;= 11.4 and cudnn &amp;gt;= 8.2&#xA;pip install &#34;jax[cuda11_cudnn82]&#34; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html&#xA;&#xA;# Installs the wheel compatible with Cuda &amp;gt;= 11.1 and cudnn &amp;gt;= 8.0.5&#xA;pip install &#34;jax[cuda11_cudnn805]&#34; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find your CUDA version with the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvcc --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some GPU functionality expects the CUDA installation to be at &lt;code&gt;/usr/local/cuda-X.X&lt;/code&gt;, where X.X should be replaced with the CUDA version number (e.g. &lt;code&gt;cuda-11.1&lt;/code&gt;). If CUDA is installed elsewhere on your system, you can either create a symlink:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ln -s /path/to/cuda /usr/local/cuda-X.X&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please let us know on &lt;a href=&#34;https://github.com/google/jax/issues&#34;&gt;the issue tracker&lt;/a&gt; if you run into any errors or problems with the prebuilt wheels.&lt;/p&gt; &#xA;&lt;h3&gt;pip installation: Google Cloud TPU&lt;/h3&gt; &#xA;&lt;p&gt;JAX also provides pre-built wheels for &lt;a href=&#34;https://cloud.google.com/tpu/docs/users-guide-tpu-vm&#34;&gt;Google Cloud TPU&lt;/a&gt;. To install JAX along with appropriate versions of &lt;code&gt;jaxlib&lt;/code&gt; and &lt;code&gt;libtpu&lt;/code&gt;, you can run the following in your cloud TPU VM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip&#xA;pip install &#34;jax[tpu]&amp;gt;=0.2.16&#34; -f https://storage.googleapis.com/jax-releases/libtpu_releases.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;pip installation: Colab TPU&lt;/h3&gt; &#xA;&lt;p&gt;Colab TPU runtimes come with JAX pre-installed, but before importing JAX you must run the following code to initialize the TPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jax.tools.colab_tpu&#xA;jax.tools.colab_tpu.setup_tpu()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Colab TPU runtimes use an older TPU architecture than Cloud TPU VMs, so installing &lt;code&gt;jax[tpu]&lt;/code&gt; should be avoided on Colab. If for any reason you would like to update the jax &amp;amp; jaxlib libraries on a Colab TPU runtime, follow the CPU instructions above (i.e. install &lt;code&gt;jax[cpu]&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Building JAX from source&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://jax.readthedocs.io/en/latest/developer.html#building-from-source&#34;&gt;Building JAX from source&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Neural network libraries&lt;/h2&gt; &#xA;&lt;p&gt;Multiple Google research groups develop and share libraries for training neural networks in JAX. If you want a fully featured library for neural network training with examples and how-to guides, try &lt;a href=&#34;https://github.com/google/flax&#34;&gt;Flax&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In addition, DeepMind has open-sourced an &lt;a href=&#34;https://deepmind.com/blog/article/using-jax-to-accelerate-our-research&#34;&gt;ecosystem of libraries around JAX&lt;/a&gt; including &lt;a href=&#34;https://github.com/deepmind/dm-haiku&#34;&gt;Haiku&lt;/a&gt; for neural network modules, &lt;a href=&#34;https://github.com/deepmind/optax&#34;&gt;Optax&lt;/a&gt; for gradient processing and optimization, &lt;a href=&#34;https://github.com/deepmind/rlax&#34;&gt;RLax&lt;/a&gt; for RL algorithms, and &lt;a href=&#34;https://github.com/deepmind/chex&#34;&gt;chex&lt;/a&gt; for reliable code and testing. (Watch the NeurIPS 2020 JAX Ecosystem at DeepMind talk &lt;a href=&#34;https://www.youtube.com/watch?v=iDxJxIyzSiM&#34;&gt;here&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Citing JAX&lt;/h2&gt; &#xA;&lt;p&gt;To cite this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{jax2018github,&#xA;  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},&#xA;  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},&#xA;  url = {http://github.com/google/jax},&#xA;  version = {0.3.13},&#xA;  year = {2018},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the above bibtex entry, names are in alphabetical order, the version number is intended to be that from &lt;a href=&#34;https://raw.githubusercontent.com/google/jax/main/jax/version.py&#34;&gt;jax/version.py&lt;/a&gt;, and the year corresponds to the project&#39;s open-source release.&lt;/p&gt; &#xA;&lt;p&gt;A nascent version of JAX, supporting only automatic differentiation and compilation to XLA, was described in a &lt;a href=&#34;https://mlsys.org/Conferences/2019/doc/2018/146.pdf&#34;&gt;paper that appeared at SysML 2018&lt;/a&gt;. We&#39;re currently working on covering JAX&#39;s ideas and capabilities in a more comprehensive and up-to-date paper.&lt;/p&gt; &#xA;&lt;h2&gt;Reference documentation&lt;/h2&gt; &#xA;&lt;p&gt;For details about the JAX API, see the &lt;a href=&#34;https://jax.readthedocs.io/&#34;&gt;reference documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For getting started as a JAX developer, see the &lt;a href=&#34;https://jax.readthedocs.io/en/latest/developer.html&#34;&gt;developer documentation&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deepmind/alphafold</title>
    <updated>2022-06-19T02:00:35Z</updated>
    <id>tag:github.com,2022-06-19:/deepmind/alphafold</id>
    <link href="https://github.com/deepmind/alphafold" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open source code for AlphaFold.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/imgs/header.jpg&#34; alt=&#34;header&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;AlphaFold&lt;/h1&gt; &#xA;&lt;p&gt;This package provides an implementation of the inference pipeline of AlphaFold v2.0. This is a completely new model that was entered in CASP14 and published in Nature. For simplicity, we refer to this model as AlphaFold throughout the rest of this document.&lt;/p&gt; &#xA;&lt;p&gt;We also provide an implementation of AlphaFold-Multimer. This represents a work in progress and AlphaFold-Multimer isn&#39;t expected to be as stable as our monomer AlphaFold system. &lt;a href=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/#updating-existing-alphafold-installation-to-include-alphafold-multimers&#34;&gt;Read the guide&lt;/a&gt; for how to upgrade and update code.&lt;/p&gt; &#xA;&lt;p&gt;Any publication that discloses findings arising from using this source code or the model parameters should &lt;a href=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/#citing-this-work&#34;&gt;cite&lt;/a&gt; the &lt;a href=&#34;https://doi.org/10.1038/s41586-021-03819-2&#34;&gt;AlphaFold paper&lt;/a&gt; and, if applicable, the &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1&#34;&gt;AlphaFold-Multimer paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please also refer to the &lt;a href=&#34;https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_MOESM1_ESM.pdf&#34;&gt;Supplementary Information&lt;/a&gt; for a detailed description of the method.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can use a slightly simplified version of AlphaFold with &lt;a href=&#34;https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb&#34;&gt;this Colab notebook&lt;/a&gt;&lt;/strong&gt; or community-supported versions (see below).&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions, please contact the AlphaFold team at &lt;a href=&#34;mailto:alphafold@deepmind.com&#34;&gt;alphafold@deepmind.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/imgs/casp14_predictions.gif&#34; alt=&#34;CASP14 predictions&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;First time setup&lt;/h2&gt; &#xA;&lt;p&gt;You will need a machine running Linux, AlphaFold does not support other operating systems.&lt;/p&gt; &#xA;&lt;p&gt;The following steps are required in order to run AlphaFold:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt; for GPU support.&lt;/li&gt; &#xA;   &lt;li&gt;Setup running &lt;a href=&#34;https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user&#34;&gt;Docker as a non-root user&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download genetic databases (see below).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download model parameters (see below).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check that AlphaFold will be able to use a GPU by running:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output of this command should show a list of your GPUs. If it doesn&#39;t, check if you followed all steps correctly when setting up the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt; or take a look at the following &lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker/issues/1447#issuecomment-801479573&#34;&gt;NVIDIA Docker issue&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you wish to run AlphaFold using Singularity (a common containerization platform on HPC systems) we recommend using some of the third party Singularity setups as linked in &lt;a href=&#34;https://github.com/deepmind/alphafold/issues/10&#34;&gt;https://github.com/deepmind/alphafold/issues/10&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/alphafold/issues/24&#34;&gt;https://github.com/deepmind/alphafold/issues/24&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Genetic databases&lt;/h3&gt; &#xA;&lt;p&gt;This step requires &lt;code&gt;aria2c&lt;/code&gt; to be installed on your machine.&lt;/p&gt; &#xA;&lt;p&gt;AlphaFold needs multiple genetic (sequence) databases to run:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bfd.mmseqs.com/&#34;&gt;BFD&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ebi.ac.uk/metagenomics/&#34;&gt;MGnify&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/&#34;&gt;PDB70&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rcsb.org/&#34;&gt;PDB&lt;/a&gt; (structures in the mmCIF format),&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rcsb.org/&#34;&gt;PDB seqres&lt;/a&gt; â€“ only for AlphaFold-Multimer,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://uniclust.mmseqs.com/&#34;&gt;Uniclust30&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uniprot.org/uniprot/&#34;&gt;UniProt&lt;/a&gt; â€“ only for AlphaFold-Multimer,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uniprot.org/help/uniref&#34;&gt;UniRef90&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We provide a script &lt;code&gt;scripts/download_all_data.sh&lt;/code&gt; that can be used to download and set up all of these databases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Default:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/download_all_data.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;will download the full databases.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;With &lt;code&gt;reduced_dbs&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/download_all_data.sh &amp;lt;DOWNLOAD_DIR&amp;gt; reduced_dbs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;will download a reduced version of the databases to be used with the &lt;code&gt;reduced_dbs&lt;/code&gt; database preset.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸ“’&lt;/span&gt; &lt;strong&gt;Note: The download directory &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt; should &lt;em&gt;not&lt;/em&gt; be a subdirectory in the AlphaFold repository directory.&lt;/strong&gt; If it is, the Docker build will be slow as the large databases will be copied during the image creation.&lt;/p&gt; &#xA;&lt;p&gt;We don&#39;t provide exactly the database versions used in CASP14 â€“ see the &lt;a href=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/#note-on-reproducibility&#34;&gt;note on reproducibility&lt;/a&gt;. Some of the databases are mirrored for speed, see &lt;a href=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/#mirrored-databases&#34;&gt;mirrored databases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸ“’&lt;/span&gt; &lt;strong&gt;Note: The total download size for the full databases is around 415 GB and the total size when unzipped is 2.2 TB. Please make sure you have a large enough hard drive space, bandwidth and time to download. We recommend using an SSD for better genetic search performance.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;download_all_data.sh&lt;/code&gt; script will also download the model parameter files. Once the script has finished, you should have the following directory structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$DOWNLOAD_DIR/                             # Total: ~ 2.2 TB (download: 438 GB)&#xA;    bfd/                                   # ~ 1.7 TB (download: 271.6 GB)&#xA;        # 6 files.&#xA;    mgnify/                                # ~ 64 GB (download: 32.9 GB)&#xA;        mgy_clusters_2018_12.fa&#xA;    params/                                # ~ 3.5 GB (download: 3.5 GB)&#xA;        # 5 CASP14 models,&#xA;        # 5 pTM models,&#xA;        # 5 AlphaFold-Multimer models,&#xA;        # LICENSE,&#xA;        # = 16 files.&#xA;    pdb70/                                 # ~ 56 GB (download: 19.5 GB)&#xA;        # 9 files.&#xA;    pdb_mmcif/                             # ~ 206 GB (download: 46 GB)&#xA;        mmcif_files/&#xA;            # About 180,000 .cif files.&#xA;        obsolete.dat&#xA;    pdb_seqres/                            # ~ 0.2 GB (download: 0.2 GB)&#xA;        pdb_seqres.txt&#xA;    small_bfd/                             # ~ 17 GB (download: 9.6 GB)&#xA;        bfd-first_non_consensus_sequences.fasta&#xA;    uniclust30/                            # ~ 86 GB (download: 24.9 GB)&#xA;        uniclust30_2018_08/&#xA;            # 13 files.&#xA;    uniprot/                               # ~ 98.3 GB (download: 49 GB)&#xA;        uniprot.fasta&#xA;    uniref90/                              # ~ 58 GB (download: 29.7 GB)&#xA;        uniref90.fasta&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;bfd/&lt;/code&gt; is only downloaded if you download the full databases, and &lt;code&gt;small_bfd/&lt;/code&gt; is only downloaded if you download the reduced databases.&lt;/p&gt; &#xA;&lt;h3&gt;Model parameters&lt;/h3&gt; &#xA;&lt;p&gt;While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters are made available under the terms of the CC BY 4.0 license. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/#license-and-disclaimer&#34;&gt;Disclaimer&lt;/a&gt; below for more detail.&lt;/p&gt; &#xA;&lt;p&gt;The AlphaFold parameters are available from &lt;a href=&#34;https://storage.googleapis.com/alphafold/alphafold_params_2022-03-02.tar&#34;&gt;https://storage.googleapis.com/alphafold/alphafold_params_2022-03-02.tar&lt;/a&gt;, and are downloaded as part of the &lt;code&gt;scripts/download_all_data.sh&lt;/code&gt; script. This script will download parameters for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5 models which were used during CASP14, and were extensively validated for structure prediction quality (see Jumper et al. 2021, Suppl. Methods 1.12 for details).&lt;/li&gt; &#xA; &lt;li&gt;5 pTM models, which were fine-tuned to produce pTM (predicted TM-score) and (PAE) predicted aligned error values alongside their structure predictions (see Jumper et al. 2021, Suppl. Methods 1.9.7 for details).&lt;/li&gt; &#xA; &lt;li&gt;5 AlphaFold-Multimer models that produce pTM and PAE values alongside their structure predictions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Updating existing AlphaFold installation to include AlphaFold-Multimers&lt;/h3&gt; &#xA;&lt;p&gt;If you have AlphaFold v2.0.0 or v2.0.1 you can either reinstall AlphaFold fully from scratch (remove everything and run the setup from scratch) or you can do an incremental update that will be significantly faster but will require a bit more work. Make sure you follow these steps in the exact order they are listed below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Update the code.&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Go to the directory with the cloned AlphaFold repository and run &lt;code&gt;git fetch origin main&lt;/code&gt; to get all code updates.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Download the UniProt and PDB seqres databases.&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;scripts/download_uniprot.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Remove &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;/pdb_mmcif&lt;/code&gt;. It is needed to have PDB SeqRes and PDB from exactly the same date. Failure to do this step will result in potential errors when searching for templates when running AlphaFold-Multimer.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;scripts/download_pdb_mmcif.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;scripts/download_pdb_seqres.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Update the model parameters.&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Remove the old model parameters in &lt;code&gt;&amp;lt;DOWNLOAD_DIR&amp;gt;/params&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Download new model parameters using &lt;code&gt;scripts/download_alphafold_params.sh &amp;lt;DOWNLOAD_DIR&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Follow &lt;a href=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/#running-alphafold&#34;&gt;Running AlphaFold&lt;/a&gt;.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;API changes between v2.0.0 and v2.1.0&lt;/h4&gt; &#xA;&lt;p&gt;We tried to keep the API as much backwards compatible as possible, but we had to change the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;RunModel.predict()&lt;/code&gt; now needs a &lt;code&gt;random_seed&lt;/code&gt; argument as MSA sampling happens inside the Multimer model.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;preset&lt;/code&gt; flag in &lt;code&gt;run_alphafold.py&lt;/code&gt; and &lt;code&gt;run_docker.py&lt;/code&gt; was split into &lt;code&gt;db_preset&lt;/code&gt; and &lt;code&gt;model_preset&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The models to use are not specified using &lt;code&gt;model_names&lt;/code&gt; but rather using the &lt;code&gt;model_preset&lt;/code&gt; flag. If you want to customize which models are used for each preset, you will have to modify the the &lt;code&gt;MODEL_PRESETS&lt;/code&gt; dictionary in &lt;code&gt;alphafold/model/config.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Setting the &lt;code&gt;data_dir&lt;/code&gt; flag is now needed when using &lt;code&gt;run_docker.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;API changes between v2.1.0 and v2.2.0&lt;/h4&gt; &#xA;&lt;p&gt;The AlphaFold-Multimer model weights have been updated, these new models have greatly reduced numbers of clashes on average and are slightly more accurate.&lt;/p&gt; &#xA;&lt;p&gt;A flag &lt;code&gt;--num_multimer_predictions_per_model&lt;/code&gt; has been added that controls how many predictions will be made per model, by default the offline system will run each model 5 times for a total of 25 predictions.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;--is_prokaryote_list&lt;/code&gt; flag has been removed along with the &lt;code&gt;is_prokaryote&lt;/code&gt; argument in &lt;code&gt;run_alphafold.predict_structure()&lt;/code&gt;, eukaryotes and prokaryotes are now paired in the same way.&lt;/p&gt; &#xA;&lt;p&gt;To use the deprecated v2.1.0 AlphaFold-Multimer model weights:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Change &lt;code&gt;SOURCE_URL&lt;/code&gt; in &lt;code&gt;scripts/download_alphafold_params.sh&lt;/code&gt; to &lt;code&gt;https://storage.googleapis.com/alphafold/alphafold_params_2022-01-19.tar&lt;/code&gt;, and download the old parameters.&lt;/li&gt; &#xA; &lt;li&gt;Remove the &lt;code&gt;_v2&lt;/code&gt; in the multimer &lt;code&gt;MODEL_PRESETS&lt;/code&gt; in &lt;code&gt;config.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Running AlphaFold&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;The simplest way to run AlphaFold is using the provided Docker script.&lt;/strong&gt; This was tested on Google Cloud with a machine using the &lt;code&gt;nvidia-gpu-cloud-image&lt;/code&gt; with 12 vCPUs, 85 GB of RAM, a 100 GB boot disk, the databases on an additional 3 TB disk, and an A100 GPU.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this repository and &lt;code&gt;cd&lt;/code&gt; into it.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/deepmind/alphafold.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the Docker image:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -f docker/Dockerfile -t alphafold .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you encounter the following error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease: The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC&#xA;E: The repository &#39;https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease&#39; is not signed.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;use the workaround described in &lt;a href=&#34;https://github.com/deepmind/alphafold/issues/463#issuecomment-1124881779&#34;&gt;https://github.com/deepmind/alphafold/issues/463#issuecomment-1124881779&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the &lt;code&gt;run_docker.py&lt;/code&gt; dependencies. Note: You may optionally wish to create a &lt;a href=&#34;https://docs.python.org/3/tutorial/venv.html&#34;&gt;Python Virtual Environment&lt;/a&gt; to prevent conflicts with your system&#39;s Python environment.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install -r docker/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure that the output directory exists (the default is &lt;code&gt;/tmp/alphafold&lt;/code&gt;) and that you have sufficient permissions to write into it. You can make sure that is the case by manually running &lt;code&gt;mkdir /tmp/alphafold&lt;/code&gt; and &lt;code&gt;chmod 770 /tmp/alphafold&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;run_docker.py&lt;/code&gt; pointing to a FASTA file containing the protein sequence(s) for which you wish to predict the structure. If you are predicting the structure of a protein that is already in PDB and you wish to avoid using it as a template, then &lt;code&gt;max_template_date&lt;/code&gt; must be set to be before the release date of the structure. You must also provide the path to the directory containing the downloaded databases. For example, for the T1050 CASP14 target:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=T1050.fasta \&#xA;  --max_template_date=2020-05-14 \&#xA;  --data_dir=$DOWNLOAD_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By default, Alphafold will attempt to use all visible GPU devices. To use a subset, specify a comma-separated list of GPU UUID(s) or index(es) using the &lt;code&gt;--gpu_devices&lt;/code&gt; flag. See &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html#gpu-enumeration&#34;&gt;GPU enumeration&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can control which AlphaFold model to run by adding the &lt;code&gt;--model_preset=&lt;/code&gt; flag. We provide the following models:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;monomer&lt;/strong&gt;: This is the original model used at CASP14 with no ensembling.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;monomer_casp14&lt;/strong&gt;: This is the original model used at CASP14 with &lt;code&gt;num_ensemble=8&lt;/code&gt;, matching our CASP14 configuration. This is largely provided for reproducibility as it is 8x more computationally expensive for limited accuracy gain (+0.1 average GDT gain on CASP14 domains).&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;monomer_ptm&lt;/strong&gt;: This is the original CASP14 model fine tuned with the pTM head, providing a pairwise confidence measure. It is slightly less accurate than the normal monomer model.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;multimer&lt;/strong&gt;: This is the &lt;a href=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/#citing-this-work&#34;&gt;AlphaFold-Multimer&lt;/a&gt; model. To use this model, provide a multi-sequence FASTA file. In addition, the UniProt database should have been downloaded.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can control MSA speed/quality tradeoff by adding &lt;code&gt;--db_preset=reduced_dbs&lt;/code&gt; or &lt;code&gt;--db_preset=full_dbs&lt;/code&gt; to the run command. We provide the following presets:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;reduced_dbs&lt;/strong&gt;: This preset is optimized for speed and lower hardware requirements. It runs with a reduced version of the BFD database. It requires 8 CPU cores (vCPUs), 8 GB of RAM, and 600 GB of disk space.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;full_dbs&lt;/strong&gt;: This runs with all genetic databases used at CASP14.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Running the command above with the &lt;code&gt;monomer&lt;/code&gt; model preset and the &lt;code&gt;reduced_dbs&lt;/code&gt; data preset would look like this:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=T1050.fasta \&#xA;  --max_template_date=2020-05-14 \&#xA;  --model_preset=monomer \&#xA;  --db_preset=reduced_dbs \&#xA;  --data_dir=$DOWNLOAD_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Running AlphaFold-Multimer&lt;/h3&gt; &#xA;&lt;p&gt;All steps are the same as when running the monomer system, but you will have to&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;provide an input fasta with multiple sequences,&lt;/li&gt; &#xA; &lt;li&gt;set &lt;code&gt;--model_preset=multimer&lt;/code&gt;,&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An example that folds a protein complex &lt;code&gt;multimer.fasta&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=multimer.fasta \&#xA;  --max_template_date=2020-05-14 \&#xA;  --model_preset=multimer \&#xA;  --data_dir=$DOWNLOAD_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default the multimer system will run 5 seeds per model (25 total predictions) for a small drop in accuracy you may wish to run a single seed per model. This can be done via the &lt;code&gt;--num_multimer_predictions_per_model&lt;/code&gt; flag, e.g. set it to &lt;code&gt;--num_multimer_predictions_per_model=1&lt;/code&gt; to run a single seed per model.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;Below are examples on how to use AlphaFold in different scenarios.&lt;/p&gt; &#xA;&lt;h4&gt;Folding a monomer&lt;/h4&gt; &#xA;&lt;p&gt;Say we have a monomer with the sequence &lt;code&gt;&amp;lt;SEQUENCE&amp;gt;&lt;/code&gt;. The input fasta should be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-fasta&#34;&gt;&amp;gt;sequence_name&#xA;&amp;lt;SEQUENCE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=monomer.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=monomer \&#xA;  --data_dir=$DOWNLOAD_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Folding a homomer&lt;/h4&gt; &#xA;&lt;p&gt;Say we have a homomer with 3 copies of the same sequence &lt;code&gt;&amp;lt;SEQUENCE&amp;gt;&lt;/code&gt;. The input fasta should be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-fasta&#34;&gt;&amp;gt;sequence_1&#xA;&amp;lt;SEQUENCE&amp;gt;&#xA;&amp;gt;sequence_2&#xA;&amp;lt;SEQUENCE&amp;gt;&#xA;&amp;gt;sequence_3&#xA;&amp;lt;SEQUENCE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=homomer.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=multimer \&#xA;  --data_dir=$DOWNLOAD_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Folding a heteromer&lt;/h4&gt; &#xA;&lt;p&gt;Say we have an A2B3 heteromer, i.e. with 2 copies of &lt;code&gt;&amp;lt;SEQUENCE A&amp;gt;&lt;/code&gt; and 3 copies of &lt;code&gt;&amp;lt;SEQUENCE B&amp;gt;&lt;/code&gt;. The input fasta should be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-fasta&#34;&gt;&amp;gt;sequence_1&#xA;&amp;lt;SEQUENCE A&amp;gt;&#xA;&amp;gt;sequence_2&#xA;&amp;lt;SEQUENCE A&amp;gt;&#xA;&amp;gt;sequence_3&#xA;&amp;lt;SEQUENCE B&amp;gt;&#xA;&amp;gt;sequence_4&#xA;&amp;lt;SEQUENCE B&amp;gt;&#xA;&amp;gt;sequence_5&#xA;&amp;lt;SEQUENCE B&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=heteromer.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=multimer \&#xA;  --data_dir=$DOWNLOAD_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Folding multiple monomers one after another&lt;/h4&gt; &#xA;&lt;p&gt;Say we have a two monomers, &lt;code&gt;monomer1.fasta&lt;/code&gt; and &lt;code&gt;monomer2.fasta&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We can fold both sequentially by using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=monomer1.fasta,monomer2.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=monomer \&#xA;  --data_dir=$DOWNLOAD_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Folding multiple multimers one after another&lt;/h4&gt; &#xA;&lt;p&gt;Say we have a two multimers, &lt;code&gt;multimer1.fasta&lt;/code&gt; and &lt;code&gt;multimer2.fasta&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We can fold both sequentially by using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 docker/run_docker.py \&#xA;  --fasta_paths=multimer1.fasta,multimer2.fasta \&#xA;  --max_template_date=2021-11-01 \&#xA;  --model_preset=multimer \&#xA;  --data_dir=$DOWNLOAD_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;AlphaFold output&lt;/h3&gt; &#xA;&lt;p&gt;The outputs will be saved in a subdirectory of the directory provided via the &lt;code&gt;--output_dir&lt;/code&gt; flag of &lt;code&gt;run_docker.py&lt;/code&gt; (defaults to &lt;code&gt;/tmp/alphafold/&lt;/code&gt;). The outputs include the computed MSAs, unrelaxed structures, relaxed structures, ranked structures, raw model outputs, prediction metadata, and section timings. The &lt;code&gt;--output_dir&lt;/code&gt; directory will have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;target_name&amp;gt;/&#xA;    features.pkl&#xA;    ranked_{0,1,2,3,4}.pdb&#xA;    ranking_debug.json&#xA;    relaxed_model_{1,2,3,4,5}.pdb&#xA;    result_model_{1,2,3,4,5}.pkl&#xA;    timings.json&#xA;    unrelaxed_model_{1,2,3,4,5}.pdb&#xA;    msas/&#xA;        bfd_uniclust_hits.a3m&#xA;        mgnify_hits.sto&#xA;        uniref90_hits.sto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The contents of each output file are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;features.pkl&lt;/code&gt; â€“ A &lt;code&gt;pickle&lt;/code&gt; file containing the input feature NumPy arrays used by the models to produce the structures.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;unrelaxed_model_*.pdb&lt;/code&gt; â€“ A PDB format text file containing the predicted structure, exactly as outputted by the model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;relaxed_model_*.pdb&lt;/code&gt; â€“ A PDB format text file containing the predicted structure, after performing an Amber relaxation procedure on the unrelaxed structure prediction (see Jumper et al. 2021, Suppl. Methods 1.8.6 for details).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ranked_*.pdb&lt;/code&gt; â€“ A PDB format text file containing the relaxed predicted structures, after reordering by model confidence. Here &lt;code&gt;ranked_0.pdb&lt;/code&gt; should contain the prediction with the highest confidence, and &lt;code&gt;ranked_4.pdb&lt;/code&gt; the prediction with the lowest confidence. To rank model confidence, we use predicted LDDT (pLDDT) scores (see Jumper et al. 2021, Suppl. Methods 1.9.6 for details).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ranking_debug.json&lt;/code&gt; â€“ A JSON format text file containing the pLDDT values used to perform the model ranking, and a mapping back to the original model names.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;timings.json&lt;/code&gt; â€“ A JSON format text file containing the times taken to run each section of the AlphaFold pipeline.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;msas/&lt;/code&gt; - A directory containing the files describing the various genetic tool hits that were used to construct the input MSA.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;result_model_*.pkl&lt;/code&gt; â€“ A &lt;code&gt;pickle&lt;/code&gt; file containing a nested dictionary of the various NumPy arrays directly produced by the model. In addition to the output of the structure module, this includes auxiliary outputs such as:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Distograms (&lt;code&gt;distogram/logits&lt;/code&gt; contains a NumPy array of shape [N_res, N_res, N_bins] and &lt;code&gt;distogram/bin_edges&lt;/code&gt; contains the definition of the bins).&lt;/li&gt; &#xA;   &lt;li&gt;Per-residue pLDDT scores (&lt;code&gt;plddt&lt;/code&gt; contains a NumPy array of shape [N_res] with the range of possible values from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;100&lt;/code&gt;, where &lt;code&gt;100&lt;/code&gt; means most confident). This can serve to identify sequence regions predicted with high confidence or as an overall per-target confidence score when averaged across residues.&lt;/li&gt; &#xA;   &lt;li&gt;Present only if using pTM models: predicted TM-score (&lt;code&gt;ptm&lt;/code&gt; field contains a scalar). As a predictor of a global superposition metric, this score is designed to also assess whether the model is confident in the overall domain packing.&lt;/li&gt; &#xA;   &lt;li&gt;Present only if using pTM models: predicted pairwise aligned errors (&lt;code&gt;predicted_aligned_error&lt;/code&gt; contains a NumPy array of shape [N_res, N_res] with the range of possible values from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;max_predicted_aligned_error&lt;/code&gt;, where &lt;code&gt;0&lt;/code&gt; means most confident). This can serve for a visualisation of domain packing confidence within the structure.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The pLDDT confidence measure is stored in the B-factor field of the output PDB files (although unlike a B-factor, higher pLDDT is better, so care must be taken when using for tasks such as molecular replacement).&lt;/p&gt; &#xA;&lt;p&gt;This code has been tested to match mean top-1 accuracy on a CASP14 test set with pLDDT ranking over 5 model predictions (some CASP targets were run with earlier versions of AlphaFold and some had manual interventions; see our forthcoming publication for details). Some targets such as T1064 may also have high individual run variance over random seeds.&lt;/p&gt; &#xA;&lt;h2&gt;Inferencing many proteins&lt;/h2&gt; &#xA;&lt;p&gt;The provided inference script is optimized for predicting the structure of a single protein, and it will compile the neural network to be specialized to exactly the size of the sequence, MSA, and templates. For large proteins, the compile time is a negligible fraction of the runtime, but it may become more significant for small proteins or if the multi-sequence alignments are already precomputed. In the bulk inference case, it may make sense to use our &lt;code&gt;make_fixed_size&lt;/code&gt; function to pad the inputs to a uniform size, thereby reducing the number of compilations required.&lt;/p&gt; &#xA;&lt;p&gt;We do not provide a bulk inference script, but it should be straightforward to develop on top of the &lt;code&gt;RunModel.predict&lt;/code&gt; method with a parallel system for precomputing multi-sequence alignments. Alternatively, this script can be run repeatedly with only moderate overhead.&lt;/p&gt; &#xA;&lt;h2&gt;Note on CASP14 reproducibility&lt;/h2&gt; &#xA;&lt;p&gt;AlphaFold&#39;s output for a small number of proteins has high inter-run variance, and may be affected by changes in the input data. The CASP14 target T1064 is a notable example; the large number of SARS-CoV-2-related sequences recently deposited changes its MSA significantly. This variability is somewhat mitigated by the model selection process; running 5 models and taking the most confident.&lt;/p&gt; &#xA;&lt;p&gt;To reproduce the results of our CASP14 system as closely as possible you must use the same database versions we used in CASP. These may not match the default versions downloaded by our scripts.&lt;/p&gt; &#xA;&lt;p&gt;For genetics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;UniRef90: &lt;a href=&#34;https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2020_01/uniref/&#34;&gt;v2020_01&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MGnify: &lt;a href=&#34;http://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2018_12/&#34;&gt;v2018_12&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Uniclust30: &lt;a href=&#34;http://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/&#34;&gt;v2018_08&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;BFD: &lt;a href=&#34;https://bfd.mmseqs.com/&#34;&gt;only version available&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For templates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PDB: (downloaded 2020-05-14)&lt;/li&gt; &#xA; &lt;li&gt;PDB70: &lt;a href=&#34;http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/old-releases/pdb70_from_mmcif_200513.tar.gz&#34;&gt;2020-05-13&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An alternative for templates is to use the latest PDB and PDB70, but pass the flag &lt;code&gt;--max_template_date=2020-05-14&lt;/code&gt;, which restricts templates only to structures that were available at the start of CASP14.&lt;/p&gt; &#xA;&lt;h2&gt;Citing this work&lt;/h2&gt; &#xA;&lt;p&gt;If you use the code or data in this package, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{AlphaFold2021,&#xA;  author  = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\&#39;\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},&#xA;  journal = {Nature},&#xA;  title   = {Highly accurate protein structure prediction with {AlphaFold}},&#xA;  year    = {2021},&#xA;  volume  = {596},&#xA;  number  = {7873},&#xA;  pages   = {583--589},&#xA;  doi     = {10.1038/s41586-021-03819-2}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition, if you use the AlphaFold-Multimer mode, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article {AlphaFold-Multimer2021,&#xA;  author       = {Evans, Richard and O{\textquoteright}Neill, Michael and Pritzel, Alexander and Antropova, Natasha and Senior, Andrew and Green, Tim and {\v{Z}}{\&#39;\i}dek, Augustin and Bates, Russ and Blackwell, Sam and Yim, Jason and Ronneberger, Olaf and Bodenstein, Sebastian and Zielinski, Michal and Bridgland, Alex and Potapenko, Anna and Cowie, Andrew and Tunyasuvunakool, Kathryn and Jain, Rishub and Clancy, Ellen and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},&#xA;  journal      = {bioRxiv}&#xA;  title        = {Protein complex prediction with AlphaFold-Multimer},&#xA;  year         = {2021},&#xA;  elocation-id = {2021.10.04.463034},&#xA;  doi          = {10.1101/2021.10.04.463034},&#xA;  URL          = {https://www.biorxiv.org/content/early/2021/10/04/2021.10.04.463034},&#xA;  eprint       = {https://www.biorxiv.org/content/early/2021/10/04/2021.10.04.463034.full.pdf},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Community contributions&lt;/h2&gt; &#xA;&lt;p&gt;Colab notebooks provided by the community (please note that these notebooks may vary from our full AlphaFold system and we did not validate their accuracy):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb&#34;&gt;ColabFold AlphaFold2 notebook&lt;/a&gt; by Martin Steinegger, Sergey Ovchinnikov and Milot Mirdita, which uses an API hosted at the SÃ¶dinglab based on the MMseqs2 server &lt;a href=&#34;https://academic.oup.com/bioinformatics/article/35/16/2856/5280135&#34;&gt;(Mirdita et al. 2019, Bioinformatics)&lt;/a&gt; for the multiple sequence alignment creation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;AlphaFold communicates with and/or references the following separate libraries and packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abseil/abseil-py&#34;&gt;Abseil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://biopython.org&#34;&gt;Biopython&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/chex&#34;&gt;Chex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.google.com/colaboratory/&#34;&gt;Colab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.docker.com&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/soedinglab/hh-suite&#34;&gt;HH Suite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://eddylab.org/software/hmmer&#34;&gt;HMMER Suite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/dm-haiku&#34;&gt;Haiku&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/corenting/immutabledict&#34;&gt;Immutabledict&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/jax/&#34;&gt;JAX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://msa.sbc.su.se/cgi-bin/msa.cgi&#34;&gt;Kalign&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;matplotlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/ml_collections&#34;&gt;ML Collections&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://numpy.org&#34;&gt;NumPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openmm/openmm&#34;&gt;OpenMM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openstructure.org&#34;&gt;OpenStructure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/avirshup/py3dmol&#34;&gt;pymol3d&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scipy.org&#34;&gt;SciPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/sonnet&#34;&gt;Sonnet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/tree&#34;&gt;Tree&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tqdm/tqdm&#34;&gt;tqdm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We thank all their contributors and maintainers!&lt;/p&gt; &#xA;&lt;h2&gt;Get in Touch&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions not covered in this overview, please contact the AlphaFold team at &lt;a href=&#34;mailto:alphafold@deepmind.com&#34;&gt;alphafold@deepmind.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We would love to hear your feedback and understand how AlphaFold has been useful in your research. Share your stories with us at &lt;a href=&#34;mailto:alphafold@deepmind.com&#34;&gt;alphafold@deepmind.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License and Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2021 DeepMind Technologies Limited.&lt;/p&gt; &#xA;&lt;h3&gt;AlphaFold Code License&lt;/h3&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;h3&gt;Model Parameters License&lt;/h3&gt; &#xA;&lt;p&gt;The AlphaFold parameters are made available under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You can find details at: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Third-party software&lt;/h3&gt; &#xA;&lt;p&gt;Use of the third-party software, libraries or code referred to in the &lt;a href=&#34;https://raw.githubusercontent.com/deepmind/alphafold/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt; section above may be governed by separate terms and conditions or license provisions. Your use of the third-party software, libraries or code is subject to any such terms and you should check that you can comply with any applicable restrictions or terms and conditions before use.&lt;/p&gt; &#xA;&lt;h3&gt;Mirrored Databases&lt;/h3&gt; &#xA;&lt;p&gt;The following databases have been mirrored by DeepMind, and are available with reference to the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://bfd.mmseqs.com/&#34;&gt;BFD&lt;/a&gt; (unmodified), by Steinegger M. and SÃ¶ding J., available under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://bfd.mmseqs.com/&#34;&gt;BFD&lt;/a&gt; (modified), by Steinegger M. and SÃ¶ding J., modified by DeepMind, available under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;. See the Methods section of the &lt;a href=&#34;https://www.nature.com/articles/s41586-021-03828-1&#34;&gt;AlphaFold proteome paper&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/&#34;&gt;Uniclust30: v2018_08&lt;/a&gt; (unmodified), by Mirdita M. et al., available under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/current_release/README.txt&#34;&gt;MGnify: v2018_12&lt;/a&gt; (unmodified), by Mitchell AL et al., available free of all copyright restrictions and made fully and freely available for both non-commercial and commercial use under &lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/&#34;&gt;CC0 1.0 Universal (CC0 1.0) Public Domain Dedication&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>