<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-16T01:46:21Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PaddlePaddle/PaddleNLP</title>
    <updated>2022-10-16T01:46:21Z</updated>
    <id>tag:github.com,2022-10-16:/PaddlePaddle/PaddleNLP</id>
    <link href="https://github.com/PaddlePaddle/PaddleNLP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easy-to-use and powerful NLP library with Awesome model zoo, supporting wide-range of NLP tasks from research to industrial applications, including Neural Search, Question Answering, Information Extraction and Sentiment Analysis end-to-end system.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;ç®€ä½“ä¸­æ–‡&lt;/strong&gt;ğŸ€„ | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/README_en.md&#34;&gt;EnglishğŸŒ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/1371212/175816733-8ec25eb0-9af3-4380-9218-27c154518258.png&#34; align=&#34;middle&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-dfd.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/PaddleNLP?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.6.2+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/PaddlePaddle/PaddleNLP?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/commits&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleNLP?color=3af&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/paddlenlp/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/paddlenlp?color=9cf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PaddlePaddle/PaddleNLP?color=9cc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleNLP?color=ccf&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;#ç‰¹æ€§&#34;&gt; ç‰¹æ€§ &lt;/a&gt; | &lt;a href=&#34;#å®‰è£…&#34;&gt; å®‰è£… &lt;/a&gt; | &lt;a href=&#34;#å¿«é€Ÿå¼€å§‹&#34;&gt; å¿«é€Ÿå¼€å§‹ &lt;/a&gt; | &lt;a href=&#34;#apiæ–‡æ¡£&#34;&gt; APIæ–‡æ¡£ &lt;/a&gt; | &lt;a href=&#34;#ç¤¾åŒºäº¤æµ&#34;&gt; ç¤¾åŒºäº¤æµ &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaddleNLP&lt;/strong&gt;æ˜¯ä¸€æ¬¾&lt;strong&gt;ç®€å•æ˜“ç”¨&lt;/strong&gt;ä¸”&lt;strong&gt;åŠŸèƒ½å¼ºå¤§&lt;/strong&gt;çš„è‡ªç„¶è¯­è¨€å¤„ç†å¼€å‘åº“ã€‚èšåˆä¸šç•Œ&lt;strong&gt;ä¼˜è´¨é¢„è®­ç»ƒæ¨¡å‹&lt;/strong&gt;å¹¶æä¾›&lt;strong&gt;å¼€ç®±å³ç”¨&lt;/strong&gt;çš„å¼€å‘ä½“éªŒï¼Œè¦†ç›–NLPå¤šåœºæ™¯çš„æ¨¡å‹åº“æ­é…&lt;strong&gt;äº§ä¸šå®è·µèŒƒä¾‹&lt;/strong&gt;å¯æ»¡è¶³å¼€å‘è€…&lt;strong&gt;çµæ´»å®šåˆ¶&lt;/strong&gt;çš„éœ€æ±‚ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;News ğŸ“¢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ”¥ &lt;strong&gt;2022.9.6 å‘å¸ƒ &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/releases/tag/v2.4.0&#34;&gt;PaddleNLP v2.4&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ğŸ’ NLPå·¥å…·ï¼š&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/pipelines&#34;&gt;NLP æµæ°´çº¿ç³»ç»Ÿ Pipelines&lt;/a&gt;&lt;/strong&gt; å‘å¸ƒï¼Œæ”¯æŒå¿«é€Ÿæ­å»ºæœç´¢å¼•æ“ã€é—®ç­”ç³»ç»Ÿï¼Œå¯æ‰©å±•æ”¯æŒå„ç±»NLPç³»ç»Ÿï¼Œè®©è§£å†³ NLP ä»»åŠ¡åƒæ­ç§¯æœ¨ä¸€æ ·ä¾¿æ·ã€çµæ´»ã€é«˜æ•ˆï¼&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ’¢ äº§ä¸šåº”ç”¨ï¼šæ–°å¢ &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/text_classification&#34;&gt;æ–‡æœ¬åˆ†ç±»å…¨æµç¨‹åº”ç”¨æ–¹æ¡ˆ&lt;/a&gt;&lt;/strong&gt; ï¼Œè¦†ç›–å¤šåˆ†ç±»ã€å¤šæ ‡ç­¾ã€å±‚æ¬¡åˆ†ç±»å„ç±»åœºæ™¯ï¼Œæ”¯æŒ &lt;strong&gt;å°æ ·æœ¬å­¦ä¹ &lt;/strong&gt; å’Œ &lt;strong&gt;TrustAI&lt;/strong&gt; å¯ä¿¡è®¡ç®—æ¨¡å‹è®­ç»ƒä¸è°ƒä¼˜ï¼›&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie&#34;&gt;&lt;strong&gt;é€šç”¨ä¿¡æ¯æŠ½å– UIE èƒ½åŠ›å‡çº§&lt;/strong&gt;&lt;/a&gt;ï¼Œå‘å¸ƒ &lt;strong&gt;UIE-M&lt;/strong&gt;ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆæŠ½å–ï¼Œæ–°å¢&lt;strong&gt;UIE æ•°æ®è’¸é¦&lt;/strong&gt;æ–¹æ¡ˆï¼Œæ‰“ç ´ UIE æ¨ç†ç“¶é¢ˆï¼Œæ¨ç†é€Ÿåº¦æå‡ 100 å€ä»¥ä¸Šï¼›&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ­ AIGC å†…å®¹ç”Ÿæˆï¼šæ–°å¢ä»£ç ç”Ÿæˆ SOTA æ¨¡å‹&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples/code_generation/codegen&#34;&gt;&lt;strong&gt;CodeGen&lt;/strong&gt;&lt;/a&gt;ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ä»£ç ç”Ÿæˆï¼›é›†æˆ&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/raw/develop/docs/model_zoo/taskflow.md#%E6%96%87%E5%9B%BE%E7%94%9F%E6%88%90&#34;&gt;&lt;strong&gt;æ–‡å›¾ç”Ÿæˆæ½®æµæ¨¡å‹&lt;/strong&gt;&lt;/a&gt; DALLÂ·E Miniã€Disco Diffusionã€Stable Diffusionï¼Œæ›´å¤šè¶£ç©æ¨¡å‹ç­‰ä½ æ¥ç©ï¼›æ–°å¢&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/text_summarization&#34;&gt;&lt;strong&gt;ä¸­æ–‡æ–‡æœ¬æ‘˜è¦åº”ç”¨&lt;/strong&gt;&lt;/a&gt;ï¼ŒåŸºäºå¤§è§„æ¨¡è¯­æ–™çš„ä¸­æ–‡æ‘˜è¦æ¨¡å‹é¦–æ¬¡å‘å¸ƒï¼Œå¯æ”¯æŒ Taskflow ä¸€é”®è°ƒç”¨å’Œå®šåˆ¶è®­ç»ƒï¼›&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ’ª æ¡†æ¶å‡çº§ï¼š&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/compression.md&#34;&gt;&lt;strong&gt;æ¨¡å‹è‡ªåŠ¨å‹ç¼© API&lt;/strong&gt;&lt;/a&gt; å‘å¸ƒï¼Œè‡ªåŠ¨å¯¹æ¨¡å‹è¿›è¡Œè£å‡å’Œé‡åŒ–ï¼Œå¤§å¹…é™ä½æ¨¡å‹å‹ç¼©æŠ€æœ¯ä½¿ç”¨é—¨æ§›ï¼›&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/text_classification/multi_class/few-shot&#34;&gt;&lt;strong&gt;å°æ ·æœ¬ Prompt&lt;/strong&gt;&lt;/a&gt;èƒ½åŠ›å‘å¸ƒï¼Œé›†æˆ PETã€P-Tuningã€RGL ç­‰ç»å…¸ç®—æ³•ã€‚&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ”¥ &lt;strong&gt;2022.5.16 å‘å¸ƒ &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/releases/tag/v2.3.0&#34;&gt;PaddleNLP v2.3&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ğŸ’ å‘å¸ƒé€šç”¨ä¿¡æ¯æŠ½å–æŠ€æœ¯ &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie&#34;&gt;&lt;strong&gt;UIE&lt;/strong&gt;&lt;/a&gt;ï¼Œå•æ¨¡å‹æ”¯æŒå®ä½“è¯†åˆ«ã€å…³ç³»å’Œäº‹ä»¶æŠ½å–ã€æƒ…æ„Ÿåˆ†æç­‰å¤šç§å¼€æ”¾åŸŸä¿¡æ¯æŠ½å–ä»»åŠ¡ï¼Œä¸é™é¢†åŸŸå’ŒæŠ½å–ç›®æ ‡ï¼Œæ”¯æŒ&lt;strong&gt;é›¶æ ·æœ¬æŠ½å–&lt;/strong&gt;ä¸å…¨æµç¨‹&lt;strong&gt;å°æ ·æœ¬&lt;/strong&gt;é«˜æ•ˆå®šåˆ¶å¼€å‘ï¼›&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ˜Š å‘å¸ƒæ–‡å¿ƒå¤§æ¨¡å‹ &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/ernie-3.0&#34;&gt;&lt;strong&gt;ERNIE 3.0&lt;/strong&gt;&lt;/a&gt; è½»é‡çº§æ¨¡å‹ï¼Œåœ¨ &lt;a href=&#34;https://www.cluebenchmarks.com/&#34;&gt;CLUE &lt;/a&gt;ä¸Šå®ç°åŒè§„æ¨¡ç»“æ„æ•ˆæœæœ€ä½³ï¼Œå¹¶æä¾›&lt;strong&gt;ğŸ—œï¸æ— æŸå‹ç¼©&lt;/strong&gt;å’Œ&lt;strong&gt;âš™ï¸å…¨åœºæ™¯éƒ¨ç½²&lt;/strong&gt;æ–¹æ¡ˆï¼›&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ¥ å‘å¸ƒä¸­æ–‡åŒ»ç–—é¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹ &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/ernie-health&#34;&gt;&lt;strong&gt;ERNIE-Health&lt;/strong&gt;&lt;/a&gt;ï¼Œ&lt;a href=&#34;https://github.com/CBLUEbenchmark/CBLUE&#34;&gt;CBLUE&lt;/a&gt; ä¸­æ–‡åŒ»ç–—ä¿¡æ¯å¤„ç†è¯„æµ‹å† å†›æ¨¡å‹ï¼›&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ’¬ å‘å¸ƒå¤§è§„æ¨¡ç™¾äº¿å¼€æ”¾åŸŸå¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/plato-xl&#34;&gt;&lt;strong&gt;PLATO-XL&lt;/strong&gt;&lt;/a&gt; ï¼Œé…åˆâš¡&lt;strong&gt;FasterGeneration&lt;/strong&gt;âš¡å¿«é€Ÿå®ç°é«˜æ€§èƒ½GPUå¹¶è¡Œæ¨ç†åŠ é€Ÿã€‚&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç¤¾åŒºäº¤æµ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;å¾®ä¿¡æ‰«æäºŒç»´ç å¹¶å¡«å†™é—®å·ï¼Œå›å¤å°åŠ©æ‰‹å…³é”®è¯ï¼ˆNLPï¼‰ä¹‹åï¼Œå³å¯åŠ å…¥äº¤æµç¾¤é¢†å–ç¦åˆ©&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ä¸ä¼—å¤šç¤¾åŒºå¼€å‘è€…ä»¥åŠå®˜æ–¹å›¢é˜Ÿæ·±åº¦äº¤æµã€‚&lt;/li&gt; &#xA;   &lt;li&gt;10Gé‡ç£…NLPå­¦ä¹ å¤§ç¤¼åŒ…ï¼&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/184784832-bb97930f-a738-4480-99be-517aeb65afac.png&#34; width=&#34;150&#34; height=&#34;150&#34;&gt; &#xA;  &lt;/div&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç‰¹æ€§&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#å¼€ç®±å³ç”¨çš„nlpå·¥å…·é›†&#34;&gt; ğŸ“¦ å¼€ç®±å³ç”¨çš„NLPå·¥å…·é›† &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#ä¸°å¯Œå®Œå¤‡çš„ä¸­æ–‡æ¨¡å‹åº“&#34;&gt; ğŸ¤— ä¸°å¯Œå®Œå¤‡çš„ä¸­æ–‡æ¨¡å‹åº“ &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#äº§ä¸šçº§ç«¯åˆ°ç«¯ç³»ç»ŸèŒƒä¾‹&#34;&gt; ğŸ›ï¸ äº§ä¸šçº§ç«¯åˆ°ç«¯ç³»ç»ŸèŒƒä¾‹ &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#é«˜æ€§èƒ½åˆ†å¸ƒå¼è®­ç»ƒä¸æ¨ç†&#34;&gt; ğŸš€ é«˜æ€§èƒ½åˆ†å¸ƒå¼è®­ç»ƒä¸æ¨ç† &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h3&gt;å¼€ç®±å³ç”¨çš„NLPå·¥å…·é›†&lt;/h3&gt; &#xA;&lt;p&gt;Taskflowæä¾›ä¸°å¯Œçš„&lt;strong&gt;ğŸ“¦å¼€ç®±å³ç”¨&lt;/strong&gt;çš„äº§ä¸šçº§NLPé¢„ç½®æ¨¡å‹ï¼Œè¦†ç›–è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆä¸¤å¤§åœºæ™¯ï¼Œæä¾›&lt;strong&gt;ğŸ’ªäº§ä¸šçº§çš„æ•ˆæœ&lt;/strong&gt;ä¸&lt;strong&gt;âš¡ï¸æè‡´çš„æ¨ç†æ€§èƒ½&lt;/strong&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11793384/159693816-fda35221-9751-43bb-b05c-7fc77571dd76.gif&#34; alt=&#34;taskflow1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Taskflowæœ€æ–°é›†æˆäº†æ–‡ç”Ÿå›¾çš„è¶£ç©åº”ç”¨ï¼Œä¸‰è¡Œä»£ç ä½“éªŒ &lt;strong&gt;Stable Diffusion&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paddlenlp import Taskflow&#xA;text_to_image = Taskflow(&#34;text_to_image&#34;, model=&#34;CompVis/stable-diffusion-v1-4&#34;)&#xA;image_list = text_to_image(&#39;&#34;In the morning light,Chinese ancient buildings in the mountains,Magnificent and fantastic John Howe landscape,lake,clouds,farm,Fairy tale,light effect,Dream,Greg Rutkowski,James Gurney,artstation&#34;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;300&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/16698950/194882669-f7cc7c98-d63a-45f4-99c1-0514c6712368.png&#34;&gt; &#xA;&lt;p&gt;æ›´å¤šä½¿ç”¨æ–¹æ³•å¯å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/taskflow.md&#34;&gt;Taskflowæ–‡æ¡£&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;ä¸°å¯Œå®Œå¤‡çš„ä¸­æ–‡æ¨¡å‹åº“&lt;/h3&gt; &#xA;&lt;h4&gt;ğŸ€„ ä¸šç•Œæœ€å…¨çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹&lt;/h4&gt; &#xA;&lt;p&gt;ç²¾é€‰ 45+ ä¸ªç½‘ç»œç»“æ„å’Œ 500+ ä¸ªé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œæ¶µç›–ä¸šç•Œæœ€å…¨çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ï¼šæ—¢åŒ…æ‹¬æ–‡å¿ƒNLPå¤§æ¨¡å‹çš„ERNIEã€PLATOç­‰ï¼Œä¹Ÿè¦†ç›–BERTã€GPTã€RoBERTaã€T5ç­‰ä¸»æµç»“æ„ã€‚é€šè¿‡&lt;code&gt;AutoModel&lt;/code&gt; APIä¸€é”®âš¡&lt;strong&gt;é«˜é€Ÿä¸‹è½½&lt;/strong&gt;âš¡ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paddlenlp.transformers import *&#xA;&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;bert = AutoModel.from_pretrained(&#39;bert-wwm-chinese&#39;)&#xA;albert = AutoModel.from_pretrained(&#39;albert-chinese-tiny&#39;)&#xA;roberta = AutoModel.from_pretrained(&#39;roberta-wwm-ext&#39;)&#xA;electra = AutoModel.from_pretrained(&#39;chinese-electra-small&#39;)&#xA;gpt = AutoModelForPretraining.from_pretrained(&#39;gpt-cpm-large-cn&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;é’ˆå¯¹é¢„è®­ç»ƒæ¨¡å‹è®¡ç®—ç“¶é¢ˆï¼Œå¯ä»¥ä½¿ç”¨APIä¸€é”®ä½¿ç”¨æ–‡å¿ƒERNIE-Tinyå…¨ç³»åˆ—è½»é‡åŒ–æ¨¡å‹ï¼Œé™ä½é¢„è®­ç»ƒæ¨¡å‹éƒ¨ç½²éš¾åº¦ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 6L768H&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;# 6L384H&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-mini-zh&#39;)&#xA;# 4L384H&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-micro-zh&#39;)&#xA;# 4L312H&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-nano-zh&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¯¹é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨èŒƒå¼å¦‚è¯­ä¹‰è¡¨ç¤ºã€æ–‡æœ¬åˆ†ç±»ã€å¥å¯¹åŒ¹é…ã€åºåˆ—æ ‡æ³¨ã€é—®ç­”ç­‰ï¼Œæä¾›ç»Ÿä¸€çš„APIä½“éªŒã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import paddle&#xA;from paddlenlp.transformers import *&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;text = tokenizer(&#39;è‡ªç„¶è¯­è¨€å¤„ç†&#39;)&#xA;&#xA;# è¯­ä¹‰è¡¨ç¤º&#xA;model = AutoModel.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;sequence_output, pooled_output = model(input_ids=paddle.to_tensor([text[&#39;input_ids&#39;]]))&#xA;# æ–‡æœ¬åˆ†ç±» &amp;amp; å¥å¯¹åŒ¹é…&#xA;model = AutoModelForSequenceClassification.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;# åºåˆ—æ ‡æ³¨&#xA;model = AutoModelForTokenClassification.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;# é—®ç­”&#xA;model = AutoModelForQuestionAnswering.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;ğŸ’¯ å…¨åœºæ™¯è¦†ç›–çš„åº”ç”¨ç¤ºä¾‹&lt;/h4&gt; &#xA;&lt;p&gt;è¦†ç›–ä»å­¦æœ¯åˆ°äº§ä¸šçš„NLPåº”ç”¨ç¤ºä¾‹ï¼Œæ¶µç›–NLPåŸºç¡€æŠ€æœ¯ã€NLPç³»ç»Ÿåº”ç”¨ä»¥åŠæ‹“å±•åº”ç”¨ã€‚å…¨é¢åŸºäºé£æ¡¨æ ¸å¿ƒæ¡†æ¶2.0å…¨æ–°APIä½“ç³»å¼€å‘ï¼Œä¸ºå¼€å‘è€…æä¾›é£æ¡¨æ–‡æœ¬é¢†åŸŸçš„æœ€ä½³å®è·µã€‚&lt;/p&gt; &#xA;&lt;p&gt;ç²¾é€‰é¢„è®­ç»ƒæ¨¡å‹ç¤ºä¾‹å¯å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo&#34;&gt;Model Zoo&lt;/a&gt;ï¼Œæ›´å¤šåœºæ™¯ç¤ºä¾‹æ–‡æ¡£å¯å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples&#34;&gt;examplesç›®å½•&lt;/a&gt;ã€‚æ›´æœ‰å…è´¹ç®—åŠ›æ”¯æŒçš„&lt;a href=&#34;https://aistudio.baidu.com&#34;&gt;AI Studio&lt;/a&gt;å¹³å°çš„&lt;a href=&#34;https://aistudio.baidu.com/aistudio/personalcenter/thirdview/574995&#34;&gt;Notbookäº¤äº’å¼æ•™ç¨‹&lt;/a&gt;æä¾›å®è·µã€‚&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; PaddleNLPé¢„è®­ç»ƒæ¨¡å‹é€‚ç”¨ä»»åŠ¡æ±‡æ€»ï¼ˆ&lt;b&gt;ç‚¹å‡»å±•å¼€è¯¦æƒ…&lt;/b&gt;ï¼‰&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;Sequence Classification&lt;/th&gt; &#xA;     &lt;th&gt;Token Classification&lt;/th&gt; &#xA;     &lt;th&gt;Question Answering&lt;/th&gt; &#xA;     &lt;th&gt;Text Generation&lt;/th&gt; &#xA;     &lt;th&gt;Multiple Choice&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ALBERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BART&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BigBird&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BlenderBot&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ChineseBERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ConvBERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;CTRL&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;DistilBERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ELECTRA&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-CTM&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-Doc&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-GEN&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-Gram&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-M&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;FNet&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Funnel-Transformer&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;GPT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LayoutLM&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LayoutLMv2&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LayoutXLM&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LUKE&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;mBART&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MegatronBERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MobileBERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MPNet&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;NEZHA&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;PP-MiniLM&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ProphetNet&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Reformer&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;RemBERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;RoBERTa&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;RoFormer&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;SKEP&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;SqueezeBERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;T5&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;TinyBERT&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;UnifiedTransformer&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;XLNet&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;     &lt;td&gt;âŒ&lt;/td&gt; &#xA;     &lt;td&gt;âœ…&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;p&gt;å¯å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/index.rst&#34;&gt;Transformer æ–‡æ¡£&lt;/a&gt; æŸ¥çœ‹ç›®å‰æ”¯æŒçš„é¢„è®­ç»ƒæ¨¡å‹ç»“æ„ã€å‚æ•°å’Œè¯¦ç»†ç”¨æ³•ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;äº§ä¸šçº§ç«¯åˆ°ç«¯ç³»ç»ŸèŒƒä¾‹&lt;/h3&gt; &#xA;&lt;p&gt;PaddleNLPé’ˆå¯¹ä¿¡æ¯æŠ½å–ã€è¯­ä¹‰æ£€ç´¢ã€æ™ºèƒ½é—®ç­”ã€æƒ…æ„Ÿåˆ†æç­‰é«˜é¢‘NLPåœºæ™¯ï¼Œæä¾›äº†ç«¯åˆ°ç«¯ç³»ç»ŸèŒƒä¾‹ï¼Œæ‰“é€š&lt;em&gt;æ•°æ®æ ‡æ³¨&lt;/em&gt;-&lt;em&gt;æ¨¡å‹è®­ç»ƒ&lt;/em&gt;-&lt;em&gt;æ¨¡å‹è°ƒä¼˜&lt;/em&gt;-&lt;em&gt;é¢„æµ‹éƒ¨ç½²&lt;/em&gt;å…¨æµç¨‹ï¼ŒæŒç»­é™ä½NLPæŠ€æœ¯äº§ä¸šè½åœ°é—¨æ§›ã€‚æ›´å¤šè¯¦ç»†çš„ç³»ç»Ÿçº§äº§ä¸šèŒƒä¾‹ä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications&#34;&gt;Applications&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h4&gt;ğŸ” è¯­ä¹‰æ£€ç´¢ç³»ç»Ÿ&lt;/h4&gt; &#xA;&lt;p&gt;é’ˆå¯¹æ— ç›‘ç£æ•°æ®ã€æœ‰ç›‘ç£æ•°æ®ç­‰å¤šç§æ•°æ®æƒ…å†µï¼Œç»“åˆSimCSEã€In-batch Negativesã€ERNIE-Gramå•å¡”æ¨¡å‹ç­‰ï¼Œæ¨å‡ºå‰æ²¿çš„è¯­ä¹‰æ£€ç´¢æ–¹æ¡ˆï¼ŒåŒ…å«å¬å›ã€æ’åºç¯èŠ‚ï¼Œæ‰“é€šè®­ç»ƒã€è°ƒä¼˜ã€é«˜æ•ˆå‘é‡æ£€ç´¢å¼•æ“å»ºåº“å’ŒæŸ¥è¯¢å…¨æµç¨‹ã€‚&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168514909-8817d79a-72c4-4be1-8080-93d1f682bb46.gif&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;æ›´å¤šä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/neural_search&#34;&gt;è¯­ä¹‰æ£€ç´¢ç³»ç»Ÿ&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h4&gt;â“ æ™ºèƒ½é—®ç­”ç³»ç»Ÿ&lt;/h4&gt; &#xA;&lt;p&gt;åŸºäº&lt;a href=&#34;https://github.com/PaddlePaddle/RocketQA&#34;&gt;ğŸš€RocketQA&lt;/a&gt;æŠ€æœ¯çš„æ£€ç´¢å¼é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒFAQé—®ç­”ã€è¯´æ˜ä¹¦é—®ç­”ç­‰å¤šç§ä¸šåŠ¡åœºæ™¯ã€‚&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168514868-1babe981-c675-4f89-9168-dd0a3eede315.gif&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;æ›´å¤šä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/question_answering&#34;&gt;æ™ºèƒ½é—®ç­”ç³»ç»Ÿ&lt;/a&gt;ä¸&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/doc_vqa&#34;&gt;æ–‡æ¡£æ™ºèƒ½é—®ç­”&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;ğŸ’Œ è¯„è®ºè§‚ç‚¹æŠ½å–ä¸æƒ…æ„Ÿåˆ†æ&lt;/h4&gt; &#xA;&lt;p&gt;åŸºäºæƒ…æ„ŸçŸ¥è¯†å¢å¼ºé¢„è®­ç»ƒæ¨¡å‹SKEPï¼Œé’ˆå¯¹äº§å“è¯„è®ºè¿›è¡Œè¯„ä»·ç»´åº¦å’Œè§‚ç‚¹æŠ½å–ï¼Œä»¥åŠç»†ç²’åº¦çš„æƒ…æ„Ÿåˆ†æã€‚&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168407260-b7f92800-861c-4207-98f3-2291e0102bbe.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;æ›´å¤šä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/sentiment_analysis&#34;&gt;æƒ…æ„Ÿåˆ†æ&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h4&gt;ğŸ™ï¸ æ™ºèƒ½è¯­éŸ³æŒ‡ä»¤è§£æ&lt;/h4&gt; &#xA;&lt;p&gt;é›†æˆäº†&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech&#34;&gt;PaddleSpeech&lt;/a&gt;å’Œ&lt;a href=&#34;https://ai.baidu.com/&#34;&gt;ç™¾åº¦å¼€æ”¾å¹³å°&lt;/a&gt;çš„çš„è¯­éŸ³è¯†åˆ«å’Œ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie&#34;&gt;UIE&lt;/a&gt;é€šç”¨ä¿¡æ¯æŠ½å–ç­‰æŠ€æœ¯ï¼Œæ‰“é€ æ™ºèƒ½ä¸€ä½“åŒ–çš„è¯­éŸ³æŒ‡ä»¤è§£æç³»ç»ŸèŒƒä¾‹ï¼Œè¯¥æ–¹æ¡ˆå¯åº”ç”¨äºæ™ºèƒ½è¯­éŸ³å¡«å•ã€æ™ºèƒ½è¯­éŸ³äº¤äº’ã€æ™ºèƒ½è¯­éŸ³æ£€ç´¢ç­‰åœºæ™¯ï¼Œæé«˜äººæœºäº¤äº’æ•ˆç‡ã€‚&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/16698950/168589100-a6c6f346-97bb-47b2-ac26-8d50e71fddc5.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;æ›´å¤šä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/speech_cmd_analysis&#34;&gt;æ™ºèƒ½è¯­éŸ³æŒ‡ä»¤è§£æ&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;é«˜æ€§èƒ½åˆ†å¸ƒå¼è®­ç»ƒä¸æ¨ç†&lt;/h3&gt; &#xA;&lt;h4&gt;âš¡ FasterTokenizerï¼šé«˜æ€§èƒ½æ–‡æœ¬å¤„ç†åº“&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168407921-b4395b1d-44bd-41a0-8c58-923ba2b703ef.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;AutoTokenizer.from_pretrained(&#34;ernie-3.0-medium-zh&#34;, use_faster=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ä¸ºäº†å®ç°æ›´æè‡´çš„æ¨¡å‹éƒ¨ç½²æ€§èƒ½ï¼Œå®‰è£…FastTokenizersååªéœ€åœ¨&lt;code&gt;AutoTokenizer&lt;/code&gt; APIä¸Šæ‰“å¼€ &lt;code&gt;use_faster=True&lt;/code&gt;é€‰é¡¹ï¼Œå³å¯è°ƒç”¨C++å®ç°çš„é«˜æ€§èƒ½åˆ†è¯ç®—å­ï¼Œè½»æ¾è·å¾—è¶…Pythonç™¾ä½™å€çš„æ–‡æœ¬å¤„ç†åŠ é€Ÿï¼Œæ›´å¤šä½¿ç”¨è¯´æ˜å¯å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/faster_tokenizer&#34;&gt;FasterTokenizeræ–‡æ¡£&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h4&gt;âš¡ï¸ FasterGenerationï¼šé«˜æ€§èƒ½ç”ŸæˆåŠ é€Ÿåº“&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168407831-914dced0-3a5a-40b8-8a65-ec82bf13e53c.gif&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = GPTLMHeadModel.from_pretrained(&#39;gpt-cpm-large-cn&#39;)&#xA;...&#xA;outputs, _ = model.generate(&#xA;    input_ids=inputs_ids, max_length=10, decode_strategy=&#39;greedy_search&#39;,&#xA;    use_faster=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ç®€å•åœ°åœ¨&lt;code&gt;generate()&lt;/code&gt;APIä¸Šæ‰“å¼€&lt;code&gt;use_faster=True&lt;/code&gt;é€‰é¡¹ï¼Œè½»æ¾åœ¨Transformerã€GPTã€BARTã€PLATOã€UniLMç­‰ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ä¸Šè·å¾—5å€ä»¥ä¸ŠGPUåŠ é€Ÿï¼Œæ›´å¤šä½¿ç”¨è¯´æ˜å¯å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/faster_generation&#34;&gt;FasterGenerationæ–‡æ¡£&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h4&gt;ğŸš€ Fleetï¼šé£æ¡¨4Dæ··åˆå¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168515134-513f13e0-9902-40ef-98fa-528271dcccda.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;æ›´å¤šå…³äºåƒäº¿çº§AIæ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒä½¿ç”¨è¯´æ˜å¯å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples/language_model/gpt-3&#34;&gt;GPT-3&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å®‰è£…&lt;/h2&gt; &#xA;&lt;h3&gt;ç¯å¢ƒä¾èµ–&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python &amp;gt;= 3.6&lt;/li&gt; &#xA; &lt;li&gt;paddlepaddle &amp;gt;= 2.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;pipå®‰è£…&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install --upgrade paddlenlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;æ›´å¤šå…³äºPaddlePaddleå’ŒPaddleNLPå®‰è£…çš„è¯¦ç»†æ•™ç¨‹è¯·æŸ¥çœ‹&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/get_started/installation.rst&#34;&gt;Installation&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å¿«é€Ÿå¼€å§‹&lt;/h2&gt; &#xA;&lt;p&gt;è¿™é‡Œä»¥ä¿¡æ¯æŠ½å–-å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼ŒUIEæ¨¡å‹ä¸ºä¾‹ï¼Œæ¥è¯´æ˜å¦‚ä½•å¿«é€Ÿä½¿ç”¨PaddleNLP:&lt;/p&gt; &#xA;&lt;h3&gt;ä¸€é”®é¢„æµ‹&lt;/h3&gt; &#xA;&lt;p&gt;PaddleNLPæä¾›&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/taskflow.md&#34;&gt;ä¸€é”®é¢„æµ‹åŠŸèƒ½&lt;/a&gt;ï¼Œæ— éœ€è®­ç»ƒï¼Œç›´æ¥è¾“å…¥æ•°æ®å³å¯å¼€æ”¾åŸŸæŠ½å–ç»“æœï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from pprint import pprint&#xA;&amp;gt;&amp;gt;&amp;gt; from paddlenlp import Taskflow&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; schema = [&#39;æ—¶é—´&#39;, &#39;é€‰æ‰‹&#39;, &#39;èµ›äº‹åç§°&#39;] # Define the schema for entity extraction&#xA;&amp;gt;&amp;gt;&amp;gt; ie = Taskflow(&#39;information_extraction&#39;, schema=schema)&#xA;&amp;gt;&amp;gt;&amp;gt; pprint(ie(&#34;2æœˆ8æ—¥ä¸ŠåˆåŒ—äº¬å†¬å¥¥ä¼šè‡ªç”±å¼æ»‘é›ªå¥³å­å¤§è·³å°å†³èµ›ä¸­ä¸­å›½é€‰æ‰‹è°·çˆ±å‡Œä»¥188.25åˆ†è·å¾—é‡‘ç‰Œï¼&#34;))&#xA;[{&#39;æ—¶é—´&#39;: [{&#39;end&#39;: 6,&#xA;          &#39;probability&#39;: 0.9857378532924486,&#xA;          &#39;start&#39;: 0,&#xA;          &#39;text&#39;: &#39;2æœˆ8æ—¥ä¸Šåˆ&#39;}],&#xA;  &#39;èµ›äº‹åç§°&#39;: [{&#39;end&#39;: 23,&#xA;            &#39;probability&#39;: 0.8503089953268272,&#xA;            &#39;start&#39;: 6,&#xA;            &#39;text&#39;: &#39;åŒ—äº¬å†¬å¥¥ä¼šè‡ªç”±å¼æ»‘é›ªå¥³å­å¤§è·³å°å†³èµ›&#39;}],&#xA;  &#39;é€‰æ‰‹&#39;: [{&#39;end&#39;: 31,&#xA;          &#39;probability&#39;: 0.8981548639781138,&#xA;          &#39;start&#39;: 28,&#xA;          &#39;text&#39;: &#39;è°·çˆ±å‡Œ&#39;}]}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;å°æ ·æœ¬å­¦ä¹ &lt;/h3&gt; &#xA;&lt;p&gt;å¦‚æœå¯¹ä¸€é”®é¢„æµ‹æ•ˆæœä¸æ»¡æ„ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œæ¨¡å‹ç²¾è°ƒï¼Œè¿›ä¸€æ­¥æå‡ç‰¹å®šåœºæ™¯çš„æ•ˆæœï¼Œè¯¦è§&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie/&#34;&gt;UIEå°æ ·æœ¬å®šåˆ¶è®­ç»ƒ&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æ›´å¤šPaddleNLPå†…å®¹å¯å‚è€ƒï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo&#34;&gt;ç²¾é€‰æ¨¡å‹åº“&lt;/a&gt;ï¼ŒåŒ…å«ä¼˜è´¨é¢„è®­ç»ƒæ¨¡å‹çš„ç«¯åˆ°ç«¯å…¨æµç¨‹ä½¿ç”¨ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples&#34;&gt;å¤šåœºæ™¯ç¤ºä¾‹&lt;/a&gt;ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨PaddleNLPè§£å†³NLPå¤šç§æŠ€æœ¯é—®é¢˜ï¼ŒåŒ…å«åŸºç¡€æŠ€æœ¯ã€ç³»ç»Ÿåº”ç”¨ä¸æ‹“å±•åº”ç”¨ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/personalcenter/thirdview/574995&#34;&gt;äº¤äº’å¼æ•™ç¨‹&lt;/a&gt;ï¼Œåœ¨ğŸ†“å…è´¹ç®—åŠ›å¹³å°AI Studioä¸Šå¿«é€Ÿå­¦ä¹ PaddleNLPã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;APIæ–‡æ¡£&lt;/h2&gt; &#xA;&lt;p&gt;PaddleNLPæä¾›å…¨æµç¨‹çš„æ–‡æœ¬é¢†åŸŸAPIï¼Œå¯å¤§å¹…æå‡NLPä»»åŠ¡å»ºæ¨¡çš„æ•ˆç‡ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ”¯æŒ&lt;a href=&#34;https://www.luge.ai&#34;&gt;åƒè¨€&lt;/a&gt;ç­‰ä¸°å¯Œä¸­æ–‡æ•°æ®é›†åŠ è½½çš„&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_list.html&#34;&gt;Dataset API&lt;/a&gt;ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æä¾›ğŸ¤—Hugging Face Styleçš„APIï¼Œæ”¯æŒ &lt;strong&gt;500+&lt;/strong&gt; ä¼˜è´¨é¢„è®­ç»ƒæ¨¡å‹åŠ è½½çš„&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html&#34;&gt;Transformers API&lt;/a&gt;ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æä¾›30+å¤šè¯­è¨€è¯å‘é‡çš„&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/model_zoo/embeddings.html&#34;&gt;Embedding API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æ›´å¤šä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/&#34;&gt;APIæ–‡æ¡£&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚æœPaddleNLPå¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{=paddlenlp,&#xA;    title={PaddleNLP: An Easy-to-use and High Performance NLP Library},&#xA;    author={PaddleNLP Contributors},&#xA;    howpublished = {\url{https://github.com/PaddlePaddle/PaddleNLP}},&#xA;    year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledge&lt;/h2&gt; &#xA;&lt;p&gt;æˆ‘ä»¬å€Ÿé‰´äº†Hugging Faceçš„&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;ğŸ¤—å…³äºé¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨çš„ä¼˜ç§€è®¾è®¡ï¼Œåœ¨æ­¤å¯¹Hugging Faceä½œè€…åŠå…¶å¼€æºç¤¾åŒºè¡¨ç¤ºæ„Ÿè°¢ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PaddleNLPéµå¾ª&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/LICENSE&#34;&gt;Apache-2.0å¼€æºåè®®&lt;/a&gt;ã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>danielgatis/rembg</title>
    <updated>2022-10-16T01:46:21Z</updated>
    <id>tag:github.com,2022-10-16:/danielgatis/rembg</id>
    <link href="https://github.com/danielgatis/rembg" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Rembg is a tool to remove images background.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rembg&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pepy.tech/project/rembg&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/rembg&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/rembg/month&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/rembg/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/rembg/week&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/rembg/week&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/badge/License-MIT-blue.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/KenjieDec/RemBG&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Rembg is a tool to remove images background. That is it.&lt;/p&gt; &#xA;&lt;p style=&#34;display: flex;align-items: center;justify-content: center;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-1.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-1.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-2.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-2.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-3.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-3.out.png&#34; width=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;p style=&#34;display: flex;align-items: center;justify-content: center;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-1.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-1.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-2.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-2.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-3.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-3.out.png&#34; width=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;p style=&#34;display: flex;align-items: center;justify-content: center;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-1.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-1.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-2.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-2.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-3.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-3.out.png&#34; width=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If this project has helped you, please consider making a &lt;a href=&#34;https://www.buymeacoffee.com/danielgatis&#34;&gt;donation&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;CPU support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install rembg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;GPU support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install rembg[gpu]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a cli&lt;/h3&gt; &#xA;&lt;p&gt;Remove the background from a remote image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -s http://input.png | rembg i &amp;gt; output.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remove the background from a local file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rembg i path/to/input.png path/to/output.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remove the background from all images in a folder&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rembg p path/to/input path/to/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a server&lt;/h3&gt; &#xA;&lt;p&gt;Start the server&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rembg s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And go to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://localhost:5000/docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image with background:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Gull_portrait_ca_usa.jpg/1280px-Gull_portrait_ca_usa.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image without background:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://localhost:5000/?url=https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Gull_portrait_ca_usa.jpg/1280px-Gull_portrait_ca_usa.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also you can send the file as a FormData (multipart/form-data):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;form&#xA;    action=&#34;http://localhost:5000&#34;&#xA;    method=&#34;post&#34;&#xA;    enctype=&#34;multipart/form-data&#34;&#xA;&amp;gt;&#xA;    &amp;lt;input type=&#34;file&#34; name=&#34;file&#34; /&amp;gt;&#xA;    &amp;lt;input type=&#34;submit&#34; value=&#34;upload&#34; /&amp;gt;&#xA;&amp;lt;/form&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a library&lt;/h3&gt; &#xA;&lt;p&gt;Input and output as bytes&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rembg import remove&#xA;&#xA;input_path = &#39;input.png&#39;&#xA;output_path = &#39;output.png&#39;&#xA;&#xA;with open(input_path, &#39;rb&#39;) as i:&#xA;    with open(output_path, &#39;wb&#39;) as o:&#xA;        input = i.read()&#xA;        output = remove(input)&#xA;        o.write(output)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Input and output as a PIL image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rembg import remove&#xA;from PIL import Image&#xA;&#xA;input_path = &#39;input.png&#39;&#xA;output_path = &#39;output.png&#39;&#xA;&#xA;input = Image.open(input_path)&#xA;output = remove(input)&#xA;output.save(output_path)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Input and output as a numpy array&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rembg import remove&#xA;import cv2&#xA;&#xA;input_path = &#39;input.png&#39;&#xA;output_path = &#39;output.png&#39;&#xA;&#xA;input = cv2.imread(input_path)&#xA;output = remove(input)&#xA;cv2.imwrite(output_path, output)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a docker&lt;/h3&gt; &#xA;&lt;p&gt;Try this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p 5000:5000 danielgatis/rembg s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image with background:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Gull_portrait_ca_usa.jpg/1280px-Gull_portrait_ca_usa.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image without background:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://localhost:5000/?url=https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Gull_portrait_ca_usa.jpg/1280px-Gull_portrait_ca_usa.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;All models are downloaded and saved in the user home folder in the &lt;code&gt;.u2net&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;The available models are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;u2net (&lt;a href=&#34;https://drive.google.com/uc?id=1tCU5MM1LhRgGou5OpmpjBQbSrYIUoYab&#34;&gt;download&lt;/a&gt; - &lt;a href=&#34;http://depositfiles.com/files/ltxbqa06w&#34;&gt;alternative&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;source&lt;/a&gt;): A pre-trained model for general use cases.&lt;/li&gt; &#xA; &lt;li&gt;u2netp (&lt;a href=&#34;https://drive.google.com/uc?id=1tNuFmLv0TSNDjYIkjEdeH1IWKQdUA4HR&#34;&gt;download&lt;/a&gt; - &lt;a href=&#34;http://depositfiles.com/files/0y9i0r2fy&#34;&gt;alternative&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;source&lt;/a&gt;): A lightweight version of u2net model.&lt;/li&gt; &#xA; &lt;li&gt;u2net_human_seg (&lt;a href=&#34;https://drive.google.com/uc?id=1ZfqwVxu-1XWC1xU1GHIP-FM_Knd_AX5j&#34;&gt;download&lt;/a&gt; - &lt;a href=&#34;http://depositfiles.com/files/6spp8qpey&#34;&gt;alternative&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;source&lt;/a&gt;): A pre-trained model for human segmentation.&lt;/li&gt; &#xA; &lt;li&gt;u2net_cloth_seg (&lt;a href=&#34;https://drive.google.com/uc?id=15rKbQSXQzrKCQurUjZFg8HqzZad8bcyz&#34;&gt;download&lt;/a&gt; - &lt;a href=&#34;http://depositfiles.com/files/l3z3cxetq&#34;&gt;alternative&lt;/a&gt;, &lt;a href=&#34;https://github.com/levindabhi/cloth-segmentation&#34;&gt;source&lt;/a&gt;): A pre-trained model for Cloths Parsing from human portrait. Here clothes are parsed into 3 category: Upper body, Lower body and Full body.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;How to train your own model&lt;/h4&gt; &#xA;&lt;p&gt;If You need more fine tunned models try this: &lt;a href=&#34;https://github.com/danielgatis/rembg/issues/193#issuecomment-1055534289&#34;&gt;https://github.com/danielgatis/rembg/issues/193#issuecomment-1055534289&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Advance usage&lt;/h3&gt; &#xA;&lt;p&gt;Sometimes it is possible to achieve better results by turning on alpha matting. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -s http://input.png | rembg i -a -ae 15 &amp;gt; output.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Original&lt;/td&gt; &#xA;   &lt;td&gt;Without alpha matting&lt;/td&gt; &#xA;   &lt;td&gt;With alpha matting (-a -ae 15)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/food-1.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/food-1.out.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/food-1.out.alpha.jpg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;In the cloud&lt;/h3&gt; &#xA;&lt;p&gt;Please contact me at &lt;a href=&#34;mailto:danielgatis@gmail.com&#34;&gt;danielgatis@gmail.com&lt;/a&gt; if you need help to put it on the cloud.&lt;/p&gt; &#xA;&lt;h3&gt;References&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.09007.pdf&#34;&gt;https://arxiv.org/pdf/2005.09007.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NathanUA/U-2-Net&#34;&gt;https://github.com/NathanUA/U-2-Net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pymatting/pymatting&#34;&gt;https://github.com/pymatting/pymatting&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Buy me a coffee&lt;/h3&gt; &#xA;&lt;p&gt;Liked some of my work? Buy me a coffee (or more likely a beer)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/danielgatis&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://bmc-cdn.nyc3.digitaloceanspaces.com/BMC-button-images/custom_images/orange_img.png&#34; alt=&#34;Buy Me A Coffee&#34; style=&#34;height: auto !important;width: auto !important;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;Copyright (c) 2020-present &lt;a href=&#34;https://github.com/danielgatis&#34;&gt;Daniel Gatis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Licensed under &lt;a href=&#34;https://raw.githubusercontent.com/danielgatis/rembg/main/LICENSE.txt&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>GuyTevet/motion-diffusion-model</title>
    <updated>2022-10-16T01:46:21Z</updated>
    <id>tag:github.com,2022-10-16:/GuyTevet/motion-diffusion-model</id>
    <link href="https://github.com/GuyTevet/motion-diffusion-model" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official PyTorch implementation of the paper &#34;Human Motion Diffusion Model&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MDM: Human Motion Diffusion Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/motion-synthesis-on-humanact12?p=human-motion-diffusion-model&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/human-motion-diffusion-model/motion-synthesis-on-humanact12&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/motion-synthesis-on-humanml3d?p=human-motion-diffusion-model&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/human-motion-diffusion-model/motion-synthesis-on-humanml3d&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.14916&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-%3C2209.14916%3E-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The official PyTorch implementation of the paper &lt;a href=&#34;https://arxiv.org/abs/2209.14916&#34;&gt;&lt;strong&gt;&#34;Human Motion Diffusion Model&#34;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please visit our &lt;a href=&#34;https://guytevet.github.io/mdm-page/&#34;&gt;&lt;strong&gt;webpage&lt;/strong&gt;&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/GuyTevet/mdm-page/raw/main/static/figures/github.gif&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Bibtex&lt;/h4&gt; &#xA;&lt;p&gt;If you find this code useful in your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{tevet2022human,&#xA;  title={Human Motion Diffusion Model},&#xA;  author={Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Bermano, Amit H and Cohen-Or, Daniel},&#xA;  journal={arXiv preprint arXiv:2209.14916},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;ğŸ“¢ &lt;strong&gt;9/Oct/22&lt;/strong&gt; - Added training and evaluation scripts. Note slight env changes adapting to the new code. If you already have an installed environment, run &lt;code&gt;bash prepare/download_glove.sh; pip install clearml&lt;/code&gt; to adapt.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ“¢ &lt;strong&gt;6/Oct/22&lt;/strong&gt; - First release - sampling and rendering using pre-trained models.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;This code was tested on &lt;code&gt;Ubuntu 18.04.5 LTS&lt;/code&gt; and requires:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7&lt;/li&gt; &#xA; &lt;li&gt;conda3 or miniconda3&lt;/li&gt; &#xA; &lt;li&gt;CUDA capable GPU (one is enough)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. Setup environment&lt;/h3&gt; &#xA;&lt;p&gt;Install ffmpeg (if not already installed):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt update&#xA;sudo apt install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For windows use &lt;a href=&#34;https://www.geeksforgeeks.org/how-to-install-ffmpeg-on-windows/&#34;&gt;this&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;Setup conda env:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f environment.yml&#xA;conda activate mdm&#xA;python -m spacy download en_core_web_sm&#xA;pip install git+https://github.com/openai/CLIP.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash prepare/download_smpl_files.sh&#xA;bash prepare/download_glove.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Get data&lt;/h3&gt; &#xA;&lt;p&gt;There are two paths to get the data:&lt;/p&gt; &#xA;&lt;p&gt;(a) &lt;strong&gt;Go the easy way if&lt;/strong&gt; you just want to generate text-to-motion (excluding editing which does require motion capture data)&lt;/p&gt; &#xA;&lt;p&gt;(b) &lt;strong&gt;Get full data&lt;/strong&gt; to train and evaluate the model.&lt;/p&gt; &#xA;&lt;h4&gt;a. The easy way (text only)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt; - Clone HumanML3D, then copy the data dir to our repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ..&#xA;git clone https://github.com/EricGuo5513/HumanML3D.git&#xA;unzip ./HumanML3D/HumanML3D/texts.zip -d ./HumanML3D/HumanML3D/&#xA;cp -r HumanML3D/HumanML3D motion-diffusion-model/dataset/HumanML3D&#xA;cd motion-diffusion-model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;b. Full data (text + motion capture)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt; - Follow the instructions in &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt;, then copy the result dataset to our repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cp -r ../HumanML3D/HumanML3D ./dataset/HumanML3D&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt; - Download from &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt; (no processing needed this time) and the place result in &lt;code&gt;./dataset/KIT-ML&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3. Download the pretrained models&lt;/h3&gt; &#xA;&lt;p&gt;Download the model(s) you wish to use, then unzip and place it in &lt;code&gt;./save/&lt;/code&gt;. &lt;strong&gt;For text-to-motion, you need only the first one.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1PE0PK8e5a5j-7-Xhs5YET5U5pGh0c821/view?usp=sharing&#34;&gt;humanml-encoder-512&lt;/a&gt; (best model)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1q3soLadvVh7kJuJPd2cegMNY2xVuVudj/view?usp=sharing&#34;&gt;humanml-decoder-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1GnsW0K3UjuOkNkAWmjrGIUmeDDZrmPE5/view?usp=sharing&#34;&gt;humanml-decoder-with-emb-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1SHCRcE0es31vkJMLGf9dyLe7YsWj7pNL/view?usp=sharing&#34;&gt;kit-encoder-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Generate text-to-motion&lt;/h2&gt; &#xA;&lt;h3&gt;Generate from test set prompts&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --num_samples 10 --num_repetitions 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate from your text file&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --input_text ./assets/example_text_prompts.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate a single prompt&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --text_prompt &#34;the person walked forward and is picking up his toolbox.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can also define:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--device&lt;/code&gt; id.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--seed&lt;/code&gt; to sample different prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--motion_length&lt;/code&gt; in seconds (maximum is 9.8[sec]).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running those will get you:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;results.npy&lt;/code&gt; file with text prompts and xyz positions of the generated animation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample##_rep##.mp4&lt;/code&gt; - a stick figure animation for each generated motion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It will look something like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GuyTevet/motion-diffusion-model/main/assets/example_stick_fig.gif&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can stop here, or render the SMPL mesh using the following script.&lt;/p&gt; &#xA;&lt;h3&gt;Render SMPL mesh&lt;/h3&gt; &#xA;&lt;p&gt;To create SMPL mesh per frame run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m visualize.render_mesh --input_path /path/to/mp4/stick/figure/file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;This script outputs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sample##_rep##_smpl_params.npy&lt;/code&gt; - SMPL parameters (thetas, root translations, vertices and faces)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample##_rep##_obj&lt;/code&gt; - Mesh per frame in &lt;code&gt;.obj&lt;/code&gt; format.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;.obj&lt;/code&gt; can be integrated into Blender/Maya/3DS-MAX and rendered using them.&lt;/li&gt; &#xA; &lt;li&gt;This script is running &lt;a href=&#34;https://smplify.is.tue.mpg.de/&#34;&gt;SMPLify&lt;/a&gt; and needs GPU as well (can be specified with the &lt;code&gt;--device&lt;/code&gt; flag).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Important&lt;/strong&gt; - Do not change the original &lt;code&gt;.mp4&lt;/code&gt; path before running the script.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes for 3d makers:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You have two ways to animate the sequence: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Use the &lt;a href=&#34;https://smpl.is.tue.mpg.de/index.html&#34;&gt;SMPL add-on&lt;/a&gt; and the theta parameters saved to &lt;code&gt;sample##_rep##_smpl_params.npy&lt;/code&gt; (we always use beta=0 and the gender-neutral model).&lt;/li&gt; &#xA;   &lt;li&gt;A more straightforward way is using the mesh data itself. All meshes have the same topology (SMPL), so you just need to keyframe vertex locations. Since the OBJs are not preserving vertices order, we also save this data to the &lt;code&gt;sample##_rep##_smpl_params.npy&lt;/code&gt; file for your convenience.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Editing&lt;/h3&gt; &#xA;&lt;p&gt;ETA - Nov 22&lt;/p&gt; &#xA;&lt;h2&gt;Train your own MDM&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m train.train_mdm --save_dir save/my_humanml_trans_enc_512 --dataset humanml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m train.train_mdm --save_dir save/my_kit_trans_enc_512 --dataset kit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--device&lt;/code&gt; to define GPU id.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--arch&lt;/code&gt; to choose one of the architectures reported in the paper &lt;code&gt;{trans_enc, trans_dec, gru}&lt;/code&gt; (&lt;code&gt;trans_enc&lt;/code&gt; is default).&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--train_platform_type {ClearmlPlatform, TensorboardPlatform}&lt;/code&gt; to track results with either &lt;a href=&#34;https://clear.ml/&#34;&gt;ClearML&lt;/a&gt; or &lt;a href=&#34;https://www.tensorflow.org/tensorboard&#34;&gt;Tensorboard&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--eval_during_training&lt;/code&gt; to run a short (90 minutes) evaluation for each saved checkpoint. This will slow down training but will give you better monitoring.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluate&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Takes about 20 hours (on a single GPU)&lt;/li&gt; &#xA; &lt;li&gt;The output of this script for the pre-trained models (as was reported in the paper) is provided in the checkpoints zip file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m eval.eval_humanml --model_path ./save/humanml_trans_enc_512/model000475000.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m eval.eval_humanml --model_path ./save/kit_trans_enc_512/model000400000.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This code is standing on the shoulders of giants. We want to thank the following contributors that our code is based on:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;guided-diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/GuyTevet/MotionCLIP&#34;&gt;MotionCLIP&lt;/a&gt;, &lt;a href=&#34;https://github.com/EricGuo5513/text-to-motion&#34;&gt;text-to-motion&lt;/a&gt;, &lt;a href=&#34;https://github.com/Mathux/ACTOR&#34;&gt;actor&lt;/a&gt;, &lt;a href=&#34;https://github.com/wangsen1312/joints2smpl&#34;&gt;joints2smpl&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code is distributed under an &lt;a href=&#34;https://raw.githubusercontent.com/GuyTevet/motion-diffusion-model/main/LICENSE&#34;&gt;MIT LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that our code depends on other libraries, including CLIP, SMPL, SMPL-X, PyTorch3D, and uses datasets that each have their own respective licenses that must also be followed.&lt;/p&gt;</summary>
  </entry>
</feed>