<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-18T01:55:19Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AILab-CVC/YOLO-World</title>
    <updated>2024-02-18T01:55:19Z</updated>
    <id>tag:github.com,2024-02-18:/AILab-CVC/YOLO-World</id>
    <link href="https://github.com/AILab-CVC/YOLO-World" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-Time Open-Vocabulary Object Detection&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/assets/yolo_logo.png&#34; width=&#34;60%&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=PH8rJHYAAAAJ&#34;&gt;Tianheng Cheng&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;2,3,*&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://linsong.info/&#34;&gt;Lin Song&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;1,üìß,*&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://yxgeee.github.io/&#34;&gt;Yixiao Ge&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;1,üåü,2&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;http://eic.hust.edu.cn/professor/liuwenyu/&#34;&gt; Wenyu Liu&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;3&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://xwcv.github.io/&#34;&gt;Xinggang Wang&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;3,üìß&lt;/span&gt;&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt;&#xA; &lt;sup&gt;&lt;span&gt;1,2&lt;/span&gt;&lt;/sup&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;* Equal contribution üåü Project lead üìß Corresponding author&lt;/p&gt; &#xA; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Tencent AI Lab, &lt;sup&gt;2&lt;/sup&gt; ARC Lab, Tencent PCG &lt;sup&gt;3&lt;/sup&gt; Huazhong University of Science and Technology &lt;br&gt;&lt;/p&gt; &#xA; &lt;div&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://wondervictor.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34; alt=&#34;arxiv paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.17270&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-red&#34; alt=&#34;arxiv paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/stevengrove/YOLO-World&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97HugginngFace-Spaces-orange&#34; alt=&#34;demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/zsxkib/yolo-world&#34;&gt;&lt;img src=&#34;https://replicate.com/zsxkib/yolo-world/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2401.17270&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97HugginngFace-Paper-yellow&#34; alt=&#34;hfpaper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-GPLv3.0-blue&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;üî•[2024-2-17]:&lt;/code&gt; The largest model &lt;strong&gt;X&lt;/strong&gt; of YOLO-World is released, which achieves better zero-shot performance!&lt;br&gt; &lt;code&gt;üî•[2024-2-17]:&lt;/code&gt; We release the code &amp;amp; models for &lt;strong&gt;YOLO-World-Seg&lt;/strong&gt; now! YOLO-World now supports open-vocabulary / zero-shot object segmentation!&lt;br&gt; &lt;code&gt;[2024-2-15]:&lt;/code&gt; The pre-traind YOLO-World-L with CC3M-Lite is released!&lt;br&gt; &lt;code&gt;[2024-2-14]:&lt;/code&gt; We provide the &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/demo.py&#34;&gt;&lt;code&gt;image_demo&lt;/code&gt;&lt;/a&gt; for inference on images or directories.&lt;br&gt; &lt;code&gt;[2024-2-10]:&lt;/code&gt; We provide the &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/finetuning.md&#34;&gt;fine-tuning&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/data.md&#34;&gt;data&lt;/a&gt; details for fine-tuning YOLO-World on the COCO dataset or the custom datasets!&lt;br&gt; &lt;code&gt;[2024-2-3]:&lt;/code&gt; We support the &lt;code&gt;Gradio&lt;/code&gt; demo now in the repo and you can build the YOLO-World demo on your own device!&lt;br&gt; &lt;code&gt;[2024-2-1]:&lt;/code&gt; We&#39;ve released the code and weights of YOLO-World now!&lt;br&gt; &lt;code&gt;[2024-2-1]:&lt;/code&gt; We deploy the YOLO-World demo on &lt;a href=&#34;https://huggingface.co/spaces/stevengrove/YOLO-World&#34;&gt;HuggingFace ü§ó&lt;/a&gt;, you can try it now!&lt;br&gt; &lt;code&gt;[2024-1-31]:&lt;/code&gt; We are excited to launch &lt;strong&gt;YOLO-World&lt;/strong&gt;, a cutting-edge real-time open-vocabulary object detector.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;p&gt;YOLO-World is under active development and please stay tuned ‚òïÔ∏è!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio demo!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Complete documents for pre-training YOLO-World.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; COCO &amp;amp; LVIS fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Extra pre-trained models on more data, such as CC3M.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Deployment toolkits, e.g., ONNX or TensorRT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Inference acceleration and scripts for speed evaluation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Automatic labeling framework for image-text pairs, such as CC3M.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains the PyTorch implementation, pre-trained weights, and pre-training/fine-tuning code for YOLO-World.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;YOLO-World is pre-trained on large-scale datasets, including detection, grounding, and image-text datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;YOLO-World is the next-generation YOLO detector, with a strong open-vocabulary detection capability and grounding ability.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;YOLO-World presents a &lt;em&gt;prompt-then-detect&lt;/em&gt; paradigm for efficient user-vocabulary inference, which re-parameterizes vocabulary embeddings as parameters into the model and achieve superior inference speed. You can try to export your own detection model without extra training or fine-tuning in our &lt;a href=&#34;https://huggingface.co/spaces/stevengrove/YOLO-World&#34;&gt;online demo&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;center&gt; &#xA; &lt;img width=&#34;800px&#34; src=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/assets/yolo_arch.png&#34;&gt; &#xA;&lt;/center&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.&lt;/p&gt; &#xA;&lt;h2&gt;Main Results&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve pre-trained YOLO-World-S/M/L from scratch and evaluate on the &lt;code&gt;LVIS val-1.0&lt;/code&gt; and &lt;code&gt;LVIS minival&lt;/code&gt;. We provide the pre-trained model weights and training logs for applications/research or re-producing the results.&lt;/p&gt; &#xA;&lt;h3&gt;Zero-shot Inference on LVIS dataset&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Pre-train Data&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;fixed&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;mini&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;r&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;c&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;f&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;r&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;c&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;f&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_s_dual_3block_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-S&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/resolve/main/yolo_world_s_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_train_pretrained-18bea4d2.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_m_dual_l2norm_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/resolve/main/yolo_world_m_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_train_pretrained-2b7bd1be.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_s_dual_3block_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/resolve/main/yolo_world_l_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_train_pretrained-0e566235.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;üî• &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_s_dual_3block_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG+CC3M-Lite&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_l_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_cc3mlite_train_pretrained-7a5eea3b.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;üî• &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/pretrain/yolo_world_s_dual_3block_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py&#34;&gt;YOLO-World-X&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;O365+GoldG+CC3M-Lite&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_x_clip_base_dual_vlpan_2e-3adamw_32xb16_100e_o365_goldg_cc3mlite_train_pretrained-8cf6b025.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The evaluation results of AP&lt;sup&gt;fixed&lt;/sup&gt; are tested on LVIS &lt;code&gt;minival&lt;/code&gt; with &lt;a href=&#34;https://github.com/achalddave/large-vocab-devil&#34;&gt;fixed AP&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The evaluation results of AP&lt;sup&gt;mini&lt;/sup&gt; are tested on LVIS &lt;code&gt;minival&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The evaluation results of AP&lt;sup&gt;val&lt;/sup&gt; are tested on LVIS &lt;code&gt;val 1.0&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hf-mirror.com/&#34;&gt;HuggingFace Mirror&lt;/a&gt; provides the mirror of HuggingFace, which is a choice for users who are unable to reach.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;YOLO-World-Seg: Open-Vocabulary Instance Segmentation&lt;/h3&gt; &#xA;&lt;p&gt;We fine-tune YOLO-World on LVIS (&lt;code&gt;LVIS-Base&lt;/code&gt;) with mask annotations for open-vocabulary (zero-shot) instance segmentation.&lt;/p&gt; &#xA;&lt;p&gt;We provide two fine-tuning strategies YOLO-World towards open-vocabulary instance segmentation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;fine-tuning &lt;code&gt;all modules&lt;/code&gt;: leads to better LVIS segmentation accuracy but affects the zero-shot performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;fine-tuning the &lt;code&gt;segmentation head&lt;/code&gt;: maintains the zero-shot performanc but lowers LVIS segmentation accuracy.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Fine-tuning Data&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Fine-tuning Modules&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;mask&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;r&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;c&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sub&gt;f&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/segmentation/yolo_world_seg_m_dual_vlpan_2e-4_80e_8gpus_allmodules_finetune_lvis.py&#34;&gt;YOLO-World-Seg-M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LVIS-Base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;all modules&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_seg_m_dual_vlpan_2e-4_80e_8gpus_allmodules_finetune_lvis-ca465825.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/segmentation/yolo_world_seg_l_dual_vlpan_2e-4_80e_8gpus_allmodules_finetune_lvis.py&#34;&gt;YOLO-World-Seg-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LVIS-Base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;all modules&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_seg_l_dual_vlpan_2e-4_80e_8gpus_allmodules_finetune_lvis-8c58c916.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/segmentation/yolo_seg_world_m_dual_vlpan_2e-4_80e_8gpus_seghead_finetune_lvis.py&#34;&gt;YOLO-World-Seg-M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LVIS-Base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;seg head&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_seg_m_dual_vlpan_2e-4_80e_8gpus_seghead_finetune_lvis-7bca59a7.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/configs/segmentation/yolo_seg_world_l_dual_vlpan_2e-4_80e_8gpus_seghead_finetune_lvis.py&#34;&gt;YOLO-World-Seg-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LVIS-Base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;seg head&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_seg_l_dual_vlpan_2e-4_80e_8gpus_seghead_finetune_lvis-5a642d30.pth&#34;&gt;HF Checkpoints ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The mask AP are evaluated on the LVIS &lt;code&gt;val 1.0&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;All models are fine-tuned for 80 epochs on &lt;code&gt;LVIS-Base&lt;/code&gt; (866 categories, &lt;code&gt;common + frequent&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;The YOLO-World-Seg with only &lt;code&gt;seg head&lt;/code&gt; fine-tuned maintains the original zero-shot detection capability and segments objects.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;1. Installation&lt;/h3&gt; &#xA;&lt;p&gt;YOLO-World is developed based on &lt;code&gt;torch==1.11.0&lt;/code&gt; &lt;code&gt;mmyolo==0.6.0&lt;/code&gt; and &lt;code&gt;mmdetection==3.0.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch wheel -q&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Preparing Data&lt;/h3&gt; &#xA;&lt;p&gt;We provide the details about the pre-training data in &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/data.md&#34;&gt;docs/data&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training &amp;amp; Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We adopt the default &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/tools/train.py&#34;&gt;training&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/tools/test.py&#34;&gt;evaluation&lt;/a&gt; scripts of &lt;a href=&#34;https://github.com/open-mmlab/mmyolo&#34;&gt;mmyolo&lt;/a&gt;. We provide the configs for pre-training and fine-tuning in &lt;code&gt;configs/pretrain&lt;/code&gt; and &lt;code&gt;configs/finetune_coco&lt;/code&gt;. Training YOLO-World is easy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x tools/dist_train.sh&#xA;# sample command for pre-training, use AMP for mixed-precision training&#xA;./tools/dist_train.sh configs/pretrain/yolo_world_l_t2i_bn_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py 8 --amp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; YOLO-World is pre-trained on 4 nodes with 8 GPUs per node (32 GPUs in total). For pre-training, the &lt;code&gt;node_rank&lt;/code&gt; and &lt;code&gt;nnodes&lt;/code&gt; for multi-node training should be specified.&lt;/p&gt; &#xA;&lt;p&gt;Evaluating YOLO-World is also easy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x tools/dist_test.sh&#xA;./tools/dist_test.sh path/to/config path/to/weights 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; We mainly evaluate the performance on LVIS-minival for pre-training.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning YOLO-World&lt;/h2&gt; &#xA;&lt;p&gt;We provide the details about fine-tuning YOLO-World in &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/finetuning.md&#34;&gt;docs/fine-tuning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;We provide the details about deployment for downstream applications in &lt;a href=&#34;https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/docs/deploy.md&#34;&gt;docs/deployment&lt;/a&gt;. You can directly download the ONNX model through the online &lt;a href=&#34;https://huggingface.co/spaces/stevengrove/YOLO-World&#34;&gt;demo&lt;/a&gt; in Huggingface Spaces ü§ó.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA;&lt;p&gt;We provide the &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio&lt;/a&gt; demo for local devices:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gradio&#xA;python demo.py path/to/config path/to/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Image Demo&lt;/h3&gt; &#xA;&lt;p&gt;We provide a simple image demo for inference on images with visualization outputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python image_demo.py path/to/config path/to/weights image/path/directory &#39;person,dog,cat&#39; --topk 100 --threshold 0.005 --output-dir demo_outputs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;image&lt;/code&gt; can be a directory or a single image.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;texts&lt;/code&gt; can be a string of categories (noun phrases) which is separated by a comma. We also support &lt;code&gt;txt&lt;/code&gt; file in which each line contains a category ( noun phrases).&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;topk&lt;/code&gt; and &lt;code&gt;threshold&lt;/code&gt; control the number of predictions and the confidence threshold.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Google Golab Notebook&lt;/h3&gt; &#xA;&lt;p&gt;We sincerely thank &lt;a href=&#34;https://github.com/onuralpszr&#34;&gt;Onuralp&lt;/a&gt; for sharing the &lt;a href=&#34;https://colab.research.google.com/drive/1F_7S5lSaFM06irBCZqjhbN7MpUXo6WwO?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt;, you can have a try üòäÔºÅ&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We sincerely thank &lt;a href=&#34;https://github.com/open-mmlab/mmyolo&#34;&gt;mmyolo&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;mmdetection&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/GLIP&#34;&gt;GLIP&lt;/a&gt;, and &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; for providing their wonderful code to the community!&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you find YOLO-World is useful in your research or applications, please consider giving us a star üåü and citing it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{cheng2024yolow,&#xA;  title={YOLO-World: Real-Time Open-Vocabulary Object Detection},&#xA;  author={Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},&#xA;  journal={arXiv preprint arXiv:2401.17270},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Licence&lt;/h2&gt; &#xA;&lt;p&gt;YOLO-World is under the GPL-v3 Licence and is supported for comercial usage.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>time-series-foundation-models/lag-llama</title>
    <updated>2024-02-18T01:55:19Z</updated>
    <id>tag:github.com,2024-02-18:/time-series-foundation-models/lag-llama</id>
    <link href="https://github.com/time-series-foundation-models/lag-llama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/time-series-foundation-models/lag-llama/main/images/lagllama.webp&#34; alt=&#34;lag-llama-architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lag-Llama is the &lt;b&gt;first open-source foundation model for time series forecasting&lt;/b&gt;!&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://twitter.com/arjunashok37/status/1755261111233114165&#34;&gt;Tweet Thread&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/time-series-foundation-models/Lag-Llama&#34;&gt;Model Weights&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/drive/1XxrLW9VGPlZDw3efTvUi0hQimgJOwQG6?usp=sharing&#34;&gt;Colab Demo on Zero-Shot Forecasting&lt;/a&gt;] [&lt;a href=&#34;https://github.com/time-series-foundation-models/lag-llama&#34;&gt;GitHub&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2310.08278&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This repository houses the Lag-Llama architecture.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;b&gt;Updates&lt;/b&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;17-Feb-2024&lt;/strong&gt;: We have released a new updated &lt;a href=&#34;https://colab.research.google.com/drive/1XxrLW9VGPlZDw3efTvUi0hQimgJOwQG6?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for zero-shot forecasting that shows how one can load time series of different formats.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;7-Feb-2024&lt;/strong&gt;: We released Lag-Llama, with open-source model checkpoints and a Colab Demo for zero-shot forecasting.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Current Features:&lt;/p&gt; &#xA;&lt;p&gt;üí´ &lt;b&gt;Zero-shot forecasting&lt;/b&gt; on a dataset of &lt;b&gt;any frequency&lt;/b&gt; for &lt;b&gt;any prediction length&lt;/b&gt;, using the &lt;a href=&#34;https://colab.research.google.com/drive/1XxrLW9VGPlZDw3efTvUi0hQimgJOwQG6?usp=sharing&#34; target=&#34;_blank&#34;&gt;Colab Demo.&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Coming Soon:&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê An &lt;b&gt;online gradio demo&lt;/b&gt; where you can upload time series and get zero-shot predictions and perform finetuning.&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Features for &lt;b&gt;finetuning&lt;/b&gt; the foundation model&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Features for &lt;b&gt;pretraining&lt;/b&gt; Lag-Llama on your own large-scale data&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Scripts to &lt;b&gt;reproduce&lt;/b&gt; all results in the paper.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Stay Tuned!ü¶ô&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mit-han-lab/efficientvit</title>
    <updated>2024-02-18T01:55:19Z</updated>
    <id>tag:github.com,2024-02-18:/mit-han-lab/efficientvit</id>
    <link href="https://github.com/mit-han-lab/efficientvit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EfficientViT is a new family of vision models for efficient high-resolution vision.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction (&lt;a href=&#34;https://arxiv.org/abs/2205.14756&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/efficientvit_poster.pdf&#34;&gt;poster&lt;/a&gt;)&lt;/h1&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you are interested in getting updates, please join our mailing list &lt;a href=&#34;https://forms.gle/Z6DNkRidJ1ouxmUk9&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/02/08] Tech report of EfficientViT-SAM is available: &lt;a href=&#34;https://arxiv.org/abs/2402.05008&#34;&gt;arxiv&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/02/07] We released EfficientViT-SAM, the first accelerated SAM model that matches/outperforms SAM-ViT-H&#39;s zero-shot performance, delivering the SOTA performance-efficiency trade-off.&lt;/li&gt; &#xA; &lt;li&gt;[2023/11/20] EfficientViT is available in the &lt;a href=&#34;https://www.jetson-ai-lab.com/tutorial_efficientvit.html&#34;&gt;NVIDIA Jetson Generative AI Lab&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/09/12] EfficientViT is highlighted by &lt;a href=&#34;https://www.mit.edu/archive/spotlight/efficient-computer-vision/&#34;&gt;MIT home page&lt;/a&gt; and &lt;a href=&#34;https://news.mit.edu/2023/ai-model-high-resolution-computer-vision-0912&#34;&gt;MIT News&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/07/18] EfficientViT is accepted by ICCV 2023.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About EfficientViT Models&lt;/h2&gt; &#xA;&lt;p&gt;EfficientViT is a new family of ViT models for efficient high-resolution dense prediction vision tasks. The core building block of EfficientViT is a lightweight, multi-scale linear attention module that achieves global receptive field and multi-scale learning with only hardware-efficient operations, making EfficientViT TensorRT-friendly and suitable for GPU deployment.&lt;/p&gt; &#xA;&lt;h2&gt;Third-Party Implementation/Integration&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jetson-ai-lab.com/tutorial_efficientvit.html&#34;&gt;NVIDIA Jetson Generative AI Lab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/pytorch-image-models&#34;&gt;timm&lt;/a&gt;: &lt;a href=&#34;https://github.com/huggingface/pytorch-image-models/raw/main/timm/models/efficientvit_mit.py&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling&#34;&gt;X-AnyLabeling&lt;/a&gt;: &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling/raw/main/anylabeling/services/auto_labeling/efficientvit_sam.py&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n efficientvit python=3.10&#xA;conda activate efficientvit&#xA;conda install -c conda-forge mpi4py openmpi&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;EfficientViT Applications&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/applications/sam.md&#34;&gt;Segment Anything&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/sam_viz.pdf&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/sam_zero_shot_coco_mAP.png&#34; width=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/applications/cls.md&#34;&gt;Image Classification&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/cls_results.png&#34; width=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/applications/seg.md&#34;&gt;Semantic Segmentation&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/demo/cityscapes_l1.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Han Cai: &lt;a href=&#34;mailto:hancai@mit.edu&#34;&gt;hancai@mit.edu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ImageNet Pretrained models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Segmentation Pretrained models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ImageNet training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; EfficientViT L series, designed for cloud&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; EfficientViT for segment anything&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; EfficientViT for image generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; EfficientViT for CLIP&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; EfficientViT for super-resolution&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Segmentation training code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If EfficientViT is useful or relevant to your research, please kindly recognize our contributions by citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{cai2022efficientvit,&#xA;  title={Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition},&#xA;  author={Cai, Han and Gan, Chuang and Han, Song},&#xA;  journal={arXiv preprint arXiv:2205.14756},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>