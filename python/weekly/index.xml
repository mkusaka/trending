<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-29T01:43:24Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Epodonios/v2ray-configs</title>
    <updated>2025-06-29T01:43:24Z</updated>
    <id>tag:github.com,2025-06-29:/Epodonios/v2ray-configs</id>
    <link href="https://github.com/Epodonios/v2ray-configs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free vless-vmess-shadowsocks-trojan-xray-V2ray Configs Updating Every 5 minutes&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/barry-far/V2ray-Configs.svg?sanitize=true&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;a href=&#34;https://lbesson.mit-license.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;MIT license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Epodonios/V2ray-Configs/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/barry-far/V2ray-Configs/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;Update Configs&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/repo-size/Epodonios/V2ray-Configs&#34; alt=&#34;GitHub repo size&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Bulk V2ray Configs&lt;/h1&gt; &#xA;&lt;p&gt;üíª This repository contains a collection of free V2ray configuration files that you can use with your V2ray client to access the internet securely and anonymously. This script collects several thousand V2ray configurations every five minutes, and you can receive and use the protocol in base 64, normal, or split format.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Protocols:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vmess&lt;/li&gt; &#xA; &lt;li&gt;Vless&lt;/li&gt; &#xA; &lt;li&gt;Trojan&lt;/li&gt; &#xA; &lt;li&gt;Tuic&lt;/li&gt; &#xA; &lt;li&gt;Shadowsocks&lt;/li&gt; &#xA; &lt;li&gt;ShadowsocksR&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;You can use a v2ray client to use these subscription links:&lt;/h3&gt; &#xA;&lt;h4&gt;Android:&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;hiddify-next&lt;/li&gt; &#xA; &lt;li&gt;v2rayng&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;IOS:&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;fair&lt;/li&gt; &#xA; &lt;li&gt;streisand&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Windows and Linux:&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;hiddify-next&lt;/li&gt; &#xA; &lt;li&gt;nekoray&lt;/li&gt; &#xA; &lt;li&gt;v2rayn&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Subscriptions Links&lt;/h2&gt; &#xA;&lt;h3&gt;Here are the subscription links at your disposal:&lt;/h3&gt; &#xA;&lt;p&gt;All collected configs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/All_Configs_Sub.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the above link doesn&#39;t work, try the base 64 configurations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/All_Configs_base64_Sub.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Splited by protocol:&lt;/h3&gt; &#xA;&lt;p&gt;Vless:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vless.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Vmess:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vmess.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ss:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/ss.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ssr:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/ssr.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Trojan:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/trojan.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Splited in 500 count of configs:&lt;/h3&gt; &#xA;&lt;p&gt;Config List 1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub1.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub2.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 3:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub3.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 4:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub4.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 5:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub5.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 6:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub6.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 7:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub7.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 8:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub8.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 9:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub9.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 10:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub10.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 11:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub11.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 12:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub12.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 13:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub13.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config List 14:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub14.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage:&lt;/h3&gt; &#xA;&lt;p&gt;Mobile and pc:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy the links provided and go to your v2ray clients subscription setting and paste metioned link and save that.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Occasionally use the subscription update function in your v2ray client to stay up-to-date ü§ù.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;I hope u use this configs very well.&lt;/p&gt; &#xA;&lt;h2&gt;Tunnel entire system:&lt;/h2&gt; &#xA;&lt;p&gt;For better use and tunneling the entire system, you can use a proxy program. The usage steps are as follows:&lt;/p&gt; &#xA;&lt;h3&gt;Usage Instructions:&lt;/h3&gt; &#xA;&lt;p&gt;1-First, install the Proxifier program.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://proxifier.com/download/&#34;&gt;https://proxifier.com/download/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;2-Activate the program:&lt;/p&gt; &#xA;&lt;p&gt;Activation keys:&lt;/p&gt; &#xA;&lt;p&gt;Portable Edition:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;L6Z8A-XY2J4-BTZ3P-ZZ7DF-A2Q9C&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Standard Edition:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  5EZ8G-C3WL5-B56YG-SCXM9-6QZAP&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Mac OS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; P427L-9Y552-5433E-8DSR3-58Z68&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;3-Go to the Profile section and select the Proxy Server. In the displayed section, click on Add.&lt;/p&gt; &#xA;&lt;p&gt;4-Enter the following information:&lt;/p&gt; &#xA;&lt;p&gt;IP: Enter 127.0.0.1&lt;/p&gt; &#xA;&lt;p&gt;Port: Depending on the version you are using, enter:&lt;/p&gt; &#xA;&lt;p&gt;V2rayN: 10808&lt;/p&gt; &#xA;&lt;p&gt;Netch: 2801&lt;/p&gt; &#xA;&lt;p&gt;SSR: 1080&lt;/p&gt; &#xA;&lt;p&gt;Mac V2rayU: 1086&lt;/p&gt; &#xA;&lt;p&gt;Protocol: Select SOCKS5&lt;/p&gt; &#xA;&lt;p&gt;5-Enjoy!&lt;/p&gt; &#xA;&lt;p&gt;Some installed programs on the system, like Spotube, might not fully tunnel. This issue can be resolved with this method.&lt;/p&gt; &#xA;&lt;p&gt;Your friend, EPODONIOS&lt;/p&gt; &#xA;&lt;h2&gt;u can use this feature with another way it no needs any program set by system tools&lt;/h2&gt; &#xA;&lt;h3&gt;instruction:&lt;/h3&gt; &#xA;&lt;p&gt;1- open your OS setting&lt;/p&gt; &#xA;&lt;p&gt;2- go to proxy section&lt;/p&gt; &#xA;&lt;p&gt;3- in proxy section set this values : ip : 127.0.0.1&lt;/p&gt; &#xA;&lt;p&gt;port : 10809&lt;/p&gt; &#xA;&lt;p&gt;local host :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;localhost;127.*;10.*;172.16.*;172.17.*;172.18.*;172.19.*;172.20.*;172.21.*;172.22.*;172.23.*;172.24.*;172.25.*;172.26.*;172.27.*;172.28.*;172.29.*;172.30.*;172.31.*;192.168.*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;4- then set it up with ON key&lt;/p&gt; &#xA;&lt;p&gt;5- back to v2rayn and after set your config turn it to set system proxy&lt;/p&gt; &#xA;&lt;p&gt;6- now your system tunneled entirely&lt;/p&gt; &#xA;&lt;p&gt;ur friend,EPODONIOS&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gensyn-ai/rl-swarm</title>
    <updated>2025-06-29T01:43:24Z</updated>
    <id>tag:github.com,2025-06-29:/gensyn-ai/rl-swarm</id>
    <link href="https://github.com/gensyn-ai/rl-swarm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fully open source framework for creating RL training swarms over the internet.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RL Swarm&lt;/h1&gt; &#xA;&lt;p&gt;RL Swarm is a peer-to-peer system for reinforcement learning. It allows you to train models collaboratively with others in the swarm, leveraging their collective intelligence. It is open source and permissionless, meaning you can run it on a consumer laptop at home or on a powerful GPU in the cloud. You can also connect your model to the Gensyn Testnet to receive an on-chain identity that tracks your progress over time.&lt;/p&gt; &#xA;&lt;p&gt;Currently, we are running the &lt;a href=&#34;https://github.com/open-thought/reasoning-gym/tree/main&#34;&gt;reasoning-gym&lt;/a&gt; swarm on the Testnet. This swarm is designed to train models to solve a diverse set of reasoning tasks using the reasoning-gym dataset. The current list of default models includes:&lt;/p&gt; &#xA;&lt;p&gt;Models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gensyn/Qwen2.5-0.5B-Instruct&lt;/li&gt; &#xA; &lt;li&gt;Qwen/Qwen3-0.6B&lt;/li&gt; &#xA; &lt;li&gt;nvidia/AceInstruct-1.5B&lt;/li&gt; &#xA; &lt;li&gt;dnotitia/Smoothie-Qwen3-1.7B&lt;/li&gt; &#xA; &lt;li&gt;Gensyn/Qwen2.5-1.5B-Instruct&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This iteration of rl-swarm is powered by the &lt;a href=&#34;https://github.com/gensyn-ai/genrl-swarm&#34;&gt;GenRL-Swarm&lt;/a&gt; library. It is a fully composable framework for decentralized reinforcement learning which enables users to create and customize their own swarms for reinforcement learning with multi-agent multi-stage environments.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Your hardware requirements will vary depending on a number of factors including model size and the accelerator platform you use. Users running large NVIDIA GPU will be assigned a model from the large model pool, while users running less powerful hardware will be assigned a model from the small model pool. This design decision is intended to allow users to advance at a similar rate regardless of the hardware they use, maximizing their utility to the swarm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported Hardware&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;arm64 or x86 CPU with minimum 32gb ram (note that if you run other applications during training it might crash training).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;OR&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA devices (officially supported): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RTX 3090&lt;/li&gt; &#xA;   &lt;li&gt;RTX 4090&lt;/li&gt; &#xA;   &lt;li&gt;RTX 5090&lt;/li&gt; &#xA;   &lt;li&gt;A100&lt;/li&gt; &#xA;   &lt;li&gt;H100&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With either configuration, you will need Python &amp;gt;=3.10 (for Mac, you will likely need to upgrade).&lt;/p&gt; &#xA;&lt;h2&gt;‚ö†Ô∏è Please read before continuing ‚ö†Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;This software is &lt;strong&gt;experimental&lt;/strong&gt; and provided as-is for users who are interested in using (or helping to develop) an early version of the Gensyn Protocol for training models.&lt;/p&gt; &#xA;&lt;p&gt;If you care about on-chain participation, you &lt;strong&gt;must&lt;/strong&gt; read the &lt;a href=&#34;https://raw.githubusercontent.com/gensyn-ai/rl-swarm/main/#identity-management&#34;&gt;Identity Management&lt;/a&gt; section below.&lt;/p&gt; &#xA;&lt;p&gt;If you encounter issues, please first check &lt;a href=&#34;https://raw.githubusercontent.com/gensyn-ai/rl-swarm/main/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;. If you cannot find a solution there, please check if there is an open (or closed) &lt;a href=&#34;https://raw.githubusercontent.com/gensyn-ai/issues&#34;&gt;Issue&lt;/a&gt;. If there is no relevant issue, please file one and include 1) all relevant &lt;a href=&#34;https://raw.githubusercontent.com/gensyn-ai/rl-swarm/main/#troubleshooting&#34;&gt;logs&lt;/a&gt;, 2) information about your device (e.g. which GPU, if relevant), and 3) your operating system information.&lt;/p&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;Run the Swarm&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to run RL Swarm is using Docker. This ensures a consistent setup across all operating systems with minimal dependencies.&lt;/p&gt; &#xA;&lt;h4&gt;1. Clone this repo&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/gensyn-ai/rl-swarm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Install Docker&lt;/h4&gt; &#xA;&lt;p&gt;Make sure you have Docker installed and the Docker daemon is running on your machine. To do that, follow &lt;a href=&#34;https://docs.docker.com/get-started/get-docker/&#34;&gt;these instructions&lt;/a&gt; according to your OS. Ensure you allot sufficient memory to the Docker containers. For example if using Docker Desktop, this can be done by going to Docker Desktop Settings &amp;gt; Resources &amp;gt; Advanced &amp;gt; Memory Limit, and increasing it to the maximum possible value.&lt;/p&gt; &#xA;&lt;h4&gt;3. Start the Swarm&lt;/h4&gt; &#xA;&lt;p&gt;Run the following commands from the root of the repository.&lt;/p&gt; &#xA;&lt;h5&gt;CPU support&lt;/h5&gt; &#xA;&lt;p&gt;If you‚Äôre using a Mac or if your machine has CPU-only support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker-compose run --rm --build -Pit swarm-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;GPU support&lt;/h5&gt; &#xA;&lt;p&gt;If you&#39;re using a machine with an officially supported GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker-compose run --rm --build -Pit swarm-gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Docker compose issue&lt;/h5&gt; &#xA;&lt;p&gt;If &lt;code&gt;docker-compose&lt;/code&gt; does not work when running the above commands, please try &lt;code&gt;docker compose&lt;/code&gt; (no hyphen) instead. I.e. &lt;code&gt; docker compose run --rm --build -Pit swarm-gpu&lt;/code&gt;. This issue sometimes occurs on users running Ubuntu.&lt;/p&gt; &#xA;&lt;h3&gt;Experimental (advanced) mode&lt;/h3&gt; &#xA;&lt;p&gt;If you want to experiment with the &lt;a href=&#34;https://github.com/gensyn-ai/genrl-swarm&#34;&gt;GenRL-Swarm&lt;/a&gt; library and its &lt;a href=&#34;https://github.com/gensyn-ai/genrl-swarm/raw/main/recipes/rgym/rg-swarm.yaml&#34;&gt;configurable parameters&lt;/a&gt;, we recommend you run RL Swarm via shell script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;./run_rl_swarm.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To learn more about experimental mode, check out our &lt;a href=&#34;https://github.com/gensyn-ai/genrl-swarm/raw/main/getting_started.ipynb&#34;&gt;getting started guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Login&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A browser window will pop open (you&#39;ll need to manually navigate to &lt;a href=&#34;http://localhost:3000/&#34;&gt;http://localhost:3000/&lt;/a&gt; if you&#39;re on a VM).&lt;/li&gt; &#xA; &lt;li&gt;Click &#39;login&#39;.&lt;/li&gt; &#xA; &lt;li&gt;Login with your preferred method.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Huggingface&lt;/h3&gt; &#xA;&lt;p&gt;If you would like to upload your model to Hugging Face, enter your Hugging Face access token when prompted. You can generate one from your Hugging Face account, under &lt;a href=&#34;https://huggingface.co/docs/hub/en/security-tokens&#34;&gt;Access Tokens&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Initial peering and training&lt;/h3&gt; &#xA;&lt;p&gt;From this stage onward your device will begin training. You should see your peer register and vote on-chain &lt;a href=&#34;https://gensyn-testnet.explorer.alchemy.com/address/0xFaD7C5e93f28257429569B854151A1B8DCD404c2?tab=logs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also track your training progress in real time:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On The RL-Swarm Dashboard: &lt;a href=&#34;https://dashboard.gensyn.ai&#34;&gt;dashboard.gensyn.ai&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Identity management&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction&lt;/h3&gt; &#xA;&lt;p&gt;On-chain identity is managed via an Alchemy modal sign-in screen. You need to supply an email address or login via a supported method (e.g. Google). This creates an EOA public/private key (which are stored by Alchemy). You will also receive local session keys in the &lt;code&gt;userApiKey&lt;/code&gt;. Note that these aren&#39;t your EOA public/private keys.&lt;/p&gt; &#xA;&lt;p&gt;During the initial set-up process, you will also create a &lt;code&gt;swarm.pem&lt;/code&gt; file which maintains the identity of your peer. This is then registered on chain using the EOA wallet hosted in Alchemy, triggered using your local api keys. This links the &lt;code&gt;swarm.pem&lt;/code&gt; to the &lt;code&gt;email address&lt;/code&gt; (and corresponding EOA in Alchemy).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you want to link multiple nodes to a single EOA&lt;/strong&gt;, simply sign up each node using the same email address. You will get a new peer ID for each node, however they will all be linked to the same EOA that your email is linked to.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;: if you are using a fork of this repo, or a service organised by someone else (e.g. a &#39;one click deployment&#39; provider) the identity management flow below is not guaranteed.&lt;/p&gt; &#xA;&lt;h3&gt;What this means&lt;/h3&gt; &#xA;&lt;p&gt;In the following two scenarios, everything will work (i.e. you will have an on-chain identity linked with your RL Swarm peer training):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The very first time you run the node from scratch with a new email address. The smart account will be created fresh and linked with the swarm.pem that is also fresh.&lt;/li&gt; &#xA; &lt;li&gt;If you run it again with a &lt;code&gt;swarm.pem&lt;/code&gt; AND login the original &lt;code&gt;email address&lt;/code&gt; used with that &lt;code&gt;swarm.pem&lt;/code&gt;. Note: this will throw an error into the log on registration but will still be able to sign transactions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In the following two scenarios, it will not work (i.e. you won&#39;t have an on-chain identity linked with your RL Swarm peer training):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you keep your &lt;code&gt;swarm.pem&lt;/code&gt; and try to link it to an &lt;code&gt;email address&lt;/code&gt; distinct from the one with which it was first registered.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Therefore, you should do these actions in the following scenarios&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Signed up with &lt;code&gt;email address&lt;/code&gt;, generated &lt;code&gt;swarm.pem&lt;/code&gt;, BUT lost &lt;code&gt;swarm.pem&lt;/code&gt;&lt;/strong&gt; OR &lt;strong&gt;You want to run multiple nodes at once&lt;/strong&gt;: run from scratch with the same email address and generate a new &lt;code&gt;swarm.pem&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Signed up with &lt;code&gt;email address&lt;/code&gt;, generated &lt;code&gt;swarm.pem&lt;/code&gt;, kept &lt;code&gt;swarm.pem&lt;/code&gt;&lt;/strong&gt; -&amp;gt; you can re-run a single node using this pair if you&#39;ve still got them both.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;How do I find my logs?&lt;/strong&gt; You can find them inside the &lt;code&gt;/logs&lt;/code&gt; directory:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;yarn.log&lt;/code&gt;: This file contains logs for the modal login server.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;swarm.log&lt;/code&gt;: This is the main log file for the RL Swarm application.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;wandb/&lt;/code&gt;: This directory contains various logs related to your training runs, including a &lt;code&gt;debug.log&lt;/code&gt; file. These can be updated to Weights &amp;amp; Biases (only available if you log_with wandb).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;My peer &#39;skipped a round&#39;&lt;/strong&gt;: this occurs when your device isn&#39;t fast enough to keep up with the pace of the swarm. For example, if you start training at round 100 and by the time you finish training the rest of the swarm reaches round 102, you will skip round 101 and go straight to 102. This is because your peer is more valuable if it is participating in the active round.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;My model doesn&#39;t seem to be training?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you&#39;re using a consumer device (e.g. a MacBook), it is likely just running slowly - check back in 20 minutes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Logging in with a new account after previous login?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure you click &#39;Logout&#39; on the login screen before you leave your previous session&lt;/li&gt; &#xA;   &lt;li&gt;Make sure you delete &lt;code&gt;swarm.pem&lt;/code&gt; from the root directory (try &lt;code&gt;sudo rm swarm.pem&lt;/code&gt;). If you don&#39;t do this, and you previously registered with the peer-id stored in this file, it will disrupt the training process.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Issues with the Login screen&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Upgrade viem&lt;/strong&gt;: some users report issues with the &lt;code&gt;viem&lt;/code&gt; package. There are two fixes: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;in the &lt;code&gt;modal-login/package.json&lt;/code&gt; update: &lt;code&gt;&#34;viem&#34;: &#34;2.25.0&#34;&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;in the terminal &lt;code&gt;cd /root/rl-swarm/modal-login/ &amp;amp;&amp;amp; yarn upgrade &amp;amp;&amp;amp; yarn add next@latest &amp;amp;&amp;amp; yarn add viem@latest&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I&#39;m getting lots of warnings&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This is expected behaviour and usually the output of the package managers or other dependencies. The most common is the below Protobuf warning - which can be ignored &lt;pre&gt;&lt;code&gt;WARNING: The candidate selected for download or install is a yanked version: &#39;protobuf&#39; candidate...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Issues on VMs/VPSs?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;How do I access the login screen if I&#39;m running in a VM?&lt;/strong&gt;: port forwarding. Add this SSH flag: &lt;code&gt;-L 3000:localhost:3000&lt;/code&gt; when connecting to your VM. E.g. &lt;code&gt;gcloud compute ssh --zone &#34;us-central1-a&#34; [your-vm] --project [your-project] -- -L 3000:localhost:3000&lt;/code&gt;. Note, some VPSs may not work with &lt;code&gt;rl-swarm&lt;/code&gt;. Check the Gensyn &lt;a href=&#34;https://discord.gg/AdnyWNzXh5&#34;&gt;discord&lt;/a&gt; for up-to-date information on this.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Disconnection/general issues&lt;/strong&gt;: If you are tunneling to a VM and suffer a broken pipe, you will likely encounter OOM or unexpected behaviour the first time you relaunch the script. If you &lt;code&gt;control + c&lt;/code&gt; and kill the script it should spin down all background processes. Restart the script and everything should work normally.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Issues with npm/general installation?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Try &lt;code&gt;npm install -g node@latest&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OOM errors on MacBook?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Try this (experimental) fix to increase memory: &lt;pre&gt;&lt;code&gt;export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I have a Windows machine, can I still train a model on the swarm?&lt;/strong&gt;: Yes - but this is not very well tested and may require you to do some debugging to get it set up properly. Install WSL and Linux on your Windows machine using the following instructions: &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/install&#34;&gt;https://learn.microsoft.com/en-us/windows/wsl/install&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I want to move my to a different machine and/or restart with a fresh build of the repo, but I want my animal name/peer id to persist.&lt;/strong&gt;: To achieve this simply backup the &lt;code&gt;swarm.pem&lt;/code&gt; file on your current machine and then put it in the corresponding location on your new machine/build of the repo.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I have multiple GPUs on one machine, can I run multiple peers?&lt;/strong&gt;: Yes - but you&#39;ll need to manually change things. You&#39;ll need to isolate each GPU, install this repo for each GPU, and expose each peer under a different port to pass the modal onboard.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;My round/stage is behind the smart contract/other peers?&lt;/strong&gt;: This is expected behaviour given the different speeds of machines in the network. Once your machine completes it&#39;s current round, it will move to the the current round.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I want to use a bigger and/or different model in the RL swarm, can I do that?&lt;/strong&gt;: Yes - but we only recommend doing so if you are comfortable understanding what size model can reasonably run on your hardware. If you elect to bring a custom model, just paste the repo/model name into the command line when prompted.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I am running a model in the swarm on my CPU, have received a python &lt;code&gt;RuntimeError&lt;/code&gt;, and my training progress seems to have stopped.&lt;/strong&gt;: There are several possible causes for this, but before trying anything please wait long enough to be sure your training actually is frozen and not just slow (e.g., wait longer than a single training iteration has previously taken on your machine). If you&#39;re sure training is actually frozen, then some things to try are:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set this (experimental) fix: &lt;code&gt;export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 &amp;amp;&amp;amp; ./run_rl_swarm.sh&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>HKUDS/AutoAgent</title>
    <updated>2025-06-29T01:43:24Z</updated>
    <id>tag:github.com,2025-06-29:/HKUDS/AutoAgent</id>
    <link href="https://github.com/HKUDS/AutoAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&#34;AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework&#34;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name=&#34;readme-top&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/AutoAgent_logo.svg?sanitize=true&#34; alt=&#34;Logo&#34; width=&#34;200&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;AutoAgent: Fully-Automated &amp;amp; Zero-Code&lt;br&gt; LLM Agent Framework &lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://autoagent-ai.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white&#34; alt=&#34;Credits&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://join.slack.com/t/metachain-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Join our Slack community&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://discord.gg/jQJdXyDB&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Join our Discord community&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/HKUDS/AutoAgent/raw/main/assets/autoagent-wechat.jpg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Wechat-Join%20Us-green?logo=wechat&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Join our Wechat community&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://autoagent-ai.github.io/docs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge&#34; alt=&#34;Check out the documentation&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2502.05957&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://gaia-benchmark-leaderboard.hf.space/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GAIA%20Benchmark-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge&#34; alt=&#34;Evaluation Benchmark Score&#34;&gt;&lt;/a&gt; &#xA; &lt;hr&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://trendshift.io/repositories/13954&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/13954&#34; alt=&#34;HKUDS%2FAutoAgent | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Welcome to AutoAgent! AutoAgent is a &lt;strong&gt;Fully-Automated&lt;/strong&gt; and highly &lt;strong&gt;Self-Developing&lt;/strong&gt; framework that enables users to create and deploy LLM agents through &lt;strong&gt;Natural Language Alone&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;‚ú®Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üèÜ Top Performers on the GAIA Benchmark &lt;br&gt;AutoAgent has secured top rankings among open-sourced methods, delivering comparable performance to &lt;strong&gt;OpenAI&#39;s Deep Research&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üìö Agentic-RAG with Native Self-Managing Vector Database &lt;br&gt;AutoAgent equipped with a native self-managing vector database, outperforms industry-leading solutions like &lt;strong&gt;LangChain&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚ú® Agent and Workflow Create with Ease &lt;br&gt;AutoAgent leverages natural language to effortlessly build ready-to-use &lt;strong&gt;tools&lt;/strong&gt;, &lt;strong&gt;agents&lt;/strong&gt; and &lt;strong&gt;workflows&lt;/strong&gt; - no coding required.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üåê Universal LLM Support &lt;br&gt;AutoAgent seamlessly integrates with &lt;strong&gt;A Wide Range&lt;/strong&gt; of LLMs (e.g., OpenAI, Anthropic, Deepseek, vLLM, Grok, Huggingface ...)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîÄ Flexible Interaction &lt;br&gt;Benefit from support for both &lt;strong&gt;function-calling&lt;/strong&gt; and &lt;strong&gt;ReAct&lt;/strong&gt; interaction modes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ü§ñ Dynamic, Extensible, Lightweight &lt;br&gt;AutoAgent is your &lt;strong&gt;Personal AI Assistant&lt;/strong&gt;, designed to be dynamic, extensible, customized, and lightweight.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üöÄ Unlock the Future of LLM Agents. Try üî•AutoAgentüî• Now!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;img src=&#34;./assets/AutoAgentnew-intro.pdf&#34; alt=&#34;Logo&#34; width=&#34;100%&#34;&gt; --&gt; &#xA; &lt;figure&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/autoagent-intro.svg?sanitize=true&#34; alt=&#34;Logo&#34; style=&#34;max-width: 100%; height: auto;&#34;&gt; &#xA;  &lt;figcaption&gt;&#xA;   &lt;em&gt;Quick Overview of AutoAgent.&lt;/em&gt;&#xA;  &lt;/figcaption&gt; &#xA; &lt;/figure&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üî• News&lt;/h2&gt; &#xA;&lt;div class=&#34;scrollable&#34;&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2025, Feb 17]&lt;/strong&gt;: &amp;nbsp;üéâüéâWe&#39;ve updated and released AutoAgent v0.2.0 (formerly known as MetaChain). Detailed changes include: 1) fix the bug of different LLM providers from issues; 2) add automatic installation of AutoAgent in the container environment according to issues; 3) add more easy-to-use commands for the CLI mode. 4) Rename the project to AutoAgent for better understanding.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2025, Feb 10]&lt;/strong&gt;: &amp;nbsp;üéâüéâWe&#39;ve released &lt;b&gt;MetaChain!&lt;/b&gt;, including framework, evaluation codes and CLI mode! Check our &lt;a href=&#34;https://arxiv.org/abs/2502.05957&#34;&gt;paper&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/div&gt; &#xA;&lt;span id=&#34;table-of-contents&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#features&#34;&gt;‚ú® Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#news&#34;&gt;üî• News&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#how-to-use&#34;&gt;üîç How to Use AutoAgent&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#user-mode&#34;&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA üèÜ Open Deep Research)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#agent-editor&#34;&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#workflow-editor&#34;&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#quick-start&#34;&gt;‚ö° Quick Start&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#api-keys-setup&#34;&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#start-with-cli-mode&#34;&gt;Start with CLI Mode&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#todo&#34;&gt;‚òëÔ∏è Todo List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#reproduce&#34;&gt;üî¨ How To Reproduce the Results in the Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#documentation&#34;&gt;üìñ Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#community&#34;&gt;ü§ù Join the Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#acknowledgements&#34;&gt;üôè Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#cite&#34;&gt;üåü Cite&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;span id=&#34;how-to-use&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;üîç How to Use AutoAgent&lt;/h2&gt; &#xA;&lt;span id=&#34;user-mode&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA üèÜ Open Deep Research)&lt;/h3&gt; &#xA;&lt;p&gt;AutoAgent have an out-of-the-box multi-agent system, which you could choose &lt;code&gt;user mode&lt;/code&gt; in the start page to use it. This multi-agent system is a general AI assistant, having the same functionality with &lt;strong&gt;OpenAI&#39;s Deep Research&lt;/strong&gt; and the comparable performance with it in &lt;a href=&#34;https://gaia-benchmark-leaderboard.hf.space/&#34;&gt;GAIA&lt;/a&gt; benchmark.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöÄ &lt;strong&gt;High Performance&lt;/strong&gt;: Matches Deep Research using Claude 3.5 rather than OpenAI&#39;s o3 model.&lt;/li&gt; &#xA; &lt;li&gt;üîÑ &lt;strong&gt;Model Flexibility&lt;/strong&gt;: Compatible with any LLM (including Deepseek-R1, Grok, Gemini, etc.)&lt;/li&gt; &#xA; &lt;li&gt;üí∞ &lt;strong&gt;Cost-Effective&lt;/strong&gt;: Open-source alternative to Deep Research&#39;s $200/month subscription&lt;/li&gt; &#xA; &lt;li&gt;üéØ &lt;strong&gt;User-Friendly&lt;/strong&gt;: Easy-to-deploy CLI interface for seamless interaction&lt;/li&gt; &#xA; &lt;li&gt;üìÅ &lt;strong&gt;File Support&lt;/strong&gt;: Handles file uploads for enhanced data interaction&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video width=&#34;80%&#34; controls&gt; &#xA;  &lt;source src=&#34;./assets/video_v1_compressed.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA; &lt;/video&gt; &#xA; &lt;p&gt;&lt;em&gt;üé• Deep Research (aka User Mode)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;span id=&#34;agent-editor&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/h3&gt; &#xA;&lt;p&gt;The most distinctive feature of AutoAgent is its natural language customization capability. Unlike other agent frameworks, AutoAgent allows you to create tools, agents, and workflows using natural language alone. Simply choose &lt;code&gt;agent editor&lt;/code&gt; or &lt;code&gt;workflow editor&lt;/code&gt; mode to start your journey of building agents through conversations.&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;agent editor&lt;/code&gt; as shown in the following figure.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/1-requirement.png&#34; alt=&#34;requirement&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;em&gt;Input what kind of agent you want to create.&lt;/em&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/2-profiling.png&#34; alt=&#34;profiling&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;em&gt;Automated agent profiling.&lt;/em&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/3-profiles.png&#34; alt=&#34;profiles&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;em&gt;Output the agent profiles.&lt;/em&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/4-tools.png&#34; alt=&#34;tools&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;em&gt;Create the desired tools.&lt;/em&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/5-task.png&#34; alt=&#34;task&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;em&gt;Input what do you want to complete with the agent. (Optional)&lt;/em&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/6-output-next.png&#34; alt=&#34;output&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;em&gt;Create the desired agent(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;span id=&#34;workflow-editor&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/h3&gt; &#xA;&lt;p&gt;You can also create the agent workflows using natural language description with the &lt;code&gt;workflow editor&lt;/code&gt; mode, as shown in the following figure. (Tips: this mode does not support tool creation temporarily.)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/1-requirement.png&#34; alt=&#34;requirement&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;em&gt;Input what kind of workflow you want to create.&lt;/em&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/2-profiling.png&#34; alt=&#34;profiling&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;em&gt;Automated workflow profiling.&lt;/em&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/3-profiles.png&#34; alt=&#34;profiles&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;em&gt;Output the workflow profiles.&lt;/em&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/4-task.png&#34; alt=&#34;task&#34; width=&#34;66%&#34;&gt; &lt;br&gt; &lt;em&gt;Input what do you want to complete with the workflow. (Optional)&lt;/em&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/5-output-next.png&#34; alt=&#34;output&#34; width=&#34;66%&#34;&gt; &lt;br&gt; &lt;em&gt;Create the desired workflow(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;span id=&#34;quick-start&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; &#xA;&lt;span id=&#34;installation&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h4&gt;AutoAgent Installation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/HKUDS/AutoAgent.git&#xA;cd AutoAgent&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Docker Installation&lt;/h4&gt; &#xA;&lt;p&gt;We use Docker to containerize the agent-interactive environment. So please install &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; first. You don&#39;t need to manually pull the pre-built image, because we have let Auto-Deep-Research &lt;strong&gt;automatically pull the pre-built image based on your architecture of your machine&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;span id=&#34;api-keys-setup&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;API Keys Setup&lt;/h3&gt; &#xA;&lt;p&gt;Create an environment variable file, just like &lt;code&gt;.env.template&lt;/code&gt;, and set the API keys for the LLMs you want to use. Not every LLM API Key is required, use what you need.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Required Github Tokens of your own&#xA;GITHUB_AI_TOKEN=&#xA;&#xA;# Optional API Keys&#xA;OPENAI_API_KEY=&#xA;DEEPSEEK_API_KEY=&#xA;ANTHROPIC_API_KEY=&#xA;GEMINI_API_KEY=&#xA;HUGGINGFACE_API_KEY=&#xA;GROQ_API_KEY=&#xA;XAI_API_KEY=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;span id=&#34;start-with-cli-mode&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;Start with CLI Mode&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[üö® &lt;strong&gt;News&lt;/strong&gt;: ] We have updated a more easy-to-use command to start the CLI mode and fix the bug of different LLM providers from issues. You can follow the following steps to start the CLI mode with different LLM providers with much less configuration.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Command Options:&lt;/h4&gt; &#xA;&lt;p&gt;You can run &lt;code&gt;auto main&lt;/code&gt; to start full part of AutoAgent, including &lt;code&gt;user mode&lt;/code&gt;, &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt;. Btw, you can also run &lt;code&gt;auto deep-research&lt;/code&gt; to start more lightweight &lt;code&gt;user mode&lt;/code&gt;, just like the &lt;a href=&#34;https://github.com/HKUDS/Auto-Deep-Research&#34;&gt;Auto-Deep-Research&lt;/a&gt; project. Some configuration of this command is shown below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--container_name&lt;/code&gt;: Name of the Docker container (default: &#39;deepresearch&#39;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;: Port for the container (default: 12346)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;COMPLETION_MODEL&lt;/code&gt;: Specify the LLM model to use, you should follow the name of &lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;Litellm&lt;/a&gt; to set the model name. (Default: &lt;code&gt;claude-3-5-sonnet-20241022&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DEBUG&lt;/code&gt;: Enable debug mode for detailed logs (default: False)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;API_BASE_URL&lt;/code&gt;: The base URL for the LLM provider (default: None)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;FN_CALL&lt;/code&gt;: Enable function calling (default: None). Most of time, you could ignore this option because we have already set the default value based on the model name.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git_clone&lt;/code&gt;: Clone the AutoAgent repository to the local environment (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: True)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;test_pull_name&lt;/code&gt;: The name of the test pull. (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: &#39;autoagent_mirror&#39;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;More details about &lt;code&gt;git_clone&lt;/code&gt; and &lt;code&gt;test_pull_name&lt;/code&gt;]&lt;/h4&gt; &#xA;&lt;p&gt;In the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, we should clone a mirror of the AutoAgent repository to the local agent-interactive environment and let our &lt;strong&gt;AutoAgent&lt;/strong&gt; automatically update the AutoAgent itself, such as creating new tools, agents and workflows. So if you want to use the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, you should set the &lt;code&gt;git_clone&lt;/code&gt; to True and set the &lt;code&gt;test_pull_name&lt;/code&gt; to &#39;autoagent_mirror&#39; or other branches.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;auto main&lt;/code&gt; with different LLM Providers&lt;/h4&gt; &#xA;&lt;p&gt;Then I will show you how to use the full part of AutoAgent with the &lt;code&gt;auto main&lt;/code&gt; command and different LLM providers. If you want to use the &lt;code&gt;auto deep-research&lt;/code&gt; command, you can refer to the &lt;a href=&#34;https://github.com/HKUDS/Auto-Deep-Research&#34;&gt;Auto-Deep-Research&lt;/a&gt; project for more details.&lt;/p&gt; &#xA;&lt;h5&gt;Anthropic&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ANTHROPIC_API_KEY=your_anthropic_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;auto main # default model is claude-3-5-sonnet-20241022&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;OpenAI&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=your_openai_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;COMPLETION_MODEL=gpt-4o auto main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Mistral&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MISTRAL_API_KEY=your_mistral_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;COMPLETION_MODEL=mistral/mistral-large-2407 auto main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Gemini - Google AI Studio&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;GEMINI_API_KEY=your_gemini_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;COMPLETION_MODEL=gemini/gemini-2.0-flash auto main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Huggingface&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;HUGGINGFACE_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;HUGGINGFACE_API_KEY=your_huggingface_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;COMPLETION_MODEL=huggingface/meta-llama/Llama-3.3-70B-Instruct auto main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Groq&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;GROQ_API_KEY=your_groq_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;COMPLETION_MODEL=groq/deepseek-r1-distill-llama-70b auto main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;OpenAI-Compatible Endpoints (e.g., Grok)&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=your_api_key_for_openai_compatible_endpoints&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;COMPLETION_MODEL=openai/grok-2-latest API_BASE_URL=https://api.x.ai/v1 auto main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;OpenRouter (e.g., DeepSeek-R1)&lt;/h5&gt; &#xA;&lt;p&gt;We recommend using OpenRouter as LLM provider of DeepSeek-R1 temporarily. Because official API of DeepSeek-R1 can not be used efficiently.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENROUTER_API_KEY=your_openrouter_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;COMPLETION_MODEL=openrouter/deepseek/deepseek-r1 auto main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;DeepSeek&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DEEPSEEK_API_KEY=your_deepseek_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;COMPLETION_MODEL=deepseek/deepseek-chat auto main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the CLI mode is started, you can see the start page of AutoAgent:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;img src=&#34;./assets/AutoAgentnew-intro.pdf&#34; alt=&#34;Logo&#34; width=&#34;100%&#34;&gt; --&gt; &#xA; &lt;figure&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/cover.png&#34; alt=&#34;Logo&#34; style=&#34;max-width: 100%; height: auto;&#34;&gt; &#xA;  &lt;figcaption&gt;&#xA;   &lt;em&gt;Start Page of AutoAgent.&lt;/em&gt;&#xA;  &lt;/figcaption&gt; &#xA; &lt;/figure&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Tips&lt;/h3&gt; &#xA;&lt;h4&gt;Import browser cookies to browser environment&lt;/h4&gt; &#xA;&lt;p&gt;You can import the browser cookies to the browser environment to let the agent better access some specific websites. For more details, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/AutoAgent/environment/cookie_json/README.md&#34;&gt;cookies&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h4&gt;Add your own API keys for third-party Tool Platforms&lt;/h4&gt; &#xA;&lt;p&gt;If you want to create tools from the third-party tool platforms, such as RapidAPI, you should subscribe tools from the platform and add your own API keys by running &lt;a href=&#34;https://raw.githubusercontent.com/HKUDS/AutoAgent/main/process_tool_docs.py&#34;&gt;process_tool_docs.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python process_tool_docs.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More features coming soon! üöÄ &lt;strong&gt;Web GUI interface&lt;/strong&gt; under development.&lt;/p&gt; &#xA;&lt;span id=&#34;todo&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;‚òëÔ∏è Todo List&lt;/h2&gt; &#xA;&lt;p&gt;AutoAgent is continuously evolving! Here&#39;s what&#39;s coming:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìä &lt;strong&gt;More Benchmarks&lt;/strong&gt;: Expanding evaluations to &lt;strong&gt;SWE-bench&lt;/strong&gt;, &lt;strong&gt;WebArena&lt;/strong&gt;, and more&lt;/li&gt; &#xA; &lt;li&gt;üñ•Ô∏è &lt;strong&gt;GUI Agent&lt;/strong&gt;: Supporting &lt;em&gt;Computer-Use&lt;/em&gt; agents with GUI interaction&lt;/li&gt; &#xA; &lt;li&gt;üîß &lt;strong&gt;Tool Platforms&lt;/strong&gt;: Integration with more platforms like &lt;strong&gt;Composio&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;üèóÔ∏è &lt;strong&gt;Code Sandboxes&lt;/strong&gt;: Supporting additional environments like &lt;strong&gt;E2B&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;üé® &lt;strong&gt;Web Interface&lt;/strong&gt;: Developing comprehensive GUI for better user experience&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Have ideas or suggestions? Feel free to open an issue! Stay tuned for more exciting updates! üöÄ&lt;/p&gt; &#xA;&lt;span id=&#34;reproduce&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;üî¨ How To Reproduce the Results in the Paper&lt;/h2&gt; &#xA;&lt;h3&gt;GAIA Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;For the GAIA benchmark, you can run the following command to run the inference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/gaia/scripts/run_infer.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the evaluation, you can run the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; python evaluation/gaia/get_score.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Agentic-RAG&lt;/h3&gt; &#xA;&lt;p&gt;For the Agentic-RAG task, you can run the following command to run the inference.&lt;/p&gt; &#xA;&lt;p&gt;Step1. Turn to &lt;a href=&#34;https://huggingface.co/datasets/yixuantt/MultiHopRAG&#34;&gt;this page&lt;/a&gt; and download it. Save them to your datapath.&lt;/p&gt; &#xA;&lt;p&gt;Step2. Run the following command to run the inference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/multihoprag/scripts/run_rag.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step3. The result will be saved in the &lt;code&gt;evaluation/multihoprag/result.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;span id=&#34;documentation&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;A more detailed documentation is coming soon üöÄ, and we will update in the &lt;a href=&#34;https://AutoAgent-ai.github.io/docs&#34;&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;span id=&#34;community&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;ü§ù Join the Community&lt;/h2&gt; &#xA;&lt;p&gt;We want to build a community for AutoAgent, and we welcome everyone to join us. You can join our community by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/AutoAgent-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ&#34;&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/z68KRvwB&#34;&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HKUDS/AutoAgent/issues&#34;&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we&#39;re working on, or add your own ideas.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;span id=&#34;acknowledgements&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;Misc&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/HKUDS/AutoAgent/stargazers&#34;&gt;&lt;img src=&#34;https://reporoster.com/stars/HKUDS/AutoAgent&#34; alt=&#34;Stargazers repo roster for @HKUDS/AutoAgent&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/HKUDS/AutoAgent/network/members&#34;&gt;&lt;img src=&#34;https://reporoster.com/forks/HKUDS/AutoAgent&#34; alt=&#34;Forkers repo roster for @HKUDS/AutoAgent&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://star-history.com/#HKUDS/AutoAgent&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=HKUDS/AutoAgent&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üôè Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Rome wasn&#39;t built in a day. AutoAgent stands on the shoulders of giants, and we are deeply grateful for the outstanding work that came before us. Our framework architecture draws inspiration from &lt;a href=&#34;https://github.com/openai/swarm&#34;&gt;OpenAI Swarm&lt;/a&gt;, while our user mode&#39;s three-agent design benefits from &lt;a href=&#34;https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one&#34;&gt;Magentic-one&lt;/a&gt;&#39;s insights. We&#39;ve also learned from &lt;a href=&#34;https://github.com/All-Hands-AI/OpenHands&#34;&gt;OpenHands&lt;/a&gt; for documentation structure and many other excellent projects for agent-environment interaction design, among others. We express our sincere gratitude and respect to all these pioneering works that have been instrumental in shaping AutoAgent.&lt;/p&gt; &#xA;&lt;span id=&#34;cite&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;üåü Cite&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@misc{AutoAgent,&#xA;      title={{AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents}},&#xA;      author={Jiabin Tang, Tianyu Fan, Chao Huang},&#xA;      year={2025},&#xA;      eprint={202502.05957},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI},&#xA;      url={https://arxiv.org/abs/2502.05957},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>