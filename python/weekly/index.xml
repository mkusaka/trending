<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-30T01:44:18Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>infiniflow/ragflow</title>
    <updated>2024-06-30T01:44:18Z</updated>
    <id>tag:github.com,2024-06-30:/infiniflow/ragflow</id>
    <link href="https://github.com/infiniflow/ragflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://demo.ragflow.io/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/web/src/assets/logo-with-text.png&#34; width=&#34;520&#34; alt=&#34;ragflow logo&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README_zh.md&#34;&gt;简体中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ja.md&#34;&gt;日本語&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/infiniflow/ragflow/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;amp;label=Latest%20Release&#34; alt=&#34;Latest Release&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://demo.ragflow.io&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/Online-Demo-4e6b99&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/infiniflow/ragflow&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/docker_pull-ragflow:v0.7.0-brightgreen&#34; alt=&#34;docker pull infiniflow/ragflow:v0.7.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/infiniflow/ragflow/raw/main/LICENSE&#34;&gt; &lt;img height=&#34;21&#34; src=&#34;https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;amp;color=2e6cc4&#34; alt=&#34;license&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://ragflow.io/docs/dev/&#34;&gt;Document&lt;/a&gt; | &lt;a href=&#34;https://github.com/infiniflow/ragflow/issues/162&#34;&gt;Roadmap&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/infiniflowai&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/4XxujFgUN7&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://demo.ragflow.io&#34;&gt;Demo&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;📕 Table of Contents&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;💡 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-what-is-ragflow&#34;&gt;What is RAGFlow?&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;🎮 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;📌 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-latest-updates&#34;&gt;Latest Updates&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;🌟 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-key-features&#34;&gt;Key Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;🔎 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-system-architecture&#34;&gt;System Architecture&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;🎬 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-get-started&#34;&gt;Get Started&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;🔧 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-configurations&#34;&gt;Configurations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;🛠️ &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-build-from-source&#34;&gt;Build from source&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;🛠️ &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-launch-service-from-source&#34;&gt;Launch service from source&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;📚 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;📜 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;🏄 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;🙌 &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/#-contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;💡 What is RAGFlow?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ragflow.io/&#34;&gt;RAGFlow&lt;/a&gt; is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted data.&lt;/p&gt; &#xA;&lt;h2&gt;🎮 Demo&lt;/h2&gt; &#xA;&lt;p&gt;Try our demo at &lt;a href=&#34;https://demo.ragflow.io&#34;&gt;https://demo.ragflow.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top:20px;margin-bottom:20px;&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/infiniflow/ragflow/assets/7248/2f6baa3e-1092-4f11-866d-36f6a9d075e5&#34; width=&#34;1200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;📌 Latest Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-06-27 Supports Markdown and Docx in the Q&amp;amp;A parsing method. Supports extracting images from Docx files. Supports extracting tables from Markdown files.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-06-14 Supports PDF in the Q&amp;amp;A parsing method.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-06-06 Supports &lt;a href=&#34;https://huggingface.co/papers/2310.11511&#34;&gt;Self-RAG&lt;/a&gt;, which is enabled by default in dialog settings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-05-30 Integrates &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCE&lt;/a&gt; and &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding&#34;&gt;BGE&lt;/a&gt; reranker models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-05-28 Supports LLM Baichuan and VolcanoArk.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-05-23 Supports &lt;a href=&#34;https://arxiv.org/html/2401.18059v1&#34;&gt;RAPTOR&lt;/a&gt; for better text retrieval.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-05-21 Supports streaming output and text chunk retrieval API.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-05-15 Integrates OpenAI GPT-4o.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2024-05-08 Integrates LLM DeepSeek-V2.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🌟 Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;🍭 &lt;strong&gt;&#34;Quality in, quality out&#34;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/deepdoc/README.md&#34;&gt;Deep document understanding&lt;/a&gt;-based knowledge extraction from unstructured data with complicated formats.&lt;/li&gt; &#xA; &lt;li&gt;Finds &#34;needle in a data haystack&#34; of literally unlimited tokens.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🍱 &lt;strong&gt;Template-based chunking&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intelligent and explainable.&lt;/li&gt; &#xA; &lt;li&gt;Plenty of template options to choose from.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🌱 &lt;strong&gt;Grounded citations with reduced hallucinations&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visualization of text chunking to allow human intervention.&lt;/li&gt; &#xA; &lt;li&gt;Quick view of the key references and traceable citations to support grounded answers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🍔 &lt;strong&gt;Compatibility with heterogeneous data sources&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🛀 &lt;strong&gt;Automated and effortless RAG workflow&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streamlined RAG orchestration catered to both personal and large businesses.&lt;/li&gt; &#xA; &lt;li&gt;Configurable LLMs as well as embedding models.&lt;/li&gt; &#xA; &lt;li&gt;Multiple recall paired with fused re-ranking.&lt;/li&gt; &#xA; &lt;li&gt;Intuitive APIs for seamless integration with business.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔎 System Architecture&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top:20px;margin-bottom:20px;&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/infiniflow/ragflow/assets/12318111/d6ac5664-c237-4200-a7c2-a4a00691b485&#34; width=&#34;1000&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🎬 Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;📝 Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CPU &amp;gt;= 4 cores&lt;/li&gt; &#xA; &lt;li&gt;RAM &amp;gt;= 16 GB&lt;/li&gt; &#xA; &lt;li&gt;Disk &amp;gt;= 50 GB&lt;/li&gt; &#xA; &lt;li&gt;Docker &amp;gt;= 24.0.0 &amp;amp; Docker Compose &amp;gt;= v2.26.1 &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;If you have not installed Docker on your local machine (Windows, Mac, or Linux), see &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Install Docker Engine&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🚀 Start up the server&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;vm.max_map_count&lt;/code&gt; &amp;gt;= 262144:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;To check the value of &lt;code&gt;vm.max_map_count&lt;/code&gt;:&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sysctl vm.max_map_count&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;Reset &lt;code&gt;vm.max_map_count&lt;/code&gt; to a value at least 262144 if it is not.&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# In this case, we set it to 262144:&#xA;$ sudo sysctl -w vm.max_map_count=262144&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;This change will be reset after a system reboot. To ensure your change remains permanent, add or update the &lt;code&gt;vm.max_map_count&lt;/code&gt; value in &lt;strong&gt;/etc/sysctl.conf&lt;/strong&gt; accordingly:&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vm.max_map_count=262144&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/infiniflow/ragflow.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the pre-built Docker images and start up the server:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Running the following commands automatically downloads the &lt;em&gt;dev&lt;/em&gt; version RAGFlow Docker image. To download and run a specified Docker version, update &lt;code&gt;RAGFLOW_VERSION&lt;/code&gt; in &lt;strong&gt;docker/.env&lt;/strong&gt; to the intended version, for example &lt;code&gt;RAGFLOW_VERSION=v0.7.0&lt;/code&gt;, before running the following commands.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd ragflow/docker&#xA;$ chmod +x ./entrypoint.sh&#xA;$ docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;The core image is about 9 GB in size and may take a while to load.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check the server status after having the server up and running:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker logs -f ragflow-server&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    ____                 ______ __&#xA;   / __ \ ____ _ ____ _ / ____// /____  _      __&#xA;  / /_/ // __ `// __ `// /_   / // __ \| | /| / /&#xA; / _, _// /_/ // /_/ // __/  / // /_/ /| |/ |/ /&#xA;/_/ |_| \__,_/ \__, //_/    /_/ \____/ |__/|__/&#xA;              /____/&#xA;&#xA; * Running on all addresses (0.0.0.0)&#xA; * Running on http://127.0.0.1:9380&#xA; * Running on http://x.x.x.x:9380&#xA; INFO:werkzeug:Press CTRL+C to quit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a &lt;code&gt;network anomaly&lt;/code&gt; error because, at that moment, your RAGFlow may not be fully initialized.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In your web browser, enter the IP address of your server and log in to RAGFlow.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;With the default settings, you only need to enter &lt;code&gt;http://IP_OF_YOUR_MACHINE&lt;/code&gt; (&lt;strong&gt;sans&lt;/strong&gt; port number) as the default HTTP serving port &lt;code&gt;80&lt;/code&gt; can be omitted when using the default configurations.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt;, select the desired LLM factory in &lt;code&gt;user_default_llm&lt;/code&gt; and update the &lt;code&gt;API_KEY&lt;/code&gt; field with the corresponding API key.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;See &lt;a href=&#34;https://ragflow.io/docs/dev/llm_api_key_setup&#34;&gt;llm_api_key_setup&lt;/a&gt; for more information.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;The show is now on!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🔧 Configurations&lt;/h2&gt; &#xA;&lt;p&gt;When it comes to system configurations, you will need to manage the following files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env&#34;&gt;.env&lt;/a&gt;: Keeps the fundamental setups for the system, such as &lt;code&gt;SVR_HTTP_PORT&lt;/code&gt;, &lt;code&gt;MYSQL_PASSWORD&lt;/code&gt;, and &lt;code&gt;MINIO_PASSWORD&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt;: Configures the back-end services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt;: The system relies on &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt; to start up.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You must ensure that changes to the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env&#34;&gt;.env&lt;/a&gt; file are in line with what are in the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md&#34;&gt;./docker/README&lt;/a&gt; file provides a detailed description of the environment settings and service configurations, and you are REQUIRED to ensure that all environment settings listed in the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md&#34;&gt;./docker/README&lt;/a&gt; file are aligned with the corresponding configurations in the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To update the default HTTP serving port (80), go to &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt; and change &lt;code&gt;80:80&lt;/code&gt; to &lt;code&gt;&amp;lt;YOUR_SERVING_PORT&amp;gt;:80&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Updates to all system configurations require a system reboot to take effect:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;🛠️ Build from source&lt;/h2&gt; &#xA;&lt;p&gt;To build the Docker images from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/infiniflow/ragflow.git&#xA;$ cd ragflow/&#xA;$ docker build -t infiniflow/ragflow:dev .&#xA;$ cd ragflow/docker&#xA;$ chmod +x ./entrypoint.sh&#xA;$ docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🛠️ Launch service from source&lt;/h2&gt; &#xA;&lt;p&gt;To launch the service from source:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/infiniflow/ragflow.git&#xA;$ cd ragflow/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a virtual environment, ensuring that Anaconda or Miniconda is installed:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda create -n ragflow python=3.11.0&#xA;$ conda activate ragflow&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# If your CUDA version is higher than 12.0, run the following additional commands:&#xA;$ pip uninstall -y onnxruntime-gpu&#xA;$ pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy the entry script and configure environment variables:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get the Python path:&#xA;$ which python&#xA;# Get the ragflow project path:&#xA;$ pwd&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp docker/entrypoint.sh .&#xA;$ vi entrypoint.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Adjust configurations according to your actual situation (the following two export commands are newly added):&#xA;# - Assign the result of `which python` to `PY`.&#xA;# - Assign the result of `pwd` to `PYTHONPATH`.&#xA;# - Comment out `LD_LIBRARY_PATH`, if it is configured.&#xA;# - Optional: Add Hugging Face mirror.&#xA;PY=${PY}&#xA;export PYTHONPATH=${PYTHONPATH}&#xA;export HF_ENDPOINT=https://hf-mirror.com&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch the third-party services (MinIO, Elasticsearch, Redis, and MySQL):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd docker&#xA;$ docker compose -f docker-compose-base.yml up -d &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check the configuration files, ensuring that:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The settings in &lt;strong&gt;docker/.env&lt;/strong&gt; match those in &lt;strong&gt;conf/service_conf.yaml&lt;/strong&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The IP addresses and ports for related services in &lt;strong&gt;service_conf.yaml&lt;/strong&gt; match the local machine IP and ports exposed by the container.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch the RAGFlow backend service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod +x ./entrypoint.sh&#xA;$ bash ./entrypoint.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch the frontend service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd web&#xA;$ npm install --registry=https://registry.npmmirror.com --force&#xA;$ vim .umirc.ts&#xA;# Update proxy.target to http://127.0.0.1:9380&#xA;$ npm run dev &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy the frontend service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd web&#xA;$ npm install --registry=https://registry.npmmirror.com --force&#xA;$ umi build&#xA;$ mkdir -p /ragflow/web&#xA;$ cp -r dist /ragflow/web&#xA;$ apt install nginx -y&#xA;$ cp ../docker/nginx/proxy.conf /etc/nginx&#xA;$ cp ../docker/nginx/nginx.conf /etc/nginx&#xA;$ cp ../docker/nginx/ragflow.conf /etc/nginx/conf.d&#xA;$ systemctl start nginx&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;📚 Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragflow.io/docs/dev/&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragflow.io/docs/dev/category/user-guides&#34;&gt;User guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragflow.io/docs/dev/category/references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragflow.io/docs/dev/faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📜 Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/infiniflow/ragflow/issues/162&#34;&gt;RAGFlow Roadmap 2024&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🏄 Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/4XxujFgUN7&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/infiniflowai&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/orgs/infiniflow/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🙌 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docs/references/CONTRIBUTING.md&#34;&gt;Contribution Guidelines&lt;/a&gt; first.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Doriandarko/maestro</title>
    <updated>2024-06-30T01:44:18Z</updated>
    <id>tag:github.com,2024-06-30:/Doriandarko/maestro</id>
    <link href="https://github.com/Doriandarko/maestro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A framework for Claude Opus to intelligently orchestrate subagents.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Maestro - A Framework for Claude Opus, GPT and local LLMs to Orchestrate Subagents&lt;/h1&gt; &#xA;&lt;p&gt;This Python script demonstrates an AI-assisted task breakdown and execution workflow using the Anthropic API. It utilizes two AI models, Opus and Haiku, to break down an objective into sub-tasks, execute each sub-task, and refine the results into a cohesive final output.&lt;/p&gt; &#xA;&lt;h2&gt;New:&lt;/h2&gt; &#xA;&lt;h1&gt;Updated the original Maestro to support Claude 3.5 Sonnet&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Use Maestro with any APIs, Anthropic, Gemini, OpenAI, Cohere, etc.&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to a rewrite of the codebase using LiteLLM, it&#39;s now much easier to select the model you want.&lt;/p&gt; &#xA;&lt;p&gt;Simply&lt;/p&gt; &#xA;&lt;h4&gt;Set environment variables for API keys for the services you are using&lt;/h4&gt; &#xA;&lt;p&gt;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;YOUR KEY&#34;&lt;/p&gt; &#xA;&lt;p&gt;os.environ[&#34;ANTHROPIC_API_KEY&#34;] = &#34;YOUR KEY&#34;&lt;/p&gt; &#xA;&lt;p&gt;os.environ[&#34;GEMINI_API_KEY&#34;] = &#34;YOUR KEY&#34;&lt;/p&gt; &#xA;&lt;h4&gt;Define the models to be used for each stage&lt;/h4&gt; &#xA;&lt;p&gt;ORCHESTRATOR_MODEL = &#34;gemini/gemini-1.5-flash-latest&#34;&lt;/p&gt; &#xA;&lt;p&gt;SUB_AGENT_MODEL = &#34;gemini/gemini-1.5-flash-latest&#34;&lt;/p&gt; &#xA;&lt;p&gt;REFINER_MODEL = &#34;gemini/gemini-1.5-flash-latest&#34;&lt;/p&gt; &#xA;&lt;p&gt;Or gpt-3.5-turbo, etc.&lt;/p&gt; &#xA;&lt;p&gt;First install litellm&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install litellm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Afeter installing dependecies run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro-anyapi.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GPT-4o&lt;/h2&gt; &#xA;&lt;p&gt;The GPT script has been updated from the ground up to support the code capabilities of GPT-4o&lt;/p&gt; &#xA;&lt;p&gt;Afeter installing dependecies run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro-gpt4o.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run locally with LMStudio or Ollama&lt;/h2&gt; &#xA;&lt;h3&gt;Lmstudio&lt;/h3&gt; &#xA;&lt;p&gt;First download the app here &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;https://lmstudio.ai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then run the local server using your preferred method. I also recommend removing any system prompt for the app (leave your prompt field empty so it can take advantage of the script prompts).&lt;/p&gt; &#xA;&lt;p&gt;Then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro-lmstudio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Ollama&lt;/h3&gt; &#xA;&lt;p&gt;Mestro now runs locally thanks to the Ollama platform. Experience the power of Llama 3 locally!&lt;/p&gt; &#xA;&lt;p&gt;Before running the script&lt;/p&gt; &#xA;&lt;p&gt;Install Ollama client from here &lt;a href=&#34;https://ollama.com/download&#34;&gt;https://ollama.com/download&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ollama.pull(&#39;llama3:70b&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will depend on the model you want to use it, you only need to do it once or if you want to update the model when a new version it&#39;s out. In the script I am using both versions but you can customize the model you want to use&lt;/p&gt; &#xA;&lt;p&gt;ollama.pull(&#39;llama3:70b&#39;) ollama.pull(&#39;llama3:8b&#39;)&lt;/p&gt; &#xA;&lt;p&gt;Then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro-ollama.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Highly requested features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GROQ SUPPORT Experience the power of maestro thanks to Groq super fast api responses.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install groq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro-groq.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SEARCH 🔍&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Now, when it&#39;s creating a task for its subagent, Claude Opus will perform a search and get the best answer to help the subagent solve that task even better.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you replace your Tavil API for search to work&lt;/p&gt; &#xA;&lt;p&gt;Get one here &lt;a href=&#34;https://tavily.com/&#34;&gt;https://tavily.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT4 SUPPORT&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Add support for GPT-4 as an orchestrator in maestro-gpt.py Simply&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro-gpt.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After you complete your installs.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Breaks down an objective into manageable sub-tasks using the Opus model&lt;/li&gt; &#xA; &lt;li&gt;Executes each sub-task using the Haiku model&lt;/li&gt; &#xA; &lt;li&gt;Provides the Haiku model with memory of previous sub-tasks for context&lt;/li&gt; &#xA; &lt;li&gt;Refines the sub-task results into a final output using the Opus model&lt;/li&gt; &#xA; &lt;li&gt;Generates a detailed exchange log capturing the entire task breakdown and execution process&lt;/li&gt; &#xA; &lt;li&gt;Saves the exchange log to a Markdown file for easy reference&lt;/li&gt; &#xA; &lt;li&gt;Utilizes an improved prompt for the Opus model to better assess task completion&lt;/li&gt; &#xA; &lt;li&gt;Creates code files and folders when working on code projects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;To run this script, you need to have the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python installed&lt;/li&gt; &#xA; &lt;li&gt;Anthropic API key&lt;/li&gt; &#xA; &lt;li&gt;Required Python packages: &lt;code&gt;anthropic&lt;/code&gt; and &lt;code&gt;rich&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository or download the script file.&lt;/li&gt; &#xA; &lt;li&gt;Install the required Python packages by running the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Replace the placeholder API key in the script with your actual Anthropic API key:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client = Anthropic(api_key=&#34;YOUR_API_KEY_HERE&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If using search, replace your Tavil API&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tavily = TavilyClient(api_key=&#34;YOUR API KEY HERE&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open a terminal or command prompt and navigate to the directory containing the script.&lt;/li&gt; &#xA; &lt;li&gt;Run the script using the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Enter your objective when prompted:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Please enter your objective: Your objective here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script will start the task breakdown and execution process. It will display the progress and results in the console using formatted panels.&lt;/p&gt; &#xA;&lt;p&gt;Once the process is complete, the script will display the refined final output and save the full exchange log to a Markdown file with a filename based on the objective.&lt;/p&gt; &#xA;&lt;h2&gt;Code Structure&lt;/h2&gt; &#xA;&lt;p&gt;The script consists of the following main functions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;opus_orchestrator(objective, previous_results=None)&lt;/code&gt;: Calls the Opus model to break down the objective into sub-tasks or provide the final output. It uses an improved prompt to assess task completion and includes the phrase &#34;The task is complete:&#34; when the objective is fully achieved.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;haiku_sub_agent(prompt, previous_haiku_tasks=None)&lt;/code&gt;: Calls the Haiku model to execute a sub-task prompt, providing it with the memory of previous sub-tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;opus_refine(objective, sub_task_results)&lt;/code&gt;: Calls the Opus model to review and refine the sub-task results into a cohesive final output.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The script follows an iterative process, repeatedly calling the opus_orchestrator function to break down the objective into sub-tasks until the final output is provided. Each sub-task is then executed by the haiku_sub_agent function, and the results are stored in the task_exchanges and haiku_tasks lists.&lt;/p&gt; &#xA;&lt;p&gt;The loop terminates when the Opus model includes the phrase &#34;The task is complete:&#34; in its response, indicating that the objective has been fully achieved.&lt;/p&gt; &#xA;&lt;p&gt;Finally, the opus_refine function is called to review and refine the sub-task results into a final output. The entire exchange log, including the objective, task breakdown, and refined final output, is saved to a Markdown file.&lt;/p&gt; &#xA;&lt;h2&gt;Customization&lt;/h2&gt; &#xA;&lt;p&gt;You can customize the script according to your needs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adjust the max_tokens parameter in the client.messages.create() function calls to control the maximum number of tokens generated by the AI models.&lt;/li&gt; &#xA; &lt;li&gt;Change the models to what you prefer, like replacing Haiku with Sonnet or Opus.&lt;/li&gt; &#xA; &lt;li&gt;Modify the console output formatting by updating the rich library&#39;s Panel and Console configurations.&lt;/li&gt; &#xA; &lt;li&gt;Customize the exchange log formatting and file extension by modifying the relevant code sections.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This script is released under the MIT License.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anthropic for providing the AI models and API.&lt;/li&gt; &#xA; &lt;li&gt;Rich for the beautiful console formatting.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Flask App Integration&lt;/h2&gt; &#xA;&lt;p&gt;We have now integrated a Flask app to provide a user-friendly interface for interacting with the Maestro framework. This addition allows users to input objectives and view results through a web interface, enhancing the overall usability of the tool.&lt;/p&gt; &#xA;&lt;h3&gt;Setting Up and Running the Flask App&lt;/h3&gt; &#xA;&lt;p&gt;To set up and run the Flask app, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure Flask is installed by running &lt;code&gt;pip install Flask&lt;/code&gt; or by adding Flask to the &lt;code&gt;requirements.txt&lt;/code&gt; file and running &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the directory containing the Flask app files (&lt;code&gt;app.py&lt;/code&gt;, &lt;code&gt;templates/&lt;/code&gt;, and &lt;code&gt;static/&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Run the Flask app by executing &lt;code&gt;python app.py&lt;/code&gt; in your terminal or command prompt.&lt;/li&gt; &#xA; &lt;li&gt;Access the web interface by opening a web browser and navigating to &lt;code&gt;http://localhost:5000/&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The Flask app supports all features of the Maestro framework, allowing users to input objectives and view the orchestrated task breakdown and execution results in a structured and easy-to-read format.&lt;/p&gt; &#xA;&lt;h3&gt;UI Features&lt;/h3&gt; &#xA;&lt;p&gt;The Flask app includes the following UI features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A form for inputting objectives.&lt;/li&gt; &#xA; &lt;li&gt;A results display area where the orchestrated task breakdown and execution results are shown.&lt;/li&gt; &#xA; &lt;li&gt;Basic styling for improved readability and user experience.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This integration aims to make the Maestro framework more accessible and user-friendly, providing an intuitive way for users to leverage the power of AI-assisted task breakdown and execution.&lt;/p&gt; &#xA;&lt;h3&gt;Updated Instructions for Running the Flask App&lt;/h3&gt; &#xA;&lt;p&gt;To run the Flask app with the updated file structure, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to the &lt;code&gt;flask_app&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;python app.py&lt;/code&gt; to start the Flask server.&lt;/li&gt; &#xA; &lt;li&gt;Access the web interface by navigating to &lt;code&gt;http://localhost:5000/&lt;/code&gt; in your web browser.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This update ensures that all Flask app-related files are neatly organized within the &lt;code&gt;flask_app&lt;/code&gt; folder, simplifying the project structure and making it easier to manage.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hpcaitech/Open-Sora</title>
    <updated>2024-06-30T01:44:18Z</updated>
    <id>tag:github.com,2024-06-30:/hpcaitech/Open-Sora</id>
    <link href="https://github.com/hpcaitech/Open-Sora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-Sora: Democratizing Efficient Video Production for All&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/icon.png&#34; width=&#34;250&#34;&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://hpcaitech.github.io/Open-Sora/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gallery-View-orange?logo=&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://discord.gg/kZakZzrSUT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-247ipg9fk-KRRYmUl~u2ll2637WRURVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-ColossalAI-blueviolet?logo=slack&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/yangyou1991/status/1769411544083996787?s=61&amp;amp;t=jT0Dsx2d-MS5vS9rNM5e5g&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitter-Discuss-blue?logo=twitter&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/微信-小助手加群-green?logo=wechat&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://hpc-ai.com/blog/open-sora-v1.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Open_Sora-Blog-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/spaces/hpcai-tech/open-sora&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Gradio Demo-blue&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Open-Sora: Democratizing Efficient Video Production for All&lt;/h2&gt; &#xA;&lt;p&gt;We design and implement &lt;strong&gt;Open-Sora&lt;/strong&gt;, an initiative dedicated to &lt;strong&gt;efficiently&lt;/strong&gt; producing high-quality video. We hope to make the model, tools and all details accessible to all. By embracing &lt;strong&gt;open-source&lt;/strong&gt; principles, Open-Sora not only democratizes access to advanced video generation techniques, but also offers a streamlined and user-friendly platform that simplifies the complexities of video generation. With Open-Sora, our goal is to foster innovation, creativity, and inclusivity within the field of content creation.&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/zh_CN/README.md&#34;&gt;中文文档&lt;/a&gt;] [&lt;a href=&#34;https://cloud.luchentech.com/&#34;&gt;潞晨云&lt;/a&gt;|&lt;a href=&#34;https://cloud.luchentech.com/doc/docs/image/open-sora/&#34;&gt;OpenSora镜像&lt;/a&gt;|&lt;a href=&#34;https://www.bilibili.com/video/BV1ow4m1e7PX/?vd_source=c6b752764cd36ff0e535a768e35d98d2&#34;&gt;视频教程&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;📰 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.06.17]&lt;/strong&gt; 🔥 We released &lt;strong&gt;Open-Sora 1.2&lt;/strong&gt;, which includes &lt;strong&gt;3D-VAE&lt;/strong&gt;, &lt;strong&gt;rectified flow&lt;/strong&gt;, and &lt;strong&gt;score condition&lt;/strong&gt;. The video quality is greatly improved. &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-10-model-weights&#34;&gt;[checkpoints]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_03.md&#34;&gt;[report]&lt;/a&gt; &lt;a href=&#34;https://hpc-ai.com/blog/open-sora-from-hpc-ai-tech-team-continues-open-source-generate-any-16-second-720p-hd-video-with-one-click-model-weights-ready-to-use&#34;&gt;[blog]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.04.25]&lt;/strong&gt; 🤗 We released the &lt;a href=&#34;https://huggingface.co/spaces/hpcai-tech/open-sora&#34;&gt;Gradio demo for Open-Sora&lt;/a&gt; on Hugging Face Spaces.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.04.25]&lt;/strong&gt; We released &lt;strong&gt;Open-Sora 1.1&lt;/strong&gt;, which supports &lt;strong&gt;2s~15s, 144p to 720p, any aspect ratio&lt;/strong&gt; text-to-image, &lt;strong&gt;text-to-video, image-to-video, video-to-video, infinite time&lt;/strong&gt; generation. In addition, a full video processing pipeline is released. &lt;a href=&#34;&#34;&gt;[checkpoints]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_02.md&#34;&gt;[report]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.03.18]&lt;/strong&gt; We released &lt;strong&gt;Open-Sora 1.0&lt;/strong&gt;, a fully open-source project for video generation. Open-Sora 1.0 supports a full pipeline of video data preprocessing, training with &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/colossal_ai.png&#34; width=&#34;8%&#34;&gt;&lt;/a&gt; acceleration, inference, and more. Our model can produce 2s 512x512 videos with only 3 days training. &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-10-model-weights&#34;&gt;[checkpoints]&lt;/a&gt; &lt;a href=&#34;https://hpc-ai.com/blog/open-sora-v1.0&#34;&gt;[blog]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md&#34;&gt;[report]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.03.04]&lt;/strong&gt; Open-Sora provides training with 46% cost reduction. &lt;a href=&#34;https://hpc-ai.com/blog/open-sora&#34;&gt;[blog]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎥 Latest Demo&lt;/h2&gt; &#xA;&lt;p&gt;🔥 You can experience Open-Sora on our &lt;a href=&#34;https://huggingface.co/spaces/hpcai-tech/open-sora&#34;&gt;🤗 Gradio application on Hugging Face&lt;/a&gt;. More samples and corresponding prompts are available in our &lt;a href=&#34;https://hpcaitech.github.io/Open-Sora/&#34;&gt;Gallery&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;4s 720×1280&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;4s 720×1280&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;4s 720×1280&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/7895aab6-ed23-488c-8486-091480c26327&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/v1.2/sample_0013.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/20f07c7b-182b-4562-bbee-f1df74c86c9a&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/v1.2/sample_1718.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/3d897e0d-dc21-453a-b911-b3bda838acc2&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/v1.2/sample_0087.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/644bf938-96ce-44aa-b797-b3c0b513d64c&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/v1.2/sample_0052.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/272d88ac-4b4a-484d-a665-8d07431671d0&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/v1.2/sample_1719.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/ebbac621-c34e-4bb4-9543-1c34f8989764&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/v1.2/sample_0002.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/a1e3a1a3-4abd-45f5-8df2-6cced69da4ca&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/v1.2/sample_0011.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/d6ce9c13-28e1-4dff-9644-cc01f5f11926&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/v1.2/sample_0004.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/561978f8-f1b0-4f4d-ae7b-45bec9001b4a&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/v1.2/sample_0061.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;OpenSora 1.1 Demo&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;strong&gt;2s 240×426&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;strong&gt;2s 240×426&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/sample_16x240x426_9.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/sora_16x240x426_26.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/f7ce4aaa-528f-40a8-be7a-72e61eaacbbd&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/sora_16x240x426_27.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/5d58d71e-1fda-4d90-9ad3-5f2f7b75c6a9&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/sora_16x240x426_40.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;strong&gt;2s 426×240&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;strong&gt;4s 480×854&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/34ecb4a0-4eef-4286-ad4c-8e3a87e5a9fd&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/sora_16x426x240_24.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c1619333-25d7-42ba-a91c-18dbc1870b18&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/sample_32x480x854_9.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;strong&gt;16s 320×320&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;strong&gt;16s 224×448&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;strong&gt;2s 426×240&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/3cab536e-9b43-4b33-8da8-a0f9cf842ff2&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/sample_16s_320x320.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/9fb0b9e0-c6f4-4935-b29e-4cac10b373c4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/sample_16s_224x448.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/3e892ad2-9543-4049-b005-643a4c1bf3bf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/demo/sora_16x426x240_3.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;OpenSora 1.0 Demo&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;strong&gt;2s 512×512&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;strong&gt;2s 512×512&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;strong&gt;2s 512×512&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/de1963d3-b43b-4e68-a670-bb821ebb6f80&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_0.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/13f8338f-3d42-4b71-8142-d234fbd746cc&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_1.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/fa6a65a6-e32a-4d64-9a9e-eabb0ebb8c16&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_2.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;A serene night scene in a forested area. [...] The video is a time-lapse, capturing the transition from day to night, with the lake and forest serving as a constant backdrop.&lt;/td&gt; &#xA;    &lt;td&gt;A soaring drone footage captures the majestic beauty of a coastal cliff, [...] The water gently laps at the rock base and the greenery that clings to the top of the cliff.&lt;/td&gt; &#xA;    &lt;td&gt;The majestic beauty of a waterfall cascading down a cliff into a serene lake. [...] The camera angle provides a bird&#39;s eye view of the waterfall.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/64232f84-1b36-4750-a6c0-3e610fa9aa94&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_3.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/983a1965-a374-41a7-a76b-c07941a6c1e9&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_4.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/ec10c879-9767-4c31-865f-2e8d6cf11e65&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_5.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;A bustling city street at night, filled with the glow of car headlights and the ambient light of streetlights. [...]&lt;/td&gt; &#xA;    &lt;td&gt;The vibrant beauty of a sunflower field. The sunflowers are arranged in neat rows, creating a sense of order and symmetry. [...]&lt;/td&gt; &#xA;    &lt;td&gt;A serene underwater scene featuring a sea turtle swimming through a coral reef. The turtle, with its greenish-brown shell [...]&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Videos are downsampled to &lt;code&gt;.gif&lt;/code&gt; for display. Click for original videos. Prompts are trimmed for display, see &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/texts/t2v_samples.txt&#34;&gt;here&lt;/a&gt; for full prompts.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;🔆 New Features/Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📍 &lt;strong&gt;Open-Sora 1.2&lt;/strong&gt; released. Model weights are available &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#model-weights&#34;&gt;here&lt;/a&gt;. See our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_03.md&#34;&gt;report 1.2&lt;/a&gt;&lt;/strong&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;✅ Support rectified flow scheduling.&lt;/li&gt; &#xA; &lt;li&gt;✅ Support more conditioning including fps, aesthetic score, motion strength and camera motion.&lt;/li&gt; &#xA; &lt;li&gt;✅ Trained our 3D-VAE for temporal dimension compression.&lt;/li&gt; &#xA; &lt;li&gt;📍 &lt;strong&gt;Open-Sora 1.1&lt;/strong&gt; released. Model weights are available &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#model-weights&#34;&gt;here&lt;/a&gt;. It is trained on &lt;strong&gt;0s~15s, 144p to 720p, various aspect ratios&lt;/strong&gt; videos. See our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_02.md&#34;&gt;report 1.1&lt;/a&gt;&lt;/strong&gt; for more discussions.&lt;/li&gt; &#xA; &lt;li&gt;🔧 &lt;strong&gt;Data processing pipeline v1.1&lt;/strong&gt; is released. An automatic &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#data-processing&#34;&gt;processing pipeline&lt;/a&gt; from raw videos to (text, video clip) pairs is provided, including scene cutting $\rightarrow$ filtering(aesthetic, optical flow, OCR, etc.) $\rightarrow$ captioning $\rightarrow$ managing. With this tool, you can easily build your video dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;✅ Improved ST-DiT architecture includes rope positional encoding, qk norm, longer text length, etc.&lt;/li&gt; &#xA;  &lt;li&gt;✅ Support training with any resolution, aspect ratio, and duration (including images).&lt;/li&gt; &#xA;  &lt;li&gt;✅ Support image and video conditioning and video editing, and thus support animating images, connecting videos, etc.&lt;/li&gt; &#xA;  &lt;li&gt;📍 &lt;strong&gt;Open-Sora 1.0&lt;/strong&gt; released. Model weights are available &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#model-weights&#34;&gt;here&lt;/a&gt;. With only 400K video clips and 200 H800 days (compared with 152M samples in Stable Video Diffusion), we are able to generate 2s 512×512 videos. See our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md&#34;&gt;report 1.0&lt;/a&gt;&lt;/strong&gt; for more discussions.&lt;/li&gt; &#xA;  &lt;li&gt;✅ Three-stage training from an image diffusion model to a video diffusion model. We provide the weights for each stage.&lt;/li&gt; &#xA;  &lt;li&gt;✅ Support training acceleration including accelerated transformer, faster T5 and VAE, and sequence parallelism. Open-Sora improves &lt;strong&gt;55%&lt;/strong&gt; training speed when training on 64x512x512 videos. Details locates at &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/acceleration.md&#34;&gt;acceleration.md&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;🔧 &lt;strong&gt;Data preprocessing pipeline v1.0&lt;/strong&gt;, including &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/datasets/README.md&#34;&gt;downloading&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/scene_cut/README.md&#34;&gt;video cutting&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/caption/README.md&#34;&gt;captioning&lt;/a&gt; tools. Our data collection plan can be found at &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/datasets.md&#34;&gt;datasets.md&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;✅ We find VQ-VAE from &lt;a href=&#34;https://wilson1yan.github.io/videogpt/index.html&#34;&gt;VideoGPT&lt;/a&gt; has a low quality and thus adopt a better VAE from &lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse-original&#34;&gt;Stability-AI&lt;/a&gt;. We also find patching in the time dimension deteriorates the quality. See our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md&#34;&gt;report&lt;/a&gt;&lt;/strong&gt; for more discussions.&lt;/li&gt; &#xA;  &lt;li&gt;✅ We investigate different architectures including DiT, Latte, and our proposed STDiT. Our &lt;strong&gt;STDiT&lt;/strong&gt; achieves a better trade-off between quality and speed. See our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md&#34;&gt;report&lt;/a&gt;&lt;/strong&gt; for more discussions.&lt;/li&gt; &#xA;  &lt;li&gt;✅ Support clip and T5 text conditioning.&lt;/li&gt; &#xA;  &lt;li&gt;✅ By viewing images as one-frame videos, our project supports training DiT on both images and videos (e.g., ImageNet &amp;amp; UCF101). See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/commands.md&#34;&gt;commands.md&lt;/a&gt; for more instructions.&lt;/li&gt; &#xA;  &lt;li&gt;✅ Support inference with official weights from &lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt;, &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt;, and &lt;a href=&#34;https://pixart-alpha.github.io/&#34;&gt;PixArt&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;✅ Refactor the codebase. See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/structure.md&#34;&gt;structure.md&lt;/a&gt; to learn the project structure and how to use the config files.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;TODO list sorted by priority&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training Video-VAE and adapt our model to new VAE.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Scaling model parameters and dataset size.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Incoporate a better scheduler (rectified flow).&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Evaluation pipeline.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Complete the data processing pipeline (including dense optical flow, aesthetics scores, text-image similarity, etc.). See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/datasets.md&#34;&gt;the dataset&lt;/a&gt; for more information&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support image and video conditioning.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support variable aspect ratios, resolutions, durations.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#model-weights&#34;&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#gradio-demo&#34;&gt;Gradio Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#data-processing&#34;&gt;Data Processing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#vae-training--evaluation&#34;&gt;VAE Training &amp;amp; Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other useful documents and links are listed below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Report: each version is trained from a image base seperately (not continuously trained), while a newer version will incorporate the techniques from the previous version. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_03.md&#34;&gt;report 1.2&lt;/a&gt;: rectified flow, 3d-VAE, score condition, evaluation, etc.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_02.md&#34;&gt;report 1.1&lt;/a&gt;: multi-resolution/length/aspect-ratio, image/video conditioning/editing, data preprocessing, etc.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md&#34;&gt;report 1.0&lt;/a&gt;: architecture, captioning, etc.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/acceleration.md&#34;&gt;acceleration.md&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Repo structure: &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/structure.md&#34;&gt;structure.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Config file explanation: &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/config.md&#34;&gt;config.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Useful commands: &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/commands.md&#34;&gt;commands.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Data processing pipeline and dataset: &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/datasets.md&#34;&gt;datasets.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Each data processing tool&#39;s README: &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/datasets/README.md&#34;&gt;dataset conventions and management&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/scene_cut/README.md&#34;&gt;scene cutting&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/scoring/README.md&#34;&gt;scoring&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/caption/README.md&#34;&gt;caption&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Evaluation: &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/eval/README.md&#34;&gt;eval/README.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gallery: &lt;a href=&#34;https://hpcaitech.github.io/Open-Sora/&#34;&gt;gallery&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install from Source&lt;/h3&gt; &#xA;&lt;p&gt;For CUDA 12.1, you can install the dependencies with the following commands. Otherwise, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/installation.md&#34;&gt;Installation Documentation&lt;/a&gt; for more instructions on different cuda version, and additional dependency for data preprocessing, VAE, and model evaluation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# create a virtual env and activate (conda as an example)&#xA;conda create -n opensora python=3.9&#xA;conda activate opensora&#xA;&#xA;# download the repo&#xA;git clone https://github.com/hpcaitech/Open-Sora&#xA;cd Open-Sora&#xA;&#xA;# install torch, torchvision and xformers&#xA;pip install -r requirements/requirements-cu121.txt&#xA;&#xA;# the default installation is for inference only&#xA;pip install -v . # for development mode, `pip install -v -e .`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Optional, recommended for fast speed, especially for training) To enable &lt;code&gt;layernorm_kernel&lt;/code&gt; and &lt;code&gt;flash_attn&lt;/code&gt;, you need to install &lt;code&gt;apex&lt;/code&gt; and &lt;code&gt;flash-attn&lt;/code&gt; with the following commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install flash attention&#xA;# set enable_flash_attn=False in config to disable flash attention&#xA;pip install packaging ninja&#xA;pip install flash-attn --no-build-isolation&#xA;&#xA;# install apex&#xA;# set enable_layernorm_kernel=False in config to disable apex&#xA;pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings &#34;--build-option=--cpp_ext&#34; --config-settings &#34;--build-option=--cuda_ext&#34; git+https://github.com/NVIDIA/apex.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use Docker&lt;/h3&gt; &#xA;&lt;p&gt;Run the following command to build a docker image from Dockerfile provided.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t opensora .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the following command to start the docker container in interactive mode.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -ti --gpus all -v .:/workspace/Open-Sora opensora&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Weights&lt;/h2&gt; &#xA;&lt;h3&gt;Open-Sora 1.2 Model Weights&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Model Size&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;#iterations&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Diffusion&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;   &lt;td&gt;30M&lt;/td&gt; &#xA;   &lt;td&gt;70k&lt;/td&gt; &#xA;   &lt;td&gt;Dynamic&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/OpenSora-STDiT-v3&#34;&gt;&lt;span&gt;🔗&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VAE&lt;/td&gt; &#xA;   &lt;td&gt;384M&lt;/td&gt; &#xA;   &lt;td&gt;3M&lt;/td&gt; &#xA;   &lt;td&gt;1M&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/OpenSora-VAE-v1.2&#34;&gt;&lt;span&gt;🔗&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;See our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_03.md&#34;&gt;report 1.2&lt;/a&gt;&lt;/strong&gt; for more infomation. Weight will be automatically downloaded when you run the inference script.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For users from mainland China, try &lt;code&gt;export HF_ENDPOINT=https://hf-mirror.com&lt;/code&gt; to successfully download the weights.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Open-Sora 1.1 Model Weights&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Resolution&lt;/th&gt; &#xA;    &lt;th&gt;Model Size&lt;/th&gt; &#xA;    &lt;th&gt;Data&lt;/th&gt; &#xA;    &lt;th&gt;#iterations&lt;/th&gt; &#xA;    &lt;th&gt;Batch Size&lt;/th&gt; &#xA;    &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;mainly 144p &amp;amp; 240p&lt;/td&gt; &#xA;    &lt;td&gt;700M&lt;/td&gt; &#xA;    &lt;td&gt;10M videos + 2M images&lt;/td&gt; &#xA;    &lt;td&gt;100k&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/configs/opensora-v1-1/train/stage2.py&#34;&gt;dynamic&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/OpenSora-STDiT-v2-stage2&#34;&gt;&lt;span&gt;🔗&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;144p to 720p&lt;/td&gt; &#xA;    &lt;td&gt;700M&lt;/td&gt; &#xA;    &lt;td&gt;500K HQ videos + 1M images&lt;/td&gt; &#xA;    &lt;td&gt;4k&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/configs/opensora-v1-1/train/stage3.py&#34;&gt;dynamic&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/OpenSora-STDiT-v2-stage3&#34;&gt;&lt;span&gt;🔗&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;See our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_02.md&#34;&gt;report 1.1&lt;/a&gt;&lt;/strong&gt; for more infomation.&lt;/p&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;strong&gt;LIMITATION&lt;/strong&gt;: This version contains known issues which we are going to fix in the next version (as we save computation resource for the next release). In addition, the video generation may fail for long duration, and high resolution will have noisy results due to this problem.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Open-Sora 1.0 Model Weights&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Resolution&lt;/th&gt; &#xA;    &lt;th&gt;Model Size&lt;/th&gt; &#xA;    &lt;th&gt;Data&lt;/th&gt; &#xA;    &lt;th&gt;#iterations&lt;/th&gt; &#xA;    &lt;th&gt;Batch Size&lt;/th&gt; &#xA;    &lt;th&gt;GPU days (H800)&lt;/th&gt; &#xA;    &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;16×512×512&lt;/td&gt; &#xA;    &lt;td&gt;700M&lt;/td&gt; &#xA;    &lt;td&gt;20K HQ&lt;/td&gt; &#xA;    &lt;td&gt;20k&lt;/td&gt; &#xA;    &lt;td&gt;2×64&lt;/td&gt; &#xA;    &lt;td&gt;35&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/Open-Sora/blob/main/OpenSora-v1-HQ-16x512x512.pth&#34;&gt;&lt;span&gt;🔗&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;16×256×256&lt;/td&gt; &#xA;    &lt;td&gt;700M&lt;/td&gt; &#xA;    &lt;td&gt;20K HQ&lt;/td&gt; &#xA;    &lt;td&gt;24k&lt;/td&gt; &#xA;    &lt;td&gt;8×64&lt;/td&gt; &#xA;    &lt;td&gt;45&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/Open-Sora/blob/main/OpenSora-v1-HQ-16x256x256.pth&#34;&gt;&lt;span&gt;🔗&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;16×256×256&lt;/td&gt; &#xA;    &lt;td&gt;700M&lt;/td&gt; &#xA;    &lt;td&gt;366K&lt;/td&gt; &#xA;    &lt;td&gt;80k&lt;/td&gt; &#xA;    &lt;td&gt;8×64&lt;/td&gt; &#xA;    &lt;td&gt;117&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/Open-Sora/blob/main/OpenSora-v1-16x256x256.pth&#34;&gt;&lt;span&gt;🔗&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Training orders: 16x256x256 $\rightarrow$ 16x256x256 HQ $\rightarrow$ 16x512x512 HQ.&lt;/p&gt; &#xA; &lt;p&gt;Our model&#39;s weight is partially initialized from &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-α&lt;/a&gt;. The number of parameters is 724M. More information about training can be found in our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md&#34;&gt;report&lt;/a&gt;&lt;/strong&gt;. More about the dataset can be found in &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/datasets.md&#34;&gt;datasets.md&lt;/a&gt;. HQ means high quality.&lt;/p&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;strong&gt;LIMITATION&lt;/strong&gt;: Our model is trained on a limited budget. The quality and text alignment is relatively poor. The model performs badly, especially on generating human beings and cannot follow detailed instructions. We are working on improving the quality and text alignment.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Gradio Demo&lt;/h2&gt; &#xA;&lt;p&gt;🔥 You can experience Open-Sora on our &lt;a href=&#34;https://huggingface.co/spaces/hpcai-tech/open-sora&#34;&gt;🤗 Gradio application&lt;/a&gt; on Hugging Face online.&lt;/p&gt; &#xA;&lt;h3&gt;Local Deployment&lt;/h3&gt; &#xA;&lt;p&gt;If you want to deploy gradio locally, we have also provided a &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/gradio&#34;&gt;Gradio application&lt;/a&gt; in this repository, you can use the following the command to start an interactive web application to experience video generation with Open-Sora.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gradio spaces&#xA;python gradio/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will launch a Gradio application on your localhost. If you want to know more about the Gradio applicaiton, you can refer to the &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/gradio/README.md&#34;&gt;Gradio README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To enable prompt enhancement and other language input (e.g., 中文输入), you need to set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the environment. Check &lt;a href=&#34;https://platform.openai.com/docs/quickstart&#34;&gt;OpenAI&#39;s documentation&lt;/a&gt; to get your API key.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=YOUR_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;In the Gradio application, the basic options are as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/gradio_basic.png&#34; alt=&#34;Gradio Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to generate a video is to input a text prompt and click the &#34;&lt;strong&gt;Generate video&lt;/strong&gt;&#34; button (scroll down if you cannot find). The generated video will be displayed in the right panel. Checking the &#34;&lt;strong&gt;Enhance prompt with GPT4o&lt;/strong&gt;&#34; will use GPT-4o to refine the prompt, while &#34;&lt;strong&gt;Random Prompt&lt;/strong&gt;&#34; button will generate a random prompt by GPT-4o for you. Due to the OpenAI&#39;s API limit, the prompt refinement result has some randomness.&lt;/p&gt; &#xA;&lt;p&gt;Then, you can choose the &lt;strong&gt;resolution&lt;/strong&gt;, &lt;strong&gt;duration&lt;/strong&gt;, and &lt;strong&gt;aspect ratio&lt;/strong&gt; of the generated video. Different resolution and video length will affect the video generation speed. On a 80G H100 GPU, the generation speed (with &lt;code&gt;num_sampling_step=30&lt;/code&gt;) and peak memory usage is:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Image&lt;/th&gt; &#xA;   &lt;th&gt;2s&lt;/th&gt; &#xA;   &lt;th&gt;4s&lt;/th&gt; &#xA;   &lt;th&gt;8s&lt;/th&gt; &#xA;   &lt;th&gt;16s&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;360p&lt;/td&gt; &#xA;   &lt;td&gt;3s, 24G&lt;/td&gt; &#xA;   &lt;td&gt;18s, 27G&lt;/td&gt; &#xA;   &lt;td&gt;31s, 27G&lt;/td&gt; &#xA;   &lt;td&gt;62s, 28G&lt;/td&gt; &#xA;   &lt;td&gt;121s, 33G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;480p&lt;/td&gt; &#xA;   &lt;td&gt;2s, 24G&lt;/td&gt; &#xA;   &lt;td&gt;29s, 31G&lt;/td&gt; &#xA;   &lt;td&gt;55s, 30G&lt;/td&gt; &#xA;   &lt;td&gt;108s, 32G&lt;/td&gt; &#xA;   &lt;td&gt;219s, 36G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;720p&lt;/td&gt; &#xA;   &lt;td&gt;6s, 27G&lt;/td&gt; &#xA;   &lt;td&gt;68s, 41G&lt;/td&gt; &#xA;   &lt;td&gt;130s, 39G&lt;/td&gt; &#xA;   &lt;td&gt;260s, 45G&lt;/td&gt; &#xA;   &lt;td&gt;547s, 67G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note that besides text to video, you can also use &lt;strong&gt;image to video generation&lt;/strong&gt;. You can upload an image and then click the &#34;&lt;strong&gt;Generate video&lt;/strong&gt;&#34; button to generate a video with the image as the first frame. Or you can fill in the text prompt and click the &#34;&lt;strong&gt;Generate image&lt;/strong&gt;&#34; button to generate an image with the text prompt, and then click the &#34;&lt;strong&gt;Generate video&lt;/strong&gt;&#34; button to generate a video with the image generated with the same model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/gradio_option.png&#34; alt=&#34;Gradio Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you can specify more options, including &#34;&lt;strong&gt;Motion Strength&lt;/strong&gt;&#34;, &#34;&lt;strong&gt;Aesthetic&lt;/strong&gt;&#34; and &#34;&lt;strong&gt;Camera Motion&lt;/strong&gt;&#34;. If &#34;Enable&#34; not checked or the choice is &#34;none&#34;, the information is not passed to the model. Otherwise, the model will generate videos with the specified motion strength, aesthetic score, and camera motion.&lt;/p&gt; &#xA;&lt;p&gt;For the &lt;strong&gt;aesthetic score&lt;/strong&gt;, we recommend using values higher than 6. For &lt;strong&gt;motion strength&lt;/strong&gt;, a smaller value will lead to a smoother but less dynamic video, while a larger value will lead to a more dynamic but likely more blurry video. Thus, you can try without it and then adjust it according to the generated video. For the &lt;strong&gt;camera motion&lt;/strong&gt;, sometimes the model cannot follow the instruction well, and we are working on improving it.&lt;/p&gt; &#xA;&lt;p&gt;You can also adjust the &#34;&lt;strong&gt;Sampling steps&lt;/strong&gt;&#34;, this is directly related to the generation speed as it is the number of denoising. A number smaller than 30 usually leads to a poor generation results, while a number larger than 100 usually has no significant improvement. The &#34;&lt;strong&gt;Seed&lt;/strong&gt;&#34; is used for reproducibility, you can set it to a fixed number to generate the same video. The &#34;&lt;strong&gt;CFG Scale&lt;/strong&gt;&#34; controls how much the model follows the text prompt, a smaller value will lead to a more random video, while a larger value will lead to a more text-following video (7 is recommended).&lt;/p&gt; &#xA;&lt;p&gt;For more advanced usage, you can refer to &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/gradio/README.md#advanced-usage&#34;&gt;Gradio README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Open-Sora 1.2 Command Line Inference&lt;/h3&gt; &#xA;&lt;p&gt;The basic command line inference is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# text to video&#xA;python scripts/inference.py configs/opensora-v1-2/inference/sample.py \&#xA;  --num-frames 4s --resolution 720p --aspect-ratio 9:16 \&#xA;  --prompt &#34;a beautiful waterfall&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can add more options to the command line to customize the generation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference.py configs/opensora-v1-2/inference/sample.py \&#xA;  --num-frames 4s --resolution 720p --aspect-ratio 9:16 \&#xA;  --num-sampling-steps 30 --flow 5 --aes 6.5 \&#xA;  --prompt &#34;a beautiful waterfall&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For image to video generation and other functionalities, the API is compatible with Open-Sora 1.1. See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/commands.md&#34;&gt;here&lt;/a&gt; for more instructions.&lt;/p&gt; &#xA;&lt;p&gt;If your installation do not contain &lt;code&gt;apex&lt;/code&gt; and &lt;code&gt;flash-attn&lt;/code&gt;, you need to disable them in the config file, or via the folowing command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference.py configs/opensora-v1-2/inference/sample.py \&#xA;  --num-frames 4s --resolution 720p \&#xA;  --layernorm-kernel False --flash-attn False \&#xA;  --prompt &#34;a beautiful waterfall&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Sequence Parallelism Inference&lt;/h3&gt; &#xA;&lt;p&gt;To enable sequence parallelism, you need to use &lt;code&gt;torchrun&lt;/code&gt; to run the inference script. The following command will run the inference with 2 GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# text to video&#xA;CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 scripts/inference.py configs/opensora-v1-2/inference/sample.py \&#xA;  --num-frames 4s --resolution 720p --aspect-ratio 9:16 \&#xA;  --prompt &#34;a beautiful waterfall&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;strong&gt;LIMITATION&lt;/strong&gt;: The sequence parallelism is not supported for gradio deployment. For now, the sequence parallelism is only supported when the dimension can be divided by the number of GPUs. Thus, it may fail for some cases. We tested 4 GPUs for 720p and 2 GPUs for 480p.&lt;/p&gt; &#xA;&lt;h3&gt;GPT-4o Prompt Refinement&lt;/h3&gt; &#xA;&lt;p&gt;We find that GPT-4o can refine the prompt and improve the quality of the generated video. With this feature, you can also use other language (e.g., Chinese) as the prompt. To enable this feature, you need prepare your openai api key in the environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=YOUR_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can inference with &lt;code&gt;--llm-refine True&lt;/code&gt; to enable the GPT-4o prompt refinement, or leave prompt empty to get a random prompt generated by GPT-4o.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference.py configs/opensora-v1-2/inference/sample.py \&#xA;  --num-frames 4s --resolution 720p --llm-refine True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Open-Sora 1.1 Command Line Inference&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;p&gt;Since Open-Sora 1.1 supports inference with dynamic input size, you can pass the input size as an argument.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# text to video&#xA;python scripts/inference.py configs/opensora-v1-1/inference/sample.py --prompt &#34;A beautiful sunset over the city&#34; --num-frames 32 --image-size 480 854&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If your installation do not contain &lt;code&gt;apex&lt;/code&gt; and &lt;code&gt;flash-attn&lt;/code&gt;, you need to disable them in the config file, or via the folowing command.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference.py configs/opensora-v1-1/inference/sample.py --prompt &#34;A beautiful sunset over the city&#34; --num-frames 32 --image-size 480 854 --layernorm-kernel False --flash-attn False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/commands.md#inference-with-open-sora-11&#34;&gt;here&lt;/a&gt; for more instructions including text-to-image, image-to-video, video-to-video, and infinite time generation.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Open-Sora 1.0 Command Line Inference&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;p&gt;We have also provided an offline inference script. Run the following commands to generate samples, the required model weights will be automatically downloaded. To change sampling prompts, modify the txt file passed to &lt;code&gt;--prompt-path&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/structure.md#inference-config-demos&#34;&gt;here&lt;/a&gt; to customize the configuration.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Sample 16x512x512 (20s/sample, 100 time steps, 24 GB memory)&#xA;torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/16x512x512.py --ckpt-path OpenSora-v1-HQ-16x512x512.pth --prompt-path ./assets/texts/t2v_samples.txt&#xA;&#xA;# Sample 16x256x256 (5s/sample, 100 time steps, 22 GB memory)&#xA;torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/16x256x256.py --ckpt-path OpenSora-v1-HQ-16x256x256.pth --prompt-path ./assets/texts/t2v_samples.txt&#xA;&#xA;# Sample 64x512x512 (40s/sample, 100 time steps)&#xA;torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/64x512x512.py --ckpt-path ./path/to/your/ckpt.pth --prompt-path ./assets/texts/t2v_samples.txt&#xA;&#xA;# Sample 64x512x512 with sequence parallelism (30s/sample, 100 time steps)&#xA;# sequence parallelism is enabled automatically when nproc_per_node is larger than 1&#xA;torchrun --standalone --nproc_per_node 2 scripts/inference.py configs/opensora/inference/64x512x512.py --ckpt-path ./path/to/your/ckpt.pth --prompt-path ./assets/texts/t2v_samples.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The speed is tested on H800 GPUs. For inference with other models, see &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/commands.md&#34;&gt;here&lt;/a&gt; for more instructions. To lower the memory usage, set a smaller &lt;code&gt;vae.micro_batch_size&lt;/code&gt; in the config (slightly lower sampling speed).&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Data Processing&lt;/h2&gt; &#xA;&lt;p&gt;High-quality data is crucial for training good generation models. To this end, we establish a complete pipeline for data processing, which could seamlessly convert raw videos to high-quality video-text pairs. The pipeline is shown below. For detailed information, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/data_processing.md&#34;&gt;data processing&lt;/a&gt;. Also check out the &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/datasets.md&#34;&gt;datasets&lt;/a&gt; we use.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/report_data_pipeline.png&#34; alt=&#34;Data Processing Pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Open-Sora 1.2 Training&lt;/h3&gt; &#xA;&lt;p&gt;The training process is same as Open-Sora 1.1.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# one node&#xA;torchrun --standalone --nproc_per_node 8 scripts/train.py \&#xA;    configs/opensora-v1-2/train/stage1.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT&#xA;# multiple nodes&#xA;colossalai run --nproc_per_node 8 --hostfile hostfile scripts/train.py \&#xA;    configs/opensora-v1-2/train/stage1.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Open-Sora 1.1 Training&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;p&gt;Once you prepare the data in a &lt;code&gt;csv&lt;/code&gt; file, run the following commands to launch training on a single node.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# one node&#xA;torchrun --standalone --nproc_per_node 8 scripts/train.py \&#xA;    configs/opensora-v1-1/train/stage1.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT&#xA;# multiple nodes&#xA;colossalai run --nproc_per_node 8 --hostfile hostfile scripts/train.py \&#xA;    configs/opensora-v1-1/train/stage1.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Open-Sora 1.0 Training&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;p&gt;Once you prepare the data in a &lt;code&gt;csv&lt;/code&gt; file, run the following commands to launch training on a single node.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1 GPU, 16x256x256&#xA;torchrun --nnodes=1 --nproc_per_node=1 scripts/train.py configs/opensora/train/16x256x256.py --data-path YOUR_CSV_PATH&#xA;# 8 GPUs, 64x512x512&#xA;torchrun --nnodes=1 --nproc_per_node=8 scripts/train.py configs/opensora/train/64x512x512.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To launch training on multiple nodes, prepare a hostfile according to &lt;a href=&#34;https://colossalai.org/docs/basics/launch_colossalai/#launch-with-colossal-ai-cli&#34;&gt;ColossalAI&lt;/a&gt;, and run the following commands.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;colossalai run --nproc_per_node 8 --hostfile hostfile scripts/train.py configs/opensora/train/64x512x512.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For training other models and advanced usage, see &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/commands.md&#34;&gt;here&lt;/a&gt; for more instructions.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We support evaluation based on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Validation loss&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Vchitect/VBench/tree/master&#34;&gt;VBench&lt;/a&gt; score&lt;/li&gt; &#xA; &lt;li&gt;VBench-i2v score&lt;/li&gt; &#xA; &lt;li&gt;Batch generation for human evaluation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All the evaluation code is released in &lt;code&gt;eval&lt;/code&gt; folder. Check the &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/eval/README.md&#34;&gt;README&lt;/a&gt; for more details. Our &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_03.md#evaluation&#34;&gt;report&lt;/a&gt; also provides more information about the evaluation during training. The following table shows Open-Sora 1.2 greatly improves Open-Sora 1.0.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Total Score&lt;/th&gt; &#xA;   &lt;th&gt;Quality Score&lt;/th&gt; &#xA;   &lt;th&gt;Semantic Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-Sora V1.0&lt;/td&gt; &#xA;   &lt;td&gt;75.91%&lt;/td&gt; &#xA;   &lt;td&gt;78.81%&lt;/td&gt; &#xA;   &lt;td&gt;64.28%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-Sora V1.2&lt;/td&gt; &#xA;   &lt;td&gt;79.23%&lt;/td&gt; &#xA;   &lt;td&gt;80.71%&lt;/td&gt; &#xA;   &lt;td&gt;73.30%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;VAE Training &amp;amp; Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We train a VAE pipeline that consists of a spatial VAE followed by a temporal VAE. For more details, refer to &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/vae.md&#34;&gt;VAE Documentation&lt;/a&gt;. Before you run the following commands, follow our &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/installation.md&#34;&gt;Installation Documentation&lt;/a&gt; to install the required dependencies for VAE and Evaluation.&lt;/p&gt; &#xA;&lt;p&gt;If you want to train your own VAE, we need to prepare data in the csv following the &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#data-processing&#34;&gt;data processing&lt;/a&gt; pipeline, then run the following commands. Note that you need to adjust the number of trained epochs (&lt;code&gt;epochs&lt;/code&gt;) in the config file accordingly with respect to your own csv data size.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# stage 1 training, 380k steps, 8 GPUs&#xA;torchrun --nnodes=1 --nproc_per_node=8 scripts/train_vae.py configs/vae/train/stage1.py --data-path YOUR_CSV_PATH&#xA;# stage 2 training, 260k steps, 8 GPUs&#xA;torchrun --nnodes=1 --nproc_per_node=8 scripts/train_vae.py configs/vae/train/stage2.py --data-path YOUR_CSV_PATH&#xA;# stage 3 training, 540k steps, 24 GPUs&#xA;torchrun --nnodes=3 --nproc_per_node=8 scripts/train_vae.py configs/vae/train/stage3.py --data-path YOUR_CSV_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To evaluate the VAE performance, you need to run VAE inference first to generate the videos, then calculate scores on the generated videos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# video generation&#xA;torchrun --standalone --nnodes=1 --nproc_per_node=1 scripts/inference_vae.py configs/vae/inference/video.py --ckpt-path YOUR_VAE_CKPT_PATH --data-path YOUR_CSV_PATH --save-dir YOUR_VIDEO_DIR&#xA;# the original videos will be saved to `YOUR_VIDEO_DIR_ori`&#xA;# the reconstructed videos through the pipeline will be saved to `YOUR_VIDEO_DIR_rec`&#xA;# the reconstructed videos through the spatial VAE only will be saved to `YOUR_VIDEO_DIR_spatial`&#xA;&#xA;# score calculation&#xA;python eval/vae/eval_common_metric.py --batch_size 2 --real_video_dir YOUR_VIDEO_DIR_ori --generated_video_dir YOUR_VIDEO_DIR_rec --device cuda --sample_fps 24 --crop_size 256 --resolution 256 --num_frames 17 --sample_rate 1 --metric ssim psnr lpips flolpips&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Thanks goes to these wonderful contributors:&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=hpcaitech/Open-Sora&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;If you wish to contribute to this project, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/CONTRIBUTING.md&#34;&gt;Contribution Guideline&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Here we only list a few of the projects. For other works and datasets, please refer to our report.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;ColossalAI&lt;/a&gt;: A powerful large model parallel acceleration and optimization system.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt;: Scalable Diffusion Models with Transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/OpenDiT&#34;&gt;OpenDiT&lt;/a&gt;: An acceleration for DiT training. We adopt valuable acceleration strategies for training progress from OpenDiT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt&lt;/a&gt;: An open-source DiT-based text-to-image model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt;: An attempt to efficiently train DiT for video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse-original&#34;&gt;StabilityAI VAE&lt;/a&gt;: A powerful image VAE model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;CLIP&lt;/a&gt;: A powerful text-image embedding model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/text-to-text-transfer-transformer&#34;&gt;T5&lt;/a&gt;: A powerful text encoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;: A powerful image captioning model based on &lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-v0.1&#34;&gt;Mistral-7B&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/01-ai/Yi-34B&#34;&gt;Yi-34B&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/magic-research/PLLaVA&#34;&gt;PLLaVA&lt;/a&gt;: A powerful video captioning model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mira-space/MiraData&#34;&gt;MiraData&lt;/a&gt;: A large-scale video dataset with long durations and structured caption.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are grateful for their exceptional work and generous contribution to open source. Special thanks go to the authors of &lt;a href=&#34;https://github.com/mira-space/MiraData&#34;&gt;MiraData&lt;/a&gt; and &lt;a href=&#34;https://github.com/gnobitab/RectifiedFlow&#34;&gt;Rectified Flow&lt;/a&gt; for their valuable advice and help. We wish to express gratitude towards AK for sharing this project on social media and Hugging Face for providing free GPU resources for our online Gradio demo.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{opensora,&#xA;  author = {Zangwei Zheng and Xiangyu Peng and Tianji Yang and Chenhui Shen and Shenggui Li and Hongxin Liu and Yukun Zhou and Tianyi Li and Yang You},&#xA;  title = {Open-Sora: Democratizing Efficient Video Production for All},&#xA;  month = {March},&#xA;  year = {2024},&#xA;  url = {https://github.com/hpcaitech/Open-Sora}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#hpcaitech/Open-Sora&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=hpcaitech/Open-Sora&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>