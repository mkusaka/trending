<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-06T01:48:58Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Sanster/lama-cleaner</title>
    <updated>2022-11-06T01:48:58Z</updated>
    <id>tag:github.com,2022-11-06:/Sanster/lama-cleaner</id>
    <link href="https://github.com/Sanster/lama-cleaner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Lama Cleaner&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;A free and open-source inpainting tool powered by SOTA AI model.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt; &lt;img alt=&#34;total download&#34; src=&#34;https://pepy.tech/badge/lama-cleaner&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lama-cleaner/&#34;&gt; &lt;img alt=&#34;version&#34; src=&#34;https://img.shields.io/pypi/v/lama-cleaner&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1e3ZkAJxvkK3uzaTGu91N9TvI_Mahs0Wb?usp=sharing&#34;&gt; &lt;img alt=&#34;Open in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt; &lt;img alt=&#34;python version&#34; src=&#34;https://img.shields.io/badge/python-3.7+-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/cwq1913/lama-cleaner&#34;&gt; &lt;img alt=&#34;version&#34; src=&#34;https://img.shields.io/docker/pulls/cwq1913/lama-cleaner&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/3998421/196976498-ba1ad3ab-fa18-4c55-965f-5c6683141375.mp4&#34;&gt;https://user-images.githubusercontent.com/3998421/196976498-ba1ad3ab-fa18-4c55-965f-5c6683141375.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Completely free and open-source&lt;/li&gt; &#xA; &lt;li&gt;Fully self-hosted&lt;/li&gt; &#xA; &lt;li&gt;Classical image inpainting algorithm powered by &lt;a href=&#34;https://docs.opencv.org/3.4/df/d3d/tutorial_py_inpainting.html&#34;&gt;cv2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multiple SOTA AI models &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/saic-mdal/lama&#34;&gt;LaMa&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;LDM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/DQiaole/ZITS_inpainting&#34;&gt;ZITS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/fenglinglwb/MAT&#34;&gt;MAT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/SHI-Labs/FcF-Inpainting&#34;&gt;FcF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/runwayml/stable-diffusion&#34;&gt;SD1.5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support CPU &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;Various inpainting &lt;a href=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/#inpainting-strategy&#34;&gt;strategy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run as a desktop APP&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;1. Remove any unwanted things on the image&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Usage&lt;/th&gt; &#xA;    &lt;th&gt;Before&lt;/th&gt; &#xA;    &lt;th&gt;After&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Remove unwanted things&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_object.jpg&#34; alt=&#34;unwant_object2&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_object_clean.jpg&#34; alt=&#34;unwant_object2&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Remove unwanted person&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_person.jpg&#34; alt=&#34;unwant_person&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_person_clean.jpg&#34; alt=&#34;unwant_person&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Remove Text&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_text.jpg&#34; alt=&#34;text&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/unwant_text_clean.jpg&#34; alt=&#34;text&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Remove watermark&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/watermark.jpg&#34; alt=&#34;watermark&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/watermark_cleanup.jpg&#34; alt=&#34;watermark_clean&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;2. Fix old photo&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Usage&lt;/th&gt; &#xA;    &lt;th&gt;Before&lt;/th&gt; &#xA;    &lt;th&gt;After&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Fix old photo&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/old_photo.jpg&#34; alt=&#34;oldphoto&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/old_photo_clean.jpg&#34; alt=&#34;oldphoto_clean&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;3. Replace something on the image &lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Usage&lt;/th&gt; &#xA;    &lt;th&gt;Before&lt;/th&gt; &#xA;    &lt;th&gt;After&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Text Driven Inpainting&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/dog.jpg&#34; alt=&#34;dog&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Prompt: a fox sitting on a bench&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/assets/fox.jpg&#34; alt=&#34;fox&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to use Lama Cleaner is to install it using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install lama-cleaner&#xA;&#xA;# Models will be downloaded at first time used&#xA;lama-cleaner --model=lama --device=cpu --port=8080&#xA;# Lama Cleaner is now running at http://localhost:8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For stable-diffusion model, you need to &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-inpainting&#34;&gt;accepting the terms to access&lt;/a&gt;, and get an access token from here &lt;a href=&#34;https://huggingface.co/docs/hub/security-tokens&#34;&gt;huggingface access token&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you prefer to use docker, you can check out &lt;a href=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/#docker&#34;&gt;docker&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you hava no idea what is docker or pip, please check &lt;a href=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/scripts/README.md&#34;&gt;One Click Installer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Available command line arguments:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--model&lt;/td&gt; &#xA;   &lt;td&gt;lama/ldm/zits/mat/fcf/sd1.5 See details in &lt;a href=&#34;https://raw.githubusercontent.com/Sanster/lama-cleaner/main/#inpainting-model&#34;&gt;Inpaint Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;lama&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--hf_access_token&lt;/td&gt; &#xA;   &lt;td&gt;stable-diffusion need &lt;a href=&#34;https://huggingface.co/docs/hub/security-tokens&#34;&gt;huggingface access token&lt;/a&gt; to download model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--sd-run-local&lt;/td&gt; &#xA;   &lt;td&gt;Once the model as downloaded, you can pass this arg and remove &lt;code&gt;--hf_access_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--sd-disable-nsfw&lt;/td&gt; &#xA;   &lt;td&gt;Disable stable-diffusion NSFW checker.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--sd-cpu-textencoder&lt;/td&gt; &#xA;   &lt;td&gt;Always run stable-diffusion TextEncoder model on CPU.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--device&lt;/td&gt; &#xA;   &lt;td&gt;cuda or cpu&lt;/td&gt; &#xA;   &lt;td&gt;cuda&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--port&lt;/td&gt; &#xA;   &lt;td&gt;Port for backend flask web server&lt;/td&gt; &#xA;   &lt;td&gt;8080&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--gui&lt;/td&gt; &#xA;   &lt;td&gt;Launch lama-cleaner as a desktop application&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--gui_size&lt;/td&gt; &#xA;   &lt;td&gt;Set the window size for the application&lt;/td&gt; &#xA;   &lt;td&gt;1200 900&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--input&lt;/td&gt; &#xA;   &lt;td&gt;Path to image you want to load by default&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--debug&lt;/td&gt; &#xA;   &lt;td&gt;Enable debug mode for flask web server&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Inpainting Model&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Config&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cv2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; No GPU is required, and for simple backgrounds, the results may even be better than AI models.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LaMa&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; Generalizes well on high resolutions(~2k)&lt;br&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LDM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; Possible to get better and more detail result &lt;br&gt; &lt;span&gt;üëç&lt;/span&gt; The balance of time and quality can be achieved by adjusting &lt;code&gt;steps&lt;/code&gt; &lt;br&gt; &lt;span&gt;üòê&lt;/span&gt; Slower than GAN model&lt;br&gt; &lt;span&gt;üòê&lt;/span&gt; Need more GPU memory&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Steps&lt;/code&gt;: You can get better result with large steps, but it will be more time-consuming &lt;br&gt; &lt;code&gt;Sampler&lt;/code&gt;: ddim or &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;plms&lt;/a&gt;. In general plms can get &lt;a href=&#34;https://github.com/Sanster/lama-cleaner/releases/tag/0.13.0&#34;&gt;better results&lt;/a&gt; with fewer steps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ZITS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; Better holistic structures compared with previous methods &lt;br&gt; &lt;span&gt;üòê&lt;/span&gt; Wireframe module is &lt;strong&gt;very&lt;/strong&gt; slow on CPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Wireframe&lt;/code&gt;: Enable edge and line detect&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MAT&lt;/td&gt; &#xA;   &lt;td&gt;TODO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FcF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; Better structure and texture generation &lt;br&gt; &lt;span&gt;üòê&lt;/span&gt; Only support fixed size (512x512) input&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SD1.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;üëç&lt;/span&gt; SOTA text-to-image diffusion model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; See model comparison detail&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;LaMa vs LDM&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Original Image&lt;/th&gt; &#xA;    &lt;th&gt;LaMa&lt;/th&gt; &#xA;    &lt;th&gt;LDM&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/156923525-d6afdec3-7b98-403f-ad20-88ebc6eb8d6d.jpg&#34; alt=&#34;photo-1583445095369-9c651e7e5d34&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/156923620-a40cc066-fd4a-4d85-a29f-6458711d1247.png&#34; alt=&#34;photo-1583445095369-9c651e7e5d34_cleanup_lama&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/156923652-0d06c8c8-33ad-4a42-a717-9c99f3268933.png&#34; alt=&#34;photo-1583445095369-9c651e7e5d34_cleanup_ldm&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;strong&gt;LaMa vs ZITS&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Original Image&lt;/th&gt; &#xA;    &lt;th&gt;ZITS&lt;/th&gt; &#xA;    &lt;th&gt;LaMa&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/180464918-eb13ebfb-8718-461c-9e8b-7f6d8bb7a84f.png&#34; alt=&#34;zits_original&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/180464914-4db722c9-047f-48fe-9bb4-916ba09eb5c6.png&#34; alt=&#34;zits_compare_zits&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/180464903-ffb5f770-4372-4488-ba76-4b4a8c3323f5.png&#34; alt=&#34;zits_compare_lama&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Image is from &lt;a href=&#34;https://github.com/DQiaole/ZITS_inpainting&#34;&gt;ZITS&lt;/a&gt; paper. I didn&#39;t find a good example to show the advantages of ZITS and let me know if you have a good example. There can also be possible problems with my code, if you find them, please let me know too!&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;LaMa vs FcF&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Original Image&lt;/th&gt; &#xA;    &lt;th&gt;Lama&lt;/th&gt; &#xA;    &lt;th&gt;FcF&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/188305027-a4260545-c24e-4df7-9739-ac5dc3cae879.jpeg&#34; alt=&#34;texture&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/188305024-2064ed3e-5af4-4843-ac10-7f9da71e15f8.jpeg&#34; alt=&#34;texture_lama&#34;&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3998421/188305006-a08d2896-a65f-43d5-b9a5-ef62c3129f0c.jpeg&#34; alt=&#34;texture_fcf&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Inpainting Strategy&lt;/h2&gt; &#xA;&lt;p&gt;Lama Cleaner provides three ways to run inpainting model on images, you can change it in the settings dialog.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Strategy&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;VRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Original&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use the resolution of the original image&lt;/td&gt; &#xA;   &lt;td&gt;High&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ö°&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Resize&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Resize the image to a smaller size before inpainting. The area outside the mask will not loss quality.&lt;/td&gt; &#xA;   &lt;td&gt;Midium&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ö°&lt;/span&gt; &lt;span&gt;‚ö°&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Crop&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Crop masking area from the original image to do inpainting&lt;/td&gt; &#xA;   &lt;td&gt;Low&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ö°&lt;/span&gt; &lt;span&gt;‚ö°&lt;/span&gt; &lt;span&gt;‚ö°&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Download Model Manually&lt;/h2&gt; &#xA;&lt;p&gt;If you have problems downloading the model automatically when lama-cleaner start, you can download it manually. By default lama-cleaner will load model from &lt;code&gt;TORCH_HOME=~/.cache/torch/hub/checkpoints/&lt;/code&gt;, you can set &lt;code&gt;TORCH_HOME&lt;/code&gt; to other folder and put the models there.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Github: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_big_lama&#34;&gt;LaMa&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_ldm&#34;&gt;LDM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_zits&#34;&gt;ZITS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_mat&#34;&gt;MAT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/models/releases/tag/add_fcf&#34;&gt;FcF&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Baidu: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pan.baidu.com/s/1vUd3BVqIpK6e8N_EA_ZJfw&#34;&gt;https://pan.baidu.com/s/1vUd3BVqIpK6e8N_EA_ZJfw&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;passward: flsu&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Only needed if you plan to modify the frontend and recompile yourself.&lt;/p&gt; &#xA;&lt;h3&gt;Frontend&lt;/h3&gt; &#xA;&lt;p&gt;Frontend code are modified from &lt;a href=&#34;https://github.com/initml/cleanup.pictures&#34;&gt;cleanup.pictures&lt;/a&gt;, You can experience their great online services &lt;a href=&#34;https://cleanup.pictures/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install dependencies:&lt;code&gt;cd lama_cleaner/app/ &amp;amp;&amp;amp; yarn&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start development server: &lt;code&gt;yarn start&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build: &lt;code&gt;yarn build&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;You can use &lt;a href=&#34;&#34;&gt;pre-build docker image&lt;/a&gt; to run Lama Cleaner. The model will be downloaded to the cache directory when first time used. You can mount existing cache directory to start the container, so you don&#39;t have to download the model every time you start the container.&lt;/p&gt; &#xA;&lt;p&gt;The cache directories for different models correspond as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;lama/ldm/zits/mat/fcf: /root/.cache/torch&lt;/li&gt; &#xA; &lt;li&gt;sd1.5: /root/.cache/huggingface&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run Docker (cpu)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p 8080:8080 \&#xA;-v /path/to/torch_cache:/root/.cache/torch \&#xA;-v /path/to/huggingface_cache:/root/.cache/huggingface \&#xA;--rm cwq1913/lama-cleaner:cpu-0.24.4 \&#xA;lama-cleaner --device=cpu --port=8080 --host=0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Docker (gpu)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cuda11.6&lt;/li&gt; &#xA; &lt;li&gt;pytorch1.12.1&lt;/li&gt; &#xA; &lt;li&gt;minimum nvidia driver 510.39.01+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --gpus all -p 8080:8080 \&#xA;-v /path/to/torch_cache:/root/.cache/torch \&#xA;-v /path/to/huggingface_cache:/root/.cache/huggingface \&#xA;--rm cwq1913/lama-cleaner:gpu-0.24.4 \&#xA;lama-cleaner --device=cuda --port=8080 --host=0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Build Docker image&lt;/h3&gt; &#xA;&lt;p&gt;cpu only&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -f --build-arg version=0.x.0 ./docker/CPUDockerfile -t lamacleaner .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;gpu &amp;amp; cpu&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -f --build-arg version=0.x.0 ./docker/GPUDockerfile -t lamacleaner .&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>acantril/learn-cantrill-io-labs</title>
    <updated>2022-11-06T01:48:58Z</updated>
    <id>tag:github.com,2022-11-06:/acantril/learn-cantrill-io-labs</id>
    <link href="https://github.com/acantril/learn-cantrill-io-labs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Standard and Advanced Demos for learn.cantrill.io courses&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;learn-cantrill-io-labs&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/acantril/learn-cantrill-io-labs/raw/master/demogrid.png&#34; alt=&#34;DEMOGRID&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome .. this repo stores a collection of freely available demos and mini projects for AWS (and in the future other cloud platforms) These demos are available in three ways:-&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;here in instruction &amp;amp; architecture diagram form for free&lt;/li&gt; &#xA; &lt;li&gt;as part of &lt;a href=&#34;https://learn.cantrill.io&#34;&gt;https://learn.cantrill.io&lt;/a&gt; courses - including theory lessons and guided videos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;some&lt;/strong&gt; of the popular mini projects have been upgraded with video guides on this repo (people buying my courses supports this)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The free versions here are fully functional, with instructions &amp;amp; architecture diagrams and are maintained by me.&lt;/p&gt; &#xA;&lt;p&gt;All demos have a structure ...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Most are started using a &lt;code&gt;1-Click Deployment&lt;/code&gt; .. click a link, check a box, click create stack and wait until complete.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;01_LABSETUP&lt;/code&gt; contains assets required for the DEMO if required - &lt;strong&gt;many are linked directly from the instructions.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;02_LABINSTRUCTIONS&lt;/code&gt; contains text instructions and architecture diagrams - &lt;strong&gt;most of these are directly linked from the instructions.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A full range of Video guided versions are available in my courses at &lt;a href=&#34;https://learn.cantrill.io&#34;&gt;https://learn.cantrill.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I give free course credits for any bugs submitted via PR so please submit fixes for any issues you find&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;VIDEO GUIDED MINI PROJECTS (videos linked inside and on &lt;a href=&#34;https://youtube.com/c/learncantrill&#34;&gt;Youtube&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-elastic-wordpress-evolution&#34;&gt;Web Application Architecture Evolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-serverless-pet-cuddle-o-tron&#34;&gt;Implement a Serverless Application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-hybrid-bgpvpn&#34;&gt;Implement a Dynamic, BGP Based, Highly-Available Site-to-Site VPN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-cognito-web-identity-federation&#34;&gt;Building a serverless application using Web Identity Federation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-codepipeline-catpipeline&#34;&gt;CatPipeline - code commit, build, deploy &amp;amp; pipeline using containers and ECS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-hybrid-dns&#34;&gt;Hybrid DNS using Route53 Inbound and Outbound Endpoints&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-client-vpn&#34;&gt;Implement a simple client VPN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/00-aws-simple-demos/aws-lambda-s3-events&#34;&gt;Pixel Art Image Processing Workflow - Pixelator 3000&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-dms-database-migration&#34;&gt;Database Migration using DMS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TEXT ONLY MINI PROJECTS&lt;/h2&gt; &#xA;&lt;h2&gt;UNDER CONSTRUCTION OR BROKEN PROJECTS (DON&#39;T USE) - fixes in progress&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-hybrid-activedirectory&#34;&gt;Implement Hybrid Directory - On-premises &amp;amp; AWS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-patch-manager&#34;&gt;Patching AWS and On-premises using Systems Manager Patch Manager&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>geohot/tinygrad</title>
    <updated>2022-11-06T01:48:58Z</updated>
    <id>tag:github.com,2022-11-06:/geohot/tinygrad</id>
    <link href="https://github.com/geohot/tinygrad" rel="alternate"></link>
    <summary type="html">&lt;p&gt;You like pytorch? You like micrograd? You love tinygrad! ‚ù§Ô∏è&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geohot/tinygrad/master/docs/logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/geohot/tinygrad/workflows/Unit%20Tests/badge.svg?sanitize=true&#34; alt=&#34;Unit Tests&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For something in between a &lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;pytorch&lt;/a&gt; and a &lt;a href=&#34;https://github.com/karpathy/micrograd&#34;&gt;karpathy/micrograd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This may not be the best deep learning framework, but it is a deep learning framework.&lt;/p&gt; &#xA;&lt;p&gt;The sub 1000 line core of it is in &lt;code&gt;tinygrad/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Due to its extreme simplicity, it aims to be the easiest framework to add new accelerators to, with support for both inference and training. Support the simple basic ops, and you get SOTA &lt;a href=&#34;https://arxiv.org/abs/1905.11946&#34;&gt;vision&lt;/a&gt; &lt;code&gt;models/efficientnet.py&lt;/code&gt; and &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;language&lt;/a&gt; &lt;code&gt;models/transformer.py&lt;/code&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;We are working on support for the Apple Neural Engine and the Google TPU in the &lt;code&gt;accel/&lt;/code&gt; folder. Eventually, &lt;a href=&#34;https://geohot.github.io/blog/jekyll/update/2021/06/13/a-breakdown-of-ai-chip-companies.html&#34;&gt;we will build custom hardware&lt;/a&gt; for tinygrad, and it will be blindingly fast. Now, it is slow.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/geohot/tinygrad.git&#xA;cd tinygrad&#xA;python3 setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tinygrad.tensor import Tensor&#xA;&#xA;x = Tensor.eye(3, requires_grad=True)&#xA;y = Tensor([[2.0,0,-2.0]], requires_grad=True)&#xA;z = y.matmul(x).sum()&#xA;z.backward()&#xA;&#xA;print(x.grad)  # dz/dx&#xA;print(y.grad)  # dz/dy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Same example in torch&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;x = torch.eye(3, requires_grad=True)&#xA;y = torch.tensor([[2.0,0,-2.0]], requires_grad=True)&#xA;z = y.matmul(x).sum()&#xA;z.backward()&#xA;&#xA;print(x.grad)  # dz/dx&#xA;print(y.grad)  # dz/dy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Neural networks?&lt;/h2&gt; &#xA;&lt;p&gt;It turns out, a decent autograd tensor library is 90% of what you need for neural networks. Add an optimizer (SGD, RMSprop, and Adam implemented) from tinygrad.nn.optim, write some boilerplate minibatching code, and you have all you need.&lt;/p&gt; &#xA;&lt;h3&gt;Neural network example (from test/test_mnist.py)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tinygrad.tensor import Tensor&#xA;import tinygrad.nn.optim as optim&#xA;&#xA;class TinyBobNet:&#xA;  def __init__(self):&#xA;    self.l1 = Tensor.uniform(784, 128)&#xA;    self.l2 = Tensor.uniform(128, 10)&#xA;&#xA;  def forward(self, x):&#xA;    return x.dot(self.l1).relu().dot(self.l2).logsoftmax()&#xA;&#xA;model = TinyBobNet()&#xA;optim = optim.SGD([model.l1, model.l2], lr=0.001)&#xA;&#xA;# ... and complete like pytorch, with (x,y) data&#xA;&#xA;out = model.forward(x)&#xA;loss = out.mul(y).mean()&#xA;optim.zero_grad()&#xA;loss.backward()&#xA;optim.step()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GPU and Accelerator Support&lt;/h2&gt; &#xA;&lt;p&gt;tinygrad supports GPUs through PyOpenCL.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tinygrad.tensor import Tensor&#xA;(Tensor.ones(4,4).gpu() + Tensor.ones(4,4).gpu()).cpu()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ANE Support?! (broken)&lt;/h3&gt; &#xA;&lt;p&gt;If all you want to do is ReLU, you are in luck! You can do very fast ReLU (at least 30 MEGAReLUs/sec confirmed)&lt;/p&gt; &#xA;&lt;p&gt;Requires your Python to be signed with &lt;code&gt;ane/lib/sign_python.sh&lt;/code&gt; to add the &lt;code&gt;com.apple.ane.iokit-user-access&lt;/code&gt; entitlement, which also requires &lt;code&gt;sudo nvram boot-args=&#34;amfi_get_out_of_my_way=1 ipc_control_port_options=0&#34;&lt;/code&gt;. Build the library with &lt;code&gt;ane/lib/build.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;In order to set boot-args and for the AMFI kext to respect that arg, run &lt;code&gt;csrutil enable --without-kext --without-nvram&lt;/code&gt; in recovery mode.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tinygrad.tensor import Tensor&#xA;&#xA;a = Tensor([-2,-1,0,1,2]).ane()&#xA;b = a.relu()&#xA;print(b.cpu())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Warning: do not rely on the ANE port. It segfaults sometimes. So if you were doing something important with tinygrad and wanted to use the ANE, you might have a bad time.&lt;/p&gt; &#xA;&lt;h3&gt;hlops (in tensor.py)&lt;/h3&gt; &#xA;&lt;p&gt;hlops are syntactic sugar around mlops. They support most things torch does.&lt;/p&gt; &#xA;&lt;h3&gt;mlops&lt;/h3&gt; &#xA;&lt;p&gt;mlops are mid level ops, there&#39;s 15 of them. They understand memory allocation and derivatives&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Relu, Log, Exp                          # unary ops&#xA;Sum, Max                                # reduce ops (with axis argument)&#xA;Add, Sub, Mul, Pow                      # binary ops (no broadcasting, use expand)&#xA;Reshape, Permute, Slice, Expand, Flip   # movement ops&#xA;Conv2D(NCHW)                            # processing op (Matmul is also Conv2D)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You no longer need to write mlops for a new accelerator&lt;/p&gt; &#xA;&lt;h3&gt;Adding an accelerator (llops)&lt;/h3&gt; &#xA;&lt;p&gt;The autodiff stuff is all in mlops now so you can focus on the raw operations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Buffer                                                     # class of memory on this device&#xA;unary_op  (RELU, EXP, LOG, NEG, SIGN)                      # A -&amp;gt; A&#xA;reduce_op (SUM, MAX)                                       # A -&amp;gt; B (smaller size, B has 1 in shape)&#xA;binary_op (ADD, SUB, MUL, DIV, POW, CMPEQ)                 # A + B -&amp;gt; C (all the same size)&#xA;movement_op (RESHAPE, PERMUTE, PAD, SHRINK, EXPAND, FLIP)  # A -&amp;gt; B (different size)&#xA;processing_op (CONV)                                       # A + B -&amp;gt; C&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When tinygrad moves to lazy evaluation, optimizations will happen here.&lt;/p&gt; &#xA;&lt;h2&gt;ImageNet inference&lt;/h2&gt; &#xA;&lt;p&gt;Despite being tiny, tinygrad supports the full EfficientNet. Pass in a picture to discover what it is.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ipython3 examples/efficientnet.py https://media.istockphoto.com/photos/hen-picture-id831791190&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, if you have a webcam and cv2 installed&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ipython3 examples/efficientnet.py webcam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PROTIP: Set &#34;GPU=1&#34; environment variable if you want this to go faster.&lt;/p&gt; &#xA;&lt;p&gt;PROPROTIP: Set &#34;DEBUG=1&#34; environment variable if you want to see why it&#39;s slow.&lt;/p&gt; &#xA;&lt;h3&gt;tinygrad supports Stable Diffusion!&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;TORCH=1 python3 examples/stable_diffusion.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;(or without torch: &lt;code&gt;OPT=2 OPENCL=1 python3 examples/stable_diffusion.py&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geohot/tinygrad/master/docs/stable_diffusion_by_tinygrad.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#34;a horse sized cat eating a bagel&#34; &lt;/p&gt; &#xA;&lt;h3&gt;tinygrad supports GANs&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;code&gt;examples/mnist_gan.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geohot/tinygrad/master/docs/mnist_by_tinygrad.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;tinygrad supports yolo&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;code&gt;examples/yolov3.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geohot/tinygrad/master/docs/yolo_by_tinygrad.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;The promise of small&lt;/h2&gt; &#xA;&lt;p&gt;tinygrad will always be below 1000 lines. If it isn&#39;t, we will revert commits until tinygrad becomes smaller.&lt;/p&gt; &#xA;&lt;h3&gt;Drawing Execution Graph&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nodes are Tensors&lt;/li&gt; &#xA; &lt;li&gt;Black edge is a forward pass&lt;/li&gt; &#xA; &lt;li&gt;Blue edge is a backward pass&lt;/li&gt; &#xA; &lt;li&gt;Red edge is data the backward pass depends on&lt;/li&gt; &#xA; &lt;li&gt;Purple edge is intermediates created in the forward&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;GRAPH=1 python3 test/test_mnist.py TestMNIST.test_sgd_onestep&#xA;# requires dot, outputs /tmp/net.svg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running tests&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m pytest&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>