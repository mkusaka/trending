<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-01T01:48:48Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>iperov/DeepFaceLab</title>
    <updated>2024-12-01T01:48:48Z</updated>
    <id>tag:github.com,2024-12-01:/iperov/DeepFaceLab</id>
    <link href="https://github.com/iperov/DeepFaceLab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepFaceLab is the leading software for creating deepfakes.&lt;/p&gt;&lt;hr&gt;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h1&gt;DeepFaceLab&lt;/h1&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.05535&#34;&gt; &lt;/a&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.05535&#34;&gt;&lt;img src=&#34;https://static.arxiv.org/static/browse/0.3.0/images/icons/favicon.ico&#34; width=&#34;14&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.05535&#34;&gt;https://arxiv.org/abs/2005.05535&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/logo_tensorflow.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/logo_cuda.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/logo_directx.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;DeepFaceLab is used by such popular youtube channels as&lt;/p&gt; &#xA;    &lt;table&gt; &#xA;     &lt;thead&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/tiktok_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.tiktok.com/@deeptomcruise&#34;&gt;deeptomcruise&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/tiktok_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.tiktok.com/@1facerussia&#34;&gt;1facerussia&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/tiktok_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.tiktok.com/@arnoldschwarzneggar&#34;&gt;arnoldschwarzneggar&lt;/a&gt;&lt;/th&gt; &#xA;      &lt;/tr&gt; &#xA;     &lt;/thead&gt; &#xA;    &lt;/table&gt; &#xA;    &lt;table&gt; &#xA;     &lt;thead&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/tiktok_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.tiktok.com/@mariahcareyathome?&#34;&gt;mariahcareyathome?&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/tiktok_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.tiktok.com/@diepnep&#34;&gt;diepnep&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/tiktok_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.tiktok.com/@mr__heisenberg&#34;&gt;mr__heisenberg&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/tiktok_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.tiktok.com/@deepcaprio&#34;&gt;deepcaprio&lt;/a&gt;&lt;/th&gt; &#xA;      &lt;/tr&gt; &#xA;     &lt;/thead&gt; &#xA;    &lt;/table&gt; &#xA;    &lt;table&gt; &#xA;     &lt;thead&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCGf4OlX_aTt8DlrgiH3jN3g/videos&#34;&gt;VFXChris Ume&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCZXbWcv7fSZFTAZV4beckyw/videos&#34;&gt;Sham00k&lt;/a&gt;&lt;/th&gt; &#xA;      &lt;/tr&gt; &#xA;     &lt;/thead&gt; &#xA;    &lt;/table&gt; &#xA;    &lt;table&gt; &#xA;     &lt;thead&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=A91P2qtPT54&amp;amp;list=PLayt6616lBclvOprvrC8qKGCO-mAhPRux&#34;&gt;Collider videos&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCC0lK2Zo2BMXX-k1Ks0r7dg/videos&#34;&gt;iFake&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCFh3gL0a8BS21g-DHvXZEeQ/videos&#34;&gt;NextFace&lt;/a&gt;&lt;/th&gt; &#xA;      &lt;/tr&gt; &#xA;     &lt;/thead&gt; &#xA;    &lt;/table&gt; &#xA;    &lt;table&gt; &#xA;     &lt;thead&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCC5BbFxqLQgfnWPhprmQLVg&#34;&gt;Futuring Machine&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCRzgK52MmetD9aG8pDOID3g&#34;&gt;RepresentUS&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/c/corridorcrew/videos&#34;&gt;Corridor Crew&lt;/a&gt;&lt;/th&gt; &#xA;      &lt;/tr&gt; &#xA;     &lt;/thead&gt; &#xA;    &lt;/table&gt; &#xA;    &lt;table&gt; &#xA;     &lt;thead&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCkHecfDTcSazNZSKPEhtPVQ&#34;&gt;DeepFaker&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/c/DeepFakesinmovie/videos&#34;&gt;DeepFakes in movie&lt;/a&gt;&lt;/th&gt; &#xA;      &lt;/tr&gt; &#xA;     &lt;/thead&gt; &#xA;    &lt;/table&gt; &#xA;    &lt;table&gt; &#xA;     &lt;thead&gt; &#xA;      &lt;tr&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCkNFhcYNLQ5hr6A6lZ56mKA&#34;&gt;DeepFakeCreator&lt;/a&gt;&lt;/th&gt; &#xA;       &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/user/Jarkancio/videos&#34;&gt;Jarkan&lt;/a&gt;&lt;/th&gt; &#xA;      &lt;/tr&gt; &#xA;     &lt;/thead&gt; &#xA;    &lt;/table&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h1&gt;What can I do using DeepFaceLab?&lt;/h1&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Replace the face&lt;/h2&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/replace_the_face.jpg&#34; align=&#34;center&#34;&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;De-age the face&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/deage_0_1.jpg&#34; align=&#34;center&#34;&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/deage_0_2.jpg&#34; align=&#34;center&#34;&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=Ddx5B-84ebo&#34;&gt;https://www.youtube.com/watch?v=Ddx5B-84ebo&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Replace the head&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/head_replace_1_1.jpg&#34; align=&#34;center&#34;&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/head_replace_1_2.jpg&#34; align=&#34;center&#34;&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/youtube_icon.png&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=RTjgkhMugVw&#34;&gt;https://www.youtube.com/watch?v=RTjgkhMugVw&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h1&gt;Native resolution progress&lt;/h1&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/deepfake_progress.png&#34; align=&#34;center&#34;&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/make_everything_ok.png&#34; align=&#34;center&#34;&gt; &lt;p&gt;Unfortunately, there is no &#34;make everything ok&#34; button in DeepFaceLab. You should spend time studying the workflow and growing your skills. A skill in programs such as &lt;em&gt;AfterEffects&lt;/em&gt; or &lt;em&gt;Davinci Resolve&lt;/em&gt; is also desirable.&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Mini tutorial&lt;/h2&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=kOIMXt8KK8M&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/mini_tutorial.jpg&#34; align=&#34;center&#34;&gt; &lt;/a&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Releases&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://tinyurl.com/2p9cvt25&#34;&gt;Windows (magnet link)&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;Last release. Use torrent client to download.&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://mega.nz/folder/Po0nGQrA#dbbttiNWojCt8jzD4xYaPw&#34;&gt;Windows (Mega.nz)&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;Contains new and prev releases.&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://disk.yandex.ru/d/7i5XTKIKVg5UUg&#34;&gt;Windows (yandex.ru)&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;Contains new and prev releases.&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://github.com/nagadit/DeepFaceLab_Linux&#34;&gt;Linux (github)&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;by @nagadit&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://github.com/elemantalcode/dfl&#34;&gt;CentOS Linux (github)&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;May be outdated. By @elemantalcode&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h3&gt;Communication groups&lt;/h3&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://discord.gg/rxa7h9M6rH&#34;&gt;Discord&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;Official discord channel. English / Russian.&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Related works&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://github.com/iperov/DeepFaceLive&#34;&gt;DeepFaceLive&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;Real-time face swap for PC streaming or video calls&lt;/td&gt;&#xA;  &lt;/tr&gt;  &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;How I can help the project?&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h3&gt;Star this repo&lt;/h3&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p&gt;Register github account and push &#34;Star&#34; button.&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Meme zone&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/meme1.jpg&#34; align=&#34;center&#34;&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLab/master/doc/meme2.jpg&#34; align=&#34;center&#34;&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p&gt;&lt;sub&gt;#deepfacelab #faceswap #face-swap #deep-learning #deeplearning #deep-neural-networks #deepface #deep-face-swap #neural-networks #neural-nets #tensorflow #cuda #nvidia&lt;/sub&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>black-forest-labs/flux</title>
    <updated>2024-12-01T01:48:48Z</updated>
    <id>tag:github.com,2024-12-01:/black-forest-labs/flux</id>
    <link href="https://github.com/black-forest-labs/flux" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official inference repo for FLUX.1 models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FLUX&lt;/h1&gt; &#xA;&lt;p&gt;by Black Forest Labs: &lt;a href=&#34;https://blackforestlabs.ai&#34;&gt;https://blackforestlabs.ai&lt;/a&gt;. Documentation for our API can be found here: &lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;docs.bfl.ml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/assets/grid.jpg&#34; alt=&#34;grid&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains minimal inference code to run image generation &amp;amp; editing with our Flux models.&lt;/p&gt; &#xA;&lt;h2&gt;Local installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd $HOME &amp;amp;&amp;amp; git clone https://github.com/black-forest-labs/flux&#xA;cd $HOME/flux&#xA;python3.10 -m venv .venv&#xA;source .venv/bin/activate&#xA;pip install -e &#34;.[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;We are offering an extensive suite of models. For more information about the invidual models, please refer to the link under &lt;strong&gt;Usage&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Usage&lt;/th&gt; &#xA;   &lt;th&gt;HuggingFace repo&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 [schnell]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/text-to-image.md&#34;&gt;Text to Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-schnell&#34;&gt;https://huggingface.co/black-forest-labs/FLUX.1-schnell&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/model_licenses/LICENSE-FLUX1-schnell&#34;&gt;apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 [dev]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/text-to-image.md&#34;&gt;Text to Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-dev&#34;&gt;https://huggingface.co/black-forest-labs/FLUX.1-dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/model_licenses/LICENSE-FLUX1-dev&#34;&gt;FLUX.1-dev Non-Commercial License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 Fill [dev]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/fill.md&#34;&gt;In/Out-painting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev&#34;&gt;https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/model_licenses/LICENSE-FLUX1-dev&#34;&gt;FLUX.1-dev Non-Commercial License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 Canny [dev]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/structural-conditioning.md&#34;&gt;Structural Conditioning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev&#34;&gt;https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/model_licenses/LICENSE-FLUX1-dev&#34;&gt;FLUX.1-dev Non-Commercial License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 Depth [dev]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/structural-conditioning.md&#34;&gt;Structural Conditioning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev&#34;&gt;https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/model_licenses/LICENSE-FLUX1-dev&#34;&gt;FLUX.1-dev Non-Commercial License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 Canny [dev] LoRA&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/structural-conditioning.md&#34;&gt;Structural Conditioning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora&#34;&gt;https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/model_licenses/LICENSE-FLUX1-dev&#34;&gt;FLUX.1-dev Non-Commercial License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 Depth [dev] LoRA&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/structural-conditioning.md&#34;&gt;Structural Conditioning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora&#34;&gt;https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/model_licenses/LICENSE-FLUX1-dev&#34;&gt;FLUX.1-dev Non-Commercial License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 Redux [dev]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/image-variation.md&#34;&gt;Image variation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev&#34;&gt;https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/model_licenses/LICENSE-FLUX1-dev&#34;&gt;FLUX.1-dev Non-Commercial License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 [pro]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/text-to-image.md&#34;&gt;Text to Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;Available in our API.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX1.1 [pro]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/text-to-image.md&#34;&gt;Text to Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;Available in our API.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX1.1 [pro] Ultra/raw&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/text-to-image.md&#34;&gt;Text to Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;Available in our API.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 Fill [pro]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/fill.md&#34;&gt;In/Out-painting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;Available in our API.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 Canny [pro]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/controlnet.md&#34;&gt;Structural Conditioning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;Available in our API.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX.1 Depth [pro]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/controlnet.md&#34;&gt;Structural Conditioning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;Available in our API.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX1.1 Redux [pro]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/image-variation.md&#34;&gt;Image variation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;Available in our API.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FLUX1.1 Redux [pro] Ultra&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/black-forest-labs/flux/main/docs/image-variation.md&#34;&gt;Image variation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;Available in our API.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The weights of the autoencoder are also released under &lt;a href=&#34;https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md&#34;&gt;apache-2.0&lt;/a&gt; and can be found in the HuggingFace repos above.&lt;/p&gt; &#xA;&lt;h2&gt;API usage&lt;/h2&gt; &#xA;&lt;p&gt;Our API offers access to our models. It is documented here: &lt;a href=&#34;https://docs.bfl.ml/&#34;&gt;docs.bfl.ml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In this repository we also offer an easy python interface. To use this, you first need to register with the API on &lt;a href=&#34;https://api.bfl.ml/&#34;&gt;api.bfl.ml&lt;/a&gt;, and create a new API key.&lt;/p&gt; &#xA;&lt;p&gt;To use the API key either run &lt;code&gt;export BFL_API_KEY=&amp;lt;your_key_here&amp;gt;&lt;/code&gt; or provide it via the &lt;code&gt;api_key=&amp;lt;your_key_here&amp;gt;&lt;/code&gt; parameter. It is also expected that you have installed the package as above.&lt;/p&gt; &#xA;&lt;p&gt;Usage from python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from flux.api import ImageRequest&#xA;&#xA;# this will create an api request directly but not block until the generation is finished&#xA;request = ImageRequest(&#34;A beautiful beach&#34;, name=&#34;flux.1.1-pro&#34;)&#xA;# or: request = ImageRequest(&#34;A beautiful beach&#34;, name=&#34;flux.1.1-pro&#34;, api_key=&#34;your_key_here&#34;)&#xA;&#xA;# any of the following will block until the generation is finished&#xA;request.url&#xA;# -&amp;gt; https:&amp;lt;...&amp;gt;/sample.jpg&#xA;request.bytes&#xA;# -&amp;gt; b&#34;...&#34; bytes for the generated image&#xA;request.save(&#34;outputs/api.jpg&#34;)&#xA;# saves the sample to local storage&#xA;request.image&#xA;# -&amp;gt; a PIL image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python -m flux.api --prompt=&#34;A beautiful beach&#34; url&#xA;https:&amp;lt;...&amp;gt;/sample.jpg&#xA;&#xA;# generate and save the result&#xA;$ python -m flux.api --prompt=&#34;A beautiful beach&#34; save outputs/api&#xA;&#xA;# open the image directly&#xA;$ python -m flux.api --prompt=&#34;A beautiful beach&#34; image show&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>QwenLM/Qwen-Agent</title>
    <updated>2024-12-01T01:48:48Z</updated>
    <id>tag:github.com,2024-12-01:/QwenLM/Qwen-Agent</id>
    <link href="https://github.com/QwenLM/Qwen-Agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agent framework and applications built upon Qwen&gt;=2.0, featuring Function Calling, Code Interpreter, RAG, and Chrome extension.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent/raw/main/README_CN.md&#34;&gt;ä¸­æ–‡&lt;/a&gt; ï½œ English&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/logo-qwen-agent.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt;Qwen-Agent is a framework for developing LLM applications based on the instruction following, tool usage, planning, and memory capabilities of Qwen. It also comes with example applications such as Browser Assistant, Code Interpreter, and Custom Assistant.&lt;/p&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ðŸ”¥ðŸ”¥ðŸ”¥ Sep 18, 2024: Added &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/examples/tir_math.py&#34;&gt;Qwen2.5-Math Demo&lt;/a&gt; to showcase the Tool-Integrated Reasoning capabilities of Qwen2.5-Math. Note: The python executor is not sandboxed and is intended for local testing only, not for production use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install the stable version from PyPI:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U &#34;qwen-agent[gui,rag,code_interpreter,python_executor]&#34;&#xA;# Or use `pip install -U qwen-agent` for the minimal requirements.&#xA;# The optional requirements, specified in double brackets, are:&#xA;#   [gui] for Gradio-based GUI support;&#xA;#   [rag] for RAG support;&#xA;#   [code_interpreter] for Code Interpreter support;&#xA;#   [python_executor] for Tool-Integrated Reasoning with Qwen2.5-Math.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Alternatively, you can install the latest development version from the source:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/QwenLM/Qwen-Agent.git&#xA;cd Qwen-Agent&#xA;pip install -e ./&#34;[gui,rag,code_interpreter,python_executor]&#34;&#xA;# Or `pip install -e ./` for minimal requirements.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Preparation: Model Service&lt;/h2&gt; &#xA;&lt;p&gt;You can either use the model service provided by Alibaba Cloud&#39;s &lt;a href=&#34;https://help.aliyun.com/zh/dashscope/developer-reference/quick-start&#34;&gt;DashScope&lt;/a&gt;, or deploy and use your own model service using the open-source Qwen models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you choose to use the model service offered by DashScope, please ensure that you set the environment variable &lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt; to your unique DashScope API key.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Alternatively, if you prefer to deploy and use your own model service, please follow the instructions provided in the README of Qwen2 for deploying an OpenAI-compatible API service. Specifically, consult the &lt;a href=&#34;https://github.com/QwenLM/Qwen2?tab=readme-ov-file#vllm&#34;&gt;vLLM&lt;/a&gt; section for high-throughput GPU deployment or the &lt;a href=&#34;https://github.com/QwenLM/Qwen2?tab=readme-ov-file#ollama&#34;&gt;Ollama&lt;/a&gt; section for local CPU (+GPU) deployment.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Developing Your Own Agent&lt;/h2&gt; &#xA;&lt;p&gt;Qwen-Agent offers atomic components, such as LLMs (which inherit from &lt;code&gt;class BaseChatModel&lt;/code&gt; and come with &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent/raw/main/examples/function_calling.py&#34;&gt;function calling&lt;/a&gt;) and Tools (which inherit from &lt;code&gt;class BaseTool&lt;/code&gt;), along with high-level components like Agents (derived from &lt;code&gt;class Agent&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The following example illustrates the process of creating an agent capable of reading PDF files and utilizing tools, as well as incorporating a custom tool:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import pprint&#xA;import urllib.parse&#xA;import json5&#xA;from qwen_agent.agents import Assistant&#xA;from qwen_agent.tools.base import BaseTool, register_tool&#xA;&#xA;&#xA;# Step 1 (Optional): Add a custom tool named `my_image_gen`.&#xA;@register_tool(&#39;my_image_gen&#39;)&#xA;class MyImageGen(BaseTool):&#xA;    # The `description` tells the agent the functionality of this tool.&#xA;    description = &#39;AI painting (image generation) service, input text description, and return the image URL drawn based on text information.&#39;&#xA;    # The `parameters` tell the agent what input parameters the tool has.&#xA;    parameters = [{&#xA;        &#39;name&#39;: &#39;prompt&#39;,&#xA;        &#39;type&#39;: &#39;string&#39;,&#xA;        &#39;description&#39;: &#39;Detailed description of the desired image content, in English&#39;,&#xA;        &#39;required&#39;: True&#xA;    }]&#xA;&#xA;    def call(self, params: str, **kwargs) -&amp;gt; str:&#xA;        # `params` are the arguments generated by the LLM agent.&#xA;        prompt = json5.loads(params)[&#39;prompt&#39;]&#xA;        prompt = urllib.parse.quote(prompt)&#xA;        return json5.dumps(&#xA;            {&#39;image_url&#39;: f&#39;https://image.pollinations.ai/prompt/{prompt}&#39;},&#xA;            ensure_ascii=False)&#xA;&#xA;&#xA;# Step 2: Configure the LLM you are using.&#xA;llm_cfg = {&#xA;    # Use the model service provided by DashScope:&#xA;    &#39;model&#39;: &#39;qwen-max&#39;,&#xA;    &#39;model_server&#39;: &#39;dashscope&#39;,&#xA;    # &#39;api_key&#39;: &#39;YOUR_DASHSCOPE_API_KEY&#39;,&#xA;    # It will use the `DASHSCOPE_API_KEY&#39; environment variable if &#39;api_key&#39; is not set here.&#xA;&#xA;    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:&#xA;    # &#39;model&#39;: &#39;Qwen2-7B-Chat&#39;,&#xA;    # &#39;model_server&#39;: &#39;http://localhost:8000/v1&#39;,  # base_url, also known as api_base&#xA;    # &#39;api_key&#39;: &#39;EMPTY&#39;,&#xA;&#xA;    # (Optional) LLM hyperparameters for generation:&#xA;    &#39;generate_cfg&#39;: {&#xA;        &#39;top_p&#39;: 0.8&#xA;    }&#xA;}&#xA;&#xA;# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.&#xA;system_instruction = &#39;&#39;&#39;You are a helpful assistant.&#xA;After receiving the user&#39;s request, you should:&#xA;- first draw an image and obtain the image url,&#xA;- then run code `request.get(image_url)` to download the image,&#xA;- and finally select an image operation from the given document to process the image.&#xA;Please show the image using `plt.show()`.&#39;&#39;&#39;&#xA;tools = [&#39;my_image_gen&#39;, &#39;code_interpreter&#39;]  # `code_interpreter` is a built-in tool for executing code.&#xA;files = [&#39;./examples/resource/doc.pdf&#39;]  # Give the bot a PDF file to read.&#xA;bot = Assistant(llm=llm_cfg,&#xA;                system_message=system_instruction,&#xA;                function_list=tools,&#xA;                files=files)&#xA;&#xA;# Step 4: Run the agent as a chatbot.&#xA;messages = []  # This stores the chat history.&#xA;while True:&#xA;    # For example, enter the query &#34;draw a dog and rotate it 90 degrees&#34;.&#xA;    query = input(&#39;user query: &#39;)&#xA;    # Append the user query to the chat history.&#xA;    messages.append({&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: query})&#xA;    response = []&#xA;    for response in bot.run(messages=messages):&#xA;        # Streaming output.&#xA;        print(&#39;bot response:&#39;)&#xA;        pprint.pprint(response, indent=2)&#xA;    # Append the bot responses to the chat history.&#xA;    messages.extend(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition to using built-in agent implentations such as &lt;code&gt;class Assistant&lt;/code&gt;, you can also develop your own agent implemetation by inheriting from &lt;code&gt;class Agent&lt;/code&gt;. Please refer to the &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent/raw/main/examples&#34;&gt;examples&lt;/a&gt; directory for more usage examples.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;Do you have function calling (aka tool calling)?&lt;/h2&gt; &#xA;&lt;p&gt;Yes. The LLM classes provide &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent/raw/main/examples/function_calling.py&#34;&gt;function calling&lt;/a&gt;. Additionally, some Agent classes also are built upon the function calling capability, e.g., FnCallAgent and ReActChat.&lt;/p&gt; &#xA;&lt;h2&gt;How to do question-answering over super-long documents involving 1M tokens?&lt;/h2&gt; &#xA;&lt;p&gt;We have released &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent/raw/main/examples/assistant_rag.py&#34;&gt;a fast RAG solution&lt;/a&gt;, as well as &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent/raw/main/examples/parallel_doc_qa.py&#34;&gt;an expensive but competitive agent&lt;/a&gt;, for doing question-answering over super-long documents. They have managed to outperform native long-context models on two challenging benchmarks while being more efficient, and perform perfectly in the single-needle &#34;needle-in-the-haystack&#34; pressure test involving 1M-token contexts. See the &lt;a href=&#34;https://qwenlm.github.io/blog/qwen-agent-2405/&#34;&gt;blog&lt;/a&gt; for technical details.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/qwen-agent-2405-blog-long-context-results.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h1&gt;Application: BrowserQwen&lt;/h1&gt; &#xA;&lt;p&gt;BrowserQwen is a browser assistant built upon Qwen-Agent. Please refer to its &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent/raw/main/browser_qwen.md&#34;&gt;documentation&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;The code interpreter is not sandboxed, and it executes code in your own environment. Please do not ask Qwen to perform dangerous tasks, and do not directly use the code interpreter for production purposes.&lt;/p&gt;</summary>
  </entry>
</feed>