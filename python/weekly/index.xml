<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-24T01:52:02Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NanmiCoder/MediaCrawler</title>
    <updated>2024-03-24T01:52:02Z</updated>
    <id>tag:github.com,2024-03-24:/NanmiCoder/MediaCrawler</id>
    <link href="https://github.com/NanmiCoder/MediaCrawler" rel="alternate"></link>
    <summary type="html">&lt;p&gt;小红书笔记 | 评论爬虫、抖音视频 | 评论爬虫、快手视频 | 评论爬虫、B 站视频 ｜ 评论爬虫、微博帖子 ｜ 评论爬虫&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;声明：请遵守相关法规法条&lt;/strong&gt; &lt;br&gt; 国内爬虫违规违法案件：&lt;a href=&#34;https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China&#34;&gt;https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;最近这个仓库受到了大量的关注，本身我开源出来这个仓库的目的是提供一种新思路分享给大家学习和了解爬虫。&lt;br&gt; 但是随着热度的不断暴涨，随之而来的是该仓库可能给各平台方带来不少的服务器压力，以及一些其他不好的影响（虽然我有免责声明，但是不能控制一些使用该仓库的人）&lt;br&gt; 所以综合考虑之下，决定清除该仓库。我不想给自己带来一些不必要的麻烦，本来就没从中得到些什么，风险还很高&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Sponsor&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dashboard.ipcola.com/register?referral_code=vkybwyucyuidpne&#34;&gt;全球ip代理超新星&lt;/a&gt; &lt;a href=&#34;https://dashboard.ipcola.com/register?referral_code=vkybwyucyuidpne&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2024/03/18/LKJaWcIHQl92ip5.jpg&#34; alt=&#34;IPCola,  全球ip代理超新星-官网图&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://monica.im/invitation?c=4HCSQRYS&#34;&gt;你也可以通过注册这款免费的ChatGPT产品，帮我获取额外的GPT-4额度作为支持，也是我每天都在用的一款chrome效率插件，推荐给你，你也能获得免费额度。&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/NanmiCoder/MediaCrawler/issues/180&#34;&gt;整数智能《高级爬虫工程师》招聘&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;教程&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;以后技术教程分享会优先考虑违规问题，本身技术不分好坏，但是有的人会拿它做一些不好的事，所以坚决不做一些让自己陷入麻烦的事儿～（我比较谨慎）&lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;后续会陆陆续续给大家录制分享一些技术教程，请关注我的&lt;a href=&#34;https://space.bilibili.com/434377496&#34;&gt;B站账号&lt;/a&gt;，更新后会第一时间给你发通知.&lt;br&gt; B站空间地址：&lt;a href=&#34;https://space.bilibili.com/434377496&#34;&gt;https://space.bilibili.com/434377496&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;star 趋势图&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#NanmiCoder/MediaCrawler&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zylon-ai/private-gpt</title>
    <updated>2024-03-24T01:52:02Z</updated>
    <id>tag:github.com,2024-03-24:/zylon-ai/private-gpt</id>
    <link href="https://github.com/zylon-ai/private-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Interact with your documents using the power of GPT, 100% privately, no data leaks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🔒 PrivateGPT 📑&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/imartinez/privateGPT/actions/workflows/tests.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/imartinez/privateGPT/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.privategpt.dev/&#34;&gt;&lt;img src=&#34;https://img.shields.io/website?up_message=check%20it&amp;amp;down_message=down&amp;amp;url=https%3A%2F%2Fdocs.privategpt.dev%2F&amp;amp;label=Documentation&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/bK6mRVpErU&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1164200432894234644?logo=discord&amp;amp;label=PrivateGPT&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/ZylonPrivateGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/ZylonPrivateGPT&#34; alt=&#34;X (formerly Twitter) Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Install &amp;amp; usage docs: &lt;a href=&#34;https://docs.privategpt.dev/&#34;&gt;https://docs.privategpt.dev/&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Join the community: &lt;a href=&#34;https://twitter.com/PrivateGPT_AI&#34;&gt;Twitter&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://discord.gg/bK6mRVpErU&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zylon-ai/private-gpt/main/fern/docs/assets/ui.png?raw=true&#34; alt=&#34;Gradio UI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PrivateGPT is a production-ready AI project that allows you to ask questions about your documents using the power of Large Language Models (LLMs), even in scenarios without an Internet connection. 100% private, no data leaves your execution environment at any point.&lt;/p&gt; &#xA;&lt;p&gt;The project provides an API offering all the primitives required to build private, context-aware AI applications. It follows and extends the &lt;a href=&#34;https://openai.com/blog/openai-api&#34;&gt;OpenAI API standard&lt;/a&gt;, and supports both normal and streaming responses.&lt;/p&gt; &#xA;&lt;p&gt;The API is divided into two logical blocks:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-level API&lt;/strong&gt;, which abstracts all the complexity of a RAG (Retrieval Augmented Generation) pipeline implementation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ingestion of documents: internally managing document parsing, splitting, metadata extraction, embedding generation and storage.&lt;/li&gt; &#xA; &lt;li&gt;Chat &amp;amp; Completions using context from ingested documents: abstracting the retrieval of context, the prompt engineering and the response generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Low-level API&lt;/strong&gt;, which allows advanced users to implement their own complex pipelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Embeddings generation: based on a piece of text.&lt;/li&gt; &#xA; &lt;li&gt;Contextual chunks retrieval: given a query, returns the most relevant chunks of text from the ingested documents.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition to this, a working &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio UI&lt;/a&gt; client is provided to test the API, together with a set of useful tools such as bulk model download script, ingestion script, documents folder watch, etc.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;👂 &lt;strong&gt;Need help applying PrivateGPT to your specific use case?&lt;/strong&gt; &lt;a href=&#34;https://forms.gle/4cSDmH13RZBHV9at7&#34;&gt;Let us know more about it&lt;/a&gt; and we&#39;ll try to help! We are refining PrivateGPT through your feedback.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;🎞️ Overview&lt;/h2&gt; &#xA;&lt;p&gt;DISCLAIMER: This README is not updated as frequently as the &lt;a href=&#34;https://docs.privategpt.dev/&#34;&gt;documentation&lt;/a&gt;. Please check it out for the latest updates!&lt;/p&gt; &#xA;&lt;h3&gt;Motivation behind PrivateGPT&lt;/h3&gt; &#xA;&lt;p&gt;Generative AI is a game changer for our society, but adoption in companies of all sizes and data-sensitive domains like healthcare or legal is limited by a clear concern: &lt;strong&gt;privacy&lt;/strong&gt;. Not being able to ensure that your data is fully under your control when using third-party AI tools is a risk those industries cannot take.&lt;/p&gt; &#xA;&lt;h3&gt;Primordial version&lt;/h3&gt; &#xA;&lt;p&gt;The first version of PrivateGPT was launched in May 2023 as a novel approach to address the privacy concerns by using LLMs in a complete offline way.&lt;/p&gt; &#xA;&lt;p&gt;That version, which rapidly became a go-to project for privacy-sensitive setups and served as the seed for thousands of local-focused generative AI projects, was the foundation of what PrivateGPT is becoming nowadays; thus a simpler and more educational implementation to understand the basic concepts required to build a fully local -and therefore, private- chatGPT-like tool.&lt;/p&gt; &#xA;&lt;p&gt;If you want to keep experimenting with it, we have saved it in the &lt;a href=&#34;https://github.com/imartinez/privateGPT/tree/primordial&#34;&gt;primordial branch&lt;/a&gt; of the project.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;It is strongly recommended to do a clean clone and install of this new version of PrivateGPT if you come from the previous, primordial version.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Present and Future of PrivateGPT&lt;/h3&gt; &#xA;&lt;p&gt;PrivateGPT is now evolving towards becoming a gateway to generative AI models and primitives, including completions, document ingestion, RAG pipelines and other low-level building blocks. We want to make it easier for any developer to build AI applications and experiences, as well as provide a suitable extensive architecture for the community to keep contributing.&lt;/p&gt; &#xA;&lt;p&gt;Stay tuned to our &lt;a href=&#34;https://github.com/imartinez/privateGPT/releases&#34;&gt;releases&lt;/a&gt; to check out all the new features and changes included.&lt;/p&gt; &#xA;&lt;h2&gt;📄 Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Full documentation on installation, dependencies, configuration, running the server, deployment options, ingesting local documents, API details and UI features can be found here: &lt;a href=&#34;https://docs.privategpt.dev/&#34;&gt;https://docs.privategpt.dev/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🧩 Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Conceptually, PrivateGPT is an API that wraps a RAG pipeline and exposes its primitives.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The API is built using &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; and follows &lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;&gt;OpenAI&#39;s API scheme&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The RAG pipeline is based on &lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The design of PrivateGPT allows to easily extend and adapt both the API and the RAG implementation. Some key architectural decisions are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dependency Injection, decoupling the different components and layers.&lt;/li&gt; &#xA; &lt;li&gt;Usage of LlamaIndex abstractions such as &lt;code&gt;LLM&lt;/code&gt;, &lt;code&gt;BaseEmbedding&lt;/code&gt; or &lt;code&gt;VectorStore&lt;/code&gt;, making it immediate to change the actual implementations of those abstractions.&lt;/li&gt; &#xA; &lt;li&gt;Simplicity, adding as few layers and new abstractions as possible.&lt;/li&gt; &#xA; &lt;li&gt;Ready to use, providing a full implementation of the API and RAG pipeline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Main building blocks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;APIs are defined in &lt;code&gt;private_gpt:server:&amp;lt;api&amp;gt;&lt;/code&gt;. Each package contains an &lt;code&gt;&amp;lt;api&amp;gt;_router.py&lt;/code&gt; (FastAPI layer) and an &lt;code&gt;&amp;lt;api&amp;gt;_service.py&lt;/code&gt; (the service implementation). Each &lt;em&gt;Service&lt;/em&gt; uses LlamaIndex base abstractions instead of specific implementations, decoupling the actual implementation from its usage.&lt;/li&gt; &#xA; &lt;li&gt;Components are placed in &lt;code&gt;private_gpt:components:&amp;lt;component&amp;gt;&lt;/code&gt;. Each &lt;em&gt;Component&lt;/em&gt; is in charge of providing actual implementations to the base abstractions used in the Services - for example &lt;code&gt;LLMComponent&lt;/code&gt; is in charge of providing an actual implementation of an &lt;code&gt;LLM&lt;/code&gt; (for example &lt;code&gt;LlamaCPP&lt;/code&gt; or &lt;code&gt;OpenAI&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;💡 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcomed! To ensure code quality we have enabled several format and typing checks, just run &lt;code&gt;make check&lt;/code&gt; before committing to make sure your code is ok. Remember to test your code! You&#39;ll find a tests folder with helpers, and you can run tests using &lt;code&gt;make test&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t know what to contribute? Here is the public &lt;a href=&#34;https://github.com/users/imartinez/projects/3&#34;&gt;Project Board&lt;/a&gt; with several ideas.&lt;/p&gt; &#xA;&lt;p&gt;Head over to Discord #contributors channel and ask for write permissions on that GitHub project.&lt;/p&gt; &#xA;&lt;h2&gt;💬 Community&lt;/h2&gt; &#xA;&lt;p&gt;Join the conversation around PrivateGPT on our:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/PrivateGPT_AI&#34;&gt;Twitter (aka X)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/bK6mRVpErU&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📖 Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use PrivateGPT in a paper, check out the &lt;a href=&#34;https://raw.githubusercontent.com/zylon-ai/private-gpt/main/CITATION.cff&#34;&gt;Citation file&lt;/a&gt; for the correct citation.&lt;br&gt; You can also use the &#34;Cite this repository&#34; button in this repo to get the citation in different formats.&lt;/p&gt; &#xA;&lt;p&gt;Here are a couple of examples:&lt;/p&gt; &#xA;&lt;h4&gt;BibTeX&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{Martinez_Toro_PrivateGPT_2023,&#xA;author = {Martínez Toro, Iván and Gallego Vico, Daniel and Orgaz, Pablo},&#xA;license = {Apache-2.0},&#xA;month = may,&#xA;title = {{PrivateGPT}},&#xA;url = {https://github.com/imartinez/privateGPT},&#xA;year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;APA&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;Martínez Toro, I., Gallego Vico, D., &amp;amp; Orgaz, P. (2023). PrivateGPT [Computer software]. https://github.com/imartinez/privateGPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🤗 Partners &amp;amp; Supporters&lt;/h2&gt; &#xA;&lt;p&gt;PrivateGPT is actively supported by the teams behind:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qdrant.tech/&#34;&gt;Qdrant&lt;/a&gt;, providing the default vector database&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://buildwithfern.com/&#34;&gt;Fern&lt;/a&gt;, providing Documentation and SDKs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;, providing the base RAG framework and abstractions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This project has been strongly influenced and supported by other amazing projects like &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;GPT4All&lt;/a&gt;, &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;LlamaCpp&lt;/a&gt;, &lt;a href=&#34;https://www.trychroma.com/&#34;&gt;Chroma&lt;/a&gt; and &lt;a href=&#34;https://www.sbert.net/&#34;&gt;SentenceTransformers&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>princeton-nlp/SWE-bench</title>
    <updated>2024-03-24T01:52:02Z</updated>
    <id>tag:github.com,2024-03-24:/princeton-nlp/SWE-bench</id>
    <link href="https://github.com/princeton-nlp/SWE-bench" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICLR 2024] SWE-Bench: Can Language Models Resolve Real-world Github Issues?&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/princeton-nlp/Llamao&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/assets/swellama_banner.png&#34; width=&#34;50%&#34; alt=&#34;Kawi the SWE-Llama&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;| &lt;a href=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/docs/README_JP.md&#34;&gt;日本語&lt;/a&gt; | &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench&#34;&gt;English&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Code and data for our ICLR 2024 paper &lt;a href=&#34;http://swe-bench.github.io/paper.pdf&#34;&gt;SWE-bench: Can Language Models Resolve Real-World GitHub Issues?&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://www.python.org/&#34;&gt; &lt;img alt=&#34;Build&#34; src=&#34;https://img.shields.io/badge/Python-3.8+-1f425f.svg?color=purple&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://copyright.princeton.edu/policy&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/License-MIT-blue&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Please refer our &lt;a href=&#34;http://swe-bench.github.io&#34;&gt;website&lt;/a&gt; for the public leaderboard and the &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/CHANGELOG.md&#34;&gt;change log&lt;/a&gt; for information on the latest updates to the SWE-bench benchmark.&lt;/p&gt; &#xA;&lt;h2&gt;👋 Overview&lt;/h2&gt; &#xA;&lt;p&gt;SWE-bench is a benchmark for evaluating large language models on real world software issues collected from GitHub. Given a &lt;em&gt;codebase&lt;/em&gt; and an &lt;em&gt;issue&lt;/em&gt;, a language model is tasked with generating a &lt;em&gt;patch&lt;/em&gt; that resolves the described problem.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/assets/teaser.png&#34;&gt; &#xA;&lt;h2&gt;🚀 Set Up&lt;/h2&gt; &#xA;&lt;p&gt;To build SWE-bench from source, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository locally&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd&lt;/code&gt; into the repository.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;conda env create -f environment.yml&lt;/code&gt; to created a conda environment named &lt;code&gt;swe-bench&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Activate the environment with &lt;code&gt;conda activate swe-bench&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;💽 Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can download the SWE-bench dataset directly (&lt;a href=&#34;https://drive.google.com/uc?export=download&amp;amp;id=1SbOxHiR0eXlq2azPSSOIDZz-Hva0ETpX&#34;&gt;dev&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/uc?export=download&amp;amp;id=164g55i3_B78F6EphCZGtgSrd2GneFyRM&#34;&gt;test&lt;/a&gt; sets) or from &lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use SWE-Bench, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train your own models on our pre-processed datasets&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/inference/&#34;&gt;inference&lt;/a&gt; on existing models (either models you have on-disk like LLaMA, or models you have access to through an API like GPT-4). The inference step is where you get a repo and an issue and have the model try to generate a fix for it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/harness/&#34;&gt;Evaluate&lt;/a&gt; models against SWE-bench. This is where you take a SWE-Bench task and a model-proposed solution and evaluate its correctness.&lt;/li&gt; &#xA; &lt;li&gt;Run SWE-bench&#39;s &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/collect/&#34;&gt;data collection procedure&lt;/a&gt; on your own repositories, to make new SWE-Bench tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⬇️ Downloads&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datasets&lt;/th&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench&#34;&gt;🤗 SWE-bench&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-13b&#34;&gt;🦙 SWE-Llama 13b&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_oracle&#34;&gt;🤗 &#34;Oracle&#34; Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-13b-peft&#34;&gt;🦙 SWE-Llama 13b (PEFT)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_13K&#34;&gt;🤗 BM25 Retrieval 13K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-7b&#34;&gt;🦙 SWE-Llama 7b&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_27K&#34;&gt;🤗 BM25 Retrieval 27K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-7b-peft&#34;&gt;🦙 SWE-Llama 7b (PEFT)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_40K&#34;&gt;🤗 BM25 Retrieval 40K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_50k_llama&#34;&gt;🤗 BM25 Retrieval 50K (Llama tokens)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🍎 Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve also written the following blog posts on how to use different parts of SWE-bench. If you&#39;d like to see a post about a particular topic, please let us know via an issue.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Nov 1. 2023] Collecting Evaluation Tasks for SWE-Bench (&lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/tree/main/tutorials/collection.md&#34;&gt;🔗&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[Nov 6. 2023] Evaluating on SWE-bench (&lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/tree/main/tutorials/evaluation.md&#34;&gt;🔗&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;💫 Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We would love to hear from the broader NLP, Machine Learning, and Software Engineering research communities, and we welcome any contributions, pull requests, or issues! To do so, please either file a new pull request or issue and fill in the corresponding templates accordingly. We&#39;ll be sure to follow up shortly!&lt;/p&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;http://www.carlosejimenez.com/&#34;&gt;Carlos E. Jimenez&lt;/a&gt; and &lt;a href=&#34;https://john-b-yang.github.io/&#34;&gt;John Yang&lt;/a&gt; (Email: {carlosej, jy1682}@princeton.edu).&lt;/p&gt; &#xA;&lt;h2&gt;✍️ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please use the following citations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{&#xA;    jimenez2024swebench,&#xA;    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},&#xA;    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},&#xA;    booktitle={The Twelfth International Conference on Learning Representations},&#xA;    year={2024},&#xA;    url={https://openreview.net/forum?id=VTF8yNQM66}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🪪 License&lt;/h2&gt; &#xA;&lt;p&gt;MIT. Check &lt;code&gt;LICENSE.md&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>