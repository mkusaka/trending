<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-23T01:39:50Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>linyiLYi/bilibot</title>
    <updated>2024-05-23T01:39:50Z</updated>
    <id>tag:github.com,2024-05-23:/linyiLYi/bilibot</id>
    <link href="https://github.com/linyiLYi/bilibot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A local chatbot fine-tuned by bilibili user comments.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;哔哩哔哩聊天机器人&lt;/h1&gt; &#xA;&lt;p&gt;由&lt;a href=&#34;https://bilibili.com&#34;&gt;哔哩哔哩&lt;/a&gt;用户评论微调训练而成的本地聊天机器人。支持文字聊天，也可以通过 questions.txt 生成针对给定问题的语音对话。&lt;/p&gt; &#xA;&lt;p&gt;本项目文字生成使用的基础模型为 &lt;a href=&#34;https://huggingface.co/Qwen/Qwen1.5-32B-Chat&#34;&gt;Qwen1.5-32B-Chat&lt;/a&gt;，借助苹果 &lt;a href=&#34;https://github.com/ml-explore/mlx-examples/raw/main/llms/mlx_lm/LORA.md&#34;&gt;mlx-lm LORA 示例项目&lt;/a&gt; 对基础模型进行微调训练。语音生成部分基于开源项目 &lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt;，问题语音来自 B 站用户&lt;a href=&#34;https://space.bilibili.com/518098961&#34;&gt;白菜工厂1145号员工&lt;/a&gt;训练的派蒙语音模型。&lt;/p&gt; &#xA;&lt;h3&gt;文件结构&lt;/h3&gt; &#xA;&lt;p&gt;项目主要脚本存放在 &lt;code&gt;main/&lt;/code&gt; 文件夹下，模型存放于 &lt;code&gt;models/&lt;/code&gt; 文件夹。提示词模板、问题列表存放在 &lt;code&gt;text/&lt;/code&gt; 文件夹下。&lt;code&gt;tools/compress_model.py&lt;/code&gt; 可以对完整模型进行量化压缩，大大加快模型内容生成速度。&lt;/p&gt; &#xA;&lt;h2&gt;运行指南&lt;/h2&gt; &#xA;&lt;p&gt;本项目基于 Python 编程语言，程序运行使用的 Python 版本为 3.10，建议使用 &lt;a href=&#34;https://www.anaconda.com&#34;&gt;Anaconda&lt;/a&gt; 配置 Python 环境。以下配置过程已在 macOS 系统测试通过。&lt;/p&gt; &#xA;&lt;h3&gt;配置环境&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n bilibot python=3.10&#xA;conda activate bilibot&#xA;cd bilibot&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;模型微调训练与推理测试&lt;/h3&gt; &#xA;&lt;p&gt;使用控制台指令，借助 &lt;a href=&#34;https://github.com/ml-explore/mlx-examples/raw/main/llms/mlx_lm/LORA.md&#34;&gt;mlx-lm&lt;/a&gt; 对 Qwen1.5-32B-Chat 进行微调：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m mlx_lm.lora --model models/Qwen1.5-32B-Chat --data data/ --train --iters 1000 --batch-size 16 --lora-layers 12&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;将微调后的 &lt;code&gt;adapters&lt;/code&gt; 文件与基础模型合并：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m mlx_lm.fuse --model models/Qwen1.5-32B-Chat --save-path models/Qwen1.5-32B-Chat-FT --adapter-path models/Qwen1.5-32B-Chat-Adapters&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;对合并后的模型进行量化加速： python tools/compress_model.py&lt;/p&gt; &#xA;&lt;p&gt;对微调训练后的模型进行对话测试： python chat.py&lt;/p&gt; &#xA;&lt;h3&gt;语音生成&lt;/h3&gt; &#xA;&lt;p&gt;本项目借助开源项目 &lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt; 进行语音生成。&lt;/p&gt; &#xA;&lt;p&gt;首先参考 &lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt; 的官方指南配置环境并运行语音生成程序。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n GPTSOVITS python=3.9&#xA;conda activate GPTSOVITS&#xA;cd GPT-SoVITS&#xA;pip install -r requirements.txt&#xA;python webui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行 api 程序，分别使用端口 9880 与 9881 提供派蒙与林亦的语音生成服务，以下请使用 GPT-SoVITS 代码库完成：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python api.py -s SoVITS_weights/paimeng2_e110_s159940.pth -g GPT_weights/paimeng2-e10.ckpt -dr samples/Paimon/疑问—哇，这个，还有这个…只是和史莱姆打了一场，就有这么多结论吗？.wav -dt &#34;哇，这个，还有这个…只是和史莱姆打了一场，就有这么多结论吗？&#34; -dl &#34;zh&#34; -a 127.0.0.1 -p 9880&#xA;python api.py -s SoVITS_weights/linyi_e25_s1150.pth -g GPT_weights/linyi-e50.ckpt -dr &#34;samples/linyi/【愤怒】你这问题太弱智了，我都不知道该从哪开始骂你。.WAV&#34; -dt &#34;你这问题太弱智了，我都不知道该从哪开始骂你。&#34; -dl &#34;zh&#34; -a 127.0.0.1 -p 9881&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行问答生成程序：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python start_qa_dialogue.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;参考&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;机器学习框架 MLX，来自苹果机器学习研究组：&lt;a href=&#34;https://github.com/ml-explore/mlx&#34;&gt;https://github.com/ml-explore/mlx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;阿里通义千问 Qwen1.5：&lt;a href=&#34;https://qwenlm.github.io/zh/blog/qwen1.5/&#34;&gt;https://qwenlm.github.io/zh/blog/qwen1.5/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;开源文本转语音项目 GPT-SoVITS，作者&lt;a href=&#34;https://space.bilibili.com/5760446&#34;&gt;花儿不哭&lt;/a&gt;：&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;https://github.com/RVC-Boss/GPT-SoVITS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;派蒙语音模型，作者&lt;a href=&#34;https://space.bilibili.com/518098961&#34;&gt;白菜工厂1145号员工&lt;/a&gt;：&lt;a href=&#34;https://www.bilibili.com/video/BV1Yu4m1N79m&#34;&gt;【GPT-SoVITS】30小时超大数据集测试，堆时长真的有用吗？&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>fishaudio/fish-speech</title>
    <updated>2024-05-23T01:39:50Z</updated>
    <id>tag:github.com,2024-05-23:/fishaudio/fish-speech</id>
    <link href="https://github.com/fishaudio/fish-speech" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Brand new TTS solution&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Fish Speech&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://discord.gg/Es5qTB9BcN&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1214047546020728892?color=%23738ADB&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square&#34;&gt; &lt;/a&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&amp;amp;k=jCKlUP7QgSm9kh95UlBoYv6s1I-Apl1M&amp;amp;authKey=xI5ttVAp3do68IpEYEalwXSYZFdfxZSkah%2BctF5FIMyN2NqAa003vFtLqJyAVRfF&amp;amp;noverify=0&amp;amp;group_code=593946093&#34;&gt; &lt;img alt=&#34;QQ&#34; src=&#34;https://img.shields.io/badge/QQ Group-%2312B7F5?logo=tencent-qq&amp;amp;logoColor=white&amp;amp;style=flat-square&#34;&gt; &lt;/a&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://hub.docker.com/r/lengyue233/fish-speech&#34;&gt; &lt;img alt=&#34;Docker&#34; src=&#34;https://img.shields.io/docker/pulls/lengyue233/fish-speech?style=flat-square&amp;amp;logo=docker&#34;&gt; &lt;/a&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/fishaudio/fish-speech/actions/workflows/build-windows-package.yml&#34;&gt; &lt;img alt=&#34;Docker&#34; src=&#34;https://img.shields.io/github/actions/workflow/status/fishaudio/fish-speech/build-windows-package.yml?style=flat-square&amp;amp;label=Build%20Windows%20Package&amp;amp;logo=github&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This codebase is released under BSD-3-Clause License, and all models are released under CC-BY-NC-SA-4.0 License. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/fishaudio/fish-speech/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;此代码库根据 BSD-3-Clause 许可证发布, 所有模型根据 CC-BY-NC-SA-4.0 许可证发布。请参阅 &lt;a href=&#34;https://raw.githubusercontent.com/fishaudio/fish-speech/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; 了解更多细节.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer / 免责声明&lt;/h2&gt; &#xA;&lt;p&gt;We do not hold any responsibility for any illegal usage of the codebase. Please refer to your local laws about DMCA and other related laws.&lt;br&gt; 我们不对代码库的任何非法使用承担任何责任. 请参阅您当地关于 DMCA (数字千年法案) 和其他相关法律法规.&lt;/p&gt; &#xA;&lt;h2&gt;Online Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fs.firefly.matce.cn/&#34;&gt;Fish Speech&lt;/a&gt;&lt;br&gt; 支持中/日/英三语合成。基于原神/崩坏星穹铁道/BlueArchive角色。&lt;br&gt; 音频所有权归米哈游网络科技有限公司及YoStar所有。&lt;/p&gt; &#xA;&lt;h2&gt;Videos&lt;/h2&gt; &#xA;&lt;h4&gt;Demo Video: &lt;a href=&#34;https://www.bilibili.com/video/BV18E421371Q&#34;&gt;https://www.bilibili.com/video/BV18E421371Q&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;Tech slides Video: &lt;a href=&#34;https://www.bilibili.com/video/BV1zJ4m1K7cj&#34;&gt;https://www.bilibili.com/video/BV1zJ4m1K7cj&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;Documents / 文档&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://speech.fish.audio/en/&#34;&gt;English&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://speech.fish.audio/&#34;&gt;中文&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Samples / 例子&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://speech.fish.audio/en/samples/&#34;&gt;English&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://speech.fish.audio/samples/&#34;&gt;中文&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits / 鸣谢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/daniilrobnikov/vits2&#34;&gt;VITS2 (daniilrobnikov)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fishaudio/Bert-VITS2&#34;&gt;Bert-VITS2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/innnky/gpt-vits&#34;&gt;GPT VITS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/b04901014/MQTTS&#34;&gt;MQTTS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast&#34;&gt;GPT Fast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sponsor / 赞助&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://6block.com/&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/60573493&#34; width=&#34;100&#34; height=&#34;100&#34; alt=&#34;6Block Avatar&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://6block.com/&#34;&gt;数据处理服务器由 6Block 提供 (Data Processing sponsor by 6Block)&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;http://fs.firefly.matce.cn/&#34;&gt; &lt;img src=&#34;https://dice-forum.s3.ap-northeast-1.amazonaws.com/2024-05-10/1715299538-382065-04170e083d92c5e0eeff534d6e7704ee.jpg&#34; width=&#34;158&#34; height=&#34;80&#34; alt=&#34;6Block Avatar&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;http://fs.firefly.matce.cn/&#34;&gt;在线推理Demo服务器由淮北艾阿网络科技有限公司提供 (Online inference sponsor)&lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>phidatahq/phidata</title>
    <updated>2024-05-23T01:39:50Z</updated>
    <id>tag:github.com,2024-05-23:/phidatahq/phidata</id>
    <link href="https://github.com/phidatahq/phidata" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build AI Assistants with memory, knowledge and tools.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; phidata &lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Build AI Assistants with memory, knowledge and tools &lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/phidatahq/phidata/assets/22579644/295187f6-ac9d-41e0-abdb-38e3291ad1d1&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is phidata?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Phidata is a framework for building Autonomous Assistants&lt;/strong&gt; (aka Agents) that have long-term memory, contextual knowledge and the ability to take actions using function calling.&lt;/p&gt; &#xA;&lt;h2&gt;Why phidata?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; LLMs have limited context and cannot take actions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Add memory, knowledge and tools.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; Stores &lt;strong&gt;chat history&lt;/strong&gt; in a database and enables LLMs to have long-term conversations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Knowledge:&lt;/strong&gt; Stores information in a vector database and provides LLMs with &lt;strong&gt;business context&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tools:&lt;/strong&gt; Enable LLMs to &lt;strong&gt;take actions&lt;/strong&gt; like pulling data from an API, sending emails or querying a database.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Create an &lt;code&gt;Assistant&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Add Tools (functions), Knowledge (vectordb) and Storage (database)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Serve using Streamlit, FastApi or Django to build your AI application&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -U phidata&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart: Assistant that can search the web&lt;/h2&gt; &#xA;&lt;p&gt;Create a file &lt;code&gt;assistant.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from phi.assistant import Assistant&#xA;from phi.tools.duckduckgo import DuckDuckGo&#xA;&#xA;assistant = Assistant(tools=[DuckDuckGo()], show_tool_calls=True)&#xA;assistant.print_response(&#34;Whats happening in France?&#34;, markdown=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install libraries, export your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; and run the &lt;code&gt;Assistant&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install openai duckduckgo-search&#xA;&#xA;export OPENAI_API_KEY=sk-xxxx&#xA;&#xA;python assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation and Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read the docs at &lt;a href=&#34;https://docs.phidata.com&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;docs.phidata.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat with us on &lt;a href=&#34;https://discord.gg/4MtYHHrgA8&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/phidatahq/phidata/tree/main/cookbook/llm_os&#34;&gt;LLM OS&lt;/a&gt;: Using LLMs as the CPU for an emerging Operating System.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/phidatahq/phidata/tree/main/cookbook/examples/auto_rag&#34;&gt;Autonomous RAG&lt;/a&gt;: Gives LLMs tools to search its knowledge, web or chat history.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/phidatahq/phidata/tree/main/cookbook/llms/ollama/rag&#34;&gt;Local RAG&lt;/a&gt;: Fully local RAG with Ollama and PgVector.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/phidatahq/phidata/tree/main/cookbook/llms/groq/investment_researcher&#34;&gt;Investment Researcher&lt;/a&gt;: Generate investment reports on stocks using Llama3 and Groq.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/phidatahq/phidata/tree/main/cookbook/llms/groq/news_articles&#34;&gt;News Articles&lt;/a&gt;: Write News Articles using Llama3 and Groq.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/phidatahq/phidata/tree/main/cookbook/llms/groq/video_summary&#34;&gt;Video Summaries&lt;/a&gt;: YouTube video summaries using Llama3 and Groq.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/phidatahq/phidata/tree/main/cookbook/llms/groq/research&#34;&gt;Research Assistant&lt;/a&gt;: Write research reports using Llama3 and Groq.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Assistant that can write and run python code&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show details&lt;/summary&gt; &#xA; &lt;p&gt;The &lt;code&gt;PythonAssistant&lt;/code&gt; can achieve tasks by writing and running python code.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;python_assistant.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from phi.assistant.python import PythonAssistant&#xA;from phi.file.local.csv import CsvFile&#xA;&#xA;python_assistant = PythonAssistant(&#xA;    files=[&#xA;        CsvFile(&#xA;            path=&#34;https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv&#34;,&#xA;            description=&#34;Contains information about movies from IMDB.&#34;,&#xA;        )&#xA;    ],&#xA;    pip_install=True,&#xA;    show_tool_calls=True,&#xA;)&#xA;&#xA;python_assistant.print_response(&#34;What is the average rating of movies?&#34;, markdown=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install pandas and run the &lt;code&gt;python_assistant.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install pandas&#xA;&#xA;python python_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Assistant that can analyze data using SQL&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show details&lt;/summary&gt; &#xA; &lt;p&gt;The &lt;code&gt;DuckDbAssistant&lt;/code&gt; can perform data analysis using SQL.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;data_assistant.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;from phi.assistant.duckdb import DuckDbAssistant&#xA;&#xA;duckdb_assistant = DuckDbAssistant(&#xA;    semantic_model=json.dumps({&#xA;        &#34;tables&#34;: [&#xA;            {&#xA;                &#34;name&#34;: &#34;movies&#34;,&#xA;                &#34;description&#34;: &#34;Contains information about movies from IMDB.&#34;,&#xA;                &#34;path&#34;: &#34;https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv&#34;,&#xA;            }&#xA;        ]&#xA;    }),&#xA;)&#xA;&#xA;duckdb_assistant.print_response(&#34;What is the average rating of movies? Show me the SQL.&#34;, markdown=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install duckdb and run the &lt;code&gt;data_assistant.py&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install duckdb&#xA;&#xA;python data_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Assistant that can generate pydantic models&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show details&lt;/summary&gt; &#xA; &lt;p&gt;One of our favorite LLM features is generating structured data (i.e. a pydantic model) from text. Use this feature to extract features, generate movie scripts, produce fake data etc.&lt;/p&gt; &#xA; &lt;p&gt;Let&#39;s create an Movie Assistant to write a &lt;code&gt;MovieScript&lt;/code&gt; for us.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;movie_assistant.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List&#xA;from pydantic import BaseModel, Field&#xA;from rich.pretty import pprint&#xA;from phi.assistant import Assistant&#xA;&#xA;class MovieScript(BaseModel):&#xA;    setting: str = Field(..., description=&#34;Provide a nice setting for a blockbuster movie.&#34;)&#xA;    ending: str = Field(..., description=&#34;Ending of the movie. If not available, provide a happy ending.&#34;)&#xA;    genre: str = Field(..., description=&#34;Genre of the movie. If not available, select action, thriller or romantic comedy.&#34;)&#xA;    name: str = Field(..., description=&#34;Give a name to this movie&#34;)&#xA;    characters: List[str] = Field(..., description=&#34;Name of characters for this movie.&#34;)&#xA;    storyline: str = Field(..., description=&#34;3 sentence storyline for the movie. Make it exciting!&#34;)&#xA;&#xA;movie_assistant = Assistant(&#xA;    description=&#34;You help write movie scripts.&#34;,&#xA;    output_model=MovieScript,&#xA;)&#xA;&#xA;pprint(movie_assistant.run(&#34;New York&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Run the &lt;code&gt;movie_assistant.py&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python movie_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;The output is an object of the &lt;code&gt;MovieScript&lt;/code&gt; class, here&#39;s how it looks:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;MovieScript(&#xA;│   setting=&#39;A bustling and vibrant New York City&#39;,&#xA;│   ending=&#39;The protagonist saves the city and reconciles with their estranged family.&#39;,&#xA;│   genre=&#39;action&#39;,&#xA;│   name=&#39;City Pulse&#39;,&#xA;│   characters=[&#39;Alex Mercer&#39;, &#39;Nina Castillo&#39;, &#39;Detective Mike Johnson&#39;],&#xA;│   storyline=&#39;In the heart of New York City, a former cop turned vigilante, Alex Mercer, teams up with a street-smart activist, Nina Castillo, to take down a corrupt political figure who threatens to destroy the city. As they navigate through the intricate web of power and deception, they uncover shocking truths that push them to the brink of their abilities. With time running out, they must race against the clock to save New York and confront their own demons.&#39;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;PDF Assistant with Knowledge &amp;amp; Storage&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show details&lt;/summary&gt; &#xA; &lt;p&gt;Lets create a PDF Assistant that can answer questions from a PDF. We&#39;ll use &lt;code&gt;PgVector&lt;/code&gt; for knowledge and storage.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Knowledge Base:&lt;/strong&gt; information that the Assistant can search to improve its responses (uses a vector db).&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; provides long term memory for Assistants (uses a database).&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Run PgVector&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Install &lt;a href=&#34;https://docs.docker.com/desktop/install/mac-install/&#34;&gt;docker desktop&lt;/a&gt; and run &lt;strong&gt;PgVector&lt;/strong&gt; on port &lt;strong&gt;5532&lt;/strong&gt; using:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d \&#xA;  -e POSTGRES_DB=ai \&#xA;  -e POSTGRES_USER=ai \&#xA;  -e POSTGRES_PASSWORD=ai \&#xA;  -e PGDATA=/var/lib/postgresql/data/pgdata \&#xA;  -v pgvolume:/var/lib/postgresql/data \&#xA;  -p 5532:5432 \&#xA;  --name pgvector \&#xA;  phidata/pgvector:16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Create PDF Assistant&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Create a file &lt;code&gt;pdf_assistant.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import typer&#xA;from rich.prompt import Prompt&#xA;from typing import Optional, List&#xA;from phi.assistant import Assistant&#xA;from phi.storage.assistant.postgres import PgAssistantStorage&#xA;from phi.knowledge.pdf import PDFUrlKnowledgeBase&#xA;from phi.vectordb.pgvector import PgVector2&#xA;&#xA;db_url = &#34;postgresql+psycopg://ai:ai@localhost:5532/ai&#34;&#xA;&#xA;knowledge_base = PDFUrlKnowledgeBase(&#xA;    urls=[&#34;https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf&#34;],&#xA;    vector_db=PgVector2(collection=&#34;recipes&#34;, db_url=db_url),&#xA;)&#xA;# Comment out after first run&#xA;knowledge_base.load()&#xA;&#xA;storage = PgAssistantStorage(table_name=&#34;pdf_assistant&#34;, db_url=db_url)&#xA;&#xA;&#xA;def pdf_assistant(new: bool = False, user: str = &#34;user&#34;):&#xA;    run_id: Optional[str] = None&#xA;&#xA;    if not new:&#xA;        existing_run_ids: List[str] = storage.get_all_run_ids(user)&#xA;        if len(existing_run_ids) &amp;gt; 0:&#xA;            run_id = existing_run_ids[0]&#xA;&#xA;    assistant = Assistant(&#xA;        run_id=run_id,&#xA;        user_id=user,&#xA;        knowledge_base=knowledge_base,&#xA;        storage=storage,&#xA;        # Show tool calls in the response&#xA;        show_tool_calls=True,&#xA;        # Enable the assistant to search the knowledge base&#xA;        search_knowledge=True,&#xA;        # Enable the assistant to read the chat history&#xA;        read_chat_history=True,&#xA;    )&#xA;    if run_id is None:&#xA;        run_id = assistant.run_id&#xA;        print(f&#34;Started Run: {run_id}\n&#34;)&#xA;    else:&#xA;        print(f&#34;Continuing Run: {run_id}\n&#34;)&#xA;&#xA;    # Runs the assistant as a cli app&#xA;    assistant.cli_app(markdown=True)&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    typer.run(pdf_assistant)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Install libraries&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -U pgvector pypdf &#34;psycopg[binary]&#34; sqlalchemy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Run PDF Assistant&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python pdf_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Ask a question:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;How do I make pad thai?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;See how the Assistant searches the knowledge base and returns a response.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Message &lt;code&gt;bye&lt;/code&gt; to exit, start the assistant again using &lt;code&gt;python pdf_assistant.py&lt;/code&gt; and ask:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;What was my last message?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;See how the assistant now maintains storage across sessions.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Run the &lt;code&gt;pdf_assistant.py&lt;/code&gt; file with the &lt;code&gt;--new&lt;/code&gt; flag to start a new run.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python pdf_assistant.py --new&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Checkout the &lt;a href=&#34;https://github.com/phidatahq/phidata/tree/main/cookbook&#34;&gt;cookbook&lt;/a&gt; for more examples.&lt;/h3&gt; &#xA;&lt;h2&gt;Next Steps&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://docs.phidata.com/basics&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;basics&lt;/a&gt; to learn more about phidata.&lt;/li&gt; &#xA; &lt;li&gt;Read about &lt;a href=&#34;https://docs.phidata.com/assistants/introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Assistants&lt;/a&gt; and how to customize them.&lt;/li&gt; &#xA; &lt;li&gt;Checkout the &lt;a href=&#34;https://docs.phidata.com/examples/cookbook&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;cookbook&lt;/a&gt; for in-depth examples and code.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;Checkout the following AI Applications built using phidata:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pdf.aidev.run/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;PDF AI&lt;/a&gt; that summarizes and answers questions from PDFs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.aidev.run/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;ArXiv AI&lt;/a&gt; that answers questions about ArXiv papers using the ArXiv API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hn.aidev.run/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;HackerNews AI&lt;/a&gt; summarize stories, users and shares what&#39;s new on HackerNews.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6g2KLvwHZlU&#34; title=&#34;LLM OS&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/6g2KLvwHZlU/0.jpg&#34; alt=&#34;Building the LLM OS with gpt-4o&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fkBkNWivq-s&#34; title=&#34;Autonomous RAG&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/fkBkNWivq-s/0.jpg&#34; alt=&#34;Autonomous RAG&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-8NVHaKKNkM&#34; title=&#34;Local RAG with Llama3&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/-8NVHaKKNkM/0.jpg&#34; alt=&#34;Local RAG with Llama3&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Iv9dewmcFbs&#34; title=&#34;Llama3 Research Assistant powered by Groq&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/Iv9dewmcFbs/0.jpg&#34; alt=&#34;Llama3 Research Assistant powered by Groq&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Looking to build an AI product?&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve helped many companies build AI products, the general workflow is:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build an Assistant&lt;/strong&gt; with proprietary data to perform tasks specific to your product.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Connect your product&lt;/strong&gt; to the Assistant via an API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Monitor and Improve&lt;/strong&gt; your AI product.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We also provide dedicated support and development, &lt;a href=&#34;https://cal.com/phidata/intro&#34;&gt;book a call&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re an open-source project and welcome contributions, please read the &lt;a href=&#34;https://github.com/phidatahq/phidata/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Request a feature&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have a feature request, please open an issue or make a pull request.&lt;/li&gt; &#xA; &lt;li&gt;If you have ideas on how we can improve, please create a discussion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Our roadmap is available &lt;a href=&#34;https://github.com/orgs/phidatahq/projects/2/views/1&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;. If you have a feature request, please open an issue/discussion.&lt;/p&gt;</summary>
  </entry>
</feed>