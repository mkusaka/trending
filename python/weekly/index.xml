<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-13T01:58:30Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>charlesbel/Microsoft-Rewards-Farmer</title>
    <updated>2023-08-13T01:58:30Z</updated>
    <id>tag:github.com,2023-08-13:/charlesbel/Microsoft-Rewards-Farmer</id>
    <link href="https://github.com/charlesbel/Microsoft-Rewards-Farmer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple bot that uses selenium to farm Microsoft Rewards written in Python&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://forthebadge.com/images/badges/made-with-python.svg?sanitize=true&#34; alt=&#34;Made with Python&#34;&gt; &lt;img src=&#34;http://ForTheBadge.com/images/badges/built-by-developers.svg?sanitize=true&#34; alt=&#34;Built by Developers&#34;&gt; &lt;img src=&#34;http://ForTheBadge.com/images/badges/uses-git.svg?sanitize=true&#34; alt=&#34;Uses Git&#34;&gt; &lt;img src=&#34;http://ForTheBadge.com/images/badges/built-with-love.svg?sanitize=true&#34; alt=&#34;Build with Love&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ascii&#34;&gt;███╗   ███╗███████╗    ███████╗ █████╗ ██████╗ ███╗   ███╗███████╗██████╗ &#xA;████╗ ████║██╔════╝    ██╔════╝██╔══██╗██╔══██╗████╗ ████║██╔════╝██╔══██╗&#xA;██╔████╔██║███████╗    █████╗  ███████║██████╔╝██╔████╔██║█████╗  ██████╔╝&#xA;██║╚██╔╝██║╚════██║    ██╔══╝  ██╔══██║██╔══██╗██║╚██╔╝██║██╔══╝  ██╔══██╗&#xA;██║ ╚═╝ ██║███████║    ██║     ██║  ██║██║  ██║██║ ╚═╝ ██║███████╗██║  ██║&#xA;╚═╝     ╚═╝╚══════╝    ╚═╝     ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝╚══════╝╚═╝  ╚═╝&#xA;       by Charles Bel (@charlesbel)          version 3.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?style=for-the-badge&#34; alt=&#34;Maintained&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge&#34; alt=&#34;MIT&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;👋&lt;/span&gt; Welcome to the future of automation&lt;/h2&gt; &#xA;&lt;h3&gt;A simple bot that uses selenium to farm Microsoft Rewards written in Python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- Use it at your own risk, Microsoft may ban your account (and I would not be responsible for it)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install requirements with the following command :&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure you have Chrome installed&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;del&gt;Install ChromeDriver:&lt;/del&gt;&lt;/p&gt; &lt;p&gt;You no longer need to do this step since selenium &amp;gt;=4.10.0 include a webdriver manager&lt;/p&gt; &lt;p&gt;To update your selenium version, run this command : &lt;code&gt;pip install selenium --upgrade&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Windows Only) Make sure Visual C++ redistributable DLLs are installed&lt;/p&gt; &lt;p&gt;If they&#39;re not, install the current &#34;vc_redist.exe&#34; from this link and reboot your computer : &lt;a href=&#34;https://learn.microsoft.com/en-GB/cpp/windows/latest-supported-vc-redist?view=msvc-170&#34;&gt;https://learn.microsoft.com/en-GB/cpp/windows/latest-supported-vc-redist?view=msvc-170&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Edit the &lt;code&gt;accounts.json.sample&lt;/code&gt; with your accounts credentials and rename it by removing &lt;code&gt;.sample&lt;/code&gt; at the end (ex. &lt;code&gt;accounts.json&lt;/code&gt;)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you want to add more than one account, the syntax is the following:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    [&#xA;        {&#xA;            &#34;username&#34;: &#34;Your Email 1&#34;,&#xA;            &#34;password&#34;: &#34;Your Password 1&#34;,&#xA;            &#34;proxy&#34;: &#34;http://user:pass@host1:port&#34;&#xA;        },&#xA;        {&#xA;            &#34;username&#34;: &#34;Your Email 2&#34;,&#xA;            &#34;password&#34;: &#34;Your Password 2&#34;,&#xA;            &#34;proxy&#34;: &#34;http://user:pass@host2:port&#34;&#xA;        }&#xA;    ]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the script:&lt;/p&gt; &lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Or if you want to keep it updated (it will check on each run if a new version is available, if so, will download and run it), use :&lt;/p&gt; &lt;p&gt;&lt;code&gt;python autoupdate_main.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Launch arguments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;-v/--visible to disable headless&lt;/li&gt; &#xA; &lt;li&gt;-l/--lang to force a language (ex: en)&lt;/li&gt; &#xA; &lt;li&gt;-g/--geo to force a geolocation (ex: US)&lt;/li&gt; &#xA; &lt;li&gt;-p/--proxy to add a proxy to the whole program, supports http/https/socks4/socks5 (overrides per-account proxy in accounts.json) (ex: &lt;a href=&#34;http://user:pass@host:port&#34;&gt;http://user:pass@host:port&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bing searches (Desktop, Mobile and Edge) with User-Agents&lt;/li&gt; &#xA; &lt;li&gt;Complete automatically the daily set&lt;/li&gt; &#xA; &lt;li&gt;Complete automatically punch cards&lt;/li&gt; &#xA; &lt;li&gt;Complete automatically the others promotions&lt;/li&gt; &#xA; &lt;li&gt;Headless Mode&lt;/li&gt; &#xA; &lt;li&gt;Multi-Account Management&lt;/li&gt; &#xA; &lt;li&gt;Session storing (3.0)&lt;/li&gt; &#xA; &lt;li&gt;2FA Support (3.0)&lt;/li&gt; &#xA; &lt;li&gt;Proxy Support (3.0)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Future Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GUI&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>geekan/MetaGPT</title>
    <updated>2023-08-13T01:58:30Z</updated>
    <id>tag:github.com,2023-08-13:/geekan/MetaGPT</id>
    <link href="https://github.com/geekan/MetaGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🌟 The Multi-Agent Framework: Given one line Requirement, return PRD, Design, Tasks, Repo&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MetaGPT: The Multi-Agent Framework&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/MetaGPT-logo.jpeg&#34; alt=&#34;MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.&#34; width=&#34;150px&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Assign different roles to GPTs to form a collaborative software entity for complex tasks.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_CN.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/文档-中文版-blue.svg&#34; alt=&#34;CN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/document-English-blue.svg?sanitize=true&#34; alt=&#34;EN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_JA.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ドキュメント-日本語-blue.svg&#34; alt=&#34;JA doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/wCp6Q3fsAk&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/wCp6Q3fsAk?compact=true&amp;amp;style=flat&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ROADMAP.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ROADMAP-路线图-blue&#34; alt=&#34;roadmap&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/MetaGPT-WeChat-Personal.jpeg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WeChat-微信-blue&#34; alt=&#34;roadmap&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/DeepWisdom2019&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/MetaGPT?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;MetaGPT takes a &lt;strong&gt;one line requirement&lt;/strong&gt; as input and outputs &lt;strong&gt;user stories / competitive analysis / requirements / data structures / APIs / documents, etc.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Internally, MetaGPT includes &lt;strong&gt;product managers / architects / project managers / engineers.&lt;/strong&gt; It provides the entire process of a &lt;strong&gt;software company along with carefully orchestrated SOPs.&lt;/strong&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;Code = SOP(Team)&lt;/code&gt; is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/software_company_cd.jpeg&#34; alt=&#34;A software company consists of LLM-based roles&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Software Company Multi-Role Schematic (Gradually Implementing)&lt;/p&gt; &#xA;&lt;h2&gt;Examples (fully generated by GPT-4)&lt;/h2&gt; &#xA;&lt;p&gt;For example, if you type &lt;code&gt;python startup.py &#34;Design a RecSys like Toutiao&#34;&lt;/code&gt;, you would get many outputs, one of them is data &amp;amp; api design&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/workspace/content_rec_sys/resources/data_api_design.png&#34; alt=&#34;Jinri Toutiao Recsys Data &amp;amp; API Design&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It costs approximately &lt;strong&gt;$0.2&lt;/strong&gt; (in GPT-4 API fees) to generate one example with analysis and design, and around &lt;strong&gt;$2.0&lt;/strong&gt; for a full project.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Traditional Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Step 1: Ensure that NPM is installed on your system. Then install mermaid-js.&#xA;npm --version&#xA;sudo npm install -g @mermaid-js/mermaid-cli&#xA;&#xA;# Step 2: Ensure that Python 3.9+ is installed on your system. You can check this by using:&#xA;python --version&#xA;&#xA;# Step 3: Clone the repository to your local machine, and install it.&#xA;git clone https://github.com/geekan/metagpt&#xA;cd metagpt&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If already have Chrome, Chromium, or MS Edge installed, you can skip downloading Chromium by setting the environment variable &lt;code&gt;PUPPETEER_SKIP_CHROMIUM_DOWNLOAD&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Some people are &lt;a href=&#34;https://github.com/mermaidjs/mermaid.cli/issues/15&#34;&gt;having issues&lt;/a&gt; installing this tool globally. Installing it locally is an alternative solution,&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @mermaid-js/mermaid-cli&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;don&#39;t forget to the configuration for mmdc in config.yml&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;PUPPETEER_CONFIG: &#34;./config/puppeteer-config.json&#34;&#xA;MMDC: &#34;./node_modules/.bin/mmdc&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation by Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Step 1: Download metagpt official image and prepare config.yaml&#xA;docker pull metagpt/metagpt:v0.3.1&#xA;mkdir -p /opt/metagpt/{config,workspace}&#xA;docker run --rm metagpt/metagpt:v0.3.1 cat /app/metagpt/config/config.yaml &amp;gt; /opt/metagpt/config/key.yaml&#xA;vim /opt/metagpt/config/key.yaml # Change the config&#xA;&#xA;# Step 2: Run metagpt demo with container&#xA;docker run --rm \&#xA;    --privileged \&#xA;    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \&#xA;    -v /opt/metagpt/workspace:/app/metagpt/workspace \&#xA;    metagpt/metagpt:v0.3.1 \&#xA;    python startup.py &#34;Write a cli snake game&#34;&#xA;&#xA;# You can also start a container and execute commands in it&#xA;docker run --name metagpt -d \&#xA;    --privileged \&#xA;    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \&#xA;    -v /opt/metagpt/workspace:/app/metagpt/workspace \&#xA;    metagpt/metagpt:v0.3.1&#xA;&#xA;docker exec -it metagpt /bin/bash&#xA;$ python startup.py &#34;Write a cli snake game&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The command &lt;code&gt;docker run ...&lt;/code&gt; do the following things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run in privileged mode to have permission to run the browser&lt;/li&gt; &#xA; &lt;li&gt;Map host directory &lt;code&gt;/opt/metagpt/config&lt;/code&gt; to container directory &lt;code&gt;/app/metagpt/config&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Map host directory &lt;code&gt;/opt/metagpt/workspace&lt;/code&gt; to container directory &lt;code&gt;/app/metagpt/workspace&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute the demo command &lt;code&gt;python startup.py &#34;Write a cli snake game&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build image by yourself&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# You can also build metagpt image by yourself.&#xA;git clone https://github.com/geekan/MetaGPT.git&#xA;cd MetaGPT &amp;amp;&amp;amp; docker build -t metagpt:custom .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Configure your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in any of &lt;code&gt;config/key.yaml / config/config.yaml / env&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Priority order: &lt;code&gt;config/key.yaml &amp;gt; config/config.yaml &amp;gt; env&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Copy the configuration file and make the necessary modifications.&#xA;cp config/config.yaml config/key.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Variable Name&lt;/th&gt; &#xA;   &lt;th&gt;config/key.yaml&lt;/th&gt; &#xA;   &lt;th&gt;env&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPENAI_API_KEY # Replace with your own key&lt;/td&gt; &#xA;   &lt;td&gt;OPENAI_API_KEY: &#34;sk-...&#34;&lt;/td&gt; &#xA;   &lt;td&gt;export OPENAI_API_KEY=&#34;sk-...&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPENAI_API_BASE # Optional&lt;/td&gt; &#xA;   &lt;td&gt;OPENAI_API_BASE: &#34;https://&amp;lt;YOUR_SITE&amp;gt;/v1&#34;&lt;/td&gt; &#xA;   &lt;td&gt;export OPENAI_API_BASE=&#34;https://&amp;lt;YOUR_SITE&amp;gt;/v1&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tutorial: Initiating a startup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python startup.py &#34;Write a cli snake game&#34;&#xA;# Use code review will cost more money, but will opt for better code quality.&#xA;python startup.py &#34;Write a cli snake game&#34; --code_review True &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the script, you can find your new project in the &lt;code&gt;workspace/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;Preference of Platform or Tool&lt;/h3&gt; &#xA;&lt;p&gt;You can tell which platform or tool you want to use when stating your requirements.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python startup.py &#34;Write a cli snake game based on pygame&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;NAME&#xA;    startup.py - We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.&#xA;&#xA;SYNOPSIS&#xA;    startup.py IDEA &amp;lt;flags&amp;gt;&#xA;&#xA;DESCRIPTION&#xA;    We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.&#xA;&#xA;POSITIONAL ARGUMENTS&#xA;    IDEA&#xA;        Type: str&#xA;        Your innovative idea, such as &#34;Creating a snake game.&#34;&#xA;&#xA;FLAGS&#xA;    --investment=INVESTMENT&#xA;        Type: float&#xA;        Default: 3.0&#xA;        As an investor, you have the opportunity to contribute a certain dollar amount to this AI company.&#xA;    --n_round=N_ROUND&#xA;        Type: int&#xA;        Default: 5&#xA;&#xA;NOTES&#xA;    You can also use flags syntax for POSITIONAL ARGUMENTS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Code walkthrough&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from metagpt.software_company import SoftwareCompany&#xA;from metagpt.roles import ProjectManager, ProductManager, Architect, Engineer&#xA;&#xA;async def startup(idea: str, investment: float = 3.0, n_round: int = 5):&#xA;    &#34;&#34;&#34;Run a startup. Be a boss.&#34;&#34;&#34;&#xA;    company = SoftwareCompany()&#xA;    company.hire([ProductManager(), Architect(), ProjectManager(), Engineer()])&#xA;    company.invest(investment)&#xA;    company.start_project(idea)&#xA;    await company.run(n_round=n_round)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can check &lt;code&gt;examples&lt;/code&gt; for more details on single role (with knowledge base) and LLM only examples.&lt;/p&gt; &#xA;&lt;h2&gt;QuickStart&lt;/h2&gt; &#xA;&lt;p&gt;It is difficult to install and configure the local environment for some users. The following tutorials will allow you to quickly experience the charm of MetaGPT.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deepwisdom.feishu.cn/wiki/CyY9wdJc4iNqArku3Lncl4v8n2b&#34;&gt;MetaGPT quickstart&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;For now, cite the &lt;a href=&#34;https://arxiv.org/abs/2308.00352&#34;&gt;Arxiv paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{hong2023metagpt,&#xA;      title={MetaGPT: Meta Programming for Multi-Agent Collaborative Framework}, &#xA;      author={Sirui Hong and Xiawu Zheng and Jonathan Chen and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu},&#xA;      year={2023},&#xA;      eprint={2308.00352},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Information&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; &lt;a href=&#34;mailto:alexanderwu@fuzhi.ai&#34;&gt;alexanderwu@fuzhi.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GitHub Issues:&lt;/strong&gt; For more technical inquiries, you can also create a new issue in our &lt;a href=&#34;https://github.com/geekan/metagpt/issues&#34;&gt;GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We will respond to all questions within 2-3 business days.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d&#34;&gt;https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenBMB/ToolBench</title>
    <updated>2023-08-13T01:58:30Z</updated>
    <id>tag:github.com,2023-08-13:/OpenBMB/ToolBench</id>
    <link href="https://github.com/OpenBMB/ToolBench" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open platform for training, serving, and evaluating large language model for tool learning.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; 🛠️ToolBench🤖&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Tool_Num-3451-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/API_Num-16464-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Current_Dataset_Size-12K-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Total_API_Call-37K-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Average_Reasoning_Traces-4.1-red?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Tool_LLaMA-Released-green?style=flat-square&#34; alt=&#34;Dialogues&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#model&#34;&gt;Model&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#data&#34;&gt;Data Release&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#web-ui&#34;&gt;Web Demo&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#tool-eval&#34;&gt;Tool Eval&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/paper.pdf&#34;&gt;Paper&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/#citation&#34;&gt;Citation&lt;/a&gt; &lt;/p&gt;  &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://cdn.discordapp.com/attachments/941582479117127680/1111543600879259749/20230526075532.png&#34; width=&#34;350px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;🔨This project (ToolLLM) aims to construct &lt;strong&gt;open-source, large-scale, high-quality&lt;/strong&gt; instruction tuning SFT data to facilitate the construction of powerful LLMs with general &lt;strong&gt;tool-use&lt;/strong&gt; capability. We aim to empower open-source LLMs to master thousands of diverse real-world APIs. We achieve this by collecting a high-quality instruction-tuning dataset. It is constructed automatically using the latest ChatGPT (gpt-3.5-turbo-16k), which is upgraded with enhanced &lt;a href=&#34;https://openai.com/blog/function-calling-and-other-api-updates&#34;&gt;function call&lt;/a&gt; capabilities. We provide the dataset, the corresponding training and evaluation scripts, and a capable model ToolLLaMA fine-tuned on ToolBench.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;💁‍♂️💁💁‍♀️ Join Us on &lt;a href=&#34;https://discord.gg/asjtEkAA&#34;&gt;Discord&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Read this in &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/README_ZH.md&#34;&gt;中文&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023/8/8]&lt;/strong&gt; No more hallucination! &lt;a href=&#34;https://huggingface.co/ToolBench/ToolLLaMA-2-7b&#34;&gt;&lt;strong&gt;ToolLLaMA-2-7b&lt;/strong&gt;&lt;/a&gt; (fine-tuned from LLaMA-2-7b) is released with lower API hallucination than ChatGPT.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023/8/4]&lt;/strong&gt; We provide &lt;strong&gt;RapidAPI backend service&lt;/strong&gt; to free you from using your own RapidAPI key and subscribing the APIs. Please fill out our &lt;a href=&#34;https://forms.gle/oCHHc8DQzhGfiT9r6&#34;&gt;form&lt;/a&gt;. We will review it as soon as possible and send you the ToolBench key to get start on it!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023/8/1]&lt;/strong&gt; Our &lt;a href=&#34;https://arxiv.org/abs/2307.16789&#34;&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt; is released.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023/7/27]&lt;/strong&gt; &lt;strong&gt;New version&lt;/strong&gt; ToolBench is released.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;✨Here is an overview of the dataset construction, training, and evaluation.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/overview.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;✨✨Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Collection&lt;/strong&gt;: we gather &lt;strong&gt;16464&lt;/strong&gt; representational state transfer (REST) APIs from &lt;a href=&#34;https://rapidapi.com/hub&#34;&gt;RapidAPI&lt;/a&gt;, a platform that hosts massive real-world APIs provided by developers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Instruction Generation&lt;/strong&gt;: we curate instructions that involve both &lt;strong&gt;single-tool&lt;/strong&gt; and &lt;strong&gt;multi-tool&lt;/strong&gt; scenarios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Answer Annotation&lt;/strong&gt;: we develop a novel &lt;strong&gt;depth-first search based decision tree&lt;/strong&gt; (DFSDT) to bolster the planning and reasoning ability of LLMs, which significantly improves the annotation efficiency and successfully annotates those complex instructions that cannot be answered with CoT or ReACT. We provide responses that not only include the final answer but also incorporate the model&#39;s &lt;strong&gt;reasoning process, tool execution, and tool execution results&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Retriver&lt;/strong&gt;: we incorporate API retrieval to equip ToolLLaMA with open-domain tool-using abilities.&lt;/li&gt; &#xA; &lt;li&gt;All the data is automatically generated by OpenAI API and filtered by us, the whole data creation process is easy to scale up.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/comparison.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;We also provide &lt;strong&gt;A demo of using ToolLLaMA&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/OpenBMB/ToolBench/assets/25274507/f1151d85-747b-4fac-92ff-6c790d8d9a31&#34;&gt;https://github.com/OpenBMB/ToolBench/assets/25274507/f1151d85-747b-4fac-92ff-6c790d8d9a31&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Currently, our ToolLLaMA has reached the performance of ChatGPT (turbo-16k) in tool use, in the future, &lt;em&gt;we will continually improve the data quality and increase the coverage of real-world tools.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/performance.png&#34; width=&#34;300px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Here is the &lt;em&gt;&lt;a href=&#34;https://github.com/OpenBMB/ToolBench/tree/legacy&#34;&gt;Old version&lt;/a&gt;&lt;/em&gt; of ToolBench.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;👐ToolBench is intended solely for research and educational purposes and should not be construed as reflecting the opinions or views of the creators, owners, or contributors of this dataset. It is distributed under &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34;&gt;CC BY NC 4.0 License&lt;/a&gt;. Below is the statistics of the data :&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Tool Nums&lt;/th&gt; &#xA;   &lt;th&gt;API Nums&lt;/th&gt; &#xA;   &lt;th&gt;Instance Nums&lt;/th&gt; &#xA;   &lt;th&gt;Real API Call&lt;/th&gt; &#xA;   &lt;th&gt;Reasoning Traces&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3451&lt;/td&gt; &#xA;   &lt;td&gt;16464&lt;/td&gt; &#xA;   &lt;td&gt;12657&lt;/td&gt; &#xA;   &lt;td&gt;37204&lt;/td&gt; &#xA;   &lt;td&gt;4.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We crawl 16000+ real-world APIs from &lt;a href=&#34;https://rapidapi.com/hub&#34;&gt;RapidAPI&lt;/a&gt;, and curate realistic human instructions that involve them. Below we present a hierarchy of RapidAPI and our instruction generation process.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/instructiongeneration.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;ToolBench contains both single-tool and multi-tool scenarios. The multi-tool scenarios can be further categorized into intra-category multi-tool and intra-collection multi-tool. We utilize DFSDT method for all scenarios to our data creation. Here is an illustration for the data creation process using DFSDT method:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/answer_anno.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Data Release&lt;/h3&gt; &#xA;&lt;p&gt;Please download our dataset using the following link: &lt;a href=&#34;https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://cloud.tsinghua.edu.cn/f/c9e50625743b40bfbe10/&#34;&gt;Tsinghua Cloud&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;G1&lt;/code&gt;,&lt;code&gt;G2&lt;/code&gt;, &lt;code&gt;G3&lt;/code&gt;data refers to single-tool, intra-category multi-tool and intra-collection multi-tool data respectively. We also have an &lt;a href=&#34;https://atlas.nomic.ai/map/58aca169-c29a-447a-8f01-0d418fc4d341/030ddad7-5305-461c-ba86-27e1ca79d899&#34;&gt;Atlas Explorer&lt;/a&gt; for visualization.&lt;/li&gt; &#xA; &lt;li&gt;We split the G1, G2 and G3 data into train, eval and test parts respectively and combine the train data for training in our main experiments. &lt;code&gt;toolllama_G123_dfs_train.json&lt;/code&gt; refers to the combined train data.&lt;/li&gt; &#xA; &lt;li&gt;The tool environment related data is in &lt;code&gt;toolenv&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;We sample 100 instances from every test set. The &lt;code&gt;test_query_ids&lt;/code&gt; directory contains query ids of the test instances in each test set.&lt;/li&gt; &#xA; &lt;li&gt;The data used for tool retrieval is included in the &lt;code&gt;retrieval&lt;/code&gt; directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🤖Model&lt;/h2&gt; &#xA;&lt;p&gt;We release the &lt;a href=&#34;https://huggingface.co/ToolBench/ToolLLaMA-7b&#34;&gt;ToolLLaMA-7b&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/ToolBench/ToolLLaMA-7b-LoRA&#34;&gt;ToolLLaMA-7b-LoRA&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/ToolBench/ToolLLaMA-2-7b&#34;&gt;ToolLLaMA-2-7b&lt;/a&gt; models, which are both trained on the released dataset in a multi-task fashion. We also release the &lt;a href=&#34;https://huggingface.co/ToolBench/ToolBench_IR_bert_based_uncased&#34;&gt;tool retriever&lt;/a&gt; trained under our experimental setting.&lt;/p&gt; &#xA;&lt;h2&gt;🚀Fine-tuning&lt;/h2&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repository and navigate to the ToolBench folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:OpenBMB/ToolBench.git&#xA;cd ToolBench&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install Package (python&amp;gt;=3.9)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for ToolEval only&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r toolbench/tooleval/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Prepare the data and tool environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget --no-check-certificate &#39;https://docs.google.com/uc?export=download&amp;amp;id=1Vis-RxBstXLKC1W1agIQUJNuumPJrrw0&amp;amp;confirm=yes&#39; -O data.zip&#xA;unzip data.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training Retriever&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data preprocessing:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python preprocess/preprocess_retriever_data.py \&#xA;    --query_file data/instruction/G1_query.json \&#xA;    --index_file data/test_query_ids/G1_instruction_test_query_ids.json \&#xA;    --dataset_name G1 \&#xA;    --output_dir data/retrieval/G1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Then run the following command to train the tool retriever:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/retrieval/train.py \&#xA;    --data_path data/retrieval/G1/ \&#xA;    --model_name bert-base-uncased \&#xA;    --output_path retrieval_model \&#xA;    --num_epochs 5 \&#xA;    --train_batch_size 32 \&#xA;    --learning_rate 2e-5 \&#xA;    --warmup_steps 500 \&#xA;    --max_seq_length 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training ToolLLaMA&lt;/h3&gt; &#xA;&lt;p&gt;Our training code is based on &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;. You can use the following command to train ToolLLaMA-7b with 2 x A100 (80GB), with the preprocessed data in our &lt;a href=&#34;https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J&#34;&gt;data link&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;torchrun --nproc_per_node=2 --master_port=20001 toolbench/train/train_long_seq.py \&#xA;    --model_name_or_path huggyllama/llama-7b  \&#xA;    --data_path  data/toolllama_G123_dfs_train.json \&#xA;    --eval_data_path  data/toolllama_G123_dfs_eval.json \&#xA;    --conv_template tool-llama-single-round \&#xA;    --bf16 True \&#xA;    --output_dir toolllama \&#xA;    --num_train_epochs 2 \&#xA;    --per_device_train_batch_size 2 \&#xA;    --per_device_eval_batch_size 2 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;epoch&#34; \&#xA;    --prediction_loss_only \&#xA;    --save_strategy &#34;epoch&#34; \&#xA;    --save_total_limit 8 \&#xA;    --learning_rate 5e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.04 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --tf32 True \&#xA;    --model_max_length 8192 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to none&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also preprocess and split the data in your own way with this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python preprocess/preprocess_toolllama_data.py \&#xA;    --tool_data_dir data/answer/G1_answer \&#xA;    --method DFS_woFilter_w2 \&#xA;    --output_file data/answer/toolllama_G1_dfs.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train lora version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;deepspeed --master_port=20001 toolbench/train/train_long_seq_lora.py \&#xA;    --model_name_or_path huggyllama/llama-7b  \&#xA;    --data_path  data/toolllama_G123_dfs_train.json \&#xA;    --eval_data_path  data/toolllama_G123_dfs_eval.json \&#xA;    --conv_template tool-llama-single-round \&#xA;    --bf16 True \&#xA;    --output_dir toolllama_lora \&#xA;    --num_train_epochs 5 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 2 \&#xA;    --gradient_accumulation_steps 2 \&#xA;    --evaluation_strategy &#34;epoch&#34; \&#xA;    --prediction_loss_only \&#xA;    --save_strategy &#34;epoch&#34; \&#xA;    --save_total_limit 8 \&#xA;    --learning_rate 5e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.04 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --model_max_length 8192 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \    &#xA;    --deepspeed ds_configs/stage2.json \&#xA;    --report_to none&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference With Our RapidAPI Server&lt;/h2&gt; &#xA;&lt;p&gt;Please fill out the &lt;a href=&#34;https://forms.gle/oCHHc8DQzhGfiT9r6&#34;&gt;form&lt;/a&gt; first and after reviewing we will send you the toolbench key. Then prepare your toolbench key by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export TOOLBENCH_KEY=&#34;your_toolbench_key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For ToolLLaMA&lt;/h3&gt; &#xA;&lt;p&gt;To inference with ToolLLaMA, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model toolllama \&#xA;    --model_path ToolBench/ToolLLaMA-7b \&#xA;    --max_observation_length 1024 \&#xA;    --observ_compress_method truncate \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/toolllama_dfs \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;strong&gt;ToolLLaMA-LoRA&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model toolllama \&#xA;    --model_path huggyllama/llama-7b \&#xA;    --lora \&#xA;    --lora_path /path/to/your/downloaded/ToolLLaMA-7b-LoRA \&#xA;    --max_observation_length 1024 \&#xA;    --observ_compress_method truncate \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/toolllama_lora_dfs \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For ToolLLaMA-LoRA under &lt;strong&gt;open-domain&lt;/strong&gt; setting, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline_open_domain.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --corpus_tsv_path data/retrieval/G1/corpus.tsv \&#xA;    --retrieval_model_path /path/to/your/retrival_model \&#xA;    --retrieved_api_nums 5 \&#xA;    --backbone_model toolllama \&#xA;    --model_path huggyllama/llama-7b \&#xA;    --lora \&#xA;    --lora_path /path/to/your/toolllama_lora \&#xA;    --max_observation_length 1024 \&#xA;    --observ_compress_method truncate \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo_open_domain.json \&#xA;    --output_answer_file data/answer/toolllama_lora_dfs_open_domain \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For OpenAI Models&lt;/h3&gt; &#xA;&lt;p&gt;To use ChatGPT, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export TOOLBENCH_KEY=&#34;&#34;&#xA;export OPENAI_KEY=&#34;&#34;&#xA;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model chatgpt_function \&#xA;    --openai_key $OPENAI_KEY \&#xA;    --max_observation_length 1024 \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/chatgpt_dfs \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use Text-Davinci-003, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export TOOLBENCH_KEY=&#34;&#34;&#xA;export OPENAI_KEY=&#34;&#34;&#xA;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model davinci \&#xA;    --openai_key $OPENAI_KEY \&#xA;    --max_observation_length 1024 \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/davinci_dfs \&#xA;    --toolbench_key $TOOLBENCH_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference With Your Own RapidAPI Account&lt;/h2&gt; &#xA;&lt;p&gt;To do inference with customized RapidAPI account, pass your &lt;strong&gt;rapidapi key&lt;/strong&gt; through &lt;code&gt;rapidapi_key&lt;/code&gt; and specify the &lt;code&gt;use_rapidapi_key&lt;/code&gt; argument in the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export RAPIDAPI_KEY=&#34;&#34;&#xA;export OPENAI_KEY=&#34;&#34;&#xA;export PYTHONPATH=./&#xA;python toolbench/inference/qa_pipeline.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --backbone_model chatgpt_function \&#xA;    --openai_key $OPENAI_KEY \&#xA;    --max_observation_length 1024 \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo.json \&#xA;    --output_answer_file data/answer/chatgpt_dfs \&#xA;    --rapidapi_key $RAPIDAPI_KEY \&#xA;    --use_rapidapi_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setting up and running the interface&lt;/h2&gt; &#xA;&lt;p&gt;ToolBench contains a Web UI based on &lt;a href=&#34;https://github.com/mckaywrigley/chatbot-ui&#34;&gt;Chatbot UI&lt;/a&gt;, forked to include the use of tools in the interface. It comes in two parts: the backend server, and &lt;a href=&#34;https://github.com/lilbillybiscuit/chatbot-ui-toolllama&#34;&gt;chatbot-ui-toolllama&lt;/a&gt;. Here is a &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/ToolBench/master/assets/toolbench-demo.mp4&#34;&gt;video demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/lilbillybiscuit/chatbot-ui-toolllama&#xA;cd chatbot-ui-toolllama&#xA;npm install&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The app will be available on &lt;code&gt;http://localhost:3000/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Backend server&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=./&#xA;python toolbench/inference/toolbench_server.py \&#xA;    --tool_root_dir data/toolenv/tools/ \&#xA;    --corpus_tsv_path data/retrieval/G1/corpus.tsv \&#xA;    --retrieval_model_path /path/to/your/retrival_model \&#xA;    --retrieved_api_nums 5 \&#xA;    --backbone_model toolllama \&#xA;    --model_path huggyllama/llama-7b \&#xA;    --lora \&#xA;    --lora_path /path/to/your/toolllama_lora \&#xA;    --max_observation_length 1024 \&#xA;    --method DFS_woFilter_w2 \&#xA;    --input_query_file data/instruction/inference_query_demo_open_domain.json \&#xA;    --output_answer_file data/answer/toolllama_lora_dfs_open_domain \&#xA;    --rapidapi_key $RAPIDAPIKEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This server will be available on &lt;code&gt;http://localhost:5000/&lt;/code&gt;. To start a request, call &lt;code&gt;http://localhost:5000/stream&lt;/code&gt; with a GET or POST request containing a JSON object with the following fields:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;text&#34;: &#34;What is the weather in New York today?&#34;,&#xA;    &#34;top_k&#34;: 5,&#xA;    &#34;method&#34;: &#34;DFS_woFilter_w2&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ToolEval&lt;/h2&gt; &#xA;&lt;p&gt;By fine-tuning LLaMA on ToolBench, we obtain &lt;strong&gt;ToolLLaMA&lt;/strong&gt;. Considering that human evaluation can be time-consuming, we follow &lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;AlpacaEval&lt;/a&gt; to develop an efficient machine evaluator &lt;strong&gt;ToolEval&lt;/strong&gt;, which incorporates two evaluation metrics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pass Rate&lt;/strong&gt;: Calculates the proportion of successfully completing an instruction within limited OpenAI API calls.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Preference&lt;/strong&gt;: Measured by comparing two answers (action sequences) for a given instruction. We pre-define a set of criteria for a better answer, which are organized as prompts for ChatGPT. We provide the test instruction and two candidate answers to the evaluator and obtain its preference. We evaluate each answer pair multiple times to improve the reliability of our system. Then we calculate the &lt;strong&gt;Win Rate&lt;/strong&gt; (percentage of being preferred by the evaluator) and &lt;strong&gt;Standard Error&lt;/strong&gt; (the standard error of the Win Rate). More details can be found in our paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To validate the effectiveness of the metric &lt;strong&gt;Preference&lt;/strong&gt;, we sample among three different methods (ChatGPT+ReACT, GPT4+ReACT, and ChatGPT+DFSDT) to obtain answer pairs for &lt;em&gt;600&lt;/em&gt; test instructions. Then we engage humans to annotate human preference for them (&lt;em&gt;4&lt;/em&gt; annotations for each answer pair, &lt;em&gt;2400&lt;/em&gt; annotations in total). Our automatic evaluator, developed using ChatGPT, demonstrates a significant correlation of &lt;strong&gt;75.8%&lt;/strong&gt; with human annotators. We also obtain the agreement among different human annotators &lt;strong&gt;83.54%&lt;/strong&gt;, and the agreement between humans and our evaluator &lt;strong&gt;80.21%&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More details about ToolEval can be found in our paper.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation with ToolEval&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate a model on G1-Inst. test set, for example, run the following commands.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pass rate:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python toolbench/tooleval/pass_rate.py --answer_dir data/answer/toolllama_dfs/G1_instruction&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Win rate (Reference model: ChatGPT-ReACT):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_KEY=&#34;&#34;&#xA;export REF_MODEL_DATA=&#34;data/answer/chatgpt_cot/G1_instruction&#34;&#xA;export REF_MODEL_METHOD=&#34;CoT&#34;&#xA;export TEST_MODEL_DATA=&#34;data/answer/toolllama_dfs/G1_instruction&#34;&#xA;export TEST_MODEL_METHOD=&#34;DFS&#34;&#xA;python ./toolbench/tooleval/convert_to_answer_format.py \&#xA;    --method CoT \&#xA;    --answer_dir $REF_MODEL_DATA \&#xA;    --output ${REF_MODEL_DATA}_converted&#xA;&#xA;python ./toolbench/tooleval/convert_to_answer_format.py \&#xA;    --method DFS \&#xA;    --answer_dir $TEST_MODEL_DATA \&#xA;    --output ${TEST_MODEL_DATA}_converted&#xA;&#xA;python ./toolbench/tooleval/automatic_eval_sample.py \&#xA;    --output ${REF_MODEL_DATA}_converted \&#xA;    --ref_output ${TEST_MODEL_DATA}_converted \&#xA;    --method $REF_MODEL_METHOD \&#xA;    --use_existed_output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/OpenBMB/ToolBench/tree/master/toolbench/tooleval&#34;&gt;ToolEval&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;📊 Model Experiments Results&lt;/h3&gt; &#xA;&lt;p&gt;In our main experiments, ToolLLaMA demonstrates a compelling capability to handle both single-tool and complex multi-tool instructions. We introduce &lt;strong&gt;hallucinate rate&lt;/strong&gt;(lower is better) evaluation metric as a complement of ToolEval. An instance is considered to be a hallucinate instance, as long as the whole decision tree contains at least one hallucinated function call. Below are the main results compared with ChatGPT and Text-Davinci-003.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hallucinate rate:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;I1-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Tool.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I3-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-Davinci-003-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6.0&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;27&lt;/td&gt; &#xA;   &lt;td&gt;20.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;61&lt;/td&gt; &#xA;   &lt;td&gt;62&lt;/td&gt; &#xA;   &lt;td&gt;52&lt;/td&gt; &#xA;   &lt;td&gt;69&lt;/td&gt; &#xA;   &lt;td&gt;67&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;62.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-API Retriever&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;td&gt;37&lt;/td&gt; &#xA;   &lt;td&gt;23.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-2&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;8.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pass Rate:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;I1-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Tool.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I3-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;78&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;84&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;89&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;58&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;57&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;69.6&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT-ReACT&lt;/td&gt; &#xA;   &lt;td&gt;56&lt;/td&gt; &#xA;   &lt;td&gt;62&lt;/td&gt; &#xA;   &lt;td&gt;66&lt;/td&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;44.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-Davinci-003-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;53&lt;/td&gt; &#xA;   &lt;td&gt;58&lt;/td&gt; &#xA;   &lt;td&gt;61&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;39&lt;/td&gt; &#xA;   &lt;td&gt;47.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-Davinci-003-ReACT&lt;/td&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;18.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA&lt;/td&gt; &#xA;   &lt;td&gt;68&lt;/td&gt; &#xA;   &lt;td&gt;80&lt;/td&gt; &#xA;   &lt;td&gt;75&lt;/td&gt; &#xA;   &lt;td&gt;47&lt;/td&gt; &#xA;   &lt;td&gt;56&lt;/td&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;   &lt;td&gt;61.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;51&lt;/td&gt; &#xA;   &lt;td&gt;63&lt;/td&gt; &#xA;   &lt;td&gt;61&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;42&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;50.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-API Retriever&lt;/td&gt; &#xA;   &lt;td&gt;62&lt;/td&gt; &#xA;   &lt;td&gt;62&lt;/td&gt; &#xA;   &lt;td&gt;72&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;55&lt;/td&gt; &#xA;   &lt;td&gt;47&lt;/td&gt; &#xA;   &lt;td&gt;57.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-2&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;72&lt;/td&gt; &#xA;   &lt;td&gt;78&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;51&lt;/td&gt; &#xA;   &lt;td&gt;46&lt;/td&gt; &#xA;   &lt;td&gt;59.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Win Rate:&lt;/strong&gt; (Reference model: ChatGPT-DFSDT)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;I1-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Tool.&lt;/th&gt; &#xA;   &lt;th&gt;I1-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;I2-Cat.&lt;/th&gt; &#xA;   &lt;th&gt;I3-Inst.&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;50.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT-ReACT&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;41&lt;/td&gt; &#xA;   &lt;td&gt;43&lt;/td&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;td&gt;30.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-Davinci-003-ReACT&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;13.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-Davinci-003-DFSDT&lt;/td&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;34&lt;/td&gt; &#xA;   &lt;td&gt;43&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;31.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;59&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;48&lt;/td&gt; &#xA;   &lt;td&gt;46&lt;/td&gt; &#xA;   &lt;td&gt;48.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;43&lt;/td&gt; &#xA;   &lt;td&gt;36.4&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;42&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;51&lt;/td&gt; &#xA;   &lt;td&gt;41.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-API Retriever&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;39&lt;/td&gt; &#xA;   &lt;td&gt;44&lt;/td&gt; &#xA;   &lt;td&gt;49&lt;/td&gt; &#xA;   &lt;td&gt;49&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;55&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;47.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLLaMA-2&lt;/td&gt; &#xA;   &lt;td&gt;43&lt;/td&gt; &#xA;   &lt;td&gt;42&lt;/td&gt; &#xA;   &lt;td&gt;46&lt;/td&gt; &#xA;   &lt;td&gt;55&lt;/td&gt; &#xA;   &lt;td&gt;46&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;47.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ToolLLaMA will reach GPT-4&#39;s tool-use capability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources of Tool Learning&lt;/h2&gt; &#xA;&lt;p&gt;With the powerful capabilities of foundation models, we are eager to see their applications in manipulating various tools. For more resources, please refer to the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;BMTools&lt;/strong&gt;. [&lt;a href=&#34;https://github.com/OpenBMB/BMTools&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Learning Survey&lt;/strong&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2304.08354&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Learning Paper List&lt;/strong&gt;. [&lt;a href=&#34;https://github.com/thunlp/ToolLearningPapers&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebCPM&lt;/strong&gt;. [&lt;a href=&#34;https://github.com/thunlp/WebCPM&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to cite us if you like ToolBench.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{qin2023toolllm,&#xA;      title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, &#xA;      author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},&#xA;      year={2023},&#xA;      eprint={2307.16789},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{qin2023tool,&#xA;      title={Tool Learning with Foundation Models}, &#xA;      author={Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},&#xA;      year={2023},&#xA;      eprint={2304.08354},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>