<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-11T01:46:28Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>karpathy/minGPT</title>
    <updated>2022-09-11T01:46:28Z</updated>
    <id>tag:github.com,2022-09-11:/karpathy/minGPT</id>
    <link href="https://github.com/karpathy/minGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;minGPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt.jpg&#34; alt=&#34;mingpt&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A PyTorch re-implementation of &lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;GPT&lt;/a&gt;, both training and inference. minGPT tries to be small, clean, interpretable and educational, as most of the currently available GPT model implementations can a bit sprawling. GPT is not a complicated model and this implementation is appropriately about 300 lines of code (see &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/model.py&#34;&gt;mingpt/model.py&lt;/a&gt;). All that&#39;s going on is that a sequence of indices feeds into a &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Transformer&lt;/a&gt;, and a probability distribution over the next index in the sequence comes out. The majority of the complexity is just being clever with batching (both across examples and over sequence length) for efficiency.&lt;/p&gt; &#xA;&lt;p&gt;The minGPT library is three files: &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/model.py&#34;&gt;mingpt/model.py&lt;/a&gt; contains the actual Transformer model definition, &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/bpe.py&#34;&gt;mingpt/bpe.py&lt;/a&gt; contains a mildly refactored Byte Pair Encoder that translates between text and sequences of integers exactly like OpenAI did in GPT, &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/trainer.py&#34;&gt;mingpt/trainer.py&lt;/a&gt; is (GPT-independent) PyTorch boilerplate code that trains the model. Then there are a number of demos and projects that use the library in the &lt;code&gt;projects&lt;/code&gt; folder:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;projects/adder&lt;/code&gt; trains a GPT from scratch to add numbers (inspired by the addition section in the GPT-3 paper)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;projects/chargpt&lt;/code&gt; trains a GPT to be a character-level language model on some input text file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;demo.ipynb&lt;/code&gt; shows a minimal usage of the &lt;code&gt;GPT&lt;/code&gt; and &lt;code&gt;Trainer&lt;/code&gt; in a notebook format on a simple sorting example&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;generate.ipynb&lt;/code&gt; shows how one can load a pretrained GPT2 and generate text given some prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Library Installation&lt;/h3&gt; &#xA;&lt;p&gt;If you want to &lt;code&gt;import mingpt&lt;/code&gt; into your project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/karpathy/minGPT.git&#xA;cd minGPT&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s how you&#39;d instantiate a GPT-2 (124M param version):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mingpt.model import GPT&#xA;model_config = GPT.get_default_config()&#xA;model_config.model_type = &#39;gpt2&#39;&#xA;model_config.vocab_size = 50257 # openai&#39;s model vocabulary&#xA;model_config.block_size = 1024  # openai&#39;s model block_size (i.e. input context length)&#xA;model = GPT(model_config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here&#39;s how you&#39;d train it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# your subclass of torch.utils.data.Dataset that emits example&#xA;# torch LongTensor of lengths up to 1024, with integers from [0,50257)&#xA;train_dataset = YourDataset()&#xA;&#xA;from mingpt.trainer import Trainer&#xA;train_config = Trainer.get_default_config()&#xA;train_config.learning_rate = 5e-4 # many possible options, see the file&#xA;train_config.max_iters = 1000&#xA;train_config.batch_size = 32&#xA;trainer = Trainer(train_config, model, train_dataset)&#xA;trainer.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;demo.ipynb&lt;/code&gt; for a more concrete example.&lt;/p&gt; &#xA;&lt;h3&gt;Unit tests&lt;/h3&gt; &#xA;&lt;p&gt;Coverage is not super amazing just yet but:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m unittest discover tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;todos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;add gpt-2 finetuning demo on arbitrary given text file&lt;/li&gt; &#xA; &lt;li&gt;add dialog agent demo&lt;/li&gt; &#xA; &lt;li&gt;better docs of outcomes for existing projects (adder, chargpt)&lt;/li&gt; &#xA; &lt;li&gt;add mixed precision and related training scaling goodies&lt;/li&gt; &#xA; &lt;li&gt;distributed training support&lt;/li&gt; &#xA; &lt;li&gt;reproduce some benchmarks in projects/, e.g. text8 or other language modeling&lt;/li&gt; &#xA; &lt;li&gt;proper logging instead of print statement amateur hour haha&lt;/li&gt; &#xA; &lt;li&gt;i probably should have a requirements.txt file...&lt;/li&gt; &#xA; &lt;li&gt;it should be possible to load in many other model weights other than just gpt2-*&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;References&lt;/h3&gt; &#xA;&lt;p&gt;Code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;openai/gpt-2&lt;/a&gt; has the model definition in TensorFlow, but not the training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/image-gpt&#34;&gt;openai/image-gpt&lt;/a&gt; has some more modern gpt-3 like modification in its code, good reference as well&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;huggingface/transformers&lt;/a&gt; has a &lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling&#34;&gt;language-modeling example&lt;/a&gt;. It is full-featured but as a result also somewhat challenging to trace. E.g. some large functions have as much as 90% unused code behind various branching statements that is unused in the default setting of simple language modeling&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Papers + some implementation notes:&lt;/p&gt; &#xA;&lt;h4&gt;Improving Language Understanding by Generative Pre-Training (GPT-1)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our model largely follows the original transformer work&lt;/li&gt; &#xA; &lt;li&gt;We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.&lt;/li&gt; &#xA; &lt;li&gt;Adam max learning rate of 2.5e-4. (later GPT-3 for this model size uses 6e-4)&lt;/li&gt; &#xA; &lt;li&gt;LR decay: increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule&lt;/li&gt; &#xA; &lt;li&gt;We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.&lt;/li&gt; &#xA; &lt;li&gt;Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient&lt;/li&gt; &#xA; &lt;li&gt;bytepair encoding (BPE) vocabulary with 40,000 merges&lt;/li&gt; &#xA; &lt;li&gt;residual, embedding, and attention dropouts with a rate of 0.1 for regularization.&lt;/li&gt; &#xA; &lt;li&gt;modified version of L2 regularization proposed in (37), with w = 0.01 on all non bias or gain weights&lt;/li&gt; &#xA; &lt;li&gt;For the activation function, we used the Gaussian Error Linear Unit (GELU).&lt;/li&gt; &#xA; &lt;li&gt;We used learned position embeddings instead of the sinusoidal version proposed in the original work&lt;/li&gt; &#xA; &lt;li&gt;For finetuning: We add dropout to the classifier with a rate of 0.1. learning rate of 6.25e-5 and a batchsize of 32. 3 epochs. We use a linear learning rate decay schedule with warmup over 0.2% of training. Œª was set to 0.5.&lt;/li&gt; &#xA; &lt;li&gt;GPT-1 model is 12 layers and d_model 768, ~117M params&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Language Models are Unsupervised Multitask Learners (GPT-2)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LayerNorm was moved to the input of each sub-block, similar to a pre-activation residual network&lt;/li&gt; &#xA; &lt;li&gt;an additional layer normalization was added after the final self-attention block.&lt;/li&gt; &#xA; &lt;li&gt;modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/‚àöN where N is the number of residual layers. (weird because in their released code i can only find a simple use of the old 0.02... in their release of image-gpt I found it used for c_proj, and even then only for attn, not for mlp. huh. &lt;a href=&#34;https://github.com/openai/image-gpt/raw/master/src/model.py&#34;&gt;https://github.com/openai/image-gpt/blob/master/src/model.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;the vocabulary is expanded to 50,257&lt;/li&gt; &#xA; &lt;li&gt;increase the context size from 512 to 1024 tokens&lt;/li&gt; &#xA; &lt;li&gt;larger batchsize of 512 is used&lt;/li&gt; &#xA; &lt;li&gt;GPT-2 used 48 layers and d_model 1600 (vs. original 12 layers and d_model 768). ~1.542B params&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Language Models are Few-Shot Learners (GPT-3)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT-3: 96 layers, 96 heads, with d_model of 12,288 (175B parameters).&lt;/li&gt; &#xA; &lt;li&gt;GPT-1-like: 12 layers, 12 heads, d_model 768 (125M)&lt;/li&gt; &#xA; &lt;li&gt;We use the same model and architecture as GPT-2, including the modified initialization, pre-normalization, and reversible tokenization described therein&lt;/li&gt; &#xA; &lt;li&gt;we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer&lt;/li&gt; &#xA; &lt;li&gt;we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 ‚àó dmodel&lt;/li&gt; &#xA; &lt;li&gt;all models use a context window of nctx = 2048 tokens.&lt;/li&gt; &#xA; &lt;li&gt;Adam with Œ≤1 = 0.9, Œ≤2 = 0.95, and eps = 10‚àí8&lt;/li&gt; &#xA; &lt;li&gt;All models use weight decay of 0.1 to provide a small amount of regularization. (NOTE: GPT-1 used 0.01 I believe, see above)&lt;/li&gt; &#xA; &lt;li&gt;clip the global norm of the gradient at 1.0&lt;/li&gt; &#xA; &lt;li&gt;Linear LR warmup over the first 375 million tokens. Then use cosine decay for learning rate down to 10% of its value, over 260 billion tokens.&lt;/li&gt; &#xA; &lt;li&gt;gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size.&lt;/li&gt; &#xA; &lt;li&gt;full 2048-sized time context window is always used, with a special END OF DOCUMENT token delimiter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Generative Pretraining from Pixels (Image GPT)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When working with images, we pick the identity permutation œÄi = i for 1 ‚â§ i ‚â§ n, also known as raster order.&lt;/li&gt; &#xA; &lt;li&gt;we create our own 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512.&lt;/li&gt; &#xA; &lt;li&gt;Our largest model, iGPT-XL, contains L = 60 layers and uses an embedding size of d = 3072 for a total of 6.8B parameters.&lt;/li&gt; &#xA; &lt;li&gt;Our next largest model, iGPT-L, is essentially identical to GPT-2 with L = 48 layers, but contains a slightly smaller embedding size of d = 1536 (vs 1600) for a total of 1.4B parameters.&lt;/li&gt; &#xA; &lt;li&gt;We use the same model code as GPT-2, except that we initialize weights in the layerdependent fashion as in Sparse Transformer (Child et al., 2019) and zero-initialize all projections producing logits.&lt;/li&gt; &#xA; &lt;li&gt;We also train iGPT-M, a 455M parameter model with L = 36 and d = 1024&lt;/li&gt; &#xA; &lt;li&gt;iGPT-S, a 76M parameter model with L = 24 and d = 512 (okay, and how many heads? looks like the Github code claims 8)&lt;/li&gt; &#xA; &lt;li&gt;When pre-training iGPT-XL, we use a batch size of 64 and train for 2M iterations, and for all other models we use a batch size of 128 and train for 1M iterations.&lt;/li&gt; &#xA; &lt;li&gt;Adam with Œ≤1 = 0.9 and Œ≤2 = 0.95&lt;/li&gt; &#xA; &lt;li&gt;The learning rate is warmed up for one epoch, and then decays to 0&lt;/li&gt; &#xA; &lt;li&gt;We did not use weight decay because applying a small weight decay of 0.01 did not change representation quality.&lt;/li&gt; &#xA; &lt;li&gt;iGPT-S lr 0.003&lt;/li&gt; &#xA; &lt;li&gt;No dropout is used.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>xinntao/Real-ESRGAN</title>
    <updated>2022-09-11T01:46:28Z</updated>
    <id>tag:github.com,2022-09-11:/xinntao/Real-ESRGAN</id>
    <link href="https://github.com/xinntao/Real-ESRGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-ESRGAN aims at developing Practical Algorithms for General Image/Video Restoration.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/assets/realesrgan_logo.png&#34; height=&#34;120&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/README_CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/b&gt;&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;üëÄ&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-demos-videos&#34;&gt;&lt;strong&gt;Demos&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üö©&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-updates&#34;&gt;&lt;strong&gt;Updates&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; ‚ö°&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-quick-inference&#34;&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üè∞&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/model_zoo.md&#34;&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üîß&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-dependencies-and-installation&#34;&gt;Install&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üíª&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/Training.md&#34;&gt;Train&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; ‚ùì&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/FAQ.md&#34;&gt;FAQ&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üé®&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/CONTRIBUTING.md&#34;&gt;Contribution&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/xinntao/Real-ESRGAN/total.svg?sanitize=true&#34; alt=&#34;download&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/realesrgan/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/realesrgan&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/xinntao/Real-ESRGAN&#34; alt=&#34;Open issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed/xinntao/Real-ESRGAN&#34; alt=&#34;Closed issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/xinntao/Real-ESRGAN.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/.github/workflows/pylint.yml&#34;&gt;&lt;img src=&#34;https://github.com/xinntao/Real-ESRGAN/actions/workflows/pylint.yml/badge.svg?sanitize=true&#34; alt=&#34;python lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/.github/workflows/publish-pip.yml&#34;&gt;&lt;img src=&#34;https://github.com/xinntao/Real-ESRGAN/actions/workflows/publish-pip.yml/badge.svg?sanitize=true&#34; alt=&#34;Publish-pip&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üî• &lt;strong&gt;AnimeVideo-v3 model (Âä®Êº´ËßÜÈ¢ëÂ∞èÊ®°Âûã)&lt;/strong&gt;. Please see [&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_video_model.md&#34;&gt;&lt;em&gt;anime video models&lt;/em&gt;&lt;/a&gt;] and [&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_comparisons.md&#34;&gt;&lt;em&gt;comparisons&lt;/em&gt;&lt;/a&gt;]&lt;br&gt; üî• &lt;strong&gt;RealESRGAN_x4plus_anime_6B&lt;/strong&gt; for anime images &lt;strong&gt;(Âä®Êº´ÊèíÂõæÊ®°Âûã)&lt;/strong&gt;. Please see [&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_model.md&#34;&gt;&lt;em&gt;anime_model&lt;/em&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- 1. You can try in our website: [ARC Demo](https://arc.tencent.com/en/ai-demos/imgRestore) (now only support RealESRGAN_x4plus_anime_6B) --&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;span&gt;üí•&lt;/span&gt; &lt;strong&gt;Add&lt;/strong&gt; online demo: &lt;a href=&#34;https://replicate.com/xinntao/realesrgan&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=Replicate&amp;amp;color=blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN (&lt;strong&gt;anime videos&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Portable &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip&#34;&gt;Windows&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip&#34;&gt;Linux&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip&#34;&gt;MacOS&lt;/a&gt; &lt;strong&gt;executable files for Intel/AMD/Nvidia GPU&lt;/strong&gt;. You can find more information &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#portable-executable-files-ncnn&#34;&gt;here&lt;/a&gt;. The ncnn implementation is in &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can watch enhanced animations in &lt;a href=&#34;https://v.qq.com/s/topic/v_child/render/fC4iyCAM.html&#34;&gt;Tencent Video&lt;/a&gt;. Ê¨¢ËøéËßÇÁúã&lt;a href=&#34;https://v.qq.com/s/topic/v_child/render/fC4iyCAM.html&#34;&gt;ËÖæËÆØËßÜÈ¢ëÂä®Êº´‰øÆÂ§ç&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Real-ESRGAN aims at developing &lt;strong&gt;Practical Algorithms for General Image/Video Restoration&lt;/strong&gt;.&lt;br&gt; We extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data.&lt;/p&gt; &#xA;&lt;p&gt;üåå Thanks for your valuable feedbacks/suggestions. All the feedbacks are updated in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/feedback.md&#34;&gt;feedback.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If Real-ESRGAN is helpful, please help to ‚≠ê this repo or recommend it to your friends üòä &lt;br&gt; Other recommended projects:&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt;: A practical algorithm for real-world face restoration &lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/BasicSR&#34;&gt;BasicSR&lt;/a&gt;: An open-source image and video restoration toolbox&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;facexlib&lt;/a&gt;: A collection that provides useful face-relation functions.&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/HandyView&#34;&gt;HandyView&lt;/a&gt;: A PyQt5-based image viewer that is handy for view and comparison &lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/HandyFigure&#34;&gt;HandyFigure&lt;/a&gt;: Open source of paper figures &lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üìñ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2107.10833&#34;&gt;Paper&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://www.youtube.com/watch?v=fxHWoDSSvSc&#34;&gt;YouTube Video&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://www.bilibili.com/video/BV1H34y1m7sS/&#34;&gt;BÁ´ôËÆ≤Ëß£&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://xinntao.github.io/projects/RealESRGAN_src/RealESRGAN_poster.pdf&#34;&gt;Poster&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://docs.google.com/presentation/d/1QtW6Iy8rm8rGLsJ0Ldti6kP-7Qyzy6XL/edit?usp=sharing&amp;amp;ouid=109799856763657548160&amp;amp;rtpof=true&amp;amp;sd=true&#34;&gt;PPT slides&lt;/a&gt;]&lt;br&gt; &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, Liangbin Xie, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&#34;&gt;Chao Dong&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://arc.tencent.com/en/ai-demos/imgRestore&#34;&gt;Tencent ARC Lab&lt;/a&gt;; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/assets/teaser.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- Updates ---------------------------&gt; &#xA;&lt;h2&gt;üö© Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ Update the &lt;strong&gt;RealESRGAN AnimeVideo-v3&lt;/strong&gt; model. Please see &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_video_model.md&#34;&gt;anime video models&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_comparisons.md&#34;&gt;comparisons&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Add small models for anime videos. More details are in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_video_model.md&#34;&gt;anime video models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Add the ncnn implementation &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Add &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth&#34;&gt;&lt;em&gt;RealESRGAN_x4plus_anime_6B.pth&lt;/em&gt;&lt;/a&gt;, which is optimized for &lt;strong&gt;anime&lt;/strong&gt; images with much smaller model size. More details and comparisons with &lt;a href=&#34;https://github.com/nihui/waifu2x-ncnn-vulkan&#34;&gt;waifu2x&lt;/a&gt; are in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_model.md&#34;&gt;&lt;strong&gt;anime_model.md&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Support finetuning on your own data or paired data (&lt;em&gt;i.e.&lt;/em&gt;, finetuning ESRGAN). See &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/Training.md#Finetune-Real-ESRGAN-on-your-own-dataset&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Integrate &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt; to support &lt;strong&gt;face enhancement&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/Real-ESRGAN&#34;&gt;Gradio Web Demo&lt;/a&gt;. Thanks &lt;a href=&#34;https://github.com/AK391&#34;&gt;@AK391&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Support arbitrary scale with &lt;code&gt;--outscale&lt;/code&gt; (It actually further resizes outputs with &lt;code&gt;LANCZOS4&lt;/code&gt;). Add &lt;em&gt;RealESRGAN_x2plus.pth&lt;/em&gt; model.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/inference_realesrgan.py&#34;&gt;The inference code&lt;/a&gt; supports: 1) &lt;strong&gt;tile&lt;/strong&gt; options; 2) images with &lt;strong&gt;alpha channel&lt;/strong&gt;; 3) &lt;strong&gt;gray&lt;/strong&gt; images; 4) &lt;strong&gt;16-bit&lt;/strong&gt; images.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ The training codes have been released. A detailed guide can be found in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/Training.md&#34;&gt;Training.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- Demo videos ---------------------------&gt; &#xA;&lt;h2&gt;üëÄ Demos Videos&lt;/h2&gt; &#xA;&lt;h4&gt;Bilibili&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1ja41117zb&#34;&gt;Â§ßÈóπÂ§©ÂÆ´ÁâáÊÆµ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1wY4y1L7hT/&#34;&gt;Anime dance cut Âä®Êº´È≠îÊÄßËàûËπà&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1i3411L7Gy/&#34;&gt;Êµ∑Ë¥ºÁéãÁâáÊÆµ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;YouTube&lt;/h4&gt; &#xA;&lt;h2&gt;üîß Dependencies and Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 1.7&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/xinntao/Real-ESRGAN.git&#xA;cd Real-ESRGAN&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependent packages&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install basicsr - https://github.com/xinntao/BasicSR&#xA;# We use BasicSR for both training and inference&#xA;pip install basicsr&#xA;# facexlib and gfpgan are for face enhancement&#xA;pip install facexlib&#xA;pip install gfpgan&#xA;pip install -r requirements.txt&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;‚ö° Quick Inference&lt;/h2&gt; &#xA;&lt;p&gt;There are usually three ways to inference Real-ESRGAN.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#online-inference&#34;&gt;Online inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#portable-executable-files-ncnn&#34;&gt;Portable executable files (NCNN)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#python-script&#34;&gt;Python script&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Online inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can try in our website: &lt;a href=&#34;https://arc.tencent.com/en/ai-demos/imgRestore&#34;&gt;ARC Demo&lt;/a&gt; (now only support RealESRGAN_x4plus_anime_6B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN (&lt;strong&gt;anime videos&lt;/strong&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Portable executable files (NCNN)&lt;/h3&gt; &#xA;&lt;p&gt;You can download &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip&#34;&gt;Windows&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip&#34;&gt;Linux&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip&#34;&gt;MacOS&lt;/a&gt; &lt;strong&gt;executable files for Intel/AMD/Nvidia GPU&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This executable file is &lt;strong&gt;portable&lt;/strong&gt; and includes all the binaries and models required. No CUDA or PyTorch environment is needed.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can simply run the following command (the Windows example, more information is in the README.md of each executable files):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./realesrgan-ncnn-vulkan.exe -i input.jpg -o output.png -n model_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have provided five models:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;realesrgan-x4plus (default)&lt;/li&gt; &#xA; &lt;li&gt;realesrnet-x4plus&lt;/li&gt; &#xA; &lt;li&gt;realesrgan-x4plus-anime (optimized for anime images, small model size)&lt;/li&gt; &#xA; &lt;li&gt;realesr-animevideov3 (animation video)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;-n&lt;/code&gt; argument for other models, for example, &lt;code&gt;./realesrgan-ncnn-vulkan.exe -i input.jpg -o output.png -n realesrnet-x4plus&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Usage of portable executable files&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Please refer to &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan#computer-usages&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Note that it does not support all the functions (such as &lt;code&gt;outscale&lt;/code&gt;) as the python script &lt;code&gt;inference_realesrgan.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: realesrgan-ncnn-vulkan.exe -i infile -o outfile [options]...&#xA;&#xA;  -h                   show this help&#xA;  -i input-path        input image path (jpg/png/webp) or directory&#xA;  -o output-path       output image path (jpg/png/webp) or directory&#xA;  -s scale             upscale ratio (can be 2, 3, 4. default=4)&#xA;  -t tile-size         tile size (&amp;gt;=32/0=auto, default=0) can be 0,0,0 for multi-gpu&#xA;  -m model-path        folder path to the pre-trained models. default=models&#xA;  -n model-name        model name (default=realesr-animevideov3, can be realesr-animevideov3 | realesrgan-x4plus | realesrgan-x4plus-anime | realesrnet-x4plus)&#xA;  -g gpu-id            gpu device to use (default=auto) can be 0,1,2 for multi-gpu&#xA;  -j load:proc:save    thread count for load/proc/save (default=1:2:2) can be 1:2,2,2:2 for multi-gpu&#xA;  -x                   enable tta mode&#34;&#xA;  -f format            output image format (jpg/png/webp, default=ext/png)&#xA;  -v                   verbose output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that it may introduce block inconsistency (and also generate slightly different results from the PyTorch implementation), because this executable file first crops the input image into several tiles, and then processes them separately, finally stitches together.&lt;/p&gt; &#xA;&lt;h3&gt;Python script&lt;/h3&gt; &#xA;&lt;h4&gt;Usage of python script&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can use X4 model for &lt;strong&gt;arbitrary output size&lt;/strong&gt; with the argument &lt;code&gt;outscale&lt;/code&gt;. The program will further perform cheap resize operation after the Real-ESRGAN output.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile -o outfile [options]...&#xA;&#xA;A common command: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile --outscale 3.5 --face_enhance&#xA;&#xA;  -h                   show this help&#xA;  -i --input           Input image or folder. Default: inputs&#xA;  -o --output          Output folder. Default: results&#xA;  -n --model_name      Model name. Default: RealESRGAN_x4plus&#xA;  -s, --outscale       The final upsampling scale of the image. Default: 4&#xA;  --suffix             Suffix of the restored image. Default: out&#xA;  -t, --tile           Tile size, 0 for no tile during testing. Default: 0&#xA;  --face_enhance       Whether to use GFPGAN to enhance face. Default: False&#xA;  --fp32               Use fp32 precision during inference. Default: fp16 (half precision).&#xA;  --ext                Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Inference general images&lt;/h4&gt; &#xA;&lt;p&gt;Download pre-trained models: &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth&#34;&gt;RealESRGAN_x4plus.pth&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference_realesrgan.py -n RealESRGAN_x4plus -i inputs --face_enhance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results are in the &lt;code&gt;results&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;h4&gt;Inference anime images&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xinntao/public-figures/master/Real-ESRGAN/cmp_realesrgan_anime_1.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Pre-trained models: &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth&#34;&gt;RealESRGAN_x4plus_anime_6B&lt;/a&gt;&lt;br&gt; More details and comparisons with &lt;a href=&#34;https://github.com/nihui/waifu2x-ncnn-vulkan&#34;&gt;waifu2x&lt;/a&gt; are in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_model.md&#34;&gt;&lt;strong&gt;anime_model.md&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download model&#xA;wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models&#xA;# inference&#xA;python inference_realesrgan.py -n RealESRGAN_x4plus_anime_6B -i inputs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results are in the &lt;code&gt;results&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{wang2021realesrgan,&#xA;    author    = {Xintao Wang and Liangbin Xie and Chao Dong and Ying Shan},&#xA;    title     = {Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data},&#xA;    booktitle = {International Conference on Computer Vision Workshops (ICCVW)},&#xA;    date      = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìß Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question, please email &lt;code&gt;xintao.wang@outlook.com&lt;/code&gt; or &lt;code&gt;xintaowang@tencent.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- Projects that use Real-ESRGAN ---------------------------&gt; &#xA;&lt;h2&gt;üß© Projects that use Real-ESRGAN&lt;/h2&gt; &#xA;&lt;p&gt;If you develop/use Real-ESRGAN in your projects, welcome to let me know.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NCNN-Android: &lt;a href=&#34;https://github.com/tumuyan/RealSR-NCNN-Android&#34;&gt;RealSR-NCNN-Android&lt;/a&gt; by &lt;a href=&#34;https://github.com/tumuyan&#34;&gt;tumuyan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VapourSynth: &lt;a href=&#34;https://github.com/HolyWu/vs-realesrgan&#34;&gt;vs-realesrgan&lt;/a&gt; by &lt;a href=&#34;https://github.com/HolyWu&#34;&gt;HolyWu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;NCNN: &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;GUI&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AaronFeng753/Waifu2x-Extension-GUI&#34;&gt;Waifu2x-Extension-GUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/AaronFeng753&#34;&gt;AaronFeng753&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Justin62628/Squirrel-RIFE&#34;&gt;Squirrel-RIFE&lt;/a&gt; by &lt;a href=&#34;https://github.com/Justin62628&#34;&gt;Justin62628&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scifx/Real-GUI&#34;&gt;Real-GUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/scifx&#34;&gt;scifx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/net2cn/Real-ESRGAN_GUI&#34;&gt;Real-ESRGAN_GUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/net2cn&#34;&gt;net2cn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WGzeyu/Real-ESRGAN-EGUI&#34;&gt;Real-ESRGAN-EGUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/WGzeyu&#34;&gt;WGzeyu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shangar21/anime_upscaler&#34;&gt;anime_upscaler&lt;/a&gt; by &lt;a href=&#34;https://github.com/shangar21&#34;&gt;shangar21&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ó Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for all the contributors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AK391&#34;&gt;AK391&lt;/a&gt;: Integrate RealESRGAN to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/Real-ESRGAN&#34;&gt;Gradio Web Demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Asiimoviet&#34;&gt;Asiimoviet&lt;/a&gt;: Translate the README.md to Chinese (‰∏≠Êñá).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/2ji3150&#34;&gt;2ji3150&lt;/a&gt;: Thanks for the &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/issues/131&#34;&gt;detailed and valuable feedbacks/suggestions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jared-02&#34;&gt;Jared-02&lt;/a&gt;: Translate the Training.md to Chinese (‰∏≠Êñá).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>coqui-ai/TTS</title>
    <updated>2022-09-11T01:46:28Z</updated>
    <id>tag:github.com,2022-09-11:/coqui-ai/TTS</id>
    <link href="https://github.com/coqui-ai/TTS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üê∏üí¨ - a deep learning toolkit for Text-to-Speech, battle-tested in research and production&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png&#34; height=&#34;56&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;üê∏TTS is a library for advanced Text-to-Speech generation. It&#39;s built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality. üê∏TTS comes with pretrained models, tools for measuring dataset quality and already used in &lt;strong&gt;20+ languages&lt;/strong&gt; for products and research projects.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/coqui-ai/TTS?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/coqui-ai/TTS.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MPL-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/TTS&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/TTS.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/coqui-ai/TTS/raw/master/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667&#34; alt=&#34;Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/tts&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/tts&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zenodo.org/badge/latestdoi/265612440&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/265612440.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;a href=&#34;https://tts.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/tts/badge/?version=latest&amp;amp;style=plastic&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üì∞ &lt;a href=&#34;https://coqui.ai/?subscription=true&#34;&gt;&lt;strong&gt;Subscribe to üê∏Coqui.ai Newsletter&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üì¢ &lt;a href=&#34;https://erogol.github.io/ddc-samples/&#34;&gt;English Voice Samples&lt;/a&gt; and &lt;a href=&#34;https://soundcloud.com/user-565970875/pocket-article-wavernn-and-tacotron2&#34;&gt;SoundCloud playlist&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üìÑ &lt;a href=&#34;https://github.com/erogol/TTS-papers&#34;&gt;Text-to-Speech paper collection&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2&#34;&gt; &#xA;&lt;h2&gt;üí¨ Where to ask questions&lt;/h2&gt; &#xA;&lt;p&gt;Please use our dedicated channels for questions and discussion. Help is much more valuable if it&#39;s shared publicly so that more people can benefit from it.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Platforms&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üö® &lt;strong&gt;Bug Reports&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/tts/issues&#34;&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üéÅ &lt;strong&gt;Feature Requests &amp;amp; Ideas&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/tts/issues&#34;&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üë©‚Äçüíª &lt;strong&gt;Usage Questions&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/discussions&#34;&gt;Github Discussions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üóØ &lt;strong&gt;General Discussion&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/discussions&#34;&gt;Github Discussions&lt;/a&gt; or &lt;a href=&#34;https://gitter.im/coqui-ai/TTS?utm_source=share-link&amp;amp;utm_medium=link&amp;amp;utm_campaign=share-link&#34;&gt;Gitter Room&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üîó Links and Resources&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üíº &lt;strong&gt;Documentation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tts.readthedocs.io/en/latest/&#34;&gt;ReadTheDocs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üíæ &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/tree/dev#install-tts&#34;&gt;TTS/README.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üë©‚Äçüíª &lt;strong&gt;Contributing&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üìå &lt;strong&gt;Road Map&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/issues/378&#34;&gt;Main Development Plans&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üöÄ &lt;strong&gt;Released Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/releases&#34;&gt;TTS Releases&lt;/a&gt; and &lt;a href=&#34;https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models&#34;&gt;Experimental Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ü•á TTS Performance&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Underlined &#34;TTS*&#34; and &#34;Judy*&#34; are üê∏TTS models&lt;/p&gt; &#xA;&lt;!-- [Details...](https://github.com/coqui-ai/TTS/wiki/Mean-Opinion-Score-Results) --&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;High-performance Deep Learning models for Text2Speech tasks. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).&lt;/li&gt; &#xA;   &lt;li&gt;Speaker Encoder to compute speaker embeddings efficiently.&lt;/li&gt; &#xA;   &lt;li&gt;Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fast and efficient model training.&lt;/li&gt; &#xA; &lt;li&gt;Detailed training logs on the terminal and Tensorboard.&lt;/li&gt; &#xA; &lt;li&gt;Support for Multi-speaker TTS.&lt;/li&gt; &#xA; &lt;li&gt;Efficient, flexible, lightweight but feature complete &lt;code&gt;Trainer API&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Released and ready-to-use models.&lt;/li&gt; &#xA; &lt;li&gt;Tools to curate Text2Speech datasets under&lt;code&gt;dataset_analysis&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Utilities to use and test your models.&lt;/li&gt; &#xA; &lt;li&gt;Modular (but not too much) code base enabling easy implementation of new ideas.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implemented Models&lt;/h2&gt; &#xA;&lt;h3&gt;Text-to-Spectrogram&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tacotron: &lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tacotron2: &lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Glow-TTS: &lt;a href=&#34;https://arxiv.org/abs/2005.11129&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Speedy-Speech: &lt;a href=&#34;https://arxiv.org/abs/2008.03802&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Align-TTS: &lt;a href=&#34;https://arxiv.org/abs/2003.01950&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastPitch: &lt;a href=&#34;https://arxiv.org/pdf/2006.06873.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastSpeech: &lt;a href=&#34;https://arxiv.org/abs/1905.09263&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SC-GlowTTS: &lt;a href=&#34;https://arxiv.org/abs/2104.05557&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;End-to-End Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VITS: &lt;a href=&#34;https://arxiv.org/pdf/2106.06103&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YourTTS: &lt;a href=&#34;https://arxiv.org/abs/2112.02418&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Attention Methods&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Guided Attention: &lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Forward Backward Decoding: &lt;a href=&#34;https://arxiv.org/abs/1907.09006&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Graves Attention: &lt;a href=&#34;https://arxiv.org/abs/1910.10288&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Double Decoder Consistency: &lt;a href=&#34;https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dynamic Convolutional Attention: &lt;a href=&#34;https://arxiv.org/pdf/1910.10288.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alignment Network: &lt;a href=&#34;https://arxiv.org/abs/2108.10447&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Speaker Encoder&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GE2E: &lt;a href=&#34;https://arxiv.org/abs/1710.10467&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Angular Loss: &lt;a href=&#34;https://arxiv.org/pdf/2003.11982.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Vocoders&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MelGAN: &lt;a href=&#34;https://arxiv.org/abs/1910.06711&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MultiBandMelGAN: &lt;a href=&#34;https://arxiv.org/abs/2005.05106&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ParallelWaveGAN: &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GAN-TTS discriminators: &lt;a href=&#34;https://arxiv.org/abs/1909.11646&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WaveRNN: &lt;a href=&#34;https://github.com/fatchord/WaveRNN/&#34;&gt;origin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WaveGrad: &lt;a href=&#34;https://arxiv.org/abs/2009.00713&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;HiFiGAN: &lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;UnivNet: &lt;a href=&#34;https://arxiv.org/abs/2106.07889&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also help us implement more models.&lt;/p&gt; &#xA;&lt;h2&gt;Install TTS&lt;/h2&gt; &#xA;&lt;p&gt;üê∏TTS is tested on Ubuntu 18.04 with &lt;strong&gt;python &amp;gt;= 3.7, &amp;lt; 3.11.&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are only interested in &lt;a href=&#34;https://tts.readthedocs.io/en/latest/inference.html&#34;&gt;synthesizing speech&lt;/a&gt; with the released üê∏TTS models, installing from PyPI is the easiest option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install TTS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you plan to code or train models, clone üê∏TTS and install it locally.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/coqui-ai/TTS&#xA;pip install -e .[all,dev,notebooks]  # Select the relevant extras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on Ubuntu (Debian), you can also run following commands for installation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.&#xA;$ make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on Windows, üëë@GuyPaddock wrote installation instructions &lt;a href=&#34;https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Use TTS&lt;/h2&gt; &#xA;&lt;h3&gt;Single Speaker Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;List provided models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --list_models&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get model info (for both tts_models and vocoder_models):&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Query by type/name: The model_info_by_name uses the name as it from the --list_models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_name tts_models/tr/common-voice/glow-tts&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Query by type/idx: The model_query_idx uses the corresponding idx from --list_models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_idx &#34;&amp;lt;model_type&amp;gt;/&amp;lt;model_query_idx&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_idx tts_models/3 &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run TTS with default models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run a TTS model with its default vocoder model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run with specific TTS and vocoder models from the list:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --vocoder_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --vocoder_name &#34;vocoder_models/en/ljspeech/univnet&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own TTS model (Using Griffin-Lim Vocoder):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own TTS and Vocoder models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_path path/to/config.json --config_path path/to/model.pth --out_path output/path/speech.wav&#xA;    --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Multi-speaker Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;List the available speakers and choose as &amp;lt;speaker_id&amp;gt; among them:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;  --list_speaker_idxs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the multi-speaker TTS model with the target speaker ID:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS.&#34; --out_path output/path/speech.wav --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;  --speaker_idx &amp;lt;speaker_id&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own multi-speaker TTS model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav --model_path path/to/config.json --config_path path/to/model.pth --speakers_file_path path/to/speaker.json --speaker_idx &amp;lt;speaker_id&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Directory Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)&#xA;|- utils/           (common utilities.)&#xA;|- TTS&#xA;    |- bin/             (folder for all the executables.)&#xA;      |- train*.py                  (train your target model.)&#xA;      |- distribute.py              (train your TTS model using Multiple GPUs.)&#xA;      |- compute_statistics.py      (compute dataset statistics for normalization.)&#xA;      |- ...&#xA;    |- tts/             (text to speech models)&#xA;        |- layers/          (model layer definitions)&#xA;        |- models/          (model definitions)&#xA;        |- utils/           (model specific utilities.)&#xA;    |- speaker_encoder/ (Speaker Encoder models.)&#xA;        |- (same)&#xA;    |- vocoder/         (Vocoder models.)&#xA;        |- (same)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>