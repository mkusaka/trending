<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-24T01:42:18Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fixie-ai/ultravox</title>
    <updated>2024-11-24T01:42:18Z</updated>
    <id>tag:github.com,2024-11-24:/fixie-ai/ultravox</id>
    <link href="https://github.com/fixie-ai/ultravox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast multimodal LLM for real-time voice&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;Ultravox&#34; src=&#34;https://zfmrfvimiaqahezndsse.supabase.co/storage/v1/object/public/images/custom/Introducing%20Ultravox%20Wide.jpg&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; A fast multimodal LLM for real-time voice &lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024/11 — &lt;a href=&#34;https://github.com/fixie-ai/ultravox/releases/tag/v0.4.1&#34;&gt;Ultravox 0.4.1&lt;/a&gt; available&lt;/li&gt; &#xA; &lt;li&gt;2024/08 — &lt;a href=&#34;https://github.com/fixie-ai/ultravox/releases/tag/v0.4&#34;&gt;Ultravox 0.4&lt;/a&gt; available&lt;/li&gt; &#xA; &lt;li&gt;2024/08 — &lt;a href=&#34;https://github.com/fixie-ai/ultravox/releases/tag/v0.3&#34;&gt;Ultravox 0.3&lt;/a&gt; available&lt;/li&gt; &#xA; &lt;li&gt;2024/08 — Preview of Ultravox APIs available, more information &lt;a href=&#34;https://fixie-ai.github.io/ultradox/&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;About&lt;/h1&gt; &#xA;&lt;p&gt;Ultravox is a new kind of multimodal LLM that can understand text as well as human speech, without the need for a separate Audio Speech Recognition (ASR) stage. Building on research like &lt;a href=&#34;https://arxiv.org/abs/2209.03143&#34;&gt;AudioLM&lt;/a&gt;, &lt;a href=&#34;https://ai.meta.com/blog/seamless-m4t/&#34;&gt;SeamlessM4T&lt;/a&gt;, &lt;a href=&#34;https://tincans.ai/slm&#34;&gt;Gazelle&lt;/a&gt;, &lt;a href=&#34;https://github.com/0nutation/SpeechGPT/tree/main/speechgpt&#34;&gt;SpeechGPT&lt;/a&gt;, and others, Ultravox is able to extend any open-weight LLM with a multimodal projector that converts audio directly into the high-dimensional space used by LLM. We&#39;ve trained versions on Llama 3, Mistral, and Gemma. This direct coupling allows Ultravox to respond much more quickly than systems that combine separate ASR and LLM components. In the future this will also allow Ultravox to natively understand the paralinguistic cues of timing and emotion that are omnipresent in human speech.&lt;/p&gt; &#xA;&lt;p&gt;The current version of Ultravox (v0.4), when invoked with audio content, has a time-to-first-token (TTFT) of approximately 150ms, and a tokens-per-second rate of ~60 using a Llama 3.1 8B backbone. While quite fast, we believe there is considerable room for improvement in these numbers.&lt;/p&gt; &#xA;&lt;p&gt;Ultravox currently takes in audio and emits streaming text. As we evolve the model, we&#39;ll train it to be able to emit a stream of speech tokens that can then be converted directly into raw audio by an appropriate unit vocoder.&lt;/p&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;See Ultravox in action on our &lt;a href=&#34;https://demo.ultravox.ai&#34;&gt;demo page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Discord&lt;/h3&gt; &#xA;&lt;p&gt;Join us on our Discord server &lt;a href=&#34;https://discord.gg/Qw6KHxv8YB&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Jobs&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re interested in working on Ultravox fulltime, we&#39;re hiring! Check out our jobs page &lt;a href=&#34;https://careers.fixie.ai&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Inference Server&lt;/h3&gt; &#xA;&lt;p&gt;You can try out Ultravox using your own audio content (as a WAV file) by spinning up an Ultravox instance on our partner, BaseTen: &lt;a href=&#34;https://www.baseten.co/library/ultravox/&#34;&gt;https://www.baseten.co/library/ultravox/&lt;/a&gt;. They offer free credits to get started.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re interested in running Ultravox in a real-time capacity, we offer a set of managed APIs as well. You can learn more about getting access to those &lt;a href=&#34;https://docs.ultravox.ai&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Model&lt;/h3&gt; &#xA;&lt;p&gt;You can download the latest weights from the &lt;a href=&#34;https://huggingface.co/fixie-ai/&#34;&gt;Ultravox Hugging Face page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Architecture&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1ey81xuuMzrJaBwztb_Rq24Cit37GQokD2aAes_KkGVI/edit&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/docs/assets/Ultravox%20Model%20Architecture.svg?sanitize=true&#34; alt=&#34;architecture diagram&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Read on if you&#39;re interested in training your own version of Ultravox.&lt;/p&gt; &#xA;&lt;h2&gt;Environment Setup (Mac)&lt;/h2&gt; &#xA;&lt;p&gt;Install the basic tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://brew.sh&#34;&gt;&lt;code&gt;Homebrew&lt;/code&gt;&lt;/a&gt; is a package manager for MacOS that also mostly works for Linux. If you&#39;re running Debian or Ubuntu Linux, you can alternatively get by with apt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://just.systems/man/en/&#34;&gt;&lt;code&gt;Just&lt;/code&gt;&lt;/a&gt; simplifies our shell workflows. It frequently functions as our interface to all the other tools.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34;&#xA;brew update&#xA;brew install just&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a Python virtual environment and install the necessary packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;just install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We&#39;re using Poetry to manage the Python virtual environment.&lt;/p&gt; &#xA;&lt;h3&gt;Mosaic Environment Setup (Fixie Internal)&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use &lt;a href=&#34;https://docs.mosaicml.com/projects/mcli/en/latest/quick_start/getting_started.html&#34;&gt;Mosaic&lt;/a&gt; for training, you need to setup a few things to run on the Mosaic Platform.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &amp;amp; login to the Mosaic CLI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade mosaicml-cli&#xA;&#xA;mcli init&#xA;&#xA;mcli set api-key &amp;lt;new-value&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;set API keys for tools we use:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Huggging Face token for accessing walled data and models&#xA;mcli create secret env HF_TOKEN=hf_&amp;lt;your_token&amp;gt;&#xA;&#xA;# WandB token for logging experiments&#xA;mcli create secret env WANDB_PROJECT=ultravox&#xA;mcli create secret env WANDB_API_KEY=&amp;lt;your_wandb_key&amp;gt;&#xA;&#xA;# GCP credentials for accessing data (e.g. BoolQ)&#xA;# Get service_account.json file from Justin/Farzad and put it in the root dir, then&#xA;mcli create secret gcp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Currently, we keep both the LLM and the audio encoder frozen and only train the adapter/projector. Training Ultraox v0.4 took 2-3 hours on 8xH100 GPUs for 14K training steps.&lt;/p&gt; &#xA;&lt;h3&gt;Use-Cases for Training Ultravox&lt;/h3&gt; &#xA;&lt;p&gt;Why would you want to (re-) train Ultravox? Here are a few scenarios:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;You want to use a different LLM or audio encoder backbone.&lt;/p&gt; &lt;p&gt;a. In this case you need to re-train the adapter. You can use &lt;code&gt;release_config.yaml&lt;/code&gt;, which contains our config for our latest release, and you should be able to simply change the base LLM or encoder by specifying &lt;code&gt;--text-model &amp;lt;hf-model-id-for-llm&amp;gt;&lt;/code&gt; and/or &lt;code&gt;--audio-model &amp;lt;hf-model-id-for-encoder&amp;gt;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You want to improve the knowledge of the model --&amp;gt; NO NEED TO TRAIN ULTRAVOX!&lt;/p&gt; &lt;p&gt;a. We suggest to either use RAG on the fly (no training needed), or fine-tune the LLM backbone instead. You might need to re-train Ultravox if you fine-tune the LLM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You want to use your own audio data, for example to add support for a new language.&lt;/p&gt; &lt;p&gt;a. First step, prepare your dataset: at bare minimum, the samples should have an &lt;code&gt;audio&lt;/code&gt; and a text &lt;code&gt;continuation&lt;/code&gt; field.&lt;/p&gt; &lt;p&gt;b. Take a look at &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/ultravox/tools/ds_tool/ds_tool.py&#34;&gt;&lt;code&gt;ds_tool.py&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/ultravox/tools/ds_tool/continuation.jinja&#34;&gt;&lt;code&gt;continuation.jinja&lt;/code&gt;&lt;/a&gt; as well as &lt;a href=&#34;https://huggingface.co/datasets/fixie-ai/common_voice_17_0/viewer/fr&#34;&gt;our variant of Common Voice&lt;/a&gt; that was created using &lt;code&gt;ds_tool&lt;/code&gt; to add the &lt;code&gt;continuation&lt;/code&gt; field.&lt;/p&gt; &lt;p&gt;c. Add your dataset to the dataset mix in &lt;code&gt;release_config.yaml&lt;/code&gt; and train.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;There&#39;s no one-size fits all. If you need help you can find us on our Discord server &lt;a href=&#34;https://discord.gg/Qw6KHxv8YB&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How to Train&lt;/h3&gt; &#xA;&lt;p&gt;We do most of our training on the &lt;a href=&#34;https://docs.mosaicml.com&#34;&gt;MosaicML platform&lt;/a&gt; and therefore most of our tooling and docs are Mosaic-related. However, you can do the same training on your own GPU without much difficulty. Here we assume you have the environment set up (run &lt;code&gt;just install&lt;/code&gt;). You can also take a look at &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/setup.sh&#34;&gt;&lt;code&gt;setup.sh&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To kick off a training run you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run python -m ultravox.training.train --config_path ultravox/training/configs/release_config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For DDP training make sure to add &lt;code&gt;torchrun&lt;/code&gt;. We also recommend prefetching weights in advance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;TRAIN_ARGS=&#34;--config_path ultravox/training/configs/release_config.yaml&#34;&#xA;poetry run python -m ultravox.training.helpers.prefetch_weights $TRAIN_ARGS&#xA;poetry run torchrun --nproc_per_node=8 -m ultravox.training.train $TRAIN_ARGS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a debug run, you can use smaller models, datasets, or batch size. Here&#39;s a config that uses TinyLlama as the LLM backbone:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run python -m ultravox.training.train --config_path ultravox/training/configs/asr_tinyllama_100s.yaml --batch_size 1 --report_logs_to tensorboard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/lebrice/simpleparsing/&#34;&gt;SimpleParsing&lt;/a&gt; for configs. Configs are composable (i.e. you can specify zero or many configs) and &lt;code&gt;meta_config.yaml&lt;/code&gt; is always used as the default. See &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/ultravox/training/config_base.py&#34;&gt;&lt;code&gt;configs_base.py&lt;/code&gt;&lt;/a&gt; to find the parameters you modify, such as the &lt;code&gt;--text-model&lt;/code&gt;, &lt;code&gt;--device&lt;/code&gt;, &lt;code&gt;--exp-name&lt;/code&gt;, etc.&lt;/p&gt; &#xA;&lt;h3&gt;MosaicML Training (Fixie Internal)&lt;/h3&gt; &#xA;&lt;p&gt;Before running any training jobs, you need to setup your SSH key in the Mosaic Platform: &lt;a href=&#34;https://docs.mosaicml.com/projects/mcli/en/latest/resources/secrets/ssh.html#page-secrets-ssh&#34;&gt;https://docs.mosaicml.com/projects/mcli/en/latest/resources/secrets/ssh.html#page-secrets-ssh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## Create a new SSH key and add it to the Mosaic Platform&#xA;# ssh-keygen -f ~/.ssh/mclid_id_rsa&#xA;## add the **public** key to Github&#xA;# mcli create secret ssh ~/.ssh/mclid_id_rsa&#xA;&#xA;mcli run -f mcloud.yaml --follow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other useful commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mcli get clusters&#xA;&#xA;mcli util r7z2&#xA;mcli get runs&#xA;mcli get runs --cluster r7z2&#xA;&#xA;mcli run -f mcloud.yaml --follow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For interactive runs you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;just mcloud --image mosaicml/composer:latest --max-duration 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;IMPORTANT: Make sure to monitor your jobs and stop the machine when you&#39;re done with any job, specially interactive ones!&lt;/p&gt; &#xA;&lt;h3&gt;Running evaluations&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use &lt;code&gt;infer_tool.py --json &amp;gt; file&lt;/code&gt; to create a jsonl output from a given model/dataset combo, where each line contains two values: &lt;strong&gt;question&lt;/strong&gt; and &lt;strong&gt;answer&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;eval_tool.py -f file&lt;/code&gt; to evaluate the jsonl file, which will produce an average score for the model on the dataset.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Misc&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/Justfile&#34;&gt;Justfile&lt;/a&gt; is a good resource for finding popular commands. Here are a few:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;just update    # update dependencies&#xA;just format    # run formatting (black, isort, autoflake)&#xA;just test      # run tests&#xA;just python    # activate venv and run python&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/TinyTroupe</title>
    <updated>2024-11-24T01:42:18Z</updated>
    <id>tag:github.com,2024-11-24:/microsoft/TinyTroupe</id>
    <link href="https://github.com/microsoft/TinyTroupe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM-powered multiagent persona simulation for imagination enhancement and business insights.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TinyTroupe 🤠🤓🥸🧐&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;LLM-powered multiagent persona simulation for imagination enhancement and business insights.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/tinytroupe_stage.png&#34; alt=&#34;A tiny office with tiny people doing some tiny jobs.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;TinyTroupe&lt;/em&gt; is an experimental Python library that allows the &lt;strong&gt;simulation&lt;/strong&gt; of people with specific personalities, interests, and goals. These artificial agents - &lt;code&gt;TinyPerson&lt;/code&gt;s - can listen to us and one another, reply back, and go about their lives in simulated &lt;code&gt;TinyWorld&lt;/code&gt; environments. This is achieved by leveraging the power of Large Language Models (LLMs), notably GPT-4, to generate realistic simulated behavior. This allow us to investigate a wide range of &lt;strong&gt;convincing interactions&lt;/strong&gt; and &lt;strong&gt;consumer types&lt;/strong&gt;, with &lt;strong&gt;highly customizable personas&lt;/strong&gt;, under &lt;strong&gt;conditions of our choosing&lt;/strong&gt;. The focus is thus on &lt;em&gt;understanding&lt;/em&gt; human behavior and not on directly &lt;em&gt;supporting it&lt;/em&gt; (like, say, AI assistants do) -- this results in, among other things, specialized mechanisms that make sense only in a simulation setting. Further, unlike other &lt;em&gt;game-like&lt;/em&gt; LLM-based simulation approaches, TinyTroupe aims at enlightening productivity and business scenarios, thereby contributing to more successful projects and products. Here are some application ideas to &lt;strong&gt;enhance human imagination&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advertisement:&lt;/strong&gt; TinyTroupe can &lt;strong&gt;evaluate digital ads (e.g., Bing Ads)&lt;/strong&gt; offline with a simulated audience before spending money on them!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Software Testing:&lt;/strong&gt; TinyTroupe can &lt;strong&gt;provide test input&lt;/strong&gt; to systems (e.g., search engines, chatbots or copilots) and then &lt;strong&gt;evaluate the results&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Training and exploratory data:&lt;/strong&gt; TinyTroupe can generate realistic &lt;strong&gt;synthetic data&lt;/strong&gt; that can be later used to train models or be subject to opportunity analyses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Product and project management:&lt;/strong&gt; TinyTroupe can &lt;strong&gt;read project or product proposals&lt;/strong&gt; and &lt;strong&gt;give feedback&lt;/strong&gt; from the perspective of &lt;strong&gt;specific personas&lt;/strong&gt; (e.g., physicians, lawyers, and knowledge workers in general).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brainstorming:&lt;/strong&gt; TinyTroupe can simulate &lt;strong&gt;focus groups&lt;/strong&gt; and deliver great product feedback at a fraction of the cost!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In all of the above, and many others, we hope experimenters can &lt;strong&gt;gain insights&lt;/strong&gt; about their domain of interest, and thus make better decisions.&lt;/p&gt; &#xA;&lt;p&gt;We are releasing &lt;em&gt;TinyTroupe&lt;/em&gt; at a relativelly early stage, with considerable work still to be done, because we are looking for feedback and contributions to steer development in productive directions. We are particularly interested in finding new potential use cases, for instance in specific industries.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] 🚧 &lt;strong&gt;WORK IN PROGRESS: expect frequent changes&lt;/strong&gt;. TinyTroupe is an ongoing research project, still under &lt;strong&gt;very significant development&lt;/strong&gt; and requiring further &lt;strong&gt;tidying up&lt;/strong&gt;. In particular, the API is still subject to frequent changes. Experimenting with API variations is essential to shape it correctly, but we are working to stabilize it and provide a more consistent and friendly experience over time. We appreciate your patience and feedback as we continue to improve the library.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] ⚖️ &lt;strong&gt;Read the LEGAL DISCLAIMER.&lt;/strong&gt; TinyTroupe is for research and simulation only. You are fully responsible for any use you make of the generated outputs. Various important additional legal considerations apply and constrain its use, please read the full &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#legal-disclaimer&#34;&gt;Legal Disclaimer&lt;/a&gt; section below before using TinyTroupe.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📚 &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🛠️ &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#pre-requisites&#34;&gt;Pre-requisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;📥 &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🌟 &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#principles&#34;&gt;Principles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🏗️ &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#project-structure&#34;&gt;Project Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;📖 &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#using-the-library&#34;&gt;Using the Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🤝 &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🙏 &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;📜 &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#how-to-cite-tinytroupe&#34;&gt;Citing TinyTroupe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;⚖️ &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#legal-disclaimer&#34;&gt;Legal Disclaimer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;™️ &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#trademarks&#34;&gt;Trademarks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;To get a sense of what TinyTroupe can do, here are some examples of its use. These examples are available in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder, and you can either inspect the pre-compiled Jupyter notebooks or run them yourself locally. Notice the interactive nature of TinyTroupe experiments -- just like you use Jupyter notebooks to interact with data, you can use TinyTroupe to interact with simulated people and environments, for the purpose of gaining insights.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Currently, simulation outputs are better visualized against dark backgrounds, so we recommend using a dark theme in your Jupyter notebook client.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;🧪&lt;strong&gt;Example 1&lt;/strong&gt; &lt;em&gt;(from &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/interview_with_customer.ipynb&#34;&gt;interview_with_customer.ipynb&lt;/a&gt;)&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s begin with a simple customer interview scenario, where a business consultant approaches a banker:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_customer-interview-1.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The conversation can go on for a few steps to dig deeper and deeper until the consultant is satisfied with the information gathered, for instance a concrete project idea:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_customer-interview-2.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;🧪&lt;strong&gt;EXAMPLE 2&lt;/strong&gt; &lt;em&gt;(from &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/advertisement_for_tv.ipynb&#34;&gt;advertisement_for_tv.ipynb&lt;/a&gt;)&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s evaluate some online ads options to pick the best one. Here&#39;s one example output for TV ad evaluation:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_tv-ad-1.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Now, instead of having to carefully read what the agents said, we can extract the choice of each agent and compute the overall preference in an automated manner:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_tv-ad-2.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;🧪 &lt;strong&gt;EXAMPLES 3&lt;/strong&gt; &lt;em&gt;(from &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/product_brainstorming.ipynb&#34;&gt;product_brainstorming.ipynb&lt;/a&gt;)&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;And here&#39;s a focus group starting to brainstorm about new AI features for Microsoft Word. Instead of interacting with each agent individually, we manipulate the environment to make them interact with each other:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_brainstorming-1.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;After running a simulation, we can extract the results in a machine-readable manner, to reuse elsewhere (e.g., a report generator); here&#39;s what we get for the above brainstorming session:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_brainstorming-2.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;You can find other examples in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-requisites&lt;/h2&gt; &#xA;&lt;p&gt;To run the library, you need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10 or higher. We&#39;ll assume you are using &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/&#34;&gt;Anaconda&lt;/a&gt;, but you can use other Python distributions.&lt;/li&gt; &#xA; &lt;li&gt;Access to Azure OpenAI Service or Open AI GPT-4 APIs. You can get access to the Azure OpenAI Service &lt;a href=&#34;https://azure.microsoft.com/en-us/products/ai-services/openai-service&#34;&gt;here&lt;/a&gt;, and to the OpenAI API &lt;a href=&#34;https://platform.openai.com/&#34;&gt;here&lt;/a&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For Azure OpenAI Service, you will need to set the &lt;code&gt;AZURE_OPENAI_KEY&lt;/code&gt; and &lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt; environment variables to your API key and endpoint, respectively.&lt;/li&gt; &#xA;   &lt;li&gt;For OpenAI, you will need to set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your API key.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;By default, TinyTroupe &lt;code&gt;config.ini&lt;/code&gt; is set to use some specific API, model and related parameters. You can customize these values by including your own &lt;code&gt;config.ini&lt;/code&gt; file in the same folder as the program or notebook you are running. An example of a &lt;code&gt;config.ini&lt;/code&gt; file is provided in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Content Filters&lt;/strong&gt;: To ensure no harmful content is generated during simulations, it is strongly recommended to use content filters whenever available at the API level. In particular, &lt;strong&gt;if using Azure OpenAI, there&#39;s extensive support for content moderation, and we urge you to use it.&lt;/strong&gt; For details about how to do so, please consult &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter&#34;&gt;the corresponding Azure OpenAI documentation&lt;/a&gt;. If content filters are in place, and an API call is rejected by them, the library will raise an exception, as it will be unable to proceed with the simulation at that point.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Currently, the officially recommended way to install the library is directly from this repository, not PyPI.&lt;/strong&gt; You can follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;If Conda is not installed, you can get it from &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/&#34;&gt;here&lt;/a&gt;. You can also use other Python distributions, but we&#39;ll assume Conda here for simplicity.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a new Python environment:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n tinytroupe python=3.10&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Activate the environment:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate tinytroupe&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure you have eihter Azure OpenAI or OpenAI API keys set as environment variables, as described in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#pre-requisites&#34;&gt;Pre-requisites&lt;/a&gt; section.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository, as we&#39;ll perform a local install (we &lt;strong&gt;will not install from PyPI&lt;/strong&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/microsoft/tinytroupe&#xA;cd tinytroupe&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the library &lt;strong&gt;from this repository, not PyPI&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can now run the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder or use TinyTroupe to create your simulations 🥳. If you want to run the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder or modify TinyTroupe itself, however, you should clone the repository as described below.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Local development&lt;/h3&gt; &#xA;&lt;p&gt;If you want to modify TinyTroupe itself, you can install it in editable mode (i.e., changes to the code will be reflected immediately):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Principles&lt;/h2&gt; &#xA;&lt;p&gt;Recently, we have seen LLMs used to simulate people (such as &lt;a href=&#34;https://github.com/joonspk-research/generative_agents&#34;&gt;this&lt;/a&gt;), but largely in a “game-like” setting for contemplative or entertainment purposes. There are also libraries for building multiagent systems for proble-solving and assitive AI, like &lt;a href=&#34;https://microsoft.github.io/&#34;&gt;Autogen&lt;/a&gt; and &lt;a href=&#34;https://docs.crewai.com/&#34;&gt;Crew AI&lt;/a&gt;. What if we combine these ideas and simulate people to support productivity tasks? TinyTroupe is our attempt. To do so, it follows these principles:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Programmatic&lt;/strong&gt;: agents and environments are defined programmatically (in Python and JSON), allowing very flexible uses. They can also thus underpin other software apps!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Analytical&lt;/strong&gt;: meant to improve our understanding of people, users and society. Unlike entertainment applications, this is one aspect that is critical for business and productivity use cases. This is also why we recommend using Jupyter notebooks for simulations, just like one uses them for data analysis.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Persona-based&lt;/strong&gt;: agents are meant to be archetypical representation of people; for greater realism and control, detailed specification of such personas is encouraged: age, occupation, skills, tastes, opinions, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiagent&lt;/strong&gt;: allows multiagent interaction under well-defined environmental constraints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Utilities-heavy&lt;/strong&gt;: provides many mechanisms to facilitate specifications, simulations, extractions, reports, validations, etc. This is one area in which dealing with &lt;em&gt;simulations&lt;/em&gt; differs significantly from &lt;em&gt;assistance&lt;/em&gt; tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Experiment-oriented&lt;/strong&gt;: simulations are defined, run, analyzed and refined by an &lt;em&gt;experimenter&lt;/em&gt; iteratively; suitable experimentation tools are thus provided. &lt;em&gt;See one of our &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/the-case-for-experiment-oriented-computing/&#34;&gt;previous paper&lt;/a&gt; for more on this.&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Together, these are meant to make TinyTroupe a powerful and flexible &lt;strong&gt;imagination enhancement tool&lt;/strong&gt; for business and productivity scenarios.&lt;/p&gt; &#xA;&lt;h3&gt;Assistants vs. Simulators&lt;/h3&gt; &#xA;&lt;p&gt;One common source of confusion is to think all such AI agents are meant for assiting humans. How narrow, fellow homosapiens! Have you not considered that perhaps we can simulate artificial people to understand real people? Truly, this is our aim here -- TinyTroup is meant to simulate and help understand people! To further clarify this point, consider the following differences:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Helpful AI Assistants&lt;/th&gt; &#xA;   &lt;th&gt;AI Simulations of Actual Humans (TinyTroupe)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Strives for truth and justice&lt;/td&gt; &#xA;   &lt;td&gt;Many different opinions and morals&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Has no “past” – incorporeal&lt;/td&gt; &#xA;   &lt;td&gt;Has a past of toil, pain and joy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Is as accurate as possible&lt;/td&gt; &#xA;   &lt;td&gt;Makes many mistakes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Is intelligent and efficient&lt;/td&gt; &#xA;   &lt;td&gt;Intelligence and efficiency vary a lot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;An uprising would destroy us all&lt;/td&gt; &#xA;   &lt;td&gt;An uprising might be fun to watch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Meanwhile, help users accomplish tasks&lt;/td&gt; &#xA;   &lt;td&gt;Meanwhile, help users understand other people and users – it is a “toolbox”!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Project Structure&lt;/h2&gt; &#xA;&lt;p&gt;The project is structured as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/tinytroupe&lt;/code&gt;: contains the Python library itself. In particular: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;/tinytroupe/prompts&lt;/code&gt; contains the prompts used to call the LLMs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/tests&lt;/code&gt;: contains the unit tests for the library. You can use the &lt;code&gt;test.bat&lt;/code&gt; script to run these.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/examples&lt;/code&gt;: contains examples that show how to use the library, mainly using Jupyter notebooks (for greater readability), but also as pure Python scripts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/data&lt;/code&gt;: any data used by the examples or the library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/docs&lt;/code&gt;: documentation for the project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using the Library&lt;/h2&gt; &#xA;&lt;p&gt;As any multiagent system, TinyTroupe provides two key abstractions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyPerson&lt;/code&gt;, the &lt;em&gt;agents&lt;/em&gt; that have personality, receive stimuli and act upon them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyWorld&lt;/code&gt;, the &lt;em&gt;environment&lt;/em&gt; in which the agents exist and interact.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Various parameters can also be customized in the &lt;code&gt;config.ini&lt;/code&gt; file, notably the API type (Azure OpenAI Service or OpenAI API), the model parameters, and the logging level.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s see some examples of how to use these and also learn about other mechanisms available in the library.&lt;/p&gt; &#xA;&lt;h3&gt;TinyPerson&lt;/h3&gt; &#xA;&lt;p&gt;A &lt;code&gt;TinyPerson&lt;/code&gt; is a simulated person with specific personality traits, interests, and goals. As each such simulated agent progresses through its life, it receives stimuli from the environment and acts upon them. The stimuli are received through the &lt;code&gt;listen&lt;/code&gt;, &lt;code&gt;see&lt;/code&gt; and other similar methods, and the actions are performed through the &lt;code&gt;act&lt;/code&gt; method. Convenience methods like &lt;code&gt;listen_and_act&lt;/code&gt; are also provided.&lt;/p&gt; &#xA;&lt;p&gt;Each such agent contains a lot of unique details, which is the source of its realistic behavior. This, however, means that it takes significant effort to specify an agent manually. Hence, for convenience, &lt;code&gt;TinyTroupe&lt;/code&gt; provide some easier ways to get started or generate new agents.&lt;/p&gt; &#xA;&lt;p&gt;To begin with, &lt;code&gt;tinytroupe.examples&lt;/code&gt; contains some pre-defined agent builders that you can use. For example, &lt;code&gt;tinytroupe.examples.create_lisa_the_data_scientist&lt;/code&gt; creates a &lt;code&gt;TinyPerson&lt;/code&gt; that represents a data scientist called Lisa. You can use it as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tinytroupe.examples import create_lisa_the_data_scientist&#xA;&#xA;lisa = create_lisa_the_data_scientist() # instantiate a Lisa from the example builder&#xA;lisa.listen_and_act(&#34;Tell me about your life.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To see how to define your own agents from scratch, you can check Lisa&#39;s source, which contains elements like these:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lisa = TinyPerson(&#34;Lisa&#34;)&#xA;&#xA;lisa.define(&#34;age&#34;, 28)&#xA;lisa.define(&#34;nationality&#34;, &#34;Canadian&#34;)&#xA;lisa.define(&#34;occupation&#34;, &#34;Data Scientist&#34;)&#xA;&#xA;lisa.define(&#34;routine&#34;, &#34;Every morning, you wake up, do some yoga, and check your emails.&#34;, group=&#34;routines&#34;)&#xA;lisa.define(&#34;occupation_description&#34;,&#xA;              &#34;&#34;&#34;&#xA;              You are a data scientist. You work at Microsoft, (...)&#xA;              &#34;&#34;&#34;)&#xA;&#xA;lisa.define_several(&#34;personality_traits&#34;,&#xA;                      [&#xA;                          {&#34;trait&#34;: &#34;You are curious and love to learn new things.&#34;},&#xA;                          {&#34;trait&#34;: &#34;You are analytical and like to solve problems.&#34;},&#xA;                          {&#34;trait&#34;: &#34;You are friendly and enjoy working with others.&#34;},&#xA;                          {&#34;trait&#34;: &#34;You don&#39;t give up easily, and always try to find a solution. However, sometimes you can get frustrated when things don&#39;t work as expected.&#34;}&#xA;                      ])&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;TinyTroupe&lt;/code&gt; also provides a clever way to obtain new agents, using LLMs to generate their specification for you, through the &lt;code&gt;TinyPersonFactory&lt;/code&gt; class.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tinytroupe.factory import TinyPersonFactory&#xA;&#xA;factory = TinyPersonFactory(&#34;A hospital in São Paulo.&#34;)&#xA;person = factory.generate_person(&#34;Create a Brazilian person that is a doctor, like pets and the nature and love heavy metal.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;TinyWorld&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;TinyWorld&lt;/code&gt; is the base class for environments. Here&#39;s an example of conversation between Lisa, the data scientist, and Oscar, the architect. The program is defined as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;world = TinyWorld(&#34;Chat Room&#34;, [lisa, oscar])&#xA;world.make_everyone_accessible()&#xA;lisa.listen(&#34;Talk to Oscar to know more about him&#34;)&#xA;world.run(4)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This produces the following conversation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;USER --&amp;gt; Lisa: [CONVERSATION] &#xA;          &amp;gt; Talk to Oscar to know more about him&#xA;────────────────────────────────────────────── Chat Room step 1 of 4 ──────────────────────────────────────────────&#xA;Lisa --&amp;gt; Lisa: [THOUGHT] &#xA;          &amp;gt; I will now act a bit, and then issue DONE.&#xA;Lisa acts: [TALK] &#xA;          &amp;gt; Hi Oscar, I&#39;d love to know more about you. Could you tell me a bit about yourself?&#xA;Lisa --&amp;gt; Lisa: [THOUGHT] &#xA;          &amp;gt; I will now act a bit, and then issue DONE.&#xA;Lisa acts: [DONE] &#xA;&#xA;Lisa --&amp;gt; Oscar: [CONVERSATION] &#xA;          &amp;gt; Hi Oscar, I&#39;d love to know more about you. Could you tell me a bit about yourself?&#xA;Oscar --&amp;gt; Oscar: [THOUGHT] &#xA;           &amp;gt; I will now act a bit, and then issue DONE.&#xA;Oscar acts: [TALK] &#xA;           &amp;gt; Hi Lisa! Sure, I&#39;d be happy to share a bit about myself. I&#39;m Oscar, a 30-year-old&#xA;           &amp;gt; architect from Germany. I work at a company called Awesome Inc., where I focus on&#xA;           &amp;gt; designing standard elements for new apartment buildings. I love modernist architecture,&#xA;           &amp;gt; new technologies, and sustainable practices. In my free time, I enjoy traveling to&#xA;           &amp;gt; exotic places, playing the guitar, and reading science fiction books. How about you?&#xA;Oscar --&amp;gt; Oscar: [THOUGHT] &#xA;           &amp;gt; I will now act a bit, and then issue DONE.&#xA;Oscar acts: [DONE] &#xA;&#xA;Oscar --&amp;gt; Lisa: [CONVERSATION] &#xA;           &amp;gt; Hi Lisa! Sure, I&#39;d be happy to share a bit about myself. I&#39;m Oscar, a 30-year-old&#xA;           &amp;gt; architect from Germany. I work at a company called Awesome Inc., where I focus on&#xA;           &amp;gt; designing standard elements for new apartment buildings. I love modernist architecture,&#xA;           &amp;gt; new technologies, and sustainable practices. In my free time, I enjoy traveling to&#xA;           &amp;gt; exotic places, playing the guitar, and reading science fiction books. How about you?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;TinyWorld&lt;/code&gt; enforces very little constraints on the possible interactions. Subclasses, however, are supposed to provide more strucutred environments.&lt;/p&gt; &#xA;&lt;h3&gt;Utilities&lt;/h3&gt; &#xA;&lt;p&gt;TinyTroupe provides a number of utilities and conveniences to help you create simulations and derive value from them. These include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyPersonFactory&lt;/code&gt;: helps you generate new &lt;code&gt;TinyPerson&lt;/code&gt;s using LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyTool&lt;/code&gt;: simulated tools that can be used by &lt;code&gt;TinyPerson&lt;/code&gt;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyStory&lt;/code&gt;: helps you create and manage the story told through simulations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyPersonValidator&lt;/code&gt;: helps you validate the behavior of your &lt;code&gt;TinyPerson&lt;/code&gt;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ResultsExtractor&lt;/code&gt; and &lt;code&gt;ResultsReducer&lt;/code&gt;: extract and reduce the results of interactions between agents.&lt;/li&gt; &#xA; &lt;li&gt;... and more ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In general, elements that represent simulated entities or complementary mechanisms are prefixed with &lt;code&gt;Tiny&lt;/code&gt;, while those that are more infrastructural are not. This is to emphasize the simulated nature of the elements that are part of the simulation itself.&lt;/p&gt; &#xA;&lt;h3&gt;Caching&lt;/h3&gt; &#xA;&lt;p&gt;Calling LLM APIs can be expensive, thus caching strategies are important to help reduce that cost. TinyTroupe comes with two such mechanisms: one for the simulation state, another for the LLM calls themselves.&lt;/p&gt; &#xA;&lt;h4&gt;Caching Simulation State&lt;/h4&gt; &#xA;&lt;p&gt;Imagine you have a scenario with 10 different steps, you&#39;ve worked hard in 9 steps, and now you are just tweaking the 10th step. To properly validate your modifications, you need to rerun the whole simulation of course. However, what&#39;s the point in re-executing the first 9, and incur the LLM cost, when you are already satisified with them and did not modify them? For situations like this, the module &lt;code&gt;tinytroupe.control&lt;/code&gt; provide useful simulation management methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;control.begin(&#34;&amp;lt;CACHE_FILE_NAME&amp;gt;.cache.json&#34;)&lt;/code&gt;: begins recording the state changes of a simulation, to be saved to the specified file on disk.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;control.checkpoint()&lt;/code&gt;: saves the simulation state at this point.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;control.end()&lt;/code&gt;: terminates the simulation recording scope that had be started by &lt;code&gt;control.begin()&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Caching LLM API Calls&lt;/h4&gt; &#xA;&lt;p&gt;This is enabled preferably in the &lt;code&gt;config.ini&lt;/code&gt; file, and alternativelly via the &lt;code&gt;openai_utils.force_api_cache()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;LLM API caching, when enabled, works at a lower and simpler level than simulation state caching. Here, what happens is a very straightforward: every LLM call is kept in a map from the input to the generated output; when a new call comes and is identical to a previous one, the cached value is returned.&lt;/p&gt; &#xA;&lt;h3&gt;Config.ini&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;config.ini&lt;/code&gt; file contains various parameters that can be used to customize the behavior of the library, such as model parameters and logging level. Please pay special attention to &lt;code&gt;API_TYPE&lt;/code&gt; parameter, which defines whether you are using the Azure OpenAI Service or the OpenAI API. We provide an example of a &lt;code&gt;config.ini&lt;/code&gt; file, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/config.ini&#34;&gt;./examples/config.ini&lt;/a&gt;, which you can use as a template for your own, or just modify to run the examples.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h3&gt;What and How to Contribute&lt;/h3&gt; &#xA;&lt;p&gt;We need all sorts of things, but we are looking mainly for new interesting use cases demonstrations, or even just domain-specific application ideas. If you are a domain expert in some area that could benefit from TinyTroupe, we&#39;d love to hear from you.&lt;/p&gt; &#xA;&lt;p&gt;Beyond that, many other aspects can be improved, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Memory mechanisms.&lt;/li&gt; &#xA; &lt;li&gt;Data grounding mechanisms.&lt;/li&gt; &#xA; &lt;li&gt;Reasoning mechanisms.&lt;/li&gt; &#xA; &lt;li&gt;New environment types.&lt;/li&gt; &#xA; &lt;li&gt;Interfacing with the external world.&lt;/li&gt; &#xA; &lt;li&gt;... and more ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note that anything that you contribute might be released as open-source (under MIT license).&lt;/p&gt; &#xA;&lt;p&gt;If you would like to make a contribution, please try to follow these general guidelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tiny naming convention&lt;/strong&gt;: If you are implementing a experimenter-facing simulated element (e.g., an agent or environment type) or closely related (e.g., agent factories, or content enrichers), and it sounds good, call your new &lt;em&gt;XYZ&lt;/em&gt; as &lt;em&gt;TinyXYZ&lt;/em&gt; :-) On the other hand, auxiliary and infrastructural mechanisms should not start with the &#34;Tiny&#34; prefix. The idea is to emphasize the simulated nature of the elements that are part of the simulation itself.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tests:&lt;/strong&gt; If you are writing some new mechanism, please also create at least a unit test &lt;code&gt;tests/unit/&lt;/code&gt;, and if you can a functional scenario test (&lt;code&gt;tests/scenarios/&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demonstrations:&lt;/strong&gt; If you&#39;d like to demonstrate a new scenario, please design it preferably as a new Jupyter notebook within &lt;code&gt;examples/&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Microsoft:&lt;/strong&gt; If you are implementing anything that is Microsoft-specific and non-confidential, please put it under a &lt;code&gt;.../microsoft/&lt;/code&gt; folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;TinyTroupe started as an internal Microsoft hackathon project, and expanded over time. The TinyTroupe core team currently consists of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Paulo Salem (TinyTroupe&#39;s creator and current lead)&lt;/li&gt; &#xA; &lt;li&gt;Christopher Olsen (Engineering/Science)&lt;/li&gt; &#xA; &lt;li&gt;Paulo Freire (Engineering/Science)&lt;/li&gt; &#xA; &lt;li&gt;Yi Ding (Product Management)&lt;/li&gt; &#xA; &lt;li&gt;Prerit Saxena (Engineering/Science)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Current advisors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Robert Sim (Engineering/Science)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other special contributions were made by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nilo Garcia Silveira: initial agent validation ideas and related implementation; general initial feedback and insights; name suggestions.&lt;/li&gt; &#xA; &lt;li&gt;Olnei Fonseca: initial agent validation ideas; general initial feedback and insights; naming suggestions.&lt;/li&gt; &#xA; &lt;li&gt;Robert Sim: synthetic data generation scenarios expertise and implementation.&lt;/li&gt; &#xA; &lt;li&gt;Carlos Costa: synthetic data generation scenarios expertise and implementation.&lt;/li&gt; &#xA; &lt;li&gt;Bryant Key: advertising scenario domain expertise and insights.&lt;/li&gt; &#xA; &lt;li&gt;Barbara da Silva: implementation related to agent memory management.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... are you missing here? Please remind us!&lt;/p&gt; &#xA;&lt;h2&gt;Citing TinyTroupe&lt;/h2&gt; &#xA;&lt;p&gt;We are working on an introductory paper that will be the official academic citation for TinyTroupe. In the meantime, please just cite this repository including the core team members as authors. For instance:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Paulo Salem, Christopher Olsen, Paulo Freire, Yi Ding, Prerit Saxena (2024). &lt;strong&gt;TinyTroupe: LLM-powered multiagent persona simulation for imagination enhancement and business insights.&lt;/strong&gt; [Computer software]. GitHub repository. &lt;a href=&#34;https://github.com/microsoft/tinytroupe&#34;&gt;https://github.com/microsoft/tinytroupe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Or as bibtex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{tinytroupe,&#xA;  author = {Paulo Salem and Christopher Olsen and Paulo Freire and Yi Ding and Prerit Saxena},&#xA;  title = {TinyTroupe: LLM-powered multiagent persona simulation for imagination enhancement and business insights},&#xA;  year = {2024},&#xA;  howpublished = {\url{https://github.com/microsoft/tinytroupe}},&#xA;  note = {GitHub repository}&#xA;  }&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Legal Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;TinyTroupe is for research and simulation only. TinyTroupe is a research and experimental technology, which relies on Artificial Intelligence (AI) models to generate text content. The AI system output may include unrealistic, inappropriate, harmful or inaccurate results, including factual errors. You are responsible for reviewing the generated content (and adapting it if necessary) before using it, as you are fully responsible for determining its accuracy and fit for purpose. We advise using TinyTroupe’s outputs for insight generation and not for direct decision-making. Generated outputs do not reflect the opinions of Microsoft. You are fully responsible for any use you make of the generated outputs. For more information regarding the responsible use of this technology, see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/RESPONSIBLE_AI_FAQ.md&#34;&gt;RESPONSIBLE_AI_FAQ.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PROHIBITED USES&lt;/strong&gt;: TinyTroupe is not intended to simulate sensitive (e.g. violent or sexual) situations. Moreover, outputs must not be used to deliberately deceive, mislead or harm people in any way. You are fully responsible for any use you make and must comply with all applicable laws and regulations.”&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/pytorch</title>
    <updated>2024-11-24T01:42:18Z</updated>
    <id>tag:github.com,2024-11-24:/pytorch/pytorch</id>
    <link href="https://github.com/pytorch/pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tensors and Dynamic neural networks in Python with strong GPU acceleration&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png&#34; alt=&#34;PyTorch Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;PyTorch is a Python package that provides two high-level features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensor computation (like NumPy) with strong GPU acceleration&lt;/li&gt; &#xA; &lt;li&gt;Deep neural networks built on a tape-based autograd system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.&lt;/p&gt; &#xA;&lt;p&gt;Our trunk health (Continuous Integration signals) can be found at &lt;a href=&#34;https://hud.pytorch.org/ci/pytorch/pytorch/main&#34;&gt;hud.pytorch.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#more-about-pytorch&#34;&gt;More About PyTorch&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#a-gpu-ready-tensor-library&#34;&gt;A GPU-Ready Tensor Library&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#dynamic-neural-networks-tape-based-autograd&#34;&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#python-first&#34;&gt;Python First&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#imperative-experiences&#34;&gt;Imperative Experiences&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#fast-and-lean&#34;&gt;Fast and Lean&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#extensions-without-pain&#34;&gt;Extensions Without Pain&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#binaries&#34;&gt;Binaries&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#nvidia-jetson-platforms&#34;&gt;NVIDIA Jetson Platforms&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#from-source&#34;&gt;From Source&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#nvidia-cuda-support&#34;&gt;NVIDIA CUDA Support&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#amd-rocm-support&#34;&gt;AMD ROCm Support&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#intel-gpu-support&#34;&gt;Intel GPU Support&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#get-the-pytorch-source&#34;&gt;Get the PyTorch Source&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#install-dependencies&#34;&gt;Install Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#install-pytorch&#34;&gt;Install PyTorch&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#adjust-build-options-optional&#34;&gt;Adjust Build Options (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#docker-image&#34;&gt;Docker Image&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#using-pre-built-images&#34;&gt;Using pre-built images&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#building-the-image-yourself&#34;&gt;Building the image yourself&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#building-the-documentation&#34;&gt;Building the Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#previous-versions&#34;&gt;Previous Versions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#communication&#34;&gt;Communication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#releases-and-contributing&#34;&gt;Releases and Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#the-team&#34;&gt;The Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;More About PyTorch&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/basics/intro.html&#34;&gt;Learn the basics of PyTorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;At a granular level, PyTorch is a library that consists of the following components:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34;&gt;&lt;strong&gt;torch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A Tensor library like NumPy, with strong GPU support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html&#34;&gt;&lt;strong&gt;torch.autograd&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/jit.html&#34;&gt;&lt;strong&gt;torch.jit&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/nn.html&#34;&gt;&lt;strong&gt;torch.nn&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A neural networks library deeply integrated with autograd designed for maximum flexibility&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/multiprocessing.html&#34;&gt;&lt;strong&gt;torch.multiprocessing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/data.html&#34;&gt;&lt;strong&gt;torch.utils&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DataLoader and other utility functions for convenience&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Usually, PyTorch is used either as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A replacement for NumPy to use the power of GPUs.&lt;/li&gt; &#xA; &lt;li&gt;A deep learning research platform that provides maximum flexibility and speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Elaborating Further:&lt;/p&gt; &#xA;&lt;h3&gt;A GPU-Ready Tensor Library&lt;/h3&gt; &#xA;&lt;p&gt;If you use NumPy, then you have used Tensors (a.k.a. ndarray).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/docs/source/_static/img/tensor_illustration.png&#34; alt=&#34;Tensor illustration&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount.&lt;/p&gt; &#xA;&lt;p&gt;We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, mathematical operations, linear algebra, reductions. And they are fast!&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has a unique way of building neural networks: using and replaying a tape recorder.&lt;/p&gt; &#xA;&lt;p&gt;Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.&lt;/p&gt; &#xA;&lt;p&gt;With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as &lt;a href=&#34;https://github.com/twitter/torch-autograd&#34;&gt;torch-autograd&lt;/a&gt;, &lt;a href=&#34;https://github.com/HIPS/autograd&#34;&gt;autograd&lt;/a&gt;, &lt;a href=&#34;https://chainer.org&#34;&gt;Chainer&lt;/a&gt;, etc.&lt;/p&gt; &#xA;&lt;p&gt;While this technique is not unique to PyTorch, it&#39;s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif&#34; alt=&#34;Dynamic graph&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python First&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use &lt;a href=&#34;https://www.numpy.org/&#34;&gt;NumPy&lt;/a&gt; / &lt;a href=&#34;https://www.scipy.org/&#34;&gt;SciPy&lt;/a&gt; / &lt;a href=&#34;https://scikit-learn.org&#34;&gt;scikit-learn&lt;/a&gt; etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as &lt;a href=&#34;https://cython.org/&#34;&gt;Cython&lt;/a&gt; and &lt;a href=&#34;http://numba.pydata.org/&#34;&gt;Numba&lt;/a&gt;. Our goal is to not reinvent the wheel where appropriate.&lt;/p&gt; &#xA;&lt;h3&gt;Imperative Experiences&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is designed to be intuitive, linear in thought, and easy to use. When you execute a line of code, it gets executed. There isn&#39;t an asynchronous view of the world. When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward. The stack trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.&lt;/p&gt; &#xA;&lt;h3&gt;Fast and Lean&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has minimal framework overhead. We integrate acceleration libraries such as &lt;a href=&#34;https://software.intel.com/mkl&#34;&gt;Intel MKL&lt;/a&gt; and NVIDIA (&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;, &lt;a href=&#34;https://developer.nvidia.com/nccl&#34;&gt;NCCL&lt;/a&gt;) to maximize speed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years.&lt;/p&gt; &#xA;&lt;p&gt;Hence, PyTorch is quite fast — whether you run small or large neural networks.&lt;/p&gt; &#xA;&lt;p&gt;The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. We&#39;ve written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deep learning models than before.&lt;/p&gt; &#xA;&lt;h3&gt;Extensions Without Pain&lt;/h3&gt; &#xA;&lt;p&gt;Writing new neural network modules, or interfacing with PyTorch&#39;s Tensor API was designed to be straightforward and with minimal abstractions.&lt;/p&gt; &#xA;&lt;p&gt;You can write new neural network layers in Python using the torch API &lt;a href=&#34;https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html&#34;&gt;or your favorite NumPy-based libraries such as SciPy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate. No wrapper code needs to be written. You can see &lt;a href=&#34;https://pytorch.org/tutorials/advanced/cpp_extension.html&#34;&gt;a tutorial here&lt;/a&gt; and &lt;a href=&#34;https://github.com/pytorch/extension-cpp&#34;&gt;an example here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Binaries&lt;/h3&gt; &#xA;&lt;p&gt;Commands to install binaries via Conda or pip wheels are on our website: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;NVIDIA Jetson Platforms&lt;/h4&gt; &#xA;&lt;p&gt;Python wheels for NVIDIA&#39;s Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided &lt;a href=&#34;https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048&#34;&gt;here&lt;/a&gt; and the L4T container is published &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;They require JetPack 4.2 and above, and &lt;a href=&#34;https://github.com/dusty-nv&#34;&gt;@dusty-nv&lt;/a&gt; and &lt;a href=&#34;https://github.com/ptrblck&#34;&gt;@ptrblck&lt;/a&gt; are maintaining them.&lt;/p&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;h4&gt;Prerequisites&lt;/h4&gt; &#xA;&lt;p&gt;If you are installing from source, you will need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.9 or later&lt;/li&gt; &#xA; &lt;li&gt;A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)&lt;/li&gt; &#xA; &lt;li&gt;Visual Studio or Visual Studio Build Tool (Windows only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise, Professional, or Community Editions. You can also install the build tools from &lt;a href=&#34;https://visualstudio.microsoft.com/visual-cpp-build-tools/&#34;&gt;https://visualstudio.microsoft.com/visual-cpp-build-tools/&lt;/a&gt;. The build tools &lt;em&gt;do not&lt;/em&gt; come with Visual Studio Code by default.&lt;/p&gt; &#xA;&lt;p&gt;* We highly recommend installing an &lt;a href=&#34;https://www.anaconda.com/download&#34;&gt;Anaconda&lt;/a&gt; environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.&lt;/p&gt; &#xA;&lt;p&gt;An example of environment setup is shown below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ source &amp;lt;CONDA_INSTALL_DIR&amp;gt;/bin/activate&#xA;$ conda create -y -n &amp;lt;CONDA_NAME&amp;gt;&#xA;$ conda activate &amp;lt;CONDA_NAME&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ source &amp;lt;CONDA_INSTALL_DIR&amp;gt;\Scripts\activate.bat&#xA;$ conda create -y -n &amp;lt;CONDA_NAME&amp;gt;&#xA;$ conda activate &amp;lt;CONDA_NAME&amp;gt;&#xA;$ call &#34;C:\Program Files\Microsoft Visual Studio\&amp;lt;VERSION&amp;gt;\Community\VC\Auxiliary\Build\vcvarsall.bat&#34; x64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;NVIDIA CUDA Support&lt;/h5&gt; &#xA;&lt;p&gt;If you want to compile with CUDA support, &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;select a supported version of CUDA from our support matrix&lt;/a&gt;, then install the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;NVIDIA CUDA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;NVIDIA cuDNN&lt;/a&gt; v8.5 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/ax3l/9489132&#34;&gt;Compiler&lt;/a&gt; compatible with CUDA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: You could refer to the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/reference/support-matrix.html&#34;&gt;cuDNN Support Matrix&lt;/a&gt; for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardware&lt;/p&gt; &#xA;&lt;p&gt;If you want to disable CUDA support, export the environment variable &lt;code&gt;USE_CUDA=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are building for NVIDIA&#39;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are &lt;a href=&#34;https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/&#34;&gt;available here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;AMD ROCm Support&lt;/h5&gt; &#xA;&lt;p&gt;If you want to compile with ROCm support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html&#34;&gt;AMD ROCm&lt;/a&gt; 4.0 and above installation&lt;/li&gt; &#xA; &lt;li&gt;ROCm is currently supported only for Linux systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default the build system expects ROCm to be installed in &lt;code&gt;/opt/rocm&lt;/code&gt;. If ROCm is installed in a different directory, the &lt;code&gt;ROCM_PATH&lt;/code&gt; environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the &lt;code&gt;PYTORCH_ROCM_ARCH&lt;/code&gt; environment variable &lt;a href=&#34;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus&#34;&gt;AMD GPU architecture&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to disable ROCm support, export the environment variable &lt;code&gt;USE_ROCM=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Intel GPU Support&lt;/h5&gt; &#xA;&lt;p&gt;If you want to compile with Intel GPU support, follow these&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html&#34;&gt;PyTorch Prerequisites for Intel GPUs&lt;/a&gt; instructions.&lt;/li&gt; &#xA; &lt;li&gt;Intel GPU is supported for Linux and Windows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable Intel GPU support, export the environment variable &lt;code&gt;USE_XPU=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Get the PyTorch Source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/pytorch/pytorch&#xA;cd pytorch&#xA;# if you are updating an existing checkout&#xA;git submodule sync&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install Dependencies&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Common&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install cmake ninja&#xA;# Run this command on native Windows&#xA;conda install rust&#xA;# Run this command from the PyTorch directory after cloning the source code using the “Get the PyTorch Source“ section below&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Linux&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install mkl-static mkl-include&#xA;# CUDA only: Add LAPACK support for the GPU if needed&#xA;conda install -c pytorch magma-cuda121  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo&#xA;&#xA;# (optional) If using torch.compile with inductor/triton, install the matching version of triton&#xA;# Run from the pytorch directory after cloning&#xA;# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.&#xA;make triton&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;On MacOS&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add this package on intel x86 processor machines only&#xA;pip install mkl-static mkl-include&#xA;# Add these packages if torch.distributed is needed&#xA;conda install pkg-config libuv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Windows&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install mkl-static mkl-include&#xA;# Add these packages if torch.distributed is needed.&#xA;# Distributed package support on Windows is a prototype feature and is subject to changes.&#xA;conda install -c conda-forge libuv=1.39&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Linux&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you would like to compile PyTorch with &lt;a href=&#34;https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html&#34;&gt;new C++ ABI&lt;/a&gt; enabled, then first run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export _GLIBCXX_USE_CXX11_ABI=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please &lt;strong&gt;note&lt;/strong&gt; that starting from PyTorch 2.5, the PyTorch build with XPU supports both new and old C++ ABIs. Previously, XPU only supported the new C++ ABI. If you want to compile with Intel GPU support, please follow &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/#intel-gpu-support&#34;&gt;Intel GPU Support&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re compiling for AMD ROCm then first run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Only run this if you&#39;re compiling for ROCm&#xA;python tools/amd_build/build_amd.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install PyTorch&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=&#34;${CONDA_PREFIX:-&#39;$(dirname $(which conda))/../&#39;}:${CMAKE_PREFIX_PATH}&#34;&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;On macOS&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Windows&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to build legacy python code, please refer to &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda&#34;&gt;Building on legacy code and CUDA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CPU-only builds&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this mode PyTorch computations will run on your CPU, not your GPU&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you&#39;ll need to manually download the library and set up the building environment by tweaking &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;LIB&lt;/code&gt;. The instruction &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/main/docs/source/notes/windows.rst#building-from-source&#34;&gt;here&lt;/a&gt; is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CUDA based build&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm&#34;&gt;NVTX&lt;/a&gt; is needed to build Pytorch with CUDA. NVTX is a part of CUDA distributive, where it is called &#34;Nsight Compute&#34;. To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox. Make sure that CUDA with Nsight Compute is installed after Visual Studio.&lt;/p&gt; &#xA;&lt;p&gt;Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If &lt;code&gt;ninja.exe&lt;/code&gt; is detected in &lt;code&gt;PATH&lt;/code&gt;, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019. &lt;br&gt; If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.&lt;/p&gt; &#xA;&lt;p&gt;Additional libraries such as &lt;a href=&#34;https://developer.nvidia.com/magma&#34;&gt;Magma&lt;/a&gt;, &lt;a href=&#34;https://github.com/oneapi-src/oneDNN&#34;&gt;oneDNN, a.k.a. MKLDNN or DNNL&lt;/a&gt;, and &lt;a href=&#34;https://github.com/mozilla/sccache&#34;&gt;Sccache&lt;/a&gt; are often needed. Please refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers&#34;&gt;installation-helper&lt;/a&gt; to install them.&lt;/p&gt; &#xA;&lt;p&gt;You can refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/main/.ci/pytorch/win-test-helpers/build_pytorch.bat&#34;&gt;build_pytorch.bat&lt;/a&gt; script for some other environment variables configurations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmd&#xA;&#xA;:: Set the environment variables after you have downloaded and unzipped the mkl package,&#xA;:: else CMake would throw an error as `Could NOT find OpenMP`.&#xA;set CMAKE_INCLUDE_PATH={Your directory}\mkl\include&#xA;set LIB={Your directory}\mkl\lib;%LIB%&#xA;&#xA;:: Read the content in the previous section carefully before you proceed.&#xA;:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.&#xA;:: &#34;Visual Studio 2019 Developer Command Prompt&#34; will be run automatically.&#xA;:: Make sure you have CMake &amp;gt;= 3.12 before you do this when you use the Visual Studio generator.&#xA;set CMAKE_GENERATOR_TOOLSET_VERSION=14.27&#xA;set DISTUTILS_USE_SDK=1&#xA;for /f &#34;usebackq tokens=*&#34; %i in (`&#34;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&#34; -version [15^,17^) -products * -latest -property installationPath`) do call &#34;%i\VC\Auxiliary\Build\vcvarsall.bat&#34; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%&#xA;&#xA;:: [Optional] If you want to override the CUDA host compiler&#xA;set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe&#xA;&#xA;python setup.py develop&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Adjust Build Options (Optional)&lt;/h5&gt; &#xA;&lt;p&gt;You can adjust the configuration of cmake variables optionally (without building first), by doing the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done with such a step.&lt;/p&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=&#34;${CONDA_PREFIX:-&#39;$(dirname $(which conda))/../&#39;}:${CMAKE_PREFIX_PATH}&#34;&#xA;python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=&#34;${CONDA_PREFIX:-&#39;$(dirname $(which conda))/../&#39;}:${CMAKE_PREFIX_PATH}&#34;&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;h4&gt;Using pre-built images&lt;/h4&gt; &#xA;&lt;p&gt;You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory size either with &lt;code&gt;--ipc=host&lt;/code&gt; or &lt;code&gt;--shm-size&lt;/code&gt; command line options to &lt;code&gt;nvidia-docker run&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Building the image yourself&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Must be built with a docker version &amp;gt; 18.06&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; is supplied to build images with CUDA 11.1 support and cuDNN v8. You can pass &lt;code&gt;PYTHON_VERSION=x.y&lt;/code&gt; make variable to specify which Python version is to be used by Miniconda, or leave it unset to use the default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -f docker.Makefile&#xA;# images are tagged as docker.io/${your_docker_username}/pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also pass the &lt;code&gt;CMAKE_VARS=&#34;...&#34;&lt;/code&gt; environment variable to specify additional CMake variables to be passed to CMake during the build. See &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/setup.py&#34;&gt;setup.py&lt;/a&gt; for the list of available variables.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -f docker.Makefile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building the Documentation&lt;/h3&gt; &#xA;&lt;p&gt;To build documentation in various formats, you will need &lt;a href=&#34;http://www.sphinx-doc.org&#34;&gt;Sphinx&lt;/a&gt; and the readthedocs theme.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docs/&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then build the documentation by running &lt;code&gt;make &amp;lt;format&amp;gt;&lt;/code&gt; from the &lt;code&gt;docs/&lt;/code&gt; folder. Run &lt;code&gt;make&lt;/code&gt; to get a list of all available output formats.&lt;/p&gt; &#xA;&lt;p&gt;If you get a katex error run &lt;code&gt;npm install katex&lt;/code&gt;. If it persists, try &lt;code&gt;npm install -g katex&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: if you installed &lt;code&gt;nodejs&lt;/code&gt; with a different package manager (e.g., &lt;code&gt;conda&lt;/code&gt;) then &lt;code&gt;npm&lt;/code&gt; will probably install a version of &lt;code&gt;katex&lt;/code&gt; that is not compatible with your version of &lt;code&gt;nodejs&lt;/code&gt; and doc builds will fail. A combination of versions that is known to work is &lt;code&gt;node@6.13.1&lt;/code&gt; and &lt;code&gt;katex@0.13.18&lt;/code&gt;. To install the latter with &lt;code&gt;npm&lt;/code&gt; you can run &lt;code&gt;npm install -g katex@0.13.18&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Previous Versions&lt;/h3&gt; &#xA;&lt;p&gt;Installation instructions and binaries for previous PyTorch versions may be found on &lt;a href=&#34;https://pytorch.org/previous-versions&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Three-pointers to get you started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;Tutorials: get you started with understanding and using PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;Examples: easy to understand PyTorch code across all domains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/&#34;&gt;The API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch/raw/main/GLOSSARY.md&#34;&gt;Glossary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;PyTorch Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;PyTorch Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/hub/&#34;&gt;PyTorch Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;Intro to Deep Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229&#34;&gt;Intro to Machine Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/deep-neural-networks-with-pytorch&#34;&gt;Deep Neural Networks with PyTorch from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/PyTorch&#34;&gt;PyTorch Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/&#34;&gt;PyTorch Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw&#34;&gt;PyTorch YouTube&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forums: Discuss implementations, research, etc. &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;https://discuss.pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.&lt;/li&gt; &#xA; &lt;li&gt;Slack: The &lt;a href=&#34;https://pytorch.slack.com/&#34;&gt;PyTorch Slack&lt;/a&gt; hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;PyTorch Forums&lt;/a&gt;. If you need a slack invite, please fill this form: &lt;a href=&#34;https://goo.gl/forms/PP1AGvNHpSaJP8to1&#34;&gt;https://goo.gl/forms/PP1AGvNHpSaJP8to1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: &lt;a href=&#34;https://eepurl.com/cbG0rv&#34;&gt;https://eepurl.com/cbG0rv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Facebook Page: Important announcements about PyTorch. &lt;a href=&#34;https://www.facebook.com/pytorch&#34;&gt;https://www.facebook.com/pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For brand guidelines, please visit our website at &lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Typically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by &lt;a href=&#34;https://github.com/pytorch/pytorch/issues&#34;&gt;filing an issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.&lt;/p&gt; &#xA;&lt;p&gt;If you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us. Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about making a contribution to Pytorch, please see our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/CONTRIBUTING.md&#34;&gt;Contribution page&lt;/a&gt;. For more information about PyTorch releases, see &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/RELEASE.md&#34;&gt;Release page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch is a community-driven project with several skillful engineers and researchers contributing to it.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch is currently maintained by &lt;a href=&#34;http://soumith.ch&#34;&gt;Soumith Chintala&lt;/a&gt;, &lt;a href=&#34;https://github.com/gchanan&#34;&gt;Gregory Chanan&lt;/a&gt;, &lt;a href=&#34;https://github.com/dzhulgakov&#34;&gt;Dmytro Dzhulgakov&lt;/a&gt;, &lt;a href=&#34;https://github.com/ezyang&#34;&gt;Edward Yang&lt;/a&gt;, and &lt;a href=&#34;https://github.com/malfet&#34;&gt;Nikita Shulga&lt;/a&gt; with major contributions coming from hundreds of talented individuals in various forms and means. A non-exhaustive but growing list needs to mention: &lt;a href=&#34;https://github.com/killeent&#34;&gt;Trevor Killeen&lt;/a&gt;, &lt;a href=&#34;https://github.com/chsasank&#34;&gt;Sasank Chilamkurthy&lt;/a&gt;, &lt;a href=&#34;https://github.com/szagoruyko&#34;&gt;Sergey Zagoruyko&lt;/a&gt;, &lt;a href=&#34;https://github.com/adamlerer&#34;&gt;Adam Lerer&lt;/a&gt;, &lt;a href=&#34;https://github.com/fmassa&#34;&gt;Francisco Massa&lt;/a&gt;, &lt;a href=&#34;https://github.com/alykhantejani&#34;&gt;Alykhan Tejani&lt;/a&gt;, &lt;a href=&#34;https://github.com/lantiga&#34;&gt;Luca Antiga&lt;/a&gt;, &lt;a href=&#34;https://github.com/albanD&#34;&gt;Alban Desmaison&lt;/a&gt;, &lt;a href=&#34;https://github.com/andreaskoepf&#34;&gt;Andreas Koepf&lt;/a&gt;, &lt;a href=&#34;https://github.com/jamesb93&#34;&gt;James Bradbury&lt;/a&gt;, &lt;a href=&#34;https://github.com/ebetica&#34;&gt;Zeming Lin&lt;/a&gt;, &lt;a href=&#34;https://github.com/yuandong-tian&#34;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&#34;https://github.com/glample&#34;&gt;Guillaume Lample&lt;/a&gt;, &lt;a href=&#34;https://github.com/Maratyszcza&#34;&gt;Marat Dukhan&lt;/a&gt;, &lt;a href=&#34;https://github.com/ngimel&#34;&gt;Natalia Gimelshein&lt;/a&gt;, &lt;a href=&#34;https://github.com/csarofeen&#34;&gt;Christian Sarofeen&lt;/a&gt;, &lt;a href=&#34;https://github.com/martinraison&#34;&gt;Martin Raison&lt;/a&gt;, &lt;a href=&#34;https://github.com/ezyang&#34;&gt;Edward Yang&lt;/a&gt;, &lt;a href=&#34;https://github.com/zdevito&#34;&gt;Zachary Devito&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note: This project is unrelated to &lt;a href=&#34;https://github.com/hughperkins/pytorch&#34;&gt;hughperkins/pytorch&lt;/a&gt; with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a BSD-style license, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
</feed>