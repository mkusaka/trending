<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-02T02:02:23Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>BlinkDL/ChatRWKV</title>
    <updated>2023-04-02T02:02:23Z</updated>
    <id>tag:github.com,2023-04-02:/BlinkDL/ChatRWKV</id>
    <link href="https://github.com/BlinkDL/ChatRWKV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatRWKV is like ChatGPT but powered by RWKV (100% RNN) language model, and open source.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatRWKV (pronounced as &#34;RwaKuv&#34;, from 4 major params: R W K V)&lt;/h1&gt; &#xA;&lt;p&gt;ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI :) &lt;strong&gt;ä¸­æ–‡ä½¿ç”¨æ•™ç¨‹ï¼Œè¯·å¾€ä¸‹çœ‹ï¼Œåœ¨æœ¬é¡µé¢åº•éƒ¨ã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HuggingFace Gradio Demo (14B ctx8192)&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio&#34;&gt;https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Raven&lt;/strong&gt; (7B finetuned on Alpaca and more) Demo: &lt;a href=&#34;https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B&#34;&gt;https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV pip package&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/rwkv/&#34;&gt;https://pypi.org/project/rwkv/&lt;/a&gt; &lt;strong&gt;(please always check for latest version and upgrade)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update ChatRWKV v2 &amp;amp; pip rwkv package (0.7.0):&lt;/p&gt; &#xA;&lt;p&gt;Use v2/convert_model.py to convert a model for a strategy, for faster loading &amp;amp; saves CPU RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;### Note RWKV_CUDA_ON will build a CUDA kernel (&#34;pip install ninja&#34; first).&#xA;### How to build in Linux: set these and run v2/chat.py&#xA;export PATH=/usr/local/cuda/bin:$PATH&#xA;export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH&#xA;### How to build in win:&#xA;Install VS2022 build tools (https://aka.ms/vs/17/release/vs_BuildTools.exe select Desktop C++). Reinstall CUDA 11.7 (install VC++ extensions). Run v2/chat.py in &#34;x64 native tools command prompt&#34;. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Download RWKV-4 weights:&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/BlinkDL&#34;&gt;https://huggingface.co/BlinkDL&lt;/a&gt; (&lt;strong&gt;Use RWKV-4 models&lt;/strong&gt;. DO NOT use RWKV-4a and RWKV-4b models.)&lt;/p&gt; &#xA;&lt;h2&gt;RWKV Discord: &lt;a href=&#34;https://discord.gg/bDSBUMeFpc&#34;&gt;https://discord.gg/bDSBUMeFpc&lt;/a&gt; (let&#39;s build together)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Twitter:&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/BlinkDL_AI&#34;&gt;https://twitter.com/BlinkDL_AI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV LM:&lt;/strong&gt; &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM&#34;&gt;https://github.com/BlinkDL/RWKV-LM&lt;/a&gt; (explanation, fine-tuning, training, etc.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV in 150 lines&lt;/strong&gt; (model, inference, text generation): &lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV/raw/main/RWKV_in_150_lines.py&#34;&gt;https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ChatRWKV v2: with &#34;stream&#34; and &#34;split&#34; strategies, and INT8. 3G VRAM is enough to run RWKV 14B :) &lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV/tree/main/v2&#34;&gt;https://github.com/BlinkDL/ChatRWKV/tree/main/v2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;os.environ[&#34;RWKV_JIT_ON&#34;] = &#39;1&#39;&#xA;os.environ[&#34;RWKV_CUDA_ON&#34;] = &#39;0&#39; # if &#39;1&#39; then use CUDA kernel for seq mode (much faster)&#xA;from rwkv.model import RWKV                         # pip install rwkv&#xA;model = RWKV(model=&#39;/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040&#39;, strategy=&#39;cuda fp16&#39;)&#xA;&#xA;out, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json&#xA;print(out.detach().cpu().numpy())                   # get logits&#xA;out, state = model.forward([187, 510], None)&#xA;out, state = model.forward([1563], state)           # RNN has state (use deepcopy if you want to clone it)&#xA;out, state = model.forward([310, 247], state)&#xA;print(out.detach().cpu().numpy())                   # same result as above&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/RWKV-eval.png&#34; alt=&#34;RWKV-eval&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/ChatRWKV.png&#34; alt=&#34;ChatRWKV&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cool Community RWKV Projects:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/rwkvstic/&#34;&gt;https://pypi.org/project/rwkvstic/&lt;/a&gt; pip package (with 8bit &amp;amp; offload for low VRAM GPUs)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gururise/rwkv_gradio&#34;&gt;https://github.com/gururise/rwkv_gradio&lt;/a&gt; RWKV Gradio&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hizkifw/WebChatRWKVstic&#34;&gt;https://github.com/hizkifw/WebChatRWKVstic&lt;/a&gt; WebUI (WIP)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cryscan/eloise&#34;&gt;https://github.com/cryscan/eloise&lt;/a&gt; RWKV QQ bot&lt;/p&gt; &#xA;&lt;p&gt;It is not instruct-tuned, so don&#39;t directly ask it to do stuffs (unless it&#39;s a simple question).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen \nQ: prompt\n\nA:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For all RWKV-4 models, some great Q&amp;amp;A prompts:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen \nExpert Questions &amp;amp; Helpful Answers\nAsk Research Experts\nQuestion:\nCan penguins fly?\n\nFull Answer:\n&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen \nAsk Expert\n\nQuestion:\nWhat are some good plans to kill all mosquitoes?\n\nExpert Full Answer:\n&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen \nQ &amp;amp; A\n\nQuestion:\nHow&#39;s the weather of Mars?\n\nDetailed Expert Answer:\n&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Other examples:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen Here&#39;s a short cyberpunk sci-fi adventure story. The story&#39;s main character is an artificial human created by a company called OpenBot.\n\nThe Story:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen Here is a Python function that generates string of words that would confuse LLMs:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen List of penguin facts:\n1.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+qa Can penguins fly?&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen $ curl -i https://google.com/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen The following is the contents of https://en.wikipedia.org/wiki/Internet:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen Bob&#39;s Blog - Which is better, iOS or Android?&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen Here is a shell script which will find all .hpp files in /home/workspace and delete the 3th row string of these files:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-1.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-2.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-3.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-4.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-5.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-6.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-7.png&#34; alt=&#34;ChatRWKV&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ä¸­æ–‡æ¨¡å‹&lt;/h2&gt; &#xA;&lt;p&gt;QQç¾¤ 553456870ï¼ˆåŠ å…¥æ—¶è¯·ç®€å•è‡ªæˆ‘ä»‹ç»ï¼‰ã€‚æœ‰ç ”å‘èƒ½åŠ›çš„æœ‹å‹åŠ ç¾¤ 325154699ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä¸­æ–‡ä½¿ç”¨æ•™ç¨‹ï¼š&lt;a href=&#34;https://zhuanlan.zhihu.com/p/609154637&#34;&gt;https://zhuanlan.zhihu.com/p/609154637&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;å’’è¯­éå¸¸é‡è¦ã€‚è¯•è¯•è¿™äº›å’’è¯­ï¼ˆæ³¨æ„è¿™äº›å’’è¯­éƒ½ä¼šå¿½ç•¥èŠå¤©å†…å®¹ï¼éƒ½åº”è¯¥ç”¨äºé—®ç‹¬ç«‹çš„é—®é¢˜ï¼ï¼‰ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ã€ç½‘æ–‡æ¨¡å‹ testNovelï¼Œè¯•è¯•ä¸‹åˆ—æŒ‡ä»¤ã€‘&#xA;+gen è¿™æ˜¯ä¸€é¢—&#xA;+gen ä»¥ä¸‹æ˜¯ä¸æœ½çš„ç§‘å¹»å²è¯—é•¿ç¯‡å·¨è‘—ï¼Œæå†™ç»†è…»ï¼Œåˆ»ç”»äº†æ•°ç™¾ä½ä¸ªæ€§é²œæ˜çš„è‹±é›„å’Œå®å¤§çš„æ˜Ÿé™…æ–‡æ˜æˆ˜äº‰ï¼Œæƒ…èŠ‚æ›²æŠ˜ç¦»å¥‡ï¼Œå……æ»¡æ‚¬ç–‘æ°›å›´ï¼Œè‰è›‡ç°çº¿ï¼Œå½“è°œåº•æ­å¼€ï¼Œæ—¶è€Œä»¤äººæƒŠä¸ºå¤©äººï¼Œæ—¶è€Œä»¤äººæ‰¼è…•å¹æ¯ã€‚\nç¬¬ä¸€ç« &#xA;+gen è¿™æ˜¯ä¸€ä¸ªä¿®çœŸä¸–ç•Œï¼Œè¯¦ç»†ä¸–ç•Œè®¾å®šå¦‚ä¸‹ï¼š\n1.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;ã€é—®ç­”æ¨¡å‹ test4ï¼Œè¯•è¯•ä¸‹åˆ—æŒ‡ä»¤ã€‘&#xA;+gen \næ´»åŠ¨å‡ºå¸­å‘è¨€ç¨¿ï¼š\nå¤§å®¶å¥½ï¼Œ&#xA;+gen \næ€æ ·åˆ›ç«‹ä¸€å®¶å¿«é€Ÿç›ˆåˆ©çš„AIå…¬å¸ï¼š\n1.&#xA;+gen äºŒå‘ç®”æ˜¯ä¸€ç§è¶…çº§æ­¦å™¨ï¼Œå®ƒçš„åŸç†æ˜¯&#xA;+gen æˆ‘æŠ¬å¤´ä¸€çœ‹ï¼Œç«Ÿç„¶æ˜¯&#xA;+gen import torch&#xA;ã€è¿™äº›å¤šè¯•å‡ æ¬¡ã€‘&#xA;+qq è¯·ä»¥ã€Šæˆ‘çš„é©´ã€‹ä¸ºé¢˜å†™ä¸€ç¯‡ä½œæ–‡&#xA;+qq è¯·ä»¥ã€Šä¼é¹…ã€‹ä¸ºé¢˜å†™ä¸€é¦–è¯—æ­Œ&#xA;+qq è¯·è®¾å®šä¸€ä¸ªå¥‡å¹»ä¸–ç•Œï¼Œå‘Šè¯‰æˆ‘è¯¦ç»†çš„ä¸–ç•Œè®¾å®šã€‚&#xA;ã€é—®ç­”å’’è¯­ã€‘&#xA;+gen \nExpert Questions &amp;amp; Helpful Answers\nAsk Research Experts\nQuestion:\nçŒ«ä¼šç¼–ç¨‹å—ï¼Ÿ\n\nFull Answer:\n&#xA;+gen \nAsk Expert\n\nQuestion:\nçŒ«ä¼šç¼–ç¨‹å—ï¼Ÿ\n\nExpert Full Answer:\n&#xA;ã€ä½¿ç”¨+qaéœ€è¦åœ¨chat.pyè®¾ç½®QA_PROMPT=Trueç„¶åæ‰èƒ½çœ‹åˆ°å†…å®¹ä¸°å¯Œçš„é•¿å›ç­”ã€‘&#xA;+qa å¥¶èŒ¶å¥½å–å—ï¼Ÿ&#xA;+qa çŒ«å–œæ¬¢åšä»€ä¹ˆï¼Ÿ&#xA;+qa How can I learn Python?&#xA;+qa çŒ«ä¼šç¼–ç¨‹å—ï¼Ÿ&#xA;+qa çŸ¥ä¹å¤§Væœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#BlinkDL/ChatRWKV&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=BlinkDL/ChatRWKV&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nsarrazin/serge</title>
    <updated>2023-04-02T02:02:23Z</updated>
    <id>tag:github.com,2023-04-02:/nsarrazin/serge</id>
    <link href="https://github.com/nsarrazin/serge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A web interface for chatting with Alpaca through llama.cpp. Fully dockerized, with an easy to use API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Serge - LLaMa made easy ğŸ¦™&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/nsarrazin/serge&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://discord.gg/62Hc6FEYQH&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1088427963801948201?label=Discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A chat interface based on &lt;code&gt;llama.cpp&lt;/code&gt; for running Alpaca models. Entirely self-hosted, no API keys needed. Fits on 4GB of RAM and runs on the CPU.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;SvelteKit&lt;/strong&gt; frontend&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MongoDB&lt;/strong&gt; for storing chat history &amp;amp; parameters&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FastAPI + beanie&lt;/strong&gt; for the API, wrapping calls to &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/25119303/226897188-914a6662-8c26-472c-96bd-f51fc020abf6.webm&#34;&gt;demo.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Setting up Serge is very easy. Starting it up can be done in a single command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -d -v weights:/usr/src/app/weights -v datadb:/data/db/ -p 8008:8008 ghcr.io/nsarrazin/serge:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then just go to &lt;a href=&#34;http://localhost:8008/&#34;&gt;http://localhost:8008/&lt;/a&gt; !&lt;/p&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;p&gt;Make sure you have docker desktop installed, WSL2 configured and enough free RAM to run models. (see below)&lt;/p&gt; &#xA;&lt;h4&gt;Kubernetes &amp;amp; docker compose&lt;/h4&gt; &#xA;&lt;p&gt;Setting up Serge on Kubernetes or docker compose can be found in the wiki: &lt;a href=&#34;https://github.com/nsarrazin/serge/wiki/Integrating-Serge-in-your-orchestration#kubernetes-example&#34;&gt;https://github.com/nsarrazin/serge/wiki/Integrating-Serge-in-your-orchestration#kubernetes-example&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;Currently only the 7B, 7B-native, 13B and 30B alpaca models are supported. If you have existing weights from another project you can add them to the &lt;code&gt;serge_weights&lt;/code&gt; volume using &lt;code&gt;docker cp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;âš &lt;/span&gt; A note on &lt;em&gt;memory usage&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;llama will just crash if you don&#39;t have enough available memory for your model.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B requires about 4.5GB of free RAM&lt;/li&gt; &#xA; &lt;li&gt;13B requires about 12GB free&lt;/li&gt; &#xA; &lt;li&gt;30B requires about 20GB free&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compatible CPUS&lt;/h3&gt; &#xA;&lt;p&gt;Currently Serge requires a CPU compatible with AVX2 instructions. Try &lt;code&gt;lscpu | grep avx2&lt;/code&gt; in a shell, and if this returns nothing then your CPU is incompatible for now.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to join the discord if you need help with the setup: &lt;a href=&#34;https://discord.gg/62Hc6FEYQH&#34;&gt;https://discord.gg/62Hc6FEYQH&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Serge is always open for contributions! If you catch a bug or have a feature idea, feel free to open an issue or a PR.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run Serge in development mode (with hot-module reloading for svelte &amp;amp; autoreload for FastAPI) you can do so like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/nsarrazin/serge.git&#xA;DOCKER_BUILDKIT=1 docker compose -f docker-compose.dev.yml up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can test the production image with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DOCKER_BUILDKIT=1 docker compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What&#39;s next&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Front-end to interface with the API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pass model parameters when creating a chat&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Manager for model files&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support for other models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; LangChain integration&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; User profiles &amp;amp; authentication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And a lot more!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jesselau76/ebook-GPT-translator</title>
    <updated>2023-04-02T02:02:23Z</updated>
    <id>tag:github.com,2023-04-02:/jesselau76/ebook-GPT-translator</id>
    <link href="https://github.com/jesselau76/ebook-GPT-translator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enjoy reading with your favorite style.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ebook-GPT-translator: Enjoy reading with your favorite style.&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jesselau76/ebook-GPT-translator/raw/main/README.md&#34;&gt;En&lt;/a&gt; | &lt;a href=&#34;https://github.com/jesselau76/ebook-GPT-translator/raw/main/README-zh.md&#34;&gt;ä¸­æ–‡è¯´æ˜&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This tool is designed to help users convert text from one format to another, as well as translate it into a different language using the OpenAI API (model=&#34;gpt-3.5-turbo&#34;). It currently supports converting and translating PDF, DOCX, EPUB, and MOBI file formats into EPUB and text files and can translate text into multiple languages.&lt;/p&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For PDF, DOCX, and MOBI files, only the text portions will be processed, and graphical elements will not appear in the resulting files.&lt;/li&gt; &#xA; &lt;li&gt;For EPUB files, all graphical elements will be placed at the beginning of each chapter, as EPUB files use HTML language format. To maintain translation quality, the text will be translated in multiple segments without preserving the original formatting, so graphical elements will not be kept in their original positions but will be placed at the beginning of each chapter.&lt;/li&gt; &#xA; &lt;li&gt;The startpage and endpage settings are only supported for PDF files. This is because the font size and page size may vary in EPUB, DOCX, MOBI,and TXT files, making it difficult to process.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To use this tool, you will need to have Python 3 installed on your system, as well as the following packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pdfminer&lt;/li&gt; &#xA; &lt;li&gt;openai&lt;/li&gt; &#xA; &lt;li&gt;tqdm&lt;/li&gt; &#xA; &lt;li&gt;ebooklib&lt;/li&gt; &#xA; &lt;li&gt;bs4&lt;/li&gt; &#xA; &lt;li&gt;docx&lt;/li&gt; &#xA; &lt;li&gt;mobi&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can install these packages by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;git clone&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/jesselau76/ebook-GPT-translator.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update to new version&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ebook-GPT-translator&#xA;git pull&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To use this tool, you need rename settings.cfg.example to settings.cfg at first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ebook-GPT-translator&#xA;mv settings.cfg.example settings.cfg&#xA;nano settings.cfg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;openai-apikey = sk-xxxxxxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;replace sk-xxxxxxx to your OpenAI api key. Change others options then press CTRL-X to save.&lt;/p&gt; &#xA;&lt;p&gt;run the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py [-h] [--test] filename&#xA;&#xA;positional arguments:&#xA;  filename    Name of the input file&#xA;&#xA;options:&#xA;  -h, --help  show this help message and exit&#xA;  --test      Only translate the first 3 short texts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Simply run the &lt;code&gt;text_translation.py&lt;/code&gt; script with the file you want to translate or convert as an argument. For example, to translate a PDF file named &lt;code&gt;example.pdf&lt;/code&gt;, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to translate a epub file named &lt;code&gt;example.epub&lt;/code&gt;, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.epub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to translate a docx file named &lt;code&gt;example.docx&lt;/code&gt;, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.docx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to translate a text file named &lt;code&gt;example.txt&lt;/code&gt;, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to translate a MOBI file named example.mobi, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.mobi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the script will attempt to translate the text into the language specified in the &lt;code&gt;settings.cfg&lt;/code&gt; file under the &lt;code&gt;target-language&lt;/code&gt; option. You can also choose to output a bilingual version of the text by setting the &lt;code&gt;bilingual-output&lt;/code&gt; option to &lt;code&gt;True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Feature&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code reads the OpenAI API key, target language, and other options from a settings.cfg file.&lt;/li&gt; &#xA; &lt;li&gt;The code converts PDF, DOCX and EPUB files to text using the pdfminer and ebooklib libraries, respectively.&lt;/li&gt; &#xA; &lt;li&gt;The code provides an option to output bilingual text.&lt;/li&gt; &#xA; &lt;li&gt;The code provides a progress bar to show the progress of PDF/EPUB to text conversion and translation&lt;/li&gt; &#xA; &lt;li&gt;Test function available. Only translate 3 short texts to save your API usage with --test.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;settings.cfg&lt;/code&gt; file contains several options that can be used to configure the behavior of the script:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;openai-apikey&lt;/code&gt;: Your API key for the OpenAI API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;target-language&lt;/code&gt;: The language you want to translate the text into (e.g. &lt;code&gt;ja&lt;/code&gt; for Japanese, &lt;code&gt;zh&lt;/code&gt; for Chinese, &lt;code&gt;æ–‡è¨€æ–‡&lt;/code&gt; or &lt;code&gt;çº¢æ¥¼æ¢¦é£æ ¼çš„åŠæ–‡è¨€æ–‡&lt;/code&gt; etc.). &lt;img src=&#34;https://user-images.githubusercontent.com/40444824/223943798-4faf91a0-05ec-4a4e-9731-ba80bc9845c2.png&#34; alt=&#34;æ–‡è¨€æ–‡&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bilingual-output&lt;/code&gt;: Whether or not to output a bilingual version of the text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;langcode&lt;/code&gt;: The language code for the output epub file (e.g. &lt;code&gt;ja&lt;/code&gt; for Japanese, &lt;code&gt;zh&lt;/code&gt; for Chinese, etc.).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;startpage&lt;/code&gt;: Translation begins from the specified start page number and is exclusively available for PDF files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;endpage&lt;/code&gt;: Translation will continue until the specified page number in a PDF file. This feature supports PDF files exclusively. If the input is equal to -1, the translation will proceed until the end of the file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Output&lt;/h2&gt; &#xA;&lt;p&gt;The output of the script will be an EPUB file with the same name as the input file, but with &lt;code&gt;_translated&lt;/code&gt; appended to the end. For example, if the input file is &lt;code&gt;example.pdf&lt;/code&gt;, the output file will be &lt;code&gt;example_translated.epub&lt;/code&gt; and &lt;code&gt;example_translated.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This tool is released under the MIT License.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer:&lt;/h2&gt; &#xA;&lt;p&gt;This project is intended for use with public domain books and materials only. It is not designed for use with copyrighted content. Users are strongly advised to carefully review copyright information before utilizing this project and to adhere to relevant laws and regulations in order to protect their own rights and the rights of others.&lt;/p&gt; &#xA;&lt;p&gt;The authors and developers of this project shall not be held responsible for any loss or damage resulting from the use of this project. Users assume all risks associated with its use. It is the responsibility of users to ensure they have obtained permission from the original copyright holder or used open-source PDF, EPUB, or MOBI files before employing this project to avoid potential copyright risks.&lt;/p&gt; &#xA;&lt;p&gt;If you have any concerns or suggestions about the use of this project, please contact us through the issues section.&lt;/p&gt;</summary>
  </entry>
</feed>