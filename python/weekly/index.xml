<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-02T01:47:24Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vllm-project/vllm</title>
    <updated>2025-02-02T01:47:24Z</updated>
    <id>tag:github.com,2025-02-02:/vllm-project/vllm</id>
    <link href="https://github.com/vllm-project/vllm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A high-throughput and memory-efficient inference and serving engine for LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png&#34;&gt; &#xA;  &lt;img alt=&#34;vLLM&#34; src=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png&#34; width=&#34;55%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Easy, fast, and cheap LLM serving for everyone &lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://docs.vllm.ai&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://vllm.ai&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2309.06180&#34;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/jz7wjKhh6g&#34;&gt;&lt;b&gt;Discord&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://x.com/vllm_project&#34;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://slack.vllm.ai&#34;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; üî•&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post &lt;a href=&#34;https://blog.vllm.ai/2025/01/27/v1-alpha-release.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2025/01] We hosted &lt;a href=&#34;https://lu.ma/zep56hui&#34;&gt;the eighth vLLM meetup&lt;/a&gt; with Google Cloud! Please find the meetup slides from vLLM team &lt;a href=&#34;https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/12] vLLM joins &lt;a href=&#34;https://pytorch.org/blog/vllm-joins-pytorch&#34;&gt;pytorch ecosystem&lt;/a&gt;! Easy, Fast, and Cheap LLM Serving for Everyone!&lt;/li&gt; &#xA; &lt;li&gt;[2024/11] We hosted &lt;a href=&#34;https://lu.ma/h0qvrajz&#34;&gt;the seventh vLLM meetup&lt;/a&gt; with Snowflake! Please find the meetup slides from vLLM team &lt;a href=&#34;https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing&#34;&gt;here&lt;/a&gt;, and Snowflake team &lt;a href=&#34;https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] We have just created a developer slack (&lt;a href=&#34;https://slack.vllm.ai&#34;&gt;slack.vllm.ai&lt;/a&gt;) focusing on coordinating contributions and discussing features. Please feel free to join us there!&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team &lt;a href=&#34;https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing&#34;&gt;here&lt;/a&gt;. Learn more from the &lt;a href=&#34;https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR&#34;&gt;talks&lt;/a&gt; from other vLLM contributors and users!&lt;/li&gt; &#xA; &lt;li&gt;[2024/09] We hosted &lt;a href=&#34;https://lu.ma/87q3nvnh&#34;&gt;the sixth vLLM meetup&lt;/a&gt; with NVIDIA! Please find the meetup slides &lt;a href=&#34;https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/07] We hosted &lt;a href=&#34;https://lu.ma/lp0gyjqr&#34;&gt;the fifth vLLM meetup&lt;/a&gt; with AWS! Please find the meetup slides &lt;a href=&#34;https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post &lt;a href=&#34;https://blog.vllm.ai/2024/07/23/llama31.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/06] We hosted &lt;a href=&#34;https://lu.ma/agivllm&#34;&gt;the fourth vLLM meetup&lt;/a&gt; with Cloudflare and BentoML! Please find the meetup slides &lt;a href=&#34;https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/04] We hosted &lt;a href=&#34;https://robloxandvllmmeetup2024.splashthat.com/&#34;&gt;the third vLLM meetup&lt;/a&gt; with Roblox! Please find the meetup slides &lt;a href=&#34;https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/01] We hosted &lt;a href=&#34;https://lu.ma/ygxbpzhl&#34;&gt;the second vLLM meetup&lt;/a&gt; with IBM! Please find the meetup slides &lt;a href=&#34;https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/10] We hosted &lt;a href=&#34;https://lu.ma/first-vllm-meetup&#34;&gt;the first vLLM meetup&lt;/a&gt; with a16z! Please find the meetup slides &lt;a href=&#34;https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/08] We would like to express our sincere gratitude to &lt;a href=&#34;https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/&#34;&gt;Andreessen Horowitz&lt;/a&gt; (a16z) for providing a generous grant to support the open-source development and research of vLLM.&lt;/li&gt; &#xA; &lt;li&gt;[2023/06] We officially released vLLM! FastChat-vLLM integration has powered &lt;a href=&#34;https://chat.lmsys.org&#34;&gt;LMSYS Vicuna and Chatbot Arena&lt;/a&gt; since mid-April. Check out our &lt;a href=&#34;https://vllm.ai&#34;&gt;blog post&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;vLLM is a fast and easy-to-use library for LLM inference and serving.&lt;/p&gt; &#xA;&lt;p&gt;Originally developed in the &lt;a href=&#34;https://sky.cs.berkeley.edu&#34;&gt;Sky Computing Lab&lt;/a&gt; at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.&lt;/p&gt; &#xA;&lt;p&gt;vLLM is fast with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;State-of-the-art serving throughput&lt;/li&gt; &#xA; &lt;li&gt;Efficient management of attention key and value memory with &lt;a href=&#34;https://blog.vllm.ai/2023/06/20/vllm.html&#34;&gt;&lt;strong&gt;PagedAttention&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Continuous batching of incoming requests&lt;/li&gt; &#xA; &lt;li&gt;Fast model execution with CUDA/HIP graph&lt;/li&gt; &#xA; &lt;li&gt;Quantizations: &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt;, INT4, INT8, and FP8.&lt;/li&gt; &#xA; &lt;li&gt;Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.&lt;/li&gt; &#xA; &lt;li&gt;Speculative decoding&lt;/li&gt; &#xA; &lt;li&gt;Chunked prefill&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance benchmark&lt;/strong&gt;: We include a performance benchmark at the end of &lt;a href=&#34;https://blog.vllm.ai/2024/09/05/perf-update.html&#34;&gt;our blog post&lt;/a&gt;. It compares the performance of vLLM against other LLM serving engines (&lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt;, &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt; and &lt;a href=&#34;https://github.com/InternLM/lmdeploy&#34;&gt;LMDeploy&lt;/a&gt;). The implementation is under &lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/.buildkite/nightly-benchmarks/&#34;&gt;nightly-benchmarks folder&lt;/a&gt; and you can &lt;a href=&#34;https://github.com/vllm-project/vllm/issues/8176&#34;&gt;reproduce&lt;/a&gt; this benchmark using our one-click runnable script.&lt;/p&gt; &#xA;&lt;p&gt;vLLM is flexible and easy to use with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seamless integration with popular Hugging Face models&lt;/li&gt; &#xA; &lt;li&gt;High-throughput serving with various decoding algorithms, including &lt;em&gt;parallel sampling&lt;/em&gt;, &lt;em&gt;beam search&lt;/em&gt;, and more&lt;/li&gt; &#xA; &lt;li&gt;Tensor parallelism and pipeline parallelism support for distributed inference&lt;/li&gt; &#xA; &lt;li&gt;Streaming outputs&lt;/li&gt; &#xA; &lt;li&gt;OpenAI-compatible API server&lt;/li&gt; &#xA; &lt;li&gt;Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.&lt;/li&gt; &#xA; &lt;li&gt;Prefix caching support&lt;/li&gt; &#xA; &lt;li&gt;Multi-lora support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;vLLM seamlessly supports most popular open-source models on HuggingFace, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Transformer-like LLMs (e.g., Llama)&lt;/li&gt; &#xA; &lt;li&gt;Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)&lt;/li&gt; &#xA; &lt;li&gt;Embedding Models (e.g. E5-Mistral)&lt;/li&gt; &#xA; &lt;li&gt;Multi-modal LLMs (e.g., LLaVA)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Find the full list of supported models &lt;a href=&#34;https://docs.vllm.ai/en/latest/models/supported_models.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Install vLLM with &lt;code&gt;pip&lt;/code&gt; or &lt;a href=&#34;https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source&#34;&gt;from source&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install vllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visit our &lt;a href=&#34;https://docs.vllm.ai/en/latest/&#34;&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.vllm.ai/en/latest/getting_started/installation/index.html&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.vllm.ai/en/latest/getting_started/quickstart.html&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.vllm.ai/en/latest/models/supported_models.html&#34;&gt;List of Supported Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for how to get involved.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!&lt;/p&gt; &#xA;&lt;!-- Note: Please sort them in alphabetical order. --&gt; &#xA;&lt;!-- Note: Please keep these consistent with docs/source/community/sponsors.md --&gt; &#xA;&lt;p&gt;Cash Donations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a16z&lt;/li&gt; &#xA; &lt;li&gt;Dropbox&lt;/li&gt; &#xA; &lt;li&gt;Sequoia Capital&lt;/li&gt; &#xA; &lt;li&gt;Skywork AI&lt;/li&gt; &#xA; &lt;li&gt;ZhenFund&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Compute Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AMD&lt;/li&gt; &#xA; &lt;li&gt;Anyscale&lt;/li&gt; &#xA; &lt;li&gt;AWS&lt;/li&gt; &#xA; &lt;li&gt;Crusoe Cloud&lt;/li&gt; &#xA; &lt;li&gt;Databricks&lt;/li&gt; &#xA; &lt;li&gt;DeepInfra&lt;/li&gt; &#xA; &lt;li&gt;Google Cloud&lt;/li&gt; &#xA; &lt;li&gt;Lambda Lab&lt;/li&gt; &#xA; &lt;li&gt;Nebius&lt;/li&gt; &#xA; &lt;li&gt;Novita AI&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA&lt;/li&gt; &#xA; &lt;li&gt;Replicate&lt;/li&gt; &#xA; &lt;li&gt;Roblox&lt;/li&gt; &#xA; &lt;li&gt;RunPod&lt;/li&gt; &#xA; &lt;li&gt;Trainy&lt;/li&gt; &#xA; &lt;li&gt;UC Berkeley&lt;/li&gt; &#xA; &lt;li&gt;UC San Diego&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Slack Sponsor: Anyscale&lt;/p&gt; &#xA;&lt;p&gt;We also have an official fundraising venue through &lt;a href=&#34;https://opencollective.com/vllm&#34;&gt;OpenCollective&lt;/a&gt;. We plan to use the fund to support the development, maintenance, and adoption of vLLM.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use vLLM for your research, please cite our &lt;a href=&#34;https://arxiv.org/abs/2309.06180&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{kwon2023efficient,&#xA;  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},&#xA;  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},&#xA;  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For technical questions and feature requests, please use Github issues or discussions.&lt;/li&gt; &#xA; &lt;li&gt;For discussing with fellow users, please use Discord.&lt;/li&gt; &#xA; &lt;li&gt;For coordinating contributions and development, please use Slack.&lt;/li&gt; &#xA; &lt;li&gt;For security disclosures, please use Github&#39;s security advisory feature.&lt;/li&gt; &#xA; &lt;li&gt;For collaborations and partnerships, please contact us at vllm-questions AT lists.berkeley.edu.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Media Kit&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you wish to use vLLM&#39;s logo, please refer to &lt;a href=&#34;https://github.com/vllm-project/media-kit&#34;&gt;our media kit repo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>unslothai/unsloth</title>
    <updated>2025-02-02T01:47:24Z</updated>
    <id>tag:github.com,2025-02-02:/unslothai/unsloth</id>
    <link href="https://github.com/unslothai/unsloth" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 &amp; Gemma LLMs 2-5x faster with 70% less memory&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://unsloth.ai&#34;&gt;&#xA;   &lt;picture&gt; &#xA;    &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&#34;&gt; &#xA;    &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&#34;&gt; &#xA;    &lt;img alt=&#34;unsloth logo&#34; src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&#34; height=&#34;110&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA;   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start%20free%20finetune%20button.png&#34; height=&#34;48&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/unsloth&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png&#34; height=&#34;48&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.unsloth.ai&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&#34; height=&#34;48&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Finetune Llama 3.3, Mistral, Phi-4, Qwen 2.5 &amp;amp; Gemma 2-5x faster with 80% less memory!&lt;/h3&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://i.ibb.co/sJ7RhGG/image-41.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;‚ú® Finetune for Free&lt;/h2&gt; &#xA;&lt;p&gt;All notebooks are &lt;strong&gt;beginner friendly&lt;/strong&gt;! Add your dataset, click &#34;Run All&#34;, and you&#39;ll get a 2x faster finetuned model which can be exported to GGUF, Ollama, vLLM or uploaded to Hugging Face.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Unsloth supports&lt;/th&gt; &#xA;   &lt;th&gt;Free Notebooks&lt;/th&gt; &#xA;   &lt;th&gt;Performance&lt;/th&gt; &#xA;   &lt;th&gt;Memory use&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 (3B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2x faster&lt;/td&gt; &#xA;   &lt;td&gt;70% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2x faster&lt;/td&gt; &#xA;   &lt;td&gt;70% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.2 Vision (11B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2x faster&lt;/td&gt; &#xA;   &lt;td&gt;50% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2x faster&lt;/td&gt; &#xA;   &lt;td&gt;70% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemma 2 (9B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2x faster&lt;/td&gt; &#xA;   &lt;td&gt;70% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen 2.5 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2x faster&lt;/td&gt; &#xA;   &lt;td&gt;70% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.2x faster&lt;/td&gt; &#xA;   &lt;td&gt;75% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.9x faster&lt;/td&gt; &#xA;   &lt;td&gt;60% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ORPO&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.9x faster&lt;/td&gt; &#xA;   &lt;td&gt;50% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DPO Zephyr&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb&#34;&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.9x faster&lt;/td&gt; &#xA;   &lt;td&gt;50% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://docs.unsloth.ai/get-started/unsloth-notebooks&#34;&gt;all our notebooks&lt;/a&gt; and &lt;a href=&#34;https://docs.unsloth.ai/get-started/all-our-models&#34;&gt;all our models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kaggle Notebooks&lt;/strong&gt; for &lt;a href=&#34;https://www.kaggle.com/danielhanchen/kaggle-llama-3-2-1b-3b-unsloth-notebook&#34;&gt;Llama 3.2 Kaggle notebook&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook&#34;&gt;Llama 3.1 (8B)&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/&#34;&gt;Gemma 2 (9B)&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook&#34;&gt;Mistral (7B)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run notebooks for &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb&#34;&gt;Llama 3.2 conversational&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/drive/15OyFkGoCImV9dSsewU1wa2JuKB4-mDE_?usp=sharing&#34;&gt;Llama 3.1 conversational&lt;/a&gt; and &lt;a href=&#34;https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing&#34;&gt;Mistral v0.3 ChatML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;This &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb&#34;&gt;text completion notebook&lt;/a&gt; is for continued pretraining / raw text&lt;/li&gt; &#xA; &lt;li&gt;This &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-CPT.ipynb&#34;&gt;continued pretraining notebook&lt;/a&gt; is for learning another language&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;a href=&#34;https://docs.unsloth.ai/&#34;&gt;here&lt;/a&gt; for detailed documentation for Unsloth.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü¶• Unsloth.ai News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üì£ NEW! &lt;a href=&#34;https://unsloth.ai/blog/deepseek-r1&#34;&gt;DeepSeek-R1&lt;/a&gt; - the most powerful open reasoning models with Llama &amp;amp; Qwen distillations. Run or fine-tune them now! More details: &lt;a href=&#34;https://unsloth.ai/blog/deepseek-r1&#34;&gt;unsloth.ai/blog/deepseek-r1&lt;/a&gt;. All model uploads: &lt;a href=&#34;https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üì£ NEW! &lt;a href=&#34;https://unsloth.ai/blog/phi4&#34;&gt;Phi-4&lt;/a&gt; by Microsoft is now supported. We also &lt;a href=&#34;https://unsloth.ai/blog/phi4&#34;&gt;fixed bugs&lt;/a&gt; in Phi-4 and &lt;a href=&#34;https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa&#34;&gt;uploaded GGUFs, 4-bit&lt;/a&gt;. Try the &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&#34;&gt;Phi-4 Colab notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ NEW! &lt;a href=&#34;https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f&#34;&gt;Llama 3.3 (70B)&lt;/a&gt;, Meta&#39;s latest model is supported.&lt;/li&gt; &#xA; &lt;li&gt;üì£ NEW! We worked with Apple to add &lt;a href=&#34;https://arxiv.org/abs/2411.09009&#34;&gt;Cut Cross Entropy&lt;/a&gt;. Unsloth now supports 89K context for Meta&#39;s Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.&lt;/li&gt; &#xA; &lt;li&gt;üì£ Introducing Unsloth &lt;a href=&#34;https://unsloth.ai/blog/dynamic-4bit&#34;&gt;Dynamic 4-bit Quantization&lt;/a&gt;! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &amp;lt;10% more VRAM than BnB 4-bit. See our collection on &lt;a href=&#34;https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7&#34;&gt;Hugging Face here.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ &lt;a href=&#34;https://unsloth.ai/blog/vision&#34;&gt;Vision models&lt;/a&gt; now supported! &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb&#34;&gt;Llama 3.2 Vision (11B)&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb&#34;&gt;Qwen 2.5 VL (7B)&lt;/a&gt; and &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb&#34;&gt;Pixtral (12B) 2409&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click for more news&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;üì£ We found and helped fix a &lt;a href=&#34;https://unsloth.ai/blog/gradient&#34;&gt;gradient accumulation bug&lt;/a&gt;! Please update Unsloth and transformers.&lt;/li&gt; &#xA;  &lt;li&gt;üì£ Try out &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb&#34;&gt;Chat interface&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;üì£ NEW! Qwen-2.5 including &lt;a href=&#34;https://unsloth.ai/blog/qwen-coder&#34;&gt;Coder&lt;/a&gt; models are now supported with bugfixes. 14b fits in a Colab GPU! &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(14B)-Conversational.ipynb&#34;&gt;Qwen 2.5 conversational notebook&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üì£ NEW! &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb&#34;&gt;Mistral Small 22b notebook&lt;/a&gt; finetuning fits in under 16GB of VRAM!&lt;/li&gt; &#xA;  &lt;li&gt;üì£ NEW! &lt;code&gt;pip install unsloth&lt;/code&gt; now works! Head over to &lt;a href=&#34;https://pypi.org/project/unsloth/&#34;&gt;pypi&lt;/a&gt; to check it out! This allows non git pull installs. Use &lt;code&gt;pip install unsloth[colab-new]&lt;/code&gt; for non dependency installs.&lt;/li&gt; &#xA;  &lt;li&gt;üì£ NEW! Continued Pretraining &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-CPT.ipynb&#34;&gt;notebook&lt;/a&gt; for other languages like Korean!&lt;/li&gt; &#xA;  &lt;li&gt;üì£ &lt;a href=&#34;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Inference.ipynb&#34;&gt;2x faster inference&lt;/a&gt; added for all our models&lt;/li&gt; &#xA;  &lt;li&gt;üì£ We cut memory usage by a &lt;a href=&#34;https://unsloth.ai/blog/long-context&#34;&gt;further 30%&lt;/a&gt; and now support &lt;a href=&#34;https://unsloth.ai/blog/long-context&#34;&gt;4x longer context windows&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üîó Links and Resources&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üìö &lt;strong&gt;Documentation &amp;amp; Wiki&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.unsloth.ai&#34;&gt;Read Our Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img height=&#34;14&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg?sanitize=true&#34;&gt;&amp;nbsp; &lt;strong&gt;Twitter (aka X)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/unslothai&#34;&gt;Follow us on X&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üíæ &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth/tree/main#-installation-instructions&#34;&gt;unsloth/README.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü•á &lt;strong&gt;Benchmarking&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking&#34;&gt;Performance Tables&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üåê &lt;strong&gt;Released Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.unsloth.ai/get-started/all-our-models&#34;&gt;Unsloth Releases&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;‚úçÔ∏è &lt;strong&gt;Blog&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://unsloth.ai/blog&#34;&gt;Read our Blogs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img height=&#34;14&#34; src=&#34;https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png&#34;&gt;&amp;nbsp; &lt;strong&gt;Reddit&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://reddit.com/r/unsloth&#34;&gt;Join our Reddit page&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;‚≠ê Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All kernels written in &lt;a href=&#34;https://openai.com/research/triton&#34;&gt;OpenAI&#39;s Triton&lt;/a&gt; language. &lt;strong&gt;Manual backprop engine&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;0% loss in accuracy&lt;/strong&gt; - no approximation methods - all exact.&lt;/li&gt; &#xA; &lt;li&gt;No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) &lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;Check your GPU!&lt;/a&gt; GTX 1070, 1080 works, but is slow.&lt;/li&gt; &#xA; &lt;li&gt;Works on &lt;strong&gt;Linux&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt; via WSL.&lt;/li&gt; &#xA; &lt;li&gt;Supports 4bit and 16bit QLoRA / LoRA finetuning via &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open source trains 5x faster - see &lt;a href=&#34;https://unsloth.ai/&#34;&gt;Unsloth Pro&lt;/a&gt; for up to &lt;strong&gt;30x faster training&lt;/strong&gt;!&lt;/li&gt; &#xA; &lt;li&gt;If you trained a model with ü¶•Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/made%20with%20unsloth.png&#34; height=&#34;50&#34; align=&#34;center&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü•á Performance Benchmarking&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For our most detailed benchmarks, read our &lt;a href=&#34;https://unsloth.ai/blog/llama3-3&#34;&gt;Llama 3.3 Blog&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Benchmarking of Unsloth was also conducted by &lt;a href=&#34;https://huggingface.co/blog/unsloth-trl&#34;&gt;ü§óHugging Face&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;VRAM&lt;/th&gt; &#xA;   &lt;th&gt;ü¶• Unsloth speed&lt;/th&gt; &#xA;   &lt;th&gt;ü¶• VRAM reduction&lt;/th&gt; &#xA;   &lt;th&gt;ü¶• Longer context&lt;/th&gt; &#xA;   &lt;th&gt;üòä Hugging Face + FA2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.3 (70B)&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;2x&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;75%&lt;/td&gt; &#xA;   &lt;td&gt;13x longer&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;2x&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;70%&lt;/td&gt; &#xA;   &lt;td&gt;12x longer&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/sJ7RhGG/image-41.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üíæ Installation Instructions&lt;/h2&gt; &#xA;&lt;p&gt;For stable releases, use &lt;code&gt;pip install unsloth&lt;/code&gt;. We recommend &lt;code&gt;pip install &#34;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&#34;&lt;/code&gt; for most installations though.&lt;/p&gt; &#xA;&lt;h3&gt;Conda Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;‚ö†Ô∏èOnly use Conda if you have it. If not, use Pip&lt;/code&gt;. Select either &lt;code&gt;pytorch-cuda=11.8,12.1&lt;/code&gt; for CUDA 11.8 or CUDA 12.1. We support &lt;code&gt;python=3.10,3.11,3.12&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name unsloth_env \&#xA;    python=3.11 \&#xA;    pytorch-cuda=12.1 \&#xA;    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \&#xA;    -y&#xA;conda activate unsloth_env&#xA;&#xA;pip install &#34;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install --no-deps trl peft accelerate bitsandbytes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;If you&#39;re looking to install Conda in a Linux environment, &lt;a href=&#34;https://docs.anaconda.com/miniconda/&#34;&gt;read here&lt;/a&gt;, or run the below üîΩ&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/miniconda3&#xA;wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh&#xA;bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3&#xA;rm -rf ~/miniconda3/miniconda.sh&#xA;~/miniconda3/bin/conda init bash&#xA;~/miniconda3/bin/conda init zsh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Pip Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;‚ö†Ô∏èDo **NOT** use this if you have Conda.&lt;/code&gt; Pip is a bit more complex since there are dependency issues. The pip command is different for &lt;code&gt;torch 2.2,2.3,2.4,2.5&lt;/code&gt; and CUDA versions.&lt;/p&gt; &#xA;&lt;p&gt;For other torch versions, we support &lt;code&gt;torch211&lt;/code&gt;, &lt;code&gt;torch212&lt;/code&gt;, &lt;code&gt;torch220&lt;/code&gt;, &lt;code&gt;torch230&lt;/code&gt;, &lt;code&gt;torch240&lt;/code&gt; and for CUDA versions, we support &lt;code&gt;cu118&lt;/code&gt; and &lt;code&gt;cu121&lt;/code&gt; and &lt;code&gt;cu124&lt;/code&gt;. For Ampere devices (A100, H100, RTX3090) and above, use &lt;code&gt;cu118-ampere&lt;/code&gt; or &lt;code&gt;cu121-ampere&lt;/code&gt; or &lt;code&gt;cu124-ampere&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you have &lt;code&gt;torch 2.4&lt;/code&gt; and &lt;code&gt;CUDA 12.1&lt;/code&gt;, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip&#xA;pip install &#34;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another example, if you have &lt;code&gt;torch 2.5&lt;/code&gt; and &lt;code&gt;CUDA 12.4&lt;/code&gt;, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip&#xA;pip install &#34;unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And other examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&#xA;pip install &#34;unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&#xA;pip install &#34;unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, run the below in a terminal to get the &lt;strong&gt;optimal&lt;/strong&gt; pip installation command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, run the below manually in a Python REPL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try: import torch&#xA;except: raise ImportError(&#39;Install torch via `pip install torch`&#39;)&#xA;from packaging.version import Version as V&#xA;v = V(torch.__version__)&#xA;cuda = str(torch.version.cuda)&#xA;is_ampere = torch.cuda.get_device_capability()[0] &amp;gt;= 8&#xA;if cuda != &#34;12.1&#34; and cuda != &#34;11.8&#34; and cuda != &#34;12.4&#34;: raise RuntimeError(f&#34;CUDA = {cuda} not supported!&#34;)&#xA;if   v &amp;lt;= V(&#39;2.1.0&#39;): raise RuntimeError(f&#34;Torch = {v} too old!&#34;)&#xA;elif v &amp;lt;= V(&#39;2.1.1&#39;): x = &#39;cu{}{}-torch211&#39;&#xA;elif v &amp;lt;= V(&#39;2.1.2&#39;): x = &#39;cu{}{}-torch212&#39;&#xA;elif v  &amp;lt; V(&#39;2.3.0&#39;): x = &#39;cu{}{}-torch220&#39;&#xA;elif v  &amp;lt; V(&#39;2.4.0&#39;): x = &#39;cu{}{}-torch230&#39;&#xA;elif v  &amp;lt; V(&#39;2.5.0&#39;): x = &#39;cu{}{}-torch240&#39;&#xA;elif v  &amp;lt; V(&#39;2.6.0&#39;): x = &#39;cu{}{}-torch250&#39;&#xA;else: raise RuntimeError(f&#34;Torch = {v} too new!&#34;)&#xA;x = x.format(cuda.replace(&#34;.&#34;, &#34;&#34;), &#34;-ampere&#34; if is_ampere else &#34;&#34;)&#xA;print(f&#39;pip install --upgrade pip &amp;amp;&amp;amp; pip install &#34;unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git&#34;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows Installation&lt;/h3&gt; &#xA;&lt;p&gt;To run Unsloth directly on Windows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Triton from this Windows fork and follow the instructions: &lt;a href=&#34;https://github.com/woct0rdho/triton-windows&#34;&gt;https://github.com/woct0rdho/triton-windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;In the SFTTrainer, set &lt;code&gt;dataset_num_proc=1&lt;/code&gt; to avoid a crashing issue:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trainer = SFTTrainer(&#xA;    dataset_num_proc=1,&#xA;    ...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;strong&gt;advanced installation instructions&lt;/strong&gt; or if you see weird errors during installations:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;triton&lt;/code&gt;. Go to &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt; to install it. For example &lt;code&gt;pip install torch torchvision torchaudio triton&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Confirm if CUDA is installated correctly. Try &lt;code&gt;nvcc&lt;/code&gt;. If that fails, you need to install &lt;code&gt;cudatoolkit&lt;/code&gt; or CUDA drivers.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;code&gt;xformers&lt;/code&gt; manually. You can try installing &lt;code&gt;vllm&lt;/code&gt; and seeing if &lt;code&gt;vllm&lt;/code&gt; succeeds. Check if &lt;code&gt;xformers&lt;/code&gt; succeeded with &lt;code&gt;python -m xformers.info&lt;/code&gt; Go to &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;https://github.com/facebookresearch/xformers&lt;/a&gt;. Another option is to install &lt;code&gt;flash-attn&lt;/code&gt; for Ampere GPUs.&lt;/li&gt; &#xA; &lt;li&gt;Finally, install &lt;code&gt;bitsandbytes&lt;/code&gt; and check it with &lt;code&gt;python -m bitsandbytes&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üìú &lt;a href=&#34;https://docs.unsloth.ai&#34;&gt;Documentation&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to our official &lt;a href=&#34;https://docs.unsloth.ai&#34;&gt;Documentation&lt;/a&gt; for saving to GGUF, checkpointing, evaluation and more!&lt;/li&gt; &#xA; &lt;li&gt;We support Huggingface&#39;s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!&lt;/li&gt; &#xA; &lt;li&gt;We&#39;re in ü§óHugging Face&#39;s official docs! Check out the &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth&#34;&gt;SFT docs&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth&#34;&gt;DPO docs&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;If you want to download models from the ModelScope community, please use an environment variable: &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt;, and install the modelscope library by: &lt;code&gt;pip install modelscope -U&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;unsloth_cli.py also supports &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt; to download models and datasets. please remember to use the model and dataset id in the ModelScope community.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from unsloth import FastLanguageModel &#xA;from unsloth import is_bfloat16_supported&#xA;import torch&#xA;from trl import SFTTrainer&#xA;from transformers import TrainingArguments&#xA;from datasets import load_dataset&#xA;max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!&#xA;# Get LAION dataset&#xA;url = &#34;https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl&#34;&#xA;dataset = load_dataset(&#34;json&#34;, data_files = {&#34;train&#34; : url}, split = &#34;train&#34;)&#xA;&#xA;# 4bit pre quantized models we support for 4x faster downloading + no OOMs.&#xA;fourbit_models = [&#xA;    &#34;unsloth/mistral-7b-v0.3-bnb-4bit&#34;,      # New Mistral v3 2x faster!&#xA;    &#34;unsloth/mistral-7b-instruct-v0.3-bnb-4bit&#34;,&#xA;    &#34;unsloth/llama-3-8b-bnb-4bit&#34;,           # Llama-3 15 trillion tokens model 2x faster!&#xA;    &#34;unsloth/llama-3-8b-Instruct-bnb-4bit&#34;,&#xA;    &#34;unsloth/llama-3-70b-bnb-4bit&#34;,&#xA;    &#34;unsloth/Phi-3-mini-4k-instruct&#34;,        # Phi-3 2x faster!&#xA;    &#34;unsloth/Phi-3-medium-4k-instruct&#34;,&#xA;    &#34;unsloth/mistral-7b-bnb-4bit&#34;,&#xA;    &#34;unsloth/gemma-7b-bnb-4bit&#34;,             # Gemma 2.2x faster!&#xA;] # More models at https://huggingface.co/unsloth&#xA;&#xA;model, tokenizer = FastLanguageModel.from_pretrained(&#xA;    model_name = &#34;unsloth/llama-3-8b-bnb-4bit&#34;,&#xA;    max_seq_length = max_seq_length,&#xA;    dtype = None,&#xA;    load_in_4bit = True,&#xA;)&#xA;&#xA;# Do model patching and add fast LoRA weights&#xA;model = FastLanguageModel.get_peft_model(&#xA;    model,&#xA;    r = 16,&#xA;    target_modules = [&#34;q_proj&#34;, &#34;k_proj&#34;, &#34;v_proj&#34;, &#34;o_proj&#34;,&#xA;                      &#34;gate_proj&#34;, &#34;up_proj&#34;, &#34;down_proj&#34;,],&#xA;    lora_alpha = 16,&#xA;    lora_dropout = 0, # Supports any, but = 0 is optimized&#xA;    bias = &#34;none&#34;,    # Supports any, but = &#34;none&#34; is optimized&#xA;    # [NEW] &#34;unsloth&#34; uses 30% less VRAM, fits 2x larger batch sizes!&#xA;    use_gradient_checkpointing = &#34;unsloth&#34;, # True or &#34;unsloth&#34; for very long context&#xA;    random_state = 3407,&#xA;    max_seq_length = max_seq_length,&#xA;    use_rslora = False,  # We support rank stabilized LoRA&#xA;    loftq_config = None, # And LoftQ&#xA;)&#xA;&#xA;trainer = SFTTrainer(&#xA;    model = model,&#xA;    train_dataset = dataset,&#xA;    dataset_text_field = &#34;text&#34;,&#xA;    max_seq_length = max_seq_length,&#xA;    tokenizer = tokenizer,&#xA;    args = TrainingArguments(&#xA;        per_device_train_batch_size = 2,&#xA;        gradient_accumulation_steps = 4,&#xA;        warmup_steps = 10,&#xA;        max_steps = 60,&#xA;        fp16 = not is_bfloat16_supported(),&#xA;        bf16 = is_bfloat16_supported(),&#xA;        logging_steps = 1,&#xA;        output_dir = &#34;outputs&#34;,&#xA;        optim = &#34;adamw_8bit&#34;,&#xA;        seed = 3407,&#xA;    ),&#xA;)&#xA;trainer.train()&#xA;&#xA;# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like&#xA;# (1) Saving to GGUF / merging to 16bit for vLLM&#xA;# (2) Continued training from a saved LoRA adapter&#xA;# (3) Adding an evaluation loop / OOMs&#xA;# (4) Customized chat templates&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;DPO&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;DPO Support&lt;/h2&gt; &#xA;&lt;p&gt;DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;&gt;Llama-Factory&lt;/a&gt;. We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: &lt;a href=&#34;https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing&#34;&gt;notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re in ü§óHugging Face&#39;s official docs! We&#39;re on the &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth&#34;&gt;SFT docs&lt;/a&gt; and the &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth&#34;&gt;DPO docs&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#34;0&#34; # Optional set GPU device ID&#xA;&#xA;from unsloth import FastLanguageModel, PatchDPOTrainer&#xA;from unsloth import is_bfloat16_supported&#xA;PatchDPOTrainer()&#xA;import torch&#xA;from transformers import TrainingArguments&#xA;from trl import DPOTrainer&#xA;&#xA;model, tokenizer = FastLanguageModel.from_pretrained(&#xA;    model_name = &#34;unsloth/zephyr-sft-bnb-4bit&#34;,&#xA;    max_seq_length = max_seq_length,&#xA;    dtype = None,&#xA;    load_in_4bit = True,&#xA;)&#xA;&#xA;# Do model patching and add fast LoRA weights&#xA;model = FastLanguageModel.get_peft_model(&#xA;    model,&#xA;    r = 64,&#xA;    target_modules = [&#34;q_proj&#34;, &#34;k_proj&#34;, &#34;v_proj&#34;, &#34;o_proj&#34;,&#xA;                      &#34;gate_proj&#34;, &#34;up_proj&#34;, &#34;down_proj&#34;,],&#xA;    lora_alpha = 64,&#xA;    lora_dropout = 0, # Supports any, but = 0 is optimized&#xA;    bias = &#34;none&#34;,    # Supports any, but = &#34;none&#34; is optimized&#xA;    # [NEW] &#34;unsloth&#34; uses 30% less VRAM, fits 2x larger batch sizes!&#xA;    use_gradient_checkpointing = &#34;unsloth&#34;, # True or &#34;unsloth&#34; for very long context&#xA;    random_state = 3407,&#xA;    max_seq_length = max_seq_length,&#xA;)&#xA;&#xA;dpo_trainer = DPOTrainer(&#xA;    model = model,&#xA;    ref_model = None,&#xA;    args = TrainingArguments(&#xA;        per_device_train_batch_size = 4,&#xA;        gradient_accumulation_steps = 8,&#xA;        warmup_ratio = 0.1,&#xA;        num_train_epochs = 3,&#xA;        fp16 = not is_bfloat16_supported(),&#xA;        bf16 = is_bfloat16_supported(),&#xA;        logging_steps = 1,&#xA;        optim = &#34;adamw_8bit&#34;,&#xA;        seed = 42,&#xA;        output_dir = &#34;outputs&#34;,&#xA;    ),&#xA;    beta = 0.1,&#xA;    train_dataset = YOUR_DATASET_HERE,&#xA;    # eval_dataset = YOUR_DATASET_HERE,&#xA;    tokenizer = tokenizer,&#xA;    max_length = 1024,&#xA;    max_prompt_length = 512,&#xA;)&#xA;dpo_trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü•á Detailed Benchmarking Tables&lt;/h2&gt; &#xA;&lt;h3&gt;Context length benchmarks&lt;/h3&gt; &#xA;&lt;h4&gt;Llama 3.1 (8B) max. context length&lt;/h4&gt; &#xA;&lt;p&gt;We tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;GPU VRAM&lt;/th&gt; &#xA;   &lt;th&gt;ü¶•Unsloth context length&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8 GB&lt;/td&gt; &#xA;   &lt;td&gt;2,972&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12 GB&lt;/td&gt; &#xA;   &lt;td&gt;21,848&lt;/td&gt; &#xA;   &lt;td&gt;932&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16 GB&lt;/td&gt; &#xA;   &lt;td&gt;40,724&lt;/td&gt; &#xA;   &lt;td&gt;2,551&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;24 GB&lt;/td&gt; &#xA;   &lt;td&gt;78,475&lt;/td&gt; &#xA;   &lt;td&gt;5,789&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;40 GB&lt;/td&gt; &#xA;   &lt;td&gt;153,977&lt;/td&gt; &#xA;   &lt;td&gt;12,264&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;48 GB&lt;/td&gt; &#xA;   &lt;td&gt;191,728&lt;/td&gt; &#xA;   &lt;td&gt;15,502&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;80 GB&lt;/td&gt; &#xA;   &lt;td&gt;342,733&lt;/td&gt; &#xA;   &lt;td&gt;28,454&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Llama 3.3 (70B) max. context length&lt;/h4&gt; &#xA;&lt;p&gt;We tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;GPU VRAM&lt;/th&gt; &#xA;   &lt;th&gt;ü¶•Unsloth context length&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;48 GB&lt;/td&gt; &#xA;   &lt;td&gt;12,106&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;80 GB&lt;/td&gt; &#xA;   &lt;td&gt;89,389&lt;/td&gt; &#xA;   &lt;td&gt;6,916&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/sJ7RhGG/image-41.png&#34; alt=&#34;&#34;&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;You can cite the Unsloth repo as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{unsloth,&#xA;  author = {Daniel Han, Michael Han and Unsloth team},&#xA;  title = {Unsloth},&#xA;  url = {http://github.com/unslothai/unsloth},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Thank You to&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/erikwijmans&#34;&gt;Erik&lt;/a&gt; for his help adding &lt;a href=&#34;https://github.com/apple/ml-cross-entropy&#34;&gt;Apple&#39;s ML Cross Entropy&lt;/a&gt; in Unsloth&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HuyNguyen-hust&#34;&gt;HuyNguyen-hust&lt;/a&gt; for making &lt;a href=&#34;https://github.com/unslothai/unsloth/pull/238&#34;&gt;RoPE Embeddings 28% faster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RandomInternetPreson&#34;&gt;RandomInternetPreson&lt;/a&gt; for confirming WSL support&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/152334H&#34;&gt;152334H&lt;/a&gt; for experimental DPO support&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/atgctg&#34;&gt;atgctg&lt;/a&gt; for syntax highlighting&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>deepseek-ai/Janus</title>
    <updated>2025-02-02T01:47:24Z</updated>
    <id>tag:github.com,2025-02-02:/deepseek-ai/Janus</id>
    <link href="https://github.com/deepseek-ai/Janus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Janus-Series: Unified Multimodal Understanding and Generation Models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/logo.svg?sanitize=true&#34; width=&#34;60%&#34; alt=&#34;DeepSeek LLM&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;üöÄ Janus-Series: Unified Multimodal Understanding and Generation Models&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.deepseek.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Homepage&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt;  &#xA; &lt;a href=&#34;https://huggingface.co/deepseek-ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Hugging Face&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;a href=&#34;https://discord.gg/Tc7c45Zzu5&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &#xA; &lt;!-- &lt;a href=&#34;images/qr.jpeg&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img alt=&#34;Wechat&#34; src=&#34;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &#xA; &lt;!-- &lt;a href=&#34;https://twitter.com/deepseek_ai&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/LICENSE-CODE&#34;&gt; &lt;img alt=&#34;Code License&#34; src=&#34;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/LICENSE-MODEL&#34;&gt; &lt;img alt=&#34;Model License&#34; src=&#34;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#2-model-download&#34;&gt;&lt;b&gt;üì• Model Download&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#3-quick-start&#34;&gt;&lt;b&gt;‚ö° Quick Start&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#4-license&#34;&gt;&lt;b&gt;üìú License&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#5-citation&#34;&gt;&lt;b&gt;üìñ Citation&lt;/b&gt;&lt;/a&gt; &lt;br&gt; &#xA; &lt;!-- üìÑ Paper Link (&lt;a href=&#34;https://arxiv.org/abs/2410.13848&#34;&gt;&lt;b&gt;Janus&lt;/b&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2410.13848&#34;&gt;&lt;b&gt;JanusFlow&lt;/b&gt;&lt;/a&gt;) | --&gt; ü§ó Online Demo (&lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B&#34;&gt;&lt;b&gt;Janus-Pro-7B&lt;/b&gt;&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-1.3B&#34;&gt;&lt;b&gt;Janus&lt;/b&gt;&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B&#34;&gt;&lt;b&gt;JanusFlow&lt;/b&gt;&lt;/a&gt;) &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2025.01.27&lt;/strong&gt;: Janus-Pro is released, an advanced version of Janus, improving both multimodal understanding and visual generation significantly. See &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/janus_pro_tech_report.pdf&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.11.13&lt;/strong&gt;: JanusFlow is released, a new unified model with rectified flow for image generation. See &lt;a href=&#34;https://arxiv.org/abs/2411.07975&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B&#34;&gt;demo&lt;/a&gt; and &lt;a href=&#34;https://github.com/deepseek-ai/Janus?tab=readme-ov-file#janusflow&#34;&gt;usage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.10.23&lt;/strong&gt;: Evaluation code for reproducing the multimodal understanding results from the paper has been added to VLMEvalKit. Please refer to &lt;a href=&#34;https://github.com/open-compass/VLMEvalKit/pull/541&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.10.20&lt;/strong&gt;: (1) Fix a bug in &lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-1.3B/blob/main/tokenizer_config.json&#34;&gt;tokenizer_config.json&lt;/a&gt;. The previous version caused classifier-free guidance to not function properly, resulting in relatively poor visual generation quality. (2) Release Gradio demo (&lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-1.3B&#34;&gt;online demo&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#gradio-demo&#34;&gt;local&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/janus_pro_tech_report.pdf&#34;&gt;&lt;b&gt;Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Janus-Pro&lt;/strong&gt; is an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/teaser_januspro.png&#34; style=&#34;width:90%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.13848&#34;&gt;&lt;b&gt;Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Janus&lt;/strong&gt; is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/teaser.png&#34; style=&#34;width:90%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2411.07975&#34;&gt;&lt;b&gt;JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JanusFlow&lt;/strong&gt; introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/teaser_janusflow.png&#34; style=&#34;width:90%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;2. Model Download&lt;/h2&gt; &#xA;&lt;p&gt;We release Janus to the public to support a broader and more diverse range of research within both academic and commercial communities. Please note that the use of this model is subject to the terms outlined in &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#5-license&#34;&gt;License section&lt;/a&gt;. Commercial usage is permitted under these terms.&lt;/p&gt; &#xA;&lt;h3&gt;Huggingface&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Sequence Length&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-1.3B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-1.3B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JanusFlow-1.3B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/JanusFlow-1.3B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-Pro-1B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-Pro-1B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-Pro-7B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-Pro-7B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;3. Quick Start&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Janus-Pro&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA; &lt;h4&gt;Multimodal Understanding&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import torch&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;from janus.utils.io import load_pil_images&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-Pro-7B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;&amp;lt;|User|&amp;gt;&#34;,&#xA;        &#34;content&#34;: f&#34;&amp;lt;image_placeholder&amp;gt;\n{question}&#34;,&#xA;        &#34;images&#34;: [image],&#xA;    },&#xA;    {&#34;role&#34;: &#34;&amp;lt;|Assistant|&amp;gt;&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation, images=pil_images, force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# # run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# # run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True,&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Text-to-Image Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import PIL.Image&#xA;import torch&#xA;import numpy as np&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-Pro-7B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;&amp;lt;|User|&amp;gt;&#34;,&#xA;        &#34;content&#34;: &#34;A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair&#34;,&#xA;    },&#xA;    {&#34;role&#34;: &#34;&amp;lt;|Assistant|&amp;gt;&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(&#xA;    conversations=conversation,&#xA;    sft_format=vl_chat_processor.sft_format,&#xA;    system_prompt=&#34;&#34;,&#xA;)&#xA;prompt = sft_format + vl_chat_processor.image_start_tag&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;def generate(&#xA;    mmgpt: MultiModalityCausalLM,&#xA;    vl_chat_processor: VLChatProcessor,&#xA;    prompt: str,&#xA;    temperature: float = 1,&#xA;    parallel_size: int = 16,&#xA;    cfg_weight: float = 5,&#xA;    image_token_num_per_image: int = 576,&#xA;    img_size: int = 384,&#xA;    patch_size: int = 16,&#xA;):&#xA;    input_ids = vl_chat_processor.tokenizer.encode(prompt)&#xA;    input_ids = torch.LongTensor(input_ids)&#xA;&#xA;    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()&#xA;    for i in range(parallel_size*2):&#xA;        tokens[i, :] = input_ids&#xA;        if i % 2 != 0:&#xA;            tokens[i, 1:-1] = vl_chat_processor.pad_id&#xA;&#xA;    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)&#xA;&#xA;    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()&#xA;&#xA;    for i in range(image_token_num_per_image):&#xA;        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)&#xA;        hidden_states = outputs.last_hidden_state&#xA;        &#xA;        logits = mmgpt.gen_head(hidden_states[:, -1, :])&#xA;        logit_cond = logits[0::2, :]&#xA;        logit_uncond = logits[1::2, :]&#xA;        &#xA;        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)&#xA;        probs = torch.softmax(logits / temperature, dim=-1)&#xA;&#xA;        next_token = torch.multinomial(probs, num_samples=1)&#xA;        generated_tokens[:, i] = next_token.squeeze(dim=-1)&#xA;&#xA;        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)&#xA;        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)&#xA;        inputs_embeds = img_embeds.unsqueeze(dim=1)&#xA;&#xA;&#xA;    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])&#xA;    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)&#xA;&#xA;    dec = np.clip((dec + 1) / 2 * 255, 0, 255)&#xA;&#xA;    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)&#xA;    visual_img[:, :, :] = dec&#xA;&#xA;    os.makedirs(&#39;generated_samples&#39;, exist_ok=True)&#xA;    for i in range(parallel_size):&#xA;        save_path = os.path.join(&#39;generated_samples&#39;, &#34;img_{}.jpg&#34;.format(i))&#xA;        PIL.Image.fromarray(visual_img[i]).save(save_path)&#xA;&#xA;&#xA;generate(&#xA;    vl_gpt,&#xA;    vl_chat_processor,&#xA;    prompt,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA; &lt;p&gt;We have deployed online demo in &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B&#34;&gt;Huggingface&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For the local gradio demo, you can run with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install -e .[gradio]&#xA;&#xA;python demo/app_januspro.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Have Fun!&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Janus&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA; &lt;h4&gt;Multimodal Understanding&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import torch&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;from janus.utils.io import load_pil_images&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;&amp;lt;image_placeholder&amp;gt;\nConvert the formula into latex code.&#34;,&#xA;        &#34;images&#34;: [&#34;images/equation.png&#34;],&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation, images=pil_images, force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# # run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# # run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True,&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Text-to-Image Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import PIL.Image&#xA;import torch&#xA;import numpy as np&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair&#34;,&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(&#xA;    conversations=conversation,&#xA;    sft_format=vl_chat_processor.sft_format,&#xA;    system_prompt=&#34;&#34;,&#xA;)&#xA;prompt = sft_format + vl_chat_processor.image_start_tag&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;def generate(&#xA;    mmgpt: MultiModalityCausalLM,&#xA;    vl_chat_processor: VLChatProcessor,&#xA;    prompt: str,&#xA;    temperature: float = 1,&#xA;    parallel_size: int = 16,&#xA;    cfg_weight: float = 5,&#xA;    image_token_num_per_image: int = 576,&#xA;    img_size: int = 384,&#xA;    patch_size: int = 16,&#xA;):&#xA;    input_ids = vl_chat_processor.tokenizer.encode(prompt)&#xA;    input_ids = torch.LongTensor(input_ids)&#xA;&#xA;    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()&#xA;    for i in range(parallel_size*2):&#xA;        tokens[i, :] = input_ids&#xA;        if i % 2 != 0:&#xA;            tokens[i, 1:-1] = vl_chat_processor.pad_id&#xA;&#xA;    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)&#xA;&#xA;    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()&#xA;&#xA;    for i in range(image_token_num_per_image):&#xA;        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)&#xA;        hidden_states = outputs.last_hidden_state&#xA;        &#xA;        logits = mmgpt.gen_head(hidden_states[:, -1, :])&#xA;        logit_cond = logits[0::2, :]&#xA;        logit_uncond = logits[1::2, :]&#xA;        &#xA;        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)&#xA;        probs = torch.softmax(logits / temperature, dim=-1)&#xA;&#xA;        next_token = torch.multinomial(probs, num_samples=1)&#xA;        generated_tokens[:, i] = next_token.squeeze(dim=-1)&#xA;&#xA;        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)&#xA;        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)&#xA;        inputs_embeds = img_embeds.unsqueeze(dim=1)&#xA;&#xA;&#xA;    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])&#xA;    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)&#xA;&#xA;    dec = np.clip((dec + 1) / 2 * 255, 0, 255)&#xA;&#xA;    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)&#xA;    visual_img[:, :, :] = dec&#xA;&#xA;    os.makedirs(&#39;generated_samples&#39;, exist_ok=True)&#xA;    for i in range(parallel_size):&#xA;        save_path = os.path.join(&#39;generated_samples&#39;, &#34;img_{}.jpg&#34;.format(i))&#xA;        PIL.Image.fromarray(visual_img[i]).save(save_path)&#xA;&#xA;&#xA;generate(&#xA;    vl_gpt,&#xA;    vl_chat_processor,&#xA;    prompt,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA; &lt;p&gt;We have deployed online demo in &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-1.3B&#34;&gt;Huggingface&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For the local gradio demo, you can run with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install -e .[gradio]&#xA;&#xA;python demo/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Have Fun!&lt;/p&gt; &#xA; &lt;h3&gt;FastAPI Demo&lt;/h3&gt; &#xA; &lt;p&gt;It&#39;s easy to run a FastAPI server to host an API server running the same functions as gradio.&lt;/p&gt; &#xA; &lt;p&gt;To start FastAPI server, run the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python demo/fastapi_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To test the server, you can open another terminal and run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python demo/fastapi_client.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;JanusFlow&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;pip install diffusers[torch]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;ü§ó Huggingface Online Demo&lt;/h3&gt; &#xA; &lt;p&gt;Check out the demo in &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA; &lt;h4&gt;Multimodal Understanding&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import torch&#xA;from janus.janusflow.models import MultiModalityCausalLM, VLChatProcessor&#xA;from janus.utils.io import load_pil_images&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/JanusFlow-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt = MultiModalityCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;&amp;lt;image_placeholder&amp;gt;\nConvert the formula into latex code.&#34;,&#xA;        &#34;images&#34;: [&#34;images/equation.png&#34;],&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation, images=pil_images, force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# # run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# # run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True,&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Text-to-Image Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import PIL.Image&#xA;import torch&#xA;import numpy as np&#xA;from janus.janusflow.models import MultiModalityCausalLM, VLChatProcessor&#xA;import torchvision&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/JanusFlow-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt = MultiModalityCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;from diffusers.models import AutoencoderKL&#xA;# remember to use bfloat16 dtype, this vae doesn&#39;t work with fp16&#xA;vae = AutoencoderKL.from_pretrained(&#34;stabilityai/sdxl-vae&#34;)&#xA;vae = vae.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair&#34;,&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(&#xA;    conversations=conversation,&#xA;    sft_format=vl_chat_processor.sft_format,&#xA;    system_prompt=&#34;&#34;,&#xA;)&#xA;prompt = sft_format + vl_chat_processor.image_gen_tag&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;def generate(&#xA;    mmgpt: MultiModalityCausalLM,&#xA;    vl_chat_processor: VLChatProcessor,&#xA;    prompt: str,&#xA;    cfg_weight: float = 5.0,&#xA;    num_inference_steps: int = 30,&#xA;    batchsize: int = 5&#xA;):&#xA;    input_ids = vl_chat_processor.tokenizer.encode(prompt)&#xA;    input_ids = torch.LongTensor(input_ids)&#xA;    &#xA;    tokens = torch.stack([input_ids] * 2 * batchsize).cuda()&#xA;    tokens[batchsize:, 1:] = vl_chat_processor.pad_id&#xA;    inputs_embeds = vl_gpt.language_model.get_input_embeddings()(tokens)&#xA;&#xA;    # we remove the last &amp;lt;bog&amp;gt; token and replace it with t_emb later&#xA;    inputs_embeds = inputs_embeds[:, :-1, :] &#xA;    &#xA;    # generate with rectified flow ode&#xA;    # step 1: encode with vision_gen_enc&#xA;    z = torch.randn((batchsize, 4, 48, 48), dtype=torch.bfloat16).cuda()&#xA;    &#xA;    dt = 1.0 / num_inference_steps&#xA;    dt = torch.zeros_like(z).cuda().to(torch.bfloat16) + dt&#xA;    &#xA;    # step 2: run ode&#xA;    attention_mask = torch.ones((2*batchsize, inputs_embeds.shape[1]+577)).to(vl_gpt.device)&#xA;    attention_mask[batchsize:, 1:inputs_embeds.shape[1]] = 0&#xA;    attention_mask = attention_mask.int()&#xA;    for step in range(num_inference_steps):&#xA;        # prepare inputs for the llm&#xA;        z_input = torch.cat([z, z], dim=0) # for cfg&#xA;        t = step / num_inference_steps * 1000.&#xA;        t = torch.tensor([t] * z_input.shape[0]).to(dt)&#xA;        z_enc = vl_gpt.vision_gen_enc_model(z_input, t)&#xA;        z_emb, t_emb, hs = z_enc[0], z_enc[1], z_enc[2]&#xA;        z_emb = z_emb.view(z_emb.shape[0], z_emb.shape[1], -1).permute(0, 2, 1)&#xA;        z_emb = vl_gpt.vision_gen_enc_aligner(z_emb)&#xA;        llm_emb = torch.cat([inputs_embeds, t_emb.unsqueeze(1), z_emb], dim=1)&#xA;&#xA;        # input to the llm&#xA;        # we apply attention mask for CFG: 1 for tokens that are not masked, 0 for tokens that are masked.&#xA;        if step == 0:&#xA;            outputs = vl_gpt.language_model.model(inputs_embeds=llm_emb, &#xA;                                             use_cache=True, &#xA;                                             attention_mask=attention_mask,&#xA;                                             past_key_values=None)&#xA;            past_key_values = []&#xA;            for kv_cache in past_key_values:&#xA;                k, v = kv_cache[0], kv_cache[1]&#xA;                past_key_values.append((k[:, :, :inputs_embeds.shape[1], :], v[:, :, :inputs_embeds.shape[1], :]))&#xA;            past_key_values = tuple(past_key_values)&#xA;        else:&#xA;            outputs = vl_gpt.language_model.model(inputs_embeds=llm_emb, &#xA;                                             use_cache=True, &#xA;                                             attention_mask=attention_mask,&#xA;                                             past_key_values=past_key_values)&#xA;        hidden_states = outputs.last_hidden_state&#xA;        &#xA;        # transform hidden_states back to v&#xA;        hidden_states = vl_gpt.vision_gen_dec_aligner(vl_gpt.vision_gen_dec_aligner_norm(hidden_states[:, -576:, :]))&#xA;        hidden_states = hidden_states.reshape(z_emb.shape[0], 24, 24, 768).permute(0, 3, 1, 2)&#xA;        v = vl_gpt.vision_gen_dec_model(hidden_states, hs, t_emb)&#xA;        v_cond, v_uncond = torch.chunk(v, 2)&#xA;        v = cfg_weight * v_cond - (cfg_weight-1.) * v_uncond&#xA;        z = z + dt * v&#xA;        &#xA;    # step 3: decode with vision_gen_dec and sdxl vae&#xA;    decoded_image = vae.decode(z / vae.config.scaling_factor).sample&#xA;    &#xA;    os.makedirs(&#39;generated_samples&#39;, exist_ok=True)&#xA;    save_path = os.path.join(&#39;generated_samples&#39;, &#34;img.jpg&#34;)&#xA;    torchvision.utils.save_image(decoded_image.clip_(-1.0, 1.0)*0.5+0.5, save_path)&#xA;&#xA;generate(&#xA;    vl_gpt,&#xA;    vl_chat_processor,&#xA;    prompt,&#xA;    cfg_weight=2.0,&#xA;    num_inference_steps=30,&#xA;    batchsize=5&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA; &lt;p&gt;For the local gradio demo, you can run with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install -e .[gradio]&#xA;&#xA;python demo/app_janusflow.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Have Fun!&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;4. License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is licensed under &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-CODE&#34;&gt;the MIT License&lt;/a&gt;. The use of Janus models is subject to &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-MODEL&#34;&gt;DeepSeek Model License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;5. Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{chen2025janus,&#xA;  title={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling},&#xA;  author={Chen, Xiaokang and Wu, Zhiyu and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong},&#xA;  journal={arXiv preprint arXiv:2501.17811},&#xA;  year={2025}&#xA;}&#xA;&#xA;@article{wu2024janus,&#xA;  title={Janus: Decoupling visual encoding for unified multimodal understanding and generation},&#xA;  author={Wu, Chengyue and Chen, Xiaokang and Wu, Zhiyu and Ma, Yiyang and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong and others},&#xA;  journal={arXiv preprint arXiv:2410.13848},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{ma2024janusflow,&#xA;      title={JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation}, &#xA;      author={Yiyang Ma and Xingchao Liu and Xiaokang Chen and Wen Liu and Chengyue Wu and Zhiyu Wu and Zizheng Pan and Zhenda Xie and Haowei Zhang and Xingkai yu and Liang Zhao and Yisong Wang and Jiaying Liu and Chong Ruan},&#xA;      journal={arXiv preprint arXiv:2411.07975},&#xA;      year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;6. Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href=&#34;mailto:service@deepseek.com&#34;&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>