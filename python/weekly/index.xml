<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-09T02:02:43Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hiyouga/ChatGLM-Efficient-Tuning</title>
    <updated>2023-07-09T02:02:43Z</updated>
    <id>tag:github.com,2023-07-09:/hiyouga/ChatGLM-Efficient-Tuning</id>
    <link href="https://github.com/hiyouga/ChatGLM-Efficient-Tuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fine-tuning ChatGLM-6B with PEFT | 基于 PEFT 的高效 ChatGLM 微调&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGLM Efficient Tuning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning&#34; alt=&#34;GitHub Code License&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue&#34; alt=&#34;GitHub pull request&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Fine-tuning 🤖&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt; model with 🤗&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;👋 Join our &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/assets/wechat.jpg&#34;&gt;WeChat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[ English | &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/README_zh.md&#34;&gt;中文&lt;/a&gt; ]&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[23/06/25] Now we align the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/src/api_demo.py&#34;&gt;demo API&lt;/a&gt; with the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI&#39;s&lt;/a&gt; format where you can insert the fine-tuned model in arbitrary ChatGPT-based applications.&lt;/p&gt; &#xA;&lt;p&gt;[23/06/25] Now we support fine-tuning the &lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;&gt;ChatGLM2-6B&lt;/a&gt; model with our framework! Try &lt;code&gt;--use_v2&lt;/code&gt; argument to fine-tune and predict that model.&lt;/p&gt; &#xA;&lt;p&gt;[23/06/05] Now we support 4-bit LoRA training (aka &lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt;). Try &lt;code&gt;--quantization_bit 4&lt;/code&gt; argument to work with 4-bit quantized model. (experimental feature)&lt;/p&gt; &#xA;&lt;p&gt;[23/06/01] We implemented a framework supporting the efficient tuning of LLaMA and BLOOM models. Please follow &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Efficient-Tuning&#34;&gt;LLaMA-Efficient-Tuning&lt;/a&gt; if you are interested.&lt;/p&gt; &#xA;&lt;p&gt;[23/05/19] Now we support using the development set to evaluate the model while training. Try &lt;code&gt;--dev_ratio&lt;/code&gt; argument to specify the size of development set.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/29] Now we support training ChatGLM with &lt;strong&gt;Reinforcement Learning with Human Feedback (RLHF)&lt;/strong&gt; ! We provide several examples to run RLHF training, please refer to the &lt;code&gt;examples&lt;/code&gt; folder for details.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/20] Our repo achieved 100 stars within 12 days! Congratulations!&lt;/p&gt; &#xA;&lt;p&gt;[23/04/19] Now we support &lt;strong&gt;merging the weights&lt;/strong&gt; of fine-tuned models trained by LoRA! Try &lt;code&gt;--checkpoint_dir checkpoint1,checkpoint2&lt;/code&gt; argument for continually fine-tuning the models.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/18] Now we support training the &lt;strong&gt;quantized models&lt;/strong&gt; using three fine-tuning methods! Try &lt;code&gt;quantization_bit&lt;/code&gt; argument for training the model in 4/8 bits.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/12] Now we support &lt;strong&gt;training from checkpoints&lt;/strong&gt;! Use &lt;code&gt;--checkpoint_dir&lt;/code&gt; argument to specify the checkpoint model to fine-tune from.&lt;/p&gt; &#xA;&lt;p&gt;[23/04/11] Now we support training with &lt;strong&gt;combined datasets&lt;/strong&gt;! Try &lt;code&gt;--dataset dataset1,dataset2&lt;/code&gt; argument for training with multiple datasets.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Our script now supports the following datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Stanford Alpaca (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_2M_CN&#34;&gt;BELLE 2M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BELLE 1M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BELLE 0.5M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&#34;&gt;BELLE Dialogue 0.4M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&#34;&gt;BELLE School Math 0.25M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BELLE Multiturn Chat 0.8M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;Guanaco Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;Firefly 1.1M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&#34;&gt;CodeAlpaca 20k&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&#34;&gt;Alpaca CoT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/suolyer/webqa&#34;&gt;Web QA (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;UltraChat&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your HuggingFace account using these commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade huggingface_hub&#xA;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine-Tuning Methods&lt;/h2&gt; &#xA;&lt;p&gt;Our script now supports the following fine-tuning methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning the low-rank adapters of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/P-tuning-v2&#34;&gt;P-Tuning V2&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning the prefix encoder of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.14913&#34;&gt;Freeze&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning the MLPs in the last n blocks of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Full Tuning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-tuning all the parameters of the model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+ and PyTorch 1.13.1&lt;/li&gt; &#xA; &lt;li&gt;🤗Transformers, Datasets, Accelerate, PEFT and TRL&lt;/li&gt; &#xA; &lt;li&gt;protobuf, cpm-kernels and sentencepiece&lt;/li&gt; &#xA; &lt;li&gt;jieba, rouge-chinese and nltk (used at evaluation)&lt;/li&gt; &#xA; &lt;li&gt;gradio and mdtex2html (used in web_demo.py)&lt;/li&gt; &#xA; &lt;li&gt;uvicorn, fastapi and sse-starlette (used in api_demo.py)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And &lt;strong&gt;powerful GPUs&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Data Preparation (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;data/example_dataset&lt;/code&gt; for checking the details about the format of dataset files. You can either use a single &lt;code&gt;.json&lt;/code&gt; file or a &lt;a href=&#34;https://huggingface.co/docs/datasets/dataset_script&#34;&gt;dataset loading script&lt;/a&gt; with multiple files to create a custom dataset.&lt;/p&gt; &#xA;&lt;p&gt;Note: please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset. About the format of this file, please refer to &lt;code&gt;data/README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Dependence Installation (optional)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git&#xA;conda create -n chatglm_etuning python=3.10&#xA;conda activate chatglm_etuning&#xA;cd ChatGLM-Efficient-Tuning&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to enable LoRA(QLoRA) or Freeze quantization on Windows, you will be required to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.1.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning with a Single GPU&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_sft_checkpoint \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 5e-5 \&#xA;    --num_train_epochs 3.0 \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki&#34;&gt;Wiki&lt;/a&gt; about the details of the arguments.&lt;/p&gt; &#xA;&lt;h3&gt;Distributed Fine-tuning with Multiple GPUs&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate config # configure the environment&#xA;accelerate launch src/train_sft.py # arguments (same as above)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: if you are using LoRA method at fine-tuning, please provide &lt;code&gt;--ddp_find_unused_parameters False&lt;/code&gt; argument to avoid the runtime error.&lt;/p&gt; &#xA;&lt;h3&gt;Training Reward Model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_rm.py \&#xA;    --do_train \&#xA;    --dataset comparison_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_rm_checkpoint \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training with RLHF&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_ppo.py \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_sft_checkpoint \&#xA;    --reward_model path_to_rm_checkpoint \&#xA;    --output_dir path_to_ppo_checkpoint \&#xA;    --per_device_train_batch_size 2 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation (BLEU and ROUGE_CHINESE)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --do_eval \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_eval_result \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --max_samples 50 \&#xA;    --predict_with_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Predict&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --do_predict \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_predict_result \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --max_samples 50 \&#xA;    --predict_with_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/cli_demo.py \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Web Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/web_demo.py \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Export model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/export_model.py \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_export&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Hardware Requirements&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Fine-tune method&lt;/th&gt; &#xA;   &lt;th&gt;Batch size&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;28GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;10GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;8GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;P-Tuning (p=16)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;P-Tuning (p=16)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;P-Tuning (p=16)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Freeze (l=3)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Freeze (l=3)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;8ex/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;RM method&lt;/th&gt; &#xA;   &lt;th&gt;Batch size&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + rm&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;22GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + rm&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;11GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;RLHF method&lt;/th&gt; &#xA;   &lt;th&gt;Batch size&lt;/th&gt; &#xA;   &lt;th&gt;Mode&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + ppo&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;FP16&lt;/td&gt; &#xA;   &lt;td&gt;23GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA (r=8) + ppo&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: &lt;code&gt;r&lt;/code&gt; is the lora rank, &lt;code&gt;p&lt;/code&gt; is the number of prefix tokens, &lt;code&gt;l&lt;/code&gt; is the number of trainable layers, &lt;code&gt;ex/s&lt;/code&gt; is the examples per second at training. The &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; is set to &lt;code&gt;1&lt;/code&gt;. All are evaluated on a single Tesla V100 (32G) GPU, they are approximated values and may vary in different GPUs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Fine-tuning ChatGLM: A Case&lt;/h2&gt; &#xA;&lt;h3&gt;Training Results&lt;/h3&gt; &#xA;&lt;p&gt;We use the whole &lt;code&gt;alpaca_gpt4_zh&lt;/code&gt; dataset to fine-tune the ChatGLM model with LoRA (r=8) for one epoch, using the default hyper-parameters. The loss curve during training is presented below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/assets/trainer_state.jpg&#34; alt=&#34;training loss&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation Results&lt;/h3&gt; &#xA;&lt;p&gt;We select 100 instances in the &lt;code&gt;alpaca_gpt4_zh&lt;/code&gt; dataset to evaluate the fine-tuned ChatGLM model and compute the BLEU and ROUGE scores. The results are presented below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Score&lt;/th&gt; &#xA;   &lt;th&gt;Original&lt;/th&gt; &#xA;   &lt;th&gt;FZ (l=2)&lt;/th&gt; &#xA;   &lt;th&gt;PT (p=16)&lt;/th&gt; &#xA;   &lt;th&gt;LoRA (r=8)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLEU-4&lt;/td&gt; &#xA;   &lt;td&gt;15.75&lt;/td&gt; &#xA;   &lt;td&gt;16.85&lt;/td&gt; &#xA;   &lt;td&gt;16.06&lt;/td&gt; &#xA;   &lt;td&gt;17.01 (&lt;strong&gt;+1.26&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rouge-1&lt;/td&gt; &#xA;   &lt;td&gt;34.51&lt;/td&gt; &#xA;   &lt;td&gt;36.62&lt;/td&gt; &#xA;   &lt;td&gt;34.80&lt;/td&gt; &#xA;   &lt;td&gt;36.77 (&lt;strong&gt;+2.26&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rouge-2&lt;/td&gt; &#xA;   &lt;td&gt;15.11&lt;/td&gt; &#xA;   &lt;td&gt;17.04&lt;/td&gt; &#xA;   &lt;td&gt;15.32&lt;/td&gt; &#xA;   &lt;td&gt;16.83 (&lt;strong&gt;+1.72&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rouge-l&lt;/td&gt; &#xA;   &lt;td&gt;26.18&lt;/td&gt; &#xA;   &lt;td&gt;28.17&lt;/td&gt; &#xA;   &lt;td&gt;26.35&lt;/td&gt; &#xA;   &lt;td&gt;28.86 (&lt;strong&gt;+2.68&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Params (%)&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;4.35%&lt;/td&gt; &#xA;   &lt;td&gt;0.06%&lt;/td&gt; &#xA;   &lt;td&gt;0.06%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;FZ: freeze tuning, PT: P-Tuning V2 (we use &lt;code&gt;pre_seq_len=16&lt;/code&gt; for fair comparison with LoRA), Params: the percentange of trainable parameters.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Compared with Existing Implementations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning&#34;&gt;THUDM/ChatGLM-6B&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Official implementation of fine-tuning ChatGLM with &lt;a href=&#34;https://github.com/THUDM/P-tuning-v2&#34;&gt;P-Tuning v2&lt;/a&gt; on the &lt;a href=&#34;https://aclanthology.org/D19-1321.pdf&#34;&gt;ADGEN&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;Our fine-tuning script is largely depend on it. We further implement the &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; tuning method. Additionally, we &lt;strong&gt;dynamically&lt;/strong&gt; pad the inputs to the longest sequence in the batch instead of the maximum length, to accelerate the fine-tuning.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mymusise/ChatGLM-Tuning&#34;&gt;mymusise/ChatGLM-Tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unoffical implementation of fine-tuning ChatGLM with &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;We borrowed some ideas from it. Our fine-tuning script &lt;strong&gt;integrates&lt;/strong&gt; the data pre-processing part into the training procedure, so we need not generate a pre-processed dataset before training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ssbuild/chatglm_finetuning&#34;&gt;ssbuild/chatglm_finetuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM with several PEFT methods on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;Our fine-tuning script is implemented &lt;strong&gt;purely&lt;/strong&gt; with &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface transformers&lt;/a&gt; and is independent of the &lt;a href=&#34;https://github.com/ssbuild/deep_training&#34;&gt;deep_training&lt;/a&gt; framework.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lich99/ChatGLM-finetune-LoRA&#34;&gt;lich99/ChatGLM-finetune-LoRA&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM with &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; dataset.&lt;/li&gt; &#xA;   &lt;li&gt;We use the &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;Huggingface PEFT&lt;/a&gt; to provide the state-of-the-art PEFT methods.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liucongg/ChatGLM-Finetuning&#34;&gt;liucongg/ChatGLM-Finetuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM with several methods including Freeze, LoRA and P-Tuning on the industrial dataset.&lt;/li&gt; &#xA;   &lt;li&gt;We are aim to incorporate more instruction-following datasets for fine-tuning the ChatGLM model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yanqiangmiffy/InstructGLM&#34;&gt;yanqiangmiffy/InstructGLM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An unofficial implementation of fine-tuning ChatGLM that explores the ChatGLM&#39;s ability on the instruction-following datasets.&lt;/li&gt; &#xA;   &lt;li&gt;Our fine-tuning script integrates the data pre-processing part in to the training procedure.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Employing &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; to easily build applications that are capable of leveraging external knowledge upon fine-tuned ChatGLM models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implementing the alignment algorithms to align human preferrences. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat&#34;&gt;RLHF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/GanjinZero/RRHF&#34;&gt;RRHF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/OptimalScale/LMFlow&#34;&gt;RAFT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;Chinese datasets&lt;/a&gt; into the training sets. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/CLUEbenchmark/pCLUE&#34;&gt;pCLUE&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/CLUEbenchmark/CLUECorpus2020&#34;&gt;CLUECorpus&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;GuanacoDataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;FireflyDataset&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://openai.com/blog/chatgpt&#34;&gt;ChatGPT&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://openai.com/research/gpt-4&#34;&gt;GPT-4&lt;/a&gt; self-chat data into the training sets. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/project-baize/baize-chatbot&#34;&gt;Baize&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4-LLM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implementing the Freeze-Tuning and P-Tuning method.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Supporting Multi-GPUs fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Adding script for evaluation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Loading from checkpoint.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fine-tuning the quantized model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Writing a guidebook about how to fine-tune ChatGLM with this framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Combining with state-of-the-art model editing algorithms. (&lt;em&gt;e.g. &lt;a href=&#34;https://arxiv.org/abs/2110.11309&#34;&gt;MEND&lt;/a&gt;&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Incorporating the &lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;OpenAssistant Conversations Dataset&lt;/a&gt; for SFT and alignment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating the high quality Chinese instruction dataset &lt;a href=&#34;https://huggingface.co/datasets/BAAI/COIG&#34;&gt;COIG&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/ChatGLM-Efficient-Tuning/main/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;. Please follow the &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/raw/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt; to use ChatGLM-6B model.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Misc{chatglm-efficient-tuning,&#xA;  title = {ChatGLM Efficient Tuning},&#xA;  author = {hiyouga},&#xA;  howpublished = {\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;, &lt;a href=&#34;https://github.com/mymusise/ChatGLM-Tuning&#34;&gt;ChatGLM-Tuning&lt;/a&gt; and &lt;a href=&#34;https://github.com/yuanzhoulvpi2017/zero_nlp&#34;&gt;yuanzhoulvpi2017/zero_nlp&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=hiyouga/ChatGLM-Efficient-Tuning&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databrickslabs/pyspark-ai</title>
    <updated>2023-07-09T02:02:43Z</updated>
    <id>tag:github.com,2023-07-09:/databrickslabs/pyspark-ai</id>
    <link href="https://github.com/databrickslabs/pyspark-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;English SDK for Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/databrickslabs/pyspark-ai/master/docs/_static/english-sdk-spark.svg?sanitize=true&#34; alt=&#34;English SDK for Apache Spark&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The English SDK for Apache Spark is an extremely simple yet powerful tool. It takes English instructions and compile them into PySpark objects like DataFrames. Its goal is to make Spark more user-friendly and accessible, allowing you to focus your efforts on extracting insights from your data.&lt;/p&gt; &#xA;&lt;p&gt;For a more comprehensive introduction and background to our project, we have the following resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.databricks.com/blog/introducing-english-new-programming-language-apache-spark&#34;&gt;Blog Post&lt;/a&gt;: A detailed walkthrough of our project.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yj7XlTB1Jvc&amp;amp;t=511s&#34;&gt;Demo Video&lt;/a&gt;: 2023 Data + AI summit announcement video with demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ZunjkL3L62o&amp;amp;t=73s&#34;&gt;Breakout Session&lt;/a&gt;: A deep dive into the story behind the English SDK, its features, and future works at DATA+AI summit 2023.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pyspark-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuring OpenAI LLMs&lt;/h2&gt; &#xA;&lt;p&gt;As of July 2023, we have found that the GPT-4 works optimally with the English SDK. This superior AI model is readily accessible to all developers through the OpenAI API.&lt;/p&gt; &#xA;&lt;p&gt;To use OpenAI&#39;s Language Learning Models (LLMs), you can set your OpenAI secret key as the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable. This key can be found in your &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI account&lt;/a&gt;. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#39;sk-...&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the &lt;code&gt;SparkAI&lt;/code&gt; instances will use the GPT-4 model. However, you&#39;re encouraged to experiment with creating and implementing other LLMs, which can be passed during the initialization of &lt;code&gt;SparkAI&lt;/code&gt; instances for various use-cases.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Initialization&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark_ai import SparkAI&#xA;&#xA;spark_ai = SparkAI()&#xA;spark_ai.activate()  # active partial functions for Spark DataFrame&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data Ingestion&lt;/h3&gt; &#xA;&lt;p&gt;If you have &lt;a href=&#34;https://developers.google.com/docs/api/quickstart/python&#34;&gt;set up the Google Python client&lt;/a&gt;, you can ingest data via search engine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_df = spark_ai.create_df(&#34;2022 USA national auto sales by brand&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise, you can ingest data via URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_df = spark_ai.create_df(&#34;https://www.carpro.com/blog/full-year-2022-national-auto-sales-by-brand&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Take a look at the data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_df.show(n=5)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;rank&lt;/th&gt; &#xA;   &lt;th&gt;brand&lt;/th&gt; &#xA;   &lt;th&gt;us_sales_2022&lt;/th&gt; &#xA;   &lt;th&gt;sales_change_vs_2021&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Toyota&lt;/td&gt; &#xA;   &lt;td&gt;1849751&lt;/td&gt; &#xA;   &lt;td&gt;-9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;Ford&lt;/td&gt; &#xA;   &lt;td&gt;1767439&lt;/td&gt; &#xA;   &lt;td&gt;-2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;Chevrolet&lt;/td&gt; &#xA;   &lt;td&gt;1502389&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Honda&lt;/td&gt; &#xA;   &lt;td&gt;881201&lt;/td&gt; &#xA;   &lt;td&gt;-33&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;Hyundai&lt;/td&gt; &#xA;   &lt;td&gt;724265&lt;/td&gt; &#xA;   &lt;td&gt;-2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Plot&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_df.ai.plot()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/databrickslabs/pyspark-ai/master/docs/_static/auto_sales.png&#34; alt=&#34;2022 USA national auto sales by brand&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To plot with an instruction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_df.ai.plot(&#34;pie chart for US sales market shares, show the top 5 brands and the sum of others&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/databrickslabs/pyspark-ai/master/docs/_static/auto_sales_pie_char.png&#34; alt=&#34;2022 USA national auto sales_market_share by brand&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;DataFrame Transformation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_top_growth_df=auto_df.ai.transform(&#34;brand with the highest growth&#34;)&#xA;auto_top_growth_df.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;brand&lt;/th&gt; &#xA;   &lt;th&gt;us_sales_2022&lt;/th&gt; &#xA;   &lt;th&gt;sales_change_vs_2021&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cadillac&lt;/td&gt; &#xA;   &lt;td&gt;134726&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;DataFrame Explanation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_top_growth_df.ai.explain()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In summary, this dataframe is retrieving the brand with the highest sales change in 2022 compared to 2021. It presents the results sorted by sales change in descending order and only returns the top result.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;DataFrame Attribute Verification&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_top_growth_df.ai.verify(&#34;expect sales change percentage to be between -100 to 100&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;result: True&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;UDF Generation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@spark_ai.udf&#xA;def previous_years_sales(brand: str, current_year_sale: int, sales_change_percentage: float) -&amp;gt; int:&#xA;    &#34;&#34;&#34;Calculate previous years sales from sales change percentage&#34;&#34;&#34;&#xA;    ...&#xA;    &#xA;spark.udf.register(&#34;previous_years_sales&#34;, previous_years_sales)&#xA;auto_df.createOrReplaceTempView(&#34;autoDF&#34;)&#xA;&#xA;spark.sql(&#34;select brand as brand, previous_years_sales(brand, us_sales, sales_change_percentage) as 2021_sales from autoDF&#34;).show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;brand&lt;/th&gt; &#xA;   &lt;th&gt;2021_sales&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Toyota&lt;/td&gt; &#xA;   &lt;td&gt;2032693&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ford&lt;/td&gt; &#xA;   &lt;td&gt;1803509&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chevrolet&lt;/td&gt; &#xA;   &lt;td&gt;1417348&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Honda&lt;/td&gt; &#xA;   &lt;td&gt;1315225&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hyundai&lt;/td&gt; &#xA;   &lt;td&gt;739045&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Cache&lt;/h3&gt; &#xA;&lt;p&gt;The SparkAI supports a simple in-memory and persistent cache system. It keeps an in-memory staging cache, which gets updated for LLM and web search results. The staging cache can be persisted through the commit() method. Cache lookup is always performed on both in-memory staging cache and persistent cache.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark_ai.commit()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://github.com/databrickslabs/pyspark-ai/raw/master/examples/example.ipynb&#34;&gt;example.ipynb&lt;/a&gt; for more detailed usage examples.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re delighted that you&#39;re considering contributing to the English SDK for Apache Spark project! Whether you&#39;re fixing a bug or proposing a new feature, your contribution is highly appreciated.&lt;/p&gt; &#xA;&lt;p&gt;Before you start, please take a moment to read our &lt;a href=&#34;https://raw.githubusercontent.com/databrickslabs/pyspark-ai/master/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;. This guide provides an overview of how you can contribute to our project. We&#39;re currently in the early stages of development and we&#39;re working on introducing more comprehensive test cases and Github Action jobs for enhanced testing of each pull request.&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions or need assistance, feel free to open a new issue in the GitHub repository.&lt;/p&gt; &#xA;&lt;p&gt;Thank you for helping us improve the English SDK for Apache Spark. We&#39;re excited to see your contributions!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the Apache License 2.0.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>linyiLYi/snake-ai</title>
    <updated>2023-07-09T02:02:43Z</updated>
    <id>tag:github.com,2023-07-09:/linyiLYi/snake-ai</id>
    <link href="https://github.com/linyiLYi/snake-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An AI agent that beats the classic game &#34;Snake&#34;.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SnakeAI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/linyiLYi/snake-ai/master/README_CN.md&#34;&gt;简体中文&lt;/a&gt; | English | &lt;a href=&#34;https://raw.githubusercontent.com/linyiLYi/snake-ai/master/README_JA.md&#34;&gt;日本語&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project contains the program scripts for the classic game &#34;Snake&#34; and an artificial intelligence agent that can play the game automatically. The intelligent agent is trained using deep reinforcement learning and includes two versions: an agent based on a Multi-Layer Perceptron (MLP) and an agent based on a Convolution Neural Network (CNN), with the latter having a higher average game score.&lt;/p&gt; &#xA;&lt;h3&gt;File Structure&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;├───main&#xA;│   ├───logs&#xA;│   ├───trained_models_cnn&#xA;│   ├───trained_models_mlp&#xA;│   └───scripts&#xA;├───utils&#xA;│   └───scripts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The main code folder for the project is &lt;code&gt;main/&lt;/code&gt;. It contains &lt;code&gt;logs/&lt;/code&gt;, which includes terminal text and data curves of the training process (viewable using Tensorboard); &lt;code&gt;trained_models_cnn/&lt;/code&gt; and &lt;code&gt;trained_models_mlp/&lt;/code&gt; respectively contain the model weight files for the convolutional network and perceptron models at different stages, which can be used for running tests in &lt;code&gt;test_cnn.py&lt;/code&gt; and &lt;code&gt;test_mlp.py&lt;/code&gt; to observe the actual game performance of the two intelligent agents at different training stages.&lt;/p&gt; &#xA;&lt;p&gt;The other folder &lt;code&gt;utils/&lt;/code&gt; includes two utility scripts. &lt;code&gt;check_gpu_status/&lt;/code&gt; is used to check if the GPU can be called by PyTorch; &lt;code&gt;compress_code.py&lt;/code&gt; can remove all indentation and line breaks from the code, turning it into a tightly arranged single line of text for easier communication with GPT-4 when asking for code suggestions (GPT-4&#39;s understanding of code is far superior to humans and doesn&#39;t require indentation, line breaks, etc.).&lt;/p&gt; &#xA;&lt;h2&gt;Running Guide&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on the Python programming language and mainly uses external code libraries such as &lt;a href=&#34;https://www.pygame.org/news&#34;&gt;Pygame&lt;/a&gt;、&lt;a href=&#34;https://github.com/openai/gym&#34;&gt;OpenAI Gym&lt;/a&gt;、&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34;&gt;Stable-Baselines3&lt;/a&gt;. The Python version used for running the program is 3.8.16. It is recommended to use &lt;a href=&#34;https://www.anaconda.com&#34;&gt;Anaconda&lt;/a&gt; to configure the Python environment. The following setup process has been tested on the Windows 11 system. The following commands are for the console/terminal (Console/Terminal/Shell).&lt;/p&gt; &#xA;&lt;h3&gt;Environment Configuration&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a conda environment named SnakeAI with Python version 3.8.16&#xA;conda create -n SnakeAI python=3.8.16&#xA;conda activate SnakeAI&#xA;&#xA;# [Optional] To use GPU for training, manually install the full version of PyTorch&#xA;conda install pytorch=2.0.0 torchvision pytorch-cuda=11.8 -c pytorch -c nvidia&#xA;&#xA;# [Optional] Run the script to test if PyTorch can successfully call the GPU&#xA;python .\utils\check_gpu_status.py&#xA;&#xA;# Install external code libraries&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Tests&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;main/&lt;/code&gt; folder of the project contains the program scripts for the classic game &#34;Snake&#34;, based on the &lt;a href=&#34;https://www.pygame.org/news&#34;&gt;Pygame&lt;/a&gt; code library. You can directly run the following command to play the game:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent folder of the project]/snake-ai/main&#xA;python .\snake_game.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After completing the environment configuration, you can run &lt;code&gt;test_cnn.py&lt;/code&gt; or &lt;code&gt;test_mlp.py&lt;/code&gt; in the &lt;code&gt;main/&lt;/code&gt; folder to test and observe the actual performance of the two intelligent agents at different training stages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent folder of the project]/snake-ai/main&#xA;python test_cnn.py&#xA;python test_mlp.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weight files are stored in the &lt;code&gt;main/trained_models_cnn/&lt;/code&gt; and &lt;code&gt;main/trained_models_mlp/&lt;/code&gt; folders. Both test scripts call the trained models by default. If you want to observe the AI performance at different training stages, you can modify the &lt;code&gt;MODEL_PATH&lt;/code&gt; variable in the test scripts to point to the file path of other models.&lt;/p&gt; &#xA;&lt;h3&gt;Training Models&lt;/h3&gt; &#xA;&lt;p&gt;If you need to retrain the models, you can run &lt;code&gt;train_cnn.py&lt;/code&gt; or &lt;code&gt;train_mlp.py&lt;/code&gt; in the &lt;code&gt;main/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent folder of the project]/snake-ai/main&#xA;python train_cnn.py&#xA;python train_mlp.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Viewing Curves&lt;/h3&gt; &#xA;&lt;p&gt;The project includes Tensorboard curve graphs of the training process. You can use Tensorboard to view detailed data. It is recommended to use the integrated Tensorboard plugin in VSCode for direct viewing, or you can use the traditional method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent folder of the project]/snake-ai/main&#xA;tensorboard --logdir=logs/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open the default Tensorboard service address &lt;code&gt;http://localhost:6006/&lt;/code&gt; in your browser to view the interactive curve graphs of the training process.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The external code libraries used in this project include &lt;a href=&#34;https://www.pygame.org/news&#34;&gt;Pygame&lt;/a&gt;、&lt;a href=&#34;https://github.com/openai/gym&#34;&gt;OpenAI Gym&lt;/a&gt;、&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34;&gt;Stable-Baselines3&lt;/a&gt;. Thanks all the software developers for their selfless dedication to the open-source community!&lt;/p&gt; &#xA;&lt;p&gt;The convolutional neural network used in this project is from the Nature paper:&lt;/p&gt; &#xA;&lt;p&gt;[1] &lt;a href=&#34;https://www.nature.com/articles/nature14236&#34;&gt;Human-level control through deep reinforcement learning&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>