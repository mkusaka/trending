<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-21T02:03:45Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>netease-youdao/QAnything</title>
    <updated>2024-01-21T02:03:45Z</updated>
    <id>tag:github.com,2024-01-21:/netease-youdao/QAnything</id>
    <link href="https://github.com/netease-youdao/QAnything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Question and Answer based on Anything.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/netease-youdao/QAnything&#34;&gt; &#xA;  &lt;!-- Please provide path to your logo here --&gt; &lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/qanything_logo.png&#34; alt=&#34;Logo&#34; width=&#34;800&#34;&gt; &lt;/a&gt; &#xA; &lt;h1&gt;&lt;strong&gt;Q&lt;/strong&gt;uestion and &lt;strong&gt;A&lt;/strong&gt;nswer based on &lt;strong&gt;Anything&lt;/strong&gt;&lt;/h1&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/README_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://qanything.ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/try%20online-qanything.ai-purple&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://read.youdao.com#/home&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/try%20online-read.youdao.com-purple&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-yellow&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://github.com/netease-youdao/QAnything/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-red&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://twitter.com/YDopensource&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&amp;amp;style={style}&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/5uNpPsEJz8&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1197874288963895436?style=social&amp;amp;logo=discord&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Table of Contents&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#What-is-QAnything&#34;&gt;What is QAnything&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#Key-features&#34;&gt;Key features&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#Architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#FAQ&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#API-Document&#34;&gt;API Document&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#WeChat-Group&#34;&gt;WeChat Group&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#Acknowledgments&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;What is QAnything?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt;(&lt;strong&gt;Q&lt;/strong&gt;uestion and &lt;strong&gt;A&lt;/strong&gt;nswer based on &lt;strong&gt;Anything&lt;/strong&gt;) is a local knowledge base question-answering system designed to support a wide range of file formats and databases, allowing for offline installation and use.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;QAnything&lt;/code&gt;, you can simply drop any locally stored file of any format and receive accurate, fast, and reliable answers.&lt;/p&gt; &#xA;&lt;p&gt;Currently supported formats include: &lt;strong&gt;PDF, Word (doc/docx), PPT, Markdown, Eml, TXT, Images (jpg, png, etc.), Web links&lt;/strong&gt; and more formats coming soon‚Ä¶&lt;/p&gt; &#xA;&lt;h3&gt;Key features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Security&lt;/strong&gt;, supports installation and usage with network cable unplugged throughout the process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-language QA support&lt;/strong&gt;, freely switch between Chinese and English QA, regardless of the language of the document.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supports massive data QA&lt;/strong&gt;, two-stage retrieval ranking, solving the degradation problem of large-scale data retrieval; the more data, the better the performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance production-grade system&lt;/strong&gt;, directly deployable for enterprise applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-friendly&lt;/strong&gt;, no need for cumbersome configurations, one-click installation and deployment, ready to use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi knowledge base QA&lt;/strong&gt; Support selecting multiple knowledge bases for Q&amp;amp;A&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Architecture&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/qanything_arch.png&#34; width=&#34;700&#34; alt=&#34;qanything_system&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Why 2 stage retrieval?&lt;/h4&gt; &#xA;&lt;p&gt;In scenarios with a large volume of knowledge base data, the advantages of a two-stage approach are very clear. If only a first-stage embedding retrieval is used, there will be a problem of retrieval degradation as the data volume increases, as indicated by the green line in the following graph. However, after the second-stage reranking, there can be a stable increase in accuracy, &lt;strong&gt;the more data, the better the performance&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/two_stage_retrieval.jpg&#34; width=&#34;500&#34; alt=&#34;two stage retrievaal&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;QAnything uses the retrieval component &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt;, which is distinguished for its bilingual and crosslingual proficiency. BCEmbedding excels in bridging Chinese and English linguistic gaps, which achieves&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A high performance on &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/tree/master?tab=readme-ov-file#evaluate-semantic-representation-by-mteb&#34; target=&#34;_Self&#34;&gt;Semantic Representation Evaluations in MTEB&lt;/a&gt;&lt;/strong&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A new benchmark in the realm of &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/tree/master?tab=readme-ov-file#evaluate-rag-by-llamaindex&#34; target=&#34;_Self&#34;&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;1st RetrievalÔºàembeddingÔºâ&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Retrieval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PairClassification&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Classification&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Clustering&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.60&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;79.14&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.88&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;jina-embeddings-v2-base-en&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-embedding-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;65.73&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;69.00&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.29&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;38.95&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;59.43&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/EvaluationSummary/embedding_eval_summary.md&#34;&gt;Embedding Models Evaluation Summary&lt;/a&gt;„ÄÇ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2nd RetrievalÔºàrerankÔºâ&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-reranker-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/EvaluationSummary/reranker_eval_summary.md&#34;&gt;Reranker Models Evaluation Summary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;RAG Evaluations in LlamaIndexÔºàembedding and rerankÔºâ&lt;/h4&gt; &#xA;&lt;img src=&#34;https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/assets/rag_eval_multiple_domains_summary.jpg&#34;&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;code&gt;WithoutReranker&lt;/code&gt; setting, our &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; outperforms all the other embedding models.&lt;/li&gt; &#xA; &lt;li&gt;With fixing the embedding model, our &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; achieves the best performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The combination of &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; and &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; is SOTA&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to use embedding and rerank separately, please refer to &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;LLM&lt;/h4&gt; &#xA;&lt;p&gt;The open source version of QAnything is based on QwenLM and has been fine-tuned on a large number of professional question-answering datasets. It greatly enhances the ability of question-answering. If you need to use it for commercial purposes, please follow the license of QwenLM. For more details, please refer to: &lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;QwenLM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Before You Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Star us on GitHub, and be instantly notified for new release!&lt;/strong&gt; &lt;img src=&#34;https://github.com/netease-youdao/QAnything/assets/29041332/fd5e5926-b9b2-4675-9f60-6cdcaca18e14&#34; alt=&#34;star_us&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qanything.ai&#34;&gt;üèÑ Try QAnything Online&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://read.youdao.com&#34;&gt;üìö Try read.youdao.com | ÊúâÈÅìÈÄüËØª&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;üõ†Ô∏è Only use our BCEmbedding(embedding &amp;amp; rerank)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/FAQ_zh.md&#34;&gt;üìñ FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;strong&gt;For Linux&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Required item&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Minimum Requirement&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux&lt;/td&gt; &#xA;   &lt;td&gt;Single NVIDIA GPU Memory &lt;br&gt; or Double NVIDIA GPU Memory&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 16GB &lt;br&gt; &amp;gt;= 11GB + 5G&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA 3090 x 1 recommended &lt;br&gt; NVIDIA 2080TI √ó 2 recommended&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA Driver Version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 525.105.17&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CUDA Version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 12.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Docker version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 20.10.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker install&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;docker compose version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 2.23.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;docker compose install&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;&lt;strong&gt;For Winodws 11 with WSL 2&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Required item&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Minimum Requirement&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows 11 with WSL 2&lt;/td&gt; &#xA;   &lt;td&gt;Single NVIDIA GPU Memory &lt;br&gt; or Double NVIDIA GPU Memory&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 16GB &lt;br&gt; &amp;gt;= 11GB + 5G&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA 3090 &lt;br&gt; NVIDIA 2080TI √ó 2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GEFORCE EXPERIENCE&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 546.33&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://us.download.nvidia.com/GFE/GFEClient/3.27.0.120/GeForce_Experience_v3.27.0.120.exe&#34;&gt;GEFORCE EXPERIENCE download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Docker Desktop&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 4.26.1Ôºà131620Ôºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.docker.com/desktop/install/windows-install/&#34;&gt;Docker Desktop for Windows&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h3&gt;step1: pull qanything repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/netease-youdao/QAnything.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;step2: Enter the project root directory and execute the startup script.&lt;/h3&gt; &#xA;&lt;p&gt;If you are in the Windows11 system: Need to enter the WSL environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd QAnything&#xA;bash run.sh  # Start on GPU 0 by default.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;(Optional) Specify GPU startup&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd QAnything&#xA;bash run.sh 0  # gpu id 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;(Optional) Specify multi-GPU startup &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd QAnything&#xA;bash run.sh 0,1  # gpu ids: 0,1, Please confirm how many GPUs are available. Supports up to two cards for startup. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;step3: start to experience&lt;/h3&gt; &#xA;&lt;h4&gt;Front end&lt;/h4&gt; &#xA;&lt;p&gt;After successful installation, you can experience the application by entering the following addresses in your web browser.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Front end address: http://&lt;code&gt;your_host&lt;/code&gt;:5052/qanything/&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;API&lt;/h4&gt; &#xA;&lt;p&gt;If you want to visit API, please refer to the following address:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;API address: http://&lt;code&gt;your_host&lt;/code&gt;:8777/api/&lt;/li&gt; &#xA; &lt;li&gt;For detailed API documentation, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/API.md&#34;&gt;QAnything API documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Close service&lt;/h3&gt; &#xA;&lt;p&gt;If you are in the Windows11 system: Need to enter the WSL environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash close.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/FAQ_zh.md&#34;&gt;FAQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Cross-lingual: Multiple English paper Q&amp;amp;A&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/8915277f-c136-42b8-9332-78f64bf5df22&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/multi_paper_qa.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Information extraction&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/b9e3be94-183b-4143-ac49-12fa005a8a9a&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/information_extraction.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Various files&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/7ede63c1-4c7f-4557-bd2c-7c51a44c8e0b&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/various_files_qa.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Web Q&amp;amp;A&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/d30942f7-6dbd-4013-a4b6-82f7c2a5fbee&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/web_qa.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;API Document&lt;/h3&gt; &#xA;&lt;p&gt;If you need to access the API, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/API.md&#34;&gt;QAnything API documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community &amp;amp; Support&lt;/h2&gt; &#xA;&lt;h3&gt;Discord &lt;a href=&#34;https://discord.gg/5uNpPsEJz8&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1197874288963895436?style=social&amp;amp;logo=discord&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Welcome to the QAnything &lt;a href=&#34;https://discord.gg/5uNpPsEJz8&#34;&gt;Discord&lt;/a&gt; community&lt;/p&gt; &#xA;&lt;h3&gt;GitHub issues&lt;/h3&gt; &#xA;&lt;p&gt;Reach out to the maintainer at one of the following places:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/issues&#34;&gt;Github issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Contact options listed on &lt;a href=&#34;https://github.com/netease-youdao&#34;&gt;this GitHub profile&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;WeChat Group&lt;/h3&gt; &#xA;&lt;p&gt;Welcome to scan the QR code below and join the WeChat group.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/Wechat.jpg&#34; width=&#34;20%&#34; height=&#34;auto&#34;&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#netease-youdao/QAnything&amp;amp;netease-youdao/BCEmbedding&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=netease-youdao/QAnything,netease-youdao/BCEmbedding&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt; is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt; adopts dependencies from the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to our &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt; for the excellent embedding and rerank model.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;Qwen&lt;/a&gt; for strong base language models.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/triton-inference-server/server&#34;&gt;Triton Inference Server&lt;/a&gt; for providing great open source inference serving.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FasterTransformer&lt;/a&gt; for highly optimized LLM inference backend.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;Langchain&lt;/a&gt; for the wonderful llm application framework.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat&#34;&gt;Langchain-Chatchat&lt;/a&gt; for the inspiration provided on local knowledge base Q&amp;amp;A.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/milvus-io/milvus&#34;&gt;Milvus&lt;/a&gt; for the excellent semantic search library.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34;&gt;PaddleOCR&lt;/a&gt; for its ease-to-use OCR library.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/sanic-org/sanic&#34;&gt;Sanic&lt;/a&gt; for the powerful web service framework.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>MooreThreads/Moore-AnimateAnyone</title>
    <updated>2024-01-21T02:03:45Z</updated>
    <id>tag:github.com,2024-01-21:/MooreThreads/Moore-AnimateAnyone</id>
    <link href="https://github.com/MooreThreads/Moore-AnimateAnyone" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü§ó Introduction&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt; üèãÔ∏èüèãÔ∏èüèãÔ∏è We release our training codes!! Now you can train your own AnimateAnyone models. See &lt;a href=&#34;https://raw.githubusercontent.com/MooreThreads/Moore-AnimateAnyone/master/#train&#34;&gt;here&lt;/a&gt; for more details. Have fun!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt;Ôºöüî•üî•üî• We launch a HuggingFace Spaces demo of Moore-AnimateAnyone at &lt;a href=&#34;https://huggingface.co/spaces/xunsong/Moore-AnimateAnyone&#34;&gt;here&lt;/a&gt;!!&lt;/p&gt; &#xA;&lt;p&gt;This repository reproduces &lt;a href=&#34;https://github.com/HumanAIGC/AnimateAnyone&#34;&gt;AnimateAnyone&lt;/a&gt;. To align the results demonstrated by the original paper, we adopt various approaches and tricks, which may differ somewhat from the paper and another &lt;a href=&#34;https://github.com/guoqincode/Open-AnimateAnyone&#34;&gt;implementation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s worth noting that this is a very preliminary version, aiming for approximating the performance (roughly 80% under our test) showed in &lt;a href=&#34;https://github.com/HumanAIGC/AnimateAnyone&#34;&gt;AnimateAnyone&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We will continue to develop it, and also welcome feedbacks and ideas from the community. The enhanced version will also be launched on our &lt;a href=&#34;https://maliang.mthreads.com/&#34;&gt;MoBi MaLiang&lt;/a&gt; AIGC platform, running on our own full-featured GPU S4000 cloud computing platform.&lt;/p&gt; &#xA;&lt;h1&gt;üìù Release Plans&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference codes and pretrained weights&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training scripts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üéûÔ∏è Examples&lt;/h1&gt; &#xA;&lt;p&gt;Here are some results we generated, with the resolution of 512x768.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/f0454f30-6726-4ad4-80a7-5b7a15619057&#34;&gt;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/f0454f30-6726-4ad4-80a7-5b7a15619057&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/337ff231-68a3-4760-a9f9-5113654acf48&#34;&gt;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/337ff231-68a3-4760-a9f9-5113654acf48&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/9c4d852e-0a99-4607-8d63-569a1f67a8d2&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/722c6535-2901-4e23-9de9-501b22306ebd&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/17b907cc-c97e-43cd-af18-b646393c8e8a&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/86f2f6d2-df60-4333-b19b-4c5abcd5999d&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: We observe following shortcomings in current version:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The background may occur some artifacts, when the reference image has a clean background&lt;/li&gt; &#xA; &lt;li&gt;Suboptimal results may arise when there is a scale mismatch between the reference image and keypoints. We have yet to implement preprocessing techniques as mentioned in the &lt;a href=&#34;https://arxiv.org/pdf/2311.17117.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Some flickering and jittering may occur when the motion sequence is subtle or the scene is static.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;These issues will be addressed and improved in the near future. We appreciate your anticipation!&lt;/p&gt; &#xA;&lt;h1&gt;‚öíÔ∏è Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Build Environtment&lt;/h2&gt; &#xA;&lt;p&gt;We Recommend a python version &lt;code&gt;&amp;gt;=3.10&lt;/code&gt; and cuda version &lt;code&gt;=11.7&lt;/code&gt;. Then build environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# [Optional] Create a virtual env&#xA;python -m venv .venv&#xA;source .venv/bin/activate&#xA;# Install with pip:&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download weights&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatically downloading&lt;/strong&gt;: You can run the following command to download weights automatically:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/download_weights.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Weights will be placed under the &lt;code&gt;./pretrained_weights&lt;/code&gt; direcotry. The whole downloading process may take a long time.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Manually downloading&lt;/strong&gt;: You can also download weights manually, which has some steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our trained &lt;a href=&#34;https://huggingface.co/patrolli/AnimateAnyone/tree/main&#34;&gt;weights&lt;/a&gt;, which include four parts: &lt;code&gt;denoising_unet.pth&lt;/code&gt;, &lt;code&gt;reference_unet.pth&lt;/code&gt;, &lt;code&gt;pose_guider.pth&lt;/code&gt; and &lt;code&gt;motion_module.pth&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pretrained weight of based models and other components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;StableDiffusion V1.5&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/image_encoder&#34;&gt;image_encoder&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download dwpose weights (&lt;code&gt;dw-ll_ucoco_384.onnx&lt;/code&gt;, &lt;code&gt;yolox_l.onnx&lt;/code&gt;) following &lt;a href=&#34;https://github.com/IDEA-Research/DWPose?tab=readme-ov-file#-dwpose-for-controlnet&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, these weights should be orgnized as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;./pretrained_weights/&#xA;|-- DWPose&#xA;|   |-- dw-ll_ucoco_384.onnx&#xA;|   `-- yolox_l.onnx&#xA;|-- image_encoder&#xA;|   |-- config.json&#xA;|   `-- pytorch_model.bin&#xA;|-- denoising_unet.pth&#xA;|-- motion_module.pth&#xA;|-- pose_guider.pth&#xA;|-- reference_unet.pth&#xA;|-- sd-vae-ft-mse&#xA;|   |-- config.json&#xA;|   |-- diffusion_pytorch_model.bin&#xA;|   `-- diffusion_pytorch_model.safetensors&#xA;`-- stable-diffusion-v1-5&#xA;    |-- feature_extractor&#xA;    |   `-- preprocessor_config.json&#xA;    |-- model_index.json&#xA;    |-- unet&#xA;    |   |-- config.json&#xA;    |   `-- diffusion_pytorch_model.bin&#xA;    `-- v1-inference.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: If you have installed some of the pretrained models, such as &lt;code&gt;StableDiffusion V1.5&lt;/code&gt;, you can specify their paths in the config file (e.g. &lt;code&gt;./config/prompts/animation.yaml&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h1&gt;üöÄ Training and Inference&lt;/h1&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Here is the cli command for running inference scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.pose2vid --config ./configs/prompts/animation.yaml -W 512 -H 784 -L 64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can refer the format of &lt;code&gt;animation.yaml&lt;/code&gt; to add your own reference images or pose videos. To convert the raw video into a pose video (keypoint sequence), you can run with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/vid2pose.py --video_path /path/to/your/video.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span id=&#34;train&#34;&gt; Training &lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Note: package dependencies have been updated, you may upgrade your environment via &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; before training.&lt;/p&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Extract keypoints from raw videos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/extract_dwpose_from_vid.py --video_root /path/to/your/video_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Extract the meta info of dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/extract_meta_info.py --root_path /path/to/your/video_dir --dataset_name anyone &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update lines in the training config file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;data:&#xA;  meta_paths:&#xA;    - &#34;./data/anyone_meta.json&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stage1&lt;/h3&gt; &#xA;&lt;p&gt;Put &lt;a href=&#34;https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/tree/main&#34;&gt;openpose controlnet weights&lt;/a&gt; under &lt;code&gt;./pretrained_weights&lt;/code&gt;, which is used to initialize the pose_guider.&lt;/p&gt; &#xA;&lt;p&gt;Put &lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main&#34;&gt;sd-image-variation&lt;/a&gt; under &lt;code&gt;./pretrained_weights&lt;/code&gt;, which is used to initialize unet weights.&lt;/p&gt; &#xA;&lt;p&gt;Run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch train_stage_1.py --config configs/train/stage1.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stage2&lt;/h3&gt; &#xA;&lt;p&gt;Put the pretrained motion module weights &lt;code&gt;mm_sd_v15_v2.ckpt&lt;/code&gt; (&lt;a href=&#34;https://huggingface.co/guoyww/animatediff/blob/main/mm_sd_v15_v2.ckpt&#34;&gt;download link&lt;/a&gt;) under &lt;code&gt;./pretrained_weights&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Specify the stage1 training weights in the config file &lt;code&gt;stage2.yaml&lt;/code&gt;, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;stage1_ckpt_dir: &#39;./exp_output/stage1&#39;&#xA;stage1_ckpt_step: 30000 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch train_stage_2.py --config configs/train/stage2.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üé® Gradio Demo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;HuggingFace Demo&lt;/strong&gt;: We launch a quick preview demo of Moore-AnimateAnyone at &lt;a href=&#34;https://huggingface.co/spaces/xunsong/Moore-AnimateAnyone&#34;&gt;HuggingFace Spaces&lt;/a&gt;!! We appreciate the assistance provided by the HuggingFace team in setting up this demo.&lt;/p&gt; &#xA;&lt;p&gt;To reduce waiting time, we limit the size (width, height, and length) and inference steps when generating videos.&lt;/p&gt; &#xA;&lt;p&gt;If you have your own GPU resource (&amp;gt;= 16GB vram), you can run a local gradio app via following commands:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python app.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Community Contributions&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Installation for Windows users: &lt;a href=&#34;https://github.com/sdbds/Moore-AnimateAnyone-for-windows&#34;&gt;Moore-AnimateAnyone-for-windows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üñåÔ∏è Try on Mobi MaLiang&lt;/h1&gt; &#xA;&lt;p&gt;We will launched this model on our &lt;a href=&#34;https://maliang.mthreads.com/&#34;&gt;MoBi MaLiang&lt;/a&gt; AIGC platform, running on our own full-featured GPU S4000 cloud computing platform. Mobi MaLiang has now integrated various AIGC applications and functionalities (e.g. text-to-image, controllable generation...). You can experience it by &lt;a href=&#34;https://maliang.mthreads.com/&#34;&gt;clicking this link&lt;/a&gt; or scanning the QR code bellow via WeChat!&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MooreThreads/Moore-AnimateAnyone/master/assets/mini_program_maliang.png&#34; width=&#34;100&#xA;  &#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;‚öñÔ∏è Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This project is intended for academic research, and we explicitly disclaim any responsibility for user-generated content. Users are solely liable for their actions while using the generative model. The project contributors have no legal affiliation with, nor accountability for, users&#39; behaviors. It is imperative to use the generative model responsibly, adhering to both ethical and legal standards.&lt;/p&gt; &#xA;&lt;h1&gt;üôèüèª Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;We first thank the authors of &lt;a href=&#34;&#34;&gt;AnimateAnyone&lt;/a&gt;. Additionally, we would like to thank the contributors to the &lt;a href=&#34;https://github.com/magic-research/magic-animate&#34;&gt;majic-animate&lt;/a&gt;, &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;animatediff&lt;/a&gt; and &lt;a href=&#34;https://github.com/guoqincode/Open-AnimateAnyone&#34;&gt;Open-AnimateAnyone&lt;/a&gt; repositories, for their open research and exploration. Furthermore, our repo incorporates some codes from &lt;a href=&#34;https://github.com/IDEA-Research/DWPose&#34;&gt;dwpose&lt;/a&gt; and &lt;a href=&#34;https://github.com/s9roll7/animatediff-cli-prompt-travel/&#34;&gt;animatediff-cli-prompt-travel&lt;/a&gt;, and we extend our thanks to them as well.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>joaomdmoura/crewAI-examples</title>
    <updated>2024-01-21T02:03:45Z</updated>
    <id>tag:github.com,2024-01-21:/joaomdmoura/crewAI-examples</id>
    <link href="https://github.com/joaomdmoura/crewAI-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Examples for crewAI&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;crewAI is designed to facilitate the collaboration of role-playing AI agents. This is a collection of examples of different ways to use the crewAI framework to automate the processes. By &lt;a href=&#34;https://x.com/joaomdmoura&#34;&gt;@joaomdmoura&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples/tree/main/trip_planner&#34;&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples/tree/main/stock_analysis&#34;&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples/tree/main/landing_page_generator&#34;&gt;Landing Page Generator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples/tree/main/instagram_post&#34;&gt;Create Instagram Post&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples/tree/main/markdown_validator&#34;&gt;Markdown Validator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples/tree/main/azure_model&#34;&gt;Using Azure OpenAI API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>