<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-12T02:03:10Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LAION-AI/Open-Assistant</title>
    <updated>2023-02-12T02:03:10Z</updated>
    <id>tag:github.com,2023-02-12:/LAION-AI/Open-Assistant</id>
    <link href="https://github.com/LAION-AI/Open-Assistant" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;span&gt;Open-Assistant&lt;/span&gt; &lt;img width=&#34;auto&#34; height=&#34;50px&#34; src=&#34;https://github.com/LAION-AI/Open-Assistant/raw/main/assets/logo_crop.png&#34;&gt; &lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://laion-ai.github.io/Open-Assistant/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-laion--ai.github.io%2FOpen--Assistant%2F-green&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant/actions/workflows/build-frontend.yaml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/build-frontend.yaml?label=frontend&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant/actions/workflows/pre-commit.yaml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/pre-commit.yaml?label=pre-commit&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant/actions/workflows/test-api-contract.yaml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/test-api-contract.yaml?label=api&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/LAION-AI/Open-Assistant&#34; alt=&#34;GitHub release (latest by date)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Here is our website to collect data:&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://open-assistant.io&#34;&gt;open-assistant.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(project documentation lives &lt;a href=&#34;https://laion-ai.github.io/Open-Assistant/&#34;&gt;here&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LAION-AI/Open-Assistant/main/#what-is-open-assistant&#34;&gt;What is Open Assistant?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LAION-AI/Open-Assistant/main/#do-you-want-to-try-it-out&#34;&gt;Do you want to try it out?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LAION-AI/Open-Assistant/main/#the-plan&#34;&gt;The Plan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LAION-AI/Open-Assistant/main/#the-vision&#34;&gt;The Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LAION-AI/Open-Assistant/main/#how-can-you-help&#34;&gt;How can you help?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LAION-AI/Open-Assistant/main/CONTRIBUTING.md&#34;&gt;Iâ€™m in! How do I contribute?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;What is Open Assistant?&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Open Assistant is a project meant to give everyone access to a great chat based large language model. &lt;/p&gt; &#xA;&lt;p&gt;We believe that by doing this we will create a revolution in innovation in language. In the same way that stable-diffusion helped the world make art and images in new ways we hope Open Assistant can help improve the world by improving language itself.&lt;/p&gt; &#xA;&lt;h2&gt;Do you want to try it out?&lt;/h2&gt; &#xA;&lt;h3&gt;Contributing to Data Collection&lt;/h3&gt; &#xA;&lt;p&gt;The data collection frontend is now live &lt;a href=&#34;https://open-assistant.io/&#34;&gt;here&lt;/a&gt;. Log in and start taking on tasks! We want to collect a high volume of quality data. By submitting, ranking, and labelling model prompts and responses you will be directly helping to improve the capabilities of Open Assistant.&lt;/p&gt; &#xA;&lt;h3&gt;Running Locally&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;You do not need to run the project locally unless you are contributing to the development process. The website link above will take you to the public website where you can use the data collection app.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you would like to run the data collection app locally for development, you can set up an entire stack needed to run &lt;strong&gt;Open-Assistant&lt;/strong&gt;, including the website, backend, and associated dependent services, with Docker.&lt;/p&gt; &#xA;&lt;h5&gt;To start the demo, run this in the root directory of the repository:&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, navigate to &lt;code&gt;http://localhost:3000&lt;/code&gt; (It may take some time to boot up) and interact with the website.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If an issue occurs with the build, please head to the &lt;a href=&#34;https://projects.laion.ai/Open-Assistant/docs/faq&#34;&gt;FAQ&lt;/a&gt; and check out the entries about Docker.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When logging in via email, navigate to &lt;code&gt;http://localhost:1080&lt;/code&gt; to get the magic email login link.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you would like to run this in a standardized development environment (a &lt;a href=&#34;https://code.visualstudio.com/docs/devcontainers/containers&#34;&gt;&#34;devcontainer&#34;&lt;/a&gt;) using &lt;a href=&#34;https://code.visualstudio.com/docs/devcontainers/create-dev-container#_create-a-devcontainerjson-file&#34;&gt;vscode locally&lt;/a&gt; or in a web browser using &lt;a href=&#34;https://github.com/features/codespaces&#34;&gt;GitHub Codespaces&lt;/a&gt;, you can use the provided &lt;a href=&#34;https://raw.githubusercontent.com/LAION-AI/Open-Assistant/main/.devcontainer/&#34;&gt;&lt;code&gt;.devcontainer&lt;/code&gt;&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;The Plan&lt;/h2&gt; &#xA;&lt;h5&gt;We want to get to an initial MVP as fast as possible, by following the 3-steps outlined in the &lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;InstructGPT paper&lt;/a&gt;.&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Collect high-quality human generated Instruction-Fulfillment samples (prompt + response), goal &amp;gt;50k. We design a crowdsourced process to collect and reviewed prompts. We do not want to train on flooding/toxic/spam/junk/personal information data. We will have a leaderboard to motivate the community that shows progress and the most active users. Swag will be given to the top-contributors.&lt;/li&gt; &#xA; &lt;li&gt;For each of the collected prompts we will sample multiple completions. Completions of one prompt will then be shown randomly to users to rank them from best to worst. Again this should happen crowd-sourced, e.g. we need to deal with unreliable potentially malicious users. At least multiple votes by independent users have to be collected to measure the overall agreement. The gathered ranking-data will be used to train a reward model.&lt;/li&gt; &#xA; &lt;li&gt;Now follows the RLHF training phase based on the prompts and the reward model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We can then take the resulting model and continue with completion sampling step 2 for a next iteration.&lt;/p&gt; &#xA;&lt;h2&gt;The Vision&lt;/h2&gt; &#xA;&lt;p&gt;We are not going to stop at replicating ChatGPT. We want to build the assistant of the future, able to not only write email and cover letters, but do meaningful work, use APIs, dynamically research information, and much more, with the ability to be personalized and extended by anyone. And we want to do this in a way that is open and accessible, which means we must not only build a great assistant, but also make it small and efficient enough to run on consumer hardware.&lt;/p&gt; &#xA;&lt;h3&gt;Slide Decks&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1n7IrAOVOqwdYgiYrXc8Sj0He8krn5MVZO_iLkCjTtu0/edit?usp=sharing&#34;&gt;Vision &amp;amp; Roadmap&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1iaX_nxasVWlvPiSNs0cllR9L_1neZq0RJxd6MFEalUY/edit?usp=sharing&#34;&gt;Important Data Structures&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How can you help?&lt;/h2&gt; &#xA;&lt;p&gt;All open source projects begin with people like you. Open source is the belief that if we collaborate we can together gift our knowledge and technology to the world for the benefit of humanity.&lt;/p&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://raw.githubusercontent.com/LAION-AI/Open-Assistant/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Zero6992/chatGPT-discord-bot</title>
    <updated>2023-02-12T02:03:10Z</updated>
    <id>tag:github.com,2023-02-12:/Zero6992/chatGPT-discord-bot</id>
    <link href="https://github.com/Zero6992/chatGPT-discord-bot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Integrate ChatGPT into your own discord bot&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT Discord Bot&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Build your own Discord bot using ChatGPT&lt;/h3&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;h4&gt;2023-02-10 Update: ChatGPT model but requires payment&lt;/h4&gt; &#xA; &lt;h4&gt;2023-02-09 Update: Temporarily using the GPT-3 model&lt;/h4&gt; &#xA; &lt;p&gt;(Requires payment, please be aware)&lt;/p&gt; &#xA; &lt;h4&gt;2023-02-08 Update: ChatGPT API is highly unstable now&lt;/h4&gt; &#xA; &lt;h4&gt;2023-02-03 Update: ChatGPT API working again&lt;/h4&gt; &#xA; &lt;h4&gt;2023-02-02 Update: OpenAI has closed ChatGPT API, temporarily switching to using GPT-3 model&lt;/h4&gt; &#xA; &lt;h4&gt;2023-02-01 Update: Now using the official ChatGPT API&lt;/h4&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/chat [message]&lt;/code&gt; Chat with ChatGPT!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/private&lt;/code&gt; ChatGPT switch to private mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/public&lt;/code&gt; ChatGPT switch to public mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/replyall&lt;/code&gt; ChatGPT switch between replyall mode and default mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/reset&lt;/code&gt; Clear ChatGPT conversation history&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Chat&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/89479282/206497774-47d960cd-1aeb-4fba-9af5-1f9d6ff41f00.gif&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Mode&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;public mode (default)&lt;/code&gt; the bot directly reply on the channel&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/89479282/206565977-d7c5d405-fdb4-4202-bbdd-715b7c8e8415.gif&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;private mode&lt;/code&gt; the bot&#39;s reply can only be seen by the person who used the command&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/89479282/206565873-b181e600-e793-4a94-a978-47f806b986da.gif&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;replyall mode&lt;/code&gt; the bot will reply to all messages in the server without using slash commands&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; The bot will easily be triggered in &lt;code&gt;replyall&lt;/code&gt; mode, which could cause program failures&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Setup&lt;/h1&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rename the file &lt;code&gt;.env.dev&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Step 1: Create a Discord bot&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to &lt;a href=&#34;https://discord.com/developers/applications&#34;&gt;https://discord.com/developers/applications&lt;/a&gt; create an application&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build a Discord bot under the application&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get the token from bot setting&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/89479282/205949161-4b508c6d-19a7-49b6-b8ed-7525ddbef430.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Store the token to &lt;code&gt;.env&lt;/code&gt; under the &lt;code&gt;DISCORD_BOT_TOKEN&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/89479282/217743218-26e3d999-44d5-4a0b-88e1-ee23f3ffd5d8.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Turn MESSAGE CONTENT INTENT &lt;code&gt;ON&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/89479282/205949323-4354bd7d-9bb9-4f4b-a87e-deb9933a89b5.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Invite your bot to your server via OAuth2 URL Generator&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/89479282/205949600-0c7ddb40-7e82-47a0-b59a-b089f929d177.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Step 2: Generate a OpenAI API key&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to &lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;https://beta.openai.com/account/api-keys&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click Create new secret key&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/89479282/207970699-2e0cb671-8636-4e27-b1f3-b75d6db9b57e.PNG&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Store the SECRET KEY to &lt;code&gt;.env&lt;/code&gt; under the &lt;code&gt;OPENAI_KEY&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Step 3: Run the bot on the desktop&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open a terminal or command prompt&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the directory where you installed the ChatGPT Discord bot&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python3 main.py&lt;/code&gt; to start the bot&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Step 3: Run the bot with Docker&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the Docker image &amp;amp; Run the Docker container &lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inspect whether the bot works well &lt;code&gt;docker logs -t chatgpt-discord-bot&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Stop the bot:&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;docker ps&lt;/code&gt; to see the list of running services&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;docker stop &amp;lt;BOT CONTAINER ID&amp;gt;&lt;/code&gt; to stop the running bot&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Have a good chat!&lt;/h3&gt; &#xA;&lt;h2&gt;Optional: Setup starting prompt&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;A starting prompt would be invoked when the bot is first started or reset&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can set it up by modifying the content in &lt;code&gt;starting-prompt.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;All the text in the file will be fired as a prompt to the bot&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get the first message from ChatGPT in your discord channel!&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt; &lt;p&gt;Right-click the channel you want to recieve the message, &lt;code&gt;Copy ID&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/89479282/207697217-e03357b3-3b3d-44d0-b880-163217ed4a49.PNG&#34; alt=&#34;channel-id&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;paste it into &lt;code&gt;.env&lt;/code&gt; under &lt;code&gt;DISCORD_CHANNEL_ID&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>haoheliu/AudioLDM</title>
    <updated>2023-02-12T02:03:10Z</updated>
    <id>tag:github.com,2023-02-12:/haoheliu/AudioLDM</id>
    <link href="https://github.com/haoheliu/AudioLDM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AudioLDM: Generate speech, sound effects, music and beyond, with text.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Text-to-Audio Generation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.12503&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2109.13731-brightgreen.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://audioldm.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub.io-Audio_Samples-blue?logo=Github&amp;amp;style=flat-square&#34; alt=&#34;githubio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1djoA4oPP1BEyhl6_pDaROSPoOCGnf9Xo?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/jagilley/audio-ldm&#34;&gt;&lt;img src=&#34;https://replicate.com/jagilley/audio-ldm/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- # [![PyPI version](https://badge.fury.io/py/voicefixer.svg)](https://badge.fury.io/py/voicefixer) --&gt; &#xA;&lt;p&gt;Generate speech, sound effects, music and beyond.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Important tricks to make your generated audio sound better&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Try to use more adjectives to describe your sound. For example: &#34;A man is speaking clearly and slowly in a professional studio&#34; is better than &#34;A man is speaking&#34;. This can make sure AudioLDM understand what you want.&lt;/li&gt; &#xA; &lt;li&gt;Try to use different random seeds, which can affect the generation quality significantly sometimes.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s best to use general terms like &#39;man&#39; or &#39;woman&#39; instead of specific names for individuals or abstract objects that humans may not be familiar with, such as &#39;mummy&#39;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Web APP&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare running environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n audioldm python=3.8; conda activate audioldm&#xA;pip3 install audioldm==0.0.6&#xA;git clone https://github.com/haoheliu/AudioLDM; cd AudioLDM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Start the web application (powered by Gradio)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;A link will be printed out. Click the link to open the browser and play.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Commandline Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare running environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Optional&#xA;conda create -n audioldm python=3.8; conda activate audioldm&#xA;# Install AudioLDM&#xA;pip3 install audioldm==0.0.6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;text-to-audio generation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test run&#xA;audioldm -t &#34;A hammer is hitting a wooden surface&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more options on guidance scale, batchsize, seed, etc, please run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audioldm -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the evaluation of audio generative model, please refer to &lt;a href=&#34;https://github.com/haoheliu/audioldm_eval&#34;&gt;audioldm_eval&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Web Demo&lt;/h1&gt; &#xA;&lt;p&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Hugging Face Spaces ðŸ¤—&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo &lt;a href=&#34;https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;TODO&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update the checkpoint with more training steps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add AudioCaps finetuned AudioLDM-S model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Build pip installable package for commandline use&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Build Gradio web application&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add text-guided style transfer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add audio super-resolution&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add audio inpainting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite this work&lt;/h2&gt; &#xA;&lt;p&gt;If you found this tool useful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023audioldm,&#xA;  title={AudioLDM: Text-to-Audio Generation with Latent Diffusion Models},&#xA;  author={Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D},&#xA;  journal={arXiv preprint arXiv:2301.12503},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Hardware requirement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU with 8GB of dedicated VRAM&lt;/li&gt; &#xA; &lt;li&gt;A system with a 64-bit operating system (Windows 7, 8.1 or 10, Ubuntu 16.04 or later, or macOS 10.13 or later) 16GB or more of system RAM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;Part of the code is borrowed from the following repos. We would like to thank the authors of these repos for their contribution.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/LAION-AI/CLAP&#34;&gt;https://github.com/LAION-AI/CLAP&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;https://github.com/CompVis/stable-diffusion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/v-iashin/SpecVQGAN&#34;&gt;https://github.com/v-iashin/SpecVQGAN&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/toshas/torch-fidelity&#34;&gt;https://github.com/toshas/torch-fidelity&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We build the model with data from AudioSet, Freesound and BBC Sound Effect library. We share this demo based on the UK copyright exception of data for academic research.&lt;/p&gt; &#xA;&lt;!-- This code repo is strictly for research demo purpose only. For commercial use please contact us. --&gt;</summary>
  </entry>
</feed>