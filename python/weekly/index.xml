<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-28T01:49:20Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>apple/ml-neuman</title>
    <updated>2022-08-28T01:49:20Z</updated>
    <id>tag:github.com,2022-08-28:/apple/ml-neuman</id>
    <link href="https://github.com/apple/ml-neuman" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository of NeuMan: Neural Human Radiance Field from a Single Video (ECCV 2022)&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;NeuMan: Neural Human Radiance Field from a Single Video&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a reference implementation for NeuMan. NeuMan reconstructs both the background scene and an animatable human from a single video using neural radiance fields.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.12575&#34;&gt;[Paper]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-neuman/main/resources/teaser.gif&#34; height=&#34;260&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Video demos&lt;/h3&gt; &#xA;&lt;p&gt;Novel view and novel pose synthesis&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/bike.mp4&#34;&gt;[Bike]&lt;/a&gt; &lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/citron.mp4&#34;&gt;[Citron]&lt;/a&gt; &lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/demo3.mp4&#34;&gt;[Parking lot]&lt;/a&gt; &lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/jogging.mp4&#34;&gt;[Jogging]&lt;/a&gt; &lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/lab.mp4&#34;&gt;[Lab]&lt;/a&gt; &lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/seattle.mp4&#34;&gt;[Seattle]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Compositional Synthesis&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/handshake.mp4&#34;&gt;[Handshake]&lt;/a&gt; &lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/dance.mp4&#34;&gt;[Dance]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Environment&lt;/h3&gt; &#xA;&lt;p&gt;To create the environment using Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternately, you can create the environment by executing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n neuman_env python=3.7 -y;&#xA;conda activate neuman_env;&#xA;conda install pytorch==1.8.0 torchvision==0.9.0 cudatoolkit=10.2 -c pytorch;&#xA;# For RTX 30 series GPU with CUDA version 11.x, please use:&#xA;# conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;conda install -c fvcore -c iopath -c conda-forge fvcore iopath;&#xA;conda install -c bottler nvidiacub;&#xA;conda install pytorch3d -c pytorch3d;&#xA;conda install -c conda-forge igl;&#xA;pip install opencv-python joblib open3d imageio tensorboardX chumpy lpips scikit-image ipython matplotlib;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice that &lt;code&gt;pytorch3d&lt;/code&gt; requires a specific version of pytorch, in our case &lt;code&gt;pytorch=1.8.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Activate the environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda activate neuman_env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download SMPL weights:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Registration is required to download the UV map(Download UV map in OBJ format) from &lt;a href=&#34;https://smpl.is.tue.mpg.de/download.php&#34;&gt;SMPL&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Download neutral SMPL weights(SMPLIFY_CODE_V2.ZIP) from &lt;a href=&#34;https://smplify.is.tue.mpg.de/download.php&#34;&gt;SMPLify&lt;/a&gt;, extract &lt;code&gt;basicModel_neutral_lbs_10_207_0_v1.0.0.pkl&lt;/code&gt; and rename it to &lt;code&gt;SMPL_NEUTRAL.pkl&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Put the all the downloaded files into &lt;code&gt;./data/smplx&lt;/code&gt; folder with following structure:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; .&#xA; └── data&#xA;  &amp;nbsp;&amp;nbsp; └── smplx&#xA;  &amp;nbsp;&amp;nbsp;     ├── smpl&#xA;  &amp;nbsp;&amp;nbsp;     │&amp;nbsp;&amp;nbsp; └── SMPL_NEUTRAL.pkl&#xA;  &amp;nbsp;&amp;nbsp;     └── smpl_uv.obj&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download NeuMan dataset and pretrained models:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Data (&lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/dataset.zip&#34;&gt;download&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Pretrained models (&lt;a href=&#34;https://docs-assets.developer.apple.com/ml-research/datasets/neuman/pretrained.zip&#34;&gt;download&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Alternately, run the following script to set up data and pretrained models.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash setup_data_and_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(&lt;em&gt;Optional&lt;/em&gt;) Download AMASS dataset for reposing:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;AMASS dataset is used for rendering novel poses, specifically &lt;code&gt;render_reposing.py&lt;/code&gt; and &lt;code&gt;render_gathering.py&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;We used SFU mocap(SMPL+H G) subset, please download from &lt;a href=&#34;https://amass.is.tue.mpg.de/download.php&#34;&gt;AMASS&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Put the downloaded mocap data in to &lt;code&gt;./data/SFU&lt;/code&gt; folder.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; .&#xA; └── data&#xA;  &amp;nbsp;&amp;nbsp; └── SFU&#xA;  &amp;nbsp;&amp;nbsp;     ├── 0005&#xA;  &amp;nbsp;&amp;nbsp;     ├── 0007&#xA;  &amp;nbsp;&amp;nbsp;     │   ...&#xA;  &amp;nbsp;&amp;nbsp;     └── 0018&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Render using pretrained model&lt;/p&gt; &lt;p&gt;Render 360 views of a canonical human:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python render_360.py --scene_dir ./data/bike --weights_path ./out/bike_human/checkpoint.pth.tar --mode canonical_360&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Render 360 views of a posed human:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python render_360.py --scene_dir ./data/bike --weights_path ./out/bike_human/checkpoint.pth.tar --mode posed_360&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Render test views of a sequence, and evaluate the metrics:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python render_test_views.py --scene_dir ./data/bike --weights_path ./out/bike_human/checkpoint.pth.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Render novel poses with the background:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python render_reposing.py --scene_dir ./data/bike --weights_path ./out/bike_human/checkpoint.pth.tar --motion_name=jumpandroll&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Render telegathering:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python render_gathering.py --actors parkinglot seattle citron --scene_dir ./data/seattle --weights_path ./out/seattle_human/checkpoint.pth.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download NeuMan dataset&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train scene NeRF&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python train.py --scene_dir ./data/bike/ --name=bike_background --train_mode=bkg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train human NeRF&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python train.py --scene_dir ./data/bike  --name=bike_human --load_background=bike_background --train_mode=smpl_and_offset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use your own video&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Preprocess: Check &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-neuman/main/preprocess/README.md&#34;&gt;preprocess&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{jiang2022neuman,&#xA;  title={NeuMan: Neural Human Radiance Field from a Single Video},&#xA;  author={Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag},&#xA;  booktitle={Proceedings of the European conference on computer vision (ECCV)},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;The code is released under the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-neuman/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; terms.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mouredev/Hello-Python</title>
    <updated>2022-08-28T01:49:20Z</updated>
    <id>tag:github.com,2022-08-28:/mouredev/Hello-Python</id>
    <link href="https://github.com/mouredev/Hello-Python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python desde cero&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Hello Python&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://python.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.9+-yellow?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Proyecto realizado durante las emisiones en directo desde Twitch para aprender Python desde cero&lt;/h2&gt; &#xA;&lt;h3&gt;🐍 CADA SEMANA UNA NUEVA CLASE EN DIRECTO 🐍&lt;/h3&gt; &#xA;&lt;h5&gt;Si consideras útil esta actividad, apóyala haciendo &#34;★ Star&#34; en el repositorio. ¡Gracias!&lt;/h5&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;strong&gt;🔴 PRÓXIMA CLASE: Miércoles 31 de Agosto a las 20:00 (hora España)&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;En &lt;a href=&#34;https://discord.gg/B4SGGAQqKZ?event=1012325339160125491&#34;&gt;Discord&lt;/a&gt; tienes creado un evento para que consultes la hora de tu país y añadas un recordatorio.&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;Finalizada la clase, se actualizará el repositorio con los nuevos recursos&lt;/em&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Clases en vídeo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clase 1 (03/08/22): &lt;a href=&#34;https://www.twitch.tv/videos/1551265068&#34;&gt;Vídeo en Twitch con la clase completa&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Contexto, instalación, configuración, hola mundo y variables.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Clase 2 (10/08/22): &lt;a href=&#34;https://www.twitch.tv/videos/1558018826&#34;&gt;Vídeo en Twitch con la clase completa&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Operadores y Strings.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Clase 3 (17/08/22): &lt;a href=&#34;https://www.twitch.tv/videos/1564719056&#34;&gt;Vídeo en Twitch con la clase completa&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Listas y tuplas.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Clase 4 (24/08/22): &lt;a href=&#34;https://www.twitch.tv/videos/1571410092&#34;&gt;Vídeo en Twitch con la clase completa&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sets y diccionarios.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Información importante y preguntas frecuentes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;¿Las clases quedan grabadas?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Todos los directos de Twitch están disponibles 60 días en la sección &lt;a href=&#34;https://twitch.tv/mouredev/videos&#34;&gt;vídeos&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;¿Se subirá a YouTube?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;No te preocupes, antes de que se cumplan los 60 días de Twitch, iré publicando las clases en YouTube.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;¿Harás un curso?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Si la serie en Twitch recibe apoyo puedo plantearme crear algún vídeo de iniciación resumiendo los conceptos en YouTube.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;¿Hasta dónde llegará el curso?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Mi idea es repasar los conceptos básicos hasta llegar a crear un backend (en principio).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;¿Cuándo será la próxima clase?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Te recomiendo que me sigas en redes, sobre todo en &lt;a href=&#34;https://discord.gg/mouredev&#34;&gt;Discord&lt;/a&gt; e &lt;a href=&#34;https://instagram.com/mouredev&#34;&gt;Instagram&lt;/a&gt; donde creo eventos a diario con la hora de emisión (así podrás ver qué hora es en tu país).&lt;/li&gt; &#xA;   &lt;li&gt;También he creado en el Discord un canal &#34;Python&#34; para que puedas comentar lo que quieras.&lt;/li&gt; &#xA;   &lt;li&gt;Una vez finalizada la clase subiré los ficheros de código a este repositorio.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Enlaces de interés&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Web oficial de Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.python.org/es/3/tutorial/index.html&#34;&gt;Tutorial oficial de Python en Español&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Asabeneh/30-Days-Of-Python&#34;&gt;Repo 30 días de Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.codedex.io/&#34;&gt;Juego Codédex para aprender Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;Visual Studio Code&lt;/a&gt;: El editor que estoy usando&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Puedes apoyar mi trabajo haciendo &#34;☆ Star&#34; en el repo o nominarme a &#34;GitHub Star&#34;. ¡Gracias!&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://stars.github.com/nominate/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-Nominar_a_star-yellow?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;GitHub Star&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Si quieres unirte a nuestra comunidad de desarrollo, aprender programación de Apps, mejorar tus habilidades y ayudar a la continuidad del proyecto, puedes encontrarnos en:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitch-Programaci%C3%B3n_en_directo-9146FF?style=for-the-badge&amp;amp;logo=twitch&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Twitch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mouredev.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Servidor_de_la_comunidad-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mouredev.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Links_de_inter%C3%A9s-moure.dev-39E09B?style=for-the-badge&amp;amp;logo=Linktree&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mouredev/mouredev/master/mouredev_emote.png&#34; alt=&#34;https://mouredev.com&#34;&gt; Hola, mi nombre es Brais Moure.&lt;/h2&gt; &#xA;&lt;h3&gt;Freelance full-stack iOS &amp;amp; Android engineer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtube.com/mouredevapps?sub_confirmation=1&#34;&gt;&lt;img src=&#34;https://img.shields.io/youtube/channel/subscribers/UCxPD7bsocoAMq8Dj18kmGyQ?style=social&#34; alt=&#34;YouTube Channel Subscribers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitch.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitch/status/mouredev?style=social&#34; alt=&#34;Twitch Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mouredev.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/729672926432985098?style=social&amp;amp;label=Discord&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/mouredev?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/followers/mouredev?style=social&#34; alt=&#34;GitHub Followers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/mouredev?style=social&#34; alt=&#34;GitHub Followers&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Soy ingeniero de software desde hace más de 12 años. Desde hace 4 años combino mi trabajo desarrollando Apps con creación de contenido formativo sobre programación y tecnología en diferentes redes sociales como &lt;strong&gt;&lt;a href=&#34;https://moure.dev&#34;&gt;@mouredev&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;En mi perfil de GitHub tienes más información&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-MoureDev-14a1f0?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Web&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/diffusers</title>
    <updated>2022-08-28T01:49:20Z</updated>
    <id>tag:github.com,2022-08-28:/huggingface/diffusers</id>
    <link href="https://github.com/huggingface/diffusers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🤗 Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/imgs/diffusers_library.jpg&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/huggingface/datasets.svg?color=blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/diffusers/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/huggingface/diffusers.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/diffusers/main/CODE_OF_CONDUCT.md&#34;&gt; &lt;img alt=&#34;Contributor Covenant&#34; src=&#34;https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;🤗 Diffusers provides pretrained diffusion models across multiple modalities, such as vision and audio, and serves as a modular toolbox for inference and training of diffusion models.&lt;/p&gt; &#xA;&lt;p&gt;More precisely, 🤗 Diffusers offers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;State-of-the-art diffusion pipelines that can be run in inference with just a couple of lines of code (see &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines&#34;&gt;src/diffusers/pipelines&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Various noise schedulers that can be used interchangeably for the prefered speed vs. quality trade-off in inference (see &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers&#34;&gt;src/diffusers/schedulers&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Multiple types of models, such as UNet, can be used as building blocks in an end-to-end diffusion system (see &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/models&#34;&gt;src/diffusers/models&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Training examples to show how to train the most popular diffusion models (see &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples/training&#34;&gt;examples/training&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Inference examples to show how to create custom pipelines for advanced tasks such as image2image, in-painting (see &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples/inference&#34;&gt;examples/inference&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;In order to get started, we recommend taking a look at two notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb&#34;&gt;Getting started with Diffusers&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; notebook, which showcases an end-to-end example of usage for diffusion models, schedulers and pipelines. Take a look at this notebook to learn how to use the pipeline abstraction, which takes care of everything (model, scheduler, noise handling) for you, and also to understand each independent building block in the library.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb&#34;&gt;Training a diffusers model&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; notebook summarizes diffuser model training methods. This notebook takes a step-by-step approach to training your diffuser model on an image dataset, with explanatory graphics.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;strong&gt;New 🎨🎨🎨&lt;/strong&gt; Stable Diffusion is now fully compatible with &lt;code&gt;diffusers&lt;/code&gt;!&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from &lt;a href=&#34;https://github.com/CompVis&#34;&gt;CompVis&lt;/a&gt;, &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;. It&#39;s trained on 512x512 images from a subset of the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-3&#34;&gt;model card&lt;/a&gt;, read the license and tick the checkbox if you agree. You have to be a registered user in 🤗 Hugging Face Hub, and you&#39;ll also need to use an access token for the code to work. For more information on access tokens, please refer to &lt;a href=&#34;https://huggingface.co/docs/hub/security-tokens&#34;&gt;this section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# make sure you&#39;re logged in with `huggingface-cli login`&#xA;from torch import autocast&#xA;from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler&#xA;&#xA;lms = LMSDiscreteScheduler(&#xA;    beta_start=0.00085, &#xA;    beta_end=0.012, &#xA;    beta_schedule=&#34;scaled_linear&#34;&#xA;)&#xA;&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#xA;    &#34;CompVis/stable-diffusion-v1-3&#34;, &#xA;    scheduler=lms,&#xA;    use_auth_token=True&#xA;).to(&#34;cuda&#34;)&#xA;&#xA;prompt = &#34;a photo of an astronaut riding a horse on mars&#34;&#xA;with autocast(&#34;cuda&#34;):&#xA;    image = pipe(prompt)[&#34;sample&#34;][0]  &#xA;    &#xA;image.save(&#34;astronaut_rides_horse.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details, check out &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb&#34;&gt;the Stable Diffusion notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; and have a look into the &lt;a href=&#34;https://github.com/huggingface/diffusers/releases/tag/v0.2.0&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;There are many ways to try running Diffusers! Here we outline code-focused tools (primarily using &lt;code&gt;DiffusionPipeline&lt;/code&gt;s and Google Colab) and interactive web-tools.&lt;/p&gt; &#xA;&lt;h3&gt;Running Code&lt;/h3&gt; &#xA;&lt;p&gt;If you want to run the code yourself 💻, you can try out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/CompVis/ldm-text2im-large-256&#34;&gt;Text-to-Image Latent Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# !pip install diffusers transformers&#xA;from diffusers import DiffusionPipeline&#xA;&#xA;model_id = &#34;CompVis/ldm-text2im-large-256&#34;&#xA;&#xA;# load model and scheduler&#xA;ldm = DiffusionPipeline.from_pretrained(model_id)&#xA;&#xA;# run pipeline in inference (sample random noise and denoise)&#xA;prompt = &#34;A painting of a squirrel eating a burger&#34;&#xA;images = ldm([prompt], num_inference_steps=50, eta=0.3, guidance_scale=6)[&#34;sample&#34;]&#xA;&#xA;# save images&#xA;for idx, image in enumerate(images):&#xA;    image.save(f&#34;squirrel-{idx}.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/ddpm-celebahq-256&#34;&gt;Unconditional Diffusion with discrete scheduler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# !pip install diffusers&#xA;from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline&#xA;&#xA;model_id = &#34;google/ddpm-celebahq-256&#34;&#xA;&#xA;# load model and scheduler&#xA;ddpm = DDPMPipeline.from_pretrained(model_id)  # you can replace DDPMPipeline with DDIMPipeline or PNDMPipeline for faster inference&#xA;&#xA;# run pipeline in inference (sample random noise and denoise)&#xA;image = ddpm()[&#34;sample&#34;]&#xA;&#xA;# save image&#xA;image[0].save(&#34;ddpm_generated_image.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/CompVis/ldm-celebahq-256&#34;&gt;Unconditional Latent Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/ncsnpp-ffhq-1024&#34;&gt;Unconditional Diffusion with continous scheduler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Other Notebooks&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/image_2_image_using_diffusers.ipynb&#34;&gt;image-to-image generation with Stable Diffusion&lt;/a&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb&#34;&gt;tweak images via repeated Stable Diffusion seeds&lt;/a&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;,&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Web Demos&lt;/h3&gt; &#xA;&lt;p&gt;If you just want to play around with some web demos, you can try out the following 🚀 Spaces:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-to-Image Latent Diffusion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/CompVis/text2img-latent-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Faces generator&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/CompVis/celeba-latent-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DDPM with different schedulers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/fusing/celeba-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Conditional generation from sketch (&lt;em&gt;SOON&lt;/em&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/huggingface/diffuse-the-rest&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Composable diffusion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/Shuang59/Composable-Diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Definitions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt;: Neural network that models $p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)$ (see image below) and is trained end-to-end to &lt;em&gt;denoise&lt;/em&gt; a noisy input to an image. &lt;em&gt;Examples&lt;/em&gt;: UNet, Conditioned UNet, 3D UNet, Transformer UNet&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/10695622/174349667-04e9e485-793b-429a-affe-096e8199ad5b.png&#34; width=&#34;800&#34;&gt; &lt;br&gt; &lt;em&gt; Figure from DDPM paper (https://arxiv.org/abs/2006.11239). &lt;/em&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Schedulers&lt;/strong&gt;: Algorithm class for both &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;training&lt;/strong&gt;. The class provides functionality to compute previous image according to alpha, beta schedule as well as predict noise for training. &lt;em&gt;Examples&lt;/em&gt;: &lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;DDPM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.02502&#34;&gt;DDIM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PNDM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2204.13902&#34;&gt;DEIS&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/10695622/174349706-53d58acc-a4d1-4cda-b3e8-432d9dc7ad38.png&#34; width=&#34;800&#34;&gt; &lt;br&gt; &lt;em&gt; Sampling and training algorithms. Figure from DDPM paper (https://arxiv.org/abs/2006.11239). &lt;/em&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Diffusion Pipeline&lt;/strong&gt;: End-to-end pipeline that includes multiple diffusion models, possible text encoders, ... &lt;em&gt;Examples&lt;/em&gt;: Glide, Latent-Diffusion, Imagen, DALL-E 2&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/10695622/174348898-481bd7c2-5457-4830-89bc-f0907756f64c.jpeg&#34; width=&#34;550&#34;&gt; &lt;br&gt; &lt;em&gt; Figure from ImageGen (https://imagen.research.google/). &lt;/em&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Philosophy&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Readability and clarity is prefered over highly optimized code. A strong importance is put on providing readable, intuitive and elementary code design. &lt;em&gt;E.g.&lt;/em&gt;, the provided &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers&#34;&gt;schedulers&lt;/a&gt; are separated from the provided &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/models&#34;&gt;models&lt;/a&gt; and provide well-commented code that can be read alongside the original paper.&lt;/li&gt; &#xA; &lt;li&gt;Diffusers is &lt;strong&gt;modality independent&lt;/strong&gt; and focuses on providing pretrained models and tools to build systems that generate &lt;strong&gt;continous outputs&lt;/strong&gt;, &lt;em&gt;e.g.&lt;/em&gt; vision and audio.&lt;/li&gt; &#xA; &lt;li&gt;Diffusion models and schedulers are provided as concise, elementary building blocks. In contrast, diffusion pipelines are a collection of end-to-end diffusion systems that can be used out-of-the-box, should stay as close as possible to their original implementation and can include components of another library, such as text-encoders. Examples for diffusion pipelines are &lt;a href=&#34;https://github.com/openai/glide-text2im&#34;&gt;Glide&lt;/a&gt; and &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Latent Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade diffusers  # should install diffusers 0.2.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda install -c conda-forge diffusers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;In the works&lt;/h2&gt; &#xA;&lt;p&gt;For the first release, 🤗 Diffusers focuses on text-to-image diffusion techniques. However, diffusers can be used for much more than that! Over the upcoming releases, we&#39;ll be focusing on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Diffusers for audio&lt;/li&gt; &#xA; &lt;li&gt;Diffusers for reinforcement learning (initial work happening in &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/105&#34;&gt;https://github.com/huggingface/diffusers/pull/105&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Diffusers for video generation&lt;/li&gt; &#xA; &lt;li&gt;Diffusers for molecule generation (initial work happening in &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/54&#34;&gt;https://github.com/huggingface/diffusers/pull/54&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A few pipeline components are already being worked on, namely:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BDDMPipeline for spectrogram-to-sound vocoding&lt;/li&gt; &#xA; &lt;li&gt;GLIDEPipeline to support OpenAI&#39;s GLIDE model&lt;/li&gt; &#xA; &lt;li&gt;Grad-TTS for text to audio generation / conditional audio generation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We want diffusers to be a toolbox useful for diffusers models in general; if you find yourself limited in any way by the current API, or would like to see additional models, schedulers, or techniques, please open a &lt;a href=&#34;https://github.com/huggingface/diffusers/issues&#34;&gt;GitHub issue&lt;/a&gt; mentioning what you would like to see.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We&#39;d like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;@CompVis&#39; latent diffusion models library, available &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@hojonathanho original DDPM implementation, available &lt;a href=&#34;https://github.com/hojonathanho/diffusion&#34;&gt;here&lt;/a&gt; as well as the extremely useful translation into PyTorch by @pesser, available &lt;a href=&#34;https://github.com/pesser/pytorch_diffusion&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@ermongroup&#39;s DDIM implementation, available &lt;a href=&#34;https://github.com/ermongroup/ddim&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;@yang-song&#39;s Score-VE and Score-VP implementations, available &lt;a href=&#34;https://github.com/yang-song/score_sde_pytorch&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available &lt;a href=&#34;https://github.com/heejkoo/Awesome-Diffusion-Models&#34;&gt;here&lt;/a&gt; as well as @crowsonkb and @rromb for useful discussions and insights.&lt;/p&gt;</summary>
  </entry>
</feed>