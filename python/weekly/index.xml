<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-08T01:45:40Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>topoteretes/cognee</title>
    <updated>2025-06-08T01:45:40Z</updated>
    <id>tag:github.com,2025-06-08:/topoteretes/cognee</id>
    <link href="https://github.com/topoteretes/cognee" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Memory for AI Agents in 5 lines of code&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/topoteretes/cognee&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png&#34; alt=&#34;Cognee Logo&#34; height=&#34;60&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;cognee - Memory for AI Agents in 5 lines of code&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=1bezuvLwJmw&amp;amp;t=2s&#34;&gt;Demo&lt;/a&gt; . &lt;a href=&#34;https://cognee.ai&#34;&gt;Learn more&lt;/a&gt; · &lt;a href=&#34;https://discord.gg/NQPKmU5CCg&#34;&gt;Join Discord&lt;/a&gt; · &lt;a href=&#34;https://www.reddit.com/r/AIMemory/&#34;&gt;Join r/AIMemory&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://GitHub.com/topoteretes/cognee/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&amp;amp;label=Fork&amp;amp;maxAge=2592000&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/topoteretes/cognee/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&amp;amp;label=Star&amp;amp;maxAge=2592000&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/topoteretes/cognee/commit/&#34;&gt;&lt;img src=&#34;https://badgen.net/github/commits/topoteretes/cognee&#34; alt=&#34;GitHub commits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/tags/&#34;&gt;&lt;img src=&#34;https://badgen.net/github/tag/topoteretes/cognee&#34; alt=&#34;Github tag&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/cognee&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/cognee&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/topoteretes/cognee/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.producthunt.com/posts/cognee?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-cognee&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&amp;amp;theme=light&amp;amp;period=daily&amp;amp;t=1744472480704&#34; alt=&#34;cognee - Memory for AI Agents  in 5 lines of code | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/13955&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/13955&#34; alt=&#34;topoteretes%2Fcognee | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Build dynamic memory for Agents and replace RAG using scalable, modular ECL (Extract, Cognify, Load) pipelines.&lt;/p&gt; &#xA; &lt;p&gt;More on &lt;a href=&#34;https://docs.cognee.ai/use-cases&#34;&gt;use-cases&lt;/a&gt; and &lt;a href=&#34;https://github.com/topoteretes/cognee/tree/main/evals&#34;&gt;evals&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; 🌐 Available Languages : &lt;a href=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/community/README.pt.md&#34;&gt;🇵🇹 Português&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/community/README.zh.md&#34;&gt;🇨🇳 [中文]&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/community/README.ru.md&#34;&gt;🇷🇺 Русский&lt;/a&gt; &lt;/p&gt; &#xA; &lt;div style=&#34;text-align: center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png&#34; alt=&#34;Why cognee?&#34; width=&#34;50%&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Interconnect and retrieve your past conversations, documents, images and audio transcriptions&lt;/li&gt; &#xA; &lt;li&gt;Replaces RAG systems and reduces developer effort, and cost.&lt;/li&gt; &#xA; &lt;li&gt;Load data to graph and vector databases using only Pydantic&lt;/li&gt; &#xA; &lt;li&gt;Manipulate your data while ingesting from 30+ data sources&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Get started quickly with a Google Colab &lt;a href=&#34;https://colab.research.google.com/drive/1jHbWVypDgCLwjE71GSXhRL3YxYhCZzG1?usp=sharing&#34;&gt;notebook&lt;/a&gt; , &lt;a href=&#34;https://deepnote.com/workspace/cognee-382213d0-0444-4c89-8265-13770e333c02/project/cognee-demo-78ffacb9-5832-4611-bb1a-560386068b30/notebook/Notebook-1-75b24cda566d4c24ab348f7150792601?utm_source=share-modal&amp;amp;utm_medium=product-shared-content&amp;amp;utm_campaign=notebook&amp;amp;utm_content=78ffacb9-5832-4611-bb1a-560386068b30&#34;&gt;Deepnote notebook&lt;/a&gt; or &lt;a href=&#34;https://github.com/topoteretes/cognee-starter&#34;&gt;starter repo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Your contributions are at the core of making this a true open source project. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;📦 Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install Cognee using either &lt;strong&gt;pip&lt;/strong&gt;, &lt;strong&gt;poetry&lt;/strong&gt;, &lt;strong&gt;uv&lt;/strong&gt; or any other python package manager. Cognee supports Python 3.8 to 3.12&lt;/p&gt; &#xA;&lt;h3&gt;With pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install cognee&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Local Cognee installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install the local Cognee repo using &lt;strong&gt;pip&lt;/strong&gt;, &lt;strong&gt;poetry&lt;/strong&gt; and &lt;strong&gt;uv&lt;/strong&gt;. For local pip installation please make sure your pip version is above version 21.3.&lt;/p&gt; &#xA;&lt;h3&gt;with UV with all optional dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv sync --all-extras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;💻 Basic Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;os.environ[&#34;LLM_API_KEY&#34;] = &#34;YOUR OPENAI_API_KEY&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also set the variables by creating .env file, using our &lt;a href=&#34;https://github.com/topoteretes/cognee/raw/main/.env.template&#34;&gt;template.&lt;/a&gt; To use different LLM providers, for more info check out our &lt;a href=&#34;https://docs.cognee.ai&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Simple example&lt;/h3&gt; &#xA;&lt;p&gt;This script will run the default pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cognee&#xA;import asyncio&#xA;&#xA;&#xA;async def main():&#xA;    # Add text to cognee&#xA;    await cognee.add(&#34;Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval.&#34;)&#xA;&#xA;    # Generate the knowledge graph&#xA;    await cognee.cognify()&#xA;&#xA;    # Query the knowledge graph&#xA;    results = await cognee.search(&#34;Tell me about NLP&#34;)&#xA;&#xA;    # Display the results&#xA;    for result in results:&#xA;        print(result)&#xA;&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    asyncio.run(main())&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  Natural Language Processing (NLP) is a cross-disciplinary and interdisciplinary field that involves computer science and information retrieval. It focuses on the interaction between computers and human language, enabling machines to understand and process natural language.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Our paper is out! &lt;a href=&#34;https://arxiv.org/abs/2505.24478&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Read here&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;div style=&#34;text-align: center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/cognee-paper.png&#34; alt=&#34;cognee paper&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/div&gt;  &#xA;&lt;h2&gt;Cognee UI&lt;/h2&gt; &#xA;&lt;p&gt;You can also cognify your files and query using cognee UI.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/cognee-ui-2.webp&#34; width=&#34;100%&#34; alt=&#34;Cognee UI 2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try cognee UI out locally &lt;a href=&#34;https://docs.cognee.ai/how-to-guides/cognee-ui&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Understand our architecture&lt;/h2&gt; &#xA;&lt;div style=&#34;text-align: center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/topoteretes/cognee/main/assets/cognee_diagram.png&#34; alt=&#34;cognee concept diagram&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;What is AI memory:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0&#34;&gt;Learn about cognee&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Simple GraphRAG demo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f&#34;&gt;Simple GraphRAG demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;cognee with Ollama&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db&#34;&gt;cognee with local models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;We are committed to making open source an enjoyable and respectful experience for our community. See &lt;a href=&#34;https://github.com/topoteretes/cognee/raw/main/CODE_OF_CONDUCT.md&#34;&gt;&lt;code&gt;CODE_OF_CONDUCT&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;💫 Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/topoteretes/cognee/graphs/contributors&#34;&gt; &lt;img alt=&#34;contributors&#34; src=&#34;https://contrib.rocks/image?repo=topoteretes/cognee&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#topoteretes/cognee&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=topoteretes/cognee&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>scrapy/scrapy</title>
    <updated>2025-06-08T01:45:40Z</updated>
    <id>tag:github.com,2025-06-08:/scrapy/scrapy</id>
    <link href="https://github.com/scrapy/scrapy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scrapy, a fast high-level web crawling &amp; scraping framework for Python.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|logo|&lt;/p&gt; &#xA;&lt;p&gt;.. |logo| image:: &lt;a href=&#34;https://raw.githubusercontent.com/scrapy/scrapy/master/docs/_static/logo.svg&#34;&gt;https://raw.githubusercontent.com/scrapy/scrapy/master/docs/_static/logo.svg&lt;/a&gt; :target: &lt;a href=&#34;https://scrapy.org&#34;&gt;https://scrapy.org&lt;/a&gt; :alt: Scrapy :width: 480px&lt;/p&gt; &#xA;&lt;p&gt;|version| |python_version| |ubuntu| |macos| |windows| |coverage| |conda| |deepwiki|&lt;/p&gt; &#xA;&lt;p&gt;.. |version| image:: &lt;a href=&#34;https://img.shields.io/pypi/v/Scrapy.svg&#34;&gt;https://img.shields.io/pypi/v/Scrapy.svg&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/pypi/Scrapy&#34;&gt;https://pypi.org/pypi/Scrapy&lt;/a&gt; :alt: PyPI Version&lt;/p&gt; &#xA;&lt;p&gt;.. |python_version| image:: &lt;a href=&#34;https://img.shields.io/pypi/pyversions/Scrapy.svg&#34;&gt;https://img.shields.io/pypi/pyversions/Scrapy.svg&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/pypi/Scrapy&#34;&gt;https://pypi.org/pypi/Scrapy&lt;/a&gt; :alt: Supported Python Versions&lt;/p&gt; &#xA;&lt;p&gt;.. |ubuntu| image:: &lt;a href=&#34;https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg&#34;&gt;https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu&#34;&gt;https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu&lt;/a&gt; :alt: Ubuntu&lt;/p&gt; &#xA;&lt;p&gt;.. |macos| image:: &lt;a href=&#34;https://github.com/scrapy/scrapy/workflows/macOS/badge.svg&#34;&gt;https://github.com/scrapy/scrapy/workflows/macOS/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/scrapy/scrapy/actions?query=workflow%3AmacOS&#34;&gt;https://github.com/scrapy/scrapy/actions?query=workflow%3AmacOS&lt;/a&gt; :alt: macOS&lt;/p&gt; &#xA;&lt;p&gt;.. |windows| image:: &lt;a href=&#34;https://github.com/scrapy/scrapy/workflows/Windows/badge.svg&#34;&gt;https://github.com/scrapy/scrapy/workflows/Windows/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows&#34;&gt;https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows&lt;/a&gt; :alt: Windows&lt;/p&gt; &#xA;&lt;p&gt;.. |coverage| image:: &lt;a href=&#34;https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg&#34;&gt;https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg&lt;/a&gt; :target: &lt;a href=&#34;https://codecov.io/github/scrapy/scrapy?branch=master&#34;&gt;https://codecov.io/github/scrapy/scrapy?branch=master&lt;/a&gt; :alt: Coverage report&lt;/p&gt; &#xA;&lt;p&gt;.. |conda| image:: &lt;a href=&#34;https://anaconda.org/conda-forge/scrapy/badges/version.svg&#34;&gt;https://anaconda.org/conda-forge/scrapy/badges/version.svg&lt;/a&gt; :target: &lt;a href=&#34;https://anaconda.org/conda-forge/scrapy&#34;&gt;https://anaconda.org/conda-forge/scrapy&lt;/a&gt; :alt: Conda Version&lt;/p&gt; &#xA;&lt;p&gt;.. |deepwiki| image:: &lt;a href=&#34;https://deepwiki.com/badge.svg&#34;&gt;https://deepwiki.com/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://deepwiki.com/scrapy/scrapy&#34;&gt;https://deepwiki.com/scrapy/scrapy&lt;/a&gt; :alt: Ask DeepWiki&lt;/p&gt; &#xA;&lt;p&gt;Scrapy_ is a web scraping framework to extract structured data from websites. It is cross-platform, and requires Python 3.9+. It is maintained by Zyte_ (formerly Scrapinghub) and &lt;code&gt;many other contributors&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;.. _many other contributors: &lt;a href=&#34;https://github.com/scrapy/scrapy/graphs/contributors&#34;&gt;https://github.com/scrapy/scrapy/graphs/contributors&lt;/a&gt; .. _Scrapy: &lt;a href=&#34;https://scrapy.org/&#34;&gt;https://scrapy.org/&lt;/a&gt; .. _Zyte: &lt;a href=&#34;https://www.zyte.com/&#34;&gt;https://www.zyte.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install with:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install scrapy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And follow the documentation_ to learn how to use it.&lt;/p&gt; &#xA;&lt;p&gt;.. _documentation: &lt;a href=&#34;https://docs.scrapy.org/en/latest/&#34;&gt;https://docs.scrapy.org/en/latest/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you wish to contribute, see Contributing_.&lt;/p&gt; &#xA;&lt;p&gt;.. _Contributing: &lt;a href=&#34;https://docs.scrapy.org/en/master/contributing.html&#34;&gt;https://docs.scrapy.org/en/master/contributing.html&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>coleam00/local-ai-packaged</title>
    <updated>2025-06-08T01:45:40Z</updated>
    <id>tag:github.com,2025-06-08:/coleam00/local-ai-packaged</id>
    <link href="https://github.com/coleam00/local-ai-packaged" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run all your local AI together in one package - Ollama, Supabase, n8n, Open WebUI, and more!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Self-hosted AI Package&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-hosted AI Package&lt;/strong&gt; is an open, docker compose template that quickly bootstraps a fully featured Local AI and Low Code development environment including Ollama for your local LLMs, Open WebUI for an interface to chat with your N8N agents, and Supabase for your database, vector store, and authentication.&lt;/p&gt; &#xA;&lt;p&gt;This is Cole&#39;s version with a couple of improvements and the addition of Supabase, Open WebUI, Flowise, Neo4j, Langfuse, SearXNG, and Caddy! Also, the local RAG AI Agent workflows from the video will be automatically in your n8n instance if you use this setup instead of the base one provided by n8n!&lt;/p&gt; &#xA;&lt;h2&gt;Important Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://thinktank.ottomator.ai/c/local-ai/18&#34;&gt;Local AI community&lt;/a&gt; forum over in the oTTomator Think Tank&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/users/coleam00/projects/2/views/1&#34;&gt;GitHub Kanban board&lt;/a&gt; for feature implementation and bug squashing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/n8n-io/self-hosted-ai-starter-kit&#34;&gt;Original Local AI Starter Kit&lt;/a&gt; by the n8n team&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download my N8N + OpenWebUI integration &lt;a href=&#34;https://openwebui.com/f/coleam/n8n_pipe/&#34;&gt;directly on the Open WebUI site.&lt;/a&gt; (more instructions below)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/assets/n8n-demo.gif&#34; alt=&#34;n8n.io - Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Curated by &lt;a href=&#34;https://github.com/n8n-io&#34;&gt;https://github.com/n8n-io&lt;/a&gt; and &lt;a href=&#34;https://github.com/coleam00&#34;&gt;https://github.com/coleam00&lt;/a&gt;, it combines the self-hosted n8n platform with a curated list of compatible AI products and components to quickly get started with building self-hosted AI workflows.&lt;/p&gt; &#xA;&lt;h3&gt;What’s included&lt;/h3&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://n8n.io/&#34;&gt;&lt;strong&gt;Self-hosted n8n&lt;/strong&gt;&lt;/a&gt; - Low-code platform with over 400 integrations and advanced AI components&lt;/p&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://supabase.com/&#34;&gt;&lt;strong&gt;Supabase&lt;/strong&gt;&lt;/a&gt; - Open source database as a service - most widely used database for AI agents&lt;/p&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://ollama.com/&#34;&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/a&gt; - Cross-platform LLM platform to install and run the latest local LLMs&lt;/p&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://openwebui.com/&#34;&gt;&lt;strong&gt;Open WebUI&lt;/strong&gt;&lt;/a&gt; - ChatGPT-like interface to privately interact with your local models and N8N agents&lt;/p&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://flowiseai.com/&#34;&gt;&lt;strong&gt;Flowise&lt;/strong&gt;&lt;/a&gt; - No/low code AI agent builder that pairs very well with n8n&lt;/p&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://qdrant.tech/&#34;&gt;&lt;strong&gt;Qdrant&lt;/strong&gt;&lt;/a&gt; - Open source, high performance vector store with an comprehensive API. Even though you can use Supabase for RAG, this was kept unlike Postgres since it&#39;s faster than Supabase so sometimes is the better option.&lt;/p&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://neo4j.com/&#34;&gt;&lt;strong&gt;Neo4j&lt;/strong&gt;&lt;/a&gt; - Knowledge graph engine that powers tools like GraphRAG, LightRAG, and Graphiti&lt;/p&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://searxng.org/&#34;&gt;&lt;strong&gt;SearXNG&lt;/strong&gt;&lt;/a&gt; - Open source, free internet metasearch engine which aggregates results from up to 229 search services. Users are neither tracked nor profiled, hence the fit with the local AI package.&lt;/p&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://caddyserver.com/&#34;&gt;&lt;strong&gt;Caddy&lt;/strong&gt;&lt;/a&gt; - Managed HTTPS/TLS for custom domains&lt;/p&gt; &#xA;&lt;p&gt;✅ &lt;a href=&#34;https://langfuse.com/&#34;&gt;&lt;strong&gt;Langfuse&lt;/strong&gt;&lt;/a&gt; - Open source LLM engineering platform for agent observability&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before you begin, make sure you have the following software installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt; - Required to run the setup script&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://desktop.github.com/&#34;&gt;Git/GitHub Desktop&lt;/a&gt; - For easy repository management&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;Docker/Docker Desktop&lt;/a&gt; - Required to run all services&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository and navigate to the project directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone -b stable https://github.com/coleam00/local-ai-packaged.git&#xA;cd local-ai-packaged&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Before running the services, you need to set up your environment variables for Supabase following their &lt;a href=&#34;https://supabase.com/docs/guides/self-hosting/docker#securing-your-services&#34;&gt;self-hosting guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make a copy of &lt;code&gt;.env.example&lt;/code&gt; and rename it to &lt;code&gt;.env&lt;/code&gt; in the root directory of the project&lt;/li&gt; &#xA; &lt;li&gt;Set the following required environment variables: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;############&#xA;# N8N Configuration&#xA;############&#xA;N8N_ENCRYPTION_KEY=&#xA;N8N_USER_MANAGEMENT_JWT_SECRET=&#xA;&#xA;############&#xA;# Supabase Secrets&#xA;############&#xA;POSTGRES_PASSWORD=&#xA;JWT_SECRET=&#xA;ANON_KEY=&#xA;SERVICE_ROLE_KEY=&#xA;DASHBOARD_USERNAME=&#xA;DASHBOARD_PASSWORD=&#xA;POOLER_TENANT_ID=&#xA;&#xA;############&#xA;# Neo4j Secrets&#xA;############   &#xA;NEO4J_AUTH=&#xA;&#xA;############&#xA;# Langfuse credentials&#xA;############&#xA;&#xA;CLICKHOUSE_PASSWORD=&#xA;MINIO_ROOT_PASSWORD=&#xA;LANGFUSE_SALT=&#xA;NEXTAUTH_SECRET=&#xA;ENCRYPTION_KEY=  &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Make sure to generate secure random values for all secrets. Never use the example values in production.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Set the following environment variables if deploying to production, otherwise leave commented: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;############&#xA;# Caddy Config&#xA;############&#xA;&#xA;N8N_HOSTNAME=n8n.yourdomain.com&#xA;WEBUI_HOSTNAME=:openwebui.yourdomain.com&#xA;FLOWISE_HOSTNAME=:flowise.yourdomain.com&#xA;SUPABASE_HOSTNAME=:supabase.yourdomain.com&#xA;OLLAMA_HOSTNAME=:ollama.yourdomain.com&#xA;SEARXNG_HOSTNAME=searxng.yourdomain.com&#xA;NEO4J_HOSTNAME=neo4j.yourdomain.com&#xA;LETSENCRYPT_EMAIL=your-email-address&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The project includes a &lt;code&gt;start_services.py&lt;/code&gt; script that handles starting both the Supabase and local AI services. The script accepts a &lt;code&gt;--profile&lt;/code&gt; flag to specify which GPU configuration to use.&lt;/p&gt; &#xA;&lt;h3&gt;For Nvidia GPU users&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python start_services.py --profile gpu-nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] If you have not used your Nvidia GPU with Docker before, please follow the &lt;a href=&#34;https://github.com/ollama/ollama/raw/main/docs/docker.md&#34;&gt;Ollama Docker instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;For AMD GPU users on Linux&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python start_services.py --profile gpu-amd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For Mac / Apple Silicon users&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re using a Mac with an M1 or newer processor, you can&#39;t expose your GPU to the Docker instance, unfortunately. There are two options in this case:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the starter kit fully on CPU:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python start_services.py --profile cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Ollama on your Mac for faster inference, and connect to that from the n8n instance:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python start_services.py --profile none&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you want to run Ollama on your mac, check the &lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama homepage&lt;/a&gt; for installation instructions.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;For Mac users running OLLAMA locally&lt;/h4&gt; &#xA;&lt;p&gt;If you&#39;re running OLLAMA locally on your Mac (not in Docker), you need to modify the OLLAMA_HOST environment variable in the n8n service configuration. Update the x-n8n section in your Docker Compose file as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;x-n8n: &amp;amp;service-n8n&#xA;  # ... other configurations ...&#xA;  environment:&#xA;    # ... other environment variables ...&#xA;    - OLLAMA_HOST=host.docker.internal:11434&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additionally, after you see &#34;Editor is now accessible via: &lt;a href=&#34;http://localhost:5678/&#34;&gt;http://localhost:5678/&lt;/a&gt;&#34;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Head to &lt;a href=&#34;http://localhost:5678/home/credentials&#34;&gt;http://localhost:5678/home/credentials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click on &#34;Local Ollama service&#34;&lt;/li&gt; &#xA; &lt;li&gt;Change the base URL to &#34;&lt;a href=&#34;http://host.docker.internal:11434/&#34;&gt;http://host.docker.internal:11434/&lt;/a&gt;&#34;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;For everyone else&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python start_services.py --profile cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;The environment argument&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;strong&gt;start-services.py&lt;/strong&gt; script offers the possibility to pass one of two options for the environment argument, &lt;strong&gt;private&lt;/strong&gt; (default environment) and &lt;strong&gt;public&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;private:&lt;/strong&gt; you are deploying the stack in a safe environment, hence a lot of ports can be made accessible without having to worry about security&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;public:&lt;/strong&gt; the stack is deployed in a public environment, which means the attack surface should be made as small as possible. All ports except for 80 and 443 are closed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The stack initialized with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   python start_services.py --profile gpu-nvidia --environment private&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;equals the one initialized with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   python start_services.py --profile gpu-nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deploying to the Cloud&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites for the below steps&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux machine (preferably Unbuntu) with Nano, Git, and Docker installed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Extra steps&lt;/h3&gt; &#xA;&lt;p&gt;Before running the above commands to pull the repo and install everything:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the commands as root to open up the necessary ports:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ufw enable&lt;/li&gt; &#xA;   &lt;li&gt;ufw allow 80 &amp;amp;&amp;amp; ufw allow 443&lt;/li&gt; &#xA;   &lt;li&gt;ufw reload&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;hr&gt; &lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ufw does not shield ports published by docker, because the iptables rules configured by docker are analyzed before those configured by ufw. There is a solution to change this behavior, but that is out of scope for this project. Just make sure that all traffic runs through the caddy service via port 443. Port 80 should only be used to redirect to port 443.&lt;/p&gt; &#xA;  &lt;hr&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;start-services.py&lt;/strong&gt; script with the environment argument &lt;strong&gt;public&lt;/strong&gt; to indicate you are going to run the package in a public environment. The script will make sure that all ports, except for 80 and 443, are closed down, e.g.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   python start_services.py --profile gpu-nvidia --environment public&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up A records for your DNS provider to point your subdomains you&#39;ll set up in the .env file for Caddy to the IP address of your cloud instance.&lt;/p&gt; &lt;p&gt;For example, A record to point n8n to [cloud instance IP] for n8n.yourdomain.com&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;⚡️ Quick start and usage&lt;/h2&gt; &#xA;&lt;p&gt;The main component of the self-hosted AI starter kit is a docker compose file pre-configured with network and disk so there isn’t much else you need to install. After completing the installation steps above, follow the steps below to get started.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open &lt;a href=&#34;http://localhost:5678/&#34;&gt;http://localhost:5678/&lt;/a&gt; in your browser to set up n8n. You’ll only have to do this once. You are NOT creating an account with n8n in the setup here, it is only a local account for your instance!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the included workflow: &lt;a href=&#34;http://localhost:5678/workflow/vTN9y2dLXqTiDfPT&#34;&gt;http://localhost:5678/workflow/vTN9y2dLXqTiDfPT&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create credentials for every service:&lt;/p&gt; &lt;p&gt;Ollama URL: &lt;a href=&#34;http://ollama:11434&#34;&gt;http://ollama:11434&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Postgres (through Supabase): use DB, username, and password from .env. IMPORTANT: Host is &#39;db&#39; Since that is the name of the service running Supabase&lt;/p&gt; &lt;p&gt;Qdrant URL: &lt;a href=&#34;http://qdrant:6333&#34;&gt;http://qdrant:6333&lt;/a&gt; (API key can be whatever since this is running locally)&lt;/p&gt; &lt;p&gt;Google Drive: Follow &lt;a href=&#34;https://docs.n8n.io/integrations/builtin/credentials/google/&#34;&gt;this guide from n8n&lt;/a&gt;. Don&#39;t use localhost for the redirect URI, just use another domain you have, it will still work! Alternatively, you can set up &lt;a href=&#34;https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/&#34;&gt;local file triggers&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select &lt;strong&gt;Test workflow&lt;/strong&gt; to start running the workflow.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If this is the first time you’re running the workflow, you may need to wait until Ollama finishes downloading Llama3.1. You can inspect the docker console logs to check on the progress.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure to toggle the workflow as active and copy the &#34;Production&#34; webhook URL!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open &lt;a href=&#34;http://localhost:3000/&#34;&gt;http://localhost:3000/&lt;/a&gt; in your browser to set up Open WebUI. You’ll only have to do this once. You are NOT creating an account with Open WebUI in the setup here, it is only a local account for your instance!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to Workspace -&amp;gt; Functions -&amp;gt; Add Function -&amp;gt; Give name + description then paste in the code from &lt;code&gt;n8n_pipe.py&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The function is also &lt;a href=&#34;https://openwebui.com/f/coleam/n8n_pipe/&#34;&gt;published here on Open WebUI&#39;s site&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click on the gear icon and set the n8n_url to the production URL for the webhook you copied in a previous step.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Toggle the function on and now it will be available in your model dropdown in the top left!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To open n8n at any time, visit &lt;a href=&#34;http://localhost:5678/&#34;&gt;http://localhost:5678/&lt;/a&gt; in your browser. To open Open WebUI at any time, visit &lt;a href=&#34;http://localhost:3000/&#34;&gt;http://localhost:3000/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;With your n8n instance, you’ll have access to over 400 integrations and a suite of basic and advanced AI nodes such as &lt;a href=&#34;https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/&#34;&gt;AI Agent&lt;/a&gt;, &lt;a href=&#34;https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.text-classifier/&#34;&gt;Text classifier&lt;/a&gt;, and &lt;a href=&#34;https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.information-extractor/&#34;&gt;Information Extractor&lt;/a&gt; nodes. To keep everything local, just remember to use the Ollama node for your language model and Qdrant as your vector store.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] This starter kit is designed to help you get started with self-hosted AI workflows. While it’s not fully optimized for production environments, it combines robust components that work well together for proof-of-concept projects. You can customize it to meet your specific needs&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Upgrading&lt;/h2&gt; &#xA;&lt;p&gt;To update all containers to their latest versions (n8n, Open WebUI, etc.), run these commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Stop all services&#xA;docker compose -p localai -f docker-compose.yml --profile &amp;lt;your-profile&amp;gt; down&#xA;&#xA;# Pull latest versions of all containers&#xA;docker compose -p localai -f docker-compose.yml --profile &amp;lt;your-profile&amp;gt; pull&#xA;&#xA;# Start services again with your desired profile&#xA;python start_services.py --profile &amp;lt;your-profile&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;&amp;lt;your-profile&amp;gt;&lt;/code&gt; with one of: &lt;code&gt;cpu&lt;/code&gt;, &lt;code&gt;gpu-nvidia&lt;/code&gt;, &lt;code&gt;gpu-amd&lt;/code&gt;, or &lt;code&gt;none&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note: The &lt;code&gt;start_services.py&lt;/code&gt; script itself does not update containers - it only restarts them or pulls them if you are downloading these containers for the first time. To get the latest versions, you must explicitly run the commands above.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Here are solutions to common issues you might encounter:&lt;/p&gt; &#xA;&lt;h3&gt;Supabase Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Supabase Pooler Restarting&lt;/strong&gt;: If the supabase-pooler container keeps restarting itself, follow the instructions in &lt;a href=&#34;https://github.com/supabase/supabase/issues/30210#issuecomment-2456955578&#34;&gt;this GitHub issue&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Supabase Analytics Startup Failure&lt;/strong&gt;: If the supabase-analytics container fails to start after changing your Postgres password, delete the folder &lt;code&gt;supabase/docker/volumes/db/data&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;If using Docker Desktop&lt;/strong&gt;: Go into the Docker settings and make sure &#34;Expose daemon on tcp://localhost:2375 without TLS&#34; is turned on&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Supabase Service Unavailable&lt;/strong&gt; - Make sure you don&#39;t have an &#34;@&#34; character in your Postgres password! If the connection to the kong container is working (the container logs say it is receiving requests from n8n) but n8n says it cannot connect, this is generally the problem from what the community has shared. Other characters might not be allowed too, the @ symbol is just the one I know for sure!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;GPU Support Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows GPU Support&lt;/strong&gt;: If you&#39;re having trouble running Ollama with GPU support on Windows with Docker Desktop:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Open Docker Desktop settings&lt;/li&gt; &#xA;   &lt;li&gt;Enable WSL 2 backend&lt;/li&gt; &#xA;   &lt;li&gt;See the &lt;a href=&#34;https://docs.docker.com/desktop/features/gpu/&#34;&gt;Docker GPU documentation&lt;/a&gt; for more details&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux GPU Support&lt;/strong&gt;: If you&#39;re having trouble running Ollama with GPU support on Linux, follow the &lt;a href=&#34;https://github.com/ollama/ollama/raw/main/docs/docker.md&#34;&gt;Ollama Docker instructions&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👓 Recommended reading&lt;/h2&gt; &#xA;&lt;p&gt;n8n is full of useful content for getting started quickly with its AI concepts and nodes. If you run into an issue, go to &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/local-ai-packaged/main/#support&#34;&gt;support&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.n8n.io/ai-agents/&#34;&gt;AI agents for developers: from theory to practice with n8n&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.n8n.io/advanced-ai/intro-tutorial/&#34;&gt;Tutorial: Build an AI workflow in n8n&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.n8n.io/advanced-ai/langchain/langchain-n8n/&#34;&gt;Langchain Concepts in n8n&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.n8n.io/advanced-ai/examples/agent-chain-comparison/&#34;&gt;Demonstration of key differences between agents and chains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.n8n.io/advanced-ai/examples/understand-vector-databases/&#34;&gt;What are vector databases?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎥 Video walkthrough&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/pOsO40HSbOo&#34;&gt;Cole&#39;s Guide to the Local AI Starter Kit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛍️ More AI templates&lt;/h2&gt; &#xA;&lt;p&gt;For more AI workflow ideas, visit the &lt;a href=&#34;https://n8n.io/workflows/?categories=AI&#34;&gt;&lt;strong&gt;official n8n AI template gallery&lt;/strong&gt;&lt;/a&gt;. From each workflow, select the &lt;strong&gt;Use workflow&lt;/strong&gt; button to automatically import the workflow into your local n8n instance.&lt;/p&gt; &#xA;&lt;h3&gt;Learn AI key concepts&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/1954-ai-agent-chat/&#34;&gt;AI Agent Chat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/2026-ai-chat-with-any-data-source-using-the-n8n-workflow-tool/&#34;&gt;AI chat with any data source (using the n8n workflow too)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/2098-chat-with-openai-assistant-by-adding-a-memory/&#34;&gt;Chat with OpenAI Assistant (by adding a memory)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/&#34;&gt;Use an open-source LLM (via HuggingFace)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/2165-chat-with-pdf-docs-using-ai-quoting-sources/&#34;&gt;Chat with PDF docs using AI (quoting sources)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/2006-ai-agent-that-can-scrape-webpages/&#34;&gt;AI agent that can scrape webpages&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Local AI templates&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/2341-build-a-tax-code-assistant-with-qdrant-mistralai-and-openai/&#34;&gt;Tax Code Assistant&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/2339-breakdown-documents-into-study-notes-using-templating-mistralai-and-qdrant/&#34;&gt;Breakdown Documents into Study Notes with MistralAI and Qdrant&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/2335-build-a-financial-documents-assistant-using-qdrant-and-mistralai/&#34;&gt;Financial Documents Assistant using Qdrant and&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;http://mistral.ai/&#34;&gt;&amp;nbsp;Mistral.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://n8n.io/workflows/2333-recipe-recommendations-with-qdrant-and-mistral/&#34;&gt;Recipe Recommendations with Qdrant and Mistral&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tips &amp;amp; tricks&lt;/h2&gt; &#xA;&lt;h3&gt;Accessing local files&lt;/h3&gt; &#xA;&lt;p&gt;The self-hosted AI starter kit will create a shared folder (by default, located in the same directory) which is mounted to the n8n container and allows n8n to access files on disk. This folder within the n8n container is located at &lt;code&gt;/data/shared&lt;/code&gt; -- this is the path you’ll need to use in nodes that interact with the local filesystem.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nodes that interact with the local filesystem&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.filesreadwrite/&#34;&gt;Read/Write Files from Disk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/&#34;&gt;Local File Trigger&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.executecommand/&#34;&gt;Execute Command&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📜&amp;nbsp;License&lt;/h2&gt; &#xA;&lt;p&gt;This project (originally created by the n8n team, link at the top of the README) is licensed under the Apache License 2.0 - see the &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/local-ai-packaged/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
</feed>