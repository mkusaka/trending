<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-22T01:44:17Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ok-oldking/ok-wuthering-waves</title>
    <updated>2025-06-22T01:44:17Z</updated>
    <id>tag:github.com,2025-06-22:/ok-oldking/ok-wuthering-waves</id>
    <link href="https://github.com/ok-oldking/ok-wuthering-waves" rel="alternate"></link>
    <summary type="html">&lt;p&gt;é¸£æ½® åå°è‡ªåŠ¨æˆ˜æ–— è‡ªåŠ¨åˆ·å£°éª¸ ä¸€é”®æ—¥å¸¸ Automation for Wuthering Waves&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ok-oldking/ok-wuthering-waves/master/icon.png&#34; width=&#34;200&#34;&gt; &lt;br&gt; ok-ww &lt;/h1&gt; &#xA; &lt;h3&gt;&lt;i&gt;åŸºäºå›¾åƒè¯†åˆ«çš„é¸£æ½®è‡ªåŠ¨åŒ–, ä½¿ç”¨windowsæ¥å£æ¨¡æ‹Ÿç”¨æˆ·ç‚¹å‡», æ— è¯»å–æ¸¸æˆå†…å­˜æˆ–ä¾µå…¥ä¿®æ”¹æ¸¸æˆæ–‡ä»¶/æ•°æ®.&lt;/i&gt;&lt;/h3&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/platfrom-Windows-blue?color=blue&#34; alt=&#34;Static Badge&#34;&gt; &lt;a href=&#34;https://github.com/ok-oldking/ok-wuthering-waves/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/ok-oldking/ok-wuthering-waves&#34; alt=&#34;GitHub release (with filter)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ok-oldking/ok-wuthering-waves/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/ok-oldking/ok-wuthering-waves/total&#34; alt=&#34;GitHub all releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/vVyCatEBgA&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/296598043787132928?color=5865f2&amp;amp;label=%20Discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ok-oldking/ok-wuthering-waves/master/README_en.md&#34;&gt;English Readme&lt;/a&gt; | ä¸­æ–‡è¯´æ˜&lt;/h3&gt; &#xA;&lt;p&gt;æ¼”ç¤ºå’Œæ•™ç¨‹ &lt;a href=&#34;https://youtu.be/h6P1KWjdnB4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&amp;amp;logo=YouTube&amp;amp;logoColor=white&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;å…è´£å£°æ˜&lt;/h1&gt; &#xA;&lt;p&gt;æœ¬è½¯ä»¶æ˜¯ä¸€ä¸ªå¤–éƒ¨å·¥å…·ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–é¸£æ½®çš„æ¸¸æˆç©æ³•ã€‚å®ƒä»…é€šè¿‡ç°æœ‰ç”¨æˆ·ç•Œé¢ä¸æ¸¸æˆäº¤äº’ï¼Œå¹¶éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ã€‚è¯¥è½¯ä»¶åŒ…æ—¨åœ¨ç®€åŒ–ç”¨æˆ·ä¸æ¸¸æˆçš„äº¤äº’ï¼Œä¸ä¼šç ´åæ¸¸æˆå¹³è¡¡æˆ–æä¾›ä¸å…¬å¹³ä¼˜åŠ¿ï¼Œä¹Ÿä¸ä¼šä¿®æ”¹ä»»ä½•æ¸¸æˆæ–‡ä»¶æˆ–ä»£ç ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æœ¬è½¯ä»¶å¼€æºã€å…è´¹ï¼Œä»…ä¾›ä¸ªäººå­¦ä¹ äº¤æµä½¿ç”¨ï¼Œä»…é™äºä¸ªäººæ¸¸æˆè´¦å·ï¼Œä¸å¾—ç”¨äºä»»ä½•å•†ä¸šæˆ–è¥åˆ©æ€§ç›®çš„ã€‚å¼€å‘è€…å›¢é˜Ÿæ‹¥æœ‰æœ¬é¡¹ç›®çš„æœ€ç»ˆè§£é‡Šæƒã€‚ä½¿ç”¨æœ¬è½¯ä»¶äº§ç”Ÿçš„æ‰€æœ‰é—®é¢˜ä¸æœ¬é¡¹ç›®åŠå¼€å‘è€…å›¢é˜Ÿæ— å…³ã€‚è‹¥æ‚¨å‘ç°å•†å®¶ä½¿ç”¨æœ¬è½¯ä»¶è¿›è¡Œä»£ç»ƒå¹¶æ”¶è´¹ï¼Œè¿™æ˜¯å•†å®¶çš„ä¸ªäººè¡Œä¸ºï¼Œæœ¬è½¯ä»¶ä¸æˆæƒç”¨äºä»£ç»ƒæœåŠ¡ï¼Œäº§ç”Ÿçš„é—®é¢˜åŠåæœä¸æœ¬è½¯ä»¶æ— å…³ã€‚æœ¬è½¯ä»¶ä¸æˆæƒä»»ä½•äººè¿›è¡Œå”®å–ï¼Œå”®å–çš„è½¯ä»¶å¯èƒ½è¢«åŠ å…¥æ¶æ„ä»£ç ï¼Œå¯¼è‡´æ¸¸æˆè´¦å·æˆ–ç”µè„‘èµ„æ–™è¢«ç›—ï¼Œä¸æœ¬è½¯ä»¶æ— å…³ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è¯·æ³¨æ„ï¼Œæ ¹æ®åº“æ´›çš„ã€Šé¸£æ½®ã€‹å…¬å¹³è¿è¥å£°æ˜:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ä¸¥ç¦åˆ©ç”¨ä»»ä½•ç¬¬ä¸‰æ–¹å·¥å…·ç ´åæ¸¸æˆä½“éªŒã€‚&#xA;æˆ‘ä»¬å°†ä¸¥å‰æ‰“å‡»ä½¿ç”¨å¤–æŒ‚ã€åŠ é€Ÿå™¨ã€ä½œå¼Šè½¯ä»¶ã€å®è„šæœ¬ç­‰è¿è§„å·¥å…·çš„è¡Œä¸ºï¼Œè¿™äº›è¡Œä¸ºåŒ…æ‹¬ä½†ä¸é™äºè‡ªåŠ¨æŒ‚æœºã€æŠ€èƒ½åŠ é€Ÿã€æ— æ•Œæ¨¡å¼ã€ç¬ç§»ã€ä¿®æ”¹æ¸¸æˆæ•°æ®ç­‰æ“ä½œã€‚&#xA;ä¸€ç»æŸ¥è¯ï¼Œæˆ‘ä»¬å°†è§†è¿è§„æƒ…å†µå’Œæ¬¡æ•°ï¼Œé‡‡å–åŒ…æ‹¬ä½†ä¸é™äºæ‰£é™¤è¿è§„æ”¶ç›Šã€å†»ç»“æˆ–æ°¸ä¹…å°ç¦æ¸¸æˆè´¦å·ç­‰æªæ–½ã€‚&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ä½¿ç”¨æ–¹æ³•:ä¸‹è½½ç»¿è‰²ç‰ˆ7zå‹ç¼©åŒ…(250Må·¦å³), è§£å‹ååŒå‡»ok-ww.exe&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ok-oldking/ok-wuthering-waves/releases&#34;&gt;GitHubä¸‹è½½&lt;/a&gt;, å…è´¹ç½‘é¡µç›´é“¾, ä¸è¦ç‚¹å‡»ä¸‹è½½Source Code, ç‚¹å‡»ä¸‹è½½7zå‹ç¼©åŒ…&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mirrorchyan.com/zh/projects?rid=okww&#34;&gt;Mirroré…±ä¸‹è½½æ¸ é“&lt;/a&gt;, å›½å†…ç½‘é¡µç›´é“¾, ä¸‹è½½éœ€è¦è´­ä¹°CD-KEY, å·²æœ‰Mirroré…±CD-KEYå¯å…è´¹ä¸‹è½½&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pan.quark.cn/s/a1052cec4d13&#34;&gt;å¤¸å…‹ç½‘ç›˜&lt;/a&gt;, å…è´¹, ä½†éœ€è¦æ³¨å†Œå¹¶ä¸‹è½½å¤¸å…‹ç½‘ç›˜å®¢æˆ·ç«¯&lt;/li&gt; &#xA; &lt;li&gt;åŠ å…¥QQé¢‘é“å, è®¨è®ºç»„ä¸‹è½½ &lt;a href=&#34;https://pd.qq.com/s/djmm6l44y&#34;&gt;https://pd.qq.com/s/djmm6l44y&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;æœ‰å¤šå¼º?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;4Kåˆ†è¾¨ç‡æµç•…è¿è¡Œ,æ”¯æŒæ‰€æœ‰16:9åˆ†è¾¨ç‡,1600x900ä»¥ä¸Š, 1280x720ä¸æ”¯æŒæ˜¯å› ä¸ºé¸£æ½®bug, å®ƒçš„1280x720å¹¶ä¸æ˜¯1280x720. éƒ¨åˆ†åŠŸèƒ½ä¹Ÿå¯ä»¥åœ¨21:9ç­‰å®½å±åˆ†è¾¨ç‡è¿è¡Œ&lt;/li&gt; &#xA; &lt;li&gt;å¯åå°è¿è¡Œ,å¯çª—å£åŒ–,å¯å…¨å±,å±å¹•ç¼©æ”¾æ¯”ä¾‹æ— è¦æ±‚&lt;/li&gt; &#xA; &lt;li&gt;å…¨è§’è‰²è‡ªåŠ¨è¯†åˆ«ï¼Œæ— éœ€é…ç½®å‡ºæ‹›è¡¨ï¼Œä¸€é”®è¿è¡Œ&lt;/li&gt; &#xA; &lt;li&gt;åå°è‡ªåŠ¨é™éŸ³æ¸¸æˆ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;å‡ºç°é—®é¢˜è¯·æ£€æŸ¥&lt;/h3&gt; &#xA;&lt;p&gt;æœ‰é—®é¢˜ç‚¹è¿™é‡Œ, æŒ¨ä¸ªæ£€æŸ¥å†æé—®:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;è§£å‹é—®é¢˜:&lt;/strong&gt; å°†å‹ç¼©åŒ…è§£å‹åˆ°ä»…åŒ…å«è‹±æ–‡å­—ç¬¦çš„ç›®å½•ä¸­ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æ€æ¯’è½¯ä»¶å¹²æ‰°:&lt;/strong&gt; å°†ä¸‹è½½å’Œè§£å‹ç›®å½•æ·»åŠ åˆ°æ‚¨çš„æ€æ¯’è½¯ä»¶/Windows Defender ç™½åå•ä¸­ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æ˜¾ç¤ºè®¾ç½®:&lt;/strong&gt; å…³é—­æ˜¾å¡æ»¤é•œå’Œé”åŒ–ã€‚ä½¿ç”¨é»˜è®¤æ¸¸æˆäº®åº¦å¹¶ç¦ç”¨åœ¨æ¸¸æˆä¸Šæ˜¾ç¤ºFPS(å¦‚å°é£æœº)ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;è‡ªå®šä¹‰æŒ‰é”®ç»‘å®š:&lt;/strong&gt; å¦‚æ²¡æœ‰ä½¿ç”¨é»˜è®¤æŒ‰é”®ï¼Œè¯·åœ¨APPè®¾ç½®ä¸­è®¾ç½®, ä¸åœ¨è®¾ç½®é‡Œçš„æŒ‰é”®ä¸æ”¯æŒã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ç‰ˆæœ¬è¿‡æ—§:&lt;/strong&gt; ç¡®ä¿æ‚¨ä½¿ç”¨çš„æ˜¯æœ€æ–°ç‰ˆæœ¬çš„ OK-GIã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æ€§èƒ½:&lt;/strong&gt; åœ¨æ¸¸æˆä¸­ä¿æŒç¨³å®šçš„ 60 FPSï¼Œå¦‚æœéœ€è¦ï¼Œé™ä½åˆ†è¾¨ç‡ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æ¸¸æˆæ–­çº¿&lt;/strong&gt; å¦‚æœç»å¸¸å‘ç°æ–­å¼€æœåŠ¡å™¨é“¾æ¥çš„é—®é¢˜, å¯ä»¥å…ˆæ‰“å¼€æ¸¸æˆ5åˆ†é’Ÿå†å¼€å§‹ç©, æˆ–è€…æ–­å¼€åä¸è¦é€€å‡ºæ¸¸æˆ, é‡æ–°ç™»é™†&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;è¿›ä¸€æ­¥å¸®åŠ©:&lt;/strong&gt; å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·æäº¤é”™è¯¯æŠ¥å‘Šã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Python æºç è¿è¡Œ&lt;/h3&gt; &#xA;&lt;p&gt;ä»…æ”¯æŒPython 3.12&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#CPUç‰ˆæœ¬, ä½¿ç”¨openvino&#xA;pip install -r requirements.txt --upgrade #install python dependencies, æ›´æ–°ä»£ç åå¯èƒ½éœ€è¦é‡æ–°è¿è¡Œ&#xA;python main.py # run the release version&#xA;python main_debug.py # run the debug version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;å‘½ä»¤è¡Œå‚æ•°&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ok-ww.exe -t 1 -e&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;-t æˆ– --task ä»£è¡¨å¯åŠ¨åè‡ªåŠ¨æ‰§è¡Œç¬¬å‡ ä¸ªä»»åŠ¡, 1å°±æ˜¯ç¬¬ä¸€ä¸ª, ä¸€æ¡é¾™ä»»åŠ¡&lt;/li&gt; &#xA; &lt;li&gt;-e æˆ– --exit åŠ ä¸Šä»£è¡¨å¦‚æœæ‰§è¡Œå®Œä»»åŠ¡ä¹‹åè‡ªåŠ¨é€€å‡º&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;åŠ å…¥æˆ‘ä»¬&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç”±äºåŸºäº&lt;a href=&#34;https://github.com/ok-oldking/ok-script&#34;&gt;ok-script&lt;/a&gt;å¼€å‘ï¼Œé¡¹ç›®ä»£ç ä»…æœ‰3000è¡Œï¼ˆPythonï¼‰ï¼Œç®€å•æ˜“ç»´æŠ¤&lt;/li&gt; &#xA; &lt;li&gt;é¸£æ½®æ°´ç¾¤ 970523295 è¿›ç¾¤ç­”æ¡ˆ:è€ç‹åŒå­¦OK&lt;/li&gt; &#xA; &lt;li&gt;ç¾¤éƒ½æ»¡äº† åŠ QQé¢‘é“ &lt;a href=&#34;https://pd.qq.com/s/djmm6l44y&#34;&gt;https://pd.qq.com/s/djmm6l44y&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;æœ‰å…´è¶£å¼€å‘çš„è¯·åŠ å¼€å‘è€…ç¾¤926858895, è‡³å°‘è¦èƒ½çœ‹æ–‡æ¡£, ä½¿ç”¨æºç è¿è¡Œ, é—®ä½¿ç”¨é—®é¢˜å’Œæéœ€æ±‚éƒ½ä¼šè¢«è¸¢&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ç›¸å…³é¡¹ç›®&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ok-oldking/ok-genshin-impact&#34;&gt;ok-genshin-impact&lt;/a&gt; åŸç¥è‡ªåŠ¨åŒ–,ä¸€é”®æ—¥å¸¸,åå°å‰§æƒ… ( å¯åå°,æ”¯æŒå…¨æ¸¸æˆè¯­è¨€,æ”¯æŒå…¨16: 9åˆ†è¾¨ç‡)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ok-oldking/ok-gf2&#34;&gt;ok-gf2&lt;/a&gt; å°‘å‰2è¿½æ”¾è‡ªåŠ¨åŒ–,ä¸€é”®æ—¥å¸¸,ç«æŠ€åœº,å…µæ£‹æ¨æ¼”,å°˜çƒŸ (æ”¯æŒPCç‰ˆåå°)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;èµåŠ©å•†(Sponsors)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EXEç­¾å: Free code signing provided by &lt;a href=&#34;https://signpath.io/&#34;&gt;SignPath.io&lt;/a&gt;, certificate by &lt;a href=&#34;https://signpath.org/&#34;&gt;SignPath Foundation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;è‡´è°¢&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lazydog28/mc_auto_boss&#34;&gt;https://github.com/lazydog28/mc_auto_boss&lt;/a&gt; åå°ç‚¹å‡»ä»£ç &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cyclotruc/gitingest</title>
    <updated>2025-06-22T01:44:17Z</updated>
    <id>tag:github.com,2025-06-22:/cyclotruc/gitingest</id>
    <link href="https://github.com/cyclotruc/gitingest" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Replace &#39;hub&#39; with &#39;ingest&#39; in any github url to get a prompt-friendly extract of a codebase&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gitingest&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitingest.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cyclotruc/gitingest/main/docs/frontpage.png&#34; alt=&#34;Image&#34; title=&#34;Gitingest main page&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cyclotruc/gitingest/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/gitingest&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/gitingest.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/cyclotruc/gitingest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/cyclotruc/gitingest?style=social.svg?sanitize=true&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/gitingest&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/gitingest&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.com/invite/zerRaGK9EC&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/https://discord.com/invite/zerRaGK9EC&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Turn any Git repository into a prompt-friendly text ingest for LLMs.&lt;/p&gt; &#xA;&lt;p&gt;You can also replace &lt;code&gt;hub&lt;/code&gt; with &lt;code&gt;ingest&lt;/code&gt; in any GitHub URL to access the corresponding digest.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitingest.com&#34;&gt;gitingest.com&lt;/a&gt; Â· &lt;a href=&#34;https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood&#34;&gt;Chrome Extension&lt;/a&gt; Â· &lt;a href=&#34;https://addons.mozilla.org/firefox/addon/gitingest&#34;&gt;Firefox Add-on&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸš€ Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy code context&lt;/strong&gt;: Get a text digest from a Git repository URL or a directory&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart Formatting&lt;/strong&gt;: Optimized output format for LLM prompts&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Statistics about&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;File and directory structure&lt;/li&gt; &#xA;   &lt;li&gt;Size of the extract&lt;/li&gt; &#xA;   &lt;li&gt;Token count&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CLI tool&lt;/strong&gt;: Run it as a shell command&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python package&lt;/strong&gt;: Import it in your code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“š Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+&lt;/li&gt; &#xA; &lt;li&gt;For private repositories: A GitHub Personal Access Token (PAT). You can generate one at &lt;a href=&#34;https://github.com/settings/personal-access-tokens&#34;&gt;https://github.com/settings/personal-access-tokens&lt;/a&gt; (Profile â†’ Settings â†’ Developer Settings â†’ Personal Access Tokens â†’ Fine-grained Tokens)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ğŸ“¦ Installation&lt;/h3&gt; &#xA;&lt;p&gt;Gitingest is available on &lt;a href=&#34;https://pypi.org/project/gitingest/&#34;&gt;PyPI&lt;/a&gt;. You can install it using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gitingest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, it might be a good idea to use &lt;code&gt;pipx&lt;/code&gt; to install it. You can install &lt;code&gt;pipx&lt;/code&gt; using your preferred package manager.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install pipx&#xA;apt install pipx&#xA;scoop install pipx&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using pipx for the first time, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx ensurepath&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install gitingest&#xA;pipx install gitingest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ§© Browser Extension Usage&lt;/h2&gt; &#xA;&lt;!-- markdownlint-disable MD033 --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood&#34; target=&#34;_blank&#34; title=&#34;Get Gitingest Extension from Chrome Web Store&#34;&gt;&lt;img height=&#34;48&#34; src=&#34;https://github.com/user-attachments/assets/20a6e44b-fd46-4e6c-8ea6-aad436035753&#34; alt=&#34;Available in the Chrome Web Store&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://addons.mozilla.org/firefox/addon/gitingest&#34; target=&#34;_blank&#34; title=&#34;Get Gitingest Extension from Firefox Add-ons&#34;&gt;&lt;img height=&#34;48&#34; src=&#34;https://github.com/user-attachments/assets/c0e99e6b-97cf-4af2-9737-099db7d3538b&#34; alt=&#34;Get The Add-on for Firefox&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://microsoftedge.microsoft.com/addons/detail/nfobhllgcekbmpifkjlopfdfdmljmipf&#34; target=&#34;_blank&#34; title=&#34;Get Gitingest Extension from Microsoft Edge Add-ons&#34;&gt;&lt;img height=&#34;48&#34; src=&#34;https://github.com/user-attachments/assets/204157eb-4cae-4c0e-b2cb-db514419fd9e&#34; alt=&#34;Get from the Edge Add-ons&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- markdownlint-enable MD033 --&gt; &#xA;&lt;p&gt;The extension is open source at &lt;a href=&#34;https://github.com/lcandy2/gitingest-extension&#34;&gt;lcandy2/gitingest-extension&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Issues and feature requests are welcome to the repo.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ’¡ Command line usage&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;gitingest&lt;/code&gt; command line tool allows you to analyze codebases and create a text dump of their contents.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Basic usage (writes to digest.txt by default)&#xA;gitingest /path/to/directory&#xA;&#xA;# From URL&#xA;gitingest https://github.com/cyclotruc/gitingest&#xA;&#xA;# or from specific subdirectory&#xA;gitingest https://github.com/cyclotruc/gitingest/tree/main/src/gitingest/utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For private repositories, use the &lt;code&gt;--token/-t&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get your token from https://github.com/settings/personal-access-tokens&#xA;gitingest https://github.com/username/private-repo --token github_pat_...&#xA;&#xA;# Or set it as an environment variable&#xA;export GITHUB_TOKEN=github_pat_...&#xA;gitingest https://github.com/username/private-repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the digest is written to a text file (&lt;code&gt;digest.txt&lt;/code&gt;) in your current working directory. You can customize the output in two ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--output/-o &amp;lt;filename&amp;gt;&lt;/code&gt; to write to a specific file.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--output/-o -&lt;/code&gt; to output directly to &lt;code&gt;STDOUT&lt;/code&gt; (useful for piping to other tools).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See more options and usage details with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gitingest --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ Python package usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Synchronous usage&#xA;from gitingest import ingest&#xA;&#xA;summary, tree, content = ingest(&#34;path/to/directory&#34;)&#xA;&#xA;# or from URL&#xA;summary, tree, content = ingest(&#34;https://github.com/cyclotruc/gitingest&#34;)&#xA;&#xA;# or from a specific subdirectory&#xA;summary, tree, content = ingest(&#34;https://github.com/cyclotruc/gitingest/tree/main/src/gitingest/utils&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For private repositories, you can pass a token:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Using token parameter&#xA;summary, tree, content = ingest(&#34;https://github.com/username/private-repo&#34;, token=&#34;github_pat_...&#34;)&#xA;&#xA;# Or set it as an environment variable&#xA;import os&#xA;os.environ[&#34;GITHUB_TOKEN&#34;] = &#34;github_pat_...&#34;&#xA;summary, tree, content = ingest(&#34;https://github.com/username/private-repo&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this won&#39;t write a file but can be enabled with the &lt;code&gt;output&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Asynchronous usage&#xA;from gitingest import ingest_async&#xA;import asyncio&#xA;&#xA;result = asyncio.run(ingest_async(&#34;path/to/directory&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Jupyter notebook usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gitingest import ingest_async&#xA;&#xA;# Use await directly in Jupyter&#xA;summary, tree, content = await ingest_async(&#34;path/to/directory&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is because Jupyter notebooks are asynchronous by default.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ³ Self-host&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the image:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t gitingest .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the container:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d --name gitingest -p 8000:8000 gitingest&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The application will be available at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are hosting it on a domain, you can specify the allowed hostnames via env variable &lt;code&gt;ALLOWED_HOSTS&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Default: &#34;gitingest.com, *.gitingest.com, localhost, 127.0.0.1&#34;.&#xA;ALLOWED_HOSTS=&#34;example.com, localhost, 127.0.0.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; &#xA;&lt;h3&gt;Non-technical ways to contribute&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create an Issue&lt;/strong&gt;: If you find a bug or have an idea for a new feature, please &lt;a href=&#34;https://github.com/cyclotruc/gitingest/issues/new&#34;&gt;create an issue&lt;/a&gt; on GitHub. This will help us track and prioritize your request.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spread the Word&lt;/strong&gt;: If you like Gitingest, please share it with your friends, colleagues, and on social media. This will help us grow the community and make Gitingest even better.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Use Gitingest&lt;/strong&gt;: The best feedback comes from real-world usage! If you encounter any issues or have ideas for improvement, please let us know by &lt;a href=&#34;https://github.com/cyclotruc/gitingest/issues/new&#34;&gt;creating an issue&lt;/a&gt; on GitHub or by reaching out to us on &lt;a href=&#34;https://discord.com/invite/zerRaGK9EC&#34;&gt;Discord&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Technical ways to contribute&lt;/h3&gt; &#xA;&lt;p&gt;Gitingest aims to be friendly for first time contributors, with a simple Python and HTML codebase. If you need any help while working with the code, reach out to us on &lt;a href=&#34;https://discord.com/invite/zerRaGK9EC&#34;&gt;Discord&lt;/a&gt;. For detailed instructions on how to make a pull request, see &lt;a href=&#34;https://raw.githubusercontent.com/cyclotruc/gitingest/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ› ï¸ Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tailwindcss.com&#34;&gt;Tailwind CSS&lt;/a&gt; - Frontend&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fastapi/fastapi&#34;&gt;FastAPI&lt;/a&gt; - Backend framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jinja.palletsprojects.com&#34;&gt;Jinja2&lt;/a&gt; - HTML templating&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;tiktoken&lt;/a&gt; - Token estimation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PostHog/posthog&#34;&gt;posthog&lt;/a&gt; - Amazing analytics&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Looking for a JavaScript/FileSystemNode package?&lt;/h3&gt; &#xA;&lt;p&gt;Check out the NPM alternative ğŸ“¦ Repomix: &lt;a href=&#34;https://github.com/yamadashy/repomix&#34;&gt;https://github.com/yamadashy/repomix&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸš€ Project Growth&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#cyclotruc/gitingest&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=cyclotruc/gitingest&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/vggt</title>
    <updated>2025-06-22T01:44:17Z</updated>
    <id>tag:github.com,2025-06-22:/facebookresearch/vggt</id>
    <link href="https://github.com/facebookresearch/vggt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2025 Best Paper Award] VGGT: Visual Geometry Grounded Transformer&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;VGGT: Visual Geometry Grounded Transformer&lt;/h1&gt; &#xA; &lt;a href=&#34;https://jytime.github.io/data/VGGT_CVPR25.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Paper-VGGT&#34; alt=&#34;Paper PDF&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2503.11651&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2503.11651-b31b1b&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://vgg-t.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-green&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/spaces/facebook/vggt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/&#34;&gt;Visual Geometry Group, University of Oxford&lt;/a&gt;&lt;/strong&gt;; &lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://jytime.github.io/&#34;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&#34;https://silent-chen.github.io/&#34;&gt;Minghao Chen&lt;/a&gt;, &lt;a href=&#34;https://nikitakaraevv.github.io/&#34;&gt;Nikita Karaev&lt;/a&gt;, &lt;a href=&#34;https://www.robots.ox.ac.uk/~vedaldi/&#34;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&#34;https://chrirupp.github.io/&#34;&gt;Christian Rupprecht&lt;/a&gt;, &lt;a href=&#34;https://d-novotny.github.io/&#34;&gt;David Novotny&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{wang2025vggt,&#xA;  title={VGGT: Visual Geometry Grounded Transformer},&#xA;  author={Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David},&#xA;  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[June 13, 2025] Honored to receive the Best Paper Award at CVPR 2025! Apologies if Iâ€™m slow to respond to queries or GitHub issues these days. If youâ€™re interested, our oral presentation is available &lt;a href=&#34;https://docs.google.com/presentation/d/1JVuPnuZx6RgAy-U5Ezobg73XpBi7FrOh/edit?usp=sharing&amp;amp;ouid=107115712143490405606&amp;amp;rtpof=true&amp;amp;sd=true&#34;&gt;here&lt;/a&gt;. (Note: itâ€™s shared in .pptx format with animations â€” quite large, but feel free to use it as a template if helpful.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[June 2, 2025] Added a script to run VGGT and save predictions in COLMAP format, with bundle adjustment support optional. The saved COLMAP files can be directly used with &lt;a href=&#34;https://github.com/nerfstudio-project/gsplat&#34;&gt;gsplat&lt;/a&gt; or other NeRF/Gaussian splatting libraries.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[May 3, 2025] Evaluation code for reproducing our camera pose estimation results on Co3D is now available in the &lt;a href=&#34;https://github.com/facebookresearch/vggt/tree/evaluation&#34;&gt;evaluation&lt;/a&gt; branch.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[Apr 13, 2025] Training code is being gradually cleaned and uploaded to the &lt;a href=&#34;https://github.com/facebookresearch/vggt/tree/training&#34;&gt;training&lt;/a&gt; branch. It will be merged into the main branch once finalized.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Visual Geometry Grounded Transformer (VGGT, CVPR 2025) is a feed-forward neural network that directly infers all key 3D attributes of a scene, including extrinsic and intrinsic camera parameters, point maps, depth maps, and 3D point tracks, &lt;strong&gt;from one, a few, or hundreds of its views, within seconds&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;First, clone this repository to your local machine, and install the dependencies (torch, torchvision, numpy, Pillow, and huggingface_hub).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:facebookresearch/vggt.git &#xA;cd vggt&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can install VGGT as a package (&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/vggt/main/docs/package.md&#34;&gt;click here&lt;/a&gt; for details).&lt;/p&gt; &#xA;&lt;p&gt;Now, try the model with just a few lines of code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from vggt.models.vggt import VGGT&#xA;from vggt.utils.load_fn import load_and_preprocess_images&#xA;&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) &#xA;dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] &amp;gt;= 8 else torch.float16&#xA;&#xA;# Initialize the model and load the pretrained weights.&#xA;# This will automatically download the model weights the first time it&#39;s run, which may take a while.&#xA;model = VGGT.from_pretrained(&#34;facebook/VGGT-1B&#34;).to(device)&#xA;&#xA;# Load and preprocess example images (replace with your own image paths)&#xA;image_names = [&#34;path/to/imageA.png&#34;, &#34;path/to/imageB.png&#34;, &#34;path/to/imageC.png&#34;]  &#xA;images = load_and_preprocess_images(image_names).to(device)&#xA;&#xA;with torch.no_grad():&#xA;    with torch.cuda.amp.autocast(dtype=dtype):&#xA;        # Predict attributes including cameras, depth maps, and point maps.&#xA;        predictions = model(images)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model weights will be automatically downloaded from Hugging Face. If you encounter issues such as slow loading, you can manually download them &lt;a href=&#34;https://huggingface.co/facebook/VGGT-1B/blob/main/model.pt&#34;&gt;here&lt;/a&gt; and load, or:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = VGGT()&#xA;_URL = &#34;https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt&#34;&#xA;model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Detailed Usage&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to expand&lt;/summary&gt; &#xA; &lt;p&gt;You can also optionally choose which attributes (branches) to predict, as shown below. This achieves the same result as the example above. This example uses a batch size of 1 (processing a single scene), but it naturally works for multiple scenes.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from vggt.utils.pose_enc import pose_encoding_to_extri_intri&#xA;from vggt.utils.geometry import unproject_depth_map_to_point_map&#xA;&#xA;with torch.no_grad():&#xA;    with torch.cuda.amp.autocast(dtype=dtype):&#xA;        images = images[None]  # add batch dimension&#xA;        aggregated_tokens_list, ps_idx = model.aggregator(images)&#xA;                &#xA;    # Predict Cameras&#xA;    pose_enc = model.camera_head(aggregated_tokens_list)[-1]&#xA;    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)&#xA;    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])&#xA;&#xA;    # Predict Depth Maps&#xA;    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)&#xA;&#xA;    # Predict Point Maps&#xA;    point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)&#xA;        &#xA;    # Construct 3D Points from Depth Maps and Cameras&#xA;    # which usually leads to more accurate 3D points than point map branch&#xA;    point_map_by_unprojection = unproject_depth_map_to_point_map(depth_map.squeeze(0), &#xA;                                                                extrinsic.squeeze(0), &#xA;                                                                intrinsic.squeeze(0))&#xA;&#xA;    # Predict Tracks&#xA;    # choose your own points to track, with shape (N, 2) for one scene&#xA;    query_points = torch.FloatTensor([[100.0, 200.0], &#xA;                                        [60.72, 259.94]]).to(device)&#xA;    track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Furthermore, if certain pixels in the input frames are unwanted (e.g., reflective surfaces, sky, or water), you can simply mask them by setting the corresponding pixel values to 0 or 1. Precise segmentation masks aren&#39;t necessary - simple bounding box masks work effectively (check this &lt;a href=&#34;https://github.com/facebookresearch/vggt/issues/47&#34;&gt;issue&lt;/a&gt; for an example).&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Interactive Demo&lt;/h2&gt; &#xA;&lt;p&gt;We provide multiple ways to visualize your 3D reconstructions. Before using these visualization tools, install the required dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements_demo.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Interactive 3D Visualization&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; VGGT typically reconstructs a scene in less than 1 second. However, visualizing 3D points may take tens of seconds due to third-party rendering, independent of VGGT&#39;s processing time. The visualization is slow especially when the number of images is large.&lt;/p&gt; &#xA;&lt;h4&gt;Gradio Web Interface&lt;/h4&gt; &#xA;&lt;p&gt;Our Gradio-based interface allows you to upload images/videos, run reconstruction, and interactively explore the 3D scene in your browser. You can launch this in your local machine or try it on &lt;a href=&#34;https://huggingface.co/spaces/facebook/vggt&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo_gradio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to preview the Gradio interactive interface&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://jytime.github.io/data/vggt_hf_demo_screen.png&#34; alt=&#34;Gradio Web Interface Preview&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Viser 3D Viewer&lt;/h4&gt; &#xA;&lt;p&gt;Run the following command to run reconstruction and visualize the point clouds in viser. Note this script requires a path to a folder containing images. It assumes only image files under the folder. You can set &lt;code&gt;--use_point_map&lt;/code&gt; to use the point cloud from the point map branch, instead of the depth-based point cloud.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo_viser.py --image_folder path/to/your/images/folder&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Exporting to COLMAP Format&lt;/h2&gt; &#xA;&lt;p&gt;We also support exporting VGGT&#39;s predictions directly to COLMAP format, by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Feedforward prediction only&#xA;python demo_colmap.py --scene_dir=/YOUR/SCENE_DIR/ &#xA;&#xA;# With bundle adjustment&#xA;python demo_colmap.py --scene_dir=/YOUR/SCENE_DIR/ --use_ba&#xA;&#xA;# Run with bundle adjustment using reduced parameters for faster processing&#xA;# Reduces max_query_pts from 4096 (default) to 2048 and query_frame_num from 8 (default) to 5&#xA;# Trade-off: Faster execution but potentially less robust reconstruction in complex scenes (you may consider setting query_frame_num equal to your total number of images) &#xA;# See demo_colmap.py for additional bundle adjustment configuration options&#xA;python demo_colmap.py --scene_dir=/YOUR/SCENE_DIR/ --use_ba --max_query_pts=2048 --query_frame_num=5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please ensure that the images are stored in &lt;code&gt;/YOUR/SCENE_DIR/images/&lt;/code&gt;. This folder should contain only the images. Check the examples folder for the desired data structure.&lt;/p&gt; &#xA;&lt;p&gt;The reconstruction result (camera parameters and 3D points) will be automatically saved under &lt;code&gt;/YOUR/SCENE_DIR/sparse/&lt;/code&gt; in the COLMAP format, such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;SCENE_DIR/&#xA;â”œâ”€â”€ images/&#xA;â””â”€â”€ sparse/&#xA;    â”œâ”€â”€ cameras.bin&#xA;    â”œâ”€â”€ images.bin&#xA;    â””â”€â”€ points3D.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Integration with Gaussian Splatting&lt;/h2&gt; &#xA;&lt;p&gt;The exported COLMAP files can be directly used with &lt;a href=&#34;https://github.com/nerfstudio-project/gsplat&#34;&gt;gsplat&lt;/a&gt; for Gaussian Splatting training. Install &lt;code&gt;gsplat&lt;/code&gt; following their official instructions (we recommend &lt;code&gt;gsplat==1.3.0&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;p&gt;An example command to train the model is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd gsplat&#xA;python examples/simple_trainer.py  default --data_factor 1 --data_dir /YOUR/SCENE_DIR/ --result_dir /YOUR/RESULT_DIR/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Zero-shot Single-view Reconstruction&lt;/h2&gt; &#xA;&lt;p&gt;Our model shows surprisingly good performance on single-view reconstruction, although it was never trained for this task. The model does not need to duplicate the single-view image to a pair, instead, it can directly infer the 3D structure from the tokens of the single view image. Feel free to try it with our demos above, which naturally works for single-view reconstruction.&lt;/p&gt; &#xA;&lt;p&gt;We did not quantitatively test monocular depth estimation performance ourselves, but &lt;a href=&#34;https://github.com/kabouzeid&#34;&gt;@kabouzeid&lt;/a&gt; generously provided a comparison of VGGT to recent methods &lt;a href=&#34;https://github.com/facebookresearch/vggt/issues/36&#34;&gt;here&lt;/a&gt;. VGGT shows competitive or better results compared to state-of-the-art monocular approaches such as DepthAnything v2 or MoGe, despite never being explicitly trained for single-view tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Runtime and GPU Memory&lt;/h2&gt; &#xA;&lt;p&gt;We benchmark the runtime and GPU memory usage of VGGT&#39;s aggregator on a single NVIDIA H100 GPU across various input sizes.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Input Frames&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;4&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;8&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;10&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;20&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;50&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;100&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;200&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Time (s)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Memory (GB)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.88&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.63&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note that these results were obtained using Flash Attention 3, which is faster than the default Flash Attention 2 implementation while maintaining almost the same memory usage. Feel free to compile Flash Attention 3 from source to get better performance.&lt;/p&gt; &#xA;&lt;h2&gt;Research Progression&lt;/h2&gt; &#xA;&lt;p&gt;Our work builds upon a series of previous research projects. If you&#39;re interested in understanding how our research evolved, check out our previous works:&lt;/p&gt; &#xA;&lt;table border=&#34;0&#34; cellspacing=&#34;0&#34; cellpadding=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;a href=&#34;https://github.com/jytime/Deep-SfM-Revisited&#34;&gt;Deep SfM Revisited&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;white-space: pre;&#34;&gt;â”€â”€â”&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;a href=&#34;https://github.com/facebookresearch/PoseDiffusion&#34;&gt;PoseDiffusion&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;white-space: pre;&#34;&gt;â”€â”€â”€â”€â”€â–º&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/facebookresearch/vggsfm&#34;&gt;VGGSfM&lt;/a&gt; â”€â”€â–º &lt;a href=&#34;https://github.com/facebookresearch/vggt&#34;&gt;VGGT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;a href=&#34;https://github.com/facebookresearch/co-tracker&#34;&gt;CoTracker&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;white-space: pre;&#34;&gt;â”€â”€â”˜&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to these great repositories: &lt;a href=&#34;https://github.com/facebookresearch/PoseDiffusion&#34;&gt;PoseDiffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/vggsfm&#34;&gt;VGGSfM&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/co-tracker&#34;&gt;CoTracker&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;DINOv2&lt;/a&gt;, &lt;a href=&#34;https://github.com/naver/dust3r&#34;&gt;Dust3r&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/moge&#34;&gt;Moge&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/pytorch3d&#34;&gt;PyTorch3D&lt;/a&gt;, &lt;a href=&#34;https://github.com/xiongzhu666/Sky-Segmentation-and-Post-processing&#34;&gt;Sky Segmentation&lt;/a&gt;, &lt;a href=&#34;https://github.com/DepthAnything/Depth-Anything-V2&#34;&gt;Depth Anything V2&lt;/a&gt;, &lt;a href=&#34;https://github.com/YvanYin/Metric3D&#34;&gt;Metric3D&lt;/a&gt; and many other inspiring works in the community.&lt;/p&gt; &#xA;&lt;h2&gt;Checklist&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release the training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release VGGT-500M and VGGT-200M&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/vggt/main/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt; file for details about the license under which this code is made available.&lt;/p&gt;</summary>
  </entry>
</feed>