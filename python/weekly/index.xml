<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-27T01:50:10Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>allenai/olmocr</title>
    <updated>2025-04-27T01:50:10Z</updated>
    <id>tag:github.com,2025-04-27:/allenai/olmocr</id>
    <link href="https://github.com/allenai/olmocr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Toolkit for linearizing PDFs for LLM datasets/training&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;img src=&#34;https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3&#34; width=&#34;300&#34;/&gt; --&gt; &#xA; &lt;img src=&#34;https://github.com/user-attachments/assets/d70c8644-3e64-4230-98c3-c52fddaeccb6&#34; alt=&#34;olmOCR Logo&#34; width=&#34;300&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;h1&gt;olmOCR&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;GitHub License&#34; src=&#34;https://img.shields.io/github/license/allenai/OLMo&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/olmocr/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/allenai/olmocr.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://olmocr.allenai.org/papers/olmocr.pdf&#34;&gt; &lt;img alt=&#34;Tech Report&#34; src=&#34;https://img.shields.io/badge/Paper-olmOCR-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://olmocr.allenai.org&#34;&gt; &lt;img alt=&#34;Demo&#34; src=&#34;https://img.shields.io/badge/Ai2-Demo-F0529C&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/sZq3jTNVNG&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;amp;logo=discord&amp;amp;label=Ai2&amp;amp;color=%235B65E9&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;A toolkit for training language models to work with PDF documents in the wild.&lt;/p&gt; &#xA;&lt;p&gt;Try the online demo: &lt;a href=&#34;https://olmocr.allenai.org/&#34;&gt;https://olmocr.allenai.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;What is included:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A prompting strategy to get really good natural text parsing using ChatGPT 4o - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/data/buildsilver.py&#34;&gt;buildsilver.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An side-by-side eval toolkit for comparing different pipeline versions - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/eval/runeval.py&#34;&gt;runeval.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Basic filtering by language and SEO spam removal - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/filter/filter.py&#34;&gt;filter.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Finetuning code for Qwen2-VL and Molmo-O - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/train/train.py&#34;&gt;train.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Processing millions of PDFs through a finetuned model using Sglang - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/pipeline.py&#34;&gt;pipeline.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Viewing &lt;a href=&#34;https://github.com/allenai/dolma&#34;&gt;Dolma docs&lt;/a&gt; created from PDFs - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/viewer/dolmaviewer.py&#34;&gt;dolmaviewer.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 20 GB of GPU RAM&lt;/li&gt; &#xA; &lt;li&gt;30GB of free disk space&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You will need to install poppler-utils and additional fonts for rendering PDF images.&lt;/p&gt; &#xA;&lt;p&gt;Install dependencies (Ubuntu/Debian)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get update&#xA;sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set up a conda environment and install olmocr&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n olmocr python=3.11&#xA;conda activate olmocr&#xA;&#xA;git clone https://github.com/allenai/olmocr.git&#xA;cd olmocr&#xA;&#xA;# For CPU-only operations, ex. running benchmarks&#xA;pip install -e .&#xA;&#xA;# For actually converting the files with your own GPU&#xA;pip install -e .[gpu] --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Local Usage Example&lt;/h3&gt; &#xA;&lt;p&gt;For quick testing, try the &lt;a href=&#34;https://olmocr.allen.ai/&#34;&gt;web demo&lt;/a&gt;. To run locally, a GPU is required, as inference is powered by &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;sglang&lt;/a&gt; under the hood. Convert a Single PDF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline ./localworkspace --pdfs tests/gnarly_pdfs/horribleocr.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Convert an Image file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline ./localworkspace --pdfs random_page.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Convert Multiple PDFs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline ./localworkspace --pdfs tests/gnarly_pdfs/*.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results will be stored as JSON in &lt;code&gt;./localworkspace&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Viewing Results&lt;/h4&gt; &#xA;&lt;p&gt;Extracted text is stored as &lt;a href=&#34;https://github.com/allenai/dolma&#34;&gt;Dolma&lt;/a&gt;-style JSONL inside of the &lt;code&gt;./localworkspace/results&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat localworkspace/results/output_*.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;View results side-by-side with the original PDFs (uses &lt;code&gt;dolmaviewer&lt;/code&gt; command):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.viewer.dolmaviewer localworkspace/results/output_*.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now open &lt;code&gt;./dolma_previews/tests_gnarly_pdfs_horribleocr_pdf.html&lt;/code&gt; in your favorite browser.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/128922d1-63e6-4d34-84f2-d7901237da1f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multi-node / Cluster Usage&lt;/h3&gt; &#xA;&lt;p&gt;If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.&lt;/p&gt; &#xA;&lt;p&gt;For example, you can start this command on your first worker node, and it will set up a simple work queue in your AWS bucket and start converting PDFs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are at Ai2 and want to linearize millions of PDFs efficiently using &lt;a href=&#34;https://www.beaker.org&#34;&gt;beaker&lt;/a&gt;, just add the &lt;code&gt;--beaker&lt;/code&gt; flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start converting PDFs.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Full documentation for the pipeline&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline --help&#xA;usage: pipeline.py [-h] [--pdfs PDFS] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP]&#xA;                   [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS] [--apply_filter] [--stats] [--model MODEL]&#xA;                   [--model_max_context MODEL_MAX_CONTEXT] [--model_chat_template MODEL_CHAT_TEMPLATE] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM]&#xA;                   [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER]&#xA;                   [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]&#xA;                   workspace&#xA;&#xA;Manager for running millions of PDFs through a batch inference pipeline&#xA;&#xA;positional arguments:&#xA;  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --pdfs PDFS           Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths&#xA;  --workspace_profile WORKSPACE_PROFILE&#xA;                        S3 configuration profile for accessing the workspace&#xA;  --pdf_profile PDF_PROFILE&#xA;                        S3 configuration profile for accessing the raw pdf documents&#xA;  --pages_per_group PAGES_PER_GROUP&#xA;                        Aiming for this many pdf pages per work item group&#xA;  --max_page_retries MAX_PAGE_RETRIES&#xA;                        Max number of times we will retry rendering a page&#xA;  --max_page_error_rate MAX_PAGE_ERROR_RATE&#xA;                        Rate of allowable failed pages in a document, 1/250 by default&#xA;  --workers WORKERS     Number of workers to run at a time&#xA;  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam&#xA;  --stats               Instead of running any job, reports some statistics about the current workspace&#xA;  --model MODEL         List of paths where you can find the model to convert this pdf. You can specify several different paths here, and the script will try to use the&#xA;                        one which is fastest to access&#xA;  --model_max_context MODEL_MAX_CONTEXT&#xA;                        Maximum context length that the model was fine tuned under&#xA;  --model_chat_template MODEL_CHAT_TEMPLATE&#xA;                        Chat template to pass to sglang server&#xA;  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM&#xA;                        Dimension on longest side to use for rendering the pdf pages&#xA;  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN&#xA;                        Maximum amount of anchor text to use (characters)&#xA;  --beaker              Submit this job to beaker instead of running locally&#xA;  --beaker_workspace BEAKER_WORKSPACE&#xA;                        Beaker workspace to submit to&#xA;  --beaker_cluster BEAKER_CLUSTER&#xA;                        Beaker clusters you want to run on&#xA;  --beaker_gpus BEAKER_GPUS&#xA;                        Number of gpu replicas to run&#xA;  --beaker_priority BEAKER_PRIORITY&#xA;                        Beaker priority level for the job&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Team&lt;/h2&gt; &#xA;&lt;!-- start team --&gt; &#xA;&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is developed and maintained by the AllenNLP team, backed by &lt;a href=&#34;https://allenai.org/&#34;&gt;the Allen Institute for Artificial Intelligence (AI2)&lt;/a&gt;. AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see &lt;a href=&#34;https://github.com/allenai/olmocr/graphs/contributors&#34;&gt;our contributors&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;!-- end team --&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;!-- start license --&gt; &#xA;&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is licensed under &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache 2.0&lt;/a&gt;. A full copy of the license can be found &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/LICENSE&#34;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- end license --&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{olmocr,&#xA;      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},&#xA;      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},&#xA;      year={2025},&#xA;      eprint={2502.18443},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2502.18443},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>jumpserver/jumpserver</title>
    <updated>2025-04-27T01:50:10Z</updated>
    <id>tag:github.com,2025-04-27:/jumpserver/jumpserver</id>
    <link href="https://github.com/jumpserver/jumpserver" rel="alternate"></link>
    <summary type="html">&lt;p&gt;JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a name=&#34;readme-top&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://jumpserver.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://download.jumpserver.org/images/jumpserver-logo.svg?sanitize=true&#34; alt=&#34;JumpServer&#34; width=&#34;300&#34;&gt;&lt;/a&gt; &#xA; &lt;h2&gt;An open-source PAM tool (Bastion Host)&lt;/h2&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/jumpserver/jumpserver&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/W6vYXmAQG2&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1194233267294052363?style=flat&amp;amp;logo=discord&amp;amp;logoColor=%23f5f5f5&amp;amp;labelColor=%235462eb&amp;amp;color=%235462eb&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/u/jumpserver&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/jumpserver/jms_all.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jumpserver/jumpserver/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/jumpserver/jumpserver&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jumpserver/jumpserver&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/jumpserver/jumpserver?color=%231890FF&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jumpserver/jumpserver/dev/README.md&#34;&gt;English&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.zh-hans.md&#34;&gt;中文(简体)&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.zh-hant.md&#34;&gt;中文(繁體)&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.ja.md&#34;&gt;日本語&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.pt-br.md&#34;&gt;Português (Brasil)&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.es.md&#34;&gt;Español&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.ru.md&#34;&gt;Русский&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;What is JumpServer?&lt;/h2&gt; &#xA;&lt;p&gt;JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.&lt;/p&gt; &#xA;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://github.com/user-attachments/assets/dd612f3d-c958-4f84-b164-f31b75454d7f&#34;&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/user-attachments/assets/28676212-2bc4-4a9f-ae10-3be9320647e3&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/user-attachments/assets/dd612f3d-c958-4f84-b164-f31b75454d7f&#34; alt=&#34;Theme-based Image&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Prepare a clean Linux Server ( 64 bit, &amp;gt;= 4c8g )&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -sSL https://github.com/jumpserver/jumpserver/releases/latest/download/quick_start.sh | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Access JumpServer in your browser at &lt;code&gt;http://your-jumpserver-ip/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Username: &lt;code&gt;admin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Password: &lt;code&gt;ChangeMe&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UlGYRbKrpgY&#34; title=&#34;JumpServer Quickstart&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/0f32f52b-9935-485e-8534-336c63389612&#34; alt=&#34;JumpServer Quickstart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;table style=&#34;border-collapse: collapse; border: 1px solid black;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/jumpserver/jumpserver/assets/32935519/99fabe5b-0475-4a53-9116-4c370a1426c4&#34; alt=&#34;JumpServer Console&#34;&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/7c1f81af-37e8-4f07-8ac9-182895e1062e&#34; alt=&#34;JumpServer PAM&#34;&gt;&lt;/td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/jumpserver/jumpserver/assets/32935519/a424d731-1c70-4108-a7d8-5bbf387dda9a&#34; alt=&#34;JumpServer Audits&#34;&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/jumpserver/jumpserver/assets/32935519/393d2c27-a2d0-4dea-882d-00ed509e00c9&#34; alt=&#34;JumpServer Workbench&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/eaa41f66-8cc8-4f01-a001-0d258501f1c9&#34; alt=&#34;JumpServer RBAC&#34;&gt;&lt;/td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/jumpserver/jumpserver/assets/32935519/3a2611cd-8902-49b8-b82b-2a6dac851f3e&#34; alt=&#34;JumpServer Settings&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/jumpserver/jumpserver/assets/32935519/1e236093-31f7-4563-8eb1-e36d865f1568&#34; alt=&#34;JumpServer SSH&#34;&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/jumpserver/jumpserver/assets/32935519/69373a82-f7ab-41e8-b763-bbad2ba52167&#34; alt=&#34;JumpServer RDP&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/jumpserver/jumpserver/assets/32935519/5bed98c6-cbe8-4073-9597-d53c69dc3957&#34; alt=&#34;JumpServer K8s&#34;&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/jumpserver/jumpserver/assets/32935519/b80ad654-548f-42bc-ba3d-c1cfdf1b46d6&#34; alt=&#34;JumpServer DB&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Components&lt;/h2&gt; &#xA;&lt;p&gt;JumpServer consists of multiple key components, which collectively form the functional framework of JumpServer, providing users with comprehensive capabilities for operations management and security control.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/lina&#34;&gt;Lina&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/lina/releases&#34;&gt;&lt;img alt=&#34;Lina release&#34; src=&#34;https://img.shields.io/github/release/jumpserver/lina.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer Web UI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/luna&#34;&gt;Luna&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/luna/releases&#34;&gt;&lt;img alt=&#34;Luna release&#34; src=&#34;https://img.shields.io/github/release/jumpserver/luna.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer Web Terminal&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/koko&#34;&gt;KoKo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/koko/releases&#34;&gt;&lt;img alt=&#34;Koko release&#34; src=&#34;https://img.shields.io/github/release/jumpserver/koko.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer Character Protocol Connector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/lion&#34;&gt;Lion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/lion/releases&#34;&gt;&lt;img alt=&#34;Lion release&#34; src=&#34;https://img.shields.io/github/release/jumpserver/lion.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer Graphical Protocol Connector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/chen&#34;&gt;Chen&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/chen/releases&#34;&gt;&lt;img alt=&#34;Chen release&#34; src=&#34;https://img.shields.io/github/release/jumpserver/chen.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer Web DB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/tinker&#34;&gt;Tinker&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Tinker&#34; src=&#34;https://img.shields.io/badge/release-private-red&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer Remote Application Connector (Windows)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/Panda&#34;&gt;Panda&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Panda&#34; src=&#34;https://img.shields.io/badge/release-private-red&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer EE Remote Application Connector (Linux)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/razor&#34;&gt;Razor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Chen&#34; src=&#34;https://img.shields.io/badge/release-private-red&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer EE RDP Proxy Connector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/magnus&#34;&gt;Magnus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Magnus&#34; src=&#34;https://img.shields.io/badge/release-private-red&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer EE Database Proxy Connector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/nec&#34;&gt;Nec&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Nec&#34; src=&#34;https://img.shields.io/badge/release-private-red&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer EE VNC Proxy Connector&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jumpserver/facelive&#34;&gt;Facelive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Facelive&#34; src=&#34;https://img.shields.io/badge/release-private-red&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JumpServer EE Facial Recognition&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to submit PR to contribute. Please refer to &lt;a href=&#34;https://github.com/jumpserver/jumpserver/raw/dev/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (c) 2014-2025 FIT2CLOUD, All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under The GNU General Public License version 3 (GPLv3) (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.html&#34;&gt;https://www.gnu.org/licenses/gpl-3.0.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34; AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;!-- JumpServer official link --&gt; &#xA;&lt;!-- JumpServer Other link--&gt; &#xA;&lt;!-- Shield link--&gt;</summary>
  </entry>
  <entry>
    <title>bytedance/UI-TARS</title>
    <updated>2025-04-27T01:50:10Z</updated>
    <id>tag:github.com,2025-04-27:/bytedance/UI-TARS</id>
    <link href="https://github.com/bytedance/UI-TARS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bytedance/UI-TARS/main/figures/writer.png&#34; alt=&#34;Local Image&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🌐 &lt;a href=&#34;https://seed-tars.com/&#34;&gt;Website&lt;/a&gt;&amp;nbsp;&amp;nbsp; | 🤗 &lt;a href=&#34;https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B&#34;&gt;Hugging Face Models&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 🔧 &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_deploy.md&#34;&gt;Deployment&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 📑 &lt;a href=&#34;https://arxiv.org/abs/2501.12326&#34;&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; |&amp;nbsp;&amp;nbsp; 🖥️ &lt;a href=&#34;https://github.com/bytedance/UI-TARS-desktop&#34;&gt;UI-TARS-desktop&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;br&gt;🏄 &lt;a href=&#34;https://github.com/web-infra-dev/Midscene&#34;&gt;Midscene (Browser Automation) &lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🫨 &lt;a href=&#34;https://discord.gg/pTXwYVjfcs&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;p&gt;We also offer a &lt;strong&gt;UI-TARS-desktop&lt;/strong&gt; version, which can operate on your &lt;strong&gt;local personal device&lt;/strong&gt;. To use it, please visit &lt;a href=&#34;https://github.com/bytedance/UI-TARS-desktop&#34;&gt;https://github.com/bytedance/UI-TARS-desktop&lt;/a&gt;. To use UI-TARS in web automation, you may refer to the open-source project &lt;a href=&#34;https://github.com/web-infra-dev/Midscene&#34;&gt;Midscene.js&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🌟 2025.04.16: We shared the latest progress of the UI-TARS-1.5 model in our &lt;a href=&#34;https://seed-tars.com/1.5&#34;&gt;blog&lt;/a&gt;, which excels in playing games and performing GUI tasks, and we open-sourced the &lt;a href=&#34;https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B&#34;&gt;UI-TARS-1.5-7B&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;✨ 2025.03.23: We updated the OSWorld inference scripts from the original official &lt;a href=&#34;https://github.com/xlang-ai/OSWorld/raw/main/run_uitars.py&#34;&gt;OSWorld repository&lt;/a&gt;. Now, you can use the OSWorld official inference scripts to reproduce our results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;UI-TARS-1.5, an open-source multimodal agent built upon a powerful vision-language model. It is capable of effectively performing diverse tasks within virtual worlds.&lt;/p&gt; &#xA;&lt;p&gt;Leveraging the foundational architecture introduced in &lt;a href=&#34;https://arxiv.org/abs/2501.12326&#34;&gt;our recent paper&lt;/a&gt;, UI-TARS-1.5 integrates advanced reasoning enabled by reinforcement learning. This allows the model to reason through its thoughts before taking action, significantly enhancing its performance and adaptability, particularly in inference-time scaling. Our new 1.5 version achieves state-of-the-art results across a variety of standard benchmarks, demonstrating strong reasoning capabilities and notable improvements over prior models.&lt;/p&gt; &#xA;&lt;!-- ![Local Image](figures/UI-TARS.png) --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;video controls width=&#34;480&#34;&gt; &#xA;  &lt;source src=&#34;https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/GUI_demo.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA; &lt;/video&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;video controls width=&#34;480&#34;&gt; &#xA;  &lt;source src=&#34;https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/Game_demo.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA; &lt;/video&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the deploy guide &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_deploy.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For coordinates processing, refer to &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_coordinates.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For full action space parsing, refer to &lt;a href=&#34;https://github.com/xlang-ai/OSWorld/raw/main/mm_agents/uitars_agent.py&#34;&gt;OSWorld uitars_agent.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;System Prompts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/UI-TARS/main/prompts.py&#34;&gt;prompts.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Online Benchmark Evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Benchmark type&lt;/th&gt; &#xA;   &lt;th&gt;Benchmark&lt;/th&gt; &#xA;   &lt;th&gt;UI-TARS-1.5&lt;/th&gt; &#xA;   &lt;th&gt;OpenAI CUA&lt;/th&gt; &#xA;   &lt;th&gt;Claude 3.7&lt;/th&gt; &#xA;   &lt;th&gt;Previous SOTA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Computer Use&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.07972&#34;&gt;OSworld&lt;/a&gt; (100 steps)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;42.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;36.4&lt;/td&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;38.1 (200 step)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.08264&#34;&gt;Windows Agent Arena&lt;/a&gt; (50 steps)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;42.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;29.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Browser Use&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.13919&#34;&gt;WebVoyager&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;84.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;87&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;84.1&lt;/td&gt; &#xA;   &lt;td&gt;87&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2504.01382&#34;&gt;Online-Mind2web&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;75.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;71&lt;/td&gt; &#xA;   &lt;td&gt;62.9&lt;/td&gt; &#xA;   &lt;td&gt;71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Phone Use&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.14573&#34;&gt;Android World&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;64.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;59.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Grounding Capability Evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Benchmark&lt;/th&gt; &#xA;   &lt;th&gt;UI-TARS-1.5&lt;/th&gt; &#xA;   &lt;th&gt;OpenAI CUA&lt;/th&gt; &#xA;   &lt;th&gt;Claude 3.7&lt;/th&gt; &#xA;   &lt;th&gt;Previous SOTA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.23218&#34;&gt;ScreenSpot-V2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;94.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;87.9&lt;/td&gt; &#xA;   &lt;td&gt;87.6&lt;/td&gt; &#xA;   &lt;td&gt;91.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.07981v1&#34;&gt;ScreenSpotPro&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;23.4&lt;/td&gt; &#xA;   &lt;td&gt;27.7&lt;/td&gt; &#xA;   &lt;td&gt;43.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Poki Game&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/2048&#34;&gt;2048&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/cubinko&#34;&gt;cubinko&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/energy&#34;&gt;energy&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/free-the-key&#34;&gt;free-the-key&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/gem-11&#34;&gt;Gem-11&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/hex-frvr&#34;&gt;hex-frvr&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/infinity-loop&#34;&gt;Infinity-Loop&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/maze-path-of-light&#34;&gt;Maze:Path-of-Light&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/shapes&#34;&gt;shapes&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/snake-solver&#34;&gt;snake-solver&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/wood-blocks-3d&#34;&gt;wood-blocks-3d&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/yarn-untangle&#34;&gt;yarn-untangle&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/laser-maze-puzzle&#34;&gt;laser-maze-puzzle&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://poki.com/en/g/tiles-master&#34;&gt;tiles-master&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI CUA&lt;/td&gt; &#xA;   &lt;td&gt;31.04&lt;/td&gt; &#xA;   &lt;td&gt;0.00&lt;/td&gt; &#xA;   &lt;td&gt;32.80&lt;/td&gt; &#xA;   &lt;td&gt;0.00&lt;/td&gt; &#xA;   &lt;td&gt;46.27&lt;/td&gt; &#xA;   &lt;td&gt;92.25&lt;/td&gt; &#xA;   &lt;td&gt;23.08&lt;/td&gt; &#xA;   &lt;td&gt;35.00&lt;/td&gt; &#xA;   &lt;td&gt;52.18&lt;/td&gt; &#xA;   &lt;td&gt;42.86&lt;/td&gt; &#xA;   &lt;td&gt;2.02&lt;/td&gt; &#xA;   &lt;td&gt;44.56&lt;/td&gt; &#xA;   &lt;td&gt;80.00&lt;/td&gt; &#xA;   &lt;td&gt;78.27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Claude 3.7&lt;/td&gt; &#xA;   &lt;td&gt;43.05&lt;/td&gt; &#xA;   &lt;td&gt;0.00&lt;/td&gt; &#xA;   &lt;td&gt;41.60&lt;/td&gt; &#xA;   &lt;td&gt;0.00&lt;/td&gt; &#xA;   &lt;td&gt;0.00&lt;/td&gt; &#xA;   &lt;td&gt;30.76&lt;/td&gt; &#xA;   &lt;td&gt;2.31&lt;/td&gt; &#xA;   &lt;td&gt;82.00&lt;/td&gt; &#xA;   &lt;td&gt;6.26&lt;/td&gt; &#xA;   &lt;td&gt;42.86&lt;/td&gt; &#xA;   &lt;td&gt;0.00&lt;/td&gt; &#xA;   &lt;td&gt;13.77&lt;/td&gt; &#xA;   &lt;td&gt;28.00&lt;/td&gt; &#xA;   &lt;td&gt;52.18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UI-TARS-1.5&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;0.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;   &lt;td&gt;100.00&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Minecraft&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Task Type&lt;/th&gt; &#xA;   &lt;th&gt;Task Name&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://openai.com/index/vpt/&#34;&gt;VPT&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://www.nature.com/articles/s41586-025-08744-2&#34;&gt;DreamerV3&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Previous SOTA&lt;/th&gt; &#xA;   &lt;th&gt;UI-TARS-1.5 w/o Thought&lt;/th&gt; &#xA;   &lt;th&gt;UI-TARS-1.5 w/ Thought&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mine Blocks&lt;/td&gt; &#xA;   &lt;td&gt;(oak_log)&lt;/td&gt; &#xA;   &lt;td&gt;0.8&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(obsidian)&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.2&lt;/td&gt; &#xA;   &lt;td&gt;0.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(white_bed)&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.4&lt;/td&gt; &#xA;   &lt;td&gt;0.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;200 Tasks Avg.&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.06&lt;/td&gt; &#xA;   &lt;td&gt;0.03&lt;/td&gt; &#xA;   &lt;td&gt;0.32&lt;/td&gt; &#xA;   &lt;td&gt;0.35&lt;/td&gt; &#xA;   &lt;td&gt;0.42&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kill Mobs&lt;/td&gt; &#xA;   &lt;td&gt;(mooshroom)&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.3&lt;/td&gt; &#xA;   &lt;td&gt;0.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(zombie)&lt;/td&gt; &#xA;   &lt;td&gt;0.4&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.6&lt;/td&gt; &#xA;   &lt;td&gt;0.7&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(chicken)&lt;/td&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.4&lt;/td&gt; &#xA;   &lt;td&gt;0.5&lt;/td&gt; &#xA;   &lt;td&gt;0.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;100 Tasks Avg.&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.04&lt;/td&gt; &#xA;   &lt;td&gt;0.03&lt;/td&gt; &#xA;   &lt;td&gt;0.18&lt;/td&gt; &#xA;   &lt;td&gt;0.25&lt;/td&gt; &#xA;   &lt;td&gt;0.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Model Scale Comparison&lt;/h2&gt; &#xA;&lt;p&gt;Here we compare performance across different model scales of UI-TARS on the OSworld benchmark.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Benchmark Type&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;UI-TARS-72B-DPO&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;UI-TARS-1.5&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Computer Use&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.07972&#34;&gt;OSWorld&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;24.6&lt;/td&gt; &#xA;   &lt;td&gt;27.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;42.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GUI Grounding&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.07981v1&#34;&gt;ScreenSpotPro&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;38.1&lt;/td&gt; &#xA;   &lt;td&gt;49.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Limitations&lt;/h3&gt; &#xA;&lt;p&gt;While UI-TARS-1.5 represents a significant advancement in multimodal agent capabilities, we acknowledge several important limitations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Misuse:&lt;/strong&gt; Given its enhanced performance in GUI tasks, including successfully navigating authentication challenges like CAPTCHA, UI-TARS-1.5 could potentially be misused for unauthorized access or automation of protected content. To mitigate this risk, extensive internal safety evaluations are underway.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Computation:&lt;/strong&gt; UI-TARS-1.5 still requires substantial computational resources, particularly for large-scale tasks or extended gameplay scenarios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hallucination&lt;/strong&gt;: UI-TARS-1.5 may occasionally generate inaccurate descriptions, misidentify GUI elements, or take suboptimal actions based on incorrect inferences—especially in ambiguous or unfamiliar environments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model scale:&lt;/strong&gt; The released UI-TARS-1.5-7B focuses primarily on enhancing general computer use capabilities and is not specifically optimized for game-based scenarios, where the UI-TARS-1.5 still holds a significant advantage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s next&lt;/h2&gt; &#xA;&lt;p&gt;We are providing early research access to our top-performing UI-TARS-1.5 model to facilitate collaborative research. Interested researchers can contact us at &lt;a href=&#34;mailto:TARS@bytedance.com&#34;&gt;TARS@bytedance.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Looking ahead, we envision UI-TARS evolving into increasingly sophisticated agentic experiences capable of performing real-world actions, thereby empowering platforms such as &lt;a href=&#34;https://team.doubao.com/en/&#34;&gt;doubao&lt;/a&gt; to accomplish more complex tasks for you :)&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#bytedance/UI-TARS&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=bytedance/UI-TARS&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our paper and model useful in your research, feel free to give us a cite.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{qin2025ui,&#xA;  title={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},&#xA;  author={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},&#xA;  journal={arXiv preprint arXiv:2501.12326},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>