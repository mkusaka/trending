<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-12T01:54:47Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vveg26/chromego_merge</title>
    <updated>2023-11-12T01:54:47Z</updated>
    <id>tag:github.com,2023-11-12:/vveg26/chromego_merge</id>
    <link href="https://github.com/vveg26/chromego_merge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;免费节点&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;简介&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意：clash内核无法使用这些节点，你要用clashmeta&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;开启浏览器自带doh以及客户端tun模式也可绕过封锁，参考：&lt;a href=&#34;https://blog.mareep.net/posts/9993/&#34;&gt;https://blog.mareep.net/posts/9993/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;注意事项&lt;/h2&gt; &#xA;&lt;p&gt;套上warp可绕过chromego封锁的网站&lt;/p&gt; &#xA;&lt;h2&gt;如何修改为自己的warp节点&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;点击展开/折叠&lt;/summary&gt; &#xA; &lt;p&gt;可以用warp+机器人和提取wg节点替换掉配置文件中的wg信息&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://replit.com/@misaka-blog/wgcf-profile-generator&#34;&gt;warp提取wireguard网站&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://t.me/generatewarpplusbot&#34;&gt;warp+机器人&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;订阅链接分享&lt;/h2&gt; &#xA;&lt;h3&gt;不套warp版本（clashmeta）&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;不含hysteria2节点&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://mareep.netlify.app/sub/merged_proxies.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;含hysteria2节点(节点最全）&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://mareep.netlify.app/sub/merged_proxies_new.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;套warp版本（clashmeta)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;不含hysteria2节点&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://mareep.netlify.app/sub/merged_warp_proxies.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;含hysteria2节点(节点最全）&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://mareep.netlify.app/sub/merged_warp_proxies_new.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;通用链接 （shadowrocket和nekoray）&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://mareep.netlify.app/sub/shadowrocket_base64.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;sing-box订阅链接&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://sing-box-subscribe.vercel.app/config/https:/mareep.netlify.app/sub/merged_proxies_new.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;客户端推荐&lt;/h2&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zzzgydi/clash-verge/releases&#34;&gt;clash verge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MatsuriDayo/nekoray&#34;&gt;nekoray&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;android&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MatsuriDayo/NekoBoxForAndroid&#34;&gt;nekobox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MetaCubeX/ClashMetaForAndroid/releases&#34;&gt;clashmeta for android&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ios&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;shadowrocket&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;macos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MetaCubeX/ClashX.Meta/releases&#34;&gt;clashx.meta&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zzzgydi/clash-verge/releases&#34;&gt;clash verge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;shadowrocket&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alvin9999/pac2/tree/master&#34;&gt;Alvin9999&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;部分代码逻辑不够优雅&lt;/li&gt; &#xA; &lt;li&gt;sing-box节点的处理&lt;/li&gt; &#xA; &lt;li&gt;xray部分节点的处理&lt;/li&gt; &#xA; &lt;li&gt;融合ss和ssr&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lukas-blecher/LaTeX-OCR</title>
    <updated>2023-11-12T01:54:47Z</updated>
    <id>tag:github.com,2023-11-12:/lukas-blecher/LaTeX-OCR</id>
    <link href="https://github.com/lukas-blecher/LaTeX-OCR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;pix2tex: Using a ViT to convert images of equations into LaTeX code.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pix2tex - LaTeX OCR&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lukas-blecher/LaTeX-OCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pix2tex.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pix2tex/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pix2tex&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pix2tex?logo=pypi&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pix2tex&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/pix2tex?logo=pypi&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lukas-blecher/LaTeX-OCR/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&amp;amp;logo=github&#34; alt=&#34;GitHub all releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/lukasblecher/pix2tex&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/lukbl/LaTeX-OCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Using the model&lt;/h2&gt; &#xA;&lt;p&gt;To run the model you need Python 3.7+&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t have PyTorch installed. Follow their instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Install the package &lt;code&gt;pix2tex&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install &#34;pix2tex[gui]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model checkpoints will be downloaded automatically.&lt;/p&gt; &#xA;&lt;p&gt;There are three ways to get a prediction from an image.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;You can use the command line tool by calling &lt;code&gt;pix2tex&lt;/code&gt;. Here you can parse already existing images from the disk and images in your clipboard.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/katie-lim&#34;&gt;@katie-lim&lt;/a&gt;, you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with &lt;code&gt;latexocr&lt;/code&gt;. From here you can take a screenshot and the predicted latex code is rendered using &lt;a href=&#34;https://www.mathjax.org/&#34;&gt;MathJax&lt;/a&gt; and copied to your clipboard.&lt;/p&gt; &lt;p&gt;Under linux, it is possible to use the GUI with &lt;code&gt;gnome-screenshot&lt;/code&gt; (which comes with multiple monitor support) if &lt;code&gt;gnome-screenshot&lt;/code&gt; was installed beforehand. For Wayland, &lt;code&gt;grim&lt;/code&gt; and &lt;code&gt;slurp&lt;/code&gt; will be used when they are both available. Note that &lt;code&gt;gnome-screenshot&lt;/code&gt; is not compatible with wlroots-based Wayland compositors. Since &lt;code&gt;gnome-screenshot&lt;/code&gt; will be preferred when available, you may have to set the environment variable &lt;code&gt;SCREENSHOT_TOOL&lt;/code&gt; to &lt;code&gt;grim&lt;/code&gt; in this case (other available values are &lt;code&gt;gnome-screenshot&lt;/code&gt; and &lt;code&gt;pil&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &lt;p&gt;If the model is unsure about the what&#39;s in the image it might output a different prediction every time you click &#34;Retry&#34;. With the &lt;code&gt;temperature&lt;/code&gt; parameter you can control this behavior (low temperature will produce the same result).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can use an API. This has additional dependencies. Install via &lt;code&gt;pip install -U &#34;pix2tex[api]&#34;&lt;/code&gt; and run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pix2tex.api.run&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to start a &lt;a href=&#34;https://streamlit.io/&#34;&gt;Streamlit&lt;/a&gt; demo that connects to the API at port 8502. There is also a docker image available for the API: &lt;a href=&#34;https://hub.docker.com/r/lukasblecher/pix2tex&#34;&gt;https://hub.docker.com/r/lukasblecher/pix2tex&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/lukasblecher/pix2tex&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/image-size/lukasblecher/pix2tex?logo=docker&#34; alt=&#34;Docker Image Size (latest by date)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker pull lukasblecher/pix2tex:api&#xA;docker run --rm -p 8502:8502 lukasblecher/pix2tex:api&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To also run the streamlit demo run&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and navigate to &lt;a href=&#34;http://localhost:8501/&#34;&gt;http://localhost:8501/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use from within Python&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from pix2tex.cli import LatexOCR&#xA;&#xA;img = Image.open(&#39;path/to/image.png&#39;)&#xA;model = LatexOCR()&#xA;print(model(img))&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The model works best with images of smaller resolution. That&#39;s why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it&#39;s not perfect and might not be able to handle huge images optimally, so don&#39;t zoom in all the way before taking a picture.&lt;/p&gt; &#xA;&lt;p&gt;Always double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Want to use the package?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m trying to compile a documentation right now.&lt;/p&gt; &#xA;&lt;p&gt;Visit here: &lt;a href=&#34;https://pix2tex.readthedocs.io/&#34;&gt;https://pix2tex.readthedocs.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training the model &lt;a href=&#34;https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_training.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Install a couple of dependencies &lt;code&gt;pip install &#34;pix2tex[train]&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use your own tokenizer pass it via &lt;code&gt;--tokenizer&lt;/code&gt; (See below).&lt;/p&gt; &#xA;&lt;p&gt;You can find my generated training data on the &lt;a href=&#34;https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO&#34;&gt;Google Drive&lt;/a&gt; as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Edit the &lt;code&gt;data&lt;/code&gt; (and &lt;code&gt;valdata&lt;/code&gt;) entry in the config file to the newly generated &lt;code&gt;.pkl&lt;/code&gt; file. Change other hyperparameters if you want to. See &lt;code&gt;pix2tex/model/settings/config.yaml&lt;/code&gt; for a template.&lt;/li&gt; &#xA; &lt;li&gt;Now for the actual training run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pix2tex.train --config path_to_config_file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use your own data you might be interested in creating your own tokenizer with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Don&#39;t forget to update the path to the tokenizer in the config file and set &lt;code&gt;num_tokens&lt;/code&gt; to your vocabulary size.&lt;/p&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;The model consist of a ViT [&lt;a href=&#34;https://raw.githubusercontent.com/lukas-blecher/LaTeX-OCR/main/#References&#34;&gt;1&lt;/a&gt;] encoder with a ResNet backbone and a Transformer [&lt;a href=&#34;https://raw.githubusercontent.com/lukas-blecher/LaTeX-OCR/main/#References&#34;&gt;2&lt;/a&gt;] decoder.&lt;/p&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;BLEU score&lt;/th&gt; &#xA;   &lt;th&gt;normed edit distance&lt;/th&gt; &#xA;   &lt;th&gt;token accuracy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.88&lt;/td&gt; &#xA;   &lt;td&gt;0.10&lt;/td&gt; &#xA;   &lt;td&gt;0.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;We need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. &lt;a href=&#34;https://www.wikipedia.org&#34;&gt;wikipedia&lt;/a&gt;, &lt;a href=&#34;https://www.arxiv.org&#34;&gt;arXiv&lt;/a&gt;. We also use the formulae from the &lt;a href=&#34;https://zenodo.org/record/56198#.V2px0jXT6eA&#34;&gt;im2latex-100k&lt;/a&gt; [&lt;a href=&#34;https://raw.githubusercontent.com/lukas-blecher/LaTeX-OCR/main/#References&#34;&gt;3&lt;/a&gt;] dataset. All of it can be found &lt;a href=&#34;https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Requirements&lt;/h3&gt; &#xA;&lt;p&gt;In order to render the math in many different fonts we use XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ctan.org/pkg/xetex&#34;&gt;XeLaTeX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/&#34;&gt;ImageMagick&lt;/a&gt; with &lt;a href=&#34;https://www.ghostscript.com/index.html&#34;&gt;Ghostscript&lt;/a&gt;. (for converting pdf to png)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nodejs.org/&#34;&gt;Node.js&lt;/a&gt; to run &lt;a href=&#34;https://github.com/KaTeX/KaTeX&#34;&gt;KaTeX&lt;/a&gt; (for normalizing Latex code)&lt;/li&gt; &#xA; &lt;li&gt;Python 3.7+ &amp;amp; dependencies (specified in &lt;code&gt;setup.py&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Fonts&lt;/h3&gt; &#xA;&lt;p&gt;Latin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add more evaluation metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; create a GUI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add beam search&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; support handwritten formulae (kinda done, see training colab notebook)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; reduce model size (distillation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; find optimal hyperparameters&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; tweak model structure&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; fix data scraping and scrape more data&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; trace the model (&lt;a href=&#34;https://github.com/lukas-blecher/LaTeX-OCR/issues/2&#34;&gt;#2&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Contributions of any kind are welcome.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;Code taken and modified from &lt;a href=&#34;https://github.com/lucidrains&#34;&gt;lucidrains&lt;/a&gt;, &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;rwightman&lt;/a&gt;, &lt;a href=&#34;https://github.com/harvardnlp/im2markup&#34;&gt;im2markup&lt;/a&gt;, &lt;a href=&#34;https://github.com/soskek/arxiv_leaks&#34;&gt;arxiv_leaks&lt;/a&gt;, &lt;a href=&#34;https://github.com/pkra/MathJax-single-file&#34;&gt;pkra: Mathjax&lt;/a&gt;, &lt;a href=&#34;https://github.com/harupy/snipping-tool&#34;&gt;harupy: snipping tool&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;[1] &lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;An Image is Worth 16x16 Words&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2] &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[3] &lt;a href=&#34;https://arxiv.org/abs/1609.04938v2&#34;&gt;Image-to-Markup Generation with Coarse-to-Fine Attention&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>paul-gauthier/aider</title>
    <updated>2023-11-12T01:54:47Z</updated>
    <id>tag:github.com,2023-11-12:/paul-gauthier/aider</id>
    <link href="https://github.com/paul-gauthier/aider" rel="alternate"></link>
    <summary type="html">&lt;p&gt;aider is AI pair programming in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;aider is AI pair programming in your terminal&lt;/h1&gt; &#xA;&lt;p&gt;Aider is a command line tool that lets you pair program with GPT-3.5/GPT-4, to edit code stored in your local git repository. You can start a new project or work with an existing repo. Aider makes sure edits from GPT are &lt;a href=&#34;https://aider.chat/docs/faq.html#how-does-aider-use-git&#34;&gt;committed to git&lt;/a&gt; with sensible commit messages. Aider is unique in that it &lt;a href=&#34;https://aider.chat/docs/repomap.html&#34;&gt;works well with pre-existing, larger codebases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/assets/screencast.svg?sanitize=true&#34; alt=&#34;aider screencast&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/Tv2uQnR88V&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Join-Discord-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#getting-started&#34;&gt;Getting started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#example-chat-transcripts&#34;&gt;Example chat transcripts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#in-chat-commands&#34;&gt;In-chat commands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/paul-gauthier/aider/main/#tips&#34;&gt;Tips&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aider.chat/docs/faq.html#gpt-4-vs-gpt-35&#34;&gt;GPT-4 vs GPT-3.5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aider.chat/docs/install.html&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aider.chat/docs/voice.html&#34;&gt;Voice-to-code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aider.chat/docs/faq.html&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/Tv2uQnR88V&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;New GPT-4 Turbo with 128k context window&lt;/h2&gt; &#xA;&lt;p&gt;Aider supports OpenAI&#39;s new GPT-4 model that has the massive 128k context window. Early benchmark results indicate that it is &lt;a href=&#34;https://aider.chat/docs/benchmarks-speed-1106.html&#34;&gt;very fast&lt;/a&gt; and a bit &lt;a href=&#34;https://aider.chat/docs/benchmarks-1106.html&#34;&gt;better at coding&lt;/a&gt; than previous GPT-4 models.&lt;/p&gt; &#xA;&lt;p&gt;To use it, run aider like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;aider --model gpt-4-1106-preview&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://aider.chat/docs/install.html&#34;&gt;installation instructions&lt;/a&gt; for more details, but you can get started quickly like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install aider-chat&#xA;$ export OPENAI_API_KEY=your-key-goes-here&#xA;$ aider hello.js&#xA;&#xA;Using git repo: .git&#xA;Added hello.js to the chat.&#xA;&#xA;hello.js&amp;gt; write a js script that prints hello world&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example chat transcripts&lt;/h2&gt; &#xA;&lt;p&gt;Here are some example transcripts that show how you can chat with &lt;code&gt;aider&lt;/code&gt; to write and edit code with GPT-4.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://aider.chat/examples/hello-world-flask.html&#34;&gt;&lt;strong&gt;Hello World Flask App&lt;/strong&gt;&lt;/a&gt;: Start from scratch and have GPT create a simple Flask app with various endpoints, such as adding two numbers and calculating the Fibonacci sequence.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://aider.chat/examples/2048-game.html&#34;&gt;&lt;strong&gt;Javascript Game Modification&lt;/strong&gt;&lt;/a&gt;: Dive into an existing open-source repo, and get GPT&#39;s help to understand it and make modifications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://aider.chat/examples/complex-change.html&#34;&gt;&lt;strong&gt;Complex Multi-file Change with Debugging&lt;/strong&gt;&lt;/a&gt;: GPT makes a complex code change that is coordinated across multiple source files, and resolves bugs by reviewing error output and doc snippets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://aider.chat/examples/add-test.html&#34;&gt;&lt;strong&gt;Create a Black Box Test Case&lt;/strong&gt;&lt;/a&gt;: GPT creates a &#34;black box&#34; test case without access to the source of the method being tested, using only a &lt;a href=&#34;https://aider.chat/docs/repomap.html&#34;&gt;high level map of the repository based on tree-sitter&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can find more chat transcripts on the &lt;a href=&#34;https://aider.chat/examples/&#34;&gt;examples page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chat with GPT about your code by launching &lt;code&gt;aider&lt;/code&gt; from the command line with set of source files to discuss and edit together. Aider lets GPT see and edit the content of those files.&lt;/li&gt; &#xA; &lt;li&gt;GPT can write and edit code in most popular languages: python, javascript, typescript, html, css, etc.&lt;/li&gt; &#xA; &lt;li&gt;Request new features, changes, improvements, or bug fixes to your code. Ask for new test cases, updated documentation or code refactors.&lt;/li&gt; &#xA; &lt;li&gt;Aider will apply the edits suggested by GPT directly to your source files.&lt;/li&gt; &#xA; &lt;li&gt;Aider will &lt;a href=&#34;https://aider.chat/docs/faq.html#how-does-aider-use-git&#34;&gt;automatically commit each changeset to your local git repo&lt;/a&gt; with a descriptive commit message. These frequent, automatic commits provide a safety net. It&#39;s easy to undo changes or use standard git workflows to manage longer sequences of changes.&lt;/li&gt; &#xA; &lt;li&gt;You can use aider with multiple source files at once, so GPT can make coordinated code changes across all of them in a single changeset/commit.&lt;/li&gt; &#xA; &lt;li&gt;Aider can &lt;a href=&#34;https://aider.chat/docs/repomap.html&#34;&gt;give &lt;em&gt;GPT-4&lt;/em&gt; a map of your entire git repo&lt;/a&gt;, which helps it understand and modify large codebases.&lt;/li&gt; &#xA; &lt;li&gt;You can also edit files by hand using your editor while chatting with aider. Aider will notice these out-of-band edits and keep GPT up to date with the latest versions of your files. This lets you bounce back and forth between the aider chat and your editor, to collaboratively code with GPT.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Run the &lt;code&gt;aider&lt;/code&gt; tool by executing the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;aider &amp;lt;file1&amp;gt; &amp;lt;file2&amp;gt; ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your pip install did not place the &lt;code&gt;aider&lt;/code&gt; executable on your path, you can invoke aider like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m aider.main &amp;lt;file1&amp;gt; &amp;lt;file2&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;&amp;lt;file1&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;file2&amp;gt;&lt;/code&gt;, etc., with the paths to the source code files you want to work on. These files will be &#34;added to the chat session&#34;, so that GPT can see their contents and edit them according to your instructions.&lt;/p&gt; &#xA;&lt;p&gt;You can also just launch &lt;code&gt;aider&lt;/code&gt; anywhere in a git repo without naming files on the command line. It will discover all the files in the repo. You can then add and remove individual files in the chat session with the &lt;code&gt;/add&lt;/code&gt; and &lt;code&gt;/drop&lt;/code&gt; chat commands described below. If you or GPT mention one of the repo&#39;s filenames in the conversation, aider will ask if you&#39;d like to add it to the chat.&lt;/p&gt; &#xA;&lt;p&gt;Aider will work best if you think about which files need to be edited to make your change and add them to the chat. Aider has some ability to help GPT figure out which files to edit all by itself, but the most effective approach is to explicitly add the needed files to the chat yourself.&lt;/p&gt; &#xA;&lt;p&gt;Aider also has many additional command-line options, environment variables or configuration file to set many options. See &lt;code&gt;aider --help&lt;/code&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;In-chat commands&lt;/h2&gt; &#xA;&lt;p&gt;Aider supports commands from within the chat, which all start with &lt;code&gt;/&lt;/code&gt;. Here are some of the most useful in-chat commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/add &amp;lt;file&amp;gt;&lt;/code&gt;: Add matching files to the chat session.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/drop &amp;lt;file&amp;gt;&lt;/code&gt;: Remove matching files from the chat session.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/undo&lt;/code&gt;: Undo the last git commit if it was done by aider.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/diff&lt;/code&gt;: Display the diff of the last aider commit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/run &amp;lt;command&amp;gt;&lt;/code&gt;: Run a shell command and optionally add the output to the chat.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/voice&lt;/code&gt;: Speak to aider to &lt;a href=&#34;https://aider.chat/docs/voice.html&#34;&gt;request code changes with your voice&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/help&lt;/code&gt;: Show help about all commands.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://aider.chat/docs/commands.html&#34;&gt;full command docs&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Think about which files need to be edited to make your change and add them to the chat. Aider has some ability to help GPT figure out which files to edit all by itself, but the most effective approach is to explicitly add the needed files to the chat yourself.&lt;/li&gt; &#xA; &lt;li&gt;Large changes are best performed as a sequence of thoughtful bite sized steps, where you plan out the approach and overall design. Walk GPT through changes like you might with a junior dev. Ask for a refactor to prepare, then ask for the actual change. Spend the time to ask for code quality/structure improvements.&lt;/li&gt; &#xA; &lt;li&gt;Use Control-C to safely interrupt GPT if it isn&#39;t providing a useful response. The partial response remains in the conversation, so you can refer to it when you reply to GPT with more information or direction.&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;code&gt;/run&lt;/code&gt; command to run tests, linters, etc and show the output to GPT so it can fix any issues.&lt;/li&gt; &#xA; &lt;li&gt;Use Meta-ENTER (Esc+ENTER in some environments) to enter multiline chat messages. Or enter &lt;code&gt;{&lt;/code&gt; alone on the first line to start a multiline message and &lt;code&gt;}&lt;/code&gt; alone on the last line to end it.&lt;/li&gt; &#xA; &lt;li&gt;If your code is throwing an error, share the error output with GPT using &lt;code&gt;/run&lt;/code&gt; or by pasting it into the chat. Let GPT figure out and fix the bug.&lt;/li&gt; &#xA; &lt;li&gt;GPT knows about a lot of standard tools and libraries, but may get some of the fine details wrong about APIs and function arguments. You can paste doc snippets into the chat to resolve these issues.&lt;/li&gt; &#xA; &lt;li&gt;GPT can only see the content of the files you specifically &#34;add to the chat&#34;. Aider also sends GPT-4 a &lt;a href=&#34;https://aider.chat/docs/repomap.html&#34;&gt;map of your entire git repo&lt;/a&gt;. So GPT may ask to see additional files if it feels that&#39;s needed for your requests.&lt;/li&gt; &#xA; &lt;li&gt;I also shared some general &lt;a href=&#34;https://news.ycombinator.com/item?id=36211879&#34;&gt;GPT coding tips on Hacker News&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;GPT-4 vs GPT-3.5&lt;/h2&gt; &#xA;&lt;p&gt;Aider supports all of OpenAI&#39;s chat models. You can choose a model with the &lt;code&gt;--model&lt;/code&gt; command line argument.&lt;/p&gt; &#xA;&lt;p&gt;You should probably use GPT-4 if you can. For more details see the &lt;a href=&#34;https://aider.chat/docs/faq.html#gpt-4-vs-gpt-35&#34;&gt;FAQ entry that compares GPT-4 vs GPT-3.5&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a discussion of using other non-OpenAI models, see the &lt;a href=&#34;https://aider.chat/docs/faq.html#can-i-use-aider-with-other-llms-local-llms-etc&#34;&gt;FAQ about other LLMs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://aider.chat/docs/install.html&#34;&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;For more information, see the &lt;a href=&#34;https://aider.chat/docs/faq.html&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Kind words from users&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;The best AI coding assistant so far.&lt;/em&gt; -- &lt;a href=&#34;https://www.youtube.com/watch?v=df8afeb1FY8&#34;&gt;Matthew Berman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Hands down, this is the best AI coding assistant tool so far.&lt;/em&gt; -- &lt;a href=&#34;https://www.youtube.com/watch?v=MPYFPvxfGZs&#34;&gt;IndyDevDan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Aider ... has easily quadrupled my coding productivity.&lt;/em&gt; -- &lt;a href=&#34;https://news.ycombinator.com/item?id=36212100&#34;&gt;SOLAR_FIELDS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;It&#39;s a cool workflow... Aider&#39;s ergonomics are perfect for me.&lt;/em&gt; -- &lt;a href=&#34;https://news.ycombinator.com/item?id=38185326&#34;&gt;qup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;It&#39;s really like having your senior developer live right in your Git repo - truly amazing!&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/124&#34;&gt;rappster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;What an amazing tool. It&#39;s incredible.&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/6#issue-1722897858&#34;&gt;valyagolev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Aider is such an astounding thing!&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/82#issuecomment-1631876700&#34;&gt;cgrothaus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;It was WAY faster than I would be getting off the ground and making the first few working versions.&lt;/em&gt; -- &lt;a href=&#34;https://twitter.com/d_feldman/status/1662295077387923456&#34;&gt;Daniel Feldman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;THANK YOU for Aider! It really feels like a glimpse into the future of coding.&lt;/em&gt; -- &lt;a href=&#34;https://news.ycombinator.com/item?id=38205643&#34;&gt;derwiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;This project is stellar.&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/112#issuecomment-1637429008&#34;&gt;funkytaco&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Amazing project, definitely the best AI coding assistant I&#39;ve used.&lt;/em&gt; -- &lt;a href=&#34;https://github.com/paul-gauthier/aider/issues/84&#34;&gt;joshuavial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;I am an aider addict. I&#39;m getting so much more work done, but in less time.&lt;/em&gt; -- &lt;a href=&#34;https://discord.com/channels/1131200896827654144/1131200896827654149/1135913253483069470&#34;&gt;dandandan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Best agent for actual dev work in existing codebases.&lt;/em&gt; -- &lt;a href=&#34;https://twitter.com/NickADobos/status/1690408967963652097?s=20&#34;&gt;Nick Dobos&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>