<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-25T01:52:51Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/UFO</title>
    <updated>2024-02-25T01:52:51Z</updated>
    <id>tag:github.com,2024-02-25:/microsoft/UFO</id>
    <link href="https://github.com/microsoft/UFO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A UI-Focused Agent for Windows OS Interaction.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;b&gt;UFO&lt;/b&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/ufo_blue.png&#34; alt=&#34;UFO Image&#34; width=&#34;40&#34;&gt;: A &lt;b&gt;U&lt;/b&gt;I-&lt;b&gt;F&lt;/b&gt;ocused Agent for Windows &lt;b&gt;O&lt;/b&gt;S Interaction &lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.07939&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-arXiv:202402.07939-b31b1b.svg?sanitize=true&#34; alt=&#34;arxiv&#34;&gt;&lt;/a&gt;‚ÄÇ &lt;img src=&#34;https://img.shields.io/badge/Python-3776AB?&amp;amp;logo=python&amp;amp;logoColor=white-blue&amp;amp;label=3.10%20%7C%203.11&#34; alt=&#34;Python Version&#34;&gt;‚ÄÇ &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;‚ÄÇ &lt;img src=&#34;https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&#34; alt=&#34;Welcome&#34;&gt;‚ÄÇ &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=UFO_Agent&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/UFO_Agent&#34; alt=&#34;X (formerly Twitter) Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;UFO&lt;/strong&gt; is a &lt;strong&gt;UI-Focused&lt;/strong&gt; dual-agent framework to fulfill user requests on &lt;strong&gt;Windows OS&lt;/strong&gt; by seamlessly navigating and operating within individual or spanning multiple applications.&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/overview_n.png&#34;&gt; &lt;/h1&gt; &#xA;&lt;h2&gt;üïå Framework&lt;/h2&gt; &#xA;&lt;p&gt;&lt;b&gt;UFO&lt;/b&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/ufo_blue.png&#34; alt=&#34;UFO Image&#34; width=&#34;24&#34;&gt; operates as a dual-agent framework, encompassing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;AppAgent ü§ñ&lt;/b&gt;, tasked with choosing an application for fulfilling user requests. This agent may also switch to a different application when a request spans multiple applications, and the task is partially completed in the preceding application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;ActAgent üëæ&lt;/b&gt;, responsible for iteratively executing actions on the selected applications until the task is successfully concluded within a specific application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;Control Interaction üéÆ&lt;/b&gt;, is tasked with translating actions from AppAgent and ActAgent into interactions with the application and its UI controls. It&#39;s essential that the targeted controls are compatible with the Windows &lt;strong&gt;UI Automation&lt;/strong&gt; API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Both agents leverage the multi-modal capabilities of GPT-Vision to comprehend the application UI and fulfill the user&#39;s request. For more details, please consult our &lt;a href=&#34;https://arxiv.org/abs/2402.07939&#34;&gt;technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/framework.png&#34;&gt; &lt;/h1&gt; &#xA;&lt;h2&gt;üì¢ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìÖ 2024-02-14: Our &lt;a href=&#34;https://arxiv.org/abs/2402.07939&#34;&gt;technical report&lt;/a&gt; is online!&lt;/li&gt; &#xA; &lt;li&gt;üìÖ 2024-02-10: UFO is released on GitHubüéà. Happy Chinese New yearüêâ!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üí• Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;First Windows Agent&lt;/strong&gt; - UFO is the pioneering agent framework capable of translating user requests in natural language into actionable operations on Windows OS.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Interactive Mode&lt;/strong&gt; - UFO facilitates multiple sub-requests from users within the same session, enabling the completion of complex tasks seamlessly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Action Safeguard&lt;/strong&gt; - UFO incorporates safeguards to prompt user confirmation for sensitive actions, enhancing security and preventing inadvertent operations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Easy Extension&lt;/strong&gt; - UFO offers extensibility, allowing for the integration of additional functionalities and control types to tackle diverse and intricate tasks with ease.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ú® Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;üõ†Ô∏è Step 1: Installation&lt;/h3&gt; &#xA;&lt;p&gt;UFO requires &lt;strong&gt;Python &amp;gt;= 3.10&lt;/strong&gt; running on &lt;strong&gt;Windows OS &amp;gt;= 10&lt;/strong&gt;. It can be installed by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# [optional to create conda environment]&#xA;# conda create -n ufo python=3.10&#xA;# conda activate ufo&#xA;&#xA;# clone the repository&#xA;git clone https://github.com/microsoft/UFO.git&#xA;cd UFO&#xA;# install the requirements&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;‚öôÔ∏è Step 2: Configure the LLMs&lt;/h3&gt; &#xA;&lt;p&gt;Before running UFO, you need to provide your LLM configurations. You can configure &lt;code&gt;ufo/config/config.yaml&lt;/code&gt; file as follows.&lt;/p&gt; &#xA;&lt;h4&gt;OpenAI&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;API_TYPE: &#34;openai&#34; &#xA;OPENAI_API_BASE: &#34;https://api.openai.com/v1/chat/completions&#34; # The base URL for the OpenAI API&#xA;OPENAI_API_KEY: &#34;YOUR_API_KEY&#34;  # Set the value to the openai key for the llm model&#xA;OPENAI_API_MODEL: &#34;GPTV_MODEL_NAME&#34;  # The only OpenAI model by now that accepts visual input&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Azure OpenAI (AOAI)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;API_TYPE: &#34;aoai&#34; &#xA;OPENAI_API_BASE: &#34;YOUR_ENDPOINT&#34; # The AOAI API address. Format: https://{your-resource-name}.openai.azure.com/openai/deployments/{deployment-id}/chat/completions?api-version={api-version}&#xA;OPENAI_API_KEY: &#34;YOUR_API_KEY&#34;  # Set the value to the openai key for the llm model&#xA;OPENAI_API_MODEL: &#34;GPTV_MODEL_NAME&#34;  # The only OpenAI model by now that accepts visual input&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üéâ Step 3: Start UFO&lt;/h3&gt; &#xA;&lt;h4&gt;‚å®Ô∏è You can execute the following on your Windows command Line (CLI):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# assume you are in the cloned UFO folder&#xA;python -m ufo --task &amp;lt;your_task_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will start the UFO process and you can interact with it through the command line interface. If everything goes well, you will see the following message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Welcome to use UFOüõ∏, A UI-focused Agent for Windows OS Interaction. &#xA; _   _  _____   ___&#xA;| | | ||  ___| / _ \&#xA;| | | || |_   | | | |&#xA;| |_| ||  _|  | |_| |&#xA; \___/ |_|     \___/&#xA;Please enter your request to be completedüõ∏:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;‚ö†Ô∏èReminder:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before UFO executing your request, please make sure the targeted applications are active on the system.&lt;/li&gt; &#xA; &lt;li&gt;The GPT-V accepts screenshots of your desktop and application GUI as input. Please ensure that no sensitive or confidential information is visible or captured during the execution process. For further information, refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/DISCLAIMER.md&#34;&gt;DISCLAIMER.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step 4 üé•: Execution Logs&lt;/h3&gt; &#xA;&lt;p&gt;You can find the screenshots taken and request &amp;amp; response logs in the following folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ufo/logs/&amp;lt;your_task_name&amp;gt;/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may use them to debug, replay, or analyze the agent output.&lt;/p&gt; &#xA;&lt;h2&gt;‚ùìGet help&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ùîGitHub Issues (prefered)&lt;/li&gt; &#xA; &lt;li&gt;For other communications, please contact &lt;a href=&#34;mailto:ufo-agent@microsoft.com&#34;&gt;ufo-agent@microsoft.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üé¨ Demo Examples&lt;/h2&gt; &#xA;&lt;p&gt;We present two demo videos that complete user request on Windows OS using UFO. For more case study, please consult our &lt;a href=&#34;https://arxiv.org/abs/2402.07939&#34;&gt;technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;1Ô∏è‚É£üóëÔ∏è Example 1: Deleting all notes on a PowerPoint presentation.&lt;/h4&gt; &#xA;&lt;p&gt;In this example, we will demonstrate how to efficiently use UFO to delete all notes on a PowerPoint presentation with just a few simple steps. Explore this functionality to enhance your productivity and work smarter, not harder!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/UFO/assets/11352048/cf60c643-04f7-4180-9a55-5fb240627834&#34;&gt;https://github.com/microsoft/UFO/assets/11352048/cf60c643-04f7-4180-9a55-5fb240627834&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2Ô∏è‚É£üìß Example 2: Composing an email using text from multiple sources.&lt;/h4&gt; &#xA;&lt;p&gt;In this example, we will demonstrate how to utilize UFO to extract text from Word documents, describe an image, compose an email, and send it seamlessly. Enjoy the versatility and efficiency of cross-application experiences with UFO!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/UFO/assets/11352048/aa41ad47-fae7-4334-8e0b-ba71c4fc32e0&#34;&gt;https://github.com/microsoft/UFO/assets/11352048/aa41ad47-fae7-4334-8e0b-ba71c4fc32e0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìä Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Please consult the &lt;a href=&#34;https://arxiv.org/pdf/2402.07939.pdf&#34;&gt;WindowsBench&lt;/a&gt; provided in Section A of the Appendix within our technical report. Here are some tips (and requirements) to aid in completing your request:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Prior to UFO execution of your request, ensure that the targeted application is active (though it may be minimized).&lt;/li&gt; &#xA; &lt;li&gt;Occasionally, requests to GPT-V may trigger content safety measures. UFO will attempt to retry regardless, but adjusting the size or scale of the application window may prove helpful. We are actively solving this issue.&lt;/li&gt; &#xA; &lt;li&gt;Currently, UFO supports a limited set of applications and UI controls that are compatible with the Windows &lt;strong&gt;UI Automation&lt;/strong&gt; API. Our future plans include extending support to the Win32 API to enhance its capabilities.&lt;/li&gt; &#xA; &lt;li&gt;Please note that the output of GPT-V may not consistently align with the same request. If unsuccessful with your initial attempt, consider trying again.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Citation&lt;/h2&gt; &#xA;&lt;p&gt;Our technical report paper can be found &lt;a href=&#34;https://arxiv.org/abs/2402.07939&#34;&gt;here&lt;/a&gt;. If you use UFO in your research, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{ufo,&#xA;  title={{UFO: A UI-Focused Agent for Windows OS Interaction}},&#xA;  author={Zhang, Chaoyun and Li, Liqun and He, Shilin and  Zhang, Xu and Qiao, Bo and  Qin, Si and Ma, Minghua and Kang, Yu and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei and  Zhang, Qi},&#xA;  journal={arXiv preprint arXiv:2402.07939},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìù Todo List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚è© Documentation.&lt;/li&gt; &#xA; &lt;li&gt;‚è© Support local host GUI interaction model.&lt;/li&gt; &#xA; &lt;li&gt;‚è© Support more control using Win32 API.&lt;/li&gt; &#xA; &lt;li&gt;‚è© RAG enhanced UFO.&lt;/li&gt; &#xA; &lt;li&gt;‚è© Chatbox GUI for UFO.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üé® Related Project&lt;/h2&gt; &#xA;&lt;p&gt;You may also find &lt;a href=&#34;https://github.com/microsoft/TaskWeaver?tab=readme-ov-file&#34;&gt;TaskWeaver&lt;/a&gt; useful, a code-first LLM agent framework for seamlessly planning and executing data analytics tasks.&lt;/p&gt; &#xA;&lt;h2&gt;‚ö†Ô∏è Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;By choosing to run the provided code, you acknowledge and agree to the following terms and conditions regarding the functionality and data handling practices in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/DISCLAIMER.md&#34;&gt;DISCLAIMER.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/ufo_blue.png&#34; alt=&#34;logo&#34; width=&#34;30&#34;&gt; Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-deepmind/graphcast</title>
    <updated>2024-02-25T01:52:51Z</updated>
    <id>tag:github.com,2024-02-25:/google-deepmind/graphcast</id>
    <link href="https://github.com/google-deepmind/graphcast" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GraphCast: Learning skillful medium-range global weather forecasting&lt;/h1&gt; &#xA;&lt;p&gt;This package contains example code to run and train &lt;a href=&#34;https://arxiv.org/abs/2212.12794&#34;&gt;GraphCast&lt;/a&gt;. It also provides three pretrained models:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;GraphCast&lt;/code&gt;, the high-resolution model used in the GraphCast paper (0.25 degree resolution, 37 pressure levels), trained on ERA5 data from 1979 to 2017,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;GraphCast_small&lt;/code&gt;, a smaller, low-resolution version of GraphCast (1 degree resolution, 13 pressure levels, and a smaller mesh), trained on ERA5 data from 1979 to 2015, useful to run a model with lower memory and compute constraints,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;GraphCast_operational&lt;/code&gt;, a high-resolution model (0.25 degree resolution, 13 pressure levels) pre-trained on ERA5 data from 1979 to 2017 and fine-tuned on HRES data from 2016 to 2021. This model can be initialized from HRES data (does not require precipitation inputs).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The model weights, normalization statistics, and example inputs are available on &lt;a href=&#34;https://console.cloud.google.com/storage/browser/dm_graphcast&#34;&gt;Google Cloud Bucket&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Full model training requires downloading the &lt;a href=&#34;https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5&#34;&gt;ERA5&lt;/a&gt; dataset, available from &lt;a href=&#34;https://www.ecmwf.int/&#34;&gt;ECMWF&lt;/a&gt;. This can best be accessed as Zarr from &lt;a href=&#34;https://weatherbench2.readthedocs.io/en/latest/data-guide.html#era5&#34;&gt;Weatherbench2&#39;s ERA5 data&lt;/a&gt; (see the 6h downsampled versions).&lt;/p&gt; &#xA;&lt;h2&gt;Overview of files&lt;/h2&gt; &#xA;&lt;p&gt;The best starting point is to open &lt;code&gt;graphcast_demo.ipynb&lt;/code&gt; in &lt;a href=&#34;https://colab.research.google.com/github/deepmind/graphcast/blob/master/graphcast_demo.ipynb&#34;&gt;Colaboratory&lt;/a&gt;, which gives an example of loading data, generating random weights or load a pre-trained snapshot, generating predictions, computing the loss and computing gradients. The one-step implementation of GraphCast architecture, is provided in &lt;code&gt;graphcast.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Brief description of library files:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;autoregressive.py&lt;/code&gt;: Wrapper used to run (and train) the one-step GraphCast to produce a sequence of predictions by auto-regressively feeding the outputs back as inputs at each step, in JAX a differentiable way.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;casting.py&lt;/code&gt;: Wrapper used around GraphCast to make it work using BFloat16 precision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;checkpoint.py&lt;/code&gt;: Utils to serialize and deserialize trees.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;data_utils.py&lt;/code&gt;: Utils for data preprocessing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;deep_typed_graph_net.py&lt;/code&gt;: General purpose deep graph neural network (GNN) that operates on &lt;code&gt;TypedGraph&lt;/code&gt;&#39;s where both inputs and outputs are flat vectors of features for each of the nodes and edges. &lt;code&gt;graphcast.py&lt;/code&gt; uses three of these for the Grid2Mesh GNN, the Multi-mesh GNN and the Mesh2Grid GNN, respectively.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;graphcast.py&lt;/code&gt;: The main GraphCast model architecture for one-step of predictions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;grid_mesh_connectivity.py&lt;/code&gt;: Tools for converting between regular grids on a sphere and triangular meshes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;icosahedral_mesh.py&lt;/code&gt;: Definition of an icosahedral multi-mesh.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;losses.py&lt;/code&gt;: Loss computations, including latitude-weighting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_utils.py&lt;/code&gt;: Utilities to produce flat node and edge vector features from input grid data, and to manipulate the node output vectors back into a multilevel grid data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;normalization.py&lt;/code&gt;: Wrapper for the one-step GraphCast used to normalize inputs according to historical values, and targets according to historical time differences.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;predictor_base.py&lt;/code&gt;: Defines the interface of the predictor, which GraphCast and all of the wrappers implement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rollout.py&lt;/code&gt;: Similar to &lt;code&gt;autoregressive.py&lt;/code&gt; but used only at inference time using a python loop to produce longer, but non-differentiable trajectories.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;solar_radiation.py&lt;/code&gt;: Computes Top-Of-the-Atmosphere (TOA) incident solar radiation compatible with ERA5. This is used as a forcing variable and thus needs to be computed for target lead times in an operational setting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;typed_graph.py&lt;/code&gt;: Definition of &lt;code&gt;TypedGraph&lt;/code&gt;&#39;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;typed_graph_net.py&lt;/code&gt;: Implementation of simple graph neural network building blocks defined over &lt;code&gt;TypedGraph&lt;/code&gt;&#39;s that can be combined to build deeper models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;xarray_jax.py&lt;/code&gt;: A wrapper to let JAX work with &lt;code&gt;xarray&lt;/code&gt;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;xarray_tree.py&lt;/code&gt;: An implementation of tree.map_structure that works with &lt;code&gt;xarray&lt;/code&gt;s.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dependencies.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/deepmind/chex&#34;&gt;Chex&lt;/a&gt;, &lt;a href=&#34;https://github.com/dask/dask&#34;&gt;Dask&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/dm-haiku&#34;&gt;Haiku&lt;/a&gt;, &lt;a href=&#34;https://github.com/google/jax&#34;&gt;JAX&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/jaxline&#34;&gt;JAXline&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/jraph&#34;&gt;Jraph&lt;/a&gt;, &lt;a href=&#34;https://numpy.org/&#34;&gt;Numpy&lt;/a&gt;, &lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;Pandas&lt;/a&gt;, &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;, &lt;a href=&#34;https://scipy.org/&#34;&gt;SciPy&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/tree&#34;&gt;Tree&lt;/a&gt;, &lt;a href=&#34;https://github.com/mikedh/trimesh&#34;&gt;Trimesh&lt;/a&gt; and &lt;a href=&#34;https://github.com/pydata/xarray&#34;&gt;XArray&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;License and attribution&lt;/h3&gt; &#xA;&lt;p&gt;The Colab notebook and the associated code are licensed under the Apache License, Version 2.0. You may obtain a copy of the License at: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The model weights are made available for use under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0). You may obtain a copy of the License at: &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;https://creativecommons.org/licenses/by-nc-sa/4.0/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The weights were trained on ECMWF&#39;s ERA5 and HRES data. The colab includes a few examples of ERA5 and HRES data that can be used as inputs to the models. ECMWF data product are subject to the following terms:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copyright statement: Copyright &#34;¬© 2023 European Centre for Medium-Range Weather Forecasts (ECMWF)&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Source &lt;a href=&#34;http://www.ecmwf.int&#34;&gt;www.ecmwf.int&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Licence Statement: ECMWF data is published under a Creative Commons Attribution 4.0 International (CC BY 4.0). &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;https://creativecommons.org/licenses/by/4.0/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Disclaimer: ECMWF does not accept any liability whatsoever for any error or omission in the data, their availability, or for any loss or damage arising from their use.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2023 DeepMind Technologies Limited.&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;If you use this work, consider citing our &lt;a href=&#34;https://arxiv.org/abs/2212.12794&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@article{lam2022graphcast,&#xA;      title={GraphCast: Learning skillful medium-range global weather forecasting},&#xA;      author={Remi Lam and Alvaro Sanchez-Gonzalez and Matthew Willson and Peter Wirnsberger and Meire Fortunato and Alexander Pritzel and Suman Ravuri and Timo Ewalds and Ferran Alet and Zach Eaton-Rosen and Weihua Hu and Alexander Merose and Stephan Hoyer and George Holland and Jacklynn Stott and Oriol Vinyals and Shakir Mohamed and Peter Battaglia},&#xA;      year={2022},&#xA;      eprint={2212.12794},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Hillobar/Rope</title>
    <updated>2024-02-25T01:52:51Z</updated>
    <id>tag:github.com,2024-02-25:/Hillobar/Rope</id>
    <link href="https://github.com/Hillobar/Rope" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GUI-focused roop&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/Hillobar/Rope/assets/63615199/dd8ab00b-d55f-4196-a50b-f2a326fba83a&#34; alt=&#34;Screenshot 2024-02-10 091752&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Rope implements the insightface inswapper_128 model with a helpful GUI.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://discord.gg/EcdVAFJzqp&#34;&gt;Discord&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.paypal.com/donate/?hosted_button_id=Y5SB9LSXFGRF2&#34;&gt;Donate&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/Hillobar/Rope/wiki&#34;&gt;Wiki with install instructions and usage&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4Y4U0TZ8cWY&#34;&gt;Demo Video (Rope-Ruby)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;${{\color{Goldenrod}{\textsf{Last Updated 2024-02-16}}}}$&lt;/h3&gt; &#xA;&lt;h3&gt;${{\color{Goldenrod}{\textsf{Welcome to Rope-Opal!}}}}$&lt;/h3&gt; &#xA;&lt;h3&gt;${{\color{Red}{\textsf{Please grab the latest yoloface model from the link in the wiki!}}}}$&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Hillobar/Rope/assets/63615199/4b2ee574-c91e-4db2-ad66-5b775a049a6b&#34; alt=&#34;Screenshot 2024-02-10 104718&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Features:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lightning speed face swapping with all the features&lt;/li&gt; &#xA; &lt;li&gt;Upscalers&lt;/li&gt; &#xA; &lt;li&gt;Likeness modifiers&lt;/li&gt; &#xA; &lt;li&gt;Orientation management&lt;/li&gt; &#xA; &lt;li&gt;Masks: borders, differentials, auto occlusion, face parsers, text-based masking - all with strength adjustments and blending settings&lt;/li&gt; &#xA; &lt;li&gt;Mask view to evaluate masks directly&lt;/li&gt; &#xA; &lt;li&gt;Source face merging and saving&lt;/li&gt; &#xA; &lt;li&gt;Swap images or videos&lt;/li&gt; &#xA; &lt;li&gt;Auto save filename generation&lt;/li&gt; &#xA; &lt;li&gt;Dock/Undock the video player&lt;/li&gt; &#xA; &lt;li&gt;Real-time player&lt;/li&gt; &#xA; &lt;li&gt;Segment recording&lt;/li&gt; &#xA; &lt;li&gt;Fine tune your video ahead of time by creating image setting markers at specific frames.&lt;/li&gt; &#xA; &lt;li&gt;Lightening fast!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Updates for Rope-Opal-02:&lt;/h3&gt; &#xA;&lt;h3&gt;To update from Opal-01, just need to replace the .py files in /rope. You&#39;ll seem them &#39;Rope-Opal-02&#39; in the comment column&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(feature) Auto-pads input faces to increase the chance of detection&lt;/li&gt; &#xA; &lt;li&gt;(feature) Update console messages for more information&lt;/li&gt; &#xA; &lt;li&gt;(feature) Reimplimented images&lt;/li&gt; &#xA; &lt;li&gt;(fixed) Adressed error with Reference detection alignment in Restorer&lt;/li&gt; &#xA; &lt;li&gt;(feature) Added a VRAM monitor&lt;/li&gt; &#xA; &lt;li&gt;(feature) Reimplemented the VRAM clear&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Updates for Rope-Opal-01:&lt;/h3&gt; &#xA;&lt;h3&gt;To update from Opal, just need to replace the .py file in /rope. You&#39;ll seem them &#39;Rope-Opal-01&#39; in the comment column&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(Fixed) Reverted to previous model load parameters. Some people were having issues with the new settings.&lt;/li&gt; &#xA; &lt;li&gt;(Fixed) Markers now clear propoerly when changing videos&lt;/li&gt; &#xA; &lt;li&gt;(Fixed)Input Face loading errors are now handled gracefully&lt;/li&gt; &#xA; &lt;li&gt;(Feature) Added option to select Mean or Median when multi-selecting input faces&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Updates for Rope-Opal:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This next version focuses on the UI. It&#39;s completely overhauled and finally looks more like a modern app. Lots of useability improvements and room to add new features.&lt;/li&gt; &#xA; &lt;li&gt;Can now set strength to zero. This effectively turns off the swapper in the render pipeline so you can apply the rest of the options to the original image (e.g., upscale the original face w/o swapping)&lt;/li&gt; &#xA; &lt;li&gt;Recording library can be set to FFMPEG or OPENCV&lt;/li&gt; &#xA; &lt;li&gt;Real-time audio is now available while previewing. Performace that renders slower than the frame rate will cause audio lag&lt;/li&gt; &#xA; &lt;li&gt;The Differencing fuction has been reworked into the pipeline to produce better results. Consequently it currently does not show up in the mask preview.&lt;/li&gt; &#xA; &lt;li&gt;Wrestled back some VRAM from the Ruby upgrades&lt;/li&gt; &#xA; &lt;li&gt;Faster loading of some models. Upcoming releases will do furhter optimizations&lt;/li&gt; &#xA; &lt;li&gt;Adjusted the use of filtering and antialiasing&lt;/li&gt; &#xA; &lt;li&gt;Yolov8 added as a face detection model selection. FF is having good results with it, so looking forward to hearing your thoughts on its behavior in Rope&lt;/li&gt; &#xA; &lt;li&gt;Scrollbars!&lt;/li&gt; &#xA; &lt;li&gt;Save/load paramters, and reset to defaults. Rope will auto-load your saved paramters when launched.&lt;/li&gt; &#xA; &lt;li&gt;Restorers (GFPGAN, etc) now have option to choose the detection alignment method. You can trade speed vs fidelity vs texture. This includes the original Rope method that, although flawed, maintain the face textures.&lt;/li&gt; &#xA; &lt;li&gt;Detection score. Adjust how aggressive Rope is at finding faces.&lt;/li&gt; &#xA; &lt;li&gt;Added detailed help text in the lower left when hovering over UI elements.&lt;/li&gt; &#xA; &lt;li&gt;Added reverse, forward and beginning to timeline control.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Some Feature Still need to be re-implmented from Rope-Ruby. They&#39;ll be added back in the next updates.&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stop markers&lt;/li&gt; &#xA; &lt;li&gt;Framerate stats while playing&lt;/li&gt; &#xA; &lt;li&gt;Global hotkeys for moving the timeline&lt;/li&gt; &#xA; &lt;li&gt;Ongoing interface maturation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Performance:&lt;/h3&gt; &#xA;&lt;p&gt;Machine: 3090Ti (24GB), i5-13600K&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/Hillobar/Rope/assets/63615199/3e3505db-bc76-48df-b8ac-1e7e86c8d751&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;File: benchmark/target-1080p.mp4, 2048x1080, 269 frames, 25 fps, 10s Rendering time in seconds:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Crystal&lt;/th&gt; &#xA;   &lt;th&gt;Sapphire&lt;/th&gt; &#xA;   &lt;th&gt;Ruby&lt;/th&gt; &#xA;   &lt;th&gt;Opal&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Only Swap&lt;/td&gt; &#xA;   &lt;td&gt;7.3&lt;/td&gt; &#xA;   &lt;td&gt;7.5&lt;/td&gt; &#xA;   &lt;td&gt;4.4&lt;/td&gt; &#xA;   &lt;td&gt;4.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+GFPGAN&lt;/td&gt; &#xA;   &lt;td&gt;10.7&lt;/td&gt; &#xA;   &lt;td&gt;11.0&lt;/td&gt; &#xA;   &lt;td&gt;9.0&lt;/td&gt; &#xA;   &lt;td&gt;9.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+Codeformer&lt;/td&gt; &#xA;   &lt;td&gt;12.4&lt;/td&gt; &#xA;   &lt;td&gt;13.5&lt;/td&gt; &#xA;   &lt;td&gt;11.1&lt;/td&gt; &#xA;   &lt;td&gt;11.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+one word CLIP&lt;/td&gt; &#xA;   &lt;td&gt;10.4&lt;/td&gt; &#xA;   &lt;td&gt;11.2&lt;/td&gt; &#xA;   &lt;td&gt;9.1&lt;/td&gt; &#xA;   &lt;td&gt;9.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+Occluder&lt;/td&gt; &#xA;   &lt;td&gt;7.8&lt;/td&gt; &#xA;   &lt;td&gt;7.8&lt;/td&gt; &#xA;   &lt;td&gt;4.4&lt;/td&gt; &#xA;   &lt;td&gt;4.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+MouthParser&lt;/td&gt; &#xA;   &lt;td&gt;13.9&lt;/td&gt; &#xA;   &lt;td&gt;12.1&lt;/td&gt; &#xA;   &lt;td&gt;5.0&lt;/td&gt; &#xA;   &lt;td&gt;4.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Disclaimer:&lt;/h3&gt; &#xA;&lt;p&gt;Rope is a personal project that I&#39;m making available to the community as a thank you for all of the contributors ahead of me. I&#39;ve copied the disclaimer from &lt;a href=&#34;https://github.com/harisreedhar/Swap-Mukham&#34;&gt;Swap-Mukham&lt;/a&gt; here since it is well-written and applies 100% to this repo.&lt;/p&gt; &#xA;&lt;p&gt;I would like to emphasize that our swapping software is intended for responsible and ethical use only. I must stress that users are solely responsible for their actions when using our software.&lt;/p&gt; &#xA;&lt;p&gt;Intended Usage: This software is designed to assist users in creating realistic and entertaining content, such as movies, visual effects, virtual reality experiences, and other creative applications. I encourage users to explore these possibilities within the boundaries of legality, ethical considerations, and respect for others&#39; privacy.&lt;/p&gt; &#xA;&lt;p&gt;Ethical Guidelines: Users are expected to adhere to a set of ethical guidelines when using our software. These guidelines include, but are not limited to:&lt;/p&gt; &#xA;&lt;p&gt;Not creating or sharing content that could harm, defame, or harass individuals. Obtaining proper consent and permissions from individuals featured in the content before using their likeness. Avoiding the use of this technology for deceptive purposes, including misinformation or malicious intent. Respecting and abiding by applicable laws, regulations, and copyright restrictions.&lt;/p&gt; &#xA;&lt;p&gt;Privacy and Consent: Users are responsible for ensuring that they have the necessary permissions and consents from individuals whose likeness they intend to use in their creations. We strongly discourage the creation of content without explicit consent, particularly if it involves non-consensual or private content. It is essential to respect the privacy and dignity of all individuals involved.&lt;/p&gt; &#xA;&lt;p&gt;Legal Considerations: Users must understand and comply with all relevant local, regional, and international laws pertaining to this technology. This includes laws related to privacy, defamation, intellectual property rights, and other relevant legislation. Users should consult legal professionals if they have any doubts regarding the legal implications of their creations.&lt;/p&gt; &#xA;&lt;p&gt;Liability and Responsibility: We, as the creators and providers of the deep fake software, cannot be held responsible for the actions or consequences resulting from the usage of our software. Users assume full liability and responsibility for any misuse, unintended effects, or abusive behavior associated with the content they create.&lt;/p&gt; &#xA;&lt;p&gt;By using this software, users acknowledge that they have read, understood, and agreed to abide by the above guidelines and disclaimers. We strongly encourage users to approach this technology with caution, integrity, and respect for the well-being and rights of others.&lt;/p&gt; &#xA;&lt;p&gt;Remember, technology should be used to empower and inspire, not to harm or deceive. Let&#39;s strive for ethical and responsible use of deep fake technology for the betterment of society.&lt;/p&gt;</summary>
  </entry>
</feed>