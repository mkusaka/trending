<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-14T02:01:36Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>moyix/fauxpilot</title>
    <updated>2022-08-14T02:01:36Z</updated>
    <id>tag:github.com,2022-08-14:/moyix/fauxpilot</id>
    <link href="https://github.com/moyix/fauxpilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FauxPilot - an open-source GitHub Copilot server&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FauxPilot&lt;/h1&gt; &#xA;&lt;p&gt;This is an attempt to build a locally hosted version of &lt;a href=&#34;https://copilot.github.com/&#34;&gt;GitHub Copilot&lt;/a&gt;. It uses the &lt;a href=&#34;https://github.com/salesforce/CodeGen&#34;&gt;SalesForce CodeGen&lt;/a&gt; models inside of NVIDIA&#39;s &lt;a href=&#34;https://developer.nvidia.com/nvidia-triton-inference-server&#34;&gt;Triton Inference Server&lt;/a&gt; with the &lt;a href=&#34;https://github.com/triton-inference-server/fastertransformer_backend/&#34;&gt;FasterTransformer backend&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;ll need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;docker-compose&lt;/code&gt; &amp;gt;= 1.28&lt;/li&gt; &#xA; &lt;li&gt;An NVIDIA GPU with Compute Capability &amp;gt;= 7.0 and enough VRAM to run the model you want.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;&lt;code&gt;nvidia-docker&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;curl&lt;/code&gt; and &lt;code&gt;zstd&lt;/code&gt; for downloading and unpacking the models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that the VRAM requirements listed by &lt;code&gt;setup.sh&lt;/code&gt; are &lt;em&gt;total&lt;/em&gt; -- if you have multiple GPUs, you can split the model across them. So, if you have two NVIDIA RTX 3080 GPUs, you &lt;em&gt;should&lt;/em&gt; be able to run the 6B model by putting half on each GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Support and Warranty&lt;/h2&gt; &#xA;&lt;p&gt;lmao&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Run the setup script to choose a model to use. This will download the model from Huggingface and then convert it for use with FasterTransformer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./setup.sh &#xA;Models available:&#xA;[1] codegen-350M-mono (2GB total VRAM required; Python-only)&#xA;[2] codegen-350M-multi (2GB total VRAM required; multi-language)&#xA;[3] codegen-2B-mono (7GB total VRAM required; Python-only)&#xA;[4] codegen-2B-multi (7GB total VRAM required; multi-language)&#xA;[5] codegen-6B-mono (13GB total VRAM required; Python-only)&#xA;[6] codegen-6B-multi (13GB total VRAM required; multi-language)&#xA;[7] codegen-16B-mono (32GB total VRAM required; Python-only)&#xA;[8] codegen-16B-multi (32GB total VRAM required; multi-language)&#xA;Enter your choice [6]: 2&#xA;Enter number of GPUs [1]: 1&#xA;Where do you want to save the model [/home/moyix/git/fauxpilot/models]? /fastdata/mymodels&#xA;Downloading and converting the model, this will take a while...&#xA;Converting model codegen-350M-multi with 1 GPUs&#xA;Loading CodeGen model&#xA;Downloading config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 996/996 [00:00&amp;lt;00:00, 1.25MB/s]&#xA;Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 760M/760M [00:11&amp;lt;00:00, 68.3MB/s] &#xA;Creating empty GPTJ model&#xA;Converting...&#xA;Conversion complete.&#xA;Saving model to codegen-350M-multi-hf...&#xA;&#xA;=============== Argument ===============&#xA;saved_dir: /models/codegen-350M-multi-1gpu/fastertransformer/1&#xA;in_file: codegen-350M-multi-hf&#xA;trained_gpu_num: 1&#xA;infer_gpu_num: 1&#xA;processes: 4&#xA;weight_data_type: fp32&#xA;========================================&#xA;transformer.wte.weight&#xA;transformer.h.0.ln_1.weight&#xA;[... more conversion output trimmed ...]&#xA;transformer.ln_f.weight&#xA;transformer.ln_f.bias&#xA;lm_head.weight&#xA;lm_head.bias&#xA;Done! Now run ./launch.sh to start the FauxPilot server.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can just run &lt;code&gt;./launch.sh&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./launch.sh &#xA;[+] Running 2/0&#xA; ‚†ø Container fauxpilot-triton-1         Created                                                                                                                                                                                                                                                                                             0.0s&#xA; ‚†ø Container fauxpilot-copilot_proxy-1  Created                                                                                                                                                                                                                                                                                             0.0s&#xA;Attaching to fauxpilot-copilot_proxy-1, fauxpilot-triton-1&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | =============================&#xA;fauxpilot-triton-1         | == Triton Inference Server ==&#xA;fauxpilot-triton-1         | =============================&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | NVIDIA Release 22.06 (build 39726160)&#xA;fauxpilot-triton-1         | Triton Server Version 2.23.0&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | Copyright (c) 2018-2022, NVIDIA CORPORATION &amp;amp; AFFILIATES.  All rights reserved.&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | Various files include modifications (c) NVIDIA CORPORATION &amp;amp; AFFILIATES.  All rights reserved.&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | This container image and its contents are governed by the NVIDIA Deep Learning Container License.&#xA;fauxpilot-triton-1         | By pulling and using the container, you accept the terms and conditions of this license:&#xA;fauxpilot-triton-1         | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license&#xA;fauxpilot-copilot_proxy-1  | WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.&#xA;fauxpilot-copilot_proxy-1  |  * Debug mode: off&#xA;fauxpilot-copilot_proxy-1  |  * Running on all addresses (0.0.0.0)&#xA;fauxpilot-copilot_proxy-1  |    WARNING: This is a development server. Do not use it in a production deployment.&#xA;fauxpilot-copilot_proxy-1  |  * Running on http://127.0.0.1:5000&#xA;fauxpilot-copilot_proxy-1  |  * Running on http://172.18.0.3:5000 (Press CTRL+C to quit)&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | ERROR: This container was built for NVIDIA Driver Release 515.48 or later, but&#xA;fauxpilot-triton-1         |        version  was detected and compatibility mode is UNAVAILABLE.&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         |        [[]]&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:02.690042 93 pinned_memory_manager.cc:240] Pinned memory pool is created at &#39;0x7f6104000000&#39; with size 268435456&#xA;fauxpilot-triton-1         | I0803 01:51:02.690461 93 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864&#xA;fauxpilot-triton-1         | I0803 01:51:02.692434 93 model_repository_manager.cc:1191] loading: fastertransformer:1&#xA;fauxpilot-triton-1         | I0803 01:51:02.936798 93 libfastertransformer.cc:1226] TRITONBACKEND_Initialize: fastertransformer&#xA;fauxpilot-triton-1         | I0803 01:51:02.936818 93 libfastertransformer.cc:1236] Triton TRITONBACKEND API version: 1.10&#xA;fauxpilot-triton-1         | I0803 01:51:02.936821 93 libfastertransformer.cc:1242] &#39;fastertransformer&#39; TRITONBACKEND API version: 1.10&#xA;fauxpilot-triton-1         | I0803 01:51:02.936850 93 libfastertransformer.cc:1274] TRITONBACKEND_ModelInitialize: fastertransformer (version 1)&#xA;fauxpilot-triton-1         | W0803 01:51:02.937855 93 libfastertransformer.cc:149] model configuration:&#xA;fauxpilot-triton-1         | {&#xA;[... lots more output trimmed ...]&#xA;fauxpilot-triton-1         | I0803 01:51:04.711929 93 libfastertransformer.cc:321] After Loading Model:&#xA;fauxpilot-triton-1         | I0803 01:51:04.712427 93 libfastertransformer.cc:537] Model instance is created on GPU NVIDIA RTX A6000&#xA;fauxpilot-triton-1         | I0803 01:51:04.712694 93 model_repository_manager.cc:1345] successfully loaded &#39;fastertransformer&#39; version 1&#xA;fauxpilot-triton-1         | I0803 01:51:04.712841 93 server.cc:556] &#xA;fauxpilot-triton-1         | +------------------+------+&#xA;fauxpilot-triton-1         | | Repository Agent | Path |&#xA;fauxpilot-triton-1         | +------------------+------+&#xA;fauxpilot-triton-1         | +------------------+------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.712916 93 server.cc:583] &#xA;fauxpilot-triton-1         | +-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | Backend           | Path                                                                        | Config                                                                                                                                                         |&#xA;fauxpilot-triton-1         | +-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | fastertransformer | /opt/tritonserver/backends/fastertransformer/libtriton_fastertransformer.so | {&#34;cmdline&#34;:{&#34;auto-complete-config&#34;:&#34;false&#34;,&#34;min-compute-capability&#34;:&#34;6.000000&#34;,&#34;backend-directory&#34;:&#34;/opt/tritonserver/backends&#34;,&#34;default-max-batch-size&#34;:&#34;4&#34;}} |&#xA;fauxpilot-triton-1         | +-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.712959 93 server.cc:626] &#xA;fauxpilot-triton-1         | +-------------------+---------+--------+&#xA;fauxpilot-triton-1         | | Model             | Version | Status |&#xA;fauxpilot-triton-1         | +-------------------+---------+--------+&#xA;fauxpilot-triton-1         | | fastertransformer | 1       | READY  |&#xA;fauxpilot-triton-1         | +-------------------+---------+--------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.738989 93 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA RTX A6000&#xA;fauxpilot-triton-1         | I0803 01:51:04.739373 93 tritonserver.cc:2159] &#xA;fauxpilot-triton-1         | +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | Option                           | Value                                                                                                                                                                                        |&#xA;fauxpilot-triton-1         | +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | server_id                        | triton                                                                                                                                                                                       |&#xA;fauxpilot-triton-1         | | server_version                   | 2.23.0                                                                                                                                                                                       |&#xA;fauxpilot-triton-1         | | server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |&#xA;fauxpilot-triton-1         | | model_repository_path[0]         | /model                                                                                                                                                                                       |&#xA;fauxpilot-triton-1         | | model_control_mode               | MODE_NONE                                                                                                                                                                                    |&#xA;fauxpilot-triton-1         | | strict_model_config              | 1                                                                                                                                                                                            |&#xA;fauxpilot-triton-1         | | rate_limit                       | OFF                                                                                                                                                                                          |&#xA;fauxpilot-triton-1         | | pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |&#xA;fauxpilot-triton-1         | | cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |&#xA;fauxpilot-triton-1         | | response_cache_byte_size         | 0                                                                                                                                                                                            |&#xA;fauxpilot-triton-1         | | min_supported_compute_capability | 6.0                                                                                                                                                                                          |&#xA;fauxpilot-triton-1         | | strict_readiness                 | 1                                                                                                                                                                                            |&#xA;fauxpilot-triton-1         | | exit_timeout                     | 30                                                                                                                                                                                           |&#xA;fauxpilot-triton-1         | +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.740423 93 grpc_server.cc:4587] Started GRPCInferenceService at 0.0.0.0:8001&#xA;fauxpilot-triton-1         | I0803 01:51:04.740608 93 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000&#xA;fauxpilot-triton-1         | I0803 01:51:04.781561 93 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;Once everything is up and running, you should have a server listening for requests on &lt;code&gt;http://localhost:5000&lt;/code&gt;. You can now talk to it using the standard &lt;a href=&#34;https://beta.openai.com/docs/api-reference/&#34;&gt;OpenAI API&lt;/a&gt; (although the full API isn&#39;t implemented yet). For example, from Python, using the &lt;a href=&#34;https://github.com/openai/openai-python&#34;&gt;OpenAI Python bindings&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$ ipython&#xA;Python 3.8.10 (default, Mar 15 2022, 12:22:08) &#xA;Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information&#xA;IPython 8.2.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.&#xA;&#xA;In [1]: import openai&#xA;&#xA;In [2]: openai.api_key = &#39;dummy&#39;&#xA;&#xA;In [3]: openai.api_base = &#39;http://127.0.0.1:5000/v1&#39;&#xA;&#xA;In [4]: result = openai.Completion.create(engine=&#39;codegen&#39;, prompt=&#39;def hello&#39;, max_tokens=16, temperature=0.1, stop=[&#34;\n\n&#34;])&#xA;&#xA;In [5]: result&#xA;Out[5]: &#xA;&amp;lt;OpenAIObject text_completion id=cmpl-6hqu8Rcaq25078IHNJNVooU4xLY6w at 0x7f602c3d2f40&amp;gt; JSON: {&#xA;  &#34;choices&#34;: [&#xA;    {&#xA;      &#34;finish_reason&#34;: &#34;stop&#34;,&#xA;      &#34;index&#34;: 0,&#xA;      &#34;logprobs&#34;: null,&#xA;      &#34;text&#34;: &#34;() {\n    return \&#34;Hello, World!\&#34;;\n}&#34;&#xA;    }&#xA;  ],&#xA;  &#34;created&#34;: 1659492191,&#xA;  &#34;id&#34;: &#34;cmpl-6hqu8Rcaq25078IHNJNVooU4xLY6w&#34;,&#xA;  &#34;model&#34;: &#34;codegen&#34;,&#xA;  &#34;object&#34;: &#34;text_completion&#34;,&#xA;  &#34;usage&#34;: {&#xA;    &#34;completion_tokens&#34;: 15,&#xA;    &#34;prompt_tokens&#34;: 2,&#xA;    &#34;total_tokens&#34;: 17&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Copilot Plugin&lt;/h2&gt; &#xA;&lt;p&gt;Perhaps more excitingly, you can configure the official &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=GitHub.copilot&#34;&gt;VSCode Copilot plugin&lt;/a&gt; to use your local server. Just edit your &lt;code&gt;settings.json&lt;/code&gt; to add:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    &#34;github.copilot.advanced&#34;: {&#xA;        &#34;debug.overrideEngine&#34;: &#34;codegen&#34;,&#xA;        &#34;debug.testOverrideProxyUrl&#34;: &#34;http://localhost:5000&#34;,&#xA;        &#34;debug.overrideProxyUrl&#34;: &#34;http://localhost:5000&#34;&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And you should be able to use Copilot with your own locally hosted suggestions! Of course, probably a lot of stuff is subtly broken. In particular, the probabilities returned by the server are partly fake. Fixing this would require changing FasterTransformer so that it can return log-probabilities for the top k tokens rather that just the chosen token.&lt;/p&gt; &#xA;&lt;p&gt;Have fun!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MatrixTM/MHDDoS</title>
    <updated>2022-08-14T02:01:36Z</updated>
    <id>tag:github.com,2022-08-14:/MatrixTM/MHDDoS</id>
    <link href="https://github.com/MatrixTM/MHDDoS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Best DDoS Attack Script Python3, (Cyber / DDos) Attack With 56 Methods&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://cdn.discordapp.com/attachments/938175699326484490/948263435412598864/unknown_2.png&#34; width=&#34;400px&#34; height=&#34;150px&#34; alt=&#34;ddos&#34;&gt;&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;MHDDoS - DDoS Attack Script With 56 Methods&lt;/h1&gt; &#xA;&lt;em&gt;&lt;h5 align=&#34;center&#34;&gt;(Programming Language - Python 3)&lt;/h5&gt;&lt;/em&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MatrixTM/MHDDoS/main/#&#34;&gt;&lt;img alt=&#34;MH-DDoS forks&#34; src=&#34;https://img.shields.io/github/forks/MatrixTM/MHDDoS?style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MatrixTM/MHDDoS/main/#&#34;&gt;&lt;img alt=&#34;MH-DDoS last commit (main)&#34; src=&#34;https://img.shields.io/github/last-commit/MatrixTM/MHDDoS/main?color=green&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MatrixTM/MHDDoS/main/#&#34;&gt;&lt;img alt=&#34;MH-DDoS Repo stars&#34; src=&#34;https://img.shields.io/github/stars/MatrixTM/MHDDoS?style=for-the-badge&amp;amp;color=yellow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MatrixTM/MHDDoS/main/#&#34;&gt;&lt;img alt=&#34;MH-DDoS License&#34; src=&#34;https://img.shields.io/github/license/MatrixTM/MHDDoS?color=orange&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MatrixTM/MHDDoS/issues&#34;&gt;&lt;img alt=&#34;MatrixTM issues&#34; src=&#34;https://img.shields.io/github/issues/MatrixTM/MHDDoS?color=purple&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;Please Don&#39;t Attack websites without the owners consent.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://i.imgur.com/aNrHJcA.png&#34; width=&#34;1078&#34; height=&#34;433&#34; alt=&#34;POWER&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://i.imgur.com/4Q7v2wn.png&#34; width=&#34;1078&#34; height=&#34;296&#34; alt=&#34;SCRIPT&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features And Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üí£ Layer7&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://img.icons8.com/cotton/344/domain.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;get&#34;&gt; GET | GET Flood&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn0.iconfinder.com/data/icons/database-storage-5/60/server__database__fire__burn__safety-512.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;post&#34;&gt; POST | POST Flood&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/en/thumb/f/f9/OVH_Logo.svg/1200px-OVH_Logo.svg.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;ovh&#34;&gt; OVH | Bypass OVH&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn-icons-png.flaticon.com/512/1691/1691948.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;ovh&#34;&gt; RHEX | Random HEX&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn-icons-png.flaticon.com/512/4337/4337972.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;ovh&#34;&gt; STOMP | Bypass chk_captcha&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn.iconscout.com/icon/premium/png-256-thumb/cyber-bullying-2557797-2152371.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;stress&#34;&gt; STRESS | Send HTTP Packet With High Byte&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://pbs.twimg.com/profile_images/1351562987224641544/IKb4q_yd_400x400.jpg&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;dyn&#34;&gt; DYN | A New Method With Random SubDomain&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn-icons-png.flaticon.com/512/6991/6991643.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;downloader&#34;&gt; DOWNLOADER | A New Method of Reading data slowly&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn2.iconfinder.com/data/icons/poison-and-venom-fill/160/loris2-512.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;slow&#34;&gt; SLOW | Slowloris Old Method of DDoS&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://lyrahosting.com/wp-content/uploads/2020/06/ddos-how-work-icon.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;head&#34;&gt; HEAD | &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/HEAD&#34;&gt;https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/HEAD&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://img.icons8.com/plasticine/2x/null-symbol.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;null&#34;&gt; NULL | Null UserAgent and ...&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://i.pinimg.com/originals/03/2e/7d/032e7d0755cd511c753bcb6035d44f68.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;cookie&#34;&gt; COOKIE | Random Cookie PHP &#39;if (isset($_COOKIE))&#39;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn0.iconfinder.com/data/icons/dicticons-files-folders/32/office_pps-512.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;pps&#34;&gt; PPS | Only &#39;GET / HTTP/1.1\r\n\r\n&#39;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn3.iconfinder.com/data/icons/internet-security-14/48/DDoS_website_webpage_bomb_virus_protection-512.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;even&#34;&gt; EVEN | GET Method with more header&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://projectshield.withgoogle.com/static/icons/favicon.ico&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;googleshield&#34;&gt; GSB | Google Project Shield Bypass&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://seeklogo.com/images/D/ddos-guard-logo-CFEFCA409C-seeklogo.com.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;DDoSGuard&#34;&gt; DGB | DDoS Guard Bypass&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://i.imgur.com/bGL8qfw.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;ArvanCloud&#34;&gt; AVB | Arvan Cloud Bypass&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Google_%22G%22_Logo.svg/1024px-Google_%22G%22_Logo.svg.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;Google bot&#34;&gt; BOT | Like Google bot&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Apache_HTTP_Server_Logo_%282016%29.svg/1000px-Apache_HTTP_Server_Logo_%282016%29.svg.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;Apache Webserver&#34;&gt; APACHE | Apache Expliot&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://icon-library.com/images/icon-for-wordpress/icon-for-wordpress-16.jpg&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;wordpress expliot&#34;&gt; XMLRPC | WP XMLRPC expliot (add /xmlrpc.php)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://techcrunch.com/wp-content/uploads/2019/06/J2LlHqT3qJl0bG9Alpgc-1-730x438.png?w=730&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;CloudFlare&#34;&gt; CFB | CloudFlare Bypass&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://techcrunch.com/wp-content/uploads/2019/06/J2LlHqT3qJl0bG9Alpgc-1-730x438.png?w=730&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;CloudFlare UnderAttack Mode&#34;&gt; CFBUAM | CloudFlare Under Attack Mode Bypass&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;http://iclouddnsbypass.com/wp-content/uploads/2015/02/iCloudDNSBypassServer.ico&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;bypass&#34;&gt; BYPASS | Bypass Normal AntiDDoS&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn-icons-png.flaticon.com/512/905/905568.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;bypass&#34;&gt; BOMB | Bypass with codesenberg/bombardier&lt;/li&gt; &#xA;   &lt;li&gt;üî™ KILLER | run many threads to kill a target&lt;/li&gt; &#xA;   &lt;li&gt;üßÖ TOR | Bypass onion website&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üß® Layer4:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kgretzky/pwndrop/master/media/pwndrop-logo-512.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;tcp&#34;&gt; TCP | TCP Flood Bypass&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://styles.redditmedia.com/t5_2rxmiq/styles/profileIcon_snoob94cdb09-c26c-4c24-bd0c-66238623cc22-headshot.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;udp&#34;&gt; UDP | UDP Flood Bypass&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn-icons-png.flaticon.com/512/1918/1918576.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;syn&#34;&gt; SYN | SYN Flood&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn-icons-png.flaticon.com/512/1017/1017466.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;cps&#34;&gt; CPS | Open and close connections with proxy&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://icon-library.com/images/icon-ping/icon-ping-28.jpg&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;icmp&#34;&gt; ICMP | Icmp echo request flood (Layer3)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://s6.uupload.ir/files/1059643_g8hp.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;connection&#34;&gt; CONNECTION | Open connection alive with proxy&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://ia803109.us.archive.org/27/items/source-engine-video-projects/source-engine-video-projects_itemimage.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;vse&#34;&gt; VSE | Send Valve Source Engine Protocol&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://mycrackfree.com/wp-content/uploads/2018/08/TeamSpeak-Server-9.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;teamspeak 3&#34;&gt; TS3 | Send Teamspeak 3 Status Ping Protocol&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn2.downdetector.com/static/uploads/logo/75ef9fcabc1abea8fce0ebd0236a4132710fcb2e.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;fivem&#34;&gt; FIVEM | Send Fivem Status Ping Protocol&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn.iconscout.com/icon/free/png-512/redis-4-1175103.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;mem&#34;&gt; MEM | Memcached Amplification&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://lyrahosting.com/wp-content/uploads/2020/06/ddos-attack-icon.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;ntp&#34;&gt; NTP | NTP Amplification&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn-icons-png.flaticon.com/512/4712/4712139.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;mcbot&#34;&gt; MCBOT | Minecraft Bot Attack&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn.icon-icons.com/icons2/2699/PNG/512/minecraft_logo_icon_168974.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;minecraft&#34;&gt; MINECRAFT | Minecraft Status Ping Protocol&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://gam3r.ir/wp-content/uploads/2018/11/512dVKB22QL.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;minecraft pe&#34;&gt; MCPE | Minecraft PE Status Ping Protocol&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://cdn-icons-png.flaticon.com/512/2653/2653461.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;dns&#34;&gt; DNS | DNS Amplification&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://lyrahosting.com/wp-content/uploads/2020/06/ddos-attack-icon.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;chargen&#34;&gt; CHAR | Chargen Amplification&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRct5OvjSCpUftyRMm3evgdPOa-f8LbwJFO-A&amp;amp;usqp=CAU&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;cldap&#34;&gt; CLDAP | Cldap Amplification&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://help.apple.com/assets/6171BD2C588E52621824409D/6171BD2D588E5262182440A4/en_US/8b631353e070420f47530bf95f1a7fae.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;ard&#34;&gt; ARD | Apple Remote Desktop Amplification&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://www.tenforums.com/geek/gars/images/2/types/thumb__emote__esktop__onnection.png&#34; width=&#34;16&#34; height=&#34;16&#34; alt=&#34;rdp&#34;&gt; RDP | Remote Desktop Protocol Amplification&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚öôÔ∏è Tools - Run With &lt;code&gt;python3 start.py tools&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üåü CFIP | Find Real IP Address Of Websites Powered By Cloudflare&lt;/li&gt; &#xA;   &lt;li&gt;üî™ DNS | Show DNS Records Of Sites&lt;/li&gt; &#xA;   &lt;li&gt;üìç TSSRV | TeamSpeak SRV Resolver&lt;/li&gt; &#xA;   &lt;li&gt;‚ö† PING | PING Servers&lt;/li&gt; &#xA;   &lt;li&gt;üìå CHECK | Check If Websites Status&lt;/li&gt; &#xA;   &lt;li&gt;üòé DSTAT | That Shows Bytes Received, bytes Sent and their amount&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üé© Other&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;‚ùå STOP | STOP All Attacks&lt;/li&gt; &#xA;   &lt;li&gt;üå† TOOLS | Console Tools&lt;/li&gt; &#xA;   &lt;li&gt;üëë HELP | Show Usage Script&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; Our social&#39;süíª &lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://icon-library.com/images/github-icon-vector/github-icon-vector-27.jpg&#34; width=&#34;96&#34; height=&#34;96&#34; alt=&#34;github&#34;&gt; &#xA; &lt;img src=&#34;https://brandlogos.net/wp-content/uploads/2021/11/discord-logo.png&#34; width=&#34;96&#34; height=&#34;96&#34; alt=&#34;discord&#34;&gt; &#xA; &lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Telegram_logo.svg/2048px-Telegram_logo.svg.png&#34; width=&#34;96&#34; height=&#34;96&#34; alt=&#34;telegram&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t.me/DD0SChat&#34;&gt;Matrix Team Telegram group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/Rws8VAsnXw&#34;&gt;Matrix community Discord server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MatrixTM/MHDDoS/issues&#34; title=&#34;GitHub&#34;&gt;GitHub&lt;/a&gt; : &lt;a href=&#34;https://github.com/MatrixTM/MHDDoS/issues&#34; title=&#34;GitHub&#34;&gt;github&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;If u Like the project Leave a star on the repository!&lt;/h3&gt; &#xA;&lt;h2&gt;Downloads&lt;/h2&gt; &#xA;&lt;p&gt;You can download it from &lt;a href=&#34;https://github.com/MatrixTM/MHDDoS/releases&#34;&gt;GitHub Releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rthalley/dnspython&#34;&gt;dnspython&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Anorov/cloudflare-scrape&#34;&gt;cfscrape&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SecureAuthCorp/impacket&#34;&gt;impacket&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/psf/requests&#34;&gt;requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.org&#34; title=&#34;Python3&#34;&gt;Python3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MatrixTM/PyRoxy&#34;&gt;PyRoxy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ValentinBELYN/icmplib&#34;&gt;icmplib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/certifi/python-certifi&#34;&gt;certifi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/giampaolo/psutil&#34;&gt;psutil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aio-libs/yarl&#34;&gt;yarl&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Videos&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Aparat: &lt;a href=&#34;https://www.aparat.com/v/bHcP9&#34;&gt;https://www.aparat.com/v/bHcP9&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YouTube : Coming soon...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Aprat : &lt;a href=&#34;https://aparat.com/v/XPn5Z&#34;&gt;https://aparat.com/v/XPn5Z&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YouTube : Coming soon...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can read it from &lt;a href=&#34;https://github.com/MatrixTM/MHDDoS/wiki&#34;&gt;GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clone and Install Script&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/MatrixTM/MHDDoS.git&#xA;cd MHDDoS&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;üí∞ Donation Links:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://idpay.ir/mh-prodev&#34;&gt;https://idpay.ir/mh-prodev&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>jina-ai/discoart</title>
    <updated>2022-08-14T02:01:36Z</updated>
    <id>tag:github.com,2022-08-14:/jina-ai/discoart</id>
    <link href="https://github.com/jina-ai/discoart" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Create Disco Diffusion artworks in one line&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/banner.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Create compelling Disco Diffusion artworks in one line&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/discoart/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/discoart?style=flat-square&amp;amp;label=Release&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/repository/docker/jinaai/discoart&#34;&gt;&lt;img alt=&#34;Docker Cloud Build Status&#34; src=&#34;https://img.shields.io/docker/cloud/build/jinaai/discoart?logo=docker&amp;amp;logoColor=white&amp;amp;style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://slack.jina.ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-3.1k-blueviolet?logo=slack&amp;amp;logoColor=white&amp;amp;style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/jina-ai/discoart/blob/main/discoart.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Open-in%20Colab-brightgreen?logo=google-colab&amp;amp;style=flat-square&#34; alt=&#34;Open in Google Colab&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;DiscoArt is an elegant way of creating compelling Disco Diffusion&lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/#example-application&#34;&gt;[*]&lt;/a&gt;&lt;/sup&gt; artworks for generative artists, AI enthusiasts and hard-core developers. DiscoArt has a modern &amp;amp; professional API with a beautiful codebase, ensuring high usability and maintainability. It introduces handy features such as result recovery and persistence, gRPC/HTTP serving w/o TLS, post-analysis, easing the integration to larger cross-modal or multi-modal applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;&lt;a id=&#34;example-application&#34;&gt;[*]&lt;/a&gt; Disco Diffusion is a Google Colab Notebook that leverages CLIP-Guided Diffusion to allow one to create compelling and beautiful images from text prompts. &lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;üíØ &lt;strong&gt;Best-in-class&lt;/strong&gt;: industry-level engineering, top-notch code quality, lean dependencies, small RAM/VRAM footprint; important bug fixes, feature improvements &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/FEATURES.md&#34;&gt;vs. the original DD5.6&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üëº &lt;strong&gt;Available to all&lt;/strong&gt;: smooth install for &lt;em&gt;self-hosting&lt;/em&gt;, Google Colab &lt;em&gt;free tier&lt;/em&gt;, non-GUI (IPython) environment, and CLI! No brainfuck, no dependency hell, no stackoverflow.&lt;/p&gt; &#xA;&lt;p&gt;üé® &lt;strong&gt;Focus on create not code&lt;/strong&gt;: one-liner &lt;code&gt;create()&lt;/code&gt; with a Pythonic interface, autocompletion in IDE, and powerful features. Fetch real-time results anywhere anytime, no more worry on session outrage on Google Colab. Set initial state easily for more efficient parameter exploration.&lt;/p&gt; &#xA;&lt;p&gt;üè≠ &lt;strong&gt;Ready for integration &amp;amp; production&lt;/strong&gt;: built on top of &lt;a href=&#34;https://github.com/jina-ai/docarray&#34;&gt;DocArray&lt;/a&gt; data structure, enjoy smooth integration with &lt;a href=&#34;https://github.com/jina-ai/jina&#34;&gt;Jina&lt;/a&gt;, &lt;a href=&#34;https://github.com/jina-ai/clip-as-service&#34;&gt;CLIP-as-service&lt;/a&gt; and other cross-/multi-modal applications.&lt;/p&gt; &#xA;&lt;p&gt;‚òÅÔ∏è &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/#serving&#34;&gt;&lt;strong&gt;As-a-service&lt;/strong&gt;&lt;/a&gt;: simply &lt;code&gt;python -m discoart serve&lt;/code&gt;, DiscoArt is now a high-performance low-latency service supports gRPC/HTTP/websockets and TLS. Scaling up/down is one-line; Cloud-native features e.g. Kubernetes, Prometheus and Grafana is one-line. &lt;a href=&#34;https://github.com/jina-ai/jina&#34;&gt;Unbelievable simple thanks to Jina&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://twitter.com/hxiao/status/1542967938369687552?s=20&amp;amp;t=DO27EKNMADzv4WjHLQiPFA&#34;&gt;Gallery with prompts&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Do you see the &lt;code&gt;discoart-id&lt;/code&gt; in each tweet? To get the config &amp;amp; prompts, simply:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart import show_config&#xA;&#xA;show_config(&#39;discoart-id&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Python 3.7+ and CUDA-enabled PyTorch is required.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install discoart&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This applies to both &lt;em&gt;self-hosting&lt;/em&gt;, &lt;em&gt;Google Colab&lt;/em&gt;, system integration, non-GUI environments.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-hosted Jupyter&lt;/strong&gt;: to run a Jupyter Notebook on your own GPU machine, the easiest way is to &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/#run-in-docker&#34;&gt;use our prebuilt Docker image&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Use it from CLI&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/#cli&#34;&gt;&lt;code&gt;python -m discoart create&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;python -m discoart config&lt;/code&gt; are CLI commands.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Use it as a service&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/#serving&#34;&gt;&lt;code&gt;python -m discoart serve&lt;/code&gt;&lt;/a&gt; allows one to run it as gRPC/HTTP/websockets service.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;GUI&lt;/h3&gt; &#xA;&lt;p&gt;DiscoArt is the &lt;strong&gt;infrastructure&lt;/strong&gt; for creating Disco Diffusion artworks. The built-in Jupyter Notebook support gives you basic yet limited user experience, e.g. it does not offer any intuitive GUI for &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/FEATURES.md#prompt-scheduling&#34;&gt;prompt scheduling&lt;/a&gt;. Note that DiscoArt is developer-centric and API-first, hence improving consumer-facing experience is out of the scope. There are services, platforms and products (not Jina AI affiliated) that already integrate DiscoArt as a service and provide nice GUI on top of it, e.g. Fever Dreams, Replicate, RunPod and Renderflux.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see third-party GUI&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.feverdreams.app/&#34;&gt;Fever Dreams&lt;/a&gt;: a free community-powered service with nice GUI and gallery, where people generate and share their DiscoArt artworks, prompts and configs.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://replicate.com/nightmareai/disco-diffusion&#34;&gt;Replicate&lt;/a&gt;: a free form-based GUI of DiscoArt with sandbox user experience and the visualizations.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.runpod.io/blog/accelerate-your-generate-art-with-disco-diffusion-and-runpod&#34;&gt;RunPod&lt;/a&gt;: a paid GPU cloud provider that runs DiscoArt container with a simple and clean GUI to visualize the configs and creations.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://beta.renderflux.com/register?invite=bughunting&#34;&gt;Renderflux&lt;/a&gt;: a paid creative art platform that wraps DiscoArt and provides end-to-end GUI for creation management.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;strong&gt;Please be aware that these platforms, products or companies are not affiliated with Jina AI.&lt;/strong&gt; They define their own terms of services, paywall and data and privacy policies, which are not in the scope of DiscoArt MIT License.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/jina-ai/discoart/blob/main/discoart.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Open-in%20Colab-brightgreen?logo=google-colab&amp;amp;style=flat-square&#34; alt=&#34;Open in Google Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Create artworks&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart import create&#xA;&#xA;da = create()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! It will create with the &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/discoart/resources/default.yml&#34;&gt;default text prompts and parameters&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/create-demo.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Set prompts and parameters&lt;/h3&gt; &#xA;&lt;p&gt;Supported parameters are &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/discoart/resources/default.yml&#34;&gt;listed here&lt;/a&gt;. You can specify them in &lt;code&gt;create()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart import create&#xA;&#xA;da = create(&#xA;    text_prompts=&#39;A painting of sea cliffs in a tumultuous storm, Trending on ArtStation.&#39;,&#xA;    init_image=&#39;https://d2vyhzeko0lke5.cloudfront.net/2f4f6dfa5a05e078469ebe57e77b72f0.png&#39;,&#xA;    skip_steps=100,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/parameter-demo.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In case you forgot a parameter, just lookup the cheatsheet at anytime:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart import cheatsheet&#xA;&#xA;cheatsheet()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The difference on the parameters between DiscoArt and DD5.6 &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/FEATURES.md&#34;&gt;is explained here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Visualize results&lt;/h3&gt; &#xA;&lt;p&gt;Final results and intermediate results are created under the current working directory, i.e.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;./{name-docarray}/{i}-done.png&#xA;./{name-docarray}/{i}-step-{j}.png&#xA;./{name-docarray}/{i}-progress.png&#xA;./{name-docarray}/{i}-progress.gif&#xA;./{name-docarray}/da.protobuf.lz4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/result-persist.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;where:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;name-docarray&lt;/code&gt; is the name of the run, you can specify it otherwise it is a random name.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;i-*&lt;/code&gt; is up to the value of &lt;code&gt;n_batches&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;*-done-*&lt;/code&gt; is the final image on done.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;*-step-*&lt;/code&gt; is the intermediate image at certain step, updated in real-time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;*-progress.png&lt;/code&gt; is the sprite image of all intermediate results so far, updated in real-time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;*-progress.gif&lt;/code&gt; is the animated gif of all intermediate results so far, updated in real-time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;da.protobuf.lz4&lt;/code&gt; is the compressed protobuf of all intermediate results so far, updated in real-time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The save frequency is controlled by &lt;code&gt;save_rate&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Moreover, &lt;code&gt;create()&lt;/code&gt; returns &lt;code&gt;da&lt;/code&gt;, a &lt;a href=&#34;https://docarray.jina.ai/fundamentals/documentarray/&#34;&gt;DocumentArray&lt;/a&gt;-type object. It contains the following information:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All arguments passed to &lt;code&gt;create()&lt;/code&gt; function, including seed, text prompts and model parameters.&lt;/li&gt; &#xA; &lt;li&gt;4 generated image and its intermediate steps&#39; images, where &lt;code&gt;4&lt;/code&gt; is determined by &lt;code&gt;n_batches&lt;/code&gt; and is the default value.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This allows you to further post-process, analyze, export the results with powerful DocArray API.&lt;/p&gt; &#xA;&lt;p&gt;Images are stored as Data URI in &lt;code&gt;.uri&lt;/code&gt;, to save the first image as a local file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;da[0].save_uri_to_file(&#39;discoart-result.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To save all final images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for idx, d in enumerate(da):&#xA;    d.save_uri_to_file(f&#39;discoart-result-{idx}.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also display all four final images in a grid:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;da.plot_image_sprites(skip_empty=True, show_index=True, keep_aspect_ratio=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/all-results.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or display them one by one:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for d in da:&#xA;    d.display()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or take one particular run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;da[0].display()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/display.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Visualize intermediate steps&lt;/h3&gt; &#xA;&lt;p&gt;You can also zoom into a run (say the first run) and check out intermediate steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;da[0].chunks.plot_image_sprites(&#xA;    skip_empty=True, show_index=True, keep_aspect_ratio=True&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/chunks.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can &lt;code&gt;.display()&lt;/code&gt; the chunks one by one, or save one via &lt;code&gt;.save_uri_to_file()&lt;/code&gt;, or save all intermediate steps as a GIF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;da[0].chunks.save_gif(&#xA;    &#39;lighthouse.gif&#39;, show_index=True, inline_display=True, size_ratio=0.5&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/lighthouse.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that &amp;gt;=0.7.14, a 20FPS gif is generated which includes all intermedidate steps.&lt;/p&gt; &#xA;&lt;h3&gt;Show/save/load configs&lt;/h3&gt; &#xA;&lt;p&gt;To show the config of a Document/DocumentArray,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart import show_config&#xA;&#xA;show_config(da)  # show the config of the first run&#xA;show_config(da[3])  # show the config of the fourth run&#xA;show_config(&#xA;    &#39;discoart-06030a0198843332edc554ffebfbf288&#39;&#xA;)  # show the config of the run with a known DocArray ID&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To save the config of a Document/DocumentArray,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart import save_config&#xA;&#xA;save_config(da, &#39;my.yml&#39;)  # save the config of the first run&#xA;save_config(da[3], &#39;my.yml&#39;)  # save the config of the fourth run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run &lt;code&gt;create&lt;/code&gt; from a YAML config of Document/DocumentArray,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart import create, load_config&#xA;&#xA;config = load_config(&#39;my.yml&#39;)&#xA;create(**config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also export the config as an SVG image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart.config import save_config_svg&#xA;&#xA;save_config_svg(da)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/discoart-3205998582.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;One can also generate runnable Python code directly from the config:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart.config import export_python&#xA;&#xA;export_python(da)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull results anywhere anytime&lt;/h3&gt; &#xA;&lt;p&gt;If you are a free-tier Google Colab user, one annoy thing is the lost of sessions from time to time. Or sometimes you just early stop the run as the first image is not good enough, and a keyboard interrupt will prevent &lt;code&gt;.create()&lt;/code&gt; to return any result. Either case, you can easily recover the results by pulling the last session ID.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Find the session ID. It appears on top of the image. &lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/session-id.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pull the result via that ID &lt;strong&gt;on any machine at any time&lt;/strong&gt;, not necessarily on Google Colab:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import DocumentArray&#xA;&#xA;da = DocumentArray.pull(&#39;discoart-3205998582&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Reuse a Document as initial state&lt;/h3&gt; &#xA;&lt;p&gt;Consider a Document as a self-contained data with config and image, one can use it as the initial state for the future run. Its &lt;code&gt;.tags&lt;/code&gt; will be used as the initial parameters; &lt;code&gt;.uri&lt;/code&gt; if presented will be used as the initial image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart import create&#xA;from docarray import DocumentArray&#xA;&#xA;da = DocumentArray.pull(&#39;discoart-3205998582&#39;)&#xA;&#xA;create(&#xA;    init_document=da[0],&#xA;    cut_ic_pow=0.5,&#xA;    tv_scale=600,&#xA;    cut_overview=&#39;[12]*1000&#39;,&#xA;    cut_innercut=&#39;[12]*1000&#39;,&#xA;    use_secondary_model=False,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you just want to initialize from a known DocArray ID, then simply:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from discoart import create&#xA;&#xA;create(init_document=&#39;discoart-3205998582&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Environment variables&lt;/h3&gt; &#xA;&lt;p&gt;You can set environment variables to control the meta-behavior of DiscoArt. The environment variables must be set before importing DiscoArt, either in Bash or in Python via &lt;code&gt;os.environ&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DISCOART_LOG_LEVEL=&#39;DEBUG&#39; # more verbose logs&#xA;DISCOART_OPTOUT_CLOUD_BACKUP=&#39;1&#39; # opt-out from cloud backup&#xA;DISCOART_DISABLE_IPYTHON=&#39;1&#39; # disable ipython dependency&#xA;DISCOART_DISABLE_RESULT_SUMMARY=&#39;1&#39; # disable result summary after the run ends&#xA;DISCOART_DEFAULT_PARAMETERS_YAML=&#39;path/to/your-default.yml&#39; # use a custom default parameters file&#xA;DISCOART_CUT_SCHEDULES_YAML=&#39;path/to/your-schedules.yml&#39; # use a custom cut schedules file&#xA;DISCOART_MODELS_YAML=&#39;path/to/your-models.yml&#39; # use a custom list of models file&#xA;DISCOART_OUTPUT_DIR=&#39;path/to/your-output-dir&#39; # use a custom output directory for all images and results&#xA;DISCOART_CACHE_DIR=&#39;path/to/your-cache-dir&#39; # use a custom cache directory for models and downloads&#xA;DISCOART_DISABLE_REMOTE_MODELS=&#39;1&#39; # disable the listing of diffusion models on Github, remote diffusion models allows user to use latest models without updating the codebase.&#xA;DISCOART_REMOTE_MODELS_URL=&#39;https://yourdomain/models.yml&#39; # use a custom remote URL for fetching models list&#xA;DISCOART_DISABLE_CHECK_MODEL_SHA=&#39;1&#39; # disable checking local model SHA matches the remote model SHA&#xA;DISCOART_DISABLE_TQDM=&#39;1&#39; # disable tqdm progress bar on diffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;CLI&lt;/h2&gt; &#xA;&lt;p&gt;DiscoArt provides two commands &lt;code&gt;create&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; that allows you to run DiscoArt from CLI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m discoart create my.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which creates artworks from the YAML config file &lt;code&gt;my.yml&lt;/code&gt;. You can also do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat config.yml | python -m discoart create&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;So how can I have my own &lt;code&gt;my.yml&lt;/code&gt; and what does it look like? That&#39;s the second command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m discoart config my.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which forks the &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/discoart/resources/default.yml&#34;&gt;default YAML config&lt;/a&gt; and export them to &lt;code&gt;my.yml&lt;/code&gt;. Now you can modify it and run it with &lt;code&gt;python -m discoart create&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;p&gt;If no output path is specified, then &lt;code&gt;python -m discoart config&lt;/code&gt; will print the default config to stdout.&lt;/p&gt; &#xA;&lt;p&gt;To get help on a command, add &lt;code&gt;--help&lt;/code&gt; at the end, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m discoart create --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;usage: python -m discoart create [-h] [YAML_CONFIG_FILE]&#xA;&#xA;positional arguments:&#xA;  YAML_CONFIG_FILE  The YAML config file to use, default is stdin.&#xA;&#xA;optional arguments:&#xA;  -h, --help        show this help message and exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Serving&lt;/h2&gt; &#xA;&lt;p&gt;Serving DiscoArt is super easy. Simply run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m discoart serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You shall see:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/.github/serving.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now send request to the server via curl/Javascript, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl \&#xA;-X POST http://0.0.0.0:51001/post \&#xA;-H &#39;Content-Type: application/json&#39; \&#xA;-d &#39;{&#34;execEndpoint&#34;:&#34;/create&#34;, &#34;parameters&#34;: {&#34;text_prompts&#34;: [&#34;A beautiful painting of a singular lighthouse&#34;, &#34;yellow color scheme&#34;]}}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it.&lt;/p&gt; &#xA;&lt;p&gt;You can of course pass all parameters that accepted by &lt;code&gt;create()&lt;/code&gt; function in the JSON.&lt;/p&gt; &#xA;&lt;h3&gt;Polling intermediate results&lt;/h3&gt; &#xA;&lt;p&gt;We already know that &lt;code&gt;create&lt;/code&gt; function is slow even on GPU it could take 10 minutes to finish an artwork. This means the after sending the above request, the client will have to wait 10 minutes for the response. There is nothing wrong with this behavior given that everything runs synchronously. However, in practice, client may expect a progress or intermediate results in the middle instead of waiting for the end.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;/result&lt;/code&gt; endpoint is designed for this purpose. It will return the intermediate results as soon as they are available. All you need is to specify &lt;code&gt;name_docarray&lt;/code&gt; in the request parameters as you specified in &lt;code&gt;/create&lt;/code&gt; endpoint. Here is an example:&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s create &lt;code&gt;mydisco-123&lt;/code&gt; by sending the following request to &lt;code&gt;/create&lt;/code&gt; endpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl \&#xA;-X POST http://0.0.0.0:51001/post \&#xA;-H &#39;Content-Type: application/json&#39; \&#xA;-d &#39;{&#34;execEndpoint&#34;:&#34;/create&#34;, &#34;parameters&#34;: {&#34;name_docarray&#34;: &#34;mydisco-123&#34;, &#34;text_prompts&#34;: [&#34;A beautiful painting of a singular lighthouse&#34;, &#34;yellow color scheme&#34;]}}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now that the above request is being processed on the server, you can periodically check &lt;code&gt;mydisco-123&lt;/code&gt; progress by sending the following request to &lt;code&gt;/result&lt;/code&gt; endpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl \&#xA;-X POST http://0.0.0.0:51001/post \&#xA;-H &#39;Content-Type: application/json&#39; \&#xA;-d &#39;{&#34;execEndpoint&#34;:&#34;/result&#34;, &#34;parameters&#34;: {&#34;name_docarray&#34;: &#34;mydisco-123&#34;}}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A JSON will be returned with up-to-date progress, with image as DataURI, loss, steps etc. &lt;a href=&#34;https://docarray.jina.ai/fundamentals/fastapi-support/#json-schema&#34;&gt;The JSON Schema of Document/DocumentArray is described here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note, &lt;code&gt;/result&lt;/code&gt; won&#39;t be blocked by &lt;code&gt;/create&lt;/code&gt; thanks to the smart routing of Jina Gateway. To learn/play more about those endpoints, you can check ReDoc or the Swagger UI embedded in the server.&lt;/p&gt; &#xA;&lt;h3&gt;Skip &amp;amp; Cancel&lt;/h3&gt; &#xA;&lt;p&gt;Send to &lt;code&gt;/skip&lt;/code&gt;, to skip the current run and move to the next run as defined in &lt;code&gt;n_batches&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl \&#xA;-X POST http://0.0.0.0:51001/post \&#xA;-H &#39;Content-Type: application/json&#39; \&#xA;-d &#39;{&#34;execEndpoint&#34;:&#34;/skip&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Send to &lt;code&gt;/stop&lt;/code&gt;, to stop the current run cancel all runs &lt;code&gt;n_batches&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl \&#xA;-X POST http://0.0.0.0:51001/post \&#xA;-H &#39;Content-Type: application/json&#39; \&#xA;-d &#39;{&#34;execEndpoint&#34;:&#34;/stop&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Unblocking &lt;code&gt;/create&lt;/code&gt; request&lt;/h3&gt; &#xA;&lt;p&gt;It is possible to have an unblocked &lt;code&gt;/create&lt;/code&gt; endpoint: the client request to &lt;code&gt;/create&lt;/code&gt; will be &lt;strong&gt;immediately&lt;/strong&gt; returned, without waiting for the results to be finished. You now have to fully rely on &lt;code&gt;/result&lt;/code&gt; to poll the result.&lt;/p&gt; &#xA;&lt;p&gt;To enable this feature:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy-paste the &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/discoart/resources/flow.yml&#34;&gt;default &lt;code&gt;flow.yml&lt;/code&gt; file&lt;/a&gt; to &lt;code&gt;myflow.yml&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Change &lt;code&gt;floating: false&lt;/code&gt; to &lt;code&gt;floating: true&lt;/code&gt; under &lt;code&gt;discoart&lt;/code&gt; executor section;&lt;/li&gt; &#xA; &lt;li&gt;Run the following command: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m discoart serve myflow.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Beware that the request velocity is now under &lt;strong&gt;your control&lt;/strong&gt;. That is, if the client sends 10 &lt;code&gt;/create&lt;/code&gt; requests in a second, then the server will start 10 &lt;code&gt;create()&lt;/code&gt; in parallel! This can easily lead to OOM. Hence, the suggestion is only enabling this feature if you are sure that the client is not sending too many requests, e.g. you control the client request rate; or you are using DiscoArt behind a BFF (backend for frontend).&lt;/p&gt; &#xA;&lt;h3&gt;Scaling out&lt;/h3&gt; &#xA;&lt;p&gt;If you have multiple GPUs and you want to run multiple DiscoArt instances in parallel by leveraging GPUs in a time-multiplexed fashion, you can copy-paste the &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/discoart/resources/flow.yml&#34;&gt;default &lt;code&gt;flow.yml&lt;/code&gt; file&lt;/a&gt; and modify it as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;jtype: Flow&#xA;with:&#xA;  protocol: http&#xA;  monitoring: true&#xA;  port: 51001&#xA;  port_monitoring: 51002  # prometheus monitoring port&#xA;  env:&#xA;    JINA_LOG_LEVEL: debug&#xA;    DISCOART_DISABLE_IPYTHON: 1&#xA;    DISCOART_DISABLE_RESULT_SUMMARY: 1&#xA;executors:&#xA;  - name: discoart&#xA;    uses: DiscoArtExecutor&#xA;    env:&#xA;      CUDA_VISIBLE_DEVICES: RR0:3  # change this if you have multiple GPU&#xA;    replicas: 3  # change this if you have larger VRAM&#xA;  - name: poller&#xA;    uses: ResultPoller&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here &lt;code&gt;replicas: 3&lt;/code&gt; says spawning three DiscoArt instances, &lt;code&gt;CUDA_VISIBLE_DEVICES: RR0:3&lt;/code&gt; makes sure they use the first three GPUs in a round-robin fashion.&lt;/p&gt; &#xA;&lt;p&gt;Name it as &lt;code&gt;myflow.yml&lt;/code&gt; and then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m discoart serve myflow.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Customization&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jina-ai/jina&#34;&gt;Thanks to Jina&lt;/a&gt;, there are tons of things you can customize! You can change the port number; change protocol to gRPC/Websockets; add TLS encryption; enable/disable Prometheus monitoring; you can also export it to Kubernetes deployment bundle simply via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jina export kubernetes myflow.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more features and YAML configs, &lt;a href=&#34;https://docs.jina.ai&#34;&gt;please check out Jina docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Use gRPC gateway&lt;/h3&gt; &#xA;&lt;p&gt;To switch from HTTP to gRPC gateway is simple:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;jtype: Flow&#xA;with:&#xA;  protocol: grpc&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then restart the server.&lt;/p&gt; &#xA;&lt;p&gt;There are multiple advantages of using gRPC gateway:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Much faster and smaller network overhead.&lt;/li&gt; &#xA; &lt;li&gt;Feature-rich, like compression, status monitoring, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In general, if you are using the DiscoArt server behind a BFF (backend for frontend), or your DiscoArt server does &lt;strong&gt;not&lt;/strong&gt; directly serve HTTP traffic from end-users, then you should use gRPC protocol.&lt;/p&gt; &#xA;&lt;p&gt;To communicate with a gRPC DiscoArt server, one can use a Jina Client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# !pip install jina&#xA;from jina import Client&#xA;&#xA;c = Client(host=&#39;grpc://0.0.0.0:51001&#39;)&#xA;&#xA;da = c.post(&#xA;    &#39;/create&#39;,&#xA;    parameters={&#xA;        &#39;name_docarray&#39;: &#39;mydisco-123&#39;,&#xA;        &#39;text_prompts&#39;: [&#xA;            &#39;A beautiful painting of a singular lighthouse&#39;,&#xA;            &#39;yellow color scheme&#39;,&#xA;        ],&#xA;    },&#xA;)&#xA;&#xA;# check intermediate results&#xA;da = c.post(&#39;/result&#39;, parameters={&#39;name_docarray&#39;: &#39;mydisco-123&#39;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use an existing Document/DocumentArray as init Document for &lt;code&gt;create&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from jina import Client&#xA;&#xA;c = Client(host=&#39;grpc://0.0.0.0:51001&#39;)&#xA;&#xA;old_da = create(...)&#xA;&#xA;da = c.post(&#xA;    &#39;/create&#39;,&#xA;    old_da,  # this can be a DocumentArray or a single Document&#xA;    parameters={&#xA;        &#39;width_height&#39;: [1024, 768],&#xA;    },&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This equals to run &lt;code&gt;create(init_document=old_da, width_height=[1024, 768])&lt;/code&gt; on the server. Note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;follow-up parameters have higher priorities than the parameters in &lt;code&gt;init_document&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;if &lt;code&gt;init_document&lt;/code&gt; is a DocumentArray, then the first Document in the array will be used as the init Document.&lt;/li&gt; &#xA; &lt;li&gt;there is no need to do any serialization before sending, Jina automatically handles it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Hosting on Google Colab&lt;/h3&gt; &#xA;&lt;p&gt;Though not recommended, it is also possible to use Google Colab to host DiscoArt server. Please check out the following tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.jina.ai/how-to/google-colab/&#34;&gt;https://docs.jina.ai/how-to/google-colab/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clip-as-service.jina.ai/hosting/colab/&#34;&gt;https://clip-as-service.jina.ai/hosting/colab/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run in Docker&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/jinaai/discoart&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/image-size/jinaai/discoart/latest?logo=docker&amp;amp;logoColor=white&amp;amp;style=flat-square&#34; alt=&#34;Docker Image Size (tag)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide a prebuilt Docker image for running DiscoArt out of the box. To update Docker image to latest version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull jinaai/discoart:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use Jupyter notebook&lt;/h3&gt; &#xA;&lt;p&gt;The default entrypoint is starting a Jupyter notebook&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# docker build . -t jinaai/discoart  # if you want to build yourself&#xA;docker run -p 51000:8888 -v $(pwd):/home/jovyan/ -v $HOME/.cache:/root/.cache --gpus all jinaai/discoart&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can visit &lt;code&gt;http://127.0.0.1:51000&lt;/code&gt; to access the notebook&lt;/p&gt; &#xA;&lt;h3&gt;Enable GPU in Docker on Windows&lt;/h3&gt; &#xA;&lt;p&gt;You can use it on Windows Subsystem for Linux (WSL), Check the official guide &lt;a href=&#34;https://docs.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Make sure you install Windows 11 or Windows 10, version 21H2&#xA;docker run -p 8888:8888 -v $HOME/.cache:/root/.cache --gpus all jinaai/discoart&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use as a service&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# docker build . -t jinaai/discoart  # if you want to build yourself&#xA;docker run --entrypoint &#34;python&#34; -p 51001:51001 -v $(pwd):/home/jovyan/ -v $HOME/.cache:/root/.cache --gpus all jinaai/discoart -m discoart serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Your DiscoArt server is now running at &lt;code&gt;http://127.0.0.1:51001&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Release cycle&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/jinaai/discoart&#34;&gt;Docker images are built on every release&lt;/a&gt;, so one can lock it to a specific version, say &lt;code&gt;0.5.1&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 51000:8888 -v $(pwd):/home/jovyan/ -v $HOME/.cache:/root/.cache --gpus all jinaai/discoart:0.5.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What&#39;s next?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/jina-ai/discoart/blob/main/discoart.ipynb&#34;&gt;Next is create&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üòé &lt;strong&gt;If you are already a DD user&lt;/strong&gt;: you are ready to go! There is no extra learning, DiscoArt respects the same parameter semantics as DD5.6. So just unleash your creativity! &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/FEATURES.md&#34;&gt;Read more about their differences here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can always do &lt;code&gt;from discoart import cheatsheet; cheatsheet()&lt;/code&gt; to check all new/modified parameters.&lt;/p&gt; &#xA;&lt;p&gt;üë∂ &lt;strong&gt;If you are a &lt;a href=&#34;https://github.com/jina-ai/dalle-flow/&#34;&gt;DALL¬∑E Flow&lt;/a&gt; or new user&lt;/strong&gt;: you may want to take step by step, as Disco Diffusion works in a very different way than DALL¬∑E. It is much more advanced and powerful: e.g. Disco Diffusion can take weighted &amp;amp; structured text prompts; it can initialize from a image with controlled noise; and there are way more parameters one can tweak. Impatient prompt like &lt;code&gt;&#34;armchair avocado&#34;&lt;/code&gt; will give you nothing but confusion and frustration. I highly recommend you to check out the following resources before trying your own prompt:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1l8s7uS2dGqjztYSjPpzlmXLjl5PM3IGkRWI3IiCuK7g/mobilebasic&#34;&gt;Zippy&#39;s Disco Diffusion Cheatsheet v0.3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1ORymHm0Te18qKiHnhcdgGp-WSt8ZkLZvow3raiu2DVU/edit#&#34;&gt;EZ Charts - Diffusion Parameter Studies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://weirdwonderfulai.art/resources/disco-diffusion-70-plus-artist-studies/&#34;&gt;Disco Diffusion 70+ Artist Studies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sweet-hall-e72.notion.site/A-Traveler-s-Guide-to-the-Latent-Space-85efba7e5e6a40e5bd3cae980f30235f#e122e748b86e4fc0ad6a7a50e46d6e10&#34;&gt;A Traveler‚Äôs Guide to the Latent Space&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://coar.notion.site/Disco-Diffusion-Illustrated-Settings-cd4badf06e08440c99d8a93d4cd39f51&#34;&gt;Disco Diffusion Illustrated Settings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://coar.notion.site/coar/Coar-s-Disco-Diffusion-Guide-3d86d652c15d4ca986325e808bde06aa#8a3c6e9e4b6847afa56106eacb6f1f79&#34;&gt;Coar‚Äôs Disco Diffusion Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- start support-pitch --&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://slack.jina.ai&#34;&gt;Slack community&lt;/a&gt; and chat with other community members about ideas.&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne&#34;&gt;Engineering All Hands&lt;/a&gt; meet-up to discuss your use case and learn Jina&#39;s new features. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;When?&lt;/strong&gt; The second Tuesday of every month&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Where?&lt;/strong&gt; Zoom (&lt;a href=&#34;https://calendar.google.com/calendar/embed?src=c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com&amp;amp;ctz=Europe%2FBerlin&#34;&gt;see our public events calendar&lt;/a&gt;/&lt;a href=&#34;https://calendar.google.com/calendar/ical/c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com/public/basic.ics&#34;&gt;.ical&lt;/a&gt;) and &lt;a href=&#34;https://youtube.com/c/jina-ai&#34;&gt;live stream on YouTube&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Subscribe to the latest video tutorials on our &lt;a href=&#34;https://youtube.com/c/jina-ai&#34;&gt;YouTube channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Join Us&lt;/h2&gt; &#xA;&lt;p&gt;DiscoArt is backed by &lt;a href=&#34;https://jina.ai&#34;&gt;Jina AI&lt;/a&gt; and licensed under &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/discoart/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;. &lt;a href=&#34;https://jobs.jina.ai&#34;&gt;We are actively hiring&lt;/a&gt; AI engineers, solution engineers to build the next neural search ecosystem in open-source.&lt;/p&gt; &#xA;&lt;!-- end support-pitch --&gt;</summary>
  </entry>
</feed>