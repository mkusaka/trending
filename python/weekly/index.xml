<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-10T01:59:24Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenTalker/SadTalker</title>
    <updated>2024-03-10T01:59:24Z</updated>
    <id>tag:github.com,2024-03-10:/OpenTalker/SadTalker</id>
    <link href="https://github.com/OpenTalker/SadTalker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2023] SadTalkerÔºöLearning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/4397546/229094115-862c747e-7397-4b54-ba4a-bd368bfe2e0f.png&#34; width=&#34;500px&#34;&gt; &#xA; &lt;!--&lt;h2&gt; üò≠ SadTalkerÔºö &lt;span style=&#34;font-size:12px&#34;&gt;Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation &lt;/span&gt; &lt;/h2&gt; --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12194&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ArXiv-PDF-red&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://sadtalker.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://huggingface.co/spaces/vinthony/SadTalker&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/video/stable/stable_diffusion_1_5_video_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Automatic1111-Colab-green&#34; alt=&#34;sd webui-colab&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;br&gt; &lt;a href=&#34;https://replicate.com/cjwbw/sadtalker&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/sadtalker/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/rrayYqZ4tf&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/rrayYqZ4tf?style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;div&gt; &#xA;  &lt;a target=&#34;_blank&#34;&gt;Wenxuan Zhang &lt;sup&gt;*,1,2&lt;/sup&gt; &lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://vinthony.github.io/&#34; target=&#34;_blank&#34;&gt;Xiaodong Cun &lt;sup&gt;*,2&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://xuanwangvc.github.io/&#34; target=&#34;_blank&#34;&gt;Xuan Wang &lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://yzhang2016.github.io/&#34; target=&#34;_blank&#34;&gt;Yong Zhang &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://xishen0220.github.io/&#34; target=&#34;_blank&#34;&gt;Xi Shen &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://yuguo-xjtu.github.io/&#34; target=&#34;_blank&#34;&gt;Yu Guo&lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=4oXBp9UAAAAJ&#34; target=&#34;_blank&#34;&gt;Ying Shan &lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;‚ÄÉ &#xA;  &lt;a target=&#34;_blank&#34;&gt;Fei Wang &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;‚ÄÉ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;sup&gt;1&lt;/sup&gt; Xi&#39;an Jiaotong University ‚ÄÉ &#xA;  &lt;sup&gt;2&lt;/sup&gt; Tencent AI Lab ‚ÄÉ &#xA;  &lt;sup&gt;3&lt;/sup&gt; Ant Group ‚ÄÉ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;i&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12194&#34; target=&#34;_blank&#34;&gt;CVPR 2023&lt;/a&gt;&lt;/strong&gt;&lt;/i&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4397546/222490039-b1f6156b-bf00-405b-9fda-0c9a9156f991.gif&#34; alt=&#34;sadtalker&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;b&gt;TL;DR: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; single portrait image üôé‚Äç‚ôÇÔ∏è &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;+ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; audio üé§ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; = &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; talking head video üéû.&lt;/b&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The license has been updated to Apache 2.0, and we&#39;ve removed the non-commercial restriction&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SadTalker has now officially been integrated into Discord, where you can use it for free by sending files. You can also generate high-quailty videos from text prompts. Join: &lt;a href=&#34;https://discord.gg/rrayYqZ4tf&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/rrayYqZ4tf?style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We&#39;ve published a &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;stable-diffusion-webui&lt;/a&gt; extension. Check out more details &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/webui_extension.md&#34;&gt;here&lt;/a&gt;. &lt;a href=&#34;https://user-images.githubusercontent.com/4397546/231495639-5d4bb925-ea64-4a36-a519-6389917dac29.mp4&#34;&gt;Demo Video&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Full image mode is now available! &lt;a href=&#34;https://github.com/OpenTalker/SadTalker#full-bodyimage-generation&#34;&gt;More details...&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;still+enhancer in v0.0.1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;still + enhancer in v0.0.2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/bagbag1815/status/1642754319094108161&#34;&gt;input image @bagbag1815&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://user-images.githubusercontent.com/48216707/229484996-5d7be64f-2553-4c9e-a452-c5cf0b8ebafe.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA;    &lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://user-images.githubusercontent.com/4397546/230717873-355b7bf3-d3de-49f9-a439-9220e623fce7.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA;    &lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/examples/source_image/full_body_2.png&#34; width=&#34;380&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Several new modes (Still, reference, and resize modes) are now available!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We&#39;re happy to see more community demos on &lt;a href=&#34;https://search.bilibili.com/all?keyword=sadtalker&#34;&gt;bilibili&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/results?search_query=sadtalker&#34;&gt;YouTube&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/search?q=%23sadtalker&amp;amp;src&#34;&gt;X (#sadtalker)&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;The previous changelog can be found &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/changlelog.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.06.12]&lt;/strong&gt;: Added more new features in WebUI extension, see the discussion &lt;a href=&#34;https://github.com/OpenTalker/SadTalker/discussions/386&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.06.05]&lt;/strong&gt;: Released a new 512x512px (beta) face model. Fixed some bugs and improve the performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.15]&lt;/strong&gt;: Added a WebUI Colab notebook by &lt;a href=&#34;https://github.com/camenduru/&#34;&gt;@camenduru&lt;/a&gt;: &lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/video/stable/stable_diffusion_1_5_video_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Automatic1111-Colab-green&#34; alt=&#34;sd webui-colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.12]&lt;/strong&gt;: Added a more detailed WebUI installation document and fixed a problem when reinstalling.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.12]&lt;/strong&gt;: Fixed the WebUI safe issues becasue of 3rd-party packages, and optimized the output path in &lt;code&gt;sd-webui-extension&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.08]&lt;/strong&gt;: In v0.0.2, we added a logo watermark to the generated video to prevent abuse. &lt;em&gt;This watermark has since been removed in a later release.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.08]&lt;/strong&gt;: In v0.0.2, we added features for full image animation and a link to download checkpoints from Baidu. We also optimized the enhancer logic.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;To-Do&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re tracking new updates in &lt;a href=&#34;https://github.com/OpenTalker/SadTalker/issues/280&#34;&gt;issue #280&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;If you have any problems, please read our &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/FAQ.md&#34;&gt;FAQs&lt;/a&gt; before opening an issue.&lt;/p&gt; &#xA;&lt;h2&gt;1. Installation.&lt;/h2&gt; &#xA;&lt;p&gt;Community tutorials: &lt;a href=&#34;https://www.bilibili.com/video/BV1Dc411W7V6/&#34;&gt;‰∏≠ÊñáWindowsÊïôÁ®ã (Chinese Windows tutorial)&lt;/a&gt; | &lt;a href=&#34;https://br-d.fanbox.cc/posts/5685086&#34;&gt;Êó•Êú¨Ë™û„Ç≥„Éº„Çπ (Japanese tutorial)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Linux/Unix&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;Anaconda&lt;/a&gt;, Python and &lt;code&gt;git&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Creating the env and install the requirements.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/OpenTalker/SadTalker.git&#xA;&#xA;cd SadTalker &#xA;&#xA;conda create -n sadtalker python=3.8&#xA;&#xA;conda activate sadtalker&#xA;&#xA;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;&#xA;conda install ffmpeg&#xA;&#xA;pip install -r requirements.txt&#xA;&#xA;### Coqui TTS is optional for gradio demo. &#xA;### pip install TTS&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;A video tutorial in chinese is available &lt;a href=&#34;https://www.bilibili.com/video/BV1Dc411W7V6/&#34;&gt;here&lt;/a&gt;. You can also follow the following instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.python.org/downloads/windows/&#34;&gt;Python 3.8&lt;/a&gt; and check &#34;Add Python to PATH&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;git&lt;/a&gt; manually or using &lt;a href=&#34;https://scoop.sh/&#34;&gt;Scoop&lt;/a&gt;: &lt;code&gt;scoop install git&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;code&gt;ffmpeg&lt;/code&gt;, following &lt;a href=&#34;https://www.wikihow.com/Install-FFmpeg-on-Windows&#34;&gt;this tutorial&lt;/a&gt; or using &lt;a href=&#34;https://scoop.sh/&#34;&gt;scoop&lt;/a&gt;: &lt;code&gt;scoop install ffmpeg&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download the SadTalker repository by running &lt;code&gt;git clone https://github.com/Winfredy/SadTalker.git&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download the checkpoints and gfpgan models in the &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/#2-download-models&#34;&gt;downloads section&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;start.bat&lt;/code&gt; from Windows Explorer as normal, non-administrator, user, and a Gradio-powered WebUI demo will be started.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;macOS&lt;/h3&gt; &#xA;&lt;p&gt;A tutorial on installing SadTalker on macOS can be found &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/install.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Docker, WSL, etc&lt;/h3&gt; &#xA;&lt;p&gt;Please check out additional tutorials &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/install.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;2. Download Models&lt;/h2&gt; &#xA;&lt;p&gt;You can run the following script on Linux/macOS to automatically download all the models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide an offline patch (&lt;code&gt;gfpgan/&lt;/code&gt;), so no model will be downloaded when generating.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-Trained Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/1gwWh45pF7aelNP_P78uDJL8Sycep-K7j/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenTalker/SadTalker/releases&#34;&gt;GitHub Releases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pan.baidu.com/s/1kb1BCPaLOWX1JJb9Czbn6w?pwd=sadt&#34;&gt;Baidu (ÁôæÂ∫¶‰∫ëÁõò)&lt;/a&gt; (Password: &lt;code&gt;sadt&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- TODO add Hugging Face links --&gt; &#xA;&lt;h3&gt;GFPGAN Offline Patch&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/19AIBsmfcHW6BRJmeqSFlG5fL445Xmsyi?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenTalker/SadTalker/releases&#34;&gt;GitHub Releases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pan.baidu.com/s/1P4fRgk9gaSutZnn8YW034Q?pwd=sadt&#34;&gt;Baidu (ÁôæÂ∫¶‰∫ëÁõò)&lt;/a&gt; (Password: &lt;code&gt;sadt&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- TODO add Hugging Face links --&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Model Details&lt;/summary&gt; &#xA; &lt;p&gt;Model explains:&lt;/p&gt; &#xA; &lt;h5&gt;New version&lt;/h5&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00229-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00109-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/SadTalker_V0.0.2_256.safetensors&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;packaged sadtalker checkpoints of old version, 256 face render).&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/SadTalker_V0.0.2_512.safetensors&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;packaged sadtalker checkpoints of old version, 512 face render).&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;gfpgan/weights&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face detection and enhanced models used in &lt;code&gt;facexlib&lt;/code&gt; and &lt;code&gt;gfpgan&lt;/code&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h5&gt;Old version&lt;/h5&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/auido2exp_00300-model.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained ExpNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/auido2pose_00140-model.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained PoseVAE in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00229-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00109-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/facevid2vid_00189-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained face-vid2vid model from &lt;a href=&#34;https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis&#34;&gt;the reappearance of face-vid2vid&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/epoch_20.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained 3DMM extractor in &lt;a href=&#34;https://github.com/microsoft/Deep3DFaceReconstruction&#34;&gt;Deep3DFaceReconstruction&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/wav2lip.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Highly accurate lip-sync model in &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2lip&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/shape_predictor_68_face_landmarks.dat&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face landmark model used in &lt;a href=&#34;http://dlib.net/&#34;&gt;dilb&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/BFM&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;3DMM library file.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/hub&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face detection models used in &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face alignment&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;gfpgan/weights&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face detection and enhanced models used in &lt;code&gt;facexlib&lt;/code&gt; and &lt;code&gt;gfpgan&lt;/code&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;The final folder will be shown as:&lt;/p&gt; &#xA; &lt;img width=&#34;331&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/4397546/232511411-4ca75cbf-a434-48c5-9ae0-9009e8316484.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;3. Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Please read our document on &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/best_practice.md&#34;&gt;best practices and configuration tips&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;WebUI Demos&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Online Demo&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/spaces/vinthony/SadTalker&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/video/stable/stable_diffusion_1_5_video_webui_colab.ipynb&#34;&gt;SDWebUI-Colab&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb&#34;&gt;Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Local WebUI extension&lt;/strong&gt;: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/webui_extension.md&#34;&gt;WebUI docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Local gradio demo (recommanded)&lt;/strong&gt;: A Gradio instance similar to our &lt;a href=&#34;https://huggingface.co/spaces/vinthony/SadTalker&#34;&gt;Hugging Face demo&lt;/a&gt; can be run locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## you need manually install TTS(https://github.com/coqui-ai/TTS) via `pip install tts` in advanced.&#xA;python app_sadtalker.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also start it more easily:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;windows: just double click &lt;code&gt;webui.bat&lt;/code&gt;, the requirements will be installed automatically.&lt;/li&gt; &#xA; &lt;li&gt;Linux/Mac OS: run &lt;code&gt;bash webui.sh&lt;/code&gt; to start the webui.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CLI usage&lt;/h3&gt; &#xA;&lt;h5&gt;Animating a portrait image from default config:&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --driven_audio &amp;lt;audio.wav&amp;gt; \&#xA;                    --source_image &amp;lt;video.mp4 or picture.png&amp;gt; \&#xA;                    --enhancer gfpgan &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results will be saved in &lt;code&gt;results/$SOME_TIMESTAMP/*.mp4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Full body/image Generation:&lt;/h5&gt; &#xA;&lt;p&gt;Using &lt;code&gt;--still&lt;/code&gt; to generate a natural full body video. You can add &lt;code&gt;enhancer&lt;/code&gt; to improve the quality of the generated video.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --driven_audio &amp;lt;audio.wav&amp;gt; \&#xA;                    --source_image &amp;lt;video.mp4 or picture.png&amp;gt; \&#xA;                    --result_dir &amp;lt;a file to store results&amp;gt; \&#xA;                    --still \&#xA;                    --preprocess full \&#xA;                    --enhancer gfpgan &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More examples and configuration and tips can be founded in the &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/best_practice.md&#34;&gt; &amp;gt;&amp;gt;&amp;gt; best practice documents &amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zhang2022sadtalker,&#xA;  title={SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation},&#xA;  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},&#xA;  journal={arXiv preprint arXiv:2211.12194},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Facerender code borrows heavily from &lt;a href=&#34;https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis&#34;&gt;zhanglonghao&#39;s reproduction of face-vid2vid&lt;/a&gt; and &lt;a href=&#34;https://github.com/RenYurui/PIRender&#34;&gt;PIRender&lt;/a&gt;. We thank the authors for sharing their wonderful code. In training process, we also used the model from &lt;a href=&#34;https://github.com/microsoft/Deep3DFaceReconstruction&#34;&gt;Deep3DFaceReconstruction&lt;/a&gt; and &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2lip&lt;/a&gt;. We thank for their wonderful work.&lt;/p&gt; &#xA;&lt;p&gt;We also use the following 3rd-party libraries:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Utils&lt;/strong&gt;: &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;https://github.com/xinntao/facexlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Enhancement&lt;/strong&gt;: &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;https://github.com/TencentARC/GFPGAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Image/Video Enhancement&lt;/strong&gt;:&lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;https://github.com/xinntao/Real-ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Extensions:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Zz-ww/SadTalker-Video-Lip-Sync&#34;&gt;SadTalker-Video-Lip-Sync&lt;/a&gt; from &lt;a href=&#34;https://github.com/Zz-ww&#34;&gt;@Zz-ww&lt;/a&gt;: SadTalker for Video Lip Editing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/StyleHEAT&#34;&gt;StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN (ECCV 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Doubiiu/CodeTalker&#34;&gt;CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vinthony/video-retalking&#34;&gt;VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild (SIGGRAPH Asia 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Carlyx/DPE&#34;&gt;DPE: Disentanglement of Pose and Expression for General Video Portrait Editing (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/SPI/&#34;&gt;3D GAN Inversion with Facial Symmetry Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mael-zys/T2M-GPT&#34;&gt;T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an official product of Tencent.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;1. Please carefully read and comply with the open-source license applicable to this code before using it. &#xA;2. Please carefully read and comply with the intellectual property declaration applicable to this code before using it.&#xA;3. This open-source code runs completely offline and does not collect any personal information or other data. If you use this code to provide services to end-users and collect related data, please take necessary compliance measures according to applicable laws and regulations (such as publishing privacy policies, adopting necessary data security strategies, etc.). If the collected data involves personal information, user consent must be obtained (if applicable). Any legal liabilities arising from this are unrelated to Tencent.&#xA;4. Without Tencent&#39;s written permission, you are not authorized to use the names or logos legally owned by Tencent, such as &#34;Tencent.&#34; Otherwise, you may be liable for legal responsibilities.&#xA;5. This open-source code does not have the ability to directly provide services to end-users. If you need to use this code for further model training or demos, as part of your product to provide services to end-users, or for similar use, please comply with applicable laws and regulations for your product or service. Any legal liabilities arising from this are unrelated to Tencent.&#xA;6. It is prohibited to use this open-source code for activities that harm the legitimate rights and interests of others (including but not limited to fraud, deception, infringement of others&#39; portrait rights, reputation rights, etc.), or other behaviors that violate applicable laws and regulations or go against social ethics and good customs (including providing incorrect or false information, spreading pornographic, terrorist, and violent information, etc.). Otherwise, you may be liable for legal responsibilities.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LOGO: color and font suggestion: &lt;a href=&#34;https://chat.openai.com&#34;&gt;ChatGPT&lt;/a&gt;, logo font: &lt;a href=&#34;https://fonts.google.com/specimen/Montserrat+Alternates?preview.text=SadTalker&amp;amp;preview.text_type=custom&amp;amp;query=mont&#34;&gt;Montserrat Alternates &lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All the copyrights of the demo images and audio are from community users or the generation from stable diffusion. Feel free to contact us if you would like use to remove them.&lt;/p&gt; &#xA;&lt;!-- Spelling fixed on Tuesday, September 12, 2023 by @fakerybakery (https://github.com/fakerybakery). These changes are licensed under the Apache 2.0 license. --&gt;</summary>
  </entry>
  <entry>
    <title>h2oai/h2ogpt</title>
    <updated>2024-03-10T01:59:24Z</updated>
    <id>tag:github.com,2024-03-10:/h2oai/h2ogpt</id>
    <link href="https://github.com/h2oai/h2ogpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Private chat with local GPT with document, images, video, etc. 100% private, Apache 2.0. Supports oLLaMa, Mixtral, llama.cpp, and more. Demo: https://gpt.h2o.ai/ https://codellama.h2o.ai/&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;h2oGPT&lt;/h1&gt; &#xA;&lt;p&gt;Turn ‚òÖ into ‚≠ê (top-right corner) if you like the project!&lt;/p&gt; &#xA;&lt;p&gt;Query and summarize your documents or just chat with local private GPT LLMs using h2oGPT, an Apache V2 open-source project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt; offline database of any documents &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md#supported-datatypes&#34;&gt;(PDFs, Excel, Word, Images, Video Frames, Youtube, Audio, Code, Text, MarkDown, etc.)&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Persistent&lt;/strong&gt; database (Chroma, Weaviate, or in-memory FAISS) using accurate embeddings (instructor-large, all-MiniLM-L6-v2, etc.)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Efficient&lt;/strong&gt; use of context using instruct-tuned LLMs (no need for LangChain&#39;s few-shot approach)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Parallel&lt;/strong&gt; summarization and extraction, reaching an output of 80 tokens per second with the 13B LLaMa2 model&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;HYDE&lt;/strong&gt; (Hypothetical Document Embeddings) for enhanced retrieval based upon LLM responses&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Variety&lt;/strong&gt; of models supported (LLaMa2, Mistral, Falcon, Vicuna, WizardLM. With AutoGPTQ, 4-bit/8-bit, LORA, etc.) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt; support from HF and LLaMa.cpp GGML models, and &lt;strong&gt;CPU&lt;/strong&gt; support using HF, LLaMa.cpp, and GPT4ALL models&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Attention Sinks&lt;/strong&gt; for &lt;a href=&#34;https://github.com/tomaarsen/attention_sinks&#34;&gt;arbitrarily long&lt;/a&gt; generation (LLaMa-2, Mistral, MPT, Pythia, Falcon, etc.)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt; or CLI with streaming of all models &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; and &lt;strong&gt;View&lt;/strong&gt; documents through the UI (control multiple collaborative or personal collections)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Vision LLaVa&lt;/strong&gt; Model and &lt;strong&gt;Stable Diffusion&lt;/strong&gt; Image Generation&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Voice STT&lt;/strong&gt; using Whisper with streaming audio conversion&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Voice TTS&lt;/strong&gt; using MIT-Licensed Microsoft Speech T5 with multiple voices and Streaming audio conversion&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Voice TTS&lt;/strong&gt; using MPL2-Licensed TTS including Voice Cloning and Streaming audio conversion&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AI Assistant Voice Control Mode&lt;/strong&gt; for hands-free control of h2oGPT chat&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Bake-off&lt;/strong&gt; UI mode against many models at the same time&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Easy Download&lt;/strong&gt; of model artifacts and control over models like LLaMa.cpp through the UI&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt; in the UI by user/password&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;State Preservation&lt;/strong&gt; in the UI by user/password&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux, Docker, macOS, and Windows&lt;/strong&gt; support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#windows-1011-64-bit-with-full-document-qa-capability&#34;&gt;&lt;strong&gt;Easy Windows Installer&lt;/strong&gt;&lt;/a&gt; for Windows 10 64-bit (CPU/CUDA)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#macos-cpum1m2-with-full-document-qa-capability&#34;&gt;&lt;strong&gt;Easy macOS Installer&lt;/strong&gt;&lt;/a&gt; for macOS (CPU/M1/M2)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference Servers&lt;/strong&gt; support (oLLaMa, HF TGI server, vLLM, Gradio, ExLLaMa, Replicate, OpenAI, Azure OpenAI, Anthropic)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI-compliant&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Server Proxy API (h2oGPT acts as drop-in-replacement to OpenAI server)&lt;/li&gt; &#xA;   &lt;li&gt;Python client API (to talk to Gradio server)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Web-Search&lt;/strong&gt; integration with Chat and Document Q/A&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agents&lt;/strong&gt; for Search, Document Q/A, Python Code, CSV frames (Experimental, best with OpenAI currently)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluate&lt;/strong&gt; performance using reward models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quality&lt;/strong&gt; maintained with over 1000 unit and integration tests taking over 4 GPU-hours&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Get Started&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/NVIDIA/nvidia-docker?style=flat-square&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/h2oai/h2ogpt/raw/main/docs/README_LINUX.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=black&#34; alt=&#34;Linux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/h2oai/h2ogpt/raw/main/docs/README_MACOS.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;amp;logo=macos&amp;amp;logoColor=F0F0F0&#34; alt=&#34;macOS&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/h2oai/h2ogpt/raw/main/docs/README_WINDOWS.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;amp;logo=windows&amp;amp;logoColor=white&#34; alt=&#34;Windows&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/h2oai/h2ogpt/raw/main/docs/README_DOCKER.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&amp;amp;logo=docker&amp;amp;logoColor=white&#34; alt=&#34;Docker&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To quickly try out h2oGPT with limited document Q/A capability, create a fresh Python 3.10 environment and run:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CPU or MAC (M1/M2): &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# for windows/mac use &#34;set&#34; or relevant environment setting mechanism&#xA;export PIP_EXTRA_INDEX_URL=&#34;https://download.pytorch.org/whl/cpu&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Linux/Windows CPU/CUDA/ROC: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# for windows/mac use &#34;set&#34; or relevant environment setting mechanism&#xA;export PIP_EXTRA_INDEX_URL=&#34;https://download.pytorch.org/whl/cu121 https://huggingface.github.io/autogptq-index/whl/cu121&#34;&#xA;# for cu118 use export PIP_EXTRA_INDEX_URL=&#34;https://download.pytorch.org/whl/cu118 https://huggingface.github.io/autogptq-index/whl/cu118&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then choose your llama_cpp_python options, by changing &lt;code&gt;CMAKE_ARGS&lt;/code&gt; to whichever system you have according to &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#supported-backends&#34;&gt;llama_cpp_python backend documentation&lt;/a&gt;. E.g. CUDA on Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LLAMA_CUBLAS=1&#xA;export CMAKE_ARGS=&#34;-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all&#34;&#xA;export FORCE_CMAKE=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note for some reason things will fail with llama_cpp_python if don&#39;t add all cuda arches, and building with all those arches does take some time. Windows CUDA:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmdline&#34;&gt;set CMAKE_ARGS=-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all&#xA;set LLAMA_CUBLAS=1&#xA;set FORCE_CMAKE=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note for some reason things will fail with llama_cpp_python if don&#39;t add all cuda arches, and building with all those arches does take some time. Metal M1/M2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_ARGS=&#34;-DLLAMA_METAL=on&#34;&#xA;export FORCE_CMAKE=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following commands on any system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/h2oai/h2ogpt.git&#xA;cd h2ogpt&#xA;pip install -r requirements.txt&#xA;pip install -r reqs_optional/requirements_optional_langchain.txt&#xA;&#xA;pip uninstall llama_cpp_python llama_cpp_python_cuda -y&#xA;pip install -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt --no-cache-dir&#xA;&#xA;pip install -r reqs_optional/requirements_optional_langchain.urls.txt&#xA;# GPL, only run next line if that is ok:&#xA;pip install -r reqs_optional/requirements_optional_langchain.gpllike.txt&#xA;&#xA;python generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --prompt_type=zephyr --max_seq_len=4096&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, go to your browser by visiting &lt;a href=&#34;http://127.0.0.1:7860&#34;&gt;http://127.0.0.1:7860&lt;/a&gt; or &lt;a href=&#34;http://localhost:7860&#34;&gt;http://localhost:7860&lt;/a&gt;. Choose 13B for a better model than 7B.&lt;/p&gt; &#xA;&lt;p&gt;We recommend quantized models for most small-GPU systems, e.g. &lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf&#34;&gt;LLaMa-2-7B-Chat-GGUF&lt;/a&gt; for 9GB+ GPU memory or larger models like &lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-13b-chat.Q6_K.gguf&#34;&gt;LLaMa-2-13B-Chat-GGUF&lt;/a&gt; if you have 16GB+ GPU memory.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_offline.md#tldr&#34;&gt;Offline&lt;/a&gt; for how to run h2oGPT offline.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Note that for all platforms, some packages such as DocTR, Unstructured, BLIP, Stable Diffusion, etc. download models at runtime that appear to delay operations in the UI. The progress appears in the console logs.&lt;/p&gt; &#xA;&lt;h4&gt;Windows 10/11 64-bit with full document Q/A capability&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;One-Click Installer&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;CPU or GPU: Download &lt;a href=&#34;https://h2o-release.s3.amazonaws.com/h2ogpt/Jan2024/h2oGPT_0.0.1.exe&#34;&gt;h2oGPT Windows Installer&lt;/a&gt; (1.3GB file) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Once installed, feel free to change start directory for icon from &lt;code&gt;%HOMEDRIVE%\%HOMEPATH%&lt;/code&gt; to (e.g.) &lt;code&gt;%HOMEDRIVE%\%HOMEPATH%\h2ogpt_data&lt;/code&gt; so all created files (like database) go there. All paths saved are relative to this path.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;CPU: Click the h2oGPT icon in the Start menu. Give it about 15 seconds to open in a browser if many optional packages are included. By default, the browser will launch with the actual local IP address, not localhost.&lt;/li&gt; &#xA;   &lt;li&gt;GPU: Before starting, run the following commands (replace &lt;code&gt;pseud&lt;/code&gt; with your user): &lt;pre&gt;&lt;code&gt;C:\Users\pseud\AppData\Local\Programs\h2oGPT\Python\python.exe -m pip uninstall -y torch&#xA;C:\Users\pseud\AppData\Local\Programs\h2oGPT\Python\python.exe -m pip install https://h2o-release.s3.amazonaws.com/h2ogpt/torch-2.1.2%2Bcu118-cp310-cp310-win_amd64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; Now click the h2oGPT icon in the Start menu. Give it about 20 seconds to open in a browser if many optional packages are included. By default, the browser will launch with the actual local IP address, not localhost. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Some other users may have python located here: &lt;code&gt;C:\Program Files (x86)\h2oGPT\Python\python.exe&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;To debug any issues, run the following (replace &lt;code&gt;pseud&lt;/code&gt; with your user): &lt;pre&gt;&lt;code&gt;C:\Users\pseud\AppData\Local\Programs\h2oGPT\Python\python.exe &#34;C:\Users\pseud\AppData\Local\Programs\h2oGPT\h2oGPT.launch.pyw&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; Any start-up exceptions are appended to log, e.g. &lt;code&gt;C:\Users\pseud\h2ogpt_exception.log&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To control startup, tweak the python startup file, e.g. for user &lt;code&gt;pseud&lt;/code&gt;: &lt;code&gt;C:\Users\pseud\AppData\Local\Programs\h2oGPT\pkgs\win_run_app.py&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In this Python code, set ENVs anywhere before main_h2ogpt() is called &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;E.g. &lt;code&gt;os.environ[&#39;name&#39;] = &#39;value&#39;&lt;/code&gt;, e.g. &lt;code&gt;os.environ[&#39;n_jobs&#39;] = &#39;10&#39;&lt;/code&gt; (must be always a string).&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Environment variables can be changed, e.g.: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;n_jobs&lt;/code&gt;: number of cores for various tasks&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;OMP_NUM_THREADS&lt;/code&gt; thread count for LLaMa&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; which GPUs are used. Recommend set to single fast GPU, e.g. &lt;code&gt;CUDA_VISIBLE_DEVICES=0&lt;/code&gt; if have multiple GPUs. Note that UI cannot control which GPUs (or CPU mode) for LLaMa models.&lt;/li&gt; &#xA;     &lt;li&gt;Any CLI argument from &lt;code&gt;python generate.py --help&lt;/code&gt; with environment variable set as &lt;code&gt;h2ogpt_x&lt;/code&gt;, e.g. &lt;code&gt;h2ogpt_h2ocolors&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Set env &lt;code&gt;h2ogpt_server_name&lt;/code&gt; to actual IP address for LAN to see app, e.g. &lt;code&gt;h2ogpt_server_name&lt;/code&gt; to &lt;code&gt;192.168.1.172&lt;/code&gt; and allow access through firewall if have Windows Defender activated.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;One can tweak installed h2oGPT code at, e.g. &lt;code&gt;C:\Users\pseud\AppData\Local\Programs\h2oGPT&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To terminate the app, go to System Tab and click Admin and click Shutdown h2oGPT.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If startup fails, run as console and check for errors, e.g. and kill any old Python processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_WINDOWS.md&#34;&gt;Full Windows 10/11 Manual Installation Script&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Single &lt;code&gt;.bat&lt;/code&gt; file for installation (if you do not skip any optional packages, takes about 9GB filled on disk).&lt;/li&gt; &#xA;   &lt;li&gt;Recommend base Conda env, which allows for DocTR that requires pygobject that has otherwise no support (except &lt;code&gt;mysys2&lt;/code&gt; that cannot be used by h2oGPT).&lt;/li&gt; &#xA;   &lt;li&gt;Also allows for the TTS package by Coqui, which is otherwise not currently enabled in the one-click installer.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Linux (CPU/CUDA) with full document Q/A capability&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_DOCKER.md&#34;&gt;Docker Build and Run Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LINUX.md&#34;&gt;Linux Manual Install and Run Docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;macOS (CPU/M1/M2) with full document Q/A capability&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;One-click Installers (Experimental and subject to changes, we haven&#39;t tested each and every feature with these installers, we encourage the community to try them and report any issues)&lt;/p&gt; &lt;p&gt;Mar 07, 2024&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://h2o-release.s3.amazonaws.com/h2ogpt/Mar2024/h2ogpt-osx-m1-cpu&#34;&gt;h2ogpt-osx-m1-cpu&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://h2o-release.s3.amazonaws.com/h2ogpt/Mar2024/h2ogpt-osx-m1-gpu&#34;&gt;h2ogpt-osx-m1-gpu&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Nov 08, 2023&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-cpu&#34;&gt;h2ogpt-osx-m1-cpu&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-gpu&#34;&gt;h2ogpt-osx-m1-gpu&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Download the runnable file and open it from the Finder. It will take a few minutes to unpack and run the application. These one-click installers are experimental. Report any issues with steps to reproduce at &lt;a href=&#34;https://github.com/h2oai/h2ogpt/issues&#34;&gt;https://github.com/h2oai/h2ogpt/issues&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The app bundle is unsigned. If you experience any issues with running the app, run the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ xattr -dr com.apple.quarantine {file-path}/h2ogpt-osx-m1-gpu&#xA;$ chmod +x {file-path}/h2ogpt-osx-m1-gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_MACOS.md&#34;&gt;macOS Manual Install and Run Docs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Example Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h2oai/h2ogpt-4096-llama2-70b-chat&#34;&gt;Highest accuracy and speed&lt;/a&gt; on 16-bit with TGI/vLLM using ~48GB/GPU when in use (4xA100 high concurrency, 2xA100 for low concurrency)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2&#34;&gt;Middle-range accuracy&lt;/a&gt; on 16-bit with TGI/vLLM using ~45GB/GPU when in use (2xA100)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-13B-Chat-GGUF&#34;&gt;Small memory profile with ok accuracy&lt;/a&gt; 16GB GPU if full GPU offloading&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h2oai/h2ogpt-4096-llama2-13b-chat&#34;&gt;Balanced accuracy and size&lt;/a&gt; on 16-bit with TGI/vLLM using ~45GB/GPU when in use (1xA100)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF&#34;&gt;Smallest or CPU friendly&lt;/a&gt; 32GB system ram or 9GB GPU if full GPU offloading&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-70B-chat-AWQ&#34;&gt;Best for 4*A10G using g5.12xlarge&lt;/a&gt; AWQ LLaMa 70B using 4*A10G using vLLM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt; mode requires CUDA support via torch and transformers. A 7B/13B model in 16-bit uses 14GB/26GB of GPU memory to store the weights (2 bytes per weight). Compression such as 4-bit precision (bitsandbytes, AWQ, GPTQ, etc.) can further reduce memory requirements down to less than 6GB when asking a question about your documents. (For more information, see &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#low-memory-mode&#34;&gt;low-memory mode&lt;/a&gt;.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt; mode uses GPT4ALL and LLaMa.cpp, e.g. gpt4all-j, requiring about 14GB of system RAM in typical use.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Live Demos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpt.h2o.ai/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/img-small.png&#34; alt=&#34;img-small.png&#34;&gt; Live h2oGPT Document Q/A Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/h2oai/h2ogpt-chatbot&#34;&gt;ü§ó Live h2oGPT Chat Demo 1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/h2oai/h2ogpt-chatbot2&#34;&gt;ü§ó Live h2oGPT Chat Demo 2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/13RiBdAFZ6xqDwDKfW6BG_-tXfXiqPNQe?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt; h2oGPT CPU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/143-KFHs2iCqXTQLI2pFCDiR69z0dR8iE?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt; h2oGPT GPU&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference Benchmarks for Summarization &amp;amp; Generation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/h2oai/h2ogpt/raw/main/benchmarks/perf.md&#34;&gt;Benchmark results for Llama2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/h2oai/h2ogpt/raw/main/tests/test_perf_benchmarks.py&#34;&gt;pytest to create benchmark results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/h2oai/h2ogpt/raw/main/benchmarks/perf.json&#34;&gt;Raw benchmark results (JSON)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/WKhYMWcVbq&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h2oai/&#34;&gt;Models (LLaMa-2, Falcon 40, etc.) at ü§ó&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Coj72EzmX20&#34;&gt;YouTube: 100% Offline ChatGPT Alternative?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=FTm5C_vV_EY&#34;&gt;YouTube: Ultimate Open-Source LLM Showdown (6 Models Tested) - Surprising Results!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=H8Dx-iUY49s&#34;&gt;YouTube: Blazing Fast Falcon 40b üöÄ Uncensored, Open-Source, Fully Hosted, Chat With Your Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.08161.pdf&#34;&gt;Technical Paper: https://arxiv.org/pdf/2306.08161.pdf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Partners&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evalgpt.ai/&#34;&gt;Live Leaderboard&lt;/a&gt; for GPT-4 Elo Evaluation of Instruct/Chat models with &lt;a href=&#34;https://github.com/h2oai/h2o-LLM-eval&#34;&gt;h2o-LLM-eval&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Advanced fine-tuning with &lt;a href=&#34;https://github.com/h2oai/h2o-llmstudio&#34;&gt;H2O LLM Studio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Video Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/h2oai/h2ogpt/assets/2249614/2f805035-2c85-42fb-807f-fd0bca79abc6&#34;&gt;https://github.com/h2oai/h2ogpt/assets/2249614/2f805035-2c85-42fb-807f-fd0bca79abc6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;YouTube 4K version: &lt;a href=&#34;https://www.youtube.com/watch?v=_iktbj4obAI&#34;&gt;https://www.youtube.com/watch?v=_iktbj4obAI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docs Guide&lt;/h3&gt; &#xA;&lt;!--  cat README.md | ./gh-md-toc  -  But Help is heavily processed --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#get-started&#34;&gt;Get Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LINUX.md&#34;&gt;Linux (CPU or CUDA)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_MACOS.md&#34;&gt;macOS (CPU or M1/M2)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_WINDOWS.md&#34;&gt;Windows 10/11 (CPU or CUDA)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_GPU.md&#34;&gt;GPU (CUDA, AutoGPTQ, exllama) Running Details&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_CPU.md&#34;&gt;CPU Running Details&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_CLI.md&#34;&gt;CLI chat&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_ui.md&#34;&gt;Gradio UI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_CLIENT.md&#34;&gt;Client API (Gradio, OpenAI-Compliant)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_InferenceServers.md&#34;&gt;Inference Servers (oLLaMa, HF TGI server, vLLM, Gradio, ExLLaMa, Replicate, OpenAI, Azure OpenAI)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_WHEEL.md&#34;&gt;Build Python Wheel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_offline.md&#34;&gt;Offline Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#low-memory-mode&#34;&gt;Low Memory&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_DOCKER.md&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md&#34;&gt;LangChain Document Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like&#34;&gt;Compare to PrivateGPT et al.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#help&#34;&gt;Help&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md#supported-datatypes&#34;&gt;LangChain file types supported&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md#database-creation&#34;&gt;CLI Database control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md&#34;&gt;FAQ&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#model-usage-notes&#34;&gt;Model Usage Notes&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#adding-models&#34;&gt;Adding LLM Models (including using GGUF and Attention Sinks)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#add-new-embedding-model&#34;&gt;Adding Embedding Models&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#adding-prompt-templates&#34;&gt;Adding Prompts&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#in-context-learning-via-prompt-engineering&#34;&gt;In-Context Learning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#multiple-gpus&#34;&gt;Multiple GPUs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#low-memory-mode&#34;&gt;Low-Memory Usage&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#what-envs-can-i-pass-to-control-h2ogpt&#34;&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#https-access-for-server-and-client&#34;&gt;HTTPS access for server and client&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/LINKS.md&#34;&gt;Useful Links&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FINETUNE.md&#34;&gt;Fine-Tuning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/TRITON.md&#34;&gt;Triton&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md#commercial-viability&#34;&gt;Commercial viability&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#why-h2oai&#34;&gt;Why H2O.ai?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/#disclaimer&#34;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Experimental features&lt;/h3&gt; &#xA;&lt;p&gt;These are not part of normal installation instructions and are experimental.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_Agents.md&#34;&gt;Agents&lt;/a&gt; -- in Alpha testing. Optimal for OpenAI, but that also fails sometimes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Roadmap&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integration of code and resulting LLMs with downstream applications and low/no-code platforms&lt;/li&gt; &#xA; &lt;li&gt;Complement h2oGPT chatbot with other APIs like &lt;a href=&#34;https://github.com/OpenBMB/ToolBench&#34;&gt;ToolBench&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enhance the model&#39;s code completion, reasoning, and mathematical capabilities, ensure factual correctness, minimize hallucinations, and avoid repetitive output&lt;/li&gt; &#xA; &lt;li&gt;Add better agents for SQL and CSV question/answer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Development&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To create a development environment for training and generation, follow the &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/INSTALL.md&#34;&gt;installation instructions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To fine-tune any LLM models on your data, follow the &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FINETUNE.md&#34;&gt;fine-tuning instructions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To run h2oGPT tests: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install requirements-parser pytest-instafail pytest-random-order playsound==1.3.0&#xA;pytest --instafail -s -v tests&#xA;# for client tests&#xA;make -C client setup&#xA;make -C client build&#xA;pytest --instafail -s -v client/tests&#xA;# for openai server test on already-running local server&#xA;pytest -s -v -n 4 openai_server/test_openai_server.py::test_openai_client&#xA;&lt;/code&gt;&lt;/pre&gt; or tweak/run &lt;code&gt;tests/test4gpus.sh&lt;/code&gt; to run tests in parallel.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Help&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/FAQ.md&#34;&gt;FAQs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/README_LangChain.md&#34;&gt;README for LangChain&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Useful &lt;a href=&#34;https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/LINKS.md&#34;&gt;links&lt;/a&gt; for additional context and information on competitors, models, and datasets&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Some training code was based upon March 24 version of &lt;a href=&#34;https://github.com/tloen/alpaca-lora/&#34;&gt;Alpaca-LoRA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Used high-quality created data by &lt;a href=&#34;https://open-assistant.io/&#34;&gt;OpenAssistant&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Used base models by &lt;a href=&#34;https://www.eleuther.ai/&#34;&gt;EleutherAI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Used OIG data created by &lt;a href=&#34;https://laion.ai/blog/oig-dataset/&#34;&gt;LAION&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why H2O.ai?&lt;/h3&gt; &#xA;&lt;p&gt;Our &lt;a href=&#34;https://h2o.ai/company/team/&#34;&gt;Makers&lt;/a&gt; at &lt;a href=&#34;https://h2o.ai&#34;&gt;H2O.ai&lt;/a&gt; have built several world-class Machine Learning, Deep Learning and AI platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;#1 open-source machine learning platform for the enterprise &lt;a href=&#34;https://github.com/h2oai/h2o-3&#34;&gt;H2O-3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The world&#39;s best AutoML (Automatic Machine Learning) with &lt;a href=&#34;https://h2o.ai/platform/ai-cloud/make/h2o-driverless-ai/&#34;&gt;H2O Driverless AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;No-Code Deep Learning with &lt;a href=&#34;https://h2o.ai/platform/ai-cloud/make/hydrogen-torch/&#34;&gt;H2O Hydrogen Torch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Document Processing with Deep Learning in &lt;a href=&#34;https://h2o.ai/platform/ai-cloud/make/document-ai/&#34;&gt;Document AI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also built platforms for deployment and monitoring, and for data wrangling and governance:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://h2o.ai/platform/ai-cloud/operate/h2o-mlops/&#34;&gt;H2O MLOps&lt;/a&gt; to deploy and monitor models at scale&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://h2o.ai/platform/ai-cloud/make/feature-store/&#34;&gt;H2O Feature Store&lt;/a&gt; in collaboration with AT&amp;amp;T&lt;/li&gt; &#xA; &lt;li&gt;Open-source Low-Code AI App Development Frameworks &lt;a href=&#34;https://wave.h2o.ai/&#34;&gt;Wave&lt;/a&gt; and &lt;a href=&#34;https://nitro.h2o.ai/&#34;&gt;Nitro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open-source Python &lt;a href=&#34;https://github.com/h2oai/datatable/&#34;&gt;datatable&lt;/a&gt; (the engine for H2O Driverless AI feature engineering)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Many of our customers are creating models and deploying them enterprise-wide and at scale in the &lt;a href=&#34;https://h2o.ai/platform/ai-cloud/&#34;&gt;H2O AI Cloud&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-Cloud or on Premises&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://h2o.ai/platform/ai-cloud/managed&#34;&gt;Managed Cloud (SaaS)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://h2o.ai/platform/ai-cloud/hybrid&#34;&gt;Hybrid Cloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.h2o.ai/h2o-ai-cloud/&#34;&gt;AI Appstore&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are proud to have over 25 (of the world&#39;s 280) &lt;a href=&#34;https://h2o.ai/company/team/kaggle-grandmasters/&#34;&gt;Kaggle Grandmasters&lt;/a&gt; call H2O home, including three Kaggle Grandmasters who have made it to world #1.&lt;/p&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;Please read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.&lt;/li&gt; &#xA; &lt;li&gt;Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user&#39;s responsibility to critically evaluate the generated content and use it at their discretion.&lt;/li&gt; &#xA; &lt;li&gt;Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.&lt;/li&gt; &#xA; &lt;li&gt;Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.&lt;/li&gt; &#xA; &lt;li&gt;Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.&lt;/li&gt; &#xA; &lt;li&gt;Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user&#39;s responsibility to periodically review the disclaimer to stay informed about any changes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#h2oai/h2ogpt&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=h2oai/h2ogpt&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kijai/ComfyUI-SUPIR</title>
    <updated>2024-03-10T01:59:24Z</updated>
    <id>tag:github.com,2024-03-10:/kijai/ComfyUI-SUPIR</id>
    <link href="https://github.com/kijai/ComfyUI-SUPIR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SUPIR upscaling wrapper for ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI SUPIR upscaler wrapper node&lt;/h1&gt; &#xA;&lt;h2&gt;WORK IN PROGRESS&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kijai/ComfyUI-SUPIR/assets/40791699/887898d3-afe5-45d1-be08-50f6620b70eb&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installing&lt;/h1&gt; &#xA;&lt;p&gt;Either manager and install from git, or clone this repo to custom_nodes and run:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or if you use portable (run this in ComfyUI_windows_portable -folder):&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\ComfyUI-SUPIR\requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pytorch version should be pretty new too, latest stable (2.2.1) works.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;xformers&lt;/code&gt; is automatically detected and enabled if found, but it&#39;s not necessary, in some cases it can be a bit faster though:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -U xformers --no-dependencies&lt;/code&gt; (for portable &lt;code&gt;python_embeded\python.exe -m pip install -U xformers --no-dependencies&lt;/code&gt; )&lt;/p&gt; &#xA;&lt;p&gt;Get the SUPIR model(s) from the original links below, they are loaded from the normal &lt;code&gt;ComfyUI/models/checkpoints&lt;/code&gt; -folder In addition you need an SDXL model, they are loaded from the same folder.&lt;/p&gt; &#xA;&lt;p&gt;I have not included llava in this, but you can input any captions to the node and thus use anything you want to generate them, or just don&#39;t, seems to work great even without.&lt;/p&gt; &#xA;&lt;p&gt;Memory requirements are directly related to the input image resolution, the &#34;scale_by&#34; in the node simply scales the input, you can leave it at 1.0 and size your input with any other node as well. In my testing I was able to run 512x512 to 1024x1024 with a 10GB 3080 GPU, and other tests on 24GB GPU to up 3072x3072. System RAM requirements are also hefty, don&#39;t know numbers but I would guess under 32GB is going to have issues, tested with 64GB.&lt;/p&gt; &#xA;&lt;h2&gt;Update:&lt;/h2&gt; &#xA;&lt;p&gt;fp8 seems to work fine for the unet, I was able to do 512p to 2048 with under 10GB VRAM used. For the VAE it seems to cause artifacts, I recommend using tiled_vae instead.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Mirror for the models: &lt;a href=&#34;https://huggingface.co/camenduru/SUPIR/tree/main&#34;&gt;https://huggingface.co/camenduru/SUPIR/tree/main&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Tests&lt;/h1&gt; &#xA;&lt;p&gt;Video upscale test (currently the node does frames one by one from input batch):&lt;/p&gt; &#xA;&lt;p&gt;Original: &lt;a href=&#34;https://github.com/kijai/ComfyUI-SUPIR/assets/40791699/33621520-a429-4155-aa3a-ac5cd15bda56&#34;&gt;https://github.com/kijai/ComfyUI-SUPIR/assets/40791699/33621520-a429-4155-aa3a-ac5cd15bda56&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Upscaled 3x: &lt;a href=&#34;https://github.com/kijai/ComfyUI-SUPIR/assets/40791699/d6c60e0a-11c3-496d-82c6-a724758a131a&#34;&gt;https://github.com/kijai/ComfyUI-SUPIR/assets/40791699/d6c60e0a-11c3-496d-82c6-a724758a131a&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Image upscale from 3x from 512p: &lt;a href=&#34;https://github.com/kijai/ComfyUI-SUPIR/assets/40791699/545ddce4-8324-45cb-a545-6d1f527d8750&#34;&gt;https://github.com/kijai/ComfyUI-SUPIR/assets/40791699/545ddce4-8324-45cb-a545-6d1f527d8750&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Original repo: &lt;a href=&#34;https://github.com/Fanghua-Yu/SUPIR&#34;&gt;https://github.com/Fanghua-Yu/SUPIR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Models we provided:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;SUPIR-v0Q&lt;/code&gt;: &lt;a href=&#34;https://pan.baidu.com/s/1lnefCZhBTeDWijqbj1jIyw?pwd=pjq6&#34;&gt;Baidu Netdisk&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/drive/folders/1yELzm5SvAi9e7kPcO_jPp2XkTs4vK6aR?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Default training settings with paper. High generalization and high image quality in most cases.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;SUPIR-v0F&lt;/code&gt;: &lt;a href=&#34;https://pan.baidu.com/s/1AECN8NjiVuE3hvO8o-Ua6A?pwd=k2uz&#34;&gt;Baidu Netdisk&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/drive/folders/1yELzm5SvAi9e7kPcO_jPp2XkTs4vK6aR?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training with light degradation settings. Stage1 encoder of &lt;code&gt;SUPIR-v0F&lt;/code&gt; remains more details when facing light degradations.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{yu2024scaling,&#xA;  title={Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild}, &#xA;  author={Fanghua Yu and Jinjin Gu and Zheyuan Li and Jinfan Hu and Xiangtao Kong and Xintao Wang and Jingwen He and Yu Qiao and Chao Dong},&#xA;  year={2024},&#xA;  eprint={2401.13627},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üìß Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question, please email &lt;code&gt;fanghuayu96@gmail.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Non-Commercial Use Only Declaration&lt;/h2&gt; &#xA;&lt;p&gt;The SUPIR (&#34;Software&#34;) is made available for use, reproduction, and distribution strictly for non-commercial purposes. For the purposes of this declaration, &#34;non-commercial&#34; is defined as not primarily intended for or directed towards commercial advantage or monetary compensation.&lt;/p&gt; &#xA;&lt;p&gt;By using, reproducing, or distributing the Software, you agree to abide by this restriction and not to use the Software for any commercial purposes without obtaining prior written permission from Dr. Jinjin Gu.&lt;/p&gt; &#xA;&lt;p&gt;This declaration does not in any way limit the rights under any open source license that may apply to the Software; it solely adds a condition that the Software shall not be used for commercial purposes.&lt;/p&gt; &#xA;&lt;p&gt;IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt; &#xA;&lt;p&gt;For inquiries or to obtain permission for commercial use, please contact Dr. Jinjin Gu (&lt;a href=&#34;mailto:hellojasongt@gmail.com&#34;&gt;hellojasongt@gmail.com&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
</feed>