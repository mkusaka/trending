<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-08T01:44:48Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FoundationVision/VAR</title>
    <updated>2024-12-08T01:44:48Z</updated>
    <id>tag:github.com,2024-12-08:/FoundationVision/VAR</id>
    <link href="https://github.com/FoundationVision/VAR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[NeurIPS 2024 Oral][GPT beats diffusionğŸ”¥] [scaling laws in visual generationğŸ“ˆ] Official impl. of &#34;Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction&#34;. An *ultra-simple, user-friendly yet state-of-the-art* codebase for autoregressive image generation!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VAR: a new visual generation method elevates GPT-style models beyond diffusionğŸš€ &amp;amp; Scaling laws observedğŸ“ˆ&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://var.vision/demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Play%20with%20VAR%21-VAR%20demo%20platform-lightblue&#34; alt=&#34;demo platform&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv%20paper-2404.02905-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://huggingface.co/FoundationVision/var&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Weights-FoundationVision/var-yellow&#34; alt=&#34;huggingface weights&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://paperswithcode.com/sota/image-generation-on-imagenet-256x256?tag_filter=485&amp;amp;p=visual-autoregressive-modeling-scalable-image&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/State%20of%20the%20Art-Image%20Generation%20on%20ImageNet%20%28AR%29-32B1B4?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iNjA2IiBoZWlnaHQ9IjYwNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgb3ZlcmZsb3c9ImhpZGRlbiI%2BPGRlZnM%2BPGNsaXBQYXRoIGlkPSJjbGlwMCI%2BPHJlY3QgeD0iLTEiIHk9Ii0xIiB3aWR0aD0iNjA2IiBoZWlnaHQ9IjYwNiIvPjwvY2xpcFBhdGg%2BPC9kZWZzPjxnIGNsaXAtcGF0aD0idXJsKCNjbGlwMCkiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDEgMSkiPjxyZWN0IHg9IjUyOSIgeT0iNjYiIHdpZHRoPSI1NiIgaGVpZ2h0PSI0NzMiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIxOSIgeT0iNjYiIHdpZHRoPSI1NyIgaGVpZ2h0PSI0NzMiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIyNzQiIHk9IjE1MSIgd2lkdGg9IjU3IiBoZWlnaHQ9IjMwMiIgZmlsbD0iIzQ0RjJGNiIvPjxyZWN0IHg9IjEwNCIgeT0iMTUxIiB3aWR0aD0iNTciIGhlaWdodD0iMzAyIiBmaWxsPSIjNDRGMkY2Ii8%2BPHJlY3QgeD0iNDQ0IiB5PSIxNTEiIHdpZHRoPSI1NyIgaGVpZ2h0PSIzMDIiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIzNTkiIHk9IjE3MCIgd2lkdGg9IjU2IiBoZWlnaHQ9IjI2NCIgZmlsbD0iIzQ0RjJGNiIvPjxyZWN0IHg9IjE4OCIgeT0iMTcwIiB3aWR0aD0iNTciIGhlaWdodD0iMjY0IiBmaWxsPSIjNDRGMkY2Ii8%2BPHJlY3QgeD0iNzYiIHk9IjY2IiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI0ODIiIHk9IjY2IiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI3NiIgeT0iNDgyIiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI0ODIiIHk9IjQ4MiIgd2lkdGg9IjQ3IiBoZWlnaHQ9IjU3IiBmaWxsPSIjNDRGMkY2Ii8%2BPC9nPjwvc3ZnPg%3D%3D&#34; alt=&#34;SOTA&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;font-size: larger;&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34; style=&#34;font-size: larger;&#34;&gt; &lt;strong&gt;NeurIPS 2024 Oral&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/9850df90-20b1-4f29-8592-e3526d16d755&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-12:&lt;/strong&gt; We Release our Text-to-Image research based on VAR, please check &lt;a href=&#34;https://arxiv.org/abs/2412.04431&#34;&gt;Infinity&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-09:&lt;/strong&gt; VAR is accepted as &lt;strong&gt;NeurIPS 2024 Oral&lt;/strong&gt; Presentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-04:&lt;/strong&gt; &lt;a href=&#34;https://github.com/FoundationVision/VAR&#34;&gt;Visual AutoRegressive modeling&lt;/a&gt; is released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ•¹ï¸ Try and Play with VAR!&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://var.vision/demo&#34;&gt;demo website&lt;/a&gt; for you to play with VAR models and generate images interactively. Enjoy the fun of visual autoregressive modeling!&lt;/p&gt; &#xA;&lt;p&gt;We also provide &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/demo_sample.ipynb&#34;&gt;demo_sample.ipynb&lt;/a&gt; for you to see more technical details about VAR.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New?&lt;/h2&gt; &#xA;&lt;h3&gt;ğŸ”¥ Introducing VAR: a new paradigm in autoregressive visual generationâœ¨:&lt;/h3&gt; &#xA;&lt;p&gt;Visual Autoregressive Modeling (VAR) redefines the autoregressive learning on images as coarse-to-fine &#34;next-scale prediction&#34; or &#34;next-resolution prediction&#34;, diverging from the standard raster-scan &#34;next-token prediction&#34;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/3e12655c-37dc-4528-b923-ec6c4cfef178&#34; width=&#34;93%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;ğŸ”¥ For the first time, GPT-style autoregressive models surpass diffusion modelsğŸš€:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/cc30b043-fa4e-4d01-a9b1-e50650d5675d&#34; width=&#34;55%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;ğŸ”¥ Discovering power-law Scaling Laws in VAR transformersğŸ“ˆ:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/c35fb56e-896e-4e4b-9fb9-7a1c38513804&#34; width=&#34;85%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/91d7b92c-8fc3-44d9-8fb4-73d6cdb8ec1e&#34; width=&#34;85%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;ğŸ”¥ Zero-shot generalizabilityğŸ› ï¸:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/a54a4e52-6793-4130-bae2-9e459a08e96a&#34; width=&#34;70%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h4&gt;For a deep dive into our analyses, discussions, and evaluations, check out our &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;paper&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;h2&gt;VAR zoo&lt;/h2&gt; &#xA;&lt;p&gt;We provide VAR models for you to play with, which are on &lt;a href=&#34;https://huggingface.co/FoundationVision/var&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Huggingface-FoundationVision/var-yellow&#34;&gt;&lt;/a&gt; or can be downloaded from the following links:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;reso.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FID&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;rel. cost&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;HF weightsğŸ¤—&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.55&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;310M&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d16.pth&#34;&gt;var_d16.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;600M&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d20.pth&#34;&gt;var_d20.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d24.pth&#34;&gt;var_d24.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d30.pth&#34;&gt;var_d30.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d30-re&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.80&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d30.pth&#34;&gt;var_d30.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can load these models to generate images via the codes in &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/demo_sample.ipynb&#34;&gt;demo_sample.ipynb&lt;/a&gt;. Note: you need to download &lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/vae_ch160v4096z32.pth&#34;&gt;vae_ch160v4096z32.pth&lt;/a&gt; first.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;torch&amp;gt;=2.0.0&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install other pip packages via &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Prepare the &lt;a href=&#34;http://image-net.org/&#34;&gt;ImageNet&lt;/a&gt; dataset&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt; assume the ImageNet is in `/path/to/imagenet`. It should be like this:&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code&gt;/path/to/imagenet/:&#xA;    train/:&#xA;        n01440764: &#xA;            many_images.JPEG ...&#xA;        n01443537:&#xA;            many_images.JPEG ...&#xA;    val/:&#xA;        n01440764:&#xA;            ILSVRC2012_val_00000293.JPEG ...&#xA;        n01443537:&#xA;            ILSVRC2012_val_00000236.JPEG ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE: The arg &lt;code&gt;--data_path=/path/to/imagenet&lt;/code&gt; should be passed to the training script.&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) install and compile &lt;code&gt;flash-attn&lt;/code&gt; and &lt;code&gt;xformers&lt;/code&gt; for faster attention computation. Our code will automatically use them if installed. See &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/models/basic_var.py#L15-L30&#34;&gt;models/basic_var.py#L15-L30&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Training Scripts&lt;/h2&gt; &#xA;&lt;p&gt;To train VAR-{d16, d20, d24, d30, d36-s} on ImageNet 256x256 or 512x512, you can run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# d16, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1&#xA;# d20, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=20 --bs=768 --ep=250 --fp16=1 --alng=1e-3 --wpe=0.1&#xA;# d24, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=24 --bs=768 --ep=350 --tblr=8e-5 --fp16=1 --alng=1e-4 --wpe=0.01&#xA;# d30, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=30 --bs=1024 --ep=350 --tblr=8e-5 --fp16=1 --alng=1e-5 --wpe=0.01 --twde=0.08&#xA;# d36-s, 512x512 (-s means saln=1, shared AdaLN)&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=36 --saln=1 --pn=512 --bs=768 --ep=350 --tblr=8e-5 --fp16=1 --alng=5e-6 --wpe=0.01 --twde=0.08&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A folder named &lt;code&gt;local_output&lt;/code&gt; will be created to save the checkpoints and logs. You can monitor the training process by checking the logs in &lt;code&gt;local_output/log.txt&lt;/code&gt; and &lt;code&gt;local_output/stdout.txt&lt;/code&gt;, or using &lt;code&gt;tensorboard --logdir=local_output/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If your experiment is interrupted, just rerun the command, and the training will &lt;strong&gt;automatically resume&lt;/strong&gt; from the last checkpoint in &lt;code&gt;local_output/ckpt*.pth&lt;/code&gt; (see &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/utils/misc.py#L344-L357&#34;&gt;utils/misc.py#L344-L357&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Sampling &amp;amp; Zero-shot Inference&lt;/h2&gt; &#xA;&lt;p&gt;For FID evaluation, use &lt;code&gt;var.autoregressive_infer_cfg(..., cfg=1.5, top_p=0.96, top_k=900, more_smooth=False)&lt;/code&gt; to sample 50,000 images (50 per class) and save them as PNG (not JPEG) files in a folder. Pack them into a &lt;code&gt;.npz&lt;/code&gt; file via &lt;code&gt;create_npz_from_sample_folder(sample_folder)&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/utils/misc.py#L360&#34;&gt;utils/misc.py#L344&lt;/a&gt;. Then use the &lt;a href=&#34;https://github.com/openai/guided-diffusion/tree/main/evaluations&#34;&gt;OpenAI&#39;s FID evaluation toolkit&lt;/a&gt; and reference ground truth npz file of &lt;a href=&#34;https://openaipublic.blob.core.windows.net/diffusion/jul-2021/ref_batches/imagenet/256/VIRTUAL_imagenet256_labeled.npz&#34;&gt;256x256&lt;/a&gt; or &lt;a href=&#34;https://openaipublic.blob.core.windows.net/diffusion/jul-2021/ref_batches/imagenet/512/VIRTUAL_imagenet512.npz&#34;&gt;512x512&lt;/a&gt; to evaluate FID, IS, precision, and recall.&lt;/p&gt; &#xA;&lt;p&gt;Note a relatively small &lt;code&gt;cfg=1.5&lt;/code&gt; is used for trade-off between image quality and diversity. You can adjust it to &lt;code&gt;cfg=5.0&lt;/code&gt;, or sample with &lt;code&gt;autoregressive_infer_cfg(..., more_smooth=True)&lt;/code&gt; for &lt;strong&gt;better visual quality&lt;/strong&gt;. We&#39;ll provide the sampling script later.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If our work assists your research, feel free to give us a star â­ or cite us using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Article{VAR,&#xA;      title={Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction}, &#xA;      author={Keyu Tian and Yi Jiang and Zehuan Yuan and Bingyue Peng and Liwei Wang},&#xA;      year={2024},&#xA;      eprint={2404.02905},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>stanfordnlp/dspy</title>
    <updated>2024-12-08T01:44:48Z</updated>
    <id>tag:github.com,2024-12-08:/stanfordnlp/dspy</id>
    <link href="https://github.com/stanfordnlp/dspy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DSPy: The framework for programmingâ€”not promptingâ€”language models&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/static/img/dspy_logo.png&#34; width=&#34;460px&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;DSPy: &lt;em&gt;Programming&lt;/em&gt;â€”not promptingâ€”Foundation Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href=&#34;https://dspy.ai/&#34;&gt;DSPy Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pepy.tech/project/dspy-ai&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/dspy-ai&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/dspy-ai&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/dspy-ai/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;DSPy is the open-source framework for &lt;em&gt;programmingâ€”rather than promptingâ€”language models&lt;/em&gt;. It allows you to iterate fast on &lt;strong&gt;building modular AI systems&lt;/strong&gt; and provides algorithms for &lt;strong&gt;optimizing their prompts and weights&lt;/strong&gt;, whether you&#39;re building simple classifiers, sophisticated RAG pipelines, or Agent loops.&lt;/p&gt; &#xA;&lt;p&gt;DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional &lt;em&gt;Python code&lt;/em&gt; and use DSPy&#39;s tools to &lt;strong&gt;teach your LM to deliver high-quality outputs&lt;/strong&gt;. This &lt;a href=&#34;https://www.youtube.com/watch?v=JEMYuzrKLUw&#34;&gt;lecture&lt;/a&gt; is a good conceptual introduction. Meet the community, seek help, or start contributing via our GitHub repo here and our &lt;a href=&#34;https://discord.gg/XCGy2WDCQB&#34;&gt;Discord server&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation: &lt;a href=&#34;https://dspy.ai&#34;&gt;dspy.ai&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please go to the &lt;a href=&#34;https://dspy.ai&#34;&gt;DSPy Docs at dspy.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dspy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install the very latest from &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/stanfordnlp/dspy.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ“œ Citation &amp;amp; Reading More&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Jun&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2406.11695&#34;&gt;Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;[Oct&#39;23] &lt;a href=&#34;https://arxiv.org/abs/2310.03714&#34;&gt;DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; [Jul&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2407.10930&#34;&gt;Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together&lt;/a&gt;&lt;br&gt; [Jun&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2406.11706&#34;&gt;Prompts as Auto-Optimized Training Hyperparameters&lt;/a&gt;&lt;br&gt; [Feb&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models&lt;/a&gt;&lt;br&gt; [Jan&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2401.12178&#34;&gt;In-Context Learning for Extreme Multi-Label Classification&lt;/a&gt;&lt;br&gt; [Dec&#39;23] &lt;a href=&#34;https://arxiv.org/abs/2312.13382&#34;&gt;DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines&lt;/a&gt;&lt;br&gt; [Dec&#39;22] &lt;a href=&#34;https://arxiv.org/abs/2212.14024.pdf&#34;&gt;Demonstrate-Search-Predict: Composing Retrieval &amp;amp; Language Models for Knowledge-Intensive NLP&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To stay up to date or learn more, follow &lt;a href=&#34;https://twitter.com/lateinteraction&#34;&gt;@lateinteraction&lt;/a&gt; on Twitter.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;DSPy&lt;/strong&gt; logo is designed by &lt;strong&gt;Chuyi Zhang&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you use DSPy or DSP in a research paper, please cite our work as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{khattab2024dspy,&#xA;  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},&#xA;  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},&#xA;  journal={The Twelfth International Conference on Learning Representations},&#xA;  year={2024}&#xA;}&#xA;@article{khattab2022demonstrate,&#xA;  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},&#xA;  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},&#xA;  journal={arXiv preprint arXiv:2212.14024},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:&#xA;&#xA;* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) &#xA;* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) &#xA;* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)&#xA;* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)&#xA;* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)&#xA;* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) --&gt;</summary>
  </entry>
  <entry>
    <title>Guovin/iptv-api</title>
    <updated>2024-12-08T01:44:48Z</updated>
    <id>tag:github.com,2024-12-08:/Guovin/iptv-api</id>
    <link href="https://github.com/Guovin/iptv-api" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ“ºIPTVç”µè§†ç›´æ’­æºæ›´æ–°å·¥å…·ğŸš€ï¼šâœ¨å¤®è§†ã€ğŸ“¡å«è§†ã€â˜˜ï¸å¹¿ä¸œåŠå„çœä»½åœ°æ–¹å°ã€ğŸŒŠæ¸¯Â·æ¾³Â·å°ã€ğŸ¬ç”µå½±ã€ğŸ¥å’ªå’•ã€ğŸ€ä½“è‚²ã€ğŸªåŠ¨ç”»ã€ğŸ®æ¸¸æˆã€ğŸµéŸ³ä¹ã€ğŸ›ç»å…¸å‰§åœºï¼›æ”¯æŒIPv4/IPv6ï¼›æ”¯æŒè‡ªå®šä¹‰å¢åŠ é¢‘é“ï¼›æ”¯æŒç»„æ’­æºã€é…’åº—æºã€è®¢é˜…æºã€å…³é”®å­—æœç´¢ï¼›æ¯å¤©è‡ªåŠ¨æ›´æ–°ä¸¤æ¬¡ï¼Œç»“æœå¯ç”¨äºTVBoxç­‰æ’­æ”¾è½¯ä»¶ï¼›æ”¯æŒå·¥ä½œæµã€Docker(amd64/arm64/arm v7)ã€å‘½ä»¤è¡Œã€GUIè¿è¡Œæ–¹å¼ | IPTV live TV source update tool&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/static/images/logo.png&#34; alt=&#34;logo&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;IPTV-API&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; è‡ªå®šä¹‰é¢‘é“ï¼Œè‡ªåŠ¨è·å–ç›´æ’­æºæ¥å£ï¼Œæµ‹é€ŸéªŒæ•ˆåç”Ÿæˆå¯ç”¨çš„ç»“æœ&#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; é»˜è®¤ç»“æœåŒ…å«ï¼šğŸ“ºå¤®è§†é¢‘é“ã€ğŸ’°å¤®è§†ä»˜è´¹é¢‘é“ã€ğŸ“¡å«è§†é¢‘é“ã€ğŸ å¹¿ä¸œé¢‘é“ã€ğŸŒŠæ¸¯Â·æ¾³Â·å°é¢‘é“ã€ğŸ¬ç”µå½±é¢‘é“ã€ğŸ¥å’ªå’•ç›´æ’­ã€ğŸ€ä½“è‚²é¢‘é“ã€ğŸªåŠ¨ç”»é¢‘é“ã€ğŸ®æ¸¸æˆé¢‘é“ã€ğŸµéŸ³ä¹é¢‘é“ã€ğŸ›ç»å…¸å‰§åœº&#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;å…·ä½“é¢‘é“&lt;/summary&gt; &#xA; &lt;div&gt;&#xA;   ğŸ“ºå¤®è§†é¢‘é“: CCTV-1, CCTV-2, CCTV-3, CCTV-4, CCTV-5, CCTV-5+, CCTV-6, CCTV-7, CCTV-8, CCTV-9, CCTV-10, CCTV-11, CCTV-12, CCTV-13, CCTV-14, CCTV-15, CCTV-16, CCTV-17, CETV1, CETV2, CETV4, CETV5 &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸ’°å¤®è§†ä»˜è´¹é¢‘é“: æ–‡åŒ–ç²¾å“, å¤®è§†å°çƒ, é£äº‘éŸ³ä¹, ç¬¬ä¸€å‰§åœº, é£äº‘å‰§åœº, æ€€æ—§å‰§åœº, å¥³æ€§æ—¶å°š, é«˜å°”å¤«ç½‘çƒ, é£äº‘è¶³çƒ, ç”µè§†æŒ‡å—, ä¸–ç•Œåœ°ç†, å…µå™¨ç§‘æŠ€ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸ“¡å«è§†é¢‘é“: å¹¿ä¸œå«è§†, é¦™æ¸¯å«è§†, æµ™æ±Ÿå«è§†, æ¹–å—å«è§†, åŒ—äº¬å«è§†, æ¹–åŒ—å«è§†, é»‘é¾™æ±Ÿå«è§†, å®‰å¾½å«è§†, é‡åº†å«è§†, ä¸œæ–¹å«è§†, ä¸œå—å«è§†, ç”˜è‚ƒå«è§†, å¹¿è¥¿å«è§†, è´µå·å«è§†, æµ·å—å«è§†, æ²³åŒ—å«è§†, æ²³å—å«è§†, å‰æ—å«è§†, æ±Ÿè‹å«è§†, æ±Ÿè¥¿å«è§†, è¾½å®å«è§†, å†…è’™å¤å«è§†, å®å¤å«è§†, é’æµ·å«è§†, å±±ä¸œå«è§†, å±±è¥¿å«è§†, é™•è¥¿å«è§†, å››å·å«è§†, æ·±åœ³å«è§†, ä¸‰æ²™å«è§†, å¤©æ´¥å«è§†, è¥¿è—å«è§†, æ–°ç–†å«è§†, äº‘å—å«è§† &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   â˜˜ï¸å¹¿ä¸œé¢‘é“: å¹¿ä¸œç æ±Ÿ, å¹¿ä¸œä½“è‚², å¹¿ä¸œæ–°é—», å¹¿ä¸œæ°‘ç”Ÿ, å¹¿ä¸œå«è§†, å¤§æ¹¾åŒºå«è§†, å¹¿å·ç»¼åˆ, å¹¿å·å½±è§†, å¹¿å·ç«èµ›, æ±Ÿé—¨ç»¼åˆ, æ±Ÿé—¨ä¾¨ä¹¡ç”Ÿæ´», ä½›å±±ç»¼åˆ, æ·±åœ³å«è§†, æ±•å¤´ç»¼åˆ, æ±•å¤´ç»æµ, æ±•å¤´æ–‡æ—…, èŒ‚åç»¼åˆ, èŒ‚åå…¬å…± &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   â˜˜ï¸å„çœä»½åœ°æ–¹å° &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸŒŠæ¸¯Â·æ¾³Â·å°: ç¿¡ç¿ å°, æ˜ç å°, å‡¤å‡°ä¸­æ–‡, å‡¤å‡°èµ„è®¯, å‡¤å‡°é¦™æ¸¯, å‡¤å‡°å«è§†, TVBSäºšæ´², é¦™æ¸¯å«è§†, çº¬æ¥ä½“è‚², çº¬æ¥è‚²ä¹, J2, Viutv, ä¸‰ç«‹å°æ¹¾, æ— çº¿æ–°é—», ä¸‰ç«‹æ–°é—», ä¸œæ£®ç»¼åˆ, ä¸œæ£®è¶…è§†, ä¸œæ£®ç”µå½±, Nowå‰§é›†, Nowåå‰§, é–å¤©èµ„è®¯, æ˜Ÿå«å¨±ä¹, å«è§†å¡å¼ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸ¬ç”µå½±é¢‘é“: CHCå®¶åº­å½±é™¢, CHCåŠ¨ä½œç”µå½±, CHCé«˜æ¸…ç”µå½±, æ·˜å‰§åœº, æ·˜å¨±ä¹, æ·˜ç”µå½±, NewTVæƒŠæ‚šæ‚¬ç–‘, NewTVåŠ¨ä½œç”µå½±, é»‘è“ç”µå½±, çº¬æ¥ç”µå½±, é–å¤©æ˜ ç”», é–å¤©æˆå‰§, æ˜Ÿå«å¨±ä¹, è‰¾å°”è¾¾å¨±ä¹, ç»å…¸ç”µå½±, IPTVç»å…¸ç”µå½±, å¤©æ˜ ç»å…¸, æ— çº¿æ˜Ÿæ²³, æ˜Ÿç©ºå«è§†, ç§äººå½±é™¢, ä¸œæ£®ç”µå½±, é¾™ç¥¥ç”µå½±, ä¸œæ£®æ´‹ç‰‡, ä¸œæ£®è¶…è§† &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸ¥å’ªå’•ç›´æ’­: å’ªå’•ç›´æ’­1-45 &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸ€ä½“è‚²é¢‘é“: CCTV-5, CCTV-5+, å¹¿ä¸œä½“è‚², çº¬æ¥ä½“è‚², äº”æ˜Ÿä½“è‚², ä½“è‚²èµ›äº‹, åŠ²çˆ†ä½“è‚², çˆ±ä½“è‚², è¶…çº§ä½“è‚², ç²¾å“ä½“è‚², å¹¿å·ç«èµ›, æ·±åœ³ä½“è‚², ç¦å»ºä½“è‚², è¾½å®ä½“è‚², å±±ä¸œä½“è‚², æˆéƒ½ä½“è‚², å¤©æ´¥ä½“è‚², æ±Ÿè‹ä½“è‚², å®‰å¾½ç»¼è‰ºä½“è‚², å‰æ—ç¯®çƒ, ç›å½©ç¯®çƒ, ç›å½©ç¾½æ¯›çƒ, ç›å½©å¹¿åœºèˆ, é£äº‘è¶³çƒ, è¶³çƒé¢‘é“, é­…åŠ›è¶³çƒ, å¤©å…ƒå›´æ£‹, å¿«ä¹å‚é’“, JJæ–—åœ°ä¸» &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸªåŠ¨ç”»é¢‘é“: å°‘å„¿åŠ¨ç”», å¡é…·åŠ¨ç”», åŠ¨æ¼«ç§€åœº, æ–°åŠ¨æ¼«, é’æ˜¥åŠ¨æ¼«, çˆ±åŠ¨æ¼«, ä¸­å½•åŠ¨æ¼«, å®å®åŠ¨ç”», CNå¡é€š, ä¼˜æ¼«å¡é€š, é‡‘é¹°å¡é€š, ç›å½©å°‘å„¿, é»‘è“åŠ¨ç”», ç‚«åŠ¨å¡é€š, 24Hå›½æ¼«çƒ­æ’­, æµ™æ±Ÿå°‘å„¿, æ²³åŒ—å°‘å„¿ç§‘æ•™, ä¸ƒé¾™ç , ç«å½±å¿è€…, æµ·ç»µå®å®, ä¸­åå°å½“å®¶, æ–—ç ´è‹ç©¹ç„å¹»å‰§, çŒ«å’Œè€é¼ , ç»å…¸åŠ¨æ¼«, èœ¡ç¬”å°æ–°, æ¼«ç”»è§£è¯´ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸ®æ¸¸æˆé¢‘é“: æ¸¸æˆé£äº‘, æ¸¸æˆç«æŠ€, ç”µç«æ¸¸æˆ, æµ·çœ‹ç”µç«, ç”µç«å¤©å ‚, çˆ±ç”µç« &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸµéŸ³ä¹é¢‘é“: CCTV-15, é£äº‘éŸ³ä¹, éŸ³ä¹ç°åœº, éŸ³ä¹ä¹‹å£°, æ½®æµéŸ³ä¹, å¤©æ´¥éŸ³ä¹, éŸ³ä¹å¹¿æ’­, éŸ³ä¹è°ƒé¢‘å¹¿æ’­ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ğŸ›ç»å…¸å‰§åœº: ç¬‘å‚²æ±Ÿæ¹–, å¤©é¾™å…«éƒ¨, é¹¿é¼è®°, ä»™å‰‘å¥‡ä¾ ä¼ , è¥¿æ¸¸è®°, ä¸‰å›½æ¼”ä¹‰, æ°´æµ’ä¼ , æ–°ç™½å¨˜å­ä¼ å¥‡, å¤©é¾™å…«éƒ¨, æµå…¬æ¸¸è®°, å°ç¥æ¦œ, é—¯å…³ä¸œ, ä¸Šæµ·æ»©, å°„é›•è‹±é›„ä¼  &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Guovin/iptv-api/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/guovin/iptv-api&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.python.org/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-%20%3D%203.13-47c219&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Guovin/iptv-api/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/downloads/guovin/iptv-api/total&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/repository/docker/guovern/iptv-api&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/pulls/guovern/iptv-api?label=docker:iptv-api&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/repository/docker/guovern/tv-requests&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/pulls/guovern/tv-requests?label=docker:requests&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/repository/docker/guovern/tv-driver&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/pulls/guovern/tv-driver?label=docker:driver&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Guovin/iptv-api/fork&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/guovin/iptv-api&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/README_en.md&#34;&gt;English&lt;/a&gt; | ä¸­æ–‡&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E7%89%B9%E7%82%B9&#34;&gt;âœ… ç‰¹ç‚¹&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E6%9C%80%E6%96%B0%E7%BB%93%E6%9E%9C&#34;&gt;ğŸ”— æœ€æ–°ç»“æœ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/config.md&#34;&gt;âš™ï¸ é…ç½®å‚æ•°&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B&#34;&gt;ğŸš€ å¿«é€Ÿä¸Šæ‰‹&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/tutorial.md&#34;&gt;ğŸ“– è¯¦ç»†æ•™ç¨‹&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/CHANGELOG.md&#34;&gt;ğŸ—“ï¸ æ›´æ–°æ—¥å¿—&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E8%B5%9E%E8%B5%8F&#34;&gt;â¤ï¸ èµèµ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E5%85%B3%E6%B3%A8&#34;&gt;ğŸ‘€ å…³æ³¨&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E5%85%8D%E8%B4%A3%E5%A3%B0%E6%98%8E&#34;&gt;ğŸ“£ å…è´£å£°æ˜&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E8%AE%B8%E5%8F%AF%E8%AF%81&#34;&gt;âš–ï¸ è®¸å¯è¯&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç‰¹ç‚¹&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;âœ… è‡ªå®šä¹‰æ¨¡æ¿ï¼Œç”Ÿæˆæ‚¨æƒ³è¦çš„é¢‘é“&lt;/li&gt; &#xA; &lt;li&gt;âœ… æ”¯æŒå¤šç§è·å–æºæ–¹å¼ï¼šç»„æ’­æºã€é…’åº—æºã€è®¢é˜…æºã€å…³é”®å­—æœç´¢&lt;/li&gt; &#xA; &lt;li&gt;âœ… æ¥å£æµ‹é€ŸéªŒæ•ˆï¼Œå“åº”æ—¶é—´ã€åˆ†è¾¨ç‡ä¼˜å…ˆçº§ï¼Œè¿‡æ»¤æ— æ•ˆæ¥å£&lt;/li&gt; &#xA; &lt;li&gt;âœ… åå¥½è®¾ç½®ï¼šIPv6ã€æ¥å£æ¥æºæ’åºä¼˜å…ˆçº§ä¸æ•°é‡é…ç½®ã€æ¥å£ç™½åå•&lt;/li&gt; &#xA; &lt;li&gt;âœ… å®šæ—¶æ‰§è¡Œï¼ŒåŒ—äº¬æ—¶é—´æ¯æ—¥ 6:00 ä¸ 18:00 æ‰§è¡Œæ›´æ–°&lt;/li&gt; &#xA; &lt;li&gt;âœ… æ”¯æŒå¤šç§è¿è¡Œæ–¹å¼ï¼šå·¥ä½œæµã€å‘½ä»¤è¡Œã€GUI è½¯ä»¶ã€Docker(amd64/arm64/arm v7)&lt;/li&gt; &#xA; &lt;li&gt;âœ¨ æ›´å¤šåŠŸèƒ½è¯·è§&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/config.md&#34;&gt;é…ç½®å‚æ•°&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æœ€æ–°ç»“æœ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ¥å£æºï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://ghp.ci/raw.githubusercontent.com/Guovin/iptv-api/gd/output/result.m3u&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://ghp.ci/raw.githubusercontent.com/Guovin/iptv-api/gd/output/result.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ•°æ®æºï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://ghp.ci/raw.githubusercontent.com/Guovin/iptv-api/gd/source.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;é…ç½®&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/config.md&#34;&gt;é…ç½®å‚æ•°&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;å¿«é€Ÿä¸Šæ‰‹&lt;/h2&gt; &#xA;&lt;h3&gt;æ–¹å¼ä¸€ï¼šå·¥ä½œæµ&lt;/h3&gt; &#xA;&lt;p&gt;Fork æœ¬é¡¹ç›®å¹¶å¼€å¯å·¥ä½œæµæ›´æ–°ï¼Œå…·ä½“æ­¥éª¤è¯·è§&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/tutorial.md&#34;&gt;è¯¦ç»†æ•™ç¨‹&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;æ–¹å¼äºŒï¼šå‘½ä»¤è¡Œ&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install pipenv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipenv install --dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¯åŠ¨æ›´æ–°ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipenv run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¯åŠ¨æœåŠ¡ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipenv run service&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;æ–¹å¼ä¸‰ï¼šGUI è½¯ä»¶&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;ä¸‹è½½&lt;a href=&#34;https://github.com/Guovin/iptv-api/releases&#34;&gt;IPTV-API æ›´æ–°è½¯ä»¶&lt;/a&gt;ï¼Œæ‰“å¼€è½¯ä»¶ï¼Œç‚¹å‡»æ›´æ–°ï¼Œå³å¯å®Œæˆæ›´æ–°&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æˆ–è€…åœ¨é¡¹ç›®ç›®å½•ä¸‹è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå³å¯æ‰“å¼€ GUI è½¯ä»¶ï¼š&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipenv run ui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/images/ui.png&#34; alt=&#34;IPTV-APIæ›´æ–°è½¯ä»¶&#34; title=&#34;IPTV-APIæ›´æ–°è½¯ä»¶&#34; style=&#34;height:600px&#34;&gt; &#xA;&lt;h3&gt;æ–¹å¼å››ï¼šDocker&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-apiï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰ï¼šæ€§èƒ½è¦æ±‚è¾ƒé«˜ï¼Œæ›´æ–°é€Ÿåº¦è¾ƒæ…¢ï¼Œç¨³å®šæ€§ã€æˆåŠŸç‡é«˜ï¼›ä¿®æ”¹é…ç½® open_driver = False å¯åˆ‡æ¢åˆ° Lite ç‰ˆæœ¬è¿è¡Œæ¨¡å¼ï¼ˆæ¨èé…’åº—æºã€ç»„æ’­æºã€å…³é”®å­—æœç´¢ä½¿ç”¨æ­¤ç‰ˆæœ¬ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;iptv-api:liteï¼ˆç²¾ç®€ç‰ˆæœ¬ï¼‰ï¼šè½»é‡çº§ï¼Œæ€§èƒ½è¦æ±‚ä½ï¼Œæ›´æ–°é€Ÿåº¦å¿«ï¼Œç¨³å®šæ€§ä¸ç¡®å®šï¼ˆæ¨èè®¢é˜…æºä½¿ç”¨æ­¤ç‰ˆæœ¬ï¼‰&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æ‹‰å–é•œåƒï¼š&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-apiï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull guovern/iptv-api:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-api:liteï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull guovern/iptv-api:lite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;è¿è¡Œå®¹å™¨ï¼š&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-apiï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 8000:8000 guovern/iptv-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-api:liteï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 8000:8000 guovern/iptv-api:lite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å·æŒ‚è½½å‚æ•°ï¼ˆå¯é€‰ï¼‰ï¼š å®ç°å®¿ä¸»æœºæ–‡ä»¶ä¸å®¹å™¨æ–‡ä»¶åŒæ­¥ï¼Œä¿®æ”¹æ¨¡æ¿ã€é…ç½®ã€è·å–æ›´æ–°ç»“æœæ–‡ä»¶å¯ç›´æ¥åœ¨å®¿ä¸»æœºæ–‡ä»¶å¤¹ä¸‹æ“ä½œ&lt;/p&gt; &#xA;&lt;p&gt;ä»¥å®¿ä¸»æœºè·¯å¾„/etc/docker ä¸ºä¾‹ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-apiï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v /etc/docker/config:/iptv-api/config -v /etc/docker/output:/iptv-api/output -d -p 8000:8000 guovern/iptv-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-api:liteï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v /etc/docker/config:/iptv-api-lite/config -v /etc/docker/output:/iptv-api-lite/output -d -p 8000:8000 guovern/iptv-api:lite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;æ›´æ–°ç»“æœï¼š&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ¥å£åœ°å€ï¼šip:8000&lt;/li&gt; &#xA; &lt;li&gt;M3u æ¥å£ï¼šip:8000/m3u&lt;/li&gt; &#xA; &lt;li&gt;Txt æ¥å£ï¼šip:8000/txt&lt;/li&gt; &#xA; &lt;li&gt;æ¥å£å†…å®¹ï¼šip:8000/content&lt;/li&gt; &#xA; &lt;li&gt;æµ‹é€Ÿæ—¥å¿—ï¼šip:8000/log&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ›´æ–°æ—¥å¿—&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/CHANGELOG.md&#34;&gt;æ›´æ–°æ—¥å¿—&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;èµèµ&lt;/h2&gt; &#xA;&lt;div&gt;&#xA; å¼€å‘ç»´æŠ¤ä¸æ˜“ï¼Œè¯·æˆ‘å–æ¯å’–å•¡â˜•ï¸å§~&#xA;&lt;/div&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ”¯ä»˜å®&lt;/th&gt; &#xA;   &lt;th&gt;å¾®ä¿¡&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/static/images/alipay.jpg&#34; alt=&#34;æ”¯ä»˜å®æ‰«ç &#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/static/images/appreciate.jpg&#34; alt=&#34;å¾®ä¿¡æ‰«ç &#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;å…³æ³¨&lt;/h2&gt; &#xA;&lt;p&gt;å¾®ä¿¡å…¬ä¼—å·æœç´¢ Govinï¼Œæˆ–æ‰«ç ï¼Œæ¥æ”¶æ›´æ–°æ¨é€ã€å­¦ä¹ æ›´å¤šä½¿ç”¨æŠ€å·§ï¼š&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/static/images/qrcode.jpg&#34; alt=&#34;å¾®ä¿¡å…¬ä¼—å·&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;å…è´£å£°æ˜&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµç”¨é€”ï¼Œæ¥å£æ•°æ®å‡æ¥æºäºç½‘ç»œï¼Œå¦‚æœ‰ä¾µæƒï¼Œè¯·è”ç³»åˆ é™¤&lt;/p&gt; &#xA;&lt;h2&gt;è®¸å¯è¯&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/LICENSE&#34;&gt;MIT&lt;/a&gt; License Â© 2024-PRESENT &lt;a href=&#34;https://github.com/guovin&#34;&gt;Govin&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>