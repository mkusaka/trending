<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-28T01:58:02Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google-deepmind/alphageometry</title>
    <updated>2024-01-28T01:58:02Z</updated>
    <id>tag:github.com,2024-01-28:/google-deepmind/alphageometry</id>
    <link href="https://github.com/google-deepmind/alphageometry" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Solving Olympiad Geometry without Human Demonstrations&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the code necessary to reproduce DDAR and AlphaGeometry, the two geometry theorem provers introduced in the &lt;a href=&#34;https://www.nature.com/articles/s41586-023-06747-5&#34;&gt;Nature 2024&lt;/a&gt; paper:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA; &lt;em&gt;&#34;Solving Olympiad Geometry without Human Demonstrations&#34;.&lt;/em&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;center&gt; &#xA; &lt;img alt=&#34;fig1&#34; width=&#34;800px&#34; src=&#34;https://raw.githubusercontent.com/google-deepmind/alphageometry/main/fig1.svg?sanitize=true&#34;&gt; &#xA;&lt;/center&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;For the instructions presented below, we use Python 3.10.9, and dependencies with their exact version numbers listed in &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Our code depends on &lt;code&gt;meliad&lt;/code&gt;, which is not a registered package with &lt;code&gt;pip&lt;/code&gt;. See instructions below for how to manually install &lt;code&gt;meliad&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that one can still run the DDAR solver without the &lt;code&gt;meliad&lt;/code&gt; and &lt;code&gt;sentencepiece&lt;/code&gt; dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;Run the instructions&lt;/h2&gt; &#xA;&lt;p&gt;All instructions in this &lt;code&gt;README.md&lt;/code&gt; can be run in one go by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Below, we explain these instructions step-by-step.&lt;/p&gt; &#xA;&lt;h2&gt;Install dependencies, download weights and vocabulary.&lt;/h2&gt; &#xA;&lt;p&gt;Installation is done in a virtual environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;virtualenv -p python3 .&#xA;source ./bin/activate&#xA;pip install --require-hashes -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download weights and vocabulary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download.sh&#xA;DATA=ag_ckpt_vocab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, install &lt;code&gt;meliad&lt;/code&gt; separately as it is not registered with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MELIAD_PATH=meliad_lib/meliad&#xA;mkdir -p $MELIAD_PATH&#xA;git clone https://github.com/google-research/meliad $MELIAD_PATH&#xA;export PYTHONPATH=$PYTHONPATH:$MELIAD_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Set up common flags&lt;/h2&gt; &#xA;&lt;p&gt;Before running the python scripts, let us first prepare some commonly used flags. The symbolic engine needs definitions and deduction rules to operate. These definitions and rules are provided in two text files &lt;code&gt;defs.txt&lt;/code&gt; and &lt;code&gt;rules.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;DDAR_ARGS=(&#xA;  --defs_file=$(pwd)/defs.txt \&#xA;  --rules_file=$(pwd)/rules.txt \&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, we define the flags relevant to the proof search. To reproduce the simple examples below, we use lightweight values for the proof search parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;BATCH_SIZE=2&#xA;BEAM_SIZE=2&#xA;DEPTH=2&#xA;&#xA;SEARCH_ARGS=(&#xA;  --beam_size=$BEAM_SIZE&#xA;  --search_depth=$DEPTH&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: The results in our paper can be obtained by setting &lt;code&gt;BATCH_SIZE=32&lt;/code&gt;, &lt;code&gt;BEAM_SIZE=512&lt;/code&gt;, &lt;code&gt;DEPTH=16&lt;/code&gt; as described in section Methods. To stay under IMO time limits, 4 V100-GPUs and 250 CPU workers are needed as shown in Extended Data - Figure 1. Note that we also strip away other memory/speed optimizations due to internal dependencies and to promote code clarity.&lt;/p&gt; &#xA;&lt;p&gt;Assume the downloaded checkpoint and vocabulary is placed in &lt;code&gt;DATA&lt;/code&gt;, and the installed &lt;code&gt;meliad&lt;/code&gt; source code is at &lt;code&gt;MELIAD_PATH&lt;/code&gt;. We make use of the &lt;code&gt;gin&lt;/code&gt; library to manage model configurations, following &lt;code&gt;meliad&lt;/code&gt; conventions. We now define the flags relevant to the language model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;LM_ARGS=(&#xA;  --ckpt_path=$DATA \&#xA;  --vocab_path=$DATA/geometry.757.model&#xA;  --gin_search_paths=$MELIAD_PATH/transformer/configs,$(pwd) \&#xA;  --gin_file=base_htrans.gin \&#xA;  --gin_file=size/medium_150M.gin \&#xA;  --gin_file=options/positions_t5.gin \&#xA;  --gin_file=options/lr_cosine_decay.gin \&#xA;  --gin_file=options/seq_1024_nocache.gin \&#xA;  --gin_file=geometry_150M_generate.gin \&#xA;  --gin_param=DecoderOnlyLanguageModelGenerate.output_token_losses=True \&#xA;  --gin_param=TransformerTaskConfig.batch_size=$BATCH_SIZE \&#xA;  --gin_param=TransformerTaskConfig.sequence_length=128 \&#xA;  --gin_param=Trainer.restore_state_variables=False&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;TIP: Note that you can still run the DDAR solver without defining &lt;code&gt;SEARCH_ARGS&lt;/code&gt; and &lt;code&gt;LM_ARGS&lt;/code&gt;. In such case, simply disable the import of the &lt;code&gt;lm_inference&lt;/code&gt; module inside &lt;code&gt;alphageometry.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run DDAR&lt;/h2&gt; &#xA;&lt;p&gt;The script loads a problem by reading a list of problems from a text file and solves the specific problem in the list according to its name. We pass these two pieces of information through the flags &lt;code&gt;--problems_file&lt;/code&gt; and &lt;code&gt;--problem_name&lt;/code&gt;. We use &lt;code&gt;--mode=ddar&lt;/code&gt; to indicate that we want to use the DDAR solver.&lt;/p&gt; &#xA;&lt;p&gt;Below we showed this solver solving IMO 2000 P1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m alphageometry \&#xA;--alsologtostderr \&#xA;--problems_file=$(pwd)/imo_ag_30.txt \&#xA;--problem_name=translated_imo_2000_p1 \&#xA;--mode=ddar \&#xA;&#34;${DDAR_ARGS[@]}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Expect the following output&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;graph.py:468] translated_imo_2000_p1&#xA;graph.py:469] a b = segment a b; g1 = on_tline g1 a a b; g2 = on_tline g2 b b a; m = on_circle m g1 a, on_circle m g2 b; n = on_circle n g1 a, on_circle n g2 b; c = on_pline c m a b, on_circle c g1 a; d = on_pline d m a b, on_circle d g2 b; e = on_line e a c, on_line e b d; p = on_line p a n, on_line p c d; q = on_line q b n, on_line q c d ? cong e p e q&#xA;ddar.py:41] Depth 1/1000 time = 1.7772269248962402&#xA;ddar.py:41] Depth 2/1000 time = 5.63526177406311&#xA;ddar.py:41] Depth 3/1000 time = 6.883412837982178&#xA;ddar.py:41] Depth 4/1000 time = 10.275688409805298&#xA;ddar.py:41] Depth 5/1000 time = 12.048273086547852&#xA;alphageometry.py:190]&#xA;==========================&#xA; * From theorem premises:&#xA;A B G1 G2 M N C D E P Q : Points&#xA;AG_1 ⟂ AB [00]&#xA;BA ⟂ G_2B [01]&#xA;G_2M = G_2B [02]&#xA;G_1M = G_1A [03]&#xA;&#xA;...&#xA;[log omitted]&#xA;...&#xA;&#xA;036. ∠QEB = ∠(QP-EA) [46] &amp;amp; ∠(BE-QP) = ∠AEP [55] ⇒  ∠EQP = ∠QPE [56]&#xA;037. ∠PQE = ∠EPQ [56] ⇒  EP = EQ&#xA;&#xA;==========================&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output first includes a list of relevant premises that it uses, and then proof steps that gradually build up the proof. All predicates are numbered to track how they are derived from the premises, and to show that the proof is fully justified.&lt;/p&gt; &#xA;&lt;p&gt;TIP: Additionally passing the flag &lt;code&gt;--out_file=path/to/output/text/file.txt&lt;/code&gt; will write the proof to a text file.&lt;/p&gt; &#xA;&lt;p&gt;Running on all problems in &lt;code&gt;imo_ag_30.txt&lt;/code&gt; will yield solutions to 14 of them, as reported in Table 1 in our paper.&lt;/p&gt; &#xA;&lt;h2&gt;Run AlphaGeometry:&lt;/h2&gt; &#xA;&lt;p&gt;As a simple example, we load &lt;code&gt;--problem_name=orthocenter&lt;/code&gt; from &lt;code&gt;--problem_file=examples.txt&lt;/code&gt;. This time, we pass &lt;code&gt;--mode=alphageometry&lt;/code&gt; to use the AlphaGeometry solver and pass the &lt;code&gt;SEARCH_ARGS&lt;/code&gt; and &lt;code&gt;LM_ARGS&lt;/code&gt; flags.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m alphageometry \&#xA;--alsologtostderr \&#xA;--problems_file=$(pwd)/examples.txt \&#xA;--problem_name=orthocenter \&#xA;--mode=alphageometry \&#xA;&#34;${DDAR_ARGS[@]}&#34; \&#xA;&#34;${SEARCH_ARGS[@]}&#34; \&#xA;&#34;${LM_ARGS[@]}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Expect the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...&#xA;[log omitted]&#xA;...&#xA;training_loop.py:725] Total parameters: 152072288&#xA;training_loop.py:739] Total state size: 0&#xA;training_loop.py:492] Training loop: creating task for mode beam_search&#xA;&#xA;graph.py:468] orthocenter&#xA;graph.py:469] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b ? perp a d b c&#xA;ddar.py:41] Depth 1/1000 time = 0.009987592697143555 branch = 4&#xA;ddar.py:41] Depth 2/1000 time = 0.00672602653503418 branch = 0&#xA;alphageometry.py:221] DD+AR failed to solve the problem.&#xA;alphageometry.py:457] Depth 0. There are 1 nodes to expand:&#xA;alphageometry.py:460] {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00&#xA;alphageometry.py:465] Decoding from {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00&#xA;...&#xA;[log omitted]&#xA;...&#xA;alphageometry.py:470] LM output (score=-1.102287): &#34;e : C a c e 02 C b d e 03 ;&#34;&#xA;alphageometry.py:471] Translation: &#34;e = on_line e a c, on_line e b d&#34;&#xA;&#xA;alphageometry.py:480] Solving: &#34;a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c&#34;&#xA;graph.py:468]&#xA;graph.py:469] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c&#xA;ddar.py:41] Depth 1/1000 time = 0.021120786666870117&#xA;ddar.py:41] Depth 2/1000 time = 0.033370018005371094&#xA;ddar.py:41] Depth 3/1000 time = 0.04297471046447754&#xA;alphageometry.py:140]&#xA;==========================&#xA; * From theorem premises:&#xA;A B C D : Points&#xA;BD ⟂ AC [00]&#xA;CD ⟂ AB [01]&#xA;&#xA; * Auxiliary Constructions:&#xA;E : Points&#xA;E,B,D are collinear [02]&#xA;E,C,A are collinear [03]&#xA;&#xA; * Proof steps:&#xA;001. E,B,D are collinear [02] &amp;amp; E,C,A are collinear [03] &amp;amp; BD ⟂ AC [00] ⇒  ∠BEA = ∠CED [04]&#xA;002. E,B,D are collinear [02] &amp;amp; E,C,A are collinear [03] &amp;amp; BD ⟂ AC [00] ⇒  ∠BEC = ∠AED [05]&#xA;003. A,E,C are collinear [03] &amp;amp; E,B,D are collinear [02] &amp;amp; AC ⟂ BD [00] ⇒  EC ⟂ EB [06]&#xA;004. EC ⟂ EB [06] &amp;amp; CD ⟂ AB [01] ⇒  ∠(EC-BA) = ∠(EB-CD) [07]&#xA;005. E,C,A are collinear [03] &amp;amp; E,B,D are collinear [02] &amp;amp; ∠(EC-BA) = ∠(EB-CD) [07] ⇒  ∠BAE = ∠CDE [08]&#xA;006. ∠BEA = ∠CED [04] &amp;amp; ∠BAE = ∠CDE [08] (Similar Triangles)⇒  EB:EC = EA:ED [09]&#xA;007. EB:EC = EA:ED [09] &amp;amp; ∠BEC = ∠AED [05] (Similar Triangles)⇒  ∠BCE = ∠ADE [10]&#xA;008. EB:EC = EA:ED [09] &amp;amp; ∠BEC = ∠AED [05] (Similar Triangles)⇒  ∠EBC = ∠EAD [11]&#xA;009. ∠BCE = ∠ADE [10] &amp;amp; E,C,A are collinear [03] &amp;amp; E,B,D are collinear [02] &amp;amp; ∠EBC = ∠EAD [11] ⇒  AD ⟂ BC&#xA;==========================&#xA;&#xA;alphageometry.py:505] Solved.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Point &lt;code&gt;H&lt;/code&gt; is automatically renamed to &lt;code&gt;D&lt;/code&gt;, as the LM is trained on synthetic problems where the points are named alphabetically, and so it expects the same during test time.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: In this implementation of AlphaGeometry, we removed all optimizations that are dependent on internal infrastructure, e.g., parallelized model inference on multi GPUs, parallelized DDAR on multiple CPUs, parallel execution of LM and DDAR, shared pool of CPU workers across different problems, etc. We also removed some memory/speed optimizations and code abstractions in favor of code clarity.&lt;/p&gt; &#xA;&lt;p&gt;As can be seen in the output, initially DDAR failed to solve the problem. The LM proposes two auxiliary constructions (because &lt;code&gt;BATCH_SIZE=2&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;e = eqdistance e c a b, eqdistance e b a c&lt;/code&gt;, i.e., construct &lt;code&gt;E&lt;/code&gt; as the intersection of circle (center=C, radius=AB) and circle (center=B, radius=AC). This construction has a score of &lt;code&gt;-1.186&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;e = on_line e a c, on_line e b d&lt;/code&gt;, i.e., &lt;code&gt;E&lt;/code&gt; is the intersection of &lt;code&gt;AC&lt;/code&gt; and &lt;code&gt;BD&lt;/code&gt;. This construction has a higher score (&lt;code&gt;-1.102287&lt;/code&gt;) than the previous.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Since the second construction has a higher score, DDAR attempted the second construction first and found the solution right away. The proof search therefore terminates and there is no second iteration.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Before attempting to reproduce the AlphaGeometry numbers in our paper, please make sure to pass all tests in the prepared test suite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash run_tests.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: &lt;a href=&#34;https://github.com/google-deepmind/alphageometry/issues/14&#34;&gt;Issues#14&lt;/a&gt; reports that although the top beam decodes are still the same, the LM is not giving the same score for different users.&lt;/p&gt; &#xA;&lt;p&gt;Then, pass the corresponding values for &lt;code&gt;--problem_file&lt;/code&gt; (column) and &lt;code&gt;--mode&lt;/code&gt; (row), and iterate on all problems to obtain the following results:&lt;/p&gt; &#xA;&lt;center&gt; &#xA; &lt;p&gt;&lt;b&gt;Number of solved problems:&lt;/b&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;code&gt;imo_ag_30.txt&lt;/code&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;code&gt;jgex_ag_231.txt&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;ddar&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;14&lt;/td&gt; &#xA;    &lt;td&gt;198&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;alphageometry&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;25&lt;/td&gt; &#xA;    &lt;td&gt;228&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/center&gt; &#xA;&lt;h2&gt;Source code description&lt;/h2&gt; &#xA;&lt;p&gt;Files in this repository include python modules/scripts to run the solvers and resource files necessary for the script to execute. We listed below each of them and their description.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;File name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;geometry.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements nodes (Point, Line, Circle, etc) in the proof state graph.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;numericals.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the numerical engine in the dynamic geometry environment.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;graph_utils.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements utilities for the proof state graph.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;graph.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the proof state graph.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;problem.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the classes that represent the problem premises, conclusion, DAG nodes.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dd.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements DD and its traceback.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;ar.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements AR and its traceback.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;trace_back.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the recursive traceback and dependency difference algorithm.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;ddar.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the combination DD+AR.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;beam_search.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements beam decoding of a language model in JAX.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;models.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the transformer model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;transformer_layer.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the transformer layer.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;decoder_stack.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the transformer decoder stack.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lm_inference.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements an interface to a trained LM to perform decoding.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;alphageometry.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main script that loads problems, calls DD+AR or AlphaGeometry solver, and prints solutions.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;pretty.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pretty formating the solutions output by solvers.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;*_test.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tests for the corresponding module.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;download.sh&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Script to download model checkpoints and LM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;run.sh&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Script to execute instructions in README.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;run_tests.sh&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Script to execute the test suite.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Resource files:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Resource file name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;defs.txt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Definitions of different geometric construction actions.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;rules.txt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deduction rules for DD.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;geometry_150M_generate.gin&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Gin config of the LM implemented in meliad.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;imo_ag_30.txt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Problems in IMO-AG-30.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;jgex_ag_231.txt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Problems in JGEX-AG-231.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citing this work&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{AlphaGeometryTrinh2024,&#xA;  author  = {Trinh, Trieu and Wu, Yuhuai and Le, Quoc and He, He and Luong, Thang},&#xA;  journal = {Nature},&#xA;  title   = {Solving Olympiad Geometry without Human Demonstrations},&#xA;  year    = {2024},&#xA;  doi     = {10.1038/s41586-023-06747-5}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This research is a collaboration between the Google Brain team (now Google Deepmind) and the Computer Science Department of New York University. We thank Rif A. Saurous, Denny Zhou, Christian Szegedy, Delesley Hutchins, Thomas Kipf, Hieu Pham, Petar Veličković, Debidatta Dwibedi, Kyunghyun Cho, Lerrel Pinto, Alfredo Canziani, Thomas Wies, He He’s research group, Evan Chen (the USA’s IMO team coach), Mirek Olsak, Patrik Bak, and all three Nature&#39;s referees for their help and support.&lt;/p&gt; &#xA;&lt;p&gt;The code of AlphaGeometry communicates with and/or references the following separate libraries and packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abseil/abseil-py&#34;&gt;Abseil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/jax/&#34;&gt;JAX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;matplotlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://numpy.org&#34;&gt;NumPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scipy.org&#34;&gt;SciPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/meliad&#34;&gt;Meliad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/flax&#34;&gt;Flax&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/gin-config&#34;&gt;Gin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/text-to-text-transfer-transformer&#34;&gt;T5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;SentencePiece&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We thank all their contributors and maintainers!&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt; &#xA;&lt;p&gt;This research code is provided &#34;as-is&#34; to the broader research community. Google does not promise to maintain or otherwise support this code in any way.&lt;/p&gt; &#xA;&lt;h2&gt;Code License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2023 DeepMind Technologies Limited&lt;/p&gt; &#xA;&lt;p&gt;All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license. You may obtain a copy of the Apache 2.0 license at: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY). You may obtain a copy of the CC-BY license at: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the licenses for the specific language governing permissions and limitations under those licenses.&lt;/p&gt; &#xA;&lt;h2&gt;Model Parameters License&lt;/h2&gt; &#xA;&lt;p&gt;The AlphaGeometry checkpoints and vocabulary are made available under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You can find details at: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>RVC-Boss/GPT-SoVITS</title>
    <updated>2024-01-28T01:58:02Z</updated>
    <id>tag:github.com,2024-01-28:/RVC-Boss/GPT-SoVITS</id>
    <link href="https://github.com/RVC-Boss/GPT-SoVITS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;1 min voice data can also be used to train a good TTS model! (few shot voice cloning)&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;GPT-SoVITS-WebUI&lt;/h1&gt; A Powerful Few-shot Voice Conversion and Text-to-Speech WebUI.&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;amp;labelColor=orange&#34; alt=&#34;madewithlove&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://counter.seku.su/cmoe?name=gptsovits&amp;amp;theme=r34&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge&#34; alt=&#34;Licence&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/lj1995/GPT-SoVITS/tree/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20-Models%20Repo-yellow.svg?style=for-the-badge&#34; alt=&#34;Huggingface&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/README.md&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/docs/cn/README.md&#34;&gt;&lt;strong&gt;中文简体&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/docs/ja/README.md&#34;&gt;&lt;strong&gt;日本語&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Check out our &lt;a href=&#34;https://www.bilibili.com/video/BV12g4y1m7Uw&#34;&gt;demo video&lt;/a&gt; here!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS/assets/129054828/05bee1fa-bdd8-4d85-9350-80c060ab47fb&#34;&gt;https://github.com/RVC-Boss/GPT-SoVITS/assets/129054828/05bee1fa-bdd8-4d85-9350-80c060ab47fb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For users in China region, you can use AutoDL Cloud Docker to experience the full functionality online: &lt;a href=&#34;https://www.codewithgpu.com/i/RVC-Boss/GPT-SoVITS/GPT-SoVITS-Official&#34;&gt;https://www.codewithgpu.com/i/RVC-Boss/GPT-SoVITS/GPT-SoVITS-Official&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-shot TTS:&lt;/strong&gt; Input a 5-second vocal sample and experience instant text-to-speech conversion.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Few-shot TTS:&lt;/strong&gt; Fine-tune the model with just 1 minute of training data for improved voice similarity and realism.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cross-lingual Support:&lt;/strong&gt; Inference in languages different from the training dataset, currently supporting English, Japanese, and Chinese.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebUI Tools:&lt;/strong&gt; Integrated tools include voice accompaniment separation, automatic training set segmentation, Chinese ASR, and text labeling, assisting beginners in creating training datasets and GPT/SoVITS models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Environment Preparation&lt;/h2&gt; &#xA;&lt;p&gt;If you are a Windows user (tested with win&amp;gt;=10) you can install directly via the prezip. Just download the &lt;a href=&#34;https://huggingface.co/lj1995/GPT-SoVITS-windows-package/resolve/main/GPT-SoVITS-beta.7z?download=true&#34;&gt;prezip&lt;/a&gt;, unzip it and double-click go-webui.bat to start GPT-SoVITS-WebUI.&lt;/p&gt; &#xA;&lt;h3&gt;Tested Environments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.9, PyTorch 2.0.1, CUDA 11&lt;/li&gt; &#xA; &lt;li&gt;Python 3.10.13, PyTorch 2.1.2, CUDA 12.3&lt;/li&gt; &#xA; &lt;li&gt;Python 3.9, PyTorch 2.3.0.dev20240122, macOS 14.3 (Apple silicon, GPU)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: numba==0.56.4 require py&amp;lt;3.11&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;For Mac Users&lt;/h3&gt; &#xA;&lt;p&gt;If you are a Mac user, make sure you meet the following conditions for training and inferencing with GPU:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mac computers with Apple silicon or AMD GPUs&lt;/li&gt; &#xA; &lt;li&gt;macOS 12.3 or later&lt;/li&gt; &#xA; &lt;li&gt;Xcode command-line tools installed by running &lt;code&gt;xcode-select --install&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Other Macs can do inference with CPU only.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then install by using the following commands:&lt;/p&gt; &#xA;&lt;h4&gt;Create Environment&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n GPTSoVits python=3.9&#xA;conda activate GPTSoVits&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install Requirements&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;pip uninstall torch torchaudio&#xA;pip3 install --pre torch torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: For preprocessing with UVR5, it is recommended to &lt;a href=&#34;https://github.com/Anjok07/ultimatevocalremovergui&#34;&gt;download the original project GUI&lt;/a&gt; and select &#34;GPU Conversion&#34;. Additionally, there might be memory leak issues, especially during inference. Restarting the inference webUI can help.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Quick Install with Conda&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n GPTSoVits python=3.9&#xA;conda activate GPTSoVits&#xA;bash install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install Manually&lt;/h3&gt; &#xA;&lt;h4&gt;Pip Packages&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;FFmpeg&lt;/h4&gt; &#xA;&lt;h5&gt;Conda Users&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Ubuntu/Debian Users&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install ffmpeg&#xA;sudo apt install libsox-dev&#xA;conda install -c conda-forge &#39;ffmpeg&amp;lt;7&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;MacOS Users&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Windows Users&lt;/h5&gt; &#xA;&lt;p&gt;Download and place &lt;a href=&#34;https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe&#34;&gt;ffmpeg.exe&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe&#34;&gt;ffprobe.exe&lt;/a&gt; in the GPT-SoVITS root.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;Download pretrained models from &lt;a href=&#34;https://huggingface.co/lj1995/GPT-SoVITS&#34;&gt;GPT-SoVITS Models&lt;/a&gt; and place them in &lt;code&gt;GPT_SoVITS/pretrained_models&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Chinese ASR (additionally), download models from &lt;a href=&#34;https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/files&#34;&gt;Damo ASR Model&lt;/a&gt;, &lt;a href=&#34;https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/files&#34;&gt;Damo VAD Model&lt;/a&gt;, and &lt;a href=&#34;https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/files&#34;&gt;Damo Punc Model&lt;/a&gt; and place them in &lt;code&gt;tools/damo_asr/models&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For UVR5 (Vocals/Accompaniment Separation &amp;amp; Reverberation Removal, additionally), download models from &lt;a href=&#34;https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights&#34;&gt;UVR5 Weights&lt;/a&gt; and place them in &lt;code&gt;tools/uvr5/uvr5_weights&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using Docker&lt;/h3&gt; &#xA;&lt;h4&gt;docker-compose.yaml configuration&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Environment Variables：&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;is_half: Controls half-precision/double-precision. This is typically the cause if the content under the directories 4-cnhubert/5-wav32k is not generated correctly during the &#34;SSL extracting&#34; step. Adjust to True or False based on your actual situation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Volumes Configuration，The application&#39;s root directory inside the container is set to /workspace. The default docker-compose.yaml lists some practical examples for uploading/downloading content.&lt;/li&gt; &#xA; &lt;li&gt;shm_size： The default available memory for Docker Desktop on Windows is too small, which can cause abnormal operations. Adjust according to your own situation.&lt;/li&gt; &#xA; &lt;li&gt;Under the deploy section, GPU-related settings should be adjusted cautiously according to your system and actual circumstances.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Running with docker compose&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose -f &#34;docker-compose.yaml&#34; up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running with docker command&lt;/h4&gt; &#xA;&lt;p&gt;As above, modify the corresponding parameters based on your actual situation, then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --rm -it --gpus=all --env=is_half=False --volume=G:\GPT-SoVITS-DockerTest\output:/workspace/output --volume=G:\GPT-SoVITS-DockerTest\logs:/workspace/logs --volume=G:\GPT-SoVITS-DockerTest\SoVITS_weights:/workspace/SoVITS_weights --workdir=/workspace -p 9870:9870 -p 9871:9871 -p 9872:9872 -p 9873:9873 -p 9874:9874 --shm-size=&#34;16G&#34; -d breakstring/gpt-sovits:dev-20240123.03&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Dataset Format&lt;/h2&gt; &#xA;&lt;p&gt;The TTS annotation .list file format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;vocal_path|speaker_name|language|text&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Language dictionary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#39;zh&#39;: Chinese&lt;/li&gt; &#xA; &lt;li&gt;&#39;ja&#39;: Japanese&lt;/li&gt; &#xA; &lt;li&gt;&#39;en&#39;: English&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;D:\GPT-SoVITS\xxx/xxx.wav|xxx|en|I like playing Genshin.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Todo List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;&lt;strong&gt;High Priority:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Localization in Japanese and English.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; User guide.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Japanese and English dataset fine tune training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Zero-shot voice conversion (5s) / few-shot voice conversion (1min).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TTS speaking speed control.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enhanced TTS emotion control.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Experiment with changing SoVITS token inputs to probability distribution of vocabs.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve English and Japanese text frontend.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Develop tiny and larger-sized TTS models.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Colab scripts.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Try expand training dataset (2k hours -&amp;gt; 10k hours).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; better sovits base model (enhanced audio quality)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; model mix&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Special thanks to the following projects and contributors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/innnky/ar-vits&#34;&gt;ar-vits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yangdongchao/SoundStorm/tree/master/soundstorm/s1/AR&#34;&gt;SoundStorm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;vits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hcy71o/TransferTTS/raw/master/models.py#L556&#34;&gt;TransferTTS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TencentGameMate/chinese_speech_pretrain&#34;&gt;Chinese Speech Pretrain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/auspicious3000/contentvec/&#34;&gt;contentvec&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jik876/hifi-gan&#34;&gt;hifi-gan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/hfl/chinese-roberta-wwm-ext-large&#34;&gt;Chinese-Roberta-WWM-Ext-Large&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fishaudio/fish-speech/raw/main/tools/llama/generate.py#L41&#34;&gt;fish-speech&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Anjok07/ultimatevocalremovergui&#34;&gt;ultimatevocalremovergui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvpi/audio-slicer&#34;&gt;audio-slicer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cronrpc/SubFix&#34;&gt;SubFix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FFmpeg/FFmpeg&#34;&gt;FFmpeg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;gradio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Thanks to all contributors for their efforts&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS/graphs/contributors&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=RVC-Boss/GPT-SoVITS&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>InstantID/InstantID</title>
    <updated>2024-01-28T01:58:02Z</updated>
    <id>tag:github.com,2024-01-28:/InstantID/InstantID</id>
    <link href="https://github.com/InstantID/InstantID" rel="alternate"></link>
    <summary type="html">&lt;p&gt;InstantID : Zero-shot Identity-Preserving Generation in Seconds 🔥&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;InstantID&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://instantid.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.07519&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Technique-Report-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2401.07519&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Paper&amp;amp;message=Huggingface&amp;amp;color=orange&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/InstantX/InstantID&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InstantID/InstantID&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/InstantID/InstantID?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstantID : Zero-shot Identity-Preserving Generation in Seconds&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;InstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/applications.png&#34;&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/1/23] 🔥 Our pipeline has been merged into &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/examples/community/pipeline_stable_diffusion_xl_instantid.py&#34;&gt;diffusers&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[2024/1/22] 🔥 We release the &lt;a href=&#34;https://huggingface.co/InstantX/InstantID&#34;&gt;pre-trained checkpoints&lt;/a&gt;, &lt;a href=&#34;https://github.com/InstantID/InstantID/raw/main/infer.py&#34;&gt;inference code&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/InstantX/InstantID&#34;&gt;gradio demo&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[2024/1/15] 🔥 We release the &lt;a href=&#34;https://arxiv.org/abs/2401.07519&#34;&gt;technical report&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/12/11] 🔥 We launch the &lt;a href=&#34;https://instantid.github.io/&#34;&gt;project page&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/InstantX/InstantID&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/zsxkib/instant-id&#34;&gt;&lt;img src=&#34;https://replicate.com/zsxkib/instant-id/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/instantx/InstantID/summary&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/modelscope-InstantID-blue&#34; alt=&#34;ModelScope&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Stylized Synthesis&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/0.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Comparison with Previous Works&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/compare-a.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Comparison with existing tuning-free state-of-the-art techniques. InstantID achieves better fidelity and retain good text editability (faces and styles blend better).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/compare-c.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Comparison with pre-trained character LoRAs. We don&#39;t need multiple images and still can achieve competitive results as LoRAs without any training.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantID/InstantID/main/assets/compare-b.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Comparison with InsightFace Swapper (also known as ROOP or Refactor). However, in non-realistic style, our work is more flexible on the integration of face and background.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;You can directly download the model from &lt;a href=&#34;https://huggingface.co/InstantX/InstantID&#34;&gt;Huggingface&lt;/a&gt;. You also can download the model in python script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from huggingface_hub import hf_hub_download&#xA;hf_hub_download(repo_id=&#34;InstantX/InstantID&#34;, filename=&#34;ControlNetModel/config.json&#34;, local_dir=&#34;./checkpoints&#34;)&#xA;hf_hub_download(repo_id=&#34;InstantX/InstantID&#34;, filename=&#34;ControlNetModel/diffusion_pytorch_model.safetensors&#34;, local_dir=&#34;./checkpoints&#34;)&#xA;hf_hub_download(repo_id=&#34;InstantX/InstantID&#34;, filename=&#34;ip-adapter.bin&#34;, local_dir=&#34;./checkpoints&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you cannot access to Huggingface, you can use &lt;a href=&#34;https://hf-mirror.com/&#34;&gt;hf-mirror&lt;/a&gt; to download models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export HF_ENDPOINT=https://hf-mirror.com&#xA;huggingface-cli download --resume-download InstantX/InstantID --local-dir checkpoints&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For face encoder, you need to manually download via this &lt;a href=&#34;https://github.com/deepinsight/insightface/issues/1896#issuecomment-1023867304&#34;&gt;URL&lt;/a&gt; to &lt;code&gt;models/antelopev2&lt;/code&gt; as the default link is invalid. Once you have prepared all models, the folder tree should be like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  .&#xA;  ├── models&#xA;  ├── checkpoints&#xA;  ├── ip_adapter&#xA;  ├── pipeline_stable_diffusion_xl_instantid.py&#xA;  └── README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# !pip install opencv-python transformers accelerate insightface&#xA;import diffusers&#xA;from diffusers.utils import load_image&#xA;from diffusers.models import ControlNetModel&#xA;&#xA;import cv2&#xA;import torch&#xA;import numpy as np&#xA;from PIL import Image&#xA;&#xA;from insightface.app import FaceAnalysis&#xA;from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps&#xA;&#xA;# prepare &#39;antelopev2&#39; under ./models&#xA;app = FaceAnalysis(name=&#39;antelopev2&#39;, root=&#39;./&#39;, providers=[&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;])&#xA;app.prepare(ctx_id=0, det_size=(640, 640))&#xA;&#xA;# prepare models under ./checkpoints&#xA;face_adapter = f&#39;./checkpoints/ip-adapter.bin&#39;&#xA;controlnet_path = f&#39;./checkpoints/ControlNetModel&#39;&#xA;&#xA;# load IdentityNet&#xA;controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)&#xA;&#xA;base_model = &#39;wangqixun/YamerMIX_v8&#39;  # from https://civitai.com/models/84040?modelVersionId=196039&#xA;pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(&#xA;    base_model,&#xA;    controlnet=controlnet,&#xA;    torch_dtype=torch.float16&#xA;)&#xA;pipe.cuda()&#xA;&#xA;# load adapter&#xA;pipe.load_ip_adapter_instantid(face_adapter)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can customized your own face images&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load an image&#xA;face_image = load_image(&#34;./examples/yann-lecun_resize.jpg&#34;)&#xA;&#xA;# prepare face emb&#xA;face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))&#xA;face_info = sorted(face_info, key=lambda x:(x[&#39;bbox&#39;][2]-x[&#39;bbox&#39;][0])*x[&#39;bbox&#39;][3]-x[&#39;bbox&#39;][1])[-1]  # only use the maximum face&#xA;face_emb = face_info[&#39;embedding&#39;]&#xA;face_kps = draw_kps(face_image, face_info[&#39;kps&#39;])&#xA;&#xA;# prompt&#xA;prompt = &#34;film noir style, ink sketch|vector, male man, highly detailed, sharp focus, ultra sharpness, monochrome, high contrast, dramatic shadows, 1940s style, mysterious, cinematic&#34;&#xA;negative_prompt = &#34;ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, vibrant, colorful&#34;&#xA;&#xA;# generate image&#xA;image = pipe(&#xA;    prompt,&#xA;    image_embeds=face_emb,&#xA;    image=face_kps,&#xA;    controlnet_conditioning_scale=0.8,&#xA;    ip_adapter_scale=0.8,&#xA;).images[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Speed Up with LCM-LoRA&lt;/h2&gt; &#xA;&lt;p&gt;Our work is compatible with &lt;a href=&#34;https://github.com/luosiallen/latent-consistency-model&#34;&gt;LCM-LoRA&lt;/a&gt;. First, download the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from huggingface_hub import hf_hub_download&#xA;hf_hub_download(repo_id=&#34;latent-consistency/lcm-lora-sdxl&#34;, filename=&#34;pytorch_lora_weights.safetensors&#34;, local_dir=&#34;./checkpoints&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use it, you just need to load it and infer with a small num_inference_steps. Note that it is recommendated to set guidance_scale between [0, 1].&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffusers import LCMScheduler&#xA;&#xA;lcm_lora_path = &#34;./checkpoints/pytorch_lora_weights.safetensors&#34;&#xA;&#xA;pipe.load_lora_weights(lcm_lora_path)&#xA;pipe.fuse_lora()&#xA;pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)&#xA;&#xA;num_inference_steps = 10&#xA;guidance_scale = 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Start a local gradio demo&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python gradio_demo/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For higher similarity, increase the weight of controlnet_conditioning_scale (IdentityNet) and ip_adapter_scale (Adapter).&lt;/li&gt; &#xA; &lt;li&gt;For over-saturation, decrease the ip_adapter_scale. If not work, decrease controlnet_conditioning_scale.&lt;/li&gt; &#xA; &lt;li&gt;For higher text control ability, decrease ip_adapter_scale.&lt;/li&gt; &#xA; &lt;li&gt;For specific styles, choose corresponding base model makes differences.&lt;/li&gt; &#xA; &lt;li&gt;We have not supported multi-person yet, will only use the largest face as reference pose.&lt;/li&gt; &#xA; &lt;li&gt;We provide a &lt;a href=&#34;https://github.com/ahgsql/StyleSelectorXL/raw/main/sdxl_styles.json&#34;&gt;style template&lt;/a&gt; for reference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community Resources&lt;/h2&gt; &#xA;&lt;h3&gt;Replicate Demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/zsxkib/instant-id&#34;&gt;zsxkib/instant-id&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ComfyUI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-InstantID&#34;&gt;ZHO-ZHO-ZHO/ComfyUI-InstantID&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huxiuhan/ComfyUI-InstantID&#34;&gt;huxiuhan/ComfyUI-InstantID&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sdbds/InstantID-for-windows&#34;&gt;sdbds/InstantID-for-windows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our work is highly inspired by &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IP-Adapter&lt;/a&gt; and &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;. Thanks for their great works!&lt;/li&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO&#34;&gt;ZHO-ZHO-ZHO&lt;/a&gt;, &lt;a href=&#34;https://github.com/huxiuhan&#34;&gt;huxiuhan&lt;/a&gt;, &lt;a href=&#34;https://github.com/sdbds&#34;&gt;sdbds&lt;/a&gt;, &lt;a href=&#34;https://replicate.com/zsxkib&#34;&gt;zsxkib&lt;/a&gt; for their generous contributions.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the &lt;a href=&#34;https://github.com/huggingface&#34;&gt;HuggingFace&lt;/a&gt; gradio team for their free GPU support!&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the &lt;a href=&#34;https://github.com/modelscope/modelscope&#34;&gt;ModelScope&lt;/a&gt; team for their free GPU support!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The code of InstantID is released under &lt;a href=&#34;https://github.com/InstantID/InstantID?tab=Apache-2.0-1-ov-file#readme&#34;&gt;Apache License&lt;/a&gt; for both academic and commercial usage. &lt;strong&gt;However, both manual-downloading and auto-downloading face models from insightface are for non-commercial research purposes only&lt;/strong&gt; accoreding to their &lt;a href=&#34;https://github.com/deepinsight/insightface?tab=readme-ov-file#license&#34;&gt;license&lt;/a&gt;. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#InstantID/InstantID&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=InstantID/InstantID&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;If you find InstantID useful for your research and applications, please cite us using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wang2024instantid,&#xA;  title={InstantID: Zero-shot Identity-Preserving Generation in Seconds},&#xA;  author={Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony},&#xA;  journal={arXiv preprint arXiv:2401.07519},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>