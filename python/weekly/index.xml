<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-30T02:04:30Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bhaskatripathi/pdfGPT</title>
    <updated>2023-04-30T02:04:30Z</updated>
    <id>tag:github.com,2023-04-30:/bhaskatripathi/pdfGPT</id>
    <link href="https://github.com/bhaskatripathi/pdfGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PDF GPT allows you to chat with the contents of your PDF file by using GPT capabilities. The only open source solution to turn your pdf files in a chatbot!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pdfGPT&lt;/h1&gt; &#xA;&lt;h3&gt;Problem Description :&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;When you pass a large text to Open AI, it suffers from a 4K token limit. It cannot take an entire pdf file as an input&lt;/li&gt; &#xA; &lt;li&gt;Open AI sometimes becomes overtly chatty and returns irrelevant response not directly related to your query. This is because Open AI uses poor embeddings.&lt;/li&gt; &#xA; &lt;li&gt;ChatGPT cannot directly talk to external data. Some solutions use Langchain but it is token hungry if not implemented correctly.&lt;/li&gt; &#xA; &lt;li&gt;There are a number of solutions like &lt;a href=&#34;https://www.chatpdf.com&#34;&gt;https://www.chatpdf.com&lt;/a&gt;, &lt;a href=&#34;https://www.bespacific.com/chat-with-any-pdf&#34;&gt;https://www.bespacific.com/chat-with-any-pdf&lt;/a&gt;, &lt;a href=&#34;https://www.filechat.io&#34;&gt;https://www.filechat.io&lt;/a&gt; they have poor content quality and are prone to hallucination problem. One good way to avoid hallucinations and improve truthfulness is to use improved embeddings. To solve this problem, I propose to improve embeddings with Universal Sentence Encoder family of algorithms (Read more here: &lt;a href=&#34;https://tfhub.dev/google/collections/universal-sentence-encoder/1&#34;&gt;https://tfhub.dev/google/collections/universal-sentence-encoder/1&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Solution: What is PDF GPT ?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;PDF GPT allows you to chat with an uploaded PDF file using GPT functionalities.&lt;/li&gt; &#xA; &lt;li&gt;The application intelligently breaks the document into smaller chunks and employs a powerful Deep Averaging Network Encoder to generate embeddings.&lt;/li&gt; &#xA; &lt;li&gt;A semantic search is first performed on your pdf content and the most relevant embeddings are passed to the Open AI.&lt;/li&gt; &#xA; &lt;li&gt;A custom logic generates precise responses. The returned response can even cite the page number in square brackets([]) where the information is located, adding credibility to the responses and helping to locate pertinent information quickly. The Responses are much better than the naive responses by Open AI.&lt;/li&gt; &#xA; &lt;li&gt;Andrej Karpathy mentioned in this post that KNN algorithm is most appropriate for similar problems: &lt;a href=&#34;https://twitter.com/karpathy/status/1647025230546886658&#34;&gt;https://twitter.com/karpathy/status/1647025230546886658&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;Demo URL: &lt;a href=&#34;https://bit.ly/41ZXBJM&#34;&gt;https://bit.ly/41ZXBJM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Please star this project if you like it!&lt;/p&gt; &#xA;&lt;h3&gt;UML&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;sequenceDiagram&#xA;    participant User&#xA;    participant System&#xA;&#xA;    User-&amp;gt;&amp;gt;System: Enter API Key&#xA;    User-&amp;gt;&amp;gt;System: Upload PDF/PDF URL&#xA;    User-&amp;gt;&amp;gt;System: Ask Question&#xA;    User-&amp;gt;&amp;gt;System: Submit Call to Action&#xA;&#xA;    System-&amp;gt;&amp;gt;System: Blank field Validations&#xA;    System-&amp;gt;&amp;gt;System: Convert PDF to Text&#xA;    System-&amp;gt;&amp;gt;System: Decompose Text to Chunks (150 word length)&#xA;    System-&amp;gt;&amp;gt;System: Check if embeddings file exists&#xA;    System-&amp;gt;&amp;gt;System: If file exists, load embeddings and set the fitted attribute to True&#xA;    System-&amp;gt;&amp;gt;System: If file doesn&#39;t exist, generate embeddings, fit the recommender, save embeddings to file and set fitted attribute to True&#xA;    System-&amp;gt;&amp;gt;System: Perform Semantic Search and return Top 5 Chunks with KNN&#xA;    System-&amp;gt;&amp;gt;System: Load Open AI prompt&#xA;    System-&amp;gt;&amp;gt;System: Embed Top 5 Chunks in Open AI Prompt&#xA;    System-&amp;gt;&amp;gt;System: Generate Answer with Davinci&#xA;&#xA;    System--&amp;gt;&amp;gt;User: Return Answer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Flowchart&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TB&#xA;A[Input] --&amp;gt; B[URL]&#xA;A -- Upload File manually --&amp;gt; C[Parse PDF]&#xA;B --&amp;gt; D[Parse PDF] -- Preprocess --&amp;gt; E[Dynamic Text Chunks]&#xA;C -- Preprocess --&amp;gt; E[Dynamic Text Chunks with citation history]&#xA;E --Fit--&amp;gt;F[Generate text embedding with Deep Averaging Network Encoder on each chunk]&#xA;F -- Query --&amp;gt; G[Get Top Results]&#xA;G -- K-Nearest Neighbour --&amp;gt; K[Get Nearest Neighbour - matching citation references]&#xA;K -- Generate Prompt --&amp;gt; H[Generate Answer]&#xA;H -- Output --&amp;gt; I[Output]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#bhaskatripathi/pdfGPT&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=bhaskatripathi/pdfGPT&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Also Try:&lt;/h2&gt; &#xA;&lt;p&gt;This app creates schematic architecture diagrams, UML, flowcharts, Gantt charts and many more. You simple need to mention the usecase in natural language and it will create the desired diagram. &lt;a href=&#34;https://github.com/bhaskatripathi/Text2Diagram&#34;&gt;https://github.com/bhaskatripathi/Text2Diagram&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>imClumsyPanda/langchain-ChatGLM</title>
    <updated>2023-04-30T02:04:30Z</updated>
    <id>tag:github.com,2023-04-30:/imClumsyPanda/langchain-ChatGLM</id>
    <link href="https://github.com/imClumsyPanda/langchain-ChatGLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;langchain-ChatGLM, local knowledge based ChatGLM with langchain ｜ 基于本地知识的 ChatGLM 问答&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;基于本地知识的 ChatGLM 应用实现&lt;/h1&gt; &#xA;&lt;h2&gt;介绍&lt;/h2&gt; &#xA;&lt;p&gt;🌍 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/README_en.md&#34;&gt;&lt;em&gt;READ THIS IN ENGLISH&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🤖️ 一种利用 &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt; + &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;langchain&lt;/a&gt; 实现的基于本地知识的 ChatGLM 应用。增加 &lt;a href=&#34;https://github.com/clue-ai/ChatYuan&#34;&gt;clue-ai/ChatYuan&lt;/a&gt; 项目的模型 &lt;a href=&#34;https://huggingface.co/ClueAI/ChatYuan-large-v2&#34;&gt;ClueAI/ChatYuan-large-v2&lt;/a&gt; 的支持。&lt;/p&gt; &#xA;&lt;p&gt;💡 受 &lt;a href=&#34;https://github.com/GanymedeNil&#34;&gt;GanymedeNil&lt;/a&gt; 的项目 &lt;a href=&#34;https://github.com/GanymedeNil/document.ai&#34;&gt;document.ai&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/AlexZhangji&#34;&gt;AlexZhangji&lt;/a&gt; 创建的 &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/pull/216&#34;&gt;ChatGLM-6B Pull Request&lt;/a&gt; 启发，建立了全部基于开源模型实现的本地知识问答应用。&lt;/p&gt; &#xA;&lt;p&gt;✅ 本项目中 Embedding 默认选用的是 &lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese/tree/main&#34;&gt;GanymedeNil/text2vec-large-chinese&lt;/a&gt;，LLM 默认选用的是 &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;。依托上述模型，本项目可实现全部使用&lt;strong&gt;开源&lt;/strong&gt;模型&lt;strong&gt;离线私有部署&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;⛓️ 本项目实现原理如下图所示，过程包括加载文件 -&amp;gt; 读取文本 -&amp;gt; 文本分割 -&amp;gt; 文本向量化 -&amp;gt; 问句向量化 -&amp;gt; 在文本向量中匹配出与问句向量最相似的&lt;code&gt;top k&lt;/code&gt;个 -&amp;gt; 匹配出的文本作为上下文和问题一起添加到&lt;code&gt;prompt&lt;/code&gt;中 -&amp;gt; 提交给&lt;code&gt;LLM&lt;/code&gt;生成回答。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/langchain+chatglm.png&#34; alt=&#34;实现原理图&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;🚩 本项目未涉及微调、训练过程，但可利用微调或训练对本项目效果进行优化。&lt;/p&gt; &#xA;&lt;p&gt;📓 &lt;a href=&#34;https://www.heywhale.com/mw/project/643977aa446c45f4592a1e59&#34;&gt;ModelWhale 在线运行项目&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;变更日志&lt;/h2&gt; &#xA;&lt;p&gt;参见 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/docs/CHANGELOG.md&#34;&gt;变更日志&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;硬件需求&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ChatGLM-6B 模型硬件需求&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;&lt;strong&gt;量化等级&lt;/strong&gt;&lt;/th&gt; &#xA;     &lt;th&gt;&lt;strong&gt;最低 GPU 显存&lt;/strong&gt;（推理）&lt;/th&gt; &#xA;     &lt;th&gt;&lt;strong&gt;最低 GPU 显存&lt;/strong&gt;（高效参数微调）&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;FP16（无量化）&lt;/td&gt; &#xA;     &lt;td&gt;13 GB&lt;/td&gt; &#xA;     &lt;td&gt;14 GB&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;INT8&lt;/td&gt; &#xA;     &lt;td&gt;8 GB&lt;/td&gt; &#xA;     &lt;td&gt;9 GB&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;INT4&lt;/td&gt; &#xA;     &lt;td&gt;6 GB&lt;/td&gt; &#xA;     &lt;td&gt;7 GB&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Embedding 模型硬件需求&lt;/p&gt; &lt;p&gt;本项目中默认选用的 Embedding 模型 &lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese/tree/main&#34;&gt;GanymedeNil/text2vec-large-chinese&lt;/a&gt; 约占用显存 3GB，也可修改为在 CPU 中运行。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker 部署&lt;/h2&gt; &#xA;&lt;p&gt;为了能让容器使用主机GPU资源，需要在主机上安装 &lt;a href=&#34;https://github.com/NVIDIA/nvidia-container-toolkit&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;。具体安装步骤如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get update&#xA;sudo apt-get install -y nvidia-container-toolkit-base&#xA;sudo systemctl daemon-reload &#xA;sudo systemctl restart docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;安装完成后，可以使用以下命令编译镜像和启动容器：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t chatglm-cuda:latest .&#xA;docker run --gpus all -d --name chatglm -p 7860:7860  chatglm-cuda:latest&#xA;&#xA;#若要使用离线模型，请配置好模型路径，然后此repo挂载到Container&#xA;docker run --gpus all -d --name chatglm -p 7860:7860 -v ~/github/langchain-ChatGLM:/chatGLM  chatglm-cuda:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;开发部署&lt;/h2&gt; &#xA;&lt;h3&gt;软件需求&lt;/h3&gt; &#xA;&lt;p&gt;本项目已在 Python 3.8 - 3.10，CUDA 11.7 环境下完成测试。已在 Windows、ARM 架构的 macOS、Linux 系统中完成测试。&lt;/p&gt; &#xA;&lt;h3&gt;从本地加载模型&lt;/h3&gt; &#xA;&lt;p&gt;请参考 &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B#%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B&#34;&gt;THUDM/ChatGLM-6B#从本地加载模型&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. 安装环境&lt;/h3&gt; &#xA;&lt;p&gt;参见 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/docs/INSTALL.md&#34;&gt;安装指南&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;2. 设置模型默认参数&lt;/h3&gt; &#xA;&lt;p&gt;在开始执行 Web UI 或命令行交互前，请先检查 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/configs/model_config.py&#34;&gt;configs/model_config.py&lt;/a&gt; 中的各项模型参数设计是否符合需求。&lt;/p&gt; &#xA;&lt;h3&gt;3. 执行脚本体验 Web UI 或命令行交互&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注：鉴于环境部署过程中可能遇到问题，建议首先测试命令行脚本。建议命令行脚本测试可正常运行后再运行 Web UI。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;执行 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/cli_demo.py&#34;&gt;cli_demo.py&lt;/a&gt; 脚本体验&lt;strong&gt;命令行交互&lt;/strong&gt;：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python cli_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;或执行 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/webui.py&#34;&gt;webui.py&lt;/a&gt; 脚本体验 &lt;strong&gt;Web 交互&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python webui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注：如未将模型下载至本地，请执行前检查&lt;code&gt;$HOME/.cache/huggingface/&lt;/code&gt;文件夹剩余空间，至少15G。&lt;/p&gt; &#xA;&lt;p&gt;执行后效果如下图所示： &lt;img src=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/webui_0419.png&#34; alt=&#34;webui&#34;&gt; Web UI 可以实现如下功能：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;运行前自动读取&lt;code&gt;configs/model_config.py&lt;/code&gt;中&lt;code&gt;LLM&lt;/code&gt;及&lt;code&gt;Embedding&lt;/code&gt;模型枚举及默认模型设置运行模型，如需重新加载模型，可在 &lt;code&gt;模型配置&lt;/code&gt; 标签页重新选择后点击 &lt;code&gt;重新加载模型&lt;/code&gt; 进行模型加载；&lt;/li&gt; &#xA; &lt;li&gt;可手动调节保留对话历史长度、匹配知识库文段数量，可根据显存大小自行调节；&lt;/li&gt; &#xA; &lt;li&gt;具备模式选择功能，可选择 &lt;code&gt;LLM对话&lt;/code&gt; 与 &lt;code&gt;知识库问答&lt;/code&gt; 模式进行对话，支持流式对话；&lt;/li&gt; &#xA; &lt;li&gt;添加 &lt;code&gt;配置知识库&lt;/code&gt; 功能，支持选择已有知识库或新建知识库，并可向知识库中&lt;strong&gt;新增&lt;/strong&gt;上传文件/文件夹，使用文件上传组件选择好文件后点击 &lt;code&gt;上传文件并加载知识库&lt;/code&gt;，会将所选上传文档数据加载至知识库中，并基于更新后知识库进行问答；&lt;/li&gt; &#xA; &lt;li&gt;后续版本中将会增加对知识库的修改或删除，及知识库中已导入文件的查看。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;常见问题&lt;/h3&gt; &#xA;&lt;p&gt;参见 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/docs/FAQ.md&#34;&gt;常见问题&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;以问题&lt;code&gt;chatglm-6b 的局限性具体体现在哪里，如何实现改进&lt;/code&gt;为例：&lt;/p&gt; &#xA;&lt;p&gt;未使用 langchain 接入本地文档时：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ChatGLM-6B 是一个基于 GPT-3 的聊天机器人,它的局限性主要体现在以下几个方面:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;语言模型的精度:GPT-3 是一种大型的语言模型,它的精度比较高,但是仍然有一些局限性。例如,它可能无法准确地理解某些方言、口音或者复杂的句子结构。&lt;/li&gt; &#xA;  &lt;li&gt;知识图谱的完整性:GPT-3 可以生成一些知识图谱,但是其完整性和准确性可能不够。例如,它可能无法准确地理解某些领域的专业知识和术语。&lt;/li&gt; &#xA;  &lt;li&gt;对话生成的连贯性:GPT-3 可以生成一些连贯的对话,但是其生成的对话可能不够自然。例如,它可能会生成一些虚假的、不连贯的对话或者误解用户的意图。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;为了改进 ChatGLM-6B,可以考虑以下几个方面:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;提高语言模型的精度:可以通过增加训练数据、使用更好的语言模型架构、优化模型训练过程等方式来提高语言模型的精度。&lt;/li&gt; &#xA;  &lt;li&gt;增强知识图谱的完整性:可以通过增加知识图谱的维度、使用更好的知识图谱构建方法、优化知识图谱的匹配算法等方式来增强知识图谱的完整性。&lt;/li&gt; &#xA;  &lt;li&gt;提高对话生成的连贯性:可以通过增加对话生成模型的上下文信息、使用更好的上下文信息提取方法、优化对话生成模型的算法等方式来提高对话生成的连贯性。&lt;/li&gt; &#xA;  &lt;li&gt;引入更多的人类反馈:人类反馈可以帮助 ChatGLM-6B 更好地理解用户的意图和需求,从而提高对话生成的质量和准确性。可以通过引入更多的人类客服、使用人类反馈收集器等方式来引入更多的人类反馈。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;使用 langchain 接入 ChatGLM-6B 项目的 README.md 文件后 ChatGLM 的回答：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ChatGLM-6B 是一个基于深度学习的自然语言处理模型,它在回答问题方面表现出色。但是,它也有一些局限性,具体如下:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;语言理解有限:ChatGLM-6B 只学习了中文自然语言,对于其他语言可能会存在理解上的局限性。&lt;/li&gt; &#xA;  &lt;li&gt;知识储备不足:ChatGLM-6B 的训练数据集只包含了中文语料,因此它可能无法回答一些非中文的问题或者某些特定领域的问题。&lt;/li&gt; &#xA;  &lt;li&gt;数据量有限:ChatGLM-6B 的训练数据集只有几百万条记录,因此它可能无法回答一些非常具体或者复杂的问题。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;为了改进 ChatGLM-6B,可以考虑以下几个方面:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;学习更多的语言知识:可以学习其他语言的自然语言处理技术,扩大语言理解的范围。&lt;/li&gt; &#xA;  &lt;li&gt;扩大知识储备:可以收集更多的中文语料,或者使用其他语言的数据集来扩充知识储备。&lt;/li&gt; &#xA;  &lt;li&gt;增加数据量:可以使用更大的数据集来训练 ChatGLM-6B,提高模型的表现。&lt;/li&gt; &#xA;  &lt;li&gt;引入更多的评估指标:可以引入更多的评估指标来评估模型的表现,从而发现 ChatGLM-6B 存在的不足和局限性。&lt;/li&gt; &#xA;  &lt;li&gt;改进模型架构:可以改进 ChatGLM-6B 的模型架构,提高模型的性能和表现。例如,可以使用更大的神经网络或者改进的卷积神经网络结构。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;路线图&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Langchain 应用 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 接入非结构化文档（已支持 md、pdf、docx、txt 文件格式）&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 搜索引擎与本地网页接入&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 结构化数据接入（如 csv、Excel、SQL 等）&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 知识图谱/图数据库接入&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Agent 实现&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加更多 LLM 模型支持 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;THUDM/chatglm-6b&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b-int8&#34;&gt;THUDM/chatglm-6b-int8&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b-int4&#34;&gt;THUDM/chatglm-6b-int4&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b-int4-qe&#34;&gt;THUDM/chatglm-6b-int4-qe&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/ClueAI/ChatYuan-large-v2&#34;&gt;ClueAI/ChatYuan-large-v2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加更多 Embedding 模型支持 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/nghuyong/ernie-3.0-nano-zh&#34;&gt;nghuyong/ernie-3.0-nano-zh&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/nghuyong/ernie-3.0-base-zh&#34;&gt;nghuyong/ernie-3.0-base-zh&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/shibing624/text2vec-base-chinese&#34;&gt;shibing624/text2vec-base-chinese&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese&#34;&gt;GanymedeNil/text2vec-large-chinese&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Web UI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 利用 gradio 实现 Web UI DEMO&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 添加输出内容及错误提示&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 引用标注&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加知识库管理 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 选择知识库开始问答&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 上传文件/文件夹至知识库&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 删除知识库中文件&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 利用 streamlit 实现 Web UI Demo&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加 API 支持 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 利用 fastapi 实现 API 部署方式&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 实现调用 API 的 Web UI Demo&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;项目交流群&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/qr_code_9.jpg&#34; alt=&#34;二维码&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;🎉 langchain-ChatGLM 项目交流群，如果你也对本项目感兴趣，欢迎加入群聊参与讨论交流。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Vision-CAIR/MiniGPT-4</title>
    <updated>2023-04-30T02:04:30Z</updated>
    <id>tag:github.com,2023-04-30:/Vision-CAIR/MiniGPT-4</id>
    <link href="https://github.com/Vision-CAIR/MiniGPT-4" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tsutikgiau.github.io/&#34;&gt;Deyao Zhu&lt;/a&gt;* (On Job Market!), &lt;a href=&#34;https://junchen14.github.io/&#34;&gt;Jun Chen&lt;/a&gt;* (On Job Market!), &lt;a href=&#34;https://xiaoqian-shen.github.io&#34;&gt;Xiaoqian Shen&lt;/a&gt;, &lt;a href=&#34;https://xiangli.ac.cn&#34;&gt;Xiang Li&lt;/a&gt;, and &lt;a href=&#34;https://www.mohamed-elhoseiny.com/&#34;&gt;Mohamed Elhoseiny&lt;/a&gt;. *Equal Contribution&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;King Abdullah University of Science and Technology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://minigpt-4.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.10592&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Vision-CAIR/minigpt4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Vision-CAIR/MiniGPT-4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1OK4kYsZphwt5DXchKkzMBjYF6jnkqh4R?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=__tftoxpBAw&amp;amp;feature=youtu.be&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;We now provide a pretrained MiniGPT-4 aligned with Vicuna-7B! The demo GPU memory consumption now can be as low as 12GB.&lt;/p&gt; &#xA;&lt;h2&gt;Online Demo&lt;/h2&gt; &#xA;&lt;p&gt;Click the image to chat with MiniGPT-4 around your images &lt;a href=&#34;https://minigpt-4.github.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/online_demo.png&#34; alt=&#34;demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/examples/wop_2.png&#34; alt=&#34;find wild&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/examples/ad_2.png&#34; alt=&#34;write story&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/examples/fix_1.png&#34; alt=&#34;solve problem&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/examples/rhyme_1.png&#34; alt=&#34;write Poem&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;More examples can be found in the &lt;a href=&#34;https://minigpt-4.github.io&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MiniGPT-4 aligns a frozen visual encoder from BLIP-2 with a frozen LLM, Vicuna, using just one projection layer.&lt;/li&gt; &#xA; &lt;li&gt;We train MiniGPT-4 with two stages. The first traditional pretraining stage is trained using roughly 5 million aligned image-text pairs in 10 hours using 4 A100s. After the first stage, Vicuna is able to understand the image. But the generation ability of Vicuna is heavilly impacted.&lt;/li&gt; &#xA; &lt;li&gt;To address this issue and improve usability, we propose a novel way to create high-quality image-text pairs by the model itself and ChatGPT together. Based on this, we then create a small (3500 pairs in total) yet high-quality dataset.&lt;/li&gt; &#xA; &lt;li&gt;The second finetuning stage is trained on this dataset in a conversation template to significantly improve its generation reliability and overall usability. To our surprise, this stage is computationally efficient and takes only around 7 minutes with a single A100.&lt;/li&gt; &#xA; &lt;li&gt;MiniGPT-4 yields many emerging vision-language capabilities similar to those demonstrated in GPT-4.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/figs/overview.png&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Prepare the code and the environment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Git clone our repository, creating a python environment and ativate it via the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Vision-CAIR/MiniGPT-4.git&#xA;cd MiniGPT-4&#xA;conda env create -f environment.yml&#xA;conda activate minigpt4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Prepare the pretrained Vicuna weights&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The current version of MiniGPT-4 is built on the v0 versoin of Vicuna-13B. Please refer to our instruction &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/PrepareVicuna.md&#34;&gt;here&lt;/a&gt; to prepare the Vicuna weights. The final weights would be in a single folder in a structure similar to the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;vicuna_weights&#xA;├── config.json&#xA;├── generation_config.json&#xA;├── pytorch_model.bin.index.json&#xA;├── pytorch_model-00001-of-00003.bin&#xA;...   &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, set the path to the vicuna weight in the model config file &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/minigpt4/configs/models/minigpt4.yaml#L16&#34;&gt;here&lt;/a&gt; at Line 16.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Prepare the pretrained MiniGPT-4 checkpoint&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pretrained checkpoints according to the Vicuna model you prepare.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint Aligned with Vicuna 13B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint Aligned with Vicuna 7B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1a4zLvaiDBr-36pasffmgpvH5P7CKmpze/view?usp=share_link&#34;&gt;Downlad&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R/view?usp=sharing&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Then, set the path to the pretrained checkpoint in the evaluation config file in &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/eval_configs/minigpt4_eval.yaml#L10&#34;&gt;eval_configs/minigpt4_eval.yaml&lt;/a&gt; at Line 11.&lt;/p&gt; &#xA;&lt;h3&gt;Launching Demo Locally&lt;/h3&gt; &#xA;&lt;p&gt;Try out our demo &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/demo.py&#34;&gt;demo.py&lt;/a&gt; on your local machine by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py --cfg-path eval_configs/minigpt4_eval.yaml  --gpu-id 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To save GPU memory, Vicuna loads as 8 bit by default, with a beam search width of 1. This configuration requires about 23G GPU memory for Vicuna 13B and 11.5G GPU memory for Vicuna 7B. For more powerful GPUs, you can run the model in 16 bit by setting low_resource to False in the config file &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/eval_configs/minigpt4_eval.yaml&#34;&gt;minigpt4_eval.yaml&lt;/a&gt; and use a larger beam search width.&lt;/p&gt; &#xA;&lt;p&gt;Thanks &lt;a href=&#34;https://github.com/WangRongsheng&#34;&gt;@WangRongsheng&lt;/a&gt;, you can also run our code on &lt;a href=&#34;https://colab.research.google.com/drive/1OK4kYsZphwt5DXchKkzMBjYF6jnkqh4R?usp=sharing&#34;&gt;Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;The training of MiniGPT-4 contains two alignment stages.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. First pretraining stage&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the first pretrained stage, the model is trained using image-text pairs from Laion and CC datasets to align the vision and language model. To download and prepare the datasets, please check our &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/dataset/README_1_STAGE.md&#34;&gt;first stage dataset preparation instruction&lt;/a&gt;. After the first stage, the visual features are mapped and can be understood by the language model. To launch the first stage training, run the following command. In our experiments, we use 4 A100. You can change the save path in the config file &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/train_configs/minigpt4_stage1_pretrain.yaml&#34;&gt;train_configs/minigpt4_stage1_pretrain.yaml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage1_pretrain.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A MiniGPT-4 checkpoint with only stage one training can be downloaded &lt;a href=&#34;https://drive.google.com/file/d/1u9FRRBB3VovP1HxCAlpD9Lw4t4P6-Yq8/view?usp=share_link&#34;&gt;here (13B)&lt;/a&gt; or &lt;a href=&#34;https://drive.google.com/file/d/1HihQtCEXUyBM1i9DQbaK934wW3TZi-h5/view?usp=share_link&#34;&gt;here (7B)&lt;/a&gt;. Compared to the model after stage two, this checkpoint generate incomplete and repeated sentences frequently.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Second finetuning stage&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the second stage, we use a small high quality image-text pair dataset created by ourselves and convert it to a conversation format to further align MiniGPT-4. To download and prepare our second stage dataset, please check our &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/dataset/README_2_STAGE.md&#34;&gt;second stage dataset preparation instruction&lt;/a&gt;. To launch the second stage alignment, first specify the path to the checkpoint file trained in stage 1 in &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/train_configs/minigpt4_stage2_finetune.yaml&#34;&gt;train_configs/minigpt4_stage1_pretrain.yaml&lt;/a&gt;. You can also specify the output path there. Then, run the following command. In our experiments, we use 1 A100.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage2_finetune.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the second stage alignment, MiniGPT-4 is able to talk about the image coherently and user-friendly.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/blip-2&#34;&gt;BLIP2&lt;/a&gt; The model architecture of MiniGPT-4 follows BLIP-2. Don&#39;t forget to check this great open-source work if you don&#39;t know it before!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;Lavis&lt;/a&gt; This repository is built upon Lavis!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt; The fantastic language ability of Vicuna with only 13B parameters is just amazing. And it is open-source!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re using MiniGPT-4 in your research or applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhu2022minigpt4,&#xA;      title={MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models}, &#xA;      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},&#xA;      journal={arXiv preprint arXiv:2304.10592},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is under &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/LICENSE.md&#34;&gt;BSD 3-Clause License&lt;/a&gt;. Many codes are based on &lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;Lavis&lt;/a&gt; with BSD 3-Clause License &lt;a href=&#34;https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/LICENSE_Lavis.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>