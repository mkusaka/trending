<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-28T01:41:21Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>1Panel-dev/MaxKB</title>
    <updated>2024-04-28T01:41:21Z</updated>
    <id>tag:github.com,2024-04-28:/1Panel-dev/MaxKB</id>
    <link href="https://github.com/1Panel-dev/MaxKB" rel="alternate"></link>
    <summary type="html">&lt;p&gt;💬 基于 LLM 大语言模型的知识库问答系统。开箱即用，支持快速嵌入到第三方业务系统，1Panel 官方出品。&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf&#34; alt=&#34;MaxKB&#34; width=&#34;300&#34;&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;基于 LLM 大语言模型的知识库问答系统&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.gnu.org/licenses/old-licenses/gpl-3.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF&#34; alt=&#34;License: GPL v3&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codacy.com/gh/1Panel-dev/maxkb?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=1Panel-dev/maxkb&amp;amp;utm_campaign=Badge_Grade_Dashboard&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/da67574fd82b473992781d1386b937ef&#34; alt=&#34;Codacy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/1Panel-dev/maxkb/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/1Panel-dev/maxkb&#34; alt=&#34;Latest release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/1Panel-dev/maxkb&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/1Panel-dev/maxkb?color=%231890FF&amp;amp;style=flat-square&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/1panel/maxkb&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/1panel/maxkb?label=downloads&#34; alt=&#34;Download&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;MaxKB 是一款基于 LLM 大语言模型的知识库问答系统。MaxKB = Max Knowledge Base，旨在成为企业的最强大脑。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;开箱即用&lt;/strong&gt;：支持直接上传文档、自动爬取在线文档，支持文本自动拆分、向量化，智能问答交互体验好；&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;无缝嵌入&lt;/strong&gt;：支持零编码快速嵌入到第三方业务系统；&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;多模型支持&lt;/strong&gt;：支持对接主流的大模型，包括 Ollama 本地私有大模型（如 Llama 2、Llama 3、qwen）、通义千问、OpenAI、Azure OpenAI、Kimi、智谱 AI、讯飞星火和百度千帆大模型等。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;快速开始&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -d --name=maxkb -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data 1panel/maxkb&#xA;&#xA;# 用户名: admin&#xA;# 密码: MaxKB@123..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;你也可以通过 &lt;a href=&#34;https://apps.fit2cloud.com/1panel&#34;&gt;1Panel 应用商店&lt;/a&gt; 快速部署 MaxKB + Ollama + Llama 2，30 分钟内即可上线基于本地大模型的知识库问答系统，并嵌入到第三方业务系统中。&lt;/p&gt; &#xA;&lt;p&gt;你也可以在线体验：&lt;a href=&#34;https://dataease.io/docs/v2/&#34;&gt;DataEase 小助手&lt;/a&gt;，它是基于 MaxKB 搭建的智能问答系统，已经嵌入到 DataEase 产品及在线文档中。&lt;/p&gt; &#xA;&lt;p&gt;如你有更多问题，可以查看使用手册，或者通过论坛与我们交流。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/1Panel-dev/MaxKB/wiki/1-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2&#34;&gt;使用手册&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1BE421M7YM/&#34;&gt;演示视频&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bbs.fit2cloud.com/c/mk/11&#34;&gt;论坛求助&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;技术交流群&lt;br&gt; &lt;img height=&#34;150px&#34; width=&#34;150px&#34; src=&#34;https://github.com/1Panel-dev/MaxKB/assets/52996290/a4f6303d-9667-4be0-bc2d-0110af782f67&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;UI 展示&lt;/h2&gt; &#xA;&lt;table style=&#34;border-collapse: collapse; border: 1px solid black;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/MaxKB/assets/80892890/2b893a25-ae46-48da-b6a1-61d23015565e&#34; alt=&#34;MaxKB Demo1&#34;&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/MaxKB/assets/80892890/3e50e7ff-cdc4-4a37-b430-d84975f11d4e&#34; alt=&#34;MaxKB Demo2&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/MaxKB/assets/80892890/dfdcc03f-ef36-4f75-bb82-797c0f9da1ad&#34; alt=&#34;MaxKB Demo3&#34;&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/MaxKB/assets/52996290/f8e36cad-b6d5-44bb-a9ab-8fa8e289377a&#34; alt=&#34;MaxKB Demo4&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;技术栈&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;前端：&lt;a href=&#34;https://cn.vuejs.org/&#34;&gt;Vue.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;后端：&lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;Python / Django&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LangChain：&lt;a href=&#34;https://www.langchain.com/&#34;&gt;LangChain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;向量数据库：&lt;a href=&#34;https://www.postgresql.org/&#34;&gt;PostgreSQL / pgvector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;大模型：Azure OpenAI、OpenAI、百度千帆大模型、&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;、通义千问、Kimi、智谱 AI、讯飞星火&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#1Panel-dev/MaxKB&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=1Panel-dev/MaxKB&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;我们的其他开源产品&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jumpserver/jumpserver/&#34;&gt;JumpServer&lt;/a&gt; - 广受欢迎的开源堡垒机&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dataease/dataease/&#34;&gt;DataEase&lt;/a&gt; - 人人可用的开源数据可视化分析工具&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/metersphere/metersphere/&#34;&gt;MeterSphere&lt;/a&gt; - 一站式开源自动化测试平台&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/1panel-dev/1panel/&#34;&gt;1Panel&lt;/a&gt; - 现代化、开源的 Linux 服务器运维管理面板&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/halo-dev/halo/&#34;&gt;Halo&lt;/a&gt; - 强大易用的开源建站工具&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (c) 2014-2024 飞致云 FIT2CLOUD, All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under The GNU General Public License version 3 (GPLv3) (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.html&#34;&gt;https://www.gnu.org/licenses/gpl-3.0.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>myshell-ai/OpenVoice</title>
    <updated>2024-04-28T01:41:21Z</updated>
    <id>tag:github.com,2024-04-28:/myshell-ai/OpenVoice</id>
    <link href="https://github.com/myshell-ai/OpenVoice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Instant voice cloning by MyShell.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/resources/openvoicelogo.jpg&#34; width=&#34;400&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.01479&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://research.myshell.ai/open-voice&#34;&gt;Website&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;h3&gt;OpenVoice V1&lt;/h3&gt; &#xA;&lt;p&gt;As we detailed in our &lt;a href=&#34;https://arxiv.org/abs/2312.01479&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://research.myshell.ai/open-voice&#34;&gt;website&lt;/a&gt;, the advantages of OpenVoice are three-fold:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Accurate Tone Color Cloning.&lt;/strong&gt; OpenVoice can accurately clone the reference tone color and generate speech in multiple languages and accents.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Flexible Voice Style Control.&lt;/strong&gt; OpenVoice enables granular control over voice styles, such as emotion and accent, as well as other style parameters including rhythm, pauses, and intonation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Zero-shot Cross-lingual Voice Cloning.&lt;/strong&gt; Neither of the language of the generated speech nor the language of the reference speech needs to be presented in the massive-speaker multi-lingual training dataset.&lt;/p&gt; &#xA;&lt;h3&gt;OpenVoice V2&lt;/h3&gt; &#xA;&lt;p&gt;In April 2024, we released OpenVoice V2, which includes all features in V1 and has:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Better Audio Quality.&lt;/strong&gt; OpenVoice V2 adopts a different training strategy that delivers better audio quality.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Native Multi-lingual Support.&lt;/strong&gt; English, Spanish, French, Chinese, Japanese and Korean are natively supported in OpenVoice V2.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Free Commercial Use.&lt;/strong&gt; Starting from April 2024, both V2 and V1 are released under MIT License. Free for commercial use.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/myshell-ai/OpenVoice/assets/40556743/3cba936f-82bf-476c-9e52-09f0f417bb2f&#34;&gt;Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenVoice has been powering the instant voice cloning capability of &lt;a href=&#34;https://app.myshell.ai/explore&#34;&gt;myshell.ai&lt;/a&gt; since May 2023. Until Nov 2023, the voice cloning model has been used tens of millions of times by users worldwide, and witnessed the explosive user growth on the platform.&lt;/p&gt; &#xA;&lt;h2&gt;Main Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.qinzy.tech&#34;&gt;Zengyi Qin&lt;/a&gt; at MIT and MyShell&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wl-zhao.github.io&#34;&gt;Wenliang Zhao&lt;/a&gt; at Tsinghua University&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yuxumin.github.io&#34;&gt;Xumin Yu&lt;/a&gt; at Tsinghua University&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/ethan_myshell&#34;&gt;Ethan Sun&lt;/a&gt; at MyShell&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/docs/USAGE.md&#34;&gt;usage&lt;/a&gt; for detailed instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Common Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/docs/QA.md&#34;&gt;QA&lt;/a&gt; for common questions and answers. We will regularly update the question and answer list.&lt;/p&gt; &#xA;&lt;h2&gt;Join Our Community&lt;/h2&gt; &#xA;&lt;p&gt;Join our &lt;a href=&#34;https://discord.gg/myshell&#34;&gt;Discord community&lt;/a&gt; and select the &lt;code&gt;Developer&lt;/code&gt; role upon joining to gain exclusive access to our developer-only channel! Don&#39;t miss out on valuable discussions and collaboration opportunities.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{qin2023openvoice,&#xA;  title={OpenVoice: Versatile Instant Voice Cloning},&#xA;  author={Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},&#xA;  journal={arXiv preprint arXiv:2312.01479},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;OpenVoice V1 and V2 are MIT Licensed. Free for both commercial and research use.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This implementation is based on several excellent projects, &lt;a href=&#34;https://github.com/coqui-ai/TTS&#34;&gt;TTS&lt;/a&gt;, &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;VITS&lt;/a&gt;, and &lt;a href=&#34;https://github.com/daniilrobnikov/vits2&#34;&gt;VITS2&lt;/a&gt;. Thanks for their awesome work!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mnotgod96/AppAgent</title>
    <updated>2024-04-28T01:41:21Z</updated>
    <id>tag:github.com,2024-04-28:/mnotgod96/AppAgent</id>
    <link href="https://github.com/mnotgod96/AppAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AppAgent: Multimodal Agents as Smartphone Users, an LLM-based multimodal agent framework designed to operate smartphone apps.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AppAgent-&lt;a href=&#34;https://github.com/TencentQQGYLab&#34;&gt;TencentQQGYLab&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.13771&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2312.13771-b31b1b.svg?sanitize=true&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://appagent-official.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://github.com/buaacyw/GaussianEditor/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://twitter.com/dr_chizhang&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/dr_chizhang?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA; &lt;!-- [![Model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue)](https://huggingface.co/listen2you002/ChartLlama-13b) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/listen2you002/ChartLlama-Dataset) --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://icoz69.github.io/&#34;&gt;&lt;strong&gt;Chi Zhang&lt;/strong&gt;*†&lt;/a&gt;, &lt;a href=&#34;https://github.com/yz93&#34;&gt;&lt;strong&gt;Zhao Yang&lt;/strong&gt;*&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jiaxuan-liu-9051b7105/&#34;&gt;&lt;strong&gt;Jiaxuan Liu&lt;/strong&gt;*&lt;/a&gt;, &lt;a href=&#34;http://tingxueronghua.github.io&#34;&gt;Yucheng Han&lt;/a&gt;, &lt;a href=&#34;https://chenxin.tech/&#34;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Zebiao Huang&lt;/a&gt;, &lt;br&gt; &lt;a href=&#34;https://openreview.net/profile?id=~BIN_FU2&#34;&gt;Bin Fu&lt;/a&gt;, &lt;a href=&#34;https://www.skicyyu.org/&#34;&gt;Gang Yu✦&lt;/a&gt; &lt;br&gt; (* equal contribution, † Project Leader, ✦ Corresponding Author )&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mnotgod96/AppAgent/main/assets/teaser.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ℹ️Should you encounter any issues⚠️ while using our project, please feel free to report them on &lt;a href=&#34;https://github.com/mnotgod96/AppAgent/issues&#34;&gt;GitHub Issues&lt;/a&gt; or reach out to &lt;a href=&#34;https://icoz69.github.io/&#34;&gt;Dr. Chi Zhang&lt;/a&gt; via email at &lt;a href=&#34;mailto:dr.zhang.chi@outlook.com&#34;&gt;dr.zhang.chi@outlook.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ℹ️This project will be synchronously updated on the official &lt;a href=&#34;https://github.com/TencentQQGYLab/AppAgent&#34;&gt;TencentQQGYLab&lt;/a&gt; Github Page.&lt;/p&gt; &#xA;&lt;h2&gt;📝 Changelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.2.8]&lt;/strong&gt;: Added &lt;code&gt;qwen-vl-max&lt;/code&gt; (通义千问-VL) as an alternative multi-modal model. The model is currently free to use but has a relatively poorer performance compared with GPT-4V.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.1.31]&lt;/strong&gt;: Released the &lt;a href=&#34;https://github.com/mnotgod96/AppAgent/raw/main/assets/testset.md&#34;&gt;evaluation benchmark&lt;/a&gt; used during our testing of AppAgent&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.1.2]&lt;/strong&gt;: 🔥Added an optional method for the agent to bring up a grid overlay on the screen to &lt;strong&gt;tap/swipe anywhere&lt;/strong&gt; on the screen.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12.26]&lt;/strong&gt;: Added &lt;a href=&#34;https://raw.githubusercontent.com/mnotgod96/AppAgent/main/#tips&#34;&gt;Tips&lt;/a&gt; section for better use experience; added instruction for using the &lt;strong&gt;Android Studio emulator&lt;/strong&gt; for users who do not have Android devices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12.21]&lt;/strong&gt;: 🔥🔥 Open-sourced the git repository, including the detailed configuration steps to implement our AppAgent!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔆 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We introduce a novel LLM-based multimodal agent framework designed to operate smartphone applications.&lt;/p&gt; &#xA;&lt;p&gt;Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps.&lt;/p&gt; &#xA;&lt;p&gt;Central to our agent&#39;s functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications.&lt;/p&gt; &#xA;&lt;h2&gt;✨ Demo&lt;/h2&gt; &#xA;&lt;p&gt;The demo video shows the process of using AppAgent to follow a user on X (Twitter) in the deployment phase.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mnotgod96/AppAgent/assets/40715314/db99d650-dec1-4531-b4b2-e085bfcadfb7&#34;&gt;https://github.com/mnotgod96/AppAgent/assets/40715314/db99d650-dec1-4531-b4b2-e085bfcadfb7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An interesting experiment showing AppAgent&#39;s ability to pass CAPTCHA.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mnotgod96/AppAgent/assets/27103154/5cc7ba50-dbab-42a0-a411-a9a862482548&#34;&gt;https://github.com/mnotgod96/AppAgent/assets/27103154/5cc7ba50-dbab-42a0-a411-a9a862482548&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An example of using the grid overlay to locate a UI element that is not labeled with a numeric tag.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mnotgod96/AppAgent/assets/27103154/71603333-274c-46ed-8381-2f9a34cdfc53&#34;&gt;https://github.com/mnotgod96/AppAgent/assets/27103154/71603333-274c-46ed-8381-2f9a34cdfc53&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;This section will guide you on how to quickly use &lt;code&gt;gpt-4-vision-preview&lt;/code&gt; (or &lt;code&gt;qwen-vl-max&lt;/code&gt;) as an agent to complete specific tasks for you on your Android app.&lt;/p&gt; &#xA;&lt;h3&gt;⚙️ Step 1. Prerequisites&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;On your PC, download and install &lt;a href=&#34;https://developer.android.com/tools/adb&#34;&gt;Android Debug Bridge&lt;/a&gt; (adb) which is a command-line tool that lets you communicate with your Android device from the PC.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get an Android device and enable the USB debugging that can be found in Developer Options in Settings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Connect your device to your PC using a USB cable.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) If you do not have an Android device but still want to try AppAgent. We recommend you download &lt;a href=&#34;https://developer.android.com/studio/run/emulator&#34;&gt;Android Studio&lt;/a&gt; and use the emulator that comes with it. The emulator can be found in the device manager of Android Studio. You can install apps on an emulator by downloading APK files from the internet and dragging them to the emulator. AppAgent can detect the emulated device and operate apps on it just like operating a real device.&lt;/p&gt; &lt;img width=&#34;570&#34; alt=&#34;Screenshot 2023-12-26 at 22 25 42&#34; src=&#34;https://github.com/mnotgod96/AppAgent/assets/27103154/5d76b810-1f42-44c8-b024-d63ec7776789&#34;&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this repo and install the dependencies. All scripts in this project are written in Python 3 so make sure you have installed it.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd AppAgent&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;🤖 Step 2. Configure the Agent&lt;/h3&gt; &#xA;&lt;p&gt;AppAgent needs to be powered by a multi-modal model which can receive both text and visual inputs. During our experiment , we used &lt;code&gt;gpt-4-vision-preview&lt;/code&gt; as the model to make decisions on how to take actions to complete a task on the smartphone.&lt;/p&gt; &#xA;&lt;p&gt;To configure your requests to GPT-4V, you should modify &lt;code&gt;config.yaml&lt;/code&gt; in the root directory. There are two key parameters that must be configured to try AppAgent:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;OpenAI API key: you must purchase an eligible API key from OpenAI so that you can have access to GPT-4V.&lt;/li&gt; &#xA; &lt;li&gt;Request interval: this is the time interval in seconds between consecutive GPT-4V requests to control the frequency of your requests to GPT-4V. Adjust this value according to the status of your account.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Other parameters in &lt;code&gt;config.yaml&lt;/code&gt; are well commented. Modify them as you need.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Be aware that GPT-4V is not free. Each request/response pair involved in this project costs around $0.03. Use it wisely.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can also try &lt;code&gt;qwen-vl-max&lt;/code&gt; (通义千问-VL) as the alternative multi-modal model to power the AppAgent. The model is currently free to use but its performance in the context of AppAgent is poorer compared with GPT-4V.&lt;/p&gt; &#xA;&lt;p&gt;To use it, you should create an Alibaba Cloud account and &lt;a href=&#34;https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.i1&#34;&gt;create a Dashscope API key&lt;/a&gt; to fill in the &lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt; field in the &lt;code&gt;config.yaml&lt;/code&gt; file. Change the &lt;code&gt;MODEL&lt;/code&gt; field from &lt;code&gt;OpenAI&lt;/code&gt; to &lt;code&gt;Qwen&lt;/code&gt; as well.&lt;/p&gt; &#xA;&lt;p&gt;If you want to test AppAgent using your own models, you should write a new model class in &lt;code&gt;scripts/model.py&lt;/code&gt; accordingly.&lt;/p&gt; &#xA;&lt;h3&gt;🔍 Step 3. Exploration Phase&lt;/h3&gt; &#xA;&lt;p&gt;Our paper proposed a novel solution that involves two phases, exploration, and deployment, to turn GPT-4V into a capable agent that can help users operate their Android phones when a task is given. The exploration phase starts with a task given by you, and you can choose to let the agent either explore the app on its own or learn from your demonstration. In both cases, the agent generates documentation for elements interacted during the exploration/demonstration and saves them for use in the deployment phase.&lt;/p&gt; &#xA;&lt;h4&gt;Option 1: Autonomous Exploration&lt;/h4&gt; &#xA;&lt;p&gt;This solution features a fully autonomous exploration which allows the agent to explore the use of the app by attempting the given task without any intervention from humans.&lt;/p&gt; &#xA;&lt;p&gt;To start, run &lt;code&gt;learn.py&lt;/code&gt; in the root directory. Follow the prompted instructions to select &lt;code&gt;autonomous exploration&lt;/code&gt; as the operating mode and provide the app name and task description. Then, your agent will do the job for you. Under this mode, AppAgent will reflect on its previous action making sure its action adheres to the given task and generate documentation for the elements explored.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python learn.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Option 2: Learning from Human Demonstrations&lt;/h4&gt; &#xA;&lt;p&gt;This solution requires users to demonstrate a similar task first. AppAgent will learn from the demo and generate documentations for UI elements seen during the demo.&lt;/p&gt; &#xA;&lt;p&gt;To start human demonstration, you should run &lt;code&gt;learn.py&lt;/code&gt; in the root directory. Follow the prompted instructions to select &lt;code&gt;human demonstration&lt;/code&gt; as the operating mode and provide the app name and task description. A screenshot of your phone will be captured and all interactive elements shown on the screen will be labeled with numeric tags. You need to follow the prompts to determine your next action and the target of the action. When you believe the demonstration is finished, type &lt;code&gt;stop&lt;/code&gt; to end the demo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python learn.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mnotgod96/AppAgent/main/assets/demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;📱 Step 4. Deployment Phase&lt;/h3&gt; &#xA;&lt;p&gt;After the exploration phase finishes, you can run &lt;code&gt;run.py&lt;/code&gt; in the root directory. Follow the prompted instructions to enter the name of the app, select the appropriate documentation base you want the agent to use and provide the task description. Then, your agent will do the job for you. The agent will automatically detect if there is documentation base generated before for the app; if there is no documentation found, you can also choose to run the agent without any documentation (success rate not guaranteed).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;💡 Tips&lt;a name=&#34;tips&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For an improved experience, you might permit AppAgent to undertake a broader range of tasks through autonomous exploration, or you can directly demonstrate more app functions to enhance the app documentation. Generally, the more extensive the documentation provided to the agent, the higher the likelihood of successful task completion.&lt;/li&gt; &#xA; &lt;li&gt;It is always a good practice to inspect the documentation generated by the agent. When you find some documentation not accurately describe the function of the element, manually revising the documentation is also an option.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📊 Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/mnotgod96/AppAgent/raw/main/assets/testset.md&#34;&gt;evaluation benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;📖 To-Do List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporate more LLM APIs into the project.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Open source the Benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Open source the configuration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;😉 Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@misc{yang2023appagent,&#xA;      title={AppAgent: Multimodal Agents as Smartphone Users}, &#xA;      author={Chi Zhang and Zhao Yang and Jiaxuan Liu and Yucheng Han and Xin Chen and Zebiao Huang and Bin Fu and Gang Yu},&#xA;      year={2023},&#xA;      eprint={2312.13771},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#mnotgod96/AppAgent&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=mnotgod96/AppAgent&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/mnotgod96/AppAgent/main/assets/license.txt&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>