<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-25T02:03:52Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hiyouga/LLaMA-Efficient-Tuning</title>
    <updated>2023-06-25T02:03:52Z</updated>
    <id>tag:github.com,2023-06-25:/hiyouga/LLaMA-Efficient-Tuning</id>
    <link href="https://github.com/hiyouga/LLaMA-Efficient-Tuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fine-tuning LLaMA with PEFT (PT+SFT+RLHF with QLoRA)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaMA Efficient Tuning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hiyouga/LLaMA-Efficient-Tuning?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/hiyouga/LLaMA-Efficient-Tuning&#34; alt=&#34;GitHub Code License&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/hiyouga/LLaMA-Efficient-Tuning&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue&#34; alt=&#34;GitHub pull request&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;👋 Join our &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Efficient-Tuning/main/assets/wechat.jpg&#34;&gt;WeChat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[23/06/22] Now we align the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Efficient-Tuning/main/src/api_demo.py&#34;&gt;demo API&lt;/a&gt; with the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI&#39;s&lt;/a&gt; format where you can insert the fine-tuned model in arbitrary ChatGPT-based applications.&lt;/p&gt; &#xA;&lt;p&gt;[23/06/15] Now we support training the baichuan-7B model in this repo. Try &lt;code&gt;--model_name_or_path baichuan-inc/baichuan-7B&lt;/code&gt; and &lt;code&gt;--lora_target W_pack&lt;/code&gt; arguments to use the baichuan-7B model.&lt;/p&gt; &#xA;&lt;p&gt;[23/06/03] Now we support quantized training and inference (aka &lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt;). Try &lt;code&gt;--quantization_bit 4/8&lt;/code&gt; argument to work with quantized model. (experimental feature)&lt;/p&gt; &#xA;&lt;p&gt;[23/05/31] Now we support training the BLOOM &amp;amp; BLOOMZ models in this repo. Try &lt;code&gt;--model_name_or_path bigscience/bloomz-7b1-mt&lt;/code&gt; and &lt;code&gt;--lora_target query_key_value&lt;/code&gt; arguments to use the BLOOMZ model.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; (7B/13B/33B/65B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;BLOOM&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/bigscience/bloomz&#34;&gt;BLOOMZ&lt;/a&gt; (560M/1.1B/1.7B/3B/7.1B/176B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B&#34;&gt;baichuan&lt;/a&gt; (7B)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;(Continually) pre-training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Full-parameter tuning&lt;/li&gt; &#xA;   &lt;li&gt;Partial-parameter tuning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;QLoRA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.01652&#34;&gt;Supervised fine-tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Full-parameter tuning&lt;/li&gt; &#xA;   &lt;li&gt;Partial-parameter tuning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;QLoRA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;RLHF&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;QLoRA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Provided Datasets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For pre-training: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Efficient-Tuning/main/data/wiki_demo.txt&#34;&gt;Wiki Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;For supervised fine-tuning: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Stanford Alpaca (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_2M_CN&#34;&gt;BELLE 2M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BELLE 1M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BELLE 0.5M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&#34;&gt;BELLE Dialogue 0.4M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&#34;&gt;BELLE School Math 0.25M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BELLE Multiturn Chat 0.8M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;Guanaco Dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;Firefly 1.1M&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&#34;&gt;CodeAlpaca 20k&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&#34;&gt;Alpaca CoT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/suolyer/webqa&#34;&gt;Web QA (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;UltraChat&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;For reward model training: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf&#34;&gt;HH-RLHF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Efficient-Tuning/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your HuggingFace account using these commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade huggingface_hub&#xA;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+ and PyTorch 1.13.1+&lt;/li&gt; &#xA; &lt;li&gt;🤗Transformers, Datasets, Accelerate, PEFT and TRL&lt;/li&gt; &#xA; &lt;li&gt;jieba, rouge_chinese and nltk (used at evaluation)&lt;/li&gt; &#xA; &lt;li&gt;gradio and mdtex2html (used in web_demo.py)&lt;/li&gt; &#xA; &lt;li&gt;uvicorn and fastapi (used in api_demo.py)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And &lt;strong&gt;powerful GPUs&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Data Preparation (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;data/example_dataset&lt;/code&gt; for checking the details about the format of dataset files. You can either use a single &lt;code&gt;.json&lt;/code&gt; file or a &lt;a href=&#34;https://huggingface.co/docs/datasets/dataset_script&#34;&gt;dataset loading script&lt;/a&gt; with multiple files to create a custom dataset.&lt;/p&gt; &#xA;&lt;p&gt;Note: please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset. About the format of this file, please refer to &lt;code&gt;data/README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Dependence Installation (optional)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hiyouga/LLaMA-Efficient-Tuning.git&#xA;conda create -n llama_etuning python=3.10&#xA;conda activate llama_etuning&#xA;cd LLaMA-Efficient-Tuning&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLaMA Weights Preparation (optional)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the weights of the LLaMA models.&lt;/li&gt; &#xA; &lt;li&gt;Convert them to HF format using the following command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m transformers.models.llama.convert_llama_weights_to_hf \&#xA;    --input_dir path_to_llama_weights --model_size 7B --output_dir path_to_llama_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(Continually) Pre-Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_pt.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_train \&#xA;    --dataset wiki_demo \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_pt_checkpoint \&#xA;    --overwrite_cache \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 5e-5 \&#xA;    --num_train_epochs 3.0 \&#xA;    --plot_loss \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Supervised Fine-Tuning&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_sft_checkpoint \&#xA;    --overwrite_cache \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 5e-5 \&#xA;    --num_train_epochs 3.0 \&#xA;    --plot_loss \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reward Model Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_rm.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_train \&#xA;    --dataset comparison_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --output_dir path_to_rm_checkpoint \&#xA;    --per_device_train_batch_size 4 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --plot_loss \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;PPO Training (RLHF)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_ppo.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_train \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --finetuning_type lora \&#xA;    --checkpoint_dir path_to_sft_checkpoint \&#xA;    --reward_model path_to_rm_checkpoint \&#xA;    --output_dir path_to_ppo_checkpoint \&#xA;    --per_device_train_batch_size 2 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --lr_scheduler_type cosine \&#xA;    --logging_steps 10 \&#xA;    --save_steps 1000 \&#xA;    --learning_rate 1e-5 \&#xA;    --num_train_epochs 1.0 \&#xA;    --resume_lora_training False \&#xA;    --plot_loss&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Distributed Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate config # configure the environment&#xA;accelerate launch src/train_XX.py # arguments (same as above)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation (BLEU and ROUGE_CHINESE)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --do_eval \&#xA;    --dataset alpaca_gpt4_en \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_eval_result \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --max_samples 50 \&#xA;    --predict_with_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend using &lt;code&gt;--per_device_eval_batch_size=1&lt;/code&gt; and &lt;code&gt;--max_target_length 128&lt;/code&gt; at 4/8-bit evaluation.&lt;/p&gt; &#xA;&lt;h3&gt;API / CLI / Web Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/xxx_demo.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --checkpoint_dir path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Export model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/export_model.py \&#xA;    --model_name_or_path path_to_your_model \&#xA;    --checkpoint_dir path_to_checkpoint \&#xA;    --output_dir path_to_export&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Efficient-Tuning/main/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;Model Card&lt;/a&gt; to use the LLaMA models.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the &lt;a href=&#34;https://huggingface.co/spaces/bigscience/license&#34;&gt;RAIL License&lt;/a&gt; to use the BLOOM &amp;amp; BLOOMZ models.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the &lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&#34;&gt;baichuan-7B License&lt;/a&gt; to use the baichuan-7B model.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Misc{llama-efficient-tuning,&#xA;  title = {LLaMA Efficient Tuning},&#xA;  author = {hiyouga},&#xA;  howpublished = {\url{https://github.com/hiyouga/LLaMA-Efficient-Tuning}},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo is a sibling of &lt;a href=&#34;https://github.com/hiyouga/ChatGLM-Efficient-Tuning&#34;&gt;ChatGLM-Efficient-Tuning&lt;/a&gt;. They share a similar code structure of efficient tuning on large language models.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AntonOsika/gpt-engineer</title>
    <updated>2023-06-25T02:03:52Z</updated>
    <id>tag:github.com,2023-06-25:/AntonOsika/gpt-engineer</id>
    <link href="https://github.com/AntonOsika/gpt-engineer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Specify what you want it to build, the AI asks for clarification, and then builds it.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPT Engineer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/4t5vXHhu&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/4t5vXHhu?style=flat&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AntonOsika/gpt-engineer?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/AntonOsika&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/antonosika?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Specify what you want it to build, the AI asks for clarification, and then builds it.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;GPT Engineer is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/antonosika/status/1667641038104674306&#34;&gt;Demo&lt;/a&gt; 👶🤖&lt;/p&gt; &#xA;&lt;h2&gt;Project philosophy&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple to get value&lt;/li&gt; &#xA; &lt;li&gt;Flexible and easy to add new own &#34;AI steps&#34;. See &lt;code&gt;steps.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Incrementally build towards a user experience of: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;high level prompting&lt;/li&gt; &#xA;   &lt;li&gt;giving feedback to the AI that it will remember over time&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fast handovers back and forth between AI and human&lt;/li&gt; &#xA; &lt;li&gt;Simplicity, all computation is &#34;resumable&#34; and persisted to the filesystem&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Choose either &lt;strong&gt;stable&lt;/strong&gt; or &lt;strong&gt;development&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;strong&gt;stable&lt;/strong&gt; release:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install gpt-engineer&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For &lt;strong&gt;development&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone git@github.com:AntonOsika/gpt-engineer.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd gpt-engineer&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install -e .&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(or: &lt;code&gt;make install &amp;amp;&amp;amp; source venv/bin/activate&lt;/code&gt; for a venv)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;With an api key that has GPT4 access run:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;export OPENAI_API_KEY=[your api key]&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create an empty folder. If inside the repo, you can run: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;cp -r projects/example/ projects/my-new-project&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fill in the &lt;code&gt;prompt&lt;/code&gt; file in your new folder&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt-engineer projects/my-new-project&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(Note, &lt;code&gt;gpt-engineer --help&lt;/code&gt; lets you see all available options. For example &lt;code&gt;--steps use_feedback&lt;/code&gt; lets you improve/fix code in a project)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By running gpt-engineer you agree to our &lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer/TERMS_OF_USE.md&#34;&gt;ToS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check the generated files in &lt;code&gt;projects/my-new-project/workspace&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;You can specify the &#34;identity&#34; of the AI agent by editing the files in the &lt;code&gt;preprompts&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Editing the &lt;code&gt;preprompts&lt;/code&gt;, and evolving how you write the project prompt, is currently how you make the agent remember things between projects.&lt;/p&gt; &#xA;&lt;p&gt;Each step in &lt;code&gt;steps.py&lt;/code&gt; will have its communication history with GPT4 stored in the logs folder, and can be rerun with &lt;code&gt;scripts/rerun_edited_message_logs.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;The gpt-engineer community is building the &lt;strong&gt;open platform for devs to tinker with and build their personal code-generation toolbox&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in contributing to this, we would be interested in having you!&lt;/p&gt; &#xA;&lt;p&gt;You can check for good first issues &lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer/issues&#34;&gt;here&lt;/a&gt;. Contributing document &lt;a href=&#34;https://raw.githubusercontent.com/AntonOsika/gpt-engineer/main/.github/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We are currently looking for more maintainers and community organisers. Email &lt;a href=&#34;mailto:anton.osika@gmail.com&#34;&gt;anton.osika@gmail.com&lt;/a&gt; if you are interested in an official role.&lt;/p&gt; &#xA;&lt;p&gt;If you want to see our broader ambitions, check out the &lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer/raw/main/ROADMAP.md&#34;&gt;roadmap&lt;/a&gt;, and join &lt;a href=&#34;https://discord.gg/4t5vXHhu&#34;&gt;discord &lt;/a&gt; to get input on how you can contribute to it.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer/assets/4467025/6e362e45-4a94-4b0d-973d-393a31d92d9b&#34;&gt;https://github.com/AntonOsika/gpt-engineer/assets/4467025/6e362e45-4a94-4b0d-973d-393a31d92d9b&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bentoml/OpenLLM</title>
    <updated>2023-06-25T02:03:52Z</updated>
    <id>tag:github.com,2023-06-25:/bentoml/OpenLLM</id>
    <link href="https://github.com/bentoml/OpenLLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bentoml/OpenLLM/main/assets/main-banner.png&#34; alt=&#34;Banner for OpenLLM&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;🦾 OpenLLM&lt;/h1&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/openllm&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/openllm.svg?sanitize=true&#34; alt=&#34;pypi_status&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/bentoml/OpenLLM/actions/workflows/ci.yml&#34;&gt; &lt;img src=&#34;https://github.com/bentoml/OpenLLM/actions/workflows/ci.yml/badge.svg?branch=main&#34; alt=&#34;ci&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://twitter.com/bentomlai&#34;&gt; &lt;img src=&#34;https://badgen.net/badge/icon/@bentomlai/1DA1F2?icon=twitter&amp;amp;label=Follow%20Us&#34; alt=&#34;Twitter&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://l.bentoml.com/join-openllm-discord&#34;&gt; &lt;img src=&#34;https://badgen.net/badge/icon/OpenLLM/7289da?icon=discord&amp;amp;label=Join%20Us&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt;&#xA; &lt;br&gt; &#xA; &lt;p&gt;An open platform for operating large language models (LLMs) in production.&lt;br&gt; Fine-tune, serve, deploy, and monitor any LLMs with ease.&lt;/p&gt; &#xA; &lt;i&gt;&lt;/i&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;📖 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;With OpenLLM, you can run inference with any open-source large-language models, deploy to the cloud or on-premises, and build powerful AI apps.&lt;/p&gt; &#xA;&lt;p&gt;🚂 &lt;strong&gt;State-of-the-art LLMs&lt;/strong&gt;: built-in supports a wide range of open-source LLMs and model runtime, including StableLM, Falcon, Dolly, Flan-T5, ChatGLM, StarCoder and more.&lt;/p&gt; &#xA;&lt;p&gt;🔥 &lt;strong&gt;Flexible APIs&lt;/strong&gt;: serve LLMs over RESTful API or gRPC with one command, query via WebUI, CLI, our Python/Javascript client, or any HTTP client.&lt;/p&gt; &#xA;&lt;p&gt;⛓️ &lt;strong&gt;Freedom To Build&lt;/strong&gt;: First-class support for LangChain, BentoML and Hugging Face that allows you to easily create your own AI apps by composing LLMs with other models and services.&lt;/p&gt; &#xA;&lt;p&gt;🎯 &lt;strong&gt;Streamline Deployment&lt;/strong&gt;: Automatically generate your LLM server Docker Images or deploy as serverless endpoint via &lt;a href=&#34;https://l.bentoml.com/bento-cloud&#34;&gt;☁️ BentoCloud&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;🤖️ &lt;strong&gt;Bring your own LLM&lt;/strong&gt;: Fine-tune any LLM to suit your needs with &lt;code&gt;LLM.tuning()&lt;/code&gt;. (Coming soon)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bentoml/OpenLLM/main/assets/output.gif&#34; alt=&#34;Gif showing OpenLLM Intro&#34;&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🏃‍ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To use OpenLLM, you need to have Python 3.8 (or newer) and &lt;code&gt;pip&lt;/code&gt; installed on your system. We highly recommend using a Virtual Environment to prevent package conflicts.&lt;/p&gt; &#xA;&lt;p&gt;You can install OpenLLM using pip as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To verify if it&#39;s installed correctly, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ openllm -h&#xA;&#xA;Usage: openllm [OPTIONS] COMMAND [ARGS]...&#xA;&#xA;   ██████╗ ██████╗ ███████╗███╗   ██╗██╗     ██╗     ███╗   ███╗&#xA;  ██╔═══██╗██╔══██╗██╔════╝████╗  ██║██║     ██║     ████╗ ████║&#xA;  ██║   ██║██████╔╝█████╗  ██╔██╗ ██║██║     ██║     ██╔████╔██║&#xA;  ██║   ██║██╔═══╝ ██╔══╝  ██║╚██╗██║██║     ██║     ██║╚██╔╝██║&#xA;  ╚██████╔╝██║     ███████╗██║ ╚████║███████╗███████╗██║ ╚═╝ ██║&#xA;   ╚═════╝ ╚═╝     ╚══════╝╚═╝  ╚═══╝╚══════╝╚══════╝╚═╝     ╚═╝&#xA;&#xA;  An open platform for operating large language models in production.&#xA;  Fine-tune, serve, deploy, and monitor any LLMs with ease.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Starting an LLM Server&lt;/h3&gt; &#xA;&lt;p&gt;To start an LLM server, use &lt;code&gt;openllm start&lt;/code&gt;. For example, to start a &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/opt&#34;&gt;&lt;code&gt;OPT&lt;/code&gt;&lt;/a&gt; server, do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openllm start opt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following this, a Web UI will be accessible at &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; where you can experiment with the endpoints and sample input prompts.&lt;/p&gt; &#xA;&lt;p&gt;OpenLLM provides a built-in Python client, allowing you to interact with the model. In a different terminal window or a Jupyter Notebook, create a client to start interacting with the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import openllm&#xA;&amp;gt;&amp;gt;&amp;gt; client = openllm.client.HTTPClient(&#39;http://localhost:3000&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; client.query(&#39;Explain to me the difference between &#34;further&#34; and &#34;farther&#34;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use the &lt;code&gt;openllm query&lt;/code&gt; command to query the model from the terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENLLM_ENDPOINT=http://localhost:3000&#xA;openllm query &#39;Explain to me the difference between &#34;further&#34; and &#34;farther&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visit &lt;code&gt;http://localhost:3000/docs.json&lt;/code&gt; for OpenLLM&#39;s API specification.&lt;/p&gt; &#xA;&lt;p&gt;Users can also specify different variants of the model to be served, by providing the &lt;code&gt;--model-id&lt;/code&gt; argument, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openllm start flan-t5 --model-id google/flan-t5-large&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;openllm models&lt;/code&gt; command to see the list of models and their variants supported in OpenLLM.&lt;/p&gt; &#xA;&lt;h2&gt;🧩 Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;The following models are currently supported in OpenLLM. By default, OpenLLM doesn&#39;t include dependencies to run all models. The extra model-specific dependencies can be installed with the instructions below:&lt;/p&gt; &#xA;&lt;!-- update-readme.py: start --&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Installation&lt;/th&gt; &#xA;   &lt;th&gt;Model Ids&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/flan-t5&#34;&gt;flan-t5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;openllm[flan-t5]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-small&#34;&gt;&lt;code&gt;google/flan-t5-small&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-base&#34;&gt;&lt;code&gt;google/flan-t5-base&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-large&#34;&gt;&lt;code&gt;google/flan-t5-large&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-xl&#34;&gt;&lt;code&gt;google/flan-t5-xl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-xxl&#34;&gt;&lt;code&gt;google/flan-t5-xxl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/databrickslabs/dolly&#34;&gt;dolly-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openllm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-3b&#34;&gt;&lt;code&gt;databricks/dolly-v2-3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-7b&#34;&gt;&lt;code&gt;databricks/dolly-v2-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;&lt;code&gt;databricks/dolly-v2-12b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;chatglm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;openllm[chatglm]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/thudm/chatglm-6b&#34;&gt;&lt;code&gt;thudm/chatglm-6b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/thudm/chatglm-6b-int8&#34;&gt;&lt;code&gt;thudm/chatglm-6b-int8&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/thudm/chatglm-6b-int4&#34;&gt;&lt;code&gt;thudm/chatglm-6b-int4&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bigcode-project/starcoder&#34;&gt;starcoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;openllm[starcoder]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoder&#34;&gt;&lt;code&gt;bigcode/starcoder&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoderbase&#34;&gt;&lt;code&gt;bigcode/starcoderbase&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://falconllm.tii.ae/&#34;&gt;falcon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;openllm[falcon]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-7b&#34;&gt;&lt;code&gt;tiiuae/falcon-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b&#34;&gt;&lt;code&gt;tiiuae/falcon-40b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-7b-instruct&#34;&gt;&lt;code&gt;tiiuae/falcon-7b-instruct&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b-instruct&#34;&gt;&lt;code&gt;tiiuae/falcon-40b-instruct&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Stability-AI/StableLM&#34;&gt;stablelm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openllm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b&#34;&gt;&lt;code&gt;stabilityai/stablelm-tuned-alpha-3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b&#34;&gt;&lt;code&gt;stabilityai/stablelm-tuned-alpha-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-base-alpha-3b&#34;&gt;&lt;code&gt;stabilityai/stablelm-base-alpha-3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-base-alpha-7b&#34;&gt;&lt;code&gt;stabilityai/stablelm-base-alpha-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/opt&#34;&gt;opt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openllm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-125m&#34;&gt;&lt;code&gt;facebook/opt-125m&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-350m&#34;&gt;&lt;code&gt;facebook/opt-350m&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-1.3b&#34;&gt;&lt;code&gt;facebook/opt-1.3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-2.7b&#34;&gt;&lt;code&gt;facebook/opt-2.7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-6.7b&#34;&gt;&lt;code&gt;facebook/opt-6.7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-66b&#34;&gt;&lt;code&gt;facebook/opt-66b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;!-- update-readme.py: stop --&gt; &#xA;&lt;h3&gt;Runtime Implementations (Experimental)&lt;/h3&gt; &#xA;&lt;p&gt;Different LLMs may have multiple runtime implementations. For instance, they might use Pytorch (&lt;code&gt;pt&lt;/code&gt;), Tensorflow (&lt;code&gt;tf&lt;/code&gt;), or Flax (&lt;code&gt;flax&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;If you wish to specify a particular runtime for a model, you can do so by setting the &lt;code&gt;OPENLLM_{MODEL_NAME}_FRAMEWORK={runtime}&lt;/code&gt; environment variable before running &lt;code&gt;openllm start&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want to use the Tensorflow (&lt;code&gt;tf&lt;/code&gt;) implementation for the &lt;code&gt;flan-t5&lt;/code&gt; model, you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENLLM_FLAN_T5_FRAMEWORK=tf openllm start flan-t5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; For GPU support on Flax, refers to &lt;a href=&#34;https://github.com/google/jax#pip-installation-gpu-cuda-installed-via-pip-easier&#34;&gt;Jax&#39;s installation&lt;/a&gt; to make sure that you have Jax support for the corresponding CUDA version.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Integrating a New Model&lt;/h3&gt; &#xA;&lt;p&gt;OpenLLM encourages contributions by welcoming users to incorporate their custom LLMs into the ecosystem. Check out &lt;a href=&#34;https://github.com/bentoml/OpenLLM/raw/main/ADDING_NEW_MODEL.md&#34;&gt;Adding a New Model Guide&lt;/a&gt; to see how you can do it yourself.&lt;/p&gt; &#xA;&lt;h2&gt;⚙️ Integrations&lt;/h2&gt; &#xA;&lt;p&gt;OpenLLM is not just a standalone product; it&#39;s a building block designed to integrate with other powerful tools easily. We currently offer integration with &lt;a href=&#34;https://github.com/bentoml/BentoML&#34;&gt;BentoML&lt;/a&gt; and &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;BentoML&lt;/h3&gt; &#xA;&lt;p&gt;OpenLLM models can be integrated as a &lt;a href=&#34;https://docs.bentoml.org/en/latest/concepts/runner.html&#34;&gt;Runner&lt;/a&gt; in your BentoML service. These runners have a &lt;code&gt;generate&lt;/code&gt; method that takes a string as a prompt and returns a corresponding output string. This will allow you to plug and play any OpenLLM models with your existing ML workflow.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import bentoml&#xA;import openllm&#xA;&#xA;model = &#34;opt&#34;&#xA;&#xA;llm_config = openllm.AutoConfig.for_model(model)&#xA;llm_runner = openllm.Runner(model, llm_config=llm_config)&#xA;&#xA;svc = bentoml.Service(&#xA;    name=f&#34;llm-opt-service&#34;, runners=[llm_runner]&#xA;)&#xA;&#xA;@svc.api(input=Text(), output=Text())&#xA;async def prompt(input_text: str) -&amp;gt; str:&#xA;    answer = await llm_runner.generate(input_text)&#xA;    return answer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Hugging Face Agents&lt;/h3&gt; &#xA;&lt;p&gt;OpenLLM seamlessly integrates with Hugging Face Agents.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; The HuggingFace Agent is still at experimental stage. It is recommended to OpenLLM with &lt;code&gt;pip install -r nightly-requirements.txt&lt;/code&gt; to get the latest API update for HuggingFace agent.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import transformers&#xA;&#xA;agent = transformers.HfAgent(&#34;http://localhost:3000/hf/agent&#34;)  # URL that runs the OpenLLM server&#xA;&#xA;agent.run(&#34;Is the following `text` positive or negative?&#34;, text=&#34;I don&#39;t like how this models is generate inputs&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; Only &lt;code&gt;starcoder&lt;/code&gt; is currently supported with Agent integration. The example above was also ran with four T4s on EC2 &lt;code&gt;g4dn.12xlarge&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you want to use OpenLLM client to ask questions to the running agent, you can also do so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openllm&#xA;&#xA;client = openllm.client.HTTPClient(&#34;http://localhost:3000&#34;)&#xA;&#xA;client.ask_agent(&#xA;    task=&#34;Is the following `text` positive or negative?&#34;,&#xA;    text=&#34;What are you thinking about?&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://python.langchain.com/docs/ecosystem/integrations/openllm&#34;&gt;LangChain&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;To quickly start a local LLM with &lt;code&gt;langchain&lt;/code&gt;, simply do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain.llms import OpenLLM&#xA;&#xA;llm = OpenLLM(model_name=&#34;dolly-v2&#34;, model_id=&#39;databricks/dolly-v2-7b&#39;, device_map=&#39;auto&#39;)&#xA;&#xA;llm(&#34;What is the difference between a duck and a goose? And why there are so many Goose in Canada?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;langchain.llms.OpenLLM&lt;/code&gt; has the capability to interact with remote OpenLLM Server. Given there is an OpenLLM server deployed elsewhere, you can connect to it by specifying its URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain.llms import OpenLLM&#xA;&#xA;llm = OpenLLM(server_url=&#39;http://44.23.123.1:3000&#39;, server_type=&#39;grpc&#39;)&#xA;llm(&#34;What is the difference between a duck and a goose? And why there are so many Goose in Canada?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To integrate a LangChain agent with BentoML, you can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm = OpenLLM(&#xA;    model_name=&#39;flan-t5&#39;,&#xA;    model_id=&#39;google/flan-t5-large&#39;,&#xA;    embedded=False,&#xA;)&#xA;tools = load_tools([&#34;serpapi&#34;, &#34;llm-math&#34;], llm=llm)&#xA;agent = initialize_agent(&#xA;    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION&#xA;)&#xA;svc = bentoml.Service(&#34;langchain-openllm&#34;, runners=[llm.runner])&#xA;@svc.api(input=Text(), output=Text())&#xA;def chat(input_text: str):&#xA;    return agent.run(input_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; You can find out more examples under the &lt;a href=&#34;https://github.com/bentoml/OpenLLM/tree/main/examples&#34;&gt;examples&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;🚀 Deploying to Production&lt;/h2&gt; &#xA;&lt;p&gt;To deploy your LLMs into production:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Building a Bento&lt;/strong&gt;: With OpenLLM, you can easily build a Bento for a specific model, like &lt;code&gt;dolly-v2&lt;/code&gt;, using the &lt;code&gt;build&lt;/code&gt; command.:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openllm build dolly-v2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A &lt;a href=&#34;https://docs.bentoml.org/en/latest/concepts/bento.html#what-is-a-bento&#34;&gt;Bento&lt;/a&gt;, in BentoML, is the unit of distribution. It packages your program&#39;s source code, models, files, artefacts, and dependencies.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Containerize your Bento&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bentoml containerize &amp;lt;name:version&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;BentoML offers a comprehensive set of options for deploying and hosting online ML services in production. To learn more, check out the &lt;a href=&#34;https://docs.bentoml.org/en/latest/concepts/deploy.html&#34;&gt;Deploying a Bento&lt;/a&gt; guide.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🍇 Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;OpenLLM collects usage data to enhance user experience and improve the product. We only report OpenLLM&#39;s internal API calls and ensure maximum privacy by excluding sensitive information. We will never collect user code, model data, or stack traces. For usage tracking, check out the &lt;a href=&#34;https://raw.githubusercontent.com/bentoml/OpenLLM/main/src/openllm/utils/analytics.py&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can opt out of usage tracking by using the &lt;code&gt;--do-not-track&lt;/code&gt; CLI option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openllm [command] --do-not-track&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or by setting the environment variable &lt;code&gt;OPENLLM_DO_NOT_TRACK=True&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENLLM_DO_NOT_TRACK=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;👥 Community&lt;/h2&gt; &#xA;&lt;p&gt;Engage with like-minded individuals passionate about LLMs, AI, and more on our &lt;a href=&#34;https://l.bentoml.com/join-openllm-discord&#34;&gt;Discord&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;OpenLLM is actively maintained by the BentoML team. Feel free to reach out and join us in our pursuit to make LLMs more accessible and easy to use 👉 &lt;a href=&#34;https://l.bentoml.com/join-slack&#34;&gt;Join our Slack community!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🎁 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! If you&#39;re interested in enhancing OpenLLM&#39;s capabilities or have any questions, don&#39;t hesitate to reach out in our &lt;a href=&#34;https://l.bentoml.com/join-openllm-discord&#34;&gt;discord channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Checkout our &lt;a href=&#34;https://github.com/bentoml/OpenLLM/raw/main/DEVELOPMENT.md&#34;&gt;Developer Guide&lt;/a&gt; if you wish to contribute to OpenLLM&#39;s codebase.&lt;/p&gt;</summary>
  </entry>
</feed>