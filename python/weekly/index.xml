<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-19T01:45:10Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>getomni-ai/zerox</title>
    <updated>2025-01-19T01:45:10Z</updated>
    <id>tag:github.com,2025-01-19:/getomni-ai/zerox</id>
    <link href="https://github.com/getomni-ai/zerox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PDF to Markdown with vision models&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/getomni-ai/zerox/main/examples/heroImage.png&#34; alt=&#34;Hero Image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Zerox OCR&lt;/h2&gt; &#xA;&lt;a href=&#34;https://discord.gg/smg2QfwtJ6&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/cccc0e9a-e3b2-425e-9b54-e5024681b129&#34; alt=&#34;Join us on Discord&#34; width=&#34;200px&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;A dead simple way of OCR-ing a document for AI ingestion. Documents are meant to be a visual representation after all. With weird layouts, tables, charts, etc. The vision models just make sense!&lt;/p&gt; &#xA;&lt;p&gt;The general logic:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pass in a file (pdf, docx, image, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Convert that file into a series of images&lt;/li&gt; &#xA; &lt;li&gt;Pass each image to GPT and ask nicely for Markdown&lt;/li&gt; &#xA; &lt;li&gt;Aggregate the responses and return Markdown&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Try out the hosted version here: &lt;a href=&#34;https://getomni.ai/ocr-demo&#34;&gt;https://getomni.ai/ocr-demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Zerox is available as both a Node and Python package.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/getomni-ai/zerox/main/#node-zerox&#34;&gt;Node Readme&lt;/a&gt; - &lt;a href=&#34;https://www.npmjs.com/package/zerox&#34;&gt;npm package&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/getomni-ai/zerox/main/#python-zerox&#34;&gt;Python Readme&lt;/a&gt; - &lt;a href=&#34;https://pypi.org/project/py-zerox/&#34;&gt;pip package&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Node Zerox&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm install zerox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Zerox uses &lt;code&gt;graphicsmagick&lt;/code&gt; and &lt;code&gt;ghostscript&lt;/code&gt; for the pdf =&amp;gt; image processing step. These should be pulled automatically, but you may need to manually install.&lt;/p&gt; &#xA;&lt;p&gt;On linux use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get update&#xA;sudo apt-get install -y graphicsmagick&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;With file URL&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { zerox } from &#34;zerox&#34;;&#xA;&#xA;const result = await zerox({&#xA;  filePath: &#34;https://omni-demo-data.s3.amazonaws.com/test/cs101.pdf&#34;,&#xA;  openaiAPIKey: process.env.OPENAI_API_KEY,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;From local path&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import path from &#34;path&#34;;&#xA;import { zerox } from &#34;zerox&#34;;&#xA;&#xA;const result = await zerox({&#xA;  filePath: path.resolve(__dirname, &#34;./cs101.pdf&#34;),&#xA;  openaiAPIKey: process.env.OPENAI_API_KEY,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Options&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const result = await zerox({&#xA;  // Required&#xA;  filePath: &#34;path/to/file&#34;,&#xA;  openaiAPIKey: process.env.OPENAI_API_KEY,&#xA;&#xA;  // Optional&#xA;  cleanup: true, // Clear images from tmp after run.&#xA;  concurrency: 10, // Number of pages to run at a time.&#xA;  correctOrientation: true, // True by default, attempts to identify and correct page orientation.&#xA;  errorMode: ErrorMode.IGNORE, // ErrorMode.THROW or ErrorMode.IGNORE, defaults to ErrorMode.IGNORE.&#xA;  maintainFormat: false, // Slower but helps maintain consistent formatting.&#xA;  maxRetries: 1, // Number of retries to attempt on a failed page, defaults to 1.&#xA;  maxTesseractWorkers: -1, // Maximum number of tesseract workers. Zerox will start with a lower number and only reach maxTesseractWorkers if needed.&#xA;  model: &#34;gpt-4o-mini&#34;, // Model to use (gpt-4o-mini or gpt-4o).&#xA;  onPostProcess: async ({ page, progressSummary }) =&amp;gt; Promise&amp;lt;void&amp;gt;, // Callback function to run after each page is processed.&#xA;  onPreProcess: async ({ imagePath, pageNumber }) =&amp;gt; Promise&amp;lt;void&amp;gt;, // Callback function to run before each page is processed.&#xA;  outputDir: undefined, // Save combined result.md to a file.&#xA;  pagesToConvertAsImages: -1, // Page numbers to convert to image as array (e.g. `[1, 2, 3]`) or a number (e.g. `1`). Set to -1 to convert all pages.&#xA;  tempDir: &#34;/os/tmp&#34;, // Directory to use for temporary files (default: system temp directory).&#xA;  trimEdges: true, // True by default, trims pixels from all edges that contain values similar to the given background colour, which defaults to that of the top-left pixel.&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;maintainFormat&lt;/code&gt; option trys to return the markdown in a consistent format by passing the output of a prior page in as additional context for the next page. This requires the requests to run synchronously, so it&#39;s a lot slower. But valuable if your documents have a lot of tabular data, or frequently have tables that cross pages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Request #1 =&amp;gt; page_1_image&#xA;Request #2 =&amp;gt; page_1_markdown + page_2_image&#xA;Request #3 =&amp;gt; page_2_markdown + page_3_image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Output&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;{&#xA;  completionTime: 10038,&#xA;  fileName: &#39;invoice_36258&#39;,&#xA;  inputTokens: 25543,&#xA;  outputTokens: 210,&#xA;  pages: [&#xA;    {&#xA;      content: &#39;# INVOICE # 36258\n&#39; +&#xA;        &#39;**Date:** Mar 06 2012  \n&#39; +&#xA;        &#39;**Ship Mode:** First Class  \n&#39; +&#xA;        &#39;**Balance Due:** $50.10  \n&#39; +&#xA;        &#39;## Bill To:\n&#39; +&#xA;        &#39;Aaron Bergman  \n&#39; +&#xA;        &#39;98103, Seattle,  \n&#39; +&#xA;        &#39;Washington, United States  \n&#39; +&#xA;        &#39;## Ship To:\n&#39; +&#xA;        &#39;Aaron Bergman  \n&#39; +&#xA;        &#39;98103, Seattle,  \n&#39; +&#xA;        &#39;Washington, United States  \n&#39; +&#xA;        &#39;\n&#39; +&#xA;        &#39;| Item                                       | Quantity | Rate   | Amount  |\n&#39; +&#xA;        &#39;|--------------------------------------------|----------|--------|---------|\n&#39; +&#xA;        &#34;| Global Push Button Manager&#39;s Chair, Indigo | 1        | $48.71 | $48.71  |\n&#34; +&#xA;        &#39;| Chairs, Furniture, FUR-CH-4421             |          |        |         |\n&#39; +&#xA;        &#39;\n&#39; +&#xA;        &#39;**Subtotal:** $48.71  \n&#39; +&#xA;        &#39;**Discount (20%):** $9.74  \n&#39; +&#xA;        &#39;**Shipping:** $11.13  \n&#39; +&#xA;        &#39;**Total:** $50.10  \n&#39; +&#xA;        &#39;---\n&#39; +&#xA;        &#39;**Notes:**  \n&#39; +&#xA;        &#39;Thanks for your business!  \n&#39; +&#xA;        &#39;**Terms:**  \n&#39; +&#xA;        &#39;Order ID : CA-2012-AB10015140-40974  &#39;,&#xA;      page: 1,&#xA;      contentLength: 747,&#xA;      status: &#39;SUCCESS&#39;,&#xA;    }&#xA;  ],&#xA;  summary: {&#xA;    failedPages: 0,&#xA;    successfulPages: 1,&#xA;    totalPages: 1,&#xA;  },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Python Zerox&lt;/h2&gt; &#xA;&lt;p&gt;(Python SDK - supports vision models from different providers like OpenAI, Azure OpenAI, Anthropic, AWS Bedrock etc)&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;strong&gt;poppler&lt;/strong&gt; on the system, it should be available in path variable. See the &lt;a href=&#34;https://pdf2image.readthedocs.io/en/latest/installation.html&#34;&gt;pdf2image documentation&lt;/a&gt; for instructions by platform.&lt;/li&gt; &#xA; &lt;li&gt;Install py-zerox:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install py-zerox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;pyzerox.zerox&lt;/code&gt; function is an asynchronous API that performs OCR (Optical Character Recognition) to markdown using vision models. It processes PDF files and converts them into markdown format. Make sure to set up the environment variables for the model and the model provider before using this API.&lt;/p&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;LiteLLM Documentation&lt;/a&gt; for setting up the environment and passing the correct model name.&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyzerox import zerox&#xA;import os&#xA;import json&#xA;import asyncio&#xA;&#xA;### Model Setup (Use only Vision Models) Refer: https://docs.litellm.ai/docs/providers ###&#xA;&#xA;## placeholder for additional model kwargs which might be required for some models&#xA;kwargs = {}&#xA;&#xA;## system prompt to use for the vision model&#xA;custom_system_prompt = None&#xA;&#xA;# to override&#xA;# custom_system_prompt = &#34;For the below pdf page, do something..something...&#34; ## example&#xA;&#xA;###################### Example for OpenAI ######################&#xA;model = &#34;gpt-4o-mini&#34; ## openai model&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;&#34; ## your-api-key&#xA;&#xA;&#xA;###################### Example for Azure OpenAI ######################&#xA;model = &#34;azure/gpt-4o-mini&#34; ## &#34;azure/&amp;lt;your_deployment_name&amp;gt;&#34; -&amp;gt; format &amp;lt;provider&amp;gt;/&amp;lt;model&amp;gt;&#xA;os.environ[&#34;AZURE_API_KEY&#34;] = &#34;&#34; # &#34;your-azure-api-key&#34;&#xA;os.environ[&#34;AZURE_API_BASE&#34;] = &#34;&#34; # &#34;https://example-endpoint.openai.azure.com&#34;&#xA;os.environ[&#34;AZURE_API_VERSION&#34;] = &#34;&#34; # &#34;2023-05-15&#34;&#xA;&#xA;&#xA;###################### Example for Gemini ######################&#xA;model = &#34;gemini/gpt-4o-mini&#34; ## &#34;gemini/&amp;lt;gemini_model&amp;gt;&#34; -&amp;gt; format &amp;lt;provider&amp;gt;/&amp;lt;model&amp;gt;&#xA;os.environ[&#39;GEMINI_API_KEY&#39;] = &#34;&#34; # your-gemini-api-key&#xA;&#xA;&#xA;###################### Example for Anthropic ######################&#xA;model=&#34;claude-3-opus-20240229&#34;&#xA;os.environ[&#34;ANTHROPIC_API_KEY&#34;] = &#34;&#34; # your-anthropic-api-key&#xA;&#xA;###################### Vertex ai ######################&#xA;model = &#34;vertex_ai/gemini-1.5-flash-001&#34; ## &#34;vertex_ai/&amp;lt;model_name&amp;gt;&#34; -&amp;gt; format &amp;lt;provider&amp;gt;/&amp;lt;model&amp;gt;&#xA;## GET CREDENTIALS&#xA;## RUN ##&#xA;# !gcloud auth application-default login - run this to add vertex credentials to your env&#xA;## OR ##&#xA;file_path = &#39;path/to/vertex_ai_service_account.json&#39;&#xA;&#xA;# Load the JSON file&#xA;with open(file_path, &#39;r&#39;) as file:&#xA;    vertex_credentials = json.load(file)&#xA;&#xA;# Convert to JSON string&#xA;vertex_credentials_json = json.dumps(vertex_credentials)&#xA;&#xA;vertex_credentials=vertex_credentials_json&#xA;&#xA;## extra args&#xA;kwargs = {&#34;vertex_credentials&#34;: vertex_credentials}&#xA;&#xA;###################### For other providers refer: https://docs.litellm.ai/docs/providers ######################&#xA;&#xA;# Define main async entrypoint&#xA;async def main():&#xA;    file_path = &#34;https://omni-demo-data.s3.amazonaws.com/test/cs101.pdf&#34; ## local filepath and file URL supported&#xA;&#xA;    ## process only some pages or all&#xA;    select_pages = None ## None for all, but could be int or list(int) page numbers (1 indexed)&#xA;&#xA;    output_dir = &#34;./output_test&#34; ## directory to save the consolidated markdown file&#xA;    result = await zerox(file_path=file_path, model=model, output_dir=output_dir,&#xA;                        custom_system_prompt=custom_system_prompt,select_pages=select_pages, **kwargs)&#xA;    return result&#xA;&#xA;&#xA;# run the main function:&#xA;result = asyncio.run(main())&#xA;&#xA;# print markdown result&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Parameters&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async def zerox(&#xA;    cleanup: bool = True,&#xA;    concurrency: int = 10,&#xA;    file_path: Optional[str] = &#34;&#34;,&#xA;    maintain_format: bool = False,&#xA;    model: str = &#34;gpt-4o-mini&#34;,&#xA;    output_dir: Optional[str] = None,&#xA;    temp_dir: Optional[str] = None,&#xA;    custom_system_prompt: Optional[str] = None,&#xA;    select_pages: Optional[Union[int, Iterable[int]]] = None,&#xA;    **kwargs&#xA;) -&amp;gt; ZeroxOutput:&#xA;  ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;cleanup&lt;/strong&gt; (bool, optional): Whether to clean up temporary files after processing. Defaults to True.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;concurrency&lt;/strong&gt; (int, optional): The number of concurrent processes to run. Defaults to 10.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;file_path&lt;/strong&gt; (Optional[str], optional): The path to the PDF file to process. Defaults to an empty string.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;maintain_format&lt;/strong&gt; (bool, optional): Whether to maintain the format from the previous page. Defaults to False.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;model&lt;/strong&gt; (str, optional): The model to use for generating completions. Defaults to &#34;gpt-4o-mini&#34;. Refer to LiteLLM Providers for the correct model name, as it may differ depending on the provider.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;output_dir&lt;/strong&gt; (Optional[str], optional): The directory to save the markdown output. Defaults to None.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;temp_dir&lt;/strong&gt; (str, optional): The directory to store temporary files, defaults to some named folder in system&#39;s temp directory. If already exists, the contents will be deleted before zerox uses it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;custom_system_prompt&lt;/strong&gt; (str, optional): The system prompt to use for the model, this overrides the default system prompt of zerox.Generally it is not required unless you want some specific behaviour. When set, it will raise a friendly warning. Defaults to None.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;select_pages&lt;/strong&gt; (Optional[Union[int, Iterable[int]]], optional): Pages to process, can be a single page number or an iterable of page numbers, Defaults to None&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;kwargs&lt;/strong&gt; (dict, optional): Additional keyword arguments to pass to the litellm.completion method. Refer to the LiteLLM Documentation and Completion Input for details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Returns&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ZeroxOutput: Contains the markdown content generated by the model and also some metadata (refer below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example Output (Output from &#34;azure/gpt-4o-mini&#34;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;Note: The output is mannually wrapped for this documentation for better readability.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;ZeroxOutput(&#xA;    completion_time=9432.975,&#xA;    file_name=&#39;cs101&#39;,&#xA;    input_tokens=36877,&#xA;    output_tokens=515,&#xA;    pages=[&#xA;        Page(&#xA;            content=&#39;| Type    | Description                          | Wrapper Class |\n&#39; +&#xA;                    &#39;|---------|--------------------------------------|---------------|\n&#39; +&#xA;                    &#39;| byte    | 8-bit signed 2s complement integer   | Byte          |\n&#39; +&#xA;                    &#39;| short   | 16-bit signed 2s complement integer  | Short         |\n&#39; +&#xA;                    &#39;| int     | 32-bit signed 2s complement integer  | Integer       |\n&#39; +&#xA;                    &#39;| long    | 64-bit signed 2s complement integer  | Long          |\n&#39; +&#xA;                    &#39;| float   | 32-bit IEEE 754 floating point number| Float         |\n&#39; +&#xA;                    &#39;| double  | 64-bit floating point number         | Double        |\n&#39; +&#xA;                    &#39;| boolean | may be set to true or false          | Boolean       |\n&#39; +&#xA;                    &#39;| char    | 16-bit Unicode (UTF-16) character    | Character     |\n\n&#39; +&#xA;                    &#39;Table 26.2.: Primitive types in Java\n\n&#39; +&#xA;                    &#39;### 26.3.1. Declaration &amp;amp; Assignment\n\n&#39; +&#xA;                    &#39;Java is a statically typed language meaning that all variables must be declared before you can use &#39; +&#xA;                    &#39;them or refer to them. In addition, when declaring a variable, you must specify both its type and &#39; +&#xA;                    &#39;its identifier. For example:\n\n&#39; +&#xA;                    &#39;```java\n&#39; +&#xA;                    &#39;int numUnits;\n&#39; +&#xA;                    &#39;double costPerUnit;\n&#39; +&#xA;                    &#39;char firstInitial;\n&#39; +&#xA;                    &#39;boolean isStudent;\n&#39; +&#xA;                    &#39;```\n\n&#39; +&#xA;                    &#39;Each declaration specifies the variableâ€™s type followed by the identifier and ending with a &#39; +&#xA;                    &#39;semicolon. The identifier rules are fairly standard: a name can consist of lowercase and &#39; +&#xA;                    &#39;uppercase alphabetic characters, numbers, and underscores but may not begin with a numeric &#39; +&#xA;                    &#39;character. We adopt the modern camelCasing naming convention for variables in our code. In &#39; +&#xA;                    &#39;general, variables must be assigned a value before you can use them in an expression. You do not &#39; +&#xA;                    &#39;have to immediately assign a value when you declare them (though it is good practice), but some &#39; +&#xA;                    &#39;value must be assigned before they can be used or the compiler will issue an error.\n\n&#39; +&#xA;                    &#39;The assignment operator is a single equal sign, `=` and is a right-to-left assignment. That is, &#39; +&#xA;                    &#39;the variable that we wish to assign the value to appears on the left-hand-side while the value &#39; +&#xA;                    &#39;(literal, variable or expression) is on the right-hand-side. Using our variables from before, &#39; +&#xA;                    &#39;we can assign them values:\n\n&#39; +&#xA;                    &#39;&amp;gt; 2 Instance variables, that is variables declared as part of an object do have default values. &#39; +&#xA;                    &#39;For objects, the default is `null`, for all numeric types, zero is the default value. For the &#39; +&#xA;                    &#39;boolean type, `false` is the default, and the default char value is `\\0`, the null-terminating &#39; +&#xA;                    &#39;character (zero in the ASCII table).&#39;,&#xA;            content_length=2333,&#xA;            page=1&#xA;        )&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Supported File Types&lt;/h2&gt; &#xA;&lt;p&gt;We use a combination of &lt;code&gt;libreoffice&lt;/code&gt; and &lt;code&gt;graphicsmagick&lt;/code&gt; to do document =&amp;gt; image conversion. For non-image / non-pdf files, we use libreoffice to convert that file to a pdf, and then to an image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;[&#xA;  &#34;pdf&#34;, // Portable Document Format&#xA;  &#34;doc&#34;, // Microsoft Word 97-2003&#xA;  &#34;docx&#34;, // Microsoft Word 2007-2019&#xA;  &#34;odt&#34;, // OpenDocument Text&#xA;  &#34;ott&#34;, // OpenDocument Text Template&#xA;  &#34;rtf&#34;, // Rich Text Format&#xA;  &#34;txt&#34;, // Plain Text&#xA;  &#34;html&#34;, // HTML Document&#xA;  &#34;htm&#34;, // HTML Document (alternative extension)&#xA;  &#34;xml&#34;, // XML Document&#xA;  &#34;wps&#34;, // Microsoft Works Word Processor&#xA;  &#34;wpd&#34;, // WordPerfect Document&#xA;  &#34;xls&#34;, // Microsoft Excel 97-2003&#xA;  &#34;xlsx&#34;, // Microsoft Excel 2007-2019&#xA;  &#34;ods&#34;, // OpenDocument Spreadsheet&#xA;  &#34;ots&#34;, // OpenDocument Spreadsheet Template&#xA;  &#34;csv&#34;, // Comma-Separated Values&#xA;  &#34;tsv&#34;, // Tab-Separated Values&#xA;  &#34;ppt&#34;, // Microsoft PowerPoint 97-2003&#xA;  &#34;pptx&#34;, // Microsoft PowerPoint 2007-2019&#xA;  &#34;odp&#34;, // OpenDocument Presentation&#xA;  &#34;otp&#34;, // OpenDocument Presentation Template&#xA;];&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;Litellm&lt;/a&gt;: &lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;https://github.com/BerriAI/litellm&lt;/a&gt; | This powers our python sdk to support all popular vision models from different providers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>KoljaB/RealtimeSTT</title>
    <updated>2025-01-19T01:45:10Z</updated>
    <id>tag:github.com,2025-01-19:/KoljaB/RealtimeSTT</id>
    <link href="https://github.com/KoljaB/RealtimeSTT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A robust, efficient, low-latency speech-to-text library with advanced voice activity detection, wake word activation and instant transcription.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RealtimeSTT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/RealtimeSTT/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/RealtimeSTT&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/KoljaB/RealtimeSTT&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/RealtimeSTT&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/KoljaB/RealtimeSTT/releases/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/KoljaB/RealtimeSTT.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/KoljaB/RealtimeSTT/commit/&#34;&gt;&lt;img src=&#34;https://badgen.net/github/commits/KoljaB/RealtimeSTT&#34; alt=&#34;GitHub commits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/KoljaB/RealtimeSTT/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/KoljaB/RealtimeSTT.svg?style=social&amp;amp;label=Fork&amp;amp;maxAge=2592000&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/KoljaB/RealtimeSTT/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/KoljaB/RealtimeSTT.svg?style=social&amp;amp;label=Star&amp;amp;maxAge=2592000&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Easy-to-use, low-latency speech-to-text library for realtime applications&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AudioToTextRecorderClient class, which automatically starts a server if none is running and connects to it. The class shares the same interface as AudioToTextRecorder, making it easy to upgrade or switch between the two. (Work in progress, most parameters and callbacks of AudioToTextRecorder are already implemented into AudioToTextRecorderClient, but not all. Also the server can not handle concurrent (parallel) requests yet.)&lt;/li&gt; &#xA; &lt;li&gt;reworked CLI interface (&#34;stt-server&#34; to start the server, &#34;stt&#34; to start the client, look at &#34;server&#34; folder for more info)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About the Project&lt;/h2&gt; &#xA;&lt;p&gt;RealtimeSTT listens to the microphone and transcribes voice into text.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Hint:&lt;/strong&gt; &lt;em&gt;&lt;strong&gt;Check out &lt;a href=&#34;https://github.com/KoljaB/Linguflex&#34;&gt;Linguflex&lt;/a&gt;&lt;/strong&gt;, the original project from which RealtimeSTT is spun off. It lets you control your environment by speaking and is one of the most capable and sophisticated open-source assistants currently available.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;It&#39;s ideal for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Voice Assistants&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Applications requiring &lt;strong&gt;fast and precise&lt;/strong&gt; speech-to-text conversion&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/797e6552-27cd-41b1-a7f3-e5cbc72094f5&#34;&gt;https://github.com/user-attachments/assets/797e6552-27cd-41b1-a7f3-e5cbc72094f5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Updates&lt;/h3&gt; &#xA;&lt;p&gt;Latest Version: v0.3.92&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/KoljaB/RealtimeSTT/releases&#34;&gt;release history&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Hint:&lt;/strong&gt; &lt;em&gt;Since we use the &lt;code&gt;multiprocessing&lt;/code&gt; module now, ensure to include the &lt;code&gt;if __name__ == &#39;__main__&#39;:&lt;/code&gt; protection in your code to prevent unexpected behavior, especially on platforms like Windows. For a detailed explanation on why this is important, visit the &lt;a href=&#34;https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming&#34;&gt;official Python documentation on &lt;code&gt;multiprocessing&lt;/code&gt;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Quick Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Print everything being said:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;def process_text(text):&#xA;    print(text)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    print(&#34;Wait until it says &#39;speak now&#39;&#34;)&#xA;    recorder = AudioToTextRecorder()&#xA;&#xA;    while True:&#xA;        recorder.text(process_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Type everything being said:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;import pyautogui&#xA;&#xA;def process_text(text):&#xA;    pyautogui.typewrite(text + &#34; &#34;)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    print(&#34;Wait until it says &#39;speak now&#39;&#34;)&#xA;    recorder = AudioToTextRecorder()&#xA;&#xA;    while True:&#xA;        recorder.text(process_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Will type everything being said into your selected text box&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Voice Activity Detection&lt;/strong&gt;: Automatically detects when you start and stop speaking.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Realtime Transcription&lt;/strong&gt;: Transforms speech to text in real-time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wake Word Activation&lt;/strong&gt;: Can activate upon detecting a designated wake word.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Hint&lt;/strong&gt;: &lt;em&gt;Check out &lt;a href=&#34;https://github.com/KoljaB/RealtimeTTS&#34;&gt;RealtimeTTS&lt;/a&gt;, the output counterpart of this library, for text-to-voice capabilities. Together, they form a powerful realtime audio wrapper around large language models.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Tech Stack&lt;/h2&gt; &#xA;&lt;p&gt;This library uses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Voice Activity Detection&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/wiseman/py-webrtcvad&#34;&gt;WebRTCVAD&lt;/a&gt; for initial voice activity detection.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;&gt;SileroVAD&lt;/a&gt; for more accurate verification.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speech-To-Text&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/guillaumekln/faster-whisper&#34;&gt;Faster_Whisper&lt;/a&gt; for instant (GPU-accelerated) transcription.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wake Word Detection&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Picovoice/porcupine&#34;&gt;Porcupine&lt;/a&gt; or &lt;a href=&#34;https://github.com/dscripka/openWakeWord&#34;&gt;OpenWakeWord&lt;/a&gt; for wake word detection.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;These components represent the &#34;industry standard&#34; for cutting-edge applications, providing the most modern and effective foundation for building high-end solutions.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install RealtimeSTT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will install all the necessary dependencies, including a &lt;strong&gt;CPU support only&lt;/strong&gt; version of PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;Although it is possible to run RealtimeSTT with a CPU installation only (use a small model like &#34;tiny&#34; or &#34;base&#34; in this case) you will get way better experience using CUDA (please scroll down).&lt;/p&gt; &#xA;&lt;h3&gt;Linux Installation&lt;/h3&gt; &#xA;&lt;p&gt;Before installing RealtimeSTT please execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get update&#xA;sudo apt-get install python3-dev&#xA;sudo apt-get install portaudio19-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;MacOS Installation&lt;/h3&gt; &#xA;&lt;p&gt;Before installing RealtimeSTT please execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install portaudio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GPU Support with CUDA (recommended)&lt;/h3&gt; &#xA;&lt;h3&gt;Updating PyTorch for CUDA Support&lt;/h3&gt; &#xA;&lt;p&gt;To upgrade your PyTorch installation to enable GPU support with CUDA, follow these instructions based on your specific CUDA version. This is useful if you wish to enhance the performance of RealtimeSTT with CUDA capabilities.&lt;/p&gt; &#xA;&lt;h4&gt;For CUDA 11.8:&lt;/h4&gt; &#xA;&lt;p&gt;To update PyTorch and Torchaudio to support CUDA 11.8, use the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch==2.5.1+cu118 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For CUDA 12.X:&lt;/h4&gt; &#xA;&lt;p&gt;To update PyTorch and Torchaudio to support CUDA 12.X, execute the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch==2.5.1+cu121 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;2.5.1&lt;/code&gt; with the version of PyTorch that matches your system and requirements.&lt;/p&gt; &#xA;&lt;h3&gt;Steps That Might Be Necessary Before&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;em&gt;To check if your NVIDIA GPU supports CUDA, visit the &lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;official CUDA GPUs list&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you didn&#39;t use CUDA models before, some additional steps might be needed one time before installation. These steps prepare the system for CUDA support and installation of the &lt;strong&gt;GPU-optimized&lt;/strong&gt; installation. This is recommended for those who require &lt;strong&gt;better performance&lt;/strong&gt; and have a compatible NVIDIA GPU. To use RealtimeSTT with GPU support via CUDA please also follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA CUDA Toolkit&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;select between CUDA 11.8 or CUDA 12.X Toolkit &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;for 12.X visit &lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit-archive&#34;&gt;NVIDIA CUDA Toolkit Archive&lt;/a&gt; and select latest version.&lt;/li&gt; &#xA;     &lt;li&gt;for 11.8 visit &lt;a href=&#34;https://developer.nvidia.com/cuda-11-8-0-download-archive&#34;&gt;NVIDIA CUDA Toolkit 11.8&lt;/a&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Select operating system and version.&lt;/li&gt; &#xA;   &lt;li&gt;Download and install the software.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA cuDNN&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;select between CUDA 11.8 or CUDA 12.X Toolkit &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;for 12.X visit &lt;a href=&#34;https://developer.nvidia.com/cudnn-downloads&#34;&gt;cuDNN Downloads&lt;/a&gt;. &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Select operating system and version.&lt;/li&gt; &#xA;       &lt;li&gt;Download and install the software.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;for 11.8 visit &lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-archive&#34;&gt;NVIDIA cuDNN Archive&lt;/a&gt;. &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Click on &#34;Download cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x&#34;.&lt;/li&gt; &#xA;       &lt;li&gt;Download and install the software.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install ffmpeg&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;em&gt;Installation of ffmpeg might not actually be needed to operate RealtimeSTT&lt;/em&gt; &lt;sup&gt; *thanks to jgilbert2017 for pointing this out&lt;/sup&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;You can download an installer for your OS from the &lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;ffmpeg Website&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Or use a package manager:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Ubuntu or Debian&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update &amp;amp;&amp;amp; sudo apt install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Arch Linux&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo pacman -S ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On MacOS using Homebrew&lt;/strong&gt; (&lt;a href=&#34;https://brew.sh/&#34;&gt;https://brew.sh/&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Windows using Winget&lt;/strong&gt; &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/package-manager/winget/&#34;&gt;official documentation&lt;/a&gt; :&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;winget install Gyan.FFmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Windows using Chocolatey&lt;/strong&gt; (&lt;a href=&#34;https://chocolatey.org/&#34;&gt;https://chocolatey.org/&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;choco install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Windows using Scoop&lt;/strong&gt; (&lt;a href=&#34;https://scoop.sh/&#34;&gt;https://scoop.sh/&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scoop install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Basic usage:&lt;/p&gt; &#xA;&lt;h3&gt;Manual Recording&lt;/h3&gt; &#xA;&lt;p&gt;Start and stop of recording are manually triggered.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recorder.start()&#xA;recorder.stop()&#xA;print(recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder()&#xA;    recorder.start()&#xA;    input(&#34;Press Enter to stop recording...&#34;)&#xA;    recorder.stop()&#xA;    print(&#34;Transcription: &#34;, recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Automatic Recording&lt;/h3&gt; &#xA;&lt;p&gt;Recording based on voice activity detection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with AudioToTextRecorder() as recorder:&#xA;    print(recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    with AudioToTextRecorder() as recorder:&#xA;        print(&#34;Transcription: &#34;, recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When running recorder.text in a loop it is recommended to use a callback, allowing the transcription to be run asynchronously:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def process_text(text):&#xA;    print (text)&#xA;    &#xA;while True:&#xA;    recorder.text(process_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;def process_text(text):&#xA;    print(text)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder()&#xA;&#xA;    while True:&#xA;        recorder.text(process_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Wakewords&lt;/h3&gt; &#xA;&lt;p&gt;Keyword activation before detecting voice. Write the comma-separated list of your desired activation keywords into the wake_words parameter. You can choose wake words from these list: alexa, americano, blueberry, bumblebee, computer, grapefruits, grasshopper, hey google, hey siri, jarvis, ok google, picovoice, porcupine, terminator.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recorder = AudioToTextRecorder(wake_words=&#34;jarvis&#34;)&#xA;&#xA;print(&#39;Say &#34;Jarvis&#34; then speak.&#39;)&#xA;print(recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder(wake_words=&#34;jarvis&#34;)&#xA;&#xA;    print(&#39;Say &#34;Jarvis&#34; to start recording.&#39;)&#xA;    print(recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Callbacks&lt;/h3&gt; &#xA;&lt;p&gt;You can set callback functions to be executed on different events (see &lt;a href=&#34;https://raw.githubusercontent.com/KoljaB/RealtimeSTT/master/#configuration&#34;&gt;Configuration&lt;/a&gt;) :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def my_start_callback():&#xA;    print(&#34;Recording started!&#34;)&#xA;&#xA;def my_stop_callback():&#xA;    print(&#34;Recording stopped!&#34;)&#xA;&#xA;recorder = AudioToTextRecorder(on_recording_start=my_start_callback,&#xA;                               on_recording_stop=my_stop_callback)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;def start_callback():&#xA;    print(&#34;Recording started!&#34;)&#xA;&#xA;def stop_callback():&#xA;    print(&#34;Recording stopped!&#34;)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder(on_recording_start=start_callback,&#xA;                                   on_recording_stop=stop_callback)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Feed chunks&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t want to use the local microphone set use_microphone parameter to false and provide raw PCM audiochunks in 16-bit mono (samplerate 16000) with this method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recorder.feed_audio(audio_chunk)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder(use_microphone=False)&#xA;    with open(&#34;audio_chunk.pcm&#34;, &#34;rb&#34;) as f:&#xA;        audio_chunk = f.read()&#xA;&#xA;    recorder.feed_audio(audio_chunk)&#xA;    print(&#34;Transcription: &#34;, recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Shutdown&lt;/h3&gt; &#xA;&lt;p&gt;You can shutdown the recorder safely by using the context manager protocol:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with AudioToTextRecorder() as recorder:&#xA;    [...]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can call the shutdown method manually (if using &#34;with&#34; is not feasible):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recorder.shutdown()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    with AudioToTextRecorder() as recorder:&#xA;        [...]&#xA;    # or manually shutdown if &#34;with&#34; is not used&#xA;    recorder.shutdown()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing the Library&lt;/h2&gt; &#xA;&lt;p&gt;The test subdirectory contains a set of scripts to help you evaluate and understand the capabilities of the RealtimeTTS library.&lt;/p&gt; &#xA;&lt;p&gt;Test scripts depending on RealtimeTTS library may require you to enter your azure service region within the script. When using OpenAI-, Azure- or Elevenlabs-related demo scripts the API Keys should be provided in the environment variables OPENAI_API_KEY, AZURE_SPEECH_KEY and ELEVENLABS_API_KEY (see &lt;a href=&#34;https://github.com/KoljaB/RealtimeTTS&#34;&gt;RealtimeTTS&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;simple_test.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: A &#34;hello world&#34; styled demonstration of the library&#39;s simplest usage.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;realtimestt_test.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Showcasing live-transcription.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wakeword_test.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: A demonstration of the wakeword activation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;translator.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Run &lt;code&gt;pip install openai realtimetts&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Real-time translations into six different languages.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;openai_voice_interface.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Run &lt;code&gt;pip install openai realtimetts&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Wake word activated and voice based user interface to the OpenAI API.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;advanced_talk.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Run &lt;code&gt;pip install openai keyboard realtimetts&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Choose TTS engine and voice before starting AI conversation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;minimalistic_talkbot.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Run &lt;code&gt;pip install openai realtimetts&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: A basic talkbot in 20 lines of code.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The example_app subdirectory contains a polished user interface application for the OpenAI API based on PyQt5.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;h3&gt;Initialization Parameters for &lt;code&gt;AudioToTextRecorder&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;When you initialize the &lt;code&gt;AudioToTextRecorder&lt;/code&gt; class, you have various options to customize its behavior.&lt;/p&gt; &#xA;&lt;h4&gt;General Parameters&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;model&lt;/strong&gt; (str, default=&#34;tiny&#34;): Model size or path for transcription.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Options: &#39;tiny&#39;, &#39;tiny.en&#39;, &#39;base&#39;, &#39;base.en&#39;, &#39;small&#39;, &#39;small.en&#39;, &#39;medium&#39;, &#39;medium.en&#39;, &#39;large-v1&#39;, &#39;large-v2&#39;.&lt;/li&gt; &#xA;   &lt;li&gt;Note: If a size is provided, the model will be downloaded from the Hugging Face Hub.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;language&lt;/strong&gt; (str, default=&#34;&#34;): Language code for transcription. If left empty, the model will try to auto-detect the language. Supported language codes are listed in &lt;a href=&#34;https://github.com/openai/whisper/raw/main/whisper/tokenizer.py&#34;&gt;Whisper Tokenizer library&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;compute_type&lt;/strong&gt; (str, default=&#34;default&#34;): Specifies the type of computation to be used for transcription. See &lt;a href=&#34;https://opennmt.net/CTranslate2/quantization.html&#34;&gt;Whisper Quantization&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;input_device_index&lt;/strong&gt; (int, default=0): Audio Input Device Index to use.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;gpu_device_index&lt;/strong&gt; (int, default=0): GPU Device Index to use. The model can also be loaded on multiple GPUs by passing a list of IDs (e.g. [0, 1, 2, 3]).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;device&lt;/strong&gt; (str, default=&#34;cuda&#34;): Device for model to use. Can either be &#34;cuda&#34; or &#34;cpu&#34;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_recording_start&lt;/strong&gt;: A callable function triggered when recording starts.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_recording_stop&lt;/strong&gt;: A callable function triggered when recording ends.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_transcription_start&lt;/strong&gt;: A callable function triggered when transcription starts.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ensure_sentence_starting_uppercase&lt;/strong&gt; (bool, default=True): Ensures that every sentence detected by the algorithm starts with an uppercase letter.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ensure_sentence_ends_with_period&lt;/strong&gt; (bool, default=True): Ensures that every sentence that doesn&#39;t end with punctuation such as &#34;?&#34;, &#34;!&#34; ends with a period&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;use_microphone&lt;/strong&gt; (bool, default=True): Usage of local microphone for transcription. Set to False if you want to provide chunks with feed_audio method.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;spinner&lt;/strong&gt; (bool, default=True): Provides a spinner animation text with information about the current recorder state.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;level&lt;/strong&gt; (int, default=logging.WARNING): Logging level.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;batch_size&lt;/strong&gt; (int, default=16): Batch size for the main transcription. Set to 0 to deactivate.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;init_logging&lt;/strong&gt; (bool, default=True): Whether to initialize the logging framework. Set to False to manage this yourself.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;handle_buffer_overflow&lt;/strong&gt; (bool, default=True): If set, the system will log a warning when an input overflow occurs during recording and remove the data from the buffer.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;beam_size&lt;/strong&gt; (int, default=5): The beam size to use for beam search decoding.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;initial_prompt&lt;/strong&gt; (str or iterable of int, default=None): Initial prompt to be fed to the transcription models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;suppress_tokens&lt;/strong&gt; (list of int, default=[-1]): Tokens to be suppressed from the transcription output.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_recorded_chunk&lt;/strong&gt;: A callback function that is triggered when a chunk of audio is recorded. Submits the chunk data as parameter.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;debug_mode&lt;/strong&gt; (bool, default=False): If set, the system prints additional debug information to the console.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;print_transcription_time&lt;/strong&gt; (bool, default=False): Logs the processing time of the main model transcription. This can be useful for performance monitoring and debugging.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;early_transcription_on_silence&lt;/strong&gt; (int, default=0): If set, the system will transcribe audio faster when silence is detected. Transcription will start after the specified milliseconds. Keep this value lower than &lt;code&gt;post_speech_silence_duration&lt;/code&gt;, ideally around &lt;code&gt;post_speech_silence_duration&lt;/code&gt; minus the estimated transcription time with the main model. If silence lasts longer than &lt;code&gt;post_speech_silence_duration&lt;/code&gt;, the recording is stopped, and the transcription is submitted. If voice activity resumes within this period, the transcription is discarded. This results in faster final transcriptions at the cost of additional GPU load due to some unnecessary final transcriptions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;allowed_latency_limit&lt;/strong&gt; (int, default=100): Specifies the maximum number of unprocessed chunks in the queue before discarding chunks. This helps prevent the system from being overwhelmed and losing responsiveness in real-time applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;no_log_file&lt;/strong&gt; (bool, default=False): If set, the system will skip writing the debug log file, reducing disk I/O. Useful if logging to a file is not needed and performance is a priority.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Real-time Transcription Parameters&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;em&gt;When enabling realtime description a GPU installation is strongly advised. Using realtime transcription may create high GPU loads.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;enable_realtime_transcription&lt;/strong&gt; (bool, default=False): Enables or disables real-time transcription of audio. When set to True, the audio will be transcribed continuously as it is being recorded.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;use_main_model_for_realtime&lt;/strong&gt; (bool, default=False): If set to True, the main transcription model will be used for both regular and real-time transcription. If False, a separate model specified by &lt;code&gt;realtime_model_type&lt;/code&gt; will be used for real-time transcription. Using a single model can save memory and potentially improve performance, but may not be optimized for real-time processing. Using separate models allows for a smaller, faster model for real-time transcription while keeping a more accurate model for final transcription.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;realtime_model_type&lt;/strong&gt; (str, default=&#34;tiny&#34;): Specifies the size or path of the machine learning model to be used for real-time transcription.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Valid options: &#39;tiny&#39;, &#39;tiny.en&#39;, &#39;base&#39;, &#39;base.en&#39;, &#39;small&#39;, &#39;small.en&#39;, &#39;medium&#39;, &#39;medium.en&#39;, &#39;large-v1&#39;, &#39;large-v2&#39;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;realtime_processing_pause&lt;/strong&gt; (float, default=0.2): Specifies the time interval in seconds after a chunk of audio gets transcribed. Lower values will result in more &#34;real-time&#34; (frequent) transcription updates but may increase computational load.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_realtime_transcription_update&lt;/strong&gt;: A callback function that is triggered whenever there&#39;s an update in the real-time transcription. The function is called with the newly transcribed text as its argument.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_realtime_transcription_stabilized&lt;/strong&gt;: A callback function that is triggered whenever there&#39;s an update in the real-time transcription and returns a higher quality, stabilized text as its argument.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;realtime_batch_size&lt;/strong&gt;: (int, default=16): Batch size for the real-time transcription model. Set to 0 to deactivate.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;beam_size_realtime&lt;/strong&gt; (int, default=3): The beam size to use for real-time transcription beam search decoding.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Voice Activation Parameters&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;silero_sensitivity&lt;/strong&gt; (float, default=0.6): Sensitivity for Silero&#39;s voice activity detection ranging from 0 (least sensitive) to 1 (most sensitive). Default is 0.6.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;silero_use_onnx&lt;/strong&gt; (bool, default=False): Enables usage of the pre-trained model from Silero in the ONNX (Open Neural Network Exchange) format instead of the PyTorch format. Default is False. Recommended for faster performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;silero_deactivity_detection&lt;/strong&gt; (bool, default=False): Enables the Silero model for end-of-speech detection. More robust against background noise. Utilizes additional GPU resources but improves accuracy in noisy environments. When False, uses the default WebRTC VAD, which is more sensitive but may continue recording longer due to background sounds.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;webrtc_sensitivity&lt;/strong&gt; (int, default=3): Sensitivity for the WebRTC Voice Activity Detection engine ranging from 0 (least aggressive / most sensitive) to 3 (most aggressive, least sensitive). Default is 3.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;post_speech_silence_duration&lt;/strong&gt; (float, default=0.2): Duration in seconds of silence that must follow speech before the recording is considered to be completed. This ensures that any brief pauses during speech don&#39;t prematurely end the recording.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;min_gap_between_recordings&lt;/strong&gt; (float, default=1.0): Specifies the minimum time interval in seconds that should exist between the end of one recording session and the beginning of another to prevent rapid consecutive recordings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;min_length_of_recording&lt;/strong&gt; (float, default=1.0): Specifies the minimum duration in seconds that a recording session should last to ensure meaningful audio capture, preventing excessively short or fragmented recordings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;pre_recording_buffer_duration&lt;/strong&gt; (float, default=0.2): The time span, in seconds, during which audio is buffered prior to formal recording. This helps counterbalancing the latency inherent in speech activity detection, ensuring no initial audio is missed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_vad_detect_start&lt;/strong&gt;: A callable function triggered when the system starts to listen for voice activity.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_vad_detect_stop&lt;/strong&gt;: A callable function triggered when the system stops to listen for voice activity.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Wake Word Parameters&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wakeword_backend&lt;/strong&gt; (str, default=&#34;pvporcupine&#34;): Specifies the backend library to use for wake word detection. Supported options include &#39;pvporcupine&#39; for using the Porcupine wake word engine or &#39;oww&#39; for using the OpenWakeWord engine.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;openwakeword_model_paths&lt;/strong&gt; (str, default=None): Comma-separated paths to model files for the openwakeword library. These paths point to custom models that can be used for wake word detection when the openwakeword library is selected as the wakeword_backend.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;openwakeword_inference_framework&lt;/strong&gt; (str, default=&#34;onnx&#34;): Specifies the inference framework to use with the openwakeword library. Can be either &#39;onnx&#39; for Open Neural Network Exchange format or &#39;tflite&#39; for TensorFlow Lite.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_words&lt;/strong&gt; (str, default=&#34;&#34;): Initiate recording when using the &#39;pvporcupine&#39; wakeword backend. Multiple wake words can be provided as a comma-separated string. Supported wake words are: alexa, americano, blueberry, bumblebee, computer, grapefruits, grasshopper, hey google, hey siri, jarvis, ok google, picovoice, porcupine, terminator. For the &#39;openwakeword&#39; backend, wake words are automatically extracted from the provided model files, so specifying them here is not necessary.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_words_sensitivity&lt;/strong&gt; (float, default=0.6): Sensitivity level for wake word detection (0 for least sensitive, 1 for most sensitive).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_word_activation_delay&lt;/strong&gt; (float, default=0): Duration in seconds after the start of monitoring before the system switches to wake word activation if no voice is initially detected. If set to zero, the system uses wake word activation immediately.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_word_timeout&lt;/strong&gt; (float, default=5): Duration in seconds after a wake word is recognized. If no subsequent voice activity is detected within this window, the system transitions back to an inactive state, awaiting the next wake word or voice activation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_word_buffer_duration&lt;/strong&gt; (float, default=0.1): Duration in seconds to buffer audio data during wake word detection. This helps in cutting out the wake word from the recording buffer so it does not falsely get detected along with the following spoken text, ensuring cleaner and more accurate transcription start triggers. Increase this if parts of the wake word get detected as text.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_wakeword_detected&lt;/strong&gt;: A callable function triggered when a wake word is detected.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_wakeword_timeout&lt;/strong&gt;: A callable function triggered when the system goes back to an inactive state after when no speech was detected after wake word activation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_wakeword_detection_start&lt;/strong&gt;: A callable function triggered when the system starts to listen for wake words&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_wakeword_detection_end&lt;/strong&gt;: A callable function triggered when stopping to listen for wake words (e.g. because of timeout or wake word detected)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;OpenWakeWord&lt;/h2&gt; &#xA;&lt;h3&gt;Training models&lt;/h3&gt; &#xA;&lt;p&gt;Look &lt;a href=&#34;https://github.com/dscripka/openWakeWord?tab=readme-ov-file#training-new-models&#34;&gt;here&lt;/a&gt; for information about how to train your own OpenWakeWord models. You can use a &lt;a href=&#34;https://colab.research.google.com/drive/1q1oe2zOyZp7UsB3jJiQ1IFn8z5YfjwEb?usp=sharing&#34;&gt;simple Google Colab notebook&lt;/a&gt; for a start or use a &lt;a href=&#34;https://github.com/dscripka/openWakeWord/raw/main/notebooks/automatic_model_training.ipynb&#34;&gt;more detailed notebook&lt;/a&gt; that enables more customization (can produce high quality models, but requires more development experience).&lt;/p&gt; &#xA;&lt;h3&gt;Convert model to ONNX format&lt;/h3&gt; &#xA;&lt;p&gt;You might need to use tf2onnx to convert tensorflow tflite models to onnx format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U tf2onnx&#xA;python -m tf2onnx.convert --tflite my_model_filename.tflite --output my_model_filename.onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configure RealtimeSTT&lt;/h3&gt; &#xA;&lt;p&gt;Suggested starting parameters for OpenWakeWord usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    with AudioToTextRecorder(&#xA;        wakeword_backend=&#34;oww&#34;,&#xA;        wake_words_sensitivity=0.35,&#xA;        openwakeword_model_paths=&#34;word1.onnx,word2.onnx&#34;,&#xA;        wake_word_buffer_duration=1,&#xA;        ) as recorder:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Q: I encountered the following error: &#34;Unable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so} Invalid handle. Cannot load symbol cudnnCreateTensorDescriptor.&#34; How do I fix this?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This issue arises from a mismatch between the version of &lt;code&gt;ctranslate2&lt;/code&gt; and cuDNN. The &lt;code&gt;ctranslate2&lt;/code&gt; library was updated to version 4.5.0, which uses cuDNN 9.2. There are two ways to resolve this issue:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Downgrade &lt;code&gt;ctranslate2&lt;/code&gt; to version 4.4.0&lt;/strong&gt;: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ctranslate2==4.4.0&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Upgrade cuDNN&lt;/strong&gt; on your system to version 9.2 or above.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are always welcome!&lt;/p&gt; &#xA;&lt;p&gt;Shoutout to &lt;a href=&#34;https://github.com/stevenlafl&#34;&gt;Steven Linn&lt;/a&gt; for providing docker support.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KoljaB/RealtimeSTT?tab=MIT-1-ov-file&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;Kolja Beigel&lt;br&gt; Email: &lt;a href=&#34;mailto:kolja.beigel@web.de&#34;&gt;kolja.beigel@web.de&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/KoljaB/RealtimeSTT&#34;&gt;GitHub&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dnhkng/GlaDOS</title>
    <updated>2025-01-19T01:45:10Z</updated>
    <id>tag:github.com,2025-01-19:/dnhkng/GlaDOS</id>
    <link href="https://github.com/dnhkng/GlaDOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the Personality Core for GLaDOS, the first steps towards a real-life implementation of the AI from the Portal series by Valve.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/9828&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/9828&#34; alt=&#34;dnhkng%2FGlaDOS | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;GLaDOS Personality Core&lt;/h1&gt; &#xA;&lt;p&gt;This is a project dedicated to building a real-life version of GLaDOS!&lt;/p&gt; &#xA;&lt;p&gt;NEW: If you want to chat or join the community, &lt;a href=&#34;https://discord.com/invite/ERTDKwpjNB&#34;&gt;Join our discord!&lt;/a&gt; If you want to support, &lt;a href=&#34;https://ko-fi.com/dnhkng&#34;&gt;sponsor the project here!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/c22049e4-7fba-4e84-8667-2c6657a656a0&#34;&gt;https://github.com/user-attachments/assets/c22049e4-7fba-4e84-8667-2c6657a656a0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update 3-1-2025 &lt;em&gt;Got GLaDOS running on an 8Gb SBC!&lt;/em&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/99e599bb-4701-438a-a311-8e6cd595796c&#34;&gt;https://github.com/user-attachments/assets/99e599bb-4701-438a-a311-8e6cd595796c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is really tricky, so only for hardcore geeks! Checkout the &#39;rock5b&#39; branch, and my OpenAI API for the &lt;a href=&#34;https://github.com/dnhkng/RKLLM-Gradio&#34;&gt;RK3588 NPU system&lt;/a&gt; Don&#39;t expect support for this, it&#39;s in active development, and requires lots of messing about in armbian linux etc.&lt;/p&gt; &#xA;&lt;h2&gt;Goals&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;This is a hardware and software project that will create an aware, interactive, and embodied GLaDOS.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will entail:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train GLaDOS voice generator&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generate a prompt that leads to a realistic &#34;Personality Core&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generate a medium- and long-term memory for GLaDOS (Probably a custom vector DB in a simpy Numpy array!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Give GLaDOS vision via a VLM (either a full VLM for everything, or a &#39;vision module&#39; using a tiny VLM the GLaDOS can function call!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create 3D-printable parts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Design the animatronics system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Software Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The initial goals are to develop a low-latency platform, where GLaDOS can respond to voice interactions within 600ms.&lt;/p&gt; &#xA;&lt;p&gt;To do this, the system constantly records data to a circular buffer, waiting for &lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;&gt;voice to be detected&lt;/a&gt;. When it&#39;s determined that the voice has stopped (including detection of normal pauses), it will be &lt;a href=&#34;https://github.com/huggingface/distil-whisper&#34;&gt;transcribed quickly&lt;/a&gt;. This is then passed to streaming &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;local Large Language Model&lt;/a&gt;, where the streamed text is broken by sentence, and passed to a &lt;a href=&#34;https://github.com/rhasspy/piper&#34;&gt;text-to-speech system&lt;/a&gt;. This means further sentences can be generated while the current is playing, reducing latency substantially.&lt;/p&gt; &#xA;&lt;h3&gt;Subgoals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The other aim of the project is to minimize dependencies, so this can run on constrained hardware. That means no PyTorch or other large packages.&lt;/li&gt; &#xA; &lt;li&gt;As I want to fully understand the system, I have removed a large amount of redirection: which means extracting and rewriting code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hardware System&lt;/h2&gt; &#xA;&lt;p&gt;This will be based on servo- and stepper-motors. 3D printable STL will be provided to create GlaDOS&#39;s body, and she will be given a set of animations to express herself. The vision system will allow her to track and turn toward people and things of interest.&lt;/p&gt; &#xA;&lt;h1&gt;Installation Instruction&lt;/h1&gt; &#xA;&lt;p&gt;Try this simplified process, but be aware it&#39;s still in the experimental stage! For all operating systems, you&#39;ll first need to install Ollama to run the LLM.&lt;/p&gt; &#xA;&lt;h2&gt;Install Drivers in necessary&lt;/h2&gt; &#xA;&lt;p&gt;If you are an Nvidia system with CUDA, make sure you install the necessary drivers and CUDA, info here: &lt;a href=&#34;https://onnxruntime.ai/docs/install/&#34;&gt;https://onnxruntime.ai/docs/install/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are using another accelerator (ROCm, DirectML etc.), after following the instructions below for you platform, follow up with installing the &lt;a href=&#34;https://onnxruntime.ai/docs/install/&#34;&gt;best onnxruntime version&lt;/a&gt; for your system.&lt;/p&gt; &#xA;&lt;h2&gt;Set up a local LLM server:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download and install &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt; for your operating system.&lt;/li&gt; &#xA; &lt;li&gt;Once installed, download a small 2B model for testing, at a terminal or command prompt use: &lt;code&gt;ollama pull llama3.2&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: You can use any OpenAI or Ollama compatible server, local or cloud based. Just edit the glados_config.yaml and update the completion_url, model and the api_key if necessary.&lt;/p&gt; &#xA;&lt;h2&gt;Windows Installation Process&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the Microsoft Store, search for &lt;code&gt;python&lt;/code&gt; and install Python 3.12&lt;/li&gt; &#xA; &lt;li&gt;Download this repository, either: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download and unzip this repository somewhere in your home folder, or&lt;/li&gt; &#xA;   &lt;li&gt;If you have Git set up, &lt;code&gt;git clone&lt;/code&gt; this repository using &lt;code&gt;git clone github.com/dnhkng/glados.git&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;In the repository folder, run the &lt;code&gt;install_windows.bat&lt;/code&gt;, and wait until the installation in complete.&lt;/li&gt; &#xA; &lt;li&gt;Double click &lt;code&gt;start_windows.bat&lt;/code&gt; to start GLaDOS!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;macOS Installation Process&lt;/h2&gt; &#xA;&lt;p&gt;This is still experimental. Any issues can be addressed in the Discord server. If you create an issue related to this, you will be referred to the Discord server. Note: I was getting Segfaults! Please leave feedback!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download this repository, either:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download and unzip this repository somewhere in your home folder, or&lt;/li&gt; &#xA;   &lt;li&gt;In a terminal, &lt;code&gt;git clone&lt;/code&gt; this repository using &lt;code&gt;git clone github.com/dnhkng/glados.git&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In a terminal, go to the repository folder and run these commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  chmod +x install_mac.command&#xA;  chmod +x start_mac.command&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the Finder, double click &lt;code&gt;install_mac.command&lt;/code&gt;, and wait until the installation in complete.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Double click &lt;code&gt;start_mac.command&lt;/code&gt; to start GLaDOS!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Linux Installation Process&lt;/h2&gt; &#xA;&lt;p&gt;This is still experimental. Any issues can be addressed in the Discord server. If you create an issue related to this, you will be referred to the Discord server. This has been tested on Ubuntu 24.04.1 LTS&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the PortAudio library, if you don&#39;t yet have it installed:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  sudo apt update&#xA;  sudo apt install libportaudio2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download this repository, either:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download and unzip this repository somewhere in your home folder, or&lt;/li&gt; &#xA;   &lt;li&gt;In a terminal, &lt;code&gt;git clone&lt;/code&gt; this repository using &lt;code&gt;git clone github.com/dnhkng/glados.git&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In a terminal, go to the repository folder and run these commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  chmod +x install_ubuntu.sh&#xA;  chmod +x start_ubuntu.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the a terminal in the GLaODS folder, run &lt;code&gt;./install_ubuntu.sh&lt;/code&gt;, and wait until the installation in complete.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;./start_ubuntu.sh&lt;/code&gt; to start GLaDOS!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Changing the LLM Model&lt;/h2&gt; &#xA;&lt;p&gt;To use other models, use the command: &lt;code&gt;ollama pull {modelname}&lt;/code&gt; and then add {modelname} to glados_config.yaml as the model. You can find &lt;a href=&#34;https://ollama.com/library&#34;&gt;more models here!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Common Issues&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If you find you are getting stuck in loops, as GLaDOS is hearing herself speak, you have two options: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Solve this by upgrading your hardware. You need to you either headphone, so she can&#39;t physically hear herself speak, or a conference-style room microphone/speaker. These have hardware sound cancellation, and prevent these loops.&lt;/li&gt; &#xA;   &lt;li&gt;Disable voice interruption. This means neither you nor GLaDOS can interrupt when GLaDOS is speaking. To accomplish this, edit the &lt;code&gt;glados_config.yaml&lt;/code&gt;, and change &lt;code&gt;interruptible:&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you want to the the Text UI, you should use the glados-ui.py file instead of glado.py&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Testing the submodules&lt;/h2&gt; &#xA;&lt;p&gt;You can test the systems by exploring the &#39;demo.ipynb&#39;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#dnhkng/GlaDOS&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=dnhkng/GlaDOS&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>