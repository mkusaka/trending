<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-30T02:04:05Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>haotian-liu/LLaVA</title>
    <updated>2023-07-30T02:04:05Z</updated>
    <id>tag:github.com,2023-07-30:/haotian-liu/LLaVA</id>
    <link href="https://github.com/haotian-liu/LLaVA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large Language-and-Vision Assistant built towards multimodal GPT-4 level capabilities.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸŒ‹ LLaVA: Large Language and Vision Assistant&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Visual instruction tuning towards large language and vision models with GPT-4 level capabilities.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Data.md&#34;&gt;Data&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual Instruction Tuning&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://hliu.cc&#34;&gt;Haotian Liu*&lt;/a&gt;, &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&amp;amp;hl=en/&#34;&gt;Qingyang Wu&lt;/a&gt;, &lt;a href=&#34;https://pages.cs.wisc.edu/~yongjaelee/&#34;&gt;Yong Jae Lee&lt;/a&gt; (*Equal Contribution)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png&#34; width=&#34;50%&#34;&gt;&lt;/a&gt; &lt;br&gt; Generated by &lt;a href=&#34;https://gligen.github.io/&#34;&gt;GLIGEN&lt;/a&gt; via &#34;a cute lava llama with glasses&#34; and box prompt &lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[7/19] ðŸ”¥ We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_Bench.md&#34;&gt;LLaVA Bench&lt;/a&gt; for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_from_LLaMA2.md&#34;&gt;LLaVA-from-LLaMA-2&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Release_Notes.md#7192023&#34;&gt;release notes&lt;/a&gt;, and our &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;model zoo&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[6/26] &lt;a href=&#34;https://vlp-tutorial.github.io/&#34;&gt;CVPR 2023 Tutorial&lt;/a&gt; on &lt;strong&gt;Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4&lt;/strong&gt;! Please check out [&lt;a href=&#34;https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf&#34;&gt;Slides&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2306.14895&#34;&gt;Notes&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/mkI7EPD1vp8&#34;&gt;YouTube&lt;/a&gt;] [&lt;a href=&#34;https://www.bilibili.com/video/BV1Ng4y1T7v3/&#34;&gt;Bilibli&lt;/a&gt;].&lt;/li&gt; &#xA; &lt;li&gt;[6/11] We released the preview for the mostly requested feature: DeepSpeed and LoRA support! Please see documentations &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/docs/LoRA.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[6/1] We released &lt;strong&gt;LLaVA-Med: Large Language and Vision Assistant for Biomedicine&lt;/strong&gt;, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[5/13] Interested in quantifying the emerged &lt;strong&gt;zero-shot OCR&lt;/strong&gt; performance of LLaVA and open-sourced LMM? Please check out the paper &lt;a href=&#34;https://arxiv.org/abs/2305.07895&#34;&gt;&#34;On the Hidden Mystery of OCR in Large Multimodal Models&#34;&lt;/a&gt;, where LLaVA consistently outperforms miniGPT4 on 17 out of 18 datasets, despite LlaVA being trained with an order of magnitude smaller training data.&lt;/li&gt; &#xA; &lt;li&gt;[5/6] We are releasing &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;LLaVA-Lighting-MPT-7B-preview&lt;/a&gt;, based on MPT-7B-Chat! See &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#LLaVA-MPT-7b&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[5/2] ðŸ”¥ We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#train-llava-lightning&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[5/2] We upgrade LLaVA package to v0.1 to support Vicuna v0 and v1 checkpoints, please upgrade following instructions &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[4/30] Our checkpoint with Vicuna-7b-v0 has been released &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-7b&#34;&gt;here&lt;/a&gt;! This checkpoint is more accessible and device friendly. Stay tuned for a major upgrade next week!&lt;/li&gt; &#xA; &lt;li&gt;[4/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[4/17] ðŸ”¥ We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;assets/demo.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt; --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-weights&#34;&gt;LLaVA Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#Demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Data.md&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#train&#34;&gt;Train&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to LLaVA folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/haotian-liu/LLaVA.git&#xA;cd LLaVA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n llava python=3.10 -y&#xA;conda activate llava&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install additional packages for training cases&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install ninja&#xA;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upgrade to latest code base&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git pull&#xA;pip uninstall transformers&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LLaVA Weights&lt;/h2&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt; for all public LLaVA checkpoints, and the instructions of how to use the weights.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;To run our demo, you need to prepare LLaVA checkpoints locally. Please follow the instructions &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-weights&#34;&gt;here&lt;/a&gt; to download the checkpoints.&lt;/p&gt; &#xA;&lt;h3&gt;Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;To launch a Gradio demo locally, please run the following commands one by one. If you plan to launch multiple model workers to compare between different checkpoints, you only need to launch the controller and the web server &lt;em&gt;ONCE&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You just launched the Gradio web interface. Now, you can open the web interface with the URL printed on the screen. You may notice that there is no model in the model list. Do not worry, as we have not launched any model worker yet. It will be automatically updated when you launch a model worker.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;p&gt;This is the actual &lt;em&gt;worker&lt;/em&gt; that performs the inference on the GPU. Each worker is responsible for a single model specified in &lt;code&gt;--model-path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;. Now, refresh your Gradio web UI, and you will see the model you just launched in the model list.&lt;/p&gt; &#xA;&lt;p&gt;You can launch as many workers as you want, and compare between different model checkpoints in the same Gradio interface. Please keep the &lt;code&gt;--controller&lt;/code&gt; the same, and modify the &lt;code&gt;--port&lt;/code&gt; and &lt;code&gt;--worker&lt;/code&gt; to a different port number for each worker.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port &amp;lt;different from 40000, say 40001&amp;gt; --worker http://localhost:&amp;lt;change accordingly, i.e. 40001&amp;gt; --model-path &amp;lt;ckpt2&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker (Multiple GPUs, when GPU VRAM &amp;lt;= 24GB)&lt;/h4&gt; &#xA;&lt;p&gt;If your the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs. Our latest code base will automatically try to use multiple GPUs if you have more than one GPU. You can specify which GPUs to use with &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;. Below is an example of running with the first two GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker (4-bit, 8-bit inference, quantized)&lt;/h4&gt; &#xA;&lt;p&gt;You can launch the model worker with quantized bits (4-bit, 8-bit), which allows you to run the inference with reduced GPU memory footprint, potentially allowing you to run on a GPU with as few as 12GB VRAM. Note that inference with quantized bits may not be as accurate as the full-precision model. Simply append &lt;code&gt;--load-4bit&lt;/code&gt; or &lt;code&gt;--load-8bit&lt;/code&gt; to the &lt;strong&gt;model worker&lt;/strong&gt; command that you are executing. Below is an example of running with 4-bit quantization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-llama-2-13b-chat-lightning-preview --load-4bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;p&gt;Chat about images using LLaVA without the need of Gradio interface. It also supports multiple GPUs, 4-bit and 8-bit quantized inference. With 4-bit quantization, for our LLaVA-Lightning-MPT-7B, it uses less than 8GB VRAM on a single GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.cli \&#xA;    --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview \&#xA;    --image-file &#34;https://llava-vl.github.io/static/images/view.jpg&#34; \&#xA;    --load-4bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/demo_cli.gif&#34; width=&#34;70%&#34;&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;p&gt;LLaVA training consists of two stages: (1) feature alignment stage: use approximately 600K filtered CC3M to connect a &lt;em&gt;frozen pretrained&lt;/em&gt; vision encoder to a &lt;em&gt;frozen LLM&lt;/em&gt;; (2) visual instruction tuning stage: use 150K GPT-generated multimodal instruction-following to teach the model to follow multimodal instructions.&lt;/p&gt; &#xA;&lt;p&gt;LLaVA is trained on 8 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly. Always keep the global batch size the same: &lt;code&gt;per_device_train_batch_size&lt;/code&gt; x &lt;code&gt;gradient_accumulation_steps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;We use a similar set of hyperparameters as Vicuna in finetuning. Both hyperparameters used in pretraining and finetuning are provided below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Prepare Vicuna checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;Before you start, prepare our base model Vicuna, which is an instruction-tuned chatbot. Please download its weights &lt;a href=&#34;https://github.com/lm-sys/FastChat#model-weights&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Vicuna has two versions: v0 and v1, the main difference between them is the prompt of format. We support both. To ensure the best performance, you need to specify the correct prompt version corresponding to the weights you download: &lt;code&gt;v0&lt;/code&gt; for &lt;code&gt;v0&lt;/code&gt; weights, and &lt;code&gt;v1&lt;/code&gt; for all Vicuna &lt;code&gt;v1.x&lt;/code&gt; models.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrain (feature alignment)&lt;/h3&gt; &#xA;&lt;p&gt;Please download the subset of the CC3M dataset we use in the paper &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Pretrain takes around 4 hours for LLaVA-13B on 8x A100 (80G). It takes around 2 hours for 7B checkpoints.&lt;/p&gt; &#xA;&lt;p&gt;We recommend training with DeepSpeed as it can save a lot of GPU RAM. We provide training script with DeepSpeed &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/scripts/pretrain.sh&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may run this with a single A100 GPU with the following code. Please note that the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; * &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; should be equal to 128 to keep the global batch size the same.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain: LLaVA-13B, 1x A100 (80G). Time: ~33 hours.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/vicuna-13b \&#xA;    --version [v0 or v1] \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k_images \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end False \&#xA;    --mm_use_im_patch_token False \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Visual Instruction Tuning&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare data&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Please download the annotation of our instruction tuning data &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json&#34;&gt;llava_instruct_158k.json&lt;/a&gt;, and download the COCO train2017 images &lt;a href=&#34;https://cocodataset.org/#download&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Start training!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You may download our pretrained projectors in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. It is not recommended to use legacy projectors, as they may be trained with a different version of the codebase, and if any option is off, the model will not function/train as we expected.&lt;/p&gt; &#xA;&lt;p&gt;When we initially released our paper, we used a full 3-epoch schedule on the LLaVA-Instruct-158K dataset. The scripts are provided &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/scripts/finetune_full_schedule.sh&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In our later exploration, we introduced LLaVA-Lightning, as we find that a much faster 1-epoch schedule on LLaVA-Instruct-80K can achieve fast convergence and good performance. With LLaVA Lightning, we are able to train, validate, and release LLaVA-LLaMA-2 checkpoints preview on the same day as LLaMA-2 release. If you are interested to learn more about LLaVA Lightning, please continue to the following section.&lt;/p&gt; &#xA;&lt;h3&gt;Lightning&lt;/h3&gt; &#xA;&lt;p&gt;LLaVA-Lightning can be trained on 8x A100 GPUs in just 3 hours, including both pretraining and finetuning. When using spot instances, it costs just ~$40.&lt;/p&gt; &#xA;&lt;p&gt;For LLaVA Lightning, we create two distilled subset to ensure both a broad concept coverage, and the efficiency in training. Furthermore, we only perform instruction tuning for 1 epoch, in contrast to 3 epochs in the paper. We find such schedule is effctive and can achieve fast convergence and good performance.&lt;/p&gt; &#xA;&lt;p&gt;For pretraining, we create a concept-balanced subset of LAION-CC-SBU. It consists of 558K images. Download data &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/tree/main&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For instruction tuning, we create a subset of LLaVA-Instruct-150K. It consists of 80K image-instruction pairs, consisting of 40K conversation and 40K complex reasoning data, with non-overlapping images. Download &lt;code&gt;llava_instruct_80k.json&lt;/code&gt; &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_80k.json&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Hyperparameters&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining (&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/scripts/pretrain.sh&#34;&gt;script&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Lightning&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Visual Instruction Tuning (&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/scripts/finetune.sh&#34;&gt;script&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Lightning&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;LLaVA-MPT-7b&lt;/h4&gt; &#xA;&lt;p&gt;Thanks to LLaVA-Lightning, we are able to train a checkpoint based on MPT-7B-Chat on 8x A100 GPUs in just 3 hours, including both pretraining and finetuning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This is a research preview of the LLaVA-Lightning based on MPT-7B-chat checkpoint. The usage of the model should comply with MPT-7B-chat license and agreements.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Usage&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You do not need to download our checkpoint, it will directly load from our Hugging Face model: &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;&lt;code&gt;liuhaotian/LLaVA-Lightning-MPT-7B-preview&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000&#xA;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview&#xA;python -m llava.serve.gradio_web_server --controller http://localhost:10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We use the same set of training dataset, and the hyperparameters as other &lt;em&gt;Lightning&lt;/em&gt; checkpoints.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;GPT-assisted Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Our GPT-assisted evaluation pipeline for multimodal modeling is provided for a comprehensive understanding of the capabilities of vision-language models. Please see our paper for more details.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate LLaVA responses&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python model_vqa.py \&#xA;    --model-path ./checkpoints/LLaVA-13B-v0 \&#xA;    --question-file \&#xA;    playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --image-folder \&#xA;    /path/to/coco2014_val \&#xA;    --answers-file \&#xA;    /path/to/answer-file-our.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Evaluate the generated responses. In our case, &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/playground/data/coco2014_val_qa_eval/qa90_gpt4_answer.jsonl&#34;&gt;&lt;code&gt;answer-file-ref.jsonl&lt;/code&gt;&lt;/a&gt; is the response generated by text-only GPT-4 (0314), with the context captions/boxes provided.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;OPENAI_API_KEY=&#34;sk-***********************************&#34; python llava/eval/eval_gpt_review_visual.py \&#xA;    --question playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --context llava/eval/table/caps_boxes_coco2014_val_80.jsonl \&#xA;    --answer-list \&#xA;    /path/to/answer-file-ref.jsonl \&#xA;    /path/to/answer-file-our.jsonl \&#xA;    --rule llava/eval/table/rule.json \&#xA;    --output /path/to/review.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Summarize the evaluation results&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python summarize_gpt_review.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ScienceQA&lt;/h2&gt; &#xA;&lt;p&gt;Please check out the documentation &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/ScienceQA.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find LLaVA useful for your your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{liu2023llava,&#xA;      title={Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},&#xA;      publisher={arXiv:2304.08485},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Otter: In-Context Multi-Modal Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For future project ideas, pleae check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/llama-recipes</title>
    <updated>2023-07-30T02:04:05Z</updated>
    <id>tag:github.com,2023-07-30:/facebookresearch/llama-recipes</id>
    <link href="https://github.com/facebookresearch/llama-recipes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Examples and recipes for Llama 2 model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama 2 Fine-tuning / Inference Recipes and Examples&lt;/h1&gt; &#xA;&lt;p&gt;The &#39;llama-recipes&#39; repository is a companion to the &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2 model&lt;/a&gt;. The goal of this repository is to provide examples to quickly get started with fine-tuning for domain adaptation and how to run inference for the fine-tuned models. For ease of use, the examples use Hugging Face converted versions of the models. See steps for conversion of the model &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#model-conversion-to-hugging-face&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Llama 2 is a new technology that carries potential risks with use. Testing conducted to date has not â€” and could not â€” cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/Responsible-Use-Guide.pdf&#34;&gt;Responsible Use Guide&lt;/a&gt;. More details can be found in our research paper as well. For downloading the models, follow the instructions on &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2 repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#quick-start&#34;&gt;Quick start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#model-conversion-to-hugging-face&#34;&gt;Model Conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#single-gpu&#34;&gt;Single GPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#multiple-gpus-one-node&#34;&gt;Multi GPU One Node&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#multi-gpu-multi-node&#34;&gt;Multi GPU Multi Node&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/inference.md&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#repository-organization&#34;&gt;Repository Organization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/#license&#34;&gt;License and Acceptable Use Policy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/quickstart.ipynb&#34;&gt;Llama 2 Jupyter Notebook&lt;/a&gt;: This jupyter notebook steps you through how to finetune a Llama 2 model on the text summarization task using the &lt;a href=&#34;https://huggingface.co/datasets/samsum&#34;&gt;samsum&lt;/a&gt;. The notebook uses parameter efficient finetuning (PEFT) and int8 quantization to finetune a 7B on a single GPU like an A10 with 24GB gpu memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; All the setting defined in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/&#34;&gt;config files&lt;/a&gt; can be passed as args through CLI when running the script, there is no need to change from config files directly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; In case need to run PEFT model with FSDP, please make sure to use the PyTorch Nightlies.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For more in depth information checkout the following:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/single_gpu.md&#34;&gt;Single GPU Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/mutli_gpu.md&#34;&gt;Multi-GPU Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/LLM_finetuning.md&#34;&gt;LLM Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/Dataset.md&#34;&gt;Adding custom datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/inference.md&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/FAQ.md&#34;&gt;FAQs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To run the examples, make sure to install the requirements using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;pip install -r requirements.txt&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that the above requirements.txt will install PyTorch 2.0.1 version, in case you want to run FSDP + PEFT, please make sure to install PyTorch nightlies.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Model conversion to Hugging Face&lt;/h1&gt; &#xA;&lt;p&gt;The recipes and notebooks in this folder are using the Llama 2 model definition provided by Hugging Face&#39;s transformers library.&lt;/p&gt; &#xA;&lt;p&gt;Given that the original checkpoint resides under models/7B you can install all requirements and convert the checkpoint with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## Install HuggingFace Transformers from source&#xA;pip freeze | grep transformers ## verify it is version 4.31.0 or higher&#xA;&#xA;```bash&#xA;git clone git@github.com:huggingface/transformers.git&#xA;cd transformers&#xA;pip install protobuf&#xA;python src/transformers/models/llama/convert_llama_weights_to_hf.py \&#xA;   --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Fine-tuning&lt;/h1&gt; &#xA;&lt;p&gt;For fine-tuning Llama 2 models for your domain-specific use cases recipes for PEFT, FSDP, PEFT+FSDP have been included along with a few test datasets. For details see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/LLM_finetuning.md&#34;&gt;LLM Fine-tuning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Single and Multi GPU Finetune&lt;/h2&gt; &#xA;&lt;p&gt;If you want to dive right into single or multi GPU fine-tuning, run the examples below on a single GPU like A10, T4, V100, A100 etc. All the parameters in the examples and recipes below need to be further tuned to have desired results based on the model, method, data and task at hand.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To change the dataset in the commands below pass the &lt;code&gt;dataset&lt;/code&gt; arg. Current options for dataset are &lt;code&gt;grammar_dataset&lt;/code&gt;, &lt;code&gt;alpaca_dataset&lt;/code&gt;and &lt;code&gt;samsum_dataset&lt;/code&gt;. A description of the datasets and how to add custom datasets can be found in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/Dataset.md&#34;&gt;Dataset.md&lt;/a&gt;. For &lt;code&gt;grammar_dataset&lt;/code&gt;, &lt;code&gt;alpaca_dataset&lt;/code&gt; please make sure you use the suggested instructions from &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/single_gpu.md#how-to-run-with-different-datasets&#34;&gt;here&lt;/a&gt; to set them up.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Default dataset and other LORA config has been set to &lt;code&gt;samsum_dataset&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure to set the right path to the model in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/training.py&#34;&gt;training config&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Single GPU:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#if running on multi-gpu machine&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;&#xA;python llama_finetuning.py  --use_peft --peft_method lora --quantization --model_name /patht_of_model_folder/7B --output_dir Path/to/save/PEFT/model&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we make use of Parameter Efficient Methods (PEFT) as described in the next section. To run the command above make sure to pass the &lt;code&gt;peft_method&lt;/code&gt; arg which can be set to &lt;code&gt;lora&lt;/code&gt;, &lt;code&gt;llama_adapter&lt;/code&gt; or &lt;code&gt;prefix&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; if you are running on a machine with multiple GPUs please make sure to only make one of them visible using &lt;code&gt;export CUDA_VISIBLE_DEVICES=GPU:id&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make sure you set &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/training.py&#34;&gt;save_model&lt;/a&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/training.py&#34;&gt;training.py&lt;/a&gt; to save the model. Be sure to check the other training settings in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/training.py&#34;&gt;train config&lt;/a&gt; as well as others in the config folder as needed or they can be passed as args to the training script as well.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multiple GPUs One Node:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; please make sure to use PyTorch Nightlies for using PEFT+FSDP. Also, note that int8 quantization from bit&amp;amp;bytes currently is not supported in FSDP.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;torchrun --nnodes 1 --nproc_per_node 4  llama_finetuning.py --enable_fsdp --use_peft --peft_method lora --model_name /patht_of_model_folder/7B --pure_bf16 --output_dir Path/to/save/PEFT/model&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we use FSDP as discussed in the next section which can be used along with PEFT methods. To make use of PEFT methods with FSDP make sure to pass &lt;code&gt;use_peft&lt;/code&gt; and &lt;code&gt;peft_method&lt;/code&gt; args along with &lt;code&gt;enable_fsdp&lt;/code&gt;. Here we are using &lt;code&gt;BF16&lt;/code&gt; for training.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuning using FSDP Only&lt;/h3&gt; &#xA;&lt;p&gt;If you are interested in running full parameter fine-tuning without making use of PEFT methods, please use the following command. Make sure to change the &lt;code&gt;nproc_per_node&lt;/code&gt; to your available GPUs. This has been tested with &lt;code&gt;BF16&lt;/code&gt; on 8xA100, 40GB GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;torchrun --nnodes 1 --nproc_per_node 8  llama_finetuning.py --enable_fsdp --model_name /patht_of_model_folder/7B --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multi GPU Multi Node:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;sbatch multi_node.slurm&#xA;# Change the num nodes and GPU per nodes in the script before running.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can read more about our fine-tuning strategies &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/LLM_finetuning.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Repository Organization&lt;/h1&gt; &#xA;&lt;p&gt;This repository is organized in the following way:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/configs/&#34;&gt;configs&lt;/a&gt;: Contains the configuration files for PEFT methods, FSDP, Datasets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/docs/&#34;&gt;docs&lt;/a&gt;: Example recipes for single and multi-gpu fine-tuning recipes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/ft_datasets/&#34;&gt;ft_datasets&lt;/a&gt;: Contains individual scripts for each dataset to download and process. Note: Use of any of the datasets should be in compliance with the dataset&#39;s underlying licenses (including but not limited to non-commercial uses)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/inference/&#34;&gt;inference&lt;/a&gt;: Includes examples for inference for the fine-tuned models and how to use them safely.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/model_checkpointing/&#34;&gt;model_checkpointing&lt;/a&gt;: Contains FSDP checkpoint handlers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/policies/&#34;&gt;policies&lt;/a&gt;: Contains FSDP scripts to provide different policies, such as mixed precision, transformer wrapping policy and activation checkpointing along with any precision optimizer (used for running FSDP with pure bf16 mode).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/utils/&#34;&gt;utils&lt;/a&gt;: Utility files for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;train_utils.py&lt;/code&gt; provides training/eval loop and more train utils.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;dataset_utils.py&lt;/code&gt; to get preprocessed datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;config_utils.py&lt;/code&gt; to override the configs received from CLI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;fsdp_utils.py&lt;/code&gt; provides FSDP wrapping policy for PEFT methods.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;memory_utils.py&lt;/code&gt; context manager to track different memory stats in train loop.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;See the License file &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/LICENSE&#34;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/USE_POLICY.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dabeaz-course/python-mastery</title>
    <updated>2023-07-30T02:04:05Z</updated>
    <id>tag:github.com,2023-07-30:/dabeaz-course/python-mastery</id>
    <link href="https://github.com/dabeaz-course/python-mastery" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Advanced Python Mastery (course by @dabeaz)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Advanced Python Mastery&lt;/h1&gt; &#xA;&lt;p&gt;A course by David Beazley (&lt;a href=&#34;https://www.dabeaz.com&#34;&gt;https://www.dabeaz.com&lt;/a&gt;)&lt;br&gt; Copyright (C) 2007-2023&lt;/p&gt; &#xA;&lt;h2&gt;Synopsis&lt;/h2&gt; &#xA;&lt;p&gt;An exercise-driven course on Advanced Python Programming that was battle-tested several hundred times on the corporate-training circuit for more than a decade. Written by David Beazley, author of the Python Cookbook, 3rd Edition (O&#39;Reilly) and Python Distilled (Addison-Wesley). Released under a Creative Commons license. Free of ads, tracking, pop-ups, newsletters, and AI.&lt;/p&gt; &#xA;&lt;h2&gt;Target Audience&lt;/h2&gt; &#xA;&lt;p&gt;This course is for Python programmers who want to move beyond short scripts to writing more sophisticated programs. Topics focus on programming techniques that get used in popular libraries and frameworks. The primary goal is to better understand the Python language itself so that you can understand other people&#39;s code and so that you can apply your newfound knowledge to your own projects.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;You already know some Python. This is not a course for beginners. For more introductory material, you might consider the &lt;a href=&#34;https://dabeaz-course.github.io/practical-python&#34;&gt;Practical Python Programming&lt;/a&gt; course.&lt;/p&gt; &#xA;&lt;h2&gt;How to Take the Course&lt;/h2&gt; &#xA;&lt;p&gt;To take the course, you should first fork/clone the GitHub repo to your own machine.&lt;/p&gt; &#xA;&lt;p&gt;It is assumed that you are working locally in a proper Python development environment. That means a proper installation of Python, an editor/IDE, and whatever other tools that you would normally install to work on Python. Due to the use of multiple files and module imports, the use of Notebooks is not recommended.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dabeaz-course/python-mastery/main/PythonMastery.pdf&#34;&gt;&lt;code&gt;PythonMastery.pdf&lt;/code&gt;&lt;/a&gt; file contains detailed presentation slides. Course exercises and suggested timings are clearly indicated. You&#39;ll want to keep this by your side (I recommend downloading and viewing it with a local PDF viewer). Start here!&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dabeaz-course/python-mastery/main/Exercises/index.md&#34;&gt;Exercises/&lt;/a&gt; directory has all of the course exercises.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dabeaz-course/python-mastery/main/Solutions/&#34;&gt;Solutions/&lt;/a&gt; directory has fully worked out solution code.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dabeaz-course/python-mastery/main/Data/&#34;&gt;Data/&lt;/a&gt; directory has some datafiles used during the course.&lt;/p&gt; &#xA;&lt;p&gt;The course was originally taught over 4-5 days in an in-person classroom setting with a mix of lecture and hands-on exercises. Successful completion of the course will likely require 30-50 hours of work. Exercises tend to build upon each other. Solutions are always provided in case you get stuck.&lt;/p&gt; &#xA;&lt;h2&gt;Supplemental Material&lt;/h2&gt; &#xA;&lt;p&gt;The Advanced Python Mastery course often suggested more in-depth tutorials on selected topics. These were presented at the PyCon conference and might be of interest:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dabeaz.com/generators/&#34;&gt;Generator Tricks for Systems Programmers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://dabeaz.com/coroutines/index.html&#34;&gt;A Curious Course on Coroutines and Concurrency&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dabeaz.com/py3meta/index.html&#34;&gt;Python3 Metaprogramming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dabeaz.com/finalgenerator/index.html&#34;&gt;Generators: The Final Frontier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dabeaz.com/modulepackage/index.html&#34;&gt;Modules and Packages: Live and Let Die&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Questions and Answers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Are any videos available?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No. You will be able to more quickly read the presentation slides which contain technical information. However, the &lt;a href=&#34;https://www.safaribooksonline.com/library/view/python-programming-language/9780134217314/&#34;&gt;Python Programming Language: LiveLessons&lt;/a&gt; video available on O&#39;Reilly&#39;s Safari site is closely related to the material in this course.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Can I use these materials in my own course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes. I just kindly ask that you give proper attribution.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Do you accept bug reports or pull requests?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; If you&#39;ve found a bug, please report it! However, I&#39;m not looking to expand or reorganize the course content with new topics or exercises.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Are the presentation slides available in any format other than PDF?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Is there any forum/chat where the course can be discussed?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; You can use &lt;a href=&#34;https://github.com/dabeaz-course/python-mastery/discussions&#34;&gt;GitHub discussions&lt;/a&gt; to discuss the course.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why wasn&#39;t topic/tool/library X covered?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The course was designed to be completed in an intense 4-day in-person format. It simply isn&#39;t possible to cover absolutely everything. As such, the course is focused primarily on the core Python language, not third party libraries or tooling.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why aren&#39;t features like typing, async, or pattern matching covered?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Mainly, it&#39;s an issue of calendar timing and scope. Course material was primarily developed pre-pandemic and represents Python as it was at that time. Some topics (e.g., typing or async) are sufficiently complex that they would be better covered on their own in a separate course.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why did you release the course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This course was extensively taught pre-pandemic. Post-pandemic, my teaching has shifted towards projects and CS fundamentals. However, why let a good course just languish on my computer?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: How can I help?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; If you like the course, the best way to support it is to tell other people about it.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; Advanced Python Mastery&lt;br&gt; &lt;code&gt;...&lt;/code&gt; A course by &lt;a href=&#34;https://www.dabeaz.com&#34;&gt;dabeaz&lt;/a&gt;&lt;br&gt; &lt;code&gt;...&lt;/code&gt; Copyright 2007-2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.creativecommons.org/l/by-sa/4.0/88x31.png&#34; alt=&#34;&#34;&gt;. This work is licensed under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>