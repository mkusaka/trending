<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-28T02:07:24Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pengxiao-song/LaWGPT</title>
    <updated>2023-05-28T02:07:24Z</updated>
    <id>tag:github.com,2023-05-28:/pengxiao-song/LaWGPT</id>
    <link href="https://github.com/pengxiao-song/LaWGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🎉 Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. 基于中文法律知识的大语言模型&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LaWGPT：基于中文法律知识的大语言模型&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/logo/lawgpt.jpeg&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/logo/lawgpt.jpeg&#34; width=&#34;80%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/wiki&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-Wiki-brightgreen&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-beta1.0-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-Linux-9cf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/pengxiao-song/lawgpt&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#pengxiao-song/LaWGPT&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/pengxiao-song/lawgpt?color=yellow&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;https://www.lamda.nju.edu.cn/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/support-NJU--LAMDA-9cf.svg&#34;&gt;&lt;/a&gt; --&gt; &lt;/p&gt; &#xA;&lt;p&gt;LaWGPT 是一系列基于中文法律知识的开源大语言模型。&lt;/p&gt; &#xA;&lt;p&gt;该系列模型在通用中文基座模型（如 Chinese-LLaMA、ChatGLM 等）的基础上扩充法律领域专有词表、&lt;strong&gt;大规模中文法律语料预训练&lt;/strong&gt;，增强了大模型在法律领域的基础语义理解能力。在此基础上，&lt;strong&gt;构造法律领域对话问答数据集、中国司法考试数据集进行指令精调&lt;/strong&gt;，提升了模型对法律内容的理解和执行能力。&lt;/p&gt; &#xA;&lt;p&gt;详细内容请参考&lt;a href=&#34;&#34;&gt;技术报告&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;本项目持续开展，法律领域数据集及系列模型后续相继开源，敬请关注。&lt;/p&gt; &#xA;&lt;h2&gt;更新&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;📣 2023/05/26：开放 &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/discussions&#34;&gt;Discussions 讨论区&lt;/a&gt;，欢迎朋友们交流探讨、提出意见、分享观点！&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🛠️ 2023/05/22：项目主分支结构调整，详见&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT#%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84&#34;&gt;项目结构&lt;/a&gt;；支持&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/raw/main/scripts/infer.sh&#34;&gt;命令行批量推理&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🪴 2023/05/15：发布 &lt;a href=&#34;https://github.com/pengxiao-song/awesome-chinese-legal-resources&#34;&gt;中文法律数据源汇总（Awesome Chinese Legal Resources）&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/raw/main/resources/legal_vocab.txt&#34;&gt;法律领域词表&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🌟 2023/05/13：公开发布 &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-Legal--Base--7B-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-LaWGPT--7B--beta1.0-yellow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Legal-Base-7B&lt;/strong&gt;：法律基座模型，使用 50w 中文裁判文书数据二次预训练&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;LaWGPT-7B-beta1.0&lt;/strong&gt;：法律对话模型，构造 30w 高质量法律问答数据集基于 Legal-Base-7B 指令精调&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🌟 2023/04/12：内部测试 &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-Lawgpt--7B--alpha-yellow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;LaWGPT-7B-alpha&lt;/strong&gt;：在 Chinese-LLaMA-7B 的基础上直接构造 30w 法律问答数据集指令精调&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;快速开始&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;准备代码，创建环境&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 下载代码&#xA;git clone git@github.com:pengxiao-song/LaWGPT.git&#xA;cd LaWGPT&#xA;&#xA;# 创建环境&#xA;conda create -n lawgpt python=3.10 -y&#xA;conda activate lawgpt&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;启动 web ui（可选，易于调节参数）&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;首先，执行服务启动脚本：&lt;code&gt;bash scripts/webui.sh&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;其次，访问 &lt;a href=&#34;http://127.0.0.1:7860&#34;&gt;http://127.0.0.1:7860&lt;/a&gt; ：&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p align=&#34;center&#34;&gt; &lt;img style=&#34;border-radius: 50%; box-shadow: 0 0 10px rgba(0,0,0,0.5); width: 80%;&#34; , src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-03.jpeg&#34;&gt; &lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;命令行推理（可选，支持批量测试）&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;首先，参考 &lt;code&gt;resources/example_infer_data.json&lt;/code&gt; 文件内容构造测试样本集；&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;其次，执行推理脚本：&lt;code&gt;bash scripts/infer.sh&lt;/code&gt;。其中 &lt;code&gt;--infer_data_path&lt;/code&gt; 参数为测试样本集路径，如果为空或者路径出错，则以交互模式运行。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;注意，以上步骤的默认模型为 LaWGPT-7B-alpha ，如果您想使用 LaWGPT-7B-beta1.0 模型：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;由于 &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-LLaMA&lt;/a&gt; 均未开源模型权重。根据相应开源许可，&lt;strong&gt;本项目只能发布 LoRA 权重&lt;/strong&gt;，无法发布完整的模型权重，请各位谅解。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;本项目给出&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/wiki/%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6&#34;&gt;合并方式&lt;/a&gt;，请各位获取原版权重后自行重构模型。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;项目结构&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;LaWGPT&#xA;├── assets    # 静态资源&#xA;├── resources # 项目资源&#xA;├── models    # 基座模型及 lora 权重&#xA;│   ├── base_models&#xA;│   └── lora_weights&#xA;├── outputs   # 指令微调的输出权重&#xA;├── data      # 实验数据&#xA;├── scripts   # 脚本目录&#xA;│   ├── finetune.sh # 指令微调脚本&#xA;│   └── webui.sh    # 启动服务脚本&#xA;├── templates # prompt 模板&#xA;├── tools     # 工具包&#xA;├── utils&#xA;├── train_clm.py  # 二次训练&#xA;├── finetune.py   # 指令微调&#xA;├── webui.py      # 启动服务&#xA;├── README.md&#xA;└── requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;数据构建&lt;/h2&gt; &#xA;&lt;p&gt;本项目基于中文裁判文书网公开法律文书数据、司法考试数据等数据集展开，详情参考&lt;a href=&#34;&#34;&gt;中文法律数据汇总&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;初级数据生成：根据 &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca#data-generation-process&#34;&gt;Stanford_alpaca&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct&lt;/a&gt; 方式生成对话问答数据&lt;/li&gt; &#xA; &lt;li&gt;知识引导的数据生成：通过 Knowledge-based Self-Instruct 方式基于中文法律结构化知识生成数据。&lt;/li&gt; &#xA; &lt;li&gt;引入 ChatGPT 清洗数据，辅助构造高质量数据集。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;模型训练&lt;/h2&gt; &#xA;&lt;p&gt;LawGPT 系列模型的训练过程分为两个阶段：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;第一阶段：扩充法律领域词表，在大规模法律文书及法典数据上预训练 Chinese-LLaMA&lt;/li&gt; &#xA; &lt;li&gt;第二阶段：构造法律领域对话问答数据集，在预训练模型基础上指令精调&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;二次训练流程&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;参考 &lt;code&gt;resources/example_instruction_train.json&lt;/code&gt; 构造二次训练数据集&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;code&gt;scripts/train_clm.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;指令精调步骤&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;参考 &lt;code&gt;resources/example_instruction_tune.json&lt;/code&gt; 构造指令微调数据集&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;code&gt;scripts/finetune.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;计算资源&lt;/h3&gt; &#xA;&lt;p&gt;8 张 Tesla V100-SXM2-32GB ：二次训练阶段耗时约 24h / epoch，微调阶段耗时约 12h / epoch&lt;/p&gt; &#xA;&lt;h2&gt;模型评估&lt;/h2&gt; &#xA;&lt;h3&gt;输出示例&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：酒驾撞人怎么判刑？&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/demo07.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：请给出判决意见。&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-05.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：请介绍赌博罪的定义。&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-06.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：请问加班工资怎么算？&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-04.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：民间借贷受国家保护的合法利息是多少?&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-02.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：欠了信用卡的钱还不上要坐牢吗？&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-01.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：你能否写一段抢劫罪罪名的案情描述？&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-03.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;局限性&lt;/h3&gt; &#xA;&lt;p&gt;由于计算资源、数据规模等因素限制，当前阶段 LawGPT 存在诸多局限性：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;数据资源有限、模型容量较小，导致其相对较弱的模型记忆和语言能力。因此，在面对事实性知识任务时，可能会生成不正确的结果。&lt;/li&gt; &#xA; &lt;li&gt;该系列模型只进行了初步的人类意图对齐。因此，可能产生不可预测的有害内容以及不符合人类偏好和价值观的内容。&lt;/li&gt; &#xA; &lt;li&gt;自我认知能力存在问题，中文理解能力有待增强。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;请诸君在使用前了解上述问题，以免造成误解和不必要的麻烦。&lt;/p&gt; &#xA;&lt;h2&gt;协作者&lt;/h2&gt; &#xA;&lt;p&gt;如下各位合作开展（按字母序排列）：&lt;a href=&#34;https://github.com/herobrine19&#34;&gt;@cainiao&lt;/a&gt;、&lt;a href=&#34;https://github.com/njuyxw&#34;&gt;@njuyxw&lt;/a&gt;、&lt;a href=&#34;https://github.com/pengxiao-song&#34;&gt;@pengxiao-song&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;请各位严格遵守如下约定：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;本项目任何资源&lt;strong&gt;仅供学术研究使用，严禁任何商业用途&lt;/strong&gt;。&lt;/li&gt; &#xA; &lt;li&gt;模型输出受多种不确定性因素影响，本项目当前无法保证其准确性，&lt;strong&gt;严禁用于真实法律场景&lt;/strong&gt;。&lt;/li&gt; &#xA; &lt;li&gt;本项目不承担任何法律责任，亦不对因使用相关资源和输出结果而可能产生的任何损失承担责任。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;问题反馈&lt;/h2&gt; &#xA;&lt;p&gt;如有问题，请在 GitHub Issue 中提交。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;提交问题之前，建议查阅 FAQ 及以往的 issue 看是否能解决您的问题。&lt;/li&gt; &#xA; &lt;li&gt;请礼貌讨论，构建和谐社区。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;协作者科研之余推进项目进展，由于人力有限难以实时反馈，给诸君带来不便，敬请谅解！&lt;/p&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;本项目基于如下开源项目展开，在此对相关项目和开发人员表示诚挚的感谢：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chinese-LLaMA-Alpaca: &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;https://github.com/ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLaMA: &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;https://github.com/facebookresearch/llama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alpaca: &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;alpaca-lora: &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ChatGLM-6B: &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;https://github.com/THUDM/ChatGLM-6B&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;此外，本项目基于开放数据资源，详见 &lt;a href=&#34;https://github.com/pengxiao-song/awesome-chinese-legal-resources&#34;&gt;Awesome Chinese Legal Resources&lt;/a&gt;，一并表示感谢。&lt;/p&gt; &#xA;&lt;h2&gt;引用&lt;/h2&gt; &#xA;&lt;p&gt;如果您觉得我们的工作对您有所帮助，请考虑引用该项目&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>logspace-ai/langflow</title>
    <updated>2023-05-28T02:07:24Z</updated>
    <id>tag:github.com,2023-05-28:/logspace-ai/langflow</id>
    <link href="https://github.com/logspace-ai/langflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;⛓️ LangFlow is a UI for LangChain, designed with react-flow to provide an effortless way to experiment and prototype flows.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;⛓️ LangFlow&lt;/h1&gt; &#xA;&lt;p&gt;~ A User Interface For &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; ~&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/Logspace/LangFlow&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg?sanitize=true&#34; alt=&#34;HuggingFace Spaces&#34;&gt;&lt;/a&gt; &lt;img alt=&#34;GitHub Contributors&#34; src=&#34;https://img.shields.io/github/contributors/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;GitHub Last Commit&#34; src=&#34;https://img.shields.io/github/last-commit/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;&#34; src=&#34;https://img.shields.io/github/repo-size/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;GitHub Issues&#34; src=&#34;https://img.shields.io/github/issues/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;GitHub Pull Requests&#34; src=&#34;https://img.shields.io/github/issues-pr/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;Github License&#34; src=&#34;https://img.shields.io/github/license/logspace-ai/langflow&#34;&gt; &lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/logspace-ai/langflow&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://github.com/logspace-ai/langflow/raw/main/img/langflow-demo.gif?raw=true&#34;&gt;&lt;/a&gt; &#xA;&lt;p&gt;LangFlow is a GUI for &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;, designed with &lt;a href=&#34;https://github.com/wbkd/react-flow&#34;&gt;react-flow&lt;/a&gt; to provide an effortless way to experiment and prototype flows with drag-and-drop components and a chat box.&lt;/p&gt; &#xA;&lt;h2&gt;📦 Installation&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;b&gt;Locally&lt;/b&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can install LangFlow from pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install langflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m langflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;langflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploy Langflow on Google Cloud Platform&lt;/h3&gt; &#xA;&lt;p&gt;Follow our step-by-step guide to deploy Langflow on Google Cloud Platform (GCP) using Google Cloud Shell. The guide is available in the &lt;a href=&#34;https://raw.githubusercontent.com/logspace-ai/langflow/dev/GCP_DEPLOYMENT.md&#34;&gt;&lt;strong&gt;Langflow in Google Cloud Platform&lt;/strong&gt;&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, click the &lt;strong&gt;&#34;Open in Cloud Shell&#34;&lt;/strong&gt; button below to launch Google Cloud Shell, clone the Langflow repository, and start an &lt;strong&gt;interactive tutorial&lt;/strong&gt; that will guide you through the process of setting up the necessary resources and deploying Langflow on your GCP project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/logspace-ai/langflow&amp;amp;working_dir=scripts&amp;amp;shellonly=true&amp;amp;tutorial=walkthroughtutorial_spot.md&#34;&gt;&lt;img src=&#34;https://gstatic.com/cloudssh/images/open-btn.svg?sanitize=true&#34; alt=&#34;Open in Cloud Shell&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Deploy Langflow on &lt;a href=&#34;https://github.com/jina-ai/langchain-serve&#34;&gt;Jina AI Cloud&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Langflow integrates with langchain-serve to provide a one-command deployment to Jina AI Cloud.&lt;/p&gt; &#xA;&lt;p&gt;Start by installing &lt;code&gt;langchain-serve&lt;/code&gt; with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U langchain-serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;langflow --jcloud&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;🎉 Langflow server successfully deployed on Jina AI Cloud 🎉&#xA;🔗 Click on the link to open the server (please allow ~1-2 minutes for the server to startup): https://&amp;lt;your-app&amp;gt;.wolf.jina.ai/&#xA;📖 Read more about managing the server: https://github.com/jina-ai/langchain-serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show complete (example) output&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;  🚀 Deploying Langflow server on Jina AI Cloud&#xA;  ╭───────────────────────── 🎉 Flow is available! ──────────────────────────╮&#xA;  │                                                                          │&#xA;  │   ID                    langflow-e3dd8820ec                              │&#xA;  │   Gateway (Websocket)   wss://langflow-e3dd8820ec.wolf.jina.ai           │&#xA;  │   Dashboard             https://dashboard.wolf.jina.ai/flow/e3dd8820ec   │&#xA;  │                                                                          │&#xA;  ╰──────────────────────────────────────────────────────────────────────────╯&#xA;  ╭──────────────┬──────────────────────────────────────────────────────────────────────────────╮&#xA;  │ App ID       │                     langflow-e3dd8820ec                                      │&#xA;  ├──────────────┼──────────────────────────────────────────────────────────────────────────────┤&#xA;  │ Phase        │                            Serving                                           │&#xA;  ├──────────────┼──────────────────────────────────────────────────────────────────────────────┤&#xA;  │ Endpoint     │          wss://langflow-e3dd8820ec.wolf.jina.ai                              │&#xA;  ├──────────────┼──────────────────────────────────────────────────────────────────────────────┤&#xA;  │ App logs     │                  dashboards.wolf.jina.ai                                     │&#xA;  ├──────────────┼──────────────────────────────────────────────────────────────────────────────┤&#xA;  │ Swagger UI   │          https://langflow-e3dd8820ec.wolf.jina.ai/docs                       │&#xA;  ├──────────────┼──────────────────────────────────────────────────────────────────────────────┤&#xA;  │ OpenAPI JSON │        https://langflow-e3dd8820ec.wolf.jina.ai/openapi.json                 │&#xA;  ╰──────────────┴──────────────────────────────────────────────────────────────────────────────╯&#xA;&#xA;  🎉 Langflow server successfully deployed on Jina AI Cloud 🎉&#xA;  🔗 Click on the link to open the server (please allow ~1-2 minutes for the server to startup): https://langflow-e3dd8820ec.wolf.jina.ai/&#xA;  📖 Read more about managing the server: https://github.com/jina-ai/langchain-serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;API Usage&lt;/h4&gt; &#xA;&lt;p&gt;You can use Langflow directly on your browser, or use the API endpoints on Jina AI Cloud to interact with the server.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Show API usage (with python)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;import requests&#xA;&#xA;FLOW_PATH = &#34;Time_traveller.json&#34;&#xA;&#xA;# HOST = &#39;http://localhost:7860&#39;&#xA;HOST = &#39;https://langflow-f1ed20e309.wolf.jina.ai&#39;&#xA;API_URL = f&#39;{HOST}/predict&#39;&#xA;&#xA;def predict(message):&#xA;    with open(FLOW_PATH, &#34;r&#34;) as f:&#xA;        json_data = json.load(f)&#xA;    payload = {&#39;exported_flow&#39;: json_data, &#39;message&#39;: message}&#xA;    response = requests.post(API_URL, json=payload)&#xA;    return response.json()&#xA;&#xA;&#xA;predict(&#39;Take me to 1920s Bangalore&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;result&#34;: &#34;Great choice! Bangalore in the 1920s was a vibrant city with a rich cultural and political scene. Here are some suggestions for things to see and do:\n\n1. Visit the Bangalore Palace - built in 1887, this stunning palace is a perfect example of Tudor-style architecture. It was home to the Maharaja of Mysore and is now open to the public.\n\n2. Attend a performance at the Ravindra Kalakshetra - this cultural center was built in the 1920s and is still a popular venue for music and dance performances.\n\n3. Explore the neighborhoods of Basavanagudi and Malleswaram - both of these areas have retained much of their old-world charm and are great places to walk around and soak up the atmosphere.\n\n4. Check out the Bangalore Club - founded in 1868, this exclusive social club was a favorite haunt of the British expat community in the 1920s.\n\n5. Attend a meeting of the Indian National Congress - founded in 1885, the INC was a major force in the Indian independence movement and held many meetings and rallies in Bangalore in the 1920s.\n\nHope you enjoy your trip to 1920s Bangalore!&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Read more about resource customization, cost, and management of Langflow apps on Jina AI Cloud in the &lt;strong&gt;&lt;a href=&#34;https://github.com/jina-ai/langchain-serve&#34;&gt;langchain-serve&lt;/a&gt;&lt;/strong&gt; repository.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;🎨 Creating Flows&lt;/h2&gt; &#xA;&lt;p&gt;Creating flows with LangFlow is easy. Simply drag sidebar components onto the canvas and connect them together to create your pipeline. LangFlow provides a range of &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/reference.html&#34;&gt;LangChain components&lt;/a&gt; to choose from, including LLMs, prompt serializers, agents, and chains.&lt;/p&gt; &#xA;&lt;p&gt;Explore by editing prompt parameters, link chains and agents, track an agent&#39;s thought process, and export your flow.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;re done, you can export your flow as a JSON file to use with LangChain. To do so, click the &#34;Export&#34; button in the top right corner of the canvas, then in Python, you can load the flow with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langflow import load_flow_from_json&#xA;&#xA;flow = load_flow_from_json(&#34;path/to/flow.json&#34;)&#xA;# Now you can use it like any chain&#xA;flow(&#34;Hey, have you heard of LangFlow?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;👋 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from developers of all levels to our open-source project on GitHub. If you&#39;d like to contribute, please check our &lt;a href=&#34;https://raw.githubusercontent.com/logspace-ai/langflow/dev/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; and help make LangFlow more accessible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#logspace-ai/langflow&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=logspace-ai/langflow&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📄 License&lt;/h2&gt; &#xA;&lt;p&gt;LangFlow is released under the MIT License. See the LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Rudrabha/Wav2Lip</title>
    <updated>2023-05-28T02:07:24Z</updated>
    <id>tag:github.com,2023-05-28:/Rudrabha/Wav2Lip</id>
    <link href="https://github.com/Rudrabha/Wav2Lip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains the codes of &#34;A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild&#34;, published at ACM Multimedia 2020.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;Wav2Lip&lt;/strong&gt;: &lt;em&gt;Accurately Lip-syncing Videos In The Wild&lt;/em&gt;&lt;/h1&gt; &#xA;&lt;p&gt;For commercial requests, please contact us at &lt;a href=&#34;mailto:radrabha.m@research.iiit.ac.in&#34;&gt;radrabha.m@research.iiit.ac.in&lt;/a&gt; or &lt;a href=&#34;mailto:prajwal.k@research.iiit.ac.in&#34;&gt;prajwal.k@research.iiit.ac.in&lt;/a&gt;. We have an HD model ready that can be used commercially.&lt;/p&gt; &#xA;&lt;p&gt;This code is part of the paper: &lt;em&gt;A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild&lt;/em&gt; published at ACM Multimedia 2020.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/lip-sync-on-lrs2?p=a-lip-sync-expert-is-all-you-need-for-speech&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/lip-sync-on-lrs3?p=a-lip-sync-expert-is-all-you-need-for-speech&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/lip-sync-on-lrw?p=a-lip-sync-expert-is-all-you-need-for-speech&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrw&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;📑 Original Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;📰 Project Page&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;🌀 Demo&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;⚡ Live Testing&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;📔 Colab Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2008.10010&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/&#34;&gt;Project Page&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://youtu.be/0fXaDCZNOJc&#34;&gt;Demo Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://bhaasha.iiit.ac.in/lipsync&#34;&gt;Interactive Demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1tZpDWXz49W6wDcTprANRGLo2D_EbD5J8?usp=sharing&#34;&gt;Colab Notebook&lt;/a&gt; /&lt;a href=&#34;https://colab.research.google.com/drive/1IjFW1cLevs6Ouyu4Yht4mnR4yeuMqO7Y#scrollTo=MH1m608OymLH&#34;&gt;Updated Collab Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;img src=&#34;https://drive.google.com/uc?export=view&amp;amp;id=1Wn0hPmpo4GRbCIJR8Tf20Akzdi1qjjG9&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Weights of the visual quality disc has been updated in readme!&lt;/li&gt; &#xA; &lt;li&gt;Lip-sync videos to any target speech with high accuracy &lt;span&gt;💯&lt;/span&gt;. Try our &lt;a href=&#34;https://bhaasha.iiit.ac.in/lipsync&#34;&gt;interactive demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;✨&lt;/span&gt; Works for any identity, voice, and language. Also works for CGI faces and synthetic voices.&lt;/li&gt; &#xA; &lt;li&gt;Complete training code, inference code, and pretrained models are available &lt;span&gt;💥&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Or, quick-start with the Google Colab Notebook: &lt;a href=&#34;https://colab.research.google.com/drive/1tZpDWXz49W6wDcTprANRGLo2D_EbD5J8?usp=sharing&#34;&gt;Link&lt;/a&gt;. Checkpoints and samples are available in a Google Drive &lt;a href=&#34;https://drive.google.com/drive/folders/1I-0dNLfFOSFwrfqjNa-SXuwaURHE5K4k?usp=sharing&#34;&gt;folder&lt;/a&gt; as well. There is also a &lt;a href=&#34;https://www.youtube.com/watch?v=Ic0TBhfuOrA&#34;&gt;tutorial video&lt;/a&gt; on this, courtesy of &lt;a href=&#34;https://www.youtube.com/channel/UCmGXH-jy0o2CuhqtpxbaQgA&#34;&gt;What Make Art&lt;/a&gt;. Also, thanks to &lt;a href=&#34;https://eyalgruss.com&#34;&gt;Eyal Gruss&lt;/a&gt;, there is a more accessible &lt;a href=&#34;https://j.mp/wav2lip&#34;&gt;Google Colab notebook&lt;/a&gt; with more useful features. A tutorial collab notebook is present at this &lt;a href=&#34;https://colab.research.google.com/drive/1IjFW1cLevs6Ouyu4Yht4mnR4yeuMqO7Y#scrollTo=MH1m608OymLH&#34;&gt;link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🔥&lt;/span&gt; &lt;span&gt;🔥&lt;/span&gt; Several new, reliable evaluation benchmarks and metrics &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip/tree/master/evaluation&#34;&gt;[&lt;code&gt;evaluation/&lt;/code&gt; folder of this repo]&lt;/a&gt; released. Instructions to calculate the metrics reported in the paper are also present.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;All results from this open-source code or our &lt;a href=&#34;https://bhaasha.iiit.ac.in/lipsync&#34;&gt;demo website&lt;/a&gt; should only be used for research/academic/personal purposes only. As the models are trained on the &lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html&#34;&gt;LRS2 dataset&lt;/a&gt;, any form of commercial use is strictly prohibhited. For commercial requests please contact us directly!&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Python 3.6&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ffmpeg: &lt;code&gt;sudo apt-get install ffmpeg&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install necessary packages using &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;. Alternatively, instructions for using a docker image is provided &lt;a href=&#34;https://gist.github.com/xenogenesi/e62d3d13dadbc164124c830e9c453668&#34;&gt;here&lt;/a&gt;. Have a look at &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip/issues/131#issuecomment-725478562&#34;&gt;this comment&lt;/a&gt; and comment on &lt;a href=&#34;https://gist.github.com/xenogenesi/e62d3d13dadbc164124c830e9c453668&#34;&gt;the gist&lt;/a&gt; if you encounter any issues.&lt;/li&gt; &#xA; &lt;li&gt;Face detection &lt;a href=&#34;https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth&#34;&gt;pre-trained model&lt;/a&gt; should be downloaded to &lt;code&gt;face_detection/detection/sfd/s3fd.pth&lt;/code&gt;. Alternative &lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/prajwal_k_research_iiit_ac_in/EZsy6qWuivtDnANIG73iHjIBjMSoojcIV0NULXV-yiuiIg?e=qTasa8&#34;&gt;link&lt;/a&gt; if the above does not work.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting the weights&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link to the model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Wav2Lip&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Highly accurate lip-sync&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Wav2Lip + GAN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Slightly inferior lip-sync, but better visual quality&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Expert Discriminator&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Weights of the expert discriminator&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visual Quality Discriminator&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Weights of the visual disc trained in a GAN setup&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQVqH88dTm1HjlK11eNba5gBbn15WMS0B0EZbDBttqrqkg?e=ic0ljo&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Lip-syncing videos using the pre-trained models (Inference)&lt;/h2&gt; &#xA;&lt;p&gt;You can lip-sync any video to any audio:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --checkpoint_path &amp;lt;ckpt&amp;gt; --face &amp;lt;video.mp4&amp;gt; --audio &amp;lt;an-audio-source&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result is saved (by default) in &lt;code&gt;results/result_voice.mp4&lt;/code&gt;. You can specify it as an argument, similar to several other available options. The audio source can be any file supported by &lt;code&gt;FFMPEG&lt;/code&gt; containing audio data: &lt;code&gt;*.wav&lt;/code&gt;, &lt;code&gt;*.mp3&lt;/code&gt; or even a video file, from which the code will automatically extract the audio.&lt;/p&gt; &#xA;&lt;h5&gt;Tips for better results:&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Experiment with the &lt;code&gt;--pads&lt;/code&gt; argument to adjust the detected face bounding box. Often leads to improved results. You might need to increase the bottom padding to include the chin region. E.g. &lt;code&gt;--pads 0 20 0 0&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you see the mouth position dislocated or some weird artifacts such as two mouths, then it can be because of over-smoothing the face detections. Use the &lt;code&gt;--nosmooth&lt;/code&gt; argument and give another try.&lt;/li&gt; &#xA; &lt;li&gt;Experiment with the &lt;code&gt;--resize_factor&lt;/code&gt; argument, to get a lower resolution video. Why? The models are trained on faces which were at a lower resolution. You might get better, visually pleasing results for 720p videos than for 1080p videos (in many cases, the latter works well too).&lt;/li&gt; &#xA; &lt;li&gt;The Wav2Lip model without GAN usually needs more experimenting with the above two to get the most ideal results, and sometimes, can give you a better result as well.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preparing LRS2 for training&lt;/h2&gt; &#xA;&lt;p&gt;Our models are trained on LRS2. See &lt;a href=&#34;https://raw.githubusercontent.com/Rudrabha/Wav2Lip/master/#training-on-datasets-other-than-lrs2&#34;&gt;here&lt;/a&gt; for a few suggestions regarding training on other datasets.&lt;/p&gt; &#xA;&lt;h5&gt;LRS2 dataset folder structure&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;data_root (mvlrs_v1)&#xA;├── main, pretrain (we use only main folder in this work)&#xA;|&#x9;├── list of folders&#xA;|&#x9;│   ├── five-digit numbered video IDs ending with (.mp4)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Place the LRS2 filelists (train, val, test) &lt;code&gt;.txt&lt;/code&gt; files in the &lt;code&gt;filelists/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h5&gt;Preprocess the dataset for fast training&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python preprocess.py --data_root data_root/main --preprocessed_root lrs2_preprocessed/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional options like &lt;code&gt;batch_size&lt;/code&gt; and number of GPUs to use in parallel to use can also be set.&lt;/p&gt; &#xA;&lt;h5&gt;Preprocessed LRS2 folder structure&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;preprocessed_root (lrs2_preprocessed)&#xA;├── list of folders&#xA;|&#x9;├── Folders with five-digit numbered video IDs&#xA;|&#x9;│   ├── *.jpg&#xA;|&#x9;│   ├── audio.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train!&lt;/h2&gt; &#xA;&lt;p&gt;There are two major steps: (i) Train the expert lip-sync discriminator, (ii) Train the Wav2Lip model(s).&lt;/p&gt; &#xA;&lt;h5&gt;Training the expert discriminator&lt;/h5&gt; &#xA;&lt;p&gt;You can download &lt;a href=&#34;https://raw.githubusercontent.com/Rudrabha/Wav2Lip/master/#getting-the-weights&#34;&gt;the pre-trained weights&lt;/a&gt; if you want to skip this step. To train it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python color_syncnet_train.py --data_root lrs2_preprocessed/ --checkpoint_dir &amp;lt;folder_to_save_checkpoints&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Training the Wav2Lip models&lt;/h5&gt; &#xA;&lt;p&gt;You can either train the model without the additional visual quality disriminator (&amp;lt; 1 day of training) or use the discriminator (~2 days). For the former, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python wav2lip_train.py --data_root lrs2_preprocessed/ --checkpoint_dir &amp;lt;folder_to_save_checkpoints&amp;gt; --syncnet_checkpoint_path &amp;lt;path_to_expert_disc_checkpoint&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train with the visual quality discriminator, you should run &lt;code&gt;hq_wav2lip_train.py&lt;/code&gt; instead. The arguments for both the files are similar. In both the cases, you can resume training as well. Look at &lt;code&gt;python wav2lip_train.py --help&lt;/code&gt; for more details. You can also set additional less commonly-used hyper-parameters at the bottom of the &lt;code&gt;hparams.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Training on datasets other than LRS2&lt;/h2&gt; &#xA;&lt;p&gt;Training on other datasets might require modifications to the code. Please read the following before you raise an issue:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You might not get good results by training/fine-tuning on a few minutes of a single speaker. This is a separate research problem, to which we do not have a solution yet. Thus, we would most likely not be able to resolve your issue.&lt;/li&gt; &#xA; &lt;li&gt;You must train the expert discriminator for your own dataset before training Wav2Lip.&lt;/li&gt; &#xA; &lt;li&gt;If it is your own dataset downloaded from the web, in most cases, needs to be sync-corrected.&lt;/li&gt; &#xA; &lt;li&gt;Be mindful of the FPS of the videos of your dataset. Changes to FPS would need significant code changes.&lt;/li&gt; &#xA; &lt;li&gt;The expert discriminator&#39;s eval loss should go down to ~0.25 and the Wav2Lip eval sync loss should go down to ~0.2 to get good results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When raising an issue on this topic, please let us know that you are aware of all these points.&lt;/p&gt; &#xA;&lt;p&gt;We have an HD model trained on a dataset allowing commercial usage. The size of the generated face will be 192 x 288 in our new model.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Please check the &lt;code&gt;evaluation/&lt;/code&gt; folder for the instructions.&lt;/p&gt; &#xA;&lt;h2&gt;License and Citation&lt;/h2&gt; &#xA;&lt;p&gt;Theis repository can only be used for personal/research/non-commercial purposes. However, for commercial requests, please contact us directly at &lt;a href=&#34;mailto:radrabha.m@research.iiit.ac.in&#34;&gt;radrabha.m@research.iiit.ac.in&lt;/a&gt; or &lt;a href=&#34;mailto:prajwal.k@research.iiit.ac.in&#34;&gt;prajwal.k@research.iiit.ac.in&lt;/a&gt;. We have an HD model trained on a dataset allowing commercial usage. The size of the generated face will be 192 x 288 in our new model. Please cite the following paper if you use this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{10.1145/3394171.3413532,&#xA;author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},&#xA;title = {A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild},&#xA;year = {2020},&#xA;isbn = {9781450379885},&#xA;publisher = {Association for Computing Machinery},&#xA;address = {New York, NY, USA},&#xA;url = {https://doi.org/10.1145/3394171.3413532},&#xA;doi = {10.1145/3394171.3413532},&#xA;booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},&#xA;pages = {484–492},&#xA;numpages = {9},&#xA;keywords = {lip sync, talking face generation, video generation},&#xA;location = {Seattle, WA, USA},&#xA;series = {MM &#39;20}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Parts of the code structure is inspired by this &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch&#34;&gt;TTS repository&lt;/a&gt;. We thank the author for this wonderful code. The code for Face Detection has been taken from the &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face_alignment&lt;/a&gt; repository. We thank the authors for releasing their code and models. We thank &lt;a href=&#34;https://github.com/zabique&#34;&gt;zabique&lt;/a&gt; for the tutorial collab notebook.&lt;/p&gt;</summary>
  </entry>
</feed>