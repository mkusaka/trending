<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-26T02:01:37Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Lightning-AI/lightning</title>
    <updated>2022-06-26T02:01:37Z</updated>
    <id>tag:github.com,2022-06-26:/Lightning-AI/lightning</id>
    <link href="https://github.com/Lightning-AI/lightning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build high-performance AI models with PyTorch Lightning (organized PyTorch). Deploy models with Lightning Apps (organized Python to build end-to-end ML systems).&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pl-flash-data.s3.amazonaws.com/assets_lightning/docs/images/logos/lightning-ai.png&#34; width=&#34;400px&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;Build high-performance PyTorch models and deploy them with Lightning Apps (scalable end-to-end ML systems).&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.lightning.ai/&#34;&gt;Lightning Gallery&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning/master/#key-features&#34;&gt;Key Features&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning/master/#how-to-use&#34;&gt;How To Use&lt;/a&gt; • &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/&#34;&gt;Docs&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning/master/#examples&#34;&gt;Examples&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning/master/#community&#34;&gt;Community&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning/master/#license&#34;&gt;License&lt;/a&gt; &lt;/p&gt; &#xA; &lt;!-- DO NOT ADD CONDA DOWNLOADS... README CHANGES MUST BE APPROVED BY EDEN OR WILL --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pytorch-lightning/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pytorch-lightning&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/pytorch-lightning&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/pytorch-lightning.svg?sanitize=true&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/pytorch-lightning&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/pytorch-lightning&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/pytorch-lightning&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/v/conda-forge/pytorch-lightning?label=conda&amp;amp;color=success&#34; alt=&#34;Conda&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/pytorchlightning/pytorch_lightning&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/pytorchlightning/pytorch_lightning.svg?sanitize=true&#34; alt=&#34;DockerHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/PyTorchLightning/pytorch-lightning&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/PyTorchLightning/pytorch-lightning/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/starter/new-project.html&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pytorch-lightning/badge/?version=stable&#34; alt=&#34;ReadTheDocs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/pytorch-lightning/shared_invite/zt-12iz3cds1-uyyyBYJLiaL2bqVmMN7n~A&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-chat-green.svg?logo=slack&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PytorchLightning/pytorch-lightning/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!--&#xA;[![CodeFactor](https://www.codefactor.io/repository/github/pytorchlightning/pytorch-lightning/badge)](https://www.codefactor.io/repository/github/pytorchlightning/pytorch-lightning)&#xA;--&gt; &#xA;&lt;/div&gt; &#xA;&lt;h6&gt;*Codecov is &amp;gt; 90%+ but build delays may show less&lt;/h6&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;PyTorch Lightning is just organized PyTorch&lt;/h2&gt; &#xA;&lt;p&gt;Lightning disentangles PyTorch code to decouple the science from the engineering. &lt;img src=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning/master/docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif&#34; alt=&#34;PT to PL&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build AI products with Lightning Apps&lt;/h2&gt; &#xA;&lt;p&gt;Once you&#39;re done building models, publish a paper demo or build a full production end-to-end ML system with Lightning Apps. Lightning Apps remove the cloud infrastructure boilerplate so you can focus on solving the research or business problems. Lightning Apps can run on the Lightning Cloud, your own cluster or a private cloud.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lightning.ai/&#34;&gt;Browse available Lightning apps here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pl-flash-data.s3.amazonaws.com/assets_lightning/docs/images/logos/lightning-apps-teaser.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning/master/src/lightning_app/README.md&#34;&gt;Learn more about Lightning Apps&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Lightning Design Philosophy&lt;/h2&gt; &#xA;&lt;p&gt;Lightning structures PyTorch code with these principles:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/philosophies.jpg&#34; max-height=&#34;250px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Lightning forces the following structure to your code which makes it reusable and shareable:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Research code (the LightningModule).&lt;/li&gt; &#xA; &lt;li&gt;Engineering code (you delete, and is handled by the Trainer).&lt;/li&gt; &#xA; &lt;li&gt;Non-essential research code (logging, etc... this goes in Callbacks).&lt;/li&gt; &#xA; &lt;li&gt;Data (use PyTorch DataLoaders or organize them into a LightningDataModule).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you do this, you can train on multiple-GPUs, TPUs, CPUs and even in 16-bit precision without changing your code!&lt;/p&gt; &#xA;&lt;p&gt;Get started with our &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/starter/new-project.html&#34;&gt;2 step guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Continuous Integration&lt;/h2&gt; &#xA;&lt;p&gt;Lightning is rigorously tested across multiple GPUs, TPUs CPUs and against major Python and PyTorch versions.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Current build statuses&lt;/summary&gt; &#xA; &lt;center&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;System / PyTorch ver.&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;1.8 (LTS, min. req.)&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;1.9&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;1.10 (latest)&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.7 [GPUs**]&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PytorchLightning/pytorch-lightning/_build/latest?definitionId=6&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PytorchLightning/pytorch-lightning/_apis/build/status/PL.pytorch-lightning%20(GPUs)?branchName=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.7 [TPUs***]&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://circleci.com/gh/PyTorchLightning/pytorch-lightning/tree/master&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/PyTorchLightning/pytorch-lightning/tree/master.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.8 (with Conda&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.{7,9}&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;OSX py3.{7,9}&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Windows py3.{7,9}&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;** tests run on two NVIDIA P100&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;*** tests run on Google GKE TPUv2/3. TPU py3.7 means we support Colab and Kaggle env.&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/center&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;How To Use&lt;/h2&gt; &#xA;&lt;h3&gt;Step 0: Install&lt;/h3&gt; &#xA;&lt;p&gt;Simple installation from PyPI&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pytorch-lightning&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- following section will be skipped from PyPI description --&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Other installation options&lt;/summary&gt; &#xA; &lt;!-- following section will be skipped from PyPI description --&gt; &#xA; &lt;h4&gt;Install with optional dependencies&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pytorch-lightning[&#39;extra&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Conda&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install pytorch-lightning -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Install stable 1.5.x&lt;/h4&gt; &#xA; &lt;p&gt;the actual status of 1.5 [stable] is following:&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20basic%20testing/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;CI basic testing&#34;&gt; &lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20complete%20testing/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;CI complete testing&#34;&gt; &lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/PyTorch%20&amp;amp;%20Conda/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;PyTorch &amp;amp; Conda&#34;&gt; &lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/TPU%20tests/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;TPU tests&#34;&gt; &lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/Docs%20check/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;Docs check&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Install future release from the source&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@release/1.5.x --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Install bleeding-edge - future 1.6&lt;/h4&gt; &#xA; &lt;p&gt;Install nightly from the source (no guarantees)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install https://github.com/PyTorchLightning/pytorch-lightning/archive/master.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;or from testing PyPI&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -iU https://test.pypi.org/simple/ pytorch-lightning&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- end skipping PyPI description --&gt; &#xA;&lt;h3&gt;Step 1: Add these imports&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import torch&#xA;from torch import nn&#xA;import torch.nn.functional as F&#xA;from torchvision.datasets import MNIST&#xA;from torch.utils.data import DataLoader, random_split&#xA;from torchvision import transforms&#xA;import pytorch_lightning as pl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Define a LightningModule (nn.Module subclass)&lt;/h3&gt; &#xA;&lt;p&gt;A LightningModule defines a full &lt;em&gt;system&lt;/em&gt; (ie: a GAN, autoencoder, BERT or a simple Image Classifier).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class LitAutoEncoder(pl.LightningModule):&#xA;    def __init__(self):&#xA;        super().__init__()&#xA;        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))&#xA;        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))&#xA;&#xA;    def forward(self, x):&#xA;        # in lightning, forward defines the prediction/inference actions&#xA;        embedding = self.encoder(x)&#xA;        return embedding&#xA;&#xA;    def training_step(self, batch, batch_idx):&#xA;        # training_step defines the train loop. It is independent of forward&#xA;        x, y = batch&#xA;        x = x.view(x.size(0), -1)&#xA;        z = self.encoder(x)&#xA;        x_hat = self.decoder(z)&#xA;        loss = F.mse_loss(x_hat, x)&#xA;        self.log(&#34;train_loss&#34;, loss)&#xA;        return loss&#xA;&#xA;    def configure_optimizers(self):&#xA;        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)&#xA;        return optimizer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: Training_step defines the training loop. Forward defines how the LightningModule behaves during inference/prediction.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Train!&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())&#xA;train, val = random_split(dataset, [55000, 5000])&#xA;&#xA;autoencoder = LitAutoEncoder()&#xA;trainer = pl.Trainer()&#xA;trainer.fit(autoencoder, DataLoader(train), DataLoader(val))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced features&lt;/h2&gt; &#xA;&lt;p&gt;Lightning has over &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags&#34;&gt;40+ advanced features&lt;/a&gt; designed for professional AI research at scale.&lt;/p&gt; &#xA;&lt;p&gt;Here are some examples:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/features_2.jpg&#34; max-height=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Highlighted feature code snippets&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 8 GPUs&#xA;# no code changes needed&#xA;trainer = Trainer(max_epochs=1, accelerator=&#34;gpu&#34;, devices=8)&#xA;&#xA;# 256 GPUs&#xA;trainer = Trainer(max_epochs=1, accelerator=&#34;gpu&#34;, devices=8, num_nodes=32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Train on TPUs without code changes&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# no code changes needed&#xA;trainer = Trainer(accelerator=&#34;tpu&#34;, devices=8)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;16-bit precision&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# no code changes needed&#xA;trainer = Trainer(precision=16)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Experiment managers&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pytorch_lightning import loggers&#xA;&#xA;# tensorboard&#xA;trainer = Trainer(logger=TensorBoardLogger(&#34;logs/&#34;))&#xA;&#xA;# weights and biases&#xA;trainer = Trainer(logger=loggers.WandbLogger())&#xA;&#xA;# comet&#xA;trainer = Trainer(logger=loggers.CometLogger())&#xA;&#xA;# mlflow&#xA;trainer = Trainer(logger=loggers.MLFlowLogger())&#xA;&#xA;# neptune&#xA;trainer = Trainer(logger=loggers.NeptuneLogger())&#xA;&#xA;# ... and dozens more&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;EarlyStopping&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;es = EarlyStopping(monitor=&#34;val_loss&#34;)&#xA;trainer = Trainer(callbacks=[es])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Checkpointing&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;checkpointing = ModelCheckpoint(monitor=&#34;val_loss&#34;)&#xA;trainer = Trainer(callbacks=[checkpointing])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Export to torchscript (JIT) (production use)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# torchscript&#xA;autoencoder = LitAutoEncoder()&#xA;torch.jit.save(autoencoder.to_torchscript(), &#34;model.pt&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Export to ONNX (production use)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# onnx&#xA;with tempfile.NamedTemporaryFile(suffix=&#34;.onnx&#34;, delete=False) as tmpfile:&#xA;    autoencoder = LitAutoEncoder()&#xA;    input_sample = torch.randn((1, 64))&#xA;    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=True)&#xA;    os.path.isfile(tmpfile.name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Pro-level control of training loops (advanced users)&lt;/h3&gt; &#xA;&lt;p&gt;For complex/professional level work, you have optional full control of the training loop and optimizers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class LitAutoEncoder(pl.LightningModule):&#xA;    def __init__(self):&#xA;        super().__init__()&#xA;        self.automatic_optimization = False&#xA;&#xA;    def training_step(self, batch, batch_idx):&#xA;        # access your optimizers with use_pl_optimizer=False. Default is True&#xA;        opt_a, opt_b = self.optimizers(use_pl_optimizer=True)&#xA;&#xA;        loss_a = ...&#xA;        self.manual_backward(loss_a, opt_a)&#xA;        opt_a.step()&#xA;        opt_a.zero_grad()&#xA;&#xA;        loss_b = ...&#xA;        self.manual_backward(loss_b, opt_b, retain_graph=True)&#xA;        self.manual_backward(loss_b, opt_b)&#xA;        opt_b.step()&#xA;        opt_b.zero_grad()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Advantages over unstructured PyTorch&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Models become hardware agnostic&lt;/li&gt; &#xA; &lt;li&gt;Code is clear to read because engineering code is abstracted away&lt;/li&gt; &#xA; &lt;li&gt;Easier to reproduce&lt;/li&gt; &#xA; &lt;li&gt;Make fewer mistakes because lightning handles the tricky engineering&lt;/li&gt; &#xA; &lt;li&gt;Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate&lt;/li&gt; &#xA; &lt;li&gt;Lightning has dozens of integrations with popular machine learning tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/tree/master/tests&#34;&gt;Tested rigorously with every new PR&lt;/a&gt;. We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.&lt;/li&gt; &#xA; &lt;li&gt;Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Lightning Lite&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Lightning-AI/lightning/master/docs/source/_static/images/lightning_lite/lite.gif&#34; height=&#34;200px&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;In the Lighting 1.5 release, LightningLite now enables you to leverage all the capabilities of PyTorch Lightning Accelerators without any refactoring to your training loop. Check out the &lt;a href=&#34;https://devblog.pytorchlightning.ai/scale-your-pytorch-code-with-lightninglite-d5692a303f00&#34;&gt;blogpost&lt;/a&gt; and &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/starter/lightning_lite.html&#34;&gt;docs&lt;/a&gt; for more info.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h6&gt;Hello world&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/mnist-hello-world.html&#34;&gt;MNIST hello world&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Contrastive Learning&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/self_supervised.html#byol&#34;&gt;BYOL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/self_supervised.html#cpc-v2&#34;&gt;CPC v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/self_supervised.html#moco-v2-api&#34;&gt;Moco v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/self_supervised.html#simclr&#34;&gt;SIMCLR&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;NLP&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/convolutional.html#gpt-2&#34;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html&#34;&gt;BERT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Reinforcement Learning&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/reinforce_learn.html#dqn-models&#34;&gt;DQN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/reinforce_learn.html#dueling-dqn&#34;&gt;Dueling-DQN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/reinforce_learn.html#reinforce&#34;&gt;Reinforce&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Vision&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/basic-gan.html&#34;&gt;GAN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Classic ML&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/classic_ml.html#logistic-regression&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/classic_ml.html#linear-regression&#34;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;The lightning community is maintained by&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/governance.html&#34;&gt;10+ core contributors&lt;/a&gt; who are all a mix of professional engineers, Research Scientists, and Ph.D. students from top AI labs.&lt;/li&gt; &#xA; &lt;li&gt;590+ active community contributors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want to help us build Lightning and reduce boilerplate for thousands of researchers? &lt;a href=&#34;https://devblog.pytorchlightning.ai/quick-contribution-guide-86d977171b3a&#34;&gt;Learn how to make your first contribution here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lightning is also part of the &lt;a href=&#34;https://pytorch.org/ecosystem/&#34;&gt;PyTorch ecosystem&lt;/a&gt; which requires projects to have solid testing, documentation and support.&lt;/p&gt; &#xA;&lt;h3&gt;Asking for help&lt;/h3&gt; &#xA;&lt;p&gt;If you have any questions please:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.rtfd.io/en/latest&#34;&gt;Read the docs&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/discussions&#34;&gt;Search through existing Discussions&lt;/a&gt;, or &lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/discussions/new&#34;&gt;add a new question&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/pytorch-lightning/shared_invite/zt-pw5v393p-qRaDgEk24~EjiZNBpSQFgQ&#34;&gt;Join our slack&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>mingrammer/diagrams</title>
    <updated>2022-06-26T02:01:37Z</updated>
    <id>tag:github.com,2022-06-26:/mingrammer/diagrams</id>
    <link href="https://github.com/mingrammer/diagrams" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🎨 Diagram as Code for prototyping cloud system architectures&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mingrammer/diagrams/master/assets/img/diagrams.png&#34; alt=&#34;diagrams logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Diagrams&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mingrammer/diagrams/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/diagrams&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/diagrams.svg?sanitize=true&#34; alt=&#34;pypi version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/python-%3E%3D%203.6-blue?logo=python&#34; alt=&#34;python version&#34;&gt; &lt;img src=&#34;https://github.com/mingrammer/diagrams/workflows/Run%20tests/badge.svg?branch=master&#34; alt=&#34;Run tests&#34;&gt; &lt;a href=&#34;https://www.tickgit.com/browse?repo=github.com/mingrammer/diagrams&#34;&gt;&lt;img src=&#34;https://badgen.net/https/api.tickgit.com/badgen/github.com/mingrammer/diagrams?label=todos&#34; alt=&#34;todos&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/contributors/mingrammer/diagrams&#34; alt=&#34;contributors&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/mingrammer&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&#34; alt=&#34;Buy Me A Coffee&#34; style=&#34;height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diagram as Code&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Diagrams lets you draw the cloud system architecture &lt;strong&gt;in Python code&lt;/strong&gt;. It was born for &lt;strong&gt;prototyping&lt;/strong&gt; a new system architecture design without any design tools. You can also describe or visualize the existing system architecture as well. Diagrams currently supports main major providers including: &lt;code&gt;AWS&lt;/code&gt;, &lt;code&gt;Azure&lt;/code&gt;, &lt;code&gt;GCP&lt;/code&gt;, &lt;code&gt;Kubernetes&lt;/code&gt;, &lt;code&gt;Alibaba Cloud&lt;/code&gt;, &lt;code&gt;Oracle Cloud&lt;/code&gt; etc... It also supports &lt;code&gt;On-Premise&lt;/code&gt; nodes, &lt;code&gt;SaaS&lt;/code&gt; and major &lt;code&gt;Programming&lt;/code&gt; frameworks and languages.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diagram as Code&lt;/strong&gt; also allows you to &lt;strong&gt;track&lt;/strong&gt; the architecture diagram changes in any &lt;strong&gt;version control&lt;/strong&gt; system.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE: It does not control any actual cloud resources nor does it generate cloud formation or terraform code. It is just for drawing the cloud system architecture diagrams.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Providers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/AWS-orange?logo=amazon-aws&amp;amp;color=ff9900&#34; alt=&#34;aws provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Azure-orange?logo=microsoft-azure&amp;amp;color=0089d6&#34; alt=&#34;azure provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/GCP-orange?logo=google-cloud&amp;amp;color=4285f4&#34; alt=&#34;gcp provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/IBM-orange?logo=ibm&amp;amp;color=052FAD&#34; alt=&#34;ibm provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Kubernetes-orange?logo=kubernetes&amp;amp;color=326ce5&#34; alt=&#34;kubernetes provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/AlibabaCloud-orange?logo=alibaba-cloud&amp;amp;color=ff6a00&#34; alt=&#34;alibaba cloud provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/OracleCloud-orange?logo=oracle&amp;amp;color=f80000&#34; alt=&#34;oracle cloud provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/OpenStack-orange?logo=openstack&amp;amp;color=da1a32&#34; alt=&#34;openstack provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Firebase-orange?logo=firebase&amp;amp;color=FFCA28&#34; alt=&#34;firebase provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/DigitalOcean-0080ff?logo=digitalocean&amp;amp;color=0080ff&#34; alt=&#34;digital ocean provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Elastic-orange?logo=elastic&amp;amp;color=005571&#34; alt=&#34;elastic provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/OutScale-orange?color=5f87bf&#34; alt=&#34;outscale provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/OnPremise-orange?color=5f87bf&#34; alt=&#34;on premise provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Generic-orange?color=5f87bf&#34; alt=&#34;generic provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Programming-orange?color=5f87bf&#34; alt=&#34;programming provider&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/SaaS-orange?color=5f87bf&#34; alt=&#34;saas provider&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;It requires &lt;strong&gt;Python 3.6&lt;/strong&gt; or higher, check your Python version first.&lt;/p&gt; &#xA;&lt;p&gt;It uses &lt;a href=&#34;https://www.graphviz.org/&#34;&gt;Graphviz&lt;/a&gt; to render the diagram, so you need to &lt;a href=&#34;https://graphviz.gitlab.io/download/&#34;&gt;install Graphviz&lt;/a&gt; to use &lt;strong&gt;diagrams&lt;/strong&gt;. After installing graphviz (or already have it), install the &lt;strong&gt;diagrams&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;macOS users can download the Graphviz via &lt;code&gt;brew install graphviz&lt;/code&gt; if you&#39;re using &lt;a href=&#34;https://brew.sh&#34;&gt;Homebrew&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# using pip (pip3)&#xA;$ pip install diagrams&#xA;&#xA;# using pipenv&#xA;$ pipenv install diagrams&#xA;&#xA;# using poetry&#xA;$ poetry add diagrams&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can start with &lt;a href=&#34;https://diagrams.mingrammer.com/docs/getting-started/installation#quick-start&#34;&gt;quick start&lt;/a&gt;. Check out &lt;a href=&#34;https://diagrams.mingrammer.com/docs/guides/diagram&#34;&gt;guides&lt;/a&gt; for more details, and you can find all available nodes list in &lt;a href=&#34;https://diagrams.mingrammer.com/docs/nodes/aws&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Event Processing&lt;/th&gt; &#xA;   &lt;th&gt;Stateful Architecture&lt;/th&gt; &#xA;   &lt;th&gt;Advanced Web Service&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://diagrams.mingrammer.com/img/event_processing_diagram.png&#34; alt=&#34;event processing&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://diagrams.mingrammer.com/img/stateful_architecture_diagram.png&#34; alt=&#34;stateful architecture&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://diagrams.mingrammer.com/img/advanced_web_service_with_on-premise.png&#34; alt=&#34;advanced web service with on-premise&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can find all the examples on the &lt;a href=&#34;https://diagrams.mingrammer.com/docs/getting-started/examples&#34;&gt;examples&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To contribute to diagram, check out &lt;a href=&#34;https://raw.githubusercontent.com/mingrammer/diagrams/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Let me know if you are using diagrams! I&#39;ll add you in showcase page. (I&#39;m working on it!) :)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Who uses it?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitpitch.github.io/gitpitch&#34;&gt;GitPitch&lt;/a&gt; is the perfect slide deck solution for Tech Conferences, Training, Developer Advocates, and Educators. Diagrams is now available as a dedicated &lt;a href=&#34;https://gitpitch.github.io/gitpitch/#/diagrams/cloud-architecture&#34;&gt;Cloud Diagram Markdown Widget&lt;/a&gt; so you can use Diagrams directly on any slide for conferences, meetups, and training.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Cloud-Architects/cloudiscovery&#34;&gt;Cloudiscovery&lt;/a&gt; helps you to analyze resources in your cloud (AWS/GCP/Azure/Alibaba/IBM) account. It allows you to create a diagram of analyzed cloud resource map based on this Diagrams library, so you can draw your existing cloud infrastructure with Cloudiscovery.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/feluelle/airflow-diagrams&#34;&gt;Airflow Diagrams&lt;/a&gt; is an Airflow plugin that aims to easily visualise your Airflow DAGs on service level from providers like AWS, GCP, Azure, etc. via diagrams.&lt;/p&gt; &#xA;&lt;h2&gt;Other languages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you are familiar to Go, you can use &lt;a href=&#34;https://github.com/blushft/go-diagrams&#34;&gt;go-diagrams&lt;/a&gt; as well.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mingrammer/diagrams/master/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zhiqi-li/BEVFormer</title>
    <updated>2022-06-26T02:01:37Z</updated>
    <id>tag:github.com,2022-06-26:/zhiqi-li/BEVFormer</id>
    <link href="https://github.com/zhiqi-li/BEVFormer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the official implementation of BEVFormer, a camera-only framework for autonomous driving perception, e.g., 3D object detection and semantic map segmentation.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;BEVFormer: a Cutting-edge Baseline for Camera-based Detection&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/27915819/161392594-fc0082f7-5c37-4919-830a-2dd423c1d025.mp4&#34;&gt;https://user-images.githubusercontent.com/27915819/161392594-fc0082f7-5c37-4919-830a-2dd423c1d025.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;BEVFormer: Learning Bird&#39;s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/2203.17270&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://www.zhihu.com/question/521842610/answer/2431585901&#34;&gt;Blog&lt;/a&gt; (in Chinese) | Presentation Slides at CVPR 2022 Workshop (soon) | Live-streaming video on BEV Perception (soon)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2022/6/16]: We added two BEVformer configurations, which require less GPU memory than the base version. Please pull this repo to obtain the latest codes.&lt;/li&gt; &#xA; &lt;li&gt;[2022/6/13]: We release an initial version of BEVFormer. It achieves a baseline result of &lt;strong&gt;51.7%&lt;/strong&gt; NDS on nuScenes.&lt;/li&gt; &#xA; &lt;li&gt;[2022/5/23]: 🚀🚀Built on top of BEVFormer, &lt;strong&gt;BEVFormer++&lt;/strong&gt;, gathering up all best practices in recent SOTAs and our unique modification, ranks &lt;strong&gt;1st&lt;/strong&gt; on &lt;a href=&#34;https://waymo.com/open/challenges/2022/3d-camera-only-detection/&#34;&gt;Waymo Open Datast 3D Camera-Only Detection Challenge&lt;/a&gt;. We will present BEVFormer++ on CVPR 2022 Autonomous Driving &lt;a href=&#34;https://cvpr2022.wad.vision/&#34;&gt;Workshop&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2022/3/10]: 🚀BEVFormer achieve the SOTA on &lt;a href=&#34;https://nuscenes.org/object-detection?externalData=all&amp;amp;mapData=all&amp;amp;modalities=Camera&#34;&gt;nuScenes Detection Task&lt;/a&gt; with &lt;strong&gt;56.9% NDS&lt;/strong&gt; (camera-only)!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Abstract&lt;/h1&gt; &#xA;&lt;p&gt;In this work, the authors present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, the authors design a spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, the authors propose a temporal self-attention to recurrently fuse the history BEV information. The proposed approach achieves the new state-of-the-art &lt;strong&gt;56.9%&lt;/strong&gt; in terms of NDS metric on the nuScenes test set, which is &lt;strong&gt;9.0&lt;/strong&gt; points higher than previous best arts and on par with the performance of LiDAR-based baselines.&lt;/p&gt; &#xA;&lt;h1&gt;Methods&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zhiqi-li/BEVFormer/master/figs/arch.png&#34; alt=&#34;method&#34; title=&#34;model arch&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhiqi-li/BEVFormer/master/docs/install.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhiqi-li/BEVFormer/master/docs/prepare_dataset.md&#34;&gt;Prepare Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhiqi-li/BEVFormer/master/docs/getting_started.md&#34;&gt;Run and Eval&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Model Zoo&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lr Schd&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;NDS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;memroy&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;R50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BEVFormer-tiny&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24ep&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6500M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhiqi-li/BEVFormer/master/projects/configs/bevformer/bevformer_tiny.py&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_epoch_24.pth&#34;&gt;modle&lt;/a&gt;/&lt;a href=&#34;https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_epoch_24.log&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zhiqi-li/storage/releases/download/v1.0/r101_dcn_fcos3d_pretrain.pth&#34;&gt;R101-DCN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BEVFormer-small&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24ep&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10500M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhiqi-li/BEVFormer/master/projects/configs/bevformer/bevformer_small.py&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_small_epoch_24.pth&#34;&gt;model&lt;/a&gt;/&lt;a href=&#34;https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_small_epoch_24.log&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zhiqi-li/storage/releases/download/v1.0/r101_dcn_fcos3d_pretrain.pth&#34;&gt;R101-DCN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BEVFormer-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24ep&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28500M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhiqi-li/BEVFormer/master/projects/configs/bevformer/bevformer_base.py&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_r101_dcn_24ep.pth&#34;&gt;model&lt;/a&gt;/&lt;a href=&#34;https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_r101_dcn_24ep.log&#34;&gt;log&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Catalog&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; BEV Segmentation checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; BEV Segmentation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 3D Detection checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 3D Detection code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Initialization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Bibtex&lt;/h1&gt; &#xA;&lt;p&gt;If this work is helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{li2022bevformer,&#xA;  title={BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers},&#xA;  author={Li, Zhiqi and Wang, Wenhai and Li, Hongyang and Xie, Enze and Sima, Chonghao and Lu, Tong and Qiao, Yu and Dai, Jifeng}&#xA;  journal={arXiv preprint arXiv:2203.17270},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;Many thanks to these excellent open source projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WangYueFt/detr3d&#34;&gt;detr3d&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;mmdet3d&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;↳ Stargazers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiqi-li/BEVFormer/stargazers&#34;&gt;&lt;img src=&#34;https://reporoster.com/stars/zhiqi-li/BEVFormer&#34; alt=&#34;Stargazers repo roster for @nastyox/Repo-Roster&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;↳ Forkers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiqi-li/BEVFormer/network/members&#34;&gt;&lt;img src=&#34;https://reporoster.com/forks/zhiqi-li/BEVFormer&#34; alt=&#34;Forkers repo roster for @nastyox/Repo-Roster&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>