<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-08T02:04:07Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mikf/gallery-dl</title>
    <updated>2023-01-08T02:04:07Z</updated>
    <id>tag:github.com,2023-01-08:/mikf/gallery-dl</id>
    <link href="https://github.com/mikf/gallery-dl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Command-line program to download image galleries and collections from several image hosting sites&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;========== gallery-dl&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;gallery-dl&lt;/em&gt; is a command-line program to download image galleries and collections from several image hosting sites (see &lt;code&gt;Supported Sites &amp;lt;docs/supportedsites.md&amp;gt;&lt;/code&gt;&lt;strong&gt;). It is a cross-platform tool with many &lt;code&gt;configuration options &amp;lt;docs/configuration.rst&amp;gt;&lt;/code&gt;&lt;/strong&gt; and powerful &lt;code&gt;filenaming capabilities &amp;lt;docs/formatting.md&amp;gt;&lt;/code&gt;__.&lt;/p&gt; &#xA;&lt;p&gt;|pypi| |build|&lt;/p&gt; &#xA;&lt;p&gt;.. contents::&lt;/p&gt; &#xA;&lt;h1&gt;Dependencies&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python_ 3.4+&lt;/li&gt; &#xA; &lt;li&gt;Requests_&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Optional&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FFmpeg_: Pixiv Ugoira conversion&lt;/li&gt; &#xA; &lt;li&gt;yt-dlp_ or youtube-dl_: Video downloads&lt;/li&gt; &#xA; &lt;li&gt;PySocks_: SOCKS proxy support&lt;/li&gt; &#xA; &lt;li&gt;brotli_ or brotlicffi_: Brotli compression support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Pip&lt;/h2&gt; &#xA;&lt;p&gt;The stable releases of &lt;em&gt;gallery-dl&lt;/em&gt; are distributed on PyPI_ and can be easily installed or upgraded using pip_:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m pip install -U gallery-dl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Installing the latest dev version directly from GitHub can be done with pip_ as well:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m pip install -U -I --no-deps --no-cache-dir https://github.com/mikf/gallery-dl/archive/master.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Windows users should use :code:&lt;code&gt;py -3&lt;/code&gt; instead of :code:&lt;code&gt;python3&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is advised to use the latest version of pip_, including the essential packages :code:&lt;code&gt;setuptools&lt;/code&gt; and :code:&lt;code&gt;wheel&lt;/code&gt;. To ensure these packages are up-to-date, run&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m pip install --upgrade pip setuptools wheel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Standalone Executable&lt;/h2&gt; &#xA;&lt;p&gt;Prebuilt executable files with a Python interpreter and required Python packages included are available for&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Windows &amp;lt;https://github.com/mikf/gallery-dl/releases/download/v1.24.2/gallery-dl.exe&amp;gt;&lt;/code&gt;__ (Requires &lt;code&gt;Microsoft Visual C++ Redistributable Package (x86) &amp;lt;https://aka.ms/vs/17/release/vc_redist.x86.exe&amp;gt;&lt;/code&gt;__)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Linux &amp;lt;https://github.com/mikf/gallery-dl/releases/download/v1.24.2/gallery-dl.bin&amp;gt;&lt;/code&gt;__&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Nightly Builds&lt;/h2&gt; &#xA;&lt;p&gt;| Executables build from the latest commit can be found at | &lt;a href=&#34;https://github.com/mikf/gallery-dl/actions/workflows/executables.yml&#34;&gt;https://github.com/mikf/gallery-dl/actions/workflows/executables.yml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Snap&lt;/h2&gt; &#xA;&lt;p&gt;Linux users that are using a distro that is supported by Snapd_ can install &lt;em&gt;gallery-dl&lt;/em&gt; from the Snap Store:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;snap install gallery-dl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Chocolatey&lt;/h2&gt; &#xA;&lt;p&gt;Windows users that have Chocolatey_ installed can install &lt;em&gt;gallery-dl&lt;/em&gt; from the Chocolatey Community Packages repository:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: powershell&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;choco install gallery-dl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scoop&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;gallery-dl&lt;/em&gt; is also available in the Scoop_ &#34;main&#34; bucket for Windows users:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: powershell&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scoop install gallery-dl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Homebrew&lt;/h2&gt; &#xA;&lt;p&gt;For macOS or Linux users using Homebrew:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install gallery-dl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;To use &lt;em&gt;gallery-dl&lt;/em&gt; simply call it with the URLs you wish to download images from:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl [OPTIONS]... URLS...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use :code:&lt;code&gt;gallery-dl --help&lt;/code&gt; or see &lt;code&gt;&amp;lt;docs/options.md&amp;gt;&lt;/code&gt;__ for a full list of all command-line options.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Download images; in this case from danbooru via tag search for &#39;bonocho&#39;:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl &#34;https://danbooru.donmai.us/posts?tags=bonocho&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Get the direct URL of an image from a site supporting authentication with username &amp;amp; password:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl -g -u &#34;&amp;lt;username&amp;gt;&#34; -p &#34;&amp;lt;password&amp;gt;&#34; &#34;https://twitter.com/i/web/status/604341487988576256&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Filter manga chapters by chapter number and language:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl --chapter-filter &#34;10 &amp;lt;= chapter &amp;lt; 20&#34; -o &#34;lang=fr&#34; &#34;https://mangadex.org/title/59793dd0-a2d8-41a2-9758-8197287a8539&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;| Search a remote resource for URLs and download images from them: | (URLs for which no extractor can be found will be silently ignored)&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl &#34;r:https://pastebin.com/raw/FLwrCYsT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If a site&#39;s address is nonstandard for its extractor, you can prefix the URL with the extractor&#39;s name to force the use of a specific extractor:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl &#34;tumblr:https://sometumblrblog.example&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Configuration&lt;/h1&gt; &#xA;&lt;p&gt;Configuration files for &lt;em&gt;gallery-dl&lt;/em&gt; use a JSON-based file format.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;A list of all available configuration options and their descriptions can be found in &lt;code&gt;&amp;lt;docs/configuration.rst&amp;gt;&lt;/code&gt;__.&lt;/p&gt; &#xA;&lt;p&gt;| For a default configuration file with available options set to their default values, see &lt;code&gt;&amp;lt;docs/gallery-dl.conf&amp;gt;&lt;/code&gt;__.&lt;/p&gt; &#xA;&lt;p&gt;| For a commented example with more involved settings and option usage, see &lt;code&gt;&amp;lt;docs/gallery-dl-example.conf&amp;gt;&lt;/code&gt;__.&lt;/p&gt; &#xA;&lt;h2&gt;Locations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;gallery-dl&lt;/em&gt; searches for configuration files in the following places:&lt;/p&gt; &#xA;&lt;p&gt;Windows: * &lt;code&gt;%APPDATA%\gallery-dl\config.json&lt;/code&gt; * &lt;code&gt;%USERPROFILE%\gallery-dl\config.json&lt;/code&gt; * &lt;code&gt;%USERPROFILE%\gallery-dl.conf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(``%USERPROFILE%`` usually refers to a user&#39;s home directory,&#xA;i.e. ``C:\Users\&amp;lt;username&amp;gt;\``)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Linux, macOS, etc.: * &lt;code&gt;/etc/gallery-dl.conf&lt;/code&gt; * &lt;code&gt;${XDG_CONFIG_HOME}/gallery-dl/config.json&lt;/code&gt; * &lt;code&gt;${HOME}/.config/gallery-dl/config.json&lt;/code&gt; * &lt;code&gt;${HOME}/.gallery-dl.conf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;When run as &lt;code&gt;executable &amp;lt;Standalone Executable_&amp;gt;&lt;/code&gt;__, &lt;em&gt;gallery-dl&lt;/em&gt; will also look for a &lt;code&gt;gallery-dl.conf&lt;/code&gt; file in the same directory as said executable.&lt;/p&gt; &#xA;&lt;p&gt;It is possible to use more than one configuration file at a time. In this case, any values from files after the first will get merged into the already loaded settings and potentially override previous ones.&lt;/p&gt; &#xA;&lt;h1&gt;Authentication&lt;/h1&gt; &#xA;&lt;h2&gt;Username &amp;amp; Password&lt;/h2&gt; &#xA;&lt;p&gt;Some extractors require you to provide valid login credentials in the form of a username &amp;amp; password pair. This is necessary for &lt;code&gt;nijie&lt;/code&gt; and optional for &lt;code&gt;aryion&lt;/code&gt;, &lt;code&gt;danbooru&lt;/code&gt;, &lt;code&gt;e621&lt;/code&gt;, &lt;code&gt;exhentai&lt;/code&gt;, &lt;code&gt;idolcomplex&lt;/code&gt;, &lt;code&gt;imgbb&lt;/code&gt;, &lt;code&gt;inkbunny&lt;/code&gt;, &lt;code&gt;mangadex&lt;/code&gt;, &lt;code&gt;mangoxo&lt;/code&gt;, &lt;code&gt;pillowfort&lt;/code&gt;, &lt;code&gt;sankaku&lt;/code&gt;, &lt;code&gt;subscribestar&lt;/code&gt;, &lt;code&gt;tapas&lt;/code&gt;, &lt;code&gt;tsumino&lt;/code&gt;, &lt;code&gt;twitter&lt;/code&gt;, and &lt;code&gt;zerochan&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can set the necessary information in your &lt;code&gt;configuration file &amp;lt;Configuration_&amp;gt;&lt;/code&gt;__&lt;/p&gt; &#xA;&lt;p&gt;.. code:: json&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;extractor&#34;: {&#xA;        &#34;twitter&#34;: {&#xA;            &#34;username&#34;: &#34;&amp;lt;username&amp;gt;&#34;,&#xA;            &#34;password&#34;: &#34;&amp;lt;password&amp;gt;&#34;&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can provide them directly via the :code:&lt;code&gt;-u/--username&lt;/code&gt; and :code:&lt;code&gt;-p/--password&lt;/code&gt; or via the :code:&lt;code&gt;-o/--option&lt;/code&gt; command-line options&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl -u &#34;&amp;lt;username&amp;gt;&#34; -p &#34;&amp;lt;password&amp;gt;&#34; &#34;URL&#34;&#xA;gallery-dl -o &#34;username=&amp;lt;username&amp;gt;&#34; -o &#34;password=&amp;lt;password&amp;gt;&#34; &#34;URL&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cookies&lt;/h2&gt; &#xA;&lt;p&gt;For sites where login with username &amp;amp; password is not possible due to CAPTCHA or similar, or has not been implemented yet, you can use the cookies from a browser login session and input them into &lt;em&gt;gallery-dl&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This can be done via the &lt;code&gt;cookies &amp;lt;docs/configuration.rst#extractorcookies&amp;gt;&lt;/code&gt;__ option in your configuration file by specifying&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;| the path to a Mozilla/Netscape format cookies.txt file exported by a browser addon | (e.g. &lt;code&gt;Get cookies.txt &amp;lt;https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid/&amp;gt;&lt;/code&gt;__ for Chrome, &lt;code&gt;Export Cookies &amp;lt;https://addons.mozilla.org/en-US/firefox/addon/export-cookies-txt/&amp;gt;&lt;/code&gt;__ for Firefox)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;| a list of name-value pairs gathered from your browser&#39;s web developer tools | (in &lt;code&gt;Chrome &amp;lt;https://developers.google.com/web/tools/chrome-devtools/storage/cookies&amp;gt;&lt;/code&gt;&lt;strong&gt;, in &lt;code&gt;Firefox &amp;lt;https://developer.mozilla.org/en-US/docs/Tools/Storage_Inspector&amp;gt;&lt;/code&gt;&lt;/strong&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;| the name of a browser to extract cookies from | (supported browsers are Chromium-based ones, Firefox, and Safari)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: json&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;extractor&#34;: {&#xA;        &#34;instagram&#34;: {&#xA;            &#34;cookies&#34;: &#34;$HOME/path/to/cookies.txt&#34;&#xA;        },&#xA;        &#34;patreon&#34;: {&#xA;            &#34;cookies&#34;: {&#xA;                &#34;session_id&#34;: &#34;K1T57EKu19TR49C51CDjOJoXNQLF7VbdVOiBrC9ye0a&#34;&#xA;            }&#xA;        },&#xA;        &#34;twitter&#34;: {&#xA;            &#34;cookies&#34;: [&#34;firefox&#34;]&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;| You can also specify a cookies.txt file with the :code:&lt;code&gt;--cookies&lt;/code&gt; command-line option | or a browser to extract cookies from with :code:&lt;code&gt;--cookies-from-browser&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl --cookies &#34;$HOME/path/to/cookies.txt&#34; &#34;URL&#34;&#xA;gallery-dl --cookies-from-browser firefox &#34;URL&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;OAuth&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;gallery-dl&lt;/em&gt; supports user authentication via OAuth_ for some extractors. This is necessary for &lt;code&gt;pixiv&lt;/code&gt; and optional for &lt;code&gt;deviantart&lt;/code&gt;, &lt;code&gt;flickr&lt;/code&gt;, &lt;code&gt;reddit&lt;/code&gt;, &lt;code&gt;smugmug&lt;/code&gt;, &lt;code&gt;tumblr&lt;/code&gt;, and &lt;code&gt;mastodon&lt;/code&gt; instances.&lt;/p&gt; &#xA;&lt;p&gt;Linking your account to &lt;em&gt;gallery-dl&lt;/em&gt; grants it the ability to issue requests on your account&#39;s behalf and enables it to access resources which would otherwise be unavailable to a public user.&lt;/p&gt; &#xA;&lt;p&gt;To do so, start by invoking it with &lt;code&gt;oauth:&amp;lt;sitename&amp;gt;&lt;/code&gt; as an argument. For example:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl oauth:flickr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will be sent to the site&#39;s authorization page and asked to grant read access to &lt;em&gt;gallery-dl&lt;/em&gt;. Authorize it and you will be shown one or more &#34;tokens&#34;, which should be added to your configuration file.&lt;/p&gt; &#xA;&lt;p&gt;To authenticate with a &lt;code&gt;mastodon&lt;/code&gt; instance, run &lt;em&gt;gallery-dl&lt;/em&gt; with &lt;code&gt;oauth:mastodon:&amp;lt;instance&amp;gt;&lt;/code&gt; as argument. For example:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gallery-dl oauth:mastodon:pawoo.net&#xA;gallery-dl oauth:mastodon:https://mastodon.social/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;.. _Python: &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;https://www.python.org/downloads/&lt;/a&gt; .. _PyPI: &lt;a href=&#34;https://pypi.org/&#34;&gt;https://pypi.org/&lt;/a&gt; .. _pip: &lt;a href=&#34;https://pip.pypa.io/en/stable/&#34;&gt;https://pip.pypa.io/en/stable/&lt;/a&gt; .. _Requests: &lt;a href=&#34;https://requests.readthedocs.io/en/master/&#34;&gt;https://requests.readthedocs.io/en/master/&lt;/a&gt; .. _FFmpeg: &lt;a href=&#34;https://www.ffmpeg.org/&#34;&gt;https://www.ffmpeg.org/&lt;/a&gt; .. _yt-dlp: &lt;a href=&#34;https://github.com/yt-dlp/yt-dlp&#34;&gt;https://github.com/yt-dlp/yt-dlp&lt;/a&gt; .. _youtube-dl: &lt;a href=&#34;https://ytdl-org.github.io/youtube-dl/&#34;&gt;https://ytdl-org.github.io/youtube-dl/&lt;/a&gt; .. _PySocks: &lt;a href=&#34;https://pypi.org/project/PySocks/&#34;&gt;https://pypi.org/project/PySocks/&lt;/a&gt; .. _brotli: &lt;a href=&#34;https://github.com/google/brotli&#34;&gt;https://github.com/google/brotli&lt;/a&gt; .. _brotlicffi: &lt;a href=&#34;https://github.com/python-hyper/brotlicffi&#34;&gt;https://github.com/python-hyper/brotlicffi&lt;/a&gt; .. _Snapd: &lt;a href=&#34;https://docs.snapcraft.io/installing-snapd&#34;&gt;https://docs.snapcraft.io/installing-snapd&lt;/a&gt; .. _OAuth: &lt;a href=&#34;https://en.wikipedia.org/wiki/OAuth&#34;&gt;https://en.wikipedia.org/wiki/OAuth&lt;/a&gt; .. _Chocolatey: &lt;a href=&#34;https://chocolatey.org/install&#34;&gt;https://chocolatey.org/install&lt;/a&gt; .. _Scoop: &lt;a href=&#34;https://scoop.sh&#34;&gt;https://scoop.sh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |pypi| image:: &lt;a href=&#34;https://img.shields.io/pypi/v/gallery-dl.svg&#34;&gt;https://img.shields.io/pypi/v/gallery-dl.svg&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/project/gallery-dl/&#34;&gt;https://pypi.org/project/gallery-dl/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |build| image:: &lt;a href=&#34;https://github.com/mikf/gallery-dl/workflows/tests/badge.svg&#34;&gt;https://github.com/mikf/gallery-dl/workflows/tests/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/mikf/gallery-dl/actions&#34;&gt;https://github.com/mikf/gallery-dl/actions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |gitter| image:: &lt;a href=&#34;https://badges.gitter.im/gallery-dl/main.svg&#34;&gt;https://badges.gitter.im/gallery-dl/main.svg&lt;/a&gt; :target: &lt;a href=&#34;https://gitter.im/gallery-dl/main&#34;&gt;https://gitter.im/gallery-dl/main&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>neonbjb/tortoise-tts</title>
    <updated>2023-01-08T02:04:07Z</updated>
    <id>tag:github.com,2023-01-08:/neonbjb/tortoise-tts</id>
    <link href="https://github.com/neonbjb/tortoise-tts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A multi-voice TTS system trained with an emphasis on quality&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TorToiSe&lt;/h1&gt; &#xA;&lt;p&gt;Tortoise is a text-to-speech program built with the following priorities:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Strong multi-voice capabilities.&lt;/li&gt; &#xA; &lt;li&gt;Highly realistic prosody and intonation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This repo contains all the code needed to run Tortoise TTS in inference mode.&lt;/p&gt; &#xA;&lt;p&gt;A (&lt;em&gt;very&lt;/em&gt;) rough draft of the Tortoise paper is now available in doc format. I would definitely appreciate any comments, suggestions or reviews: &lt;a href=&#34;https://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA&#34;&gt;https://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Version history&lt;/h3&gt; &#xA;&lt;h4&gt;v2.4; 2022/5/17&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed CVVP model. Found that it does not, in fact, make an appreciable difference in the output.&lt;/li&gt; &#xA; &lt;li&gt;Add better debugging support; existing tools now spit out debug files which can be used to reproduce bad runs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.3; 2022/5/12&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New CLVP-large model for further improved decoding guidance.&lt;/li&gt; &#xA; &lt;li&gt;Improvements to read.py and do_tts.py (new options)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.2; 2022/5/5&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added several new voices from the training set.&lt;/li&gt; &#xA; &lt;li&gt;Automated redaction. Wrap the text you want to use to prompt the model but not be spoken in brackets.&lt;/li&gt; &#xA; &lt;li&gt;Bug fixes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.1; 2022/5/2&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added ability to produce totally random voices.&lt;/li&gt; &#xA; &lt;li&gt;Added ability to download voice conditioning latent via a script, and then use a user-provided conditioning latent.&lt;/li&gt; &#xA; &lt;li&gt;Added ability to use your own pretrained models.&lt;/li&gt; &#xA; &lt;li&gt;Refactored directory structures.&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements &amp;amp; bug fixes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s in a name?&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;m naming my speech-related repos after Mojave desert flora and fauna. Tortoise is a bit tongue in cheek: this model is insanely slow. It leverages both an autoregressive decoder &lt;strong&gt;and&lt;/strong&gt; a diffusion decoder; both known for their low sampling rates. On a K80, expect to generate a medium sized sentence every 2 minutes.&lt;/p&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://nonint.com/static/tortoise_v2_examples.html&#34;&gt;this page&lt;/a&gt; for a large list of example outputs.&lt;/p&gt; &#xA;&lt;p&gt;Cool application of Tortoise+GPT-3 (not by me): &lt;a href=&#34;https://twitter.com/lexman_ai&#34;&gt;https://twitter.com/lexman_ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage guide&lt;/h2&gt; &#xA;&lt;h3&gt;Colab&lt;/h3&gt; &#xA;&lt;p&gt;Colab is the easiest way to try this out. I&#39;ve put together a notebook you can use here: &lt;a href=&#34;https://colab.research.google.com/drive/1wVVqUPqwiDBUVeWWOUNglpGhU3hg_cbR?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1wVVqUPqwiDBUVeWWOUNglpGhU3hg_cbR?usp=sharing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Local Installation&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use this on your own computer, you must have an NVIDIA GPU.&lt;/p&gt; &#xA;&lt;p&gt;First, install pytorch using these instructions: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;. On Windows, I &lt;strong&gt;highly&lt;/strong&gt; recommend using the Conda installation path. I have been told that if you do not do this, you will spend a lot of time chasing dependency problems.&lt;/p&gt; &#xA;&lt;p&gt;Next, install TorToiSe and it&#39;s dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/neonbjb/tortoise-tts.git&#xA;cd tortoise-tts&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on windows, you will also need to install pysoundfile: &lt;code&gt;conda install -c conda-forge pysoundfile&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;do_tts.py&lt;/h3&gt; &#xA;&lt;p&gt;This script allows you to speak a single phrase with one or more voices.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tortoise/do_tts.py --text &#34;I&#39;m going to speak this&#34; --voice random --preset fast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;read.py&lt;/h3&gt; &#xA;&lt;p&gt;This script provides tools for reading large amounts of text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tortoise/read.py --textfile &amp;lt;your text to be read&amp;gt; --voice random&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will break up the textfile into sentences, and then convert them to speech one at a time. It will output a series of spoken clips as they are generated. Once all the clips are generated, it will combine them into a single file and output that as well.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes Tortoise screws up an output. You can re-generate any bad clips by re-running &lt;code&gt;read.py&lt;/code&gt; with the --regenerate argument.&lt;/p&gt; &#xA;&lt;h3&gt;API&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise can be used programmatically, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech()&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, voice_samples=reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Voice customization guide&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise was specifically trained to be a multi-speaker model. It accomplishes this by consulting reference clips.&lt;/p&gt; &#xA;&lt;p&gt;These reference clips are recordings of a speaker that you provide to guide speech generation. These clips are used to determine many properties of the output, such as the pitch and tone of the voice, speaking speed, and even speaking defects like a lisp or stuttering. The reference clip is also used to determine non-voice related aspects of the audio output like volume, background noise, recording quality and reverb.&lt;/p&gt; &#xA;&lt;h3&gt;Random voice&lt;/h3&gt; &#xA;&lt;p&gt;I&#39;ve included a feature which randomly generates a voice. These voices don&#39;t actually exist and will be random every time you run it. The results are quite fascinating and I recommend you play around with it!&lt;/p&gt; &#xA;&lt;p&gt;You can use the random voice by passing in &#39;random&#39; as the voice name. Tortoise will take care of the rest.&lt;/p&gt; &#xA;&lt;p&gt;For the those in the ML space: this is created by projecting a random vector onto the voice conditioning latent space.&lt;/p&gt; &#xA;&lt;h3&gt;Provided voices&lt;/h3&gt; &#xA;&lt;p&gt;This repo comes with several pre-packaged voices. Voices prepended with &#34;train_&#34; came from the training set and perform far better than the others. If your goal is high quality speech, I recommend you pick one of them. If you want to see what Tortoise can do for zero-shot mimicing, take a look at the others.&lt;/p&gt; &#xA;&lt;h3&gt;Adding a new voice&lt;/h3&gt; &#xA;&lt;p&gt;To add new voices to Tortoise, you will need to do the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Gather audio clips of your speaker(s). Good sources are YouTube interviews (you can use youtube-dl to fetch the audio), audiobooks or podcasts. Guidelines for good clips are in the next section.&lt;/li&gt; &#xA; &lt;li&gt;Cut your clips into ~10 second segments. You want at least 3 clips. More is better, but I only experimented with up to 5 in my testing.&lt;/li&gt; &#xA; &lt;li&gt;Save the clips as a WAV file with floating point format and a 22,050 sample rate.&lt;/li&gt; &#xA; &lt;li&gt;Create a subdirectory in voices/&lt;/li&gt; &#xA; &lt;li&gt;Put your clips in that subdirectory.&lt;/li&gt; &#xA; &lt;li&gt;Run tortoise utilities with --voice=&amp;lt;your_subdirectory_name&amp;gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Picking good reference clips&lt;/h3&gt; &#xA;&lt;p&gt;As mentioned above, your reference clips have a profound impact on the output of Tortoise. Following are some tips for picking good clips:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Avoid clips with background music, noise or reverb. These clips were removed from the training dataset. Tortoise is unlikely to do well with them.&lt;/li&gt; &#xA; &lt;li&gt;Avoid speeches. These generally have distortion caused by the amplification system.&lt;/li&gt; &#xA; &lt;li&gt;Avoid clips from phone calls.&lt;/li&gt; &#xA; &lt;li&gt;Avoid clips that have excessive stuttering, stammering or words like &#34;uh&#34; or &#34;like&#34; in them.&lt;/li&gt; &#xA; &lt;li&gt;Try to find clips that are spoken in such a way as you wish your output to sound like. For example, if you want to hear your target voice read an audiobook, try to find clips of them reading a book.&lt;/li&gt; &#xA; &lt;li&gt;The text being spoken in the clips does not matter, but diverse text does seem to perform better.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Generation settings&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise is primarily an autoregressive decoder model combined with a diffusion model. Both of these have a lot of knobs that can be turned that I&#39;ve abstracted away for the sake of ease of use. I did this by generating thousands of clips using various permutations of the settings and using a metric for voice realism and intelligibility to measure their effects. I&#39;ve set the defaults to the best overall settings I was able to find. For specific use-cases, it might be effective to play with these settings (and it&#39;s very likely that I missed something!)&lt;/p&gt; &#xA;&lt;p&gt;These settings are not available in the normal scripts packaged with Tortoise. They are available, however, in the API. See &lt;code&gt;api.tts&lt;/code&gt; for a full list.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt engineering&lt;/h3&gt; &#xA;&lt;p&gt;Some people have discovered that it is possible to do prompt engineering with Tortoise! For example, you can evoke emotion by including things like &#34;I am really sad,&#34; before your text. I&#39;ve built an automated redaction system that you can use to take advantage of this. It works by attempting to redact any text in the prompt surrounded by brackets. For example, the prompt &#34;[I am really sad,] Please feed me.&#34; will only speak the words &#34;Please feed me&#34; (with a sad tonality).&lt;/p&gt; &#xA;&lt;h3&gt;Playing with the voice latent&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise ingests reference clips by feeding them through individually through a small submodel that produces a point latent, then taking the mean of all of the produced latents. The experimentation I have done has indicated that these point latents are quite expressive, affecting everything from tone to speaking rate to speech abnormalities.&lt;/p&gt; &#xA;&lt;p&gt;This lends itself to some neat tricks. For example, you can combine feed two different voices to tortoise and it will output what it thinks the &#34;average&#34; of those two voices sounds like.&lt;/p&gt; &#xA;&lt;h4&gt;Generating conditioning latents from voices&lt;/h4&gt; &#xA;&lt;p&gt;Use the script &lt;code&gt;get_conditioning_latents.py&lt;/code&gt; to extract conditioning latents for a voice you have installed. This script will dump the latents to a .pth pickle file. The file will contain a single tuple, (autoregressive_latent, diffusion_latent).&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, use the api.TextToSpeech.get_conditioning_latents() to fetch the latents.&lt;/p&gt; &#xA;&lt;h4&gt;Using raw conditioning latents to generate speech&lt;/h4&gt; &#xA;&lt;p&gt;After you&#39;ve played with them, you can use them to generate speech by creating a subdirectory in voices/ with a single &#34;.pth&#34; file containing the pickled conditioning latents as a tuple (autoregressive_latent, diffusion_latent).&lt;/p&gt; &#xA;&lt;h3&gt;Send me feedback!&lt;/h3&gt; &#xA;&lt;p&gt;Probabilistic models like Tortoise are best thought of as an &#34;augmented search&#34; - in this case, through the space of possible utterances of a specific string of text. The impact of community involvement in perusing these spaces (such as is being done with GPT-3 or CLIP) has really surprised me. If you find something neat that you can do with Tortoise that isn&#39;t documented here, please report it to me! I would be glad to publish it to this page.&lt;/p&gt; &#xA;&lt;h2&gt;Tortoise-detect&lt;/h2&gt; &#xA;&lt;p&gt;Out of concerns that this model might be misused, I&#39;ve built a classifier that tells the likelihood that an audio clip came from Tortoise.&lt;/p&gt; &#xA;&lt;p&gt;This classifier can be run on any computer, usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python tortoise/is_this_from_tortoise.py --clip=&amp;lt;path_to_suspicious_audio_file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This model has 100% accuracy on the contents of the results/ and voices/ folders in this repo. Still, treat this classifier as a &#34;strong signal&#34;. Classifiers can be fooled and it is likewise not impossible for this classifier to exhibit false positives.&lt;/p&gt; &#xA;&lt;h2&gt;Model architecture&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise TTS is inspired by OpenAI&#39;s DALLE, applied to speech data and using a better decoder. It is made up of 5 separate models that work together. I&#39;ve assembled a write-up of the system architecture here: &lt;a href=&#34;https://nonint.com/2022/04/25/tortoise-architectural-design-doc/&#34;&gt;https://nonint.com/2022/04/25/tortoise-architectural-design-doc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;These models were trained on my &#34;homelab&#34; server with 8 RTX 3090s over the course of several months. They were trained on a dataset consisting of ~50k hours of speech data, most of which was transcribed by &lt;a href=&#34;http://www.github.com/neonbjb/ocotillo&#34;&gt;ocotillo&lt;/a&gt;. Training was done on my own &lt;a href=&#34;https://github.com/neonbjb/DL-Art-School&#34;&gt;DLAS&lt;/a&gt; trainer.&lt;/p&gt; &#xA;&lt;p&gt;I currently do not have plans to release the training configurations or methodology. See the next section..&lt;/p&gt; &#xA;&lt;h2&gt;Ethical Considerations&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise v2 works considerably better than I had planned. When I began hearing some of the outputs of the last few versions, I began wondering whether or not I had an ethically unsound project on my hands. The ways in which a voice-cloning text-to-speech system could be misused are many. It doesn&#39;t take much creativity to think up how.&lt;/p&gt; &#xA;&lt;p&gt;After some thought, I have decided to go forward with releasing this. Following are the reasons for this choice:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It is primarily good at reading books and speaking poetry. Other forms of speech do not work well.&lt;/li&gt; &#xA; &lt;li&gt;It was trained on a dataset which does not have the voices of public figures. While it will attempt to mimic these voices if they are provided as references, it does not do so in such a way that most humans would be fooled.&lt;/li&gt; &#xA; &lt;li&gt;The above points could likely be resolved by scaling up the model and the dataset. For this reason, I am currently withholding details on how I trained the model, pending community feedback.&lt;/li&gt; &#xA; &lt;li&gt;I am releasing a separate classifier model which will tell you whether a given audio clip was generated by Tortoise or not. See &lt;code&gt;tortoise-detect&lt;/code&gt; above.&lt;/li&gt; &#xA; &lt;li&gt;If I, a tinkerer with a BS in computer science with a ~$15k computer can build this, then any motivated corporation or state can as well. I would prefer that it be in the open and everyone know the kinds of things ML can do.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Diversity&lt;/h3&gt; &#xA;&lt;p&gt;The diversity expressed by ML models is strongly tied to the datasets they were trained on.&lt;/p&gt; &#xA;&lt;p&gt;Tortoise was trained primarily on a dataset consisting of audiobooks. I made no effort to balance diversity in this dataset. For this reason, Tortoise will be particularly poor at generating the voices of minorities or of people who speak with strong accents.&lt;/p&gt; &#xA;&lt;h2&gt;Looking forward&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise v2 is about as good as I think I can do in the TTS world with the resources I have access to. A phenomenon that happens when training very large models is that as parameter count increases, the communication bandwidth needed to support distributed training of the model increases multiplicatively. On enterprise-grade hardware, this is not an issue: GPUs are attached together with exceptionally wide buses that can accommodate this bandwidth. I cannot afford enterprise hardware, though, so I am stuck.&lt;/p&gt; &#xA;&lt;p&gt;I want to mention here that I think Tortoise could do be a &lt;strong&gt;lot&lt;/strong&gt; better. The three major components of Tortoise are either vanilla Transformer Encoder stacks or Decoder stacks. Both of these types of models have a rich experimental history with scaling in the NLP realm. I see no reason to believe that the same is not true of TTS.&lt;/p&gt; &#xA;&lt;p&gt;The largest model in Tortoise v2 is considerably smaller than GPT-2 large. It is 20x smaller that the original DALLE transformer. Imagine what a TTS model trained at or near GPT-3 or DALLE scale could achieve.&lt;/p&gt; &#xA;&lt;p&gt;If you are an ethical organization with computational resources to spare interested in seeing what this model could do if properly scaled out, please reach out to me! I would love to collaborate on this.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project has garnered more praise than I expected. I am standing on the shoulders of giants, though, and I want to credit a few of the amazing folks in the community that have helped make this happen:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face, who wrote the GPT model and the generate API used by Tortoise, and who hosts the model weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.12092.pdf&#34;&gt;Ramesh et al&lt;/a&gt; who authored the DALLE paper, which is the inspiration behind Tortoise.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.09672.pdf&#34;&gt;Nichol and Dhariwal&lt;/a&gt; who authored the (revision of) the code that drives the diffusion model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.07889.pdf&#34;&gt;Jang et al&lt;/a&gt; who developed and open-sourced univnet, the vocoder this repo uses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mindslab-ai/univnet&#34;&gt;Kim and Jung&lt;/a&gt; who implemented univnet pytorch model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lucidrains&#34;&gt;lucidrains&lt;/a&gt; who writes awesome open source pytorch models, many of which are used here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/patrickvonplaten&#34;&gt;Patrick von Platen&lt;/a&gt; whose guides on setting up wav2vec were invaluable to building my dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notice&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise was built entirely by me using my own hardware. My employer was not involved in any facet of Tortoise&#39;s development.&lt;/p&gt; &#xA;&lt;p&gt;If you use this repo or the ideas therein for your research, please cite it! A bibtex entree can be found in the right pane on GitHub.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lucidrains/PaLM-rlhf-pytorch</title>
    <updated>2023-01-08T02:04:07Z</updated>
    <id>tag:github.com,2023-01-08:/lucidrains/PaLM-rlhf-pytorch</id>
    <link href="https://github.com/lucidrains/PaLM-rlhf-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;PaLM + RLHF - Pytorch (wip)&lt;/h2&gt; &#xA;&lt;p&gt;Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Maybe I&#39;ll add retrieval functionality too, à la &lt;a href=&#34;https://github.com/lucidrains/RETRO-pytorch&#34;&gt;RETRO&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in replicating something like ChatGPT out in the open, please consider joining &lt;a href=&#34;https://discord.gg/xBPBXfcFHd&#34;&gt;Laion &lt;img alt=&#34;Join us on Discord&#34; src=&#34;https://img.shields.io/discord/823813159592001537?color=5865F2&amp;amp;logo=discord&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository has gone viral without my permission. Next time, if you are promoting my unfinished repositories (notice the work in progress flag) for twitter engagement or eyeballs, at least (1) do your research or (2) be totally transparent with your readers about the capacity of the repository without resorting to clickbait. (1) I was not the first, CarperAI had been working on RLHF months before, link below. (2) There is no trained model. This is just the ship and overall map. We still need millions of dollars of compute + data to sail to the correct point in high dimensional parameter space. Even then, you need professional sailors (like Robin Rombach of Stable Diffusion fame) to actually guide the ship through turbulent times to that point.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://carper.ai/&#34;&gt;CarperAI&lt;/a&gt; had been working on &lt;a href=&#34;https://github.com/CarperAI/trlx&#34;&gt;an RLHF framework&lt;/a&gt; for large language models&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sswA4j_IUxg&#34;&gt;Yannic Kilcher&lt;/a&gt; is also working on an &lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant&#34;&gt;open sourced implementation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SWwQ3k-DWyo&#34;&gt;AI Coffeebreak w/ Letitia&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=NpmnWgQgcsA&#34;&gt;Code Emporium&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://stability.ai/&#34;&gt;Stability.ai&lt;/a&gt; for the generous sponsorship to work on cutting edge artificial intelligence research&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/&#34;&gt;🤗 Hugging Face&lt;/a&gt; and &lt;a href=&#34;https://carper.ai/&#34;&gt;CarperAI&lt;/a&gt; for penning the blog post &lt;a href=&#34;https://huggingface.co/blog/rlhf&#34;&gt;Illustrating Reinforcement Learning from Human Feedback (RLHF)&lt;/a&gt;, and the former also for their &lt;a href=&#34;https://huggingface.co/docs/accelerate/index&#34;&gt;accelerate&lt;/a&gt; library&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install palm-rlhf-pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;First train &lt;code&gt;PaLM&lt;/code&gt;, like any other autoregressive transformer&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from palm_rlhf_pytorch import PaLM&#xA;&#xA;palm = PaLM(&#xA;    num_tokens = 20000,&#xA;    dim = 512,&#xA;    depth = 12&#xA;).cuda()&#xA;&#xA;seq = torch.randint(0, 20000, (1, 2048)).cuda()&#xA;&#xA;loss = palm(seq, return_loss = True)&#xA;loss.backward()&#xA;&#xA;# after much training, you can now generate sequences&#xA;&#xA;generated = palm.generate(2048) # (1, 2048)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then train your reward model, with the curated human feedback. In the original paper, they could not get reward model to be finetuned from a pretrained transformer without overfitting, but I gave the option to finetune with &lt;code&gt;LoRA&lt;/code&gt; anyways, since it is still open research.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from palm_rlhf_pytorch import PaLM, RewardModel&#xA;&#xA;palm = PaLM(&#xA;    num_tokens = 20000,&#xA;    dim = 512,&#xA;    depth = 12,&#xA;    causal = False&#xA;)&#xA;&#xA;reward_model = RewardModel(&#xA;    palm,&#xA;    num_binned_output = 5 # say rating from 1 to 5&#xA;).cuda()&#xA;&#xA;# mock data&#xA;&#xA;seq = torch.randint(0, 20000, (1, 1024)).cuda()&#xA;prompt_mask = torch.zeros(1, 1024).bool().cuda() # which part of the sequence is prompt, which part is response&#xA;labels = torch.randint(0, 5, (1,)).cuda()&#xA;&#xA;# train&#xA;&#xA;loss = reward_model(seq, prompt_mask = prompt_mask, labels = labels)&#xA;loss.backward()&#xA;&#xA;# after much training&#xA;&#xA;reward = reward_model(seq, prompt_mask = prompt_mask)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you will pass your transformer and the rewards model to the &lt;code&gt;RLHFTrainer&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from palm_rlhf_pytorch import PaLM, RewardModel, RLHFTrainer&#xA;&#xA;# load your pretrained palm&#xA;&#xA;palm = PaLM(&#xA;    num_tokens = 20000,&#xA;    dim = 512,&#xA;    depth = 12&#xA;).cuda()&#xA;&#xA;palm.load(&#39;./path/to/pretrained/palm.pt&#39;)&#xA;&#xA;# load your pretrained reward model&#xA;&#xA;reward_model = RewardModel(&#xA;    palm,&#xA;    num_binned_output = 5&#xA;).cuda()&#xA;&#xA;reward_model.load(&#39;./path/to/pretrained/reward_model.pt&#39;)&#xA;&#xA;# ready your list of prompts for reinforcement learning&#xA;&#xA;prompts = torch.randint(0, 256, (50000, 512)).cuda() # 50k prompts&#xA;&#xA;# pass it all to the trainer and train&#xA;&#xA;trainer = RLHFTrainer(&#xA;    palm = palm,&#xA;    reward_model = reward_model,&#xA;    prompt_token_ids = prompts&#xA;)&#xA;&#xA;trainer.train(num_episodes = 50000)&#xA;&#xA;# then, if it succeeded...&#xA;# generate say 10 samples and use the reward model to return the best one&#xA;&#xA;answer = trainer.generate(2048, prompt = prompts[0], num_samples = 10) # (&amp;lt;= 2048,)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;clone base transformer with separate lora for critic&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;also allow for non-LoRA based finetuning&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;redo normalize to be able to have a masked version, not sure if anyone will ever use per token rewards / values, but good practice to implement&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;add Hugging Face accelerate and test out wandb instrumentation&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;search literature to figure out what is the latest SOTA for PPO, assuming RL field is still making progress.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;test the system using a pretrained sentiment network as reward model&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;write the memory in PPO to memmapped numpy file&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;get sampling with variable lengthed prompts working, even if it is not needed given bottleneck is human feedback&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;allow for finetuning penultimate N layers only in either actor or critic, assuming if pretrained&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;incorporate some learning points from Sparrow, given Letitia&#39;s video&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;simple web interface with django + htmx for collecting human feedback&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;equip with &lt;a href=&#34;https://github.com/hazyResearch/flash-attention&#34;&gt;the best attention&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Stiennon2020LearningTS,&#xA;    title   = {Learning to summarize from human feedback},&#xA;    author  = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan J. Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},&#xA;    journal = {ArXiv},&#xA;    year    = {2020},&#xA;    volume  = {abs/2009.01325}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Chowdhery2022PaLMSL,&#xA;    title   = {PaLM: Scaling Language Modeling with Pathways},&#xA;    author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\&#39;i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Oliveira Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Hu2021LoRALA,&#xA;    title   = {LoRA: Low-Rank Adaptation of Large Language Models},&#xA;    author  = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},&#xA;    journal = {ArXiv},&#xA;    year    = {2021},&#xA;    volume  = {abs/2106.09685}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Sun2022ALT,&#xA;  title     = {A Length-Extrapolatable Transformer},&#xA;  author    = {Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},&#xA;  year      = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>