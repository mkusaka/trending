<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-05T01:40:54Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bytedance/monolith</title>
    <updated>2025-01-05T01:40:54Z</updated>
    <id>tag:github.com,2025-01-05:/bytedance/monolith</id>
    <link href="https://github.com/bytedance/monolith" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Lightweight Recommendation System&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Monolith&lt;/p&gt; &#xA;&lt;h2&gt;What is it?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.07663&#34;&gt;Monolith&lt;/a&gt; is a deep learning framework for large scale recommendation modeling. It introduces two important features which are crucial for advanced recommendation system:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;collisionless embedding tables guarantees unique represeantion for different id features&lt;/li&gt; &#xA; &lt;li&gt;real time training captures the latest hotspots and help users to discover new intersts rapidly&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Monolith is built on the top of TensorFlow and supports batch/real-time training and serving.&lt;/p&gt; &#xA;&lt;h2&gt;Discussion Group&lt;/h2&gt; &#xA;&lt;h3&gt;Join us at Discord&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/QYTDeKxGMX&#34;&gt;https://discord.gg/QYTDeKxGMX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;p&gt;Currently, we only support compilation on the Linux.&lt;/p&gt; &#xA;&lt;p&gt;First, download bazel 3.1.0&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/bazelbuild/bazel/releases/download/3.1.0/bazel-3.1.0-installer-linux-x86_64.sh &amp;amp;&amp;amp; \&#xA;  chmod +x bazel-3.1.0-installer-linux-x86_64.sh &amp;amp;&amp;amp; \&#xA;  ./bazel-3.1.0-installer-linux-x86_64.sh &amp;amp;&amp;amp; \&#xA;  rm bazel-3.1.0-installer-linux-x86_64.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, prepare a python environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U --user pip numpy wheel packaging requests opt_einsum&#xA;pip install -U --user keras_preprocessing --no-deps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, you can build any target in the monolith. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bazel run //monolith/native_training:demo --output_filter=IGNORE_LOGS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Demo and tutorials&lt;/h3&gt; &#xA;&lt;p&gt;There are a tutorial in &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/monolith/master/markdown/demo&#34;&gt;markdown/demo&lt;/a&gt; on how to run distributed async training, and few guides on how to use the &lt;code&gt;MonolithModel&lt;/code&gt; API &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/monolith/master/markdown&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DrewThomasson/ebook2audiobook</title>
    <updated>2025-01-05T01:40:54Z</updated>
    <id>tag:github.com,2025-01-05:/DrewThomasson/ebook2audiobook</id>
    <link href="https://github.com/DrewThomasson/ebook2audiobook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning. Supports 1,107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üìö ebook2audiobook&lt;/h1&gt; &#xA;&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br&gt; using Calibre, ffmpeg, XTTSv2, Fairseq and more. Supports voice cloning and 1124 languages!&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/bg5Kx43c6w&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/https://discord.gg/bg5Kx43c6w&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;New v2.0 Web GUI Interface!&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif&#34; alt=&#34;demo_web_gui&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 1&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 2&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 3&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;README.md&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ara &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/readme/README_AR.md&#34;&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;zho &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/readme/README_CN.md&#34;&gt;‰∏≠Êñá (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;eng &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/README.md&#34;&gt;English&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;swe &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/readme/README_SWE.md&#34;&gt;Svenska (Swedish)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#ebook2audiobook&#34;&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#new-v20-web-gui-interface&#34;&gt;New v2.0 Web GUI Interface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#huggingface-space-demo&#34;&gt;Huggingface Space Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#free-google-colab&#34;&gt;Free Google Colab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos&#34;&gt;Pre-made Audio Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages&#34;&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#installation-instructions&#34;&gt;Installation Instructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface&#34;&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic-headless-usage&#34;&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#headless-custom-xtts-model-usage&#34;&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#renting-a-gpu&#34;&gt;Renting a GPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output&#34;&gt;Help command output&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models&#34;&gt;Fine Tuned TTS models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection&#34;&gt;For Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#using-docker&#34;&gt;Using Docker&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-docker-container&#34;&gt;Docker Run&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container&#34;&gt;Docker Build&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose&#34;&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide&#34;&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations&#34;&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues&#34;&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats&#34;&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output&#34;&gt;Output&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues&#34;&gt;Common Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks&#34;&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#join-our-discord-server&#34;&gt;Join Our Discord Server!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#legacy-v10&#34;&gt;Legacy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#glossary-of-sections&#34;&gt;Glossary of Sections&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ Converts eBooks to text format with Calibre.&lt;/li&gt; &#xA; &lt;li&gt;üìö Splits eBook into chapters for organized audio.&lt;/li&gt; &#xA; &lt;li&gt;üéôÔ∏è High-quality text-to-speech with &lt;a href=&#34;https://huggingface.co/coqui/XTTS-v2&#34;&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/main/examples/mms&#34;&gt;Fairseq&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üó£Ô∏è Optional voice cloning with your own voice file.&lt;/li&gt; &#xA; &lt;li&gt;üåç Supports 1107 languages (English by default). &lt;a href=&#34;https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html&#34;&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üñ•Ô∏è Designed to run on 4GB RAM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/ebook2audiobook&#34;&gt;Huggingface space demo&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/ebook2audiobook&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=for-the-badge&amp;amp;logo=huggingface&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Huggingface space is running on free cpu tier so expect very slow or timeout lol, just don&#39;t give it giant files is all&lt;/li&gt; &#xA; &lt;li&gt;Best to duplicate space or run locally.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Free Google Colab&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Free Google Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported Languages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Arabic (ara)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chinese (zho)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Czech (ces)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dutch (nld)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;English (eng)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;French (fra)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;German (deu)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hindi (hin)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hungarian (hun)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Italian (ita)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Japanese (jpn)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Korean (kor)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Polish (pol)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Portuguese (por)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Russian (rus)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spanish (spa)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Turkish (tur)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vietnamese (vie)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html&#34;&gt;** + 1107 languages via Fairseq**&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;4gb ram&lt;/li&gt; &#xA; &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation Instructions&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specify the language code when running the script in mode.&lt;/p&gt; &#xA;&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./ebook2audiobook.sh  # Run Launch script&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.\ebook2audiobook.cmd  # Run launch script&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: Add &lt;code&gt;--share&lt;/code&gt; to the end of it like this: &lt;code&gt;python app.py --share&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;--help&lt;/code&gt; parameter like this &lt;code&gt;python app.py --help&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Basic Usage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./ebook2audiobook.sh  -- --ebook &amp;lt;path_to_ebook_file&amp;gt; --voice [path_to_voice_file] --language [language_code]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.\ebook2audiobook.cmd  -- --ebook &amp;lt;path_to_ebook_file&amp;gt; --voice [path_to_voice_file] --language [language_code]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;path_to_ebook_file&amp;gt;&lt;/strong&gt;: Path to your eBook file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[path_to_voice_file]&lt;/strong&gt;: Optional for voice cloning.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[language_code]&lt;/strong&gt;: Optional to specify ISO-639-3 3+ letters language code (default is eng). ISO-639-1 2 letters code is also supported&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;--help&lt;/code&gt; parameter like this &lt;code&gt;python app.py --help&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Custom XTTS Model Usage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./ebook2audiobook.sh  -- --ebook &amp;lt;ebook_file_path&amp;gt; --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt; --custom_config &amp;lt;custom_config_path&amp;gt; --custom_vocab &amp;lt;custom_vocab_path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.\ebook2audiobook.cmd  -- --ebook &amp;lt;ebook_file_path&amp;gt; --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt; --custom_config &amp;lt;custom_config_path&amp;gt; --custom_vocab &amp;lt;custom_vocab_path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;ebook_file_path&amp;gt;&lt;/strong&gt;: Path to your eBook file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;target_voice_file_path&amp;gt;&lt;/strong&gt;: Optional for voice cloning.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&#xA;    &lt;language&gt;&lt;/language&gt;&lt;/strong&gt;: Optional to specify language.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model.pth&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;custom_config_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;custom_vocab_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;vocab.json&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;--help&lt;/code&gt; parameter like this &lt;code&gt;python app.py --help&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./ebook2audiobook.sh  --help&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.\ebook2audiobook.cmd  --help&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a id=&#34;help-command-output&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This will output the following:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;usage: app.py [-h] [--script_mode SCRIPT_MODE] [--share] [-- []]&#xA;              [--session SESSION] [--ebook EBOOK] [--ebooks_dir [EBOOKS_DIR]]&#xA;              [--voice VOICE] [--language LANGUAGE] [--device {cpu,gpu}]&#xA;              [--custom_model CUSTOM_MODEL] [--temperature TEMPERATURE]&#xA;              [--length_penalty LENGTH_PENALTY]&#xA;              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]&#xA;              [--speed SPEED] [--enable_text_splitting] [--fine_tuned FINE_TUNED]&#xA;              [--version]&#xA;&#xA;Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in  mode for direct conversion.&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --script_mode SCRIPT_MODE&#xA;                        Force the script to run in NATIVE or DOCKER_UTILS&#xA;  --share               Enable a public shareable Gradio link. Default to False.&#xA;  -- []&#xA;                        Run in  mode. Default to True if the flag is present without a value, False otherwise.&#xA;  --session SESSION     Session to reconnect in case of interruption ( mode only)&#xA;  --ebook EBOOK         Path to the ebook file for conversion. Required in  mode.&#xA;  --ebooks_dir [EBOOKS_DIR]&#xA;                        Path to the directory containing ebooks for batch conversion. Default to &#34;ebooks&#34; if &#34;default&#34; is provided.&#xA;  --voice VOICE         Path to the target voice file for TTS. Optional, must be 24khz for XTTS and 16khz for fairseq models, uses a default voice if not provided.&#xA;  --language LANGUAGE   Language for the audiobook conversion. Options: eng, zho, spa, fra, por, rus, ind, hin, ben, yor, ara, jav, jpn, kor, deu, ita, fas, tam, tel, tur, pol, hun, nld, zzzz, abi, ace, aca, acn, acr, ach, acu, guq, ade, adj, agd, agx, agn, aha, aka, knj, ake, aeu, ahk, bss, alj, sqi, alt, alp, alz, kab, amk, mmg, amh, ami, azg, agg, boj, cko, any, arl, atq, luc, hyw, apr, aia, msy, cni, cjo, cpu, cpb, asm, asa, teo, ati, djk, ava, avn, avu, awb, kwi, awa, agr, agu, ayr, ayo, abp, blx, sgb, azj-script_cyrillic, azj-script_latin, azb, bba, bhz, bvc, bfy, bgq, bdq, bdh, bqi, bjw, blz, ban, bcc-script_latin, bcc-script_arabic, bam, ptu, bcw, bqj, bno, bbb, bfa, bjz, bak, eus, bsq, akb, btd, btx, bts, bbc, bvz, bjv, bep, bkv, bzj, bem, bng, bom, btt, bha, bgw, bht, beh, sne, ubl, bcl, bim, bkd, bjr, bfo, biv, bib, bis, bzi, bqp, bpr, bps, bwq, bdv, bqc, bus, bnp, bmq, bdg, boa, ksr, bor, bru, box, bzh, bgt, sab, bul, bwu, bmv, mya, tte, cjp, cbv, kaq, cot, cbc, car, cat, ceb, cme, cbi, ceg, cly, cya, che, hne, nya, dig, dug, bgr, cek, cfm, cnh, hlt, mwq, ctd, tcz, zyp, cco, cnl, cle, chz, cpa, cso, cnt, cuc, hak, nan, xnj, cap, cax, ctg, ctu, chf, cce, crt, crq, cac-dialect_sansebasti√°ncoat√°n, cac-dialect_sanmateoixtat√°n, ckt, ncu, cdj, chv, caa, asg, con, crn, cok, crk-script_latin, crk-script_syllabics, crh, hrv, cui, ces, dan, dsh, dbq, dga, dgi, dgk, dnj-dialect_gweetaawueast, dnj-dialect_blowowest, daa, dnt, dnw, dar, tcc, dwr, ded, mzw, ntr, ddn, des, dso, nfa, dhi, gud, did, mhu, dip, dik, tbz, dts, dos, dgo, mvp, jen, dzo, idd, eka, cto, emp, enx, sja, myv, mcq, ese, evn, eza, ewe, fal, fao, far, fij, fin, fon, frd, ful, flr, gau, gbk, gag-script_cyrillic, gag-script_latin, gbi, gmv, lug, pwg, gbm, cab, grt, krs, gso, nlg, gej, gri, kik, acd, glk, gof-script_latin, gog, gkn, wsg, gjn, gqr, gor, gux, gbo, ell, grc, guh, gub, grn, gyr, guo, gde, guj, gvl, guk, rub, dah, gwr, gwi, hat, hlb, amf, hag, hnn, bgc, had, hau, hwc, hvn, hay, xed, heb, heh, hil, hif, hns, hoc, hoy, hus-dialect_westernpotosino, hus-dialect_centralveracruz, huv, hui, hap, iba, isl, dbj, ifa, ifb, ifu, ifk, ife, ign, ikk, iqw, ilb, ilo, imo, inb, ipi, irk, icr, itv, itl, atg, ixl-dialect_sanjuancotzal, ixl-dialect_sangasparchajul, ixl-dialect_santamarianebaj, nca, izr, izz, jac, jam, jvn, kac, dyo, csk, adh, jun, jbu, dyu, bex, juy, gna, urb, kbp, cwa, dtp, kbr, cgc, kki, kzf, lew, cbr, kkj, keo, kqe, kak, kyb, knb, kmd, kml, ify, xal, kbq, kay, ktb, hig, gam, cbu, xnr, kmu, kne, kan, kby, pam, cak-dialect_santamar√≠adejes√∫s, cak-dialect_southcentral, cak-dialect_yepocapa, cak-dialect_western, cak-dialect_santodomingoxenacoj, cak-dialect_central, xrb, krc, kaa, krl, pww, xsm, cbs, pss, kxf, kyz, kyu, txu, kaz, ndp, kbo, kyq, ken, ker, xte, kyg, kjh, kca, khm, kxm, kjg, nyf, kij, kia, kqr, kqp, krj, zga, kin, pkb, geb, gil, kje, kss, thk, klu, kyo, kog, kfb, kpv, bbo, xon, kma, kno, kxc, ozm, kqy, coe, kpq, kpy, kyf, kff-script_telugu, kri, rop, ktj, ted, krr, kdt, kez, cul, kle, kdi, kue, kum, kvn, cuk, kdn, xuo, key, kpz, knk, kmr-script_latin, kmr-script_arabic, kmr-script_cyrillic, xua, kru, kus, kub, kdc, kxv, blh, cwt, kwd, tnk, kwf, cwe, kyc, tye, kir, quc-dialect_north, quc-dialect_east, quc-dialect_central, lac, lsi, lbj, lhu, las, lam, lns, ljp, laj, lao, lat, lav, law, lcp, lzz, lln, lef, acf, lww, mhx, eip, lia, lif, onb, lis, loq, lob, yaz, lok, llg, ycl, lom, ngl, lon, lex, lgg, ruf, dop, lnd, ndy, lwo, lee, mev, mfz, jmc, myy, mbc, mda, mad, mag, ayz, mai, mca, mcp, mak, vmw, mgh, kde, mlg, zlm, pse, mkn, xmm, mal, xdy, div, mdy, mup, mam-dialect_central, mam-dialect_northern, mam-dialect_southern, mam-dialect_western, mqj, mcu, mzk, maw, mjl, mnk, mge, mbh, knf, mjv, mbt, obo, mbb, mzj, sjm, mrw, mar, mpg, mhr, enb, mah, myx, klv, mfh, met, mcb, mop, yua, mfy, maz, vmy, maq, mzi, maj, maa-dialect_sanantonio, maa-dialect_sanjer√≥nimo, mhy, mhi, zmz, myb, gai, mqb, mbu, med, men, mee, mwv, meq, zim, mgo, mej, mpp, min, gum, mpx, mco, mxq, pxm, mto, mim, xta, mbz, mip, mib, miy, mih, miz, xtd, mxt, xtm, mxv, xtn, mie, mil, mio, mdv, mza, mit, mxb, mpm, soy, cmo-script_latin, cmo-script_khmer, mfq, old, mfk, mif, mkl, mox, myl, mqf, mnw, mon, mog, mfe, mor, mqn, mgd, mtj, cmr, mtd, bmr, moz, mzm, mnb, mnf, unr, fmu, mur, tih, muv, muy, sur, moa, wmw, tnr, miq, mos, muh, nas, mbj, nfr, kfw, nst, nag, nch, nhe, ngu, azz, nhx, ncl, nhy, ncj, nsu, npl, nuz, nhw, nhi, nlc, nab, gld, nnb, npy, pbb, ntm, nmz, naw, nxq, ndj, ndz, ndv, new, nij, sba, gng, nga, nnq, ngp, gym, kdj, nia, nim, nin, nko, nog, lem, not, nhu, nob, bud, nus, yas, nnw, nwb, nyy, nyn, rim, lid, nuj, nyo, nzi, ann, ory, ojb-script_latin, ojb-script_syllabics, oku, bsc, bdu, orm, ury, oss, ote, otq, stn, sig, kfx, bfz, sey, pao, pau, pce, plw, pmf, pag, pap, prf, pab, pbi, pbc, pad, ata, pez, peg, pcm, pis, pny, pir, pjt, poy, pps, pls, poi, poh-dialect_eastern, poh-dialect_western, prt, pui, pan, tsz, suv, lme, quy, qvc, quz, qve, qub, qvh, qwh, qvw, quf, qvm, qul, qvn, qxn, qxh, qvs, quh, qxo, qxr, qvo, qvz, qxl, quw, kjb, kek, rah, rjs, rai, lje, rnl, rkt, rap, yea, raw, rej, rel, ril, iri, rgu, rhg, rmc-script_latin, rmc-script_cyrillic, rmo, rmy-script_latin, rmy-script_cyrillic, ron, rol, cla, rng, rug, run, lsm, spy, sck, saj, sch, sml, xsb, sbl, saq, sbd, smo, rav, sxn, sag, sbp, xsu, srm, sas, apb, sgw, tvw, lip, slu, snw, sea, sza, seh, crs, ksb, shn, sho, mcd, cbt, xsr, shk, shp, sna, cjs, jiv, snp, sya, sid, snn, sri, srx, sil, sld, akp, xog, som, bmu, khq, ses, mnx, srn, sxb, suc, tgo, suk, sun, suz, sgj, sus, swh, swe, syl, dyi, myk, spp, tap, tby, tna, shi, klw, tgl, tbk, tgj, blt, tbg, omw, tgk, tdj, tbc, tlj, tly, ttq-script_tifinagh, taj, taq, tpm, tgp, tnn, tac, rif-script_latin, rif-script_arabic, tat, tav, twb, tbl, kps, twe, ttc, kdh, tes, tex, tee, tpp, tpt, stp, tfr, twu, ter, tew, tha, nod, thl, tem, adx, bod, khg, tca, tir, txq, tik, dgr, tob, tmf, tng, tlb, ood, tpi, jic, lbw, txa, tom, toh, tnt, sda, tcs, toc, tos, neb, trn, trs, trc, tri, cof, tkr, kdl, cas, tso, tuo, iou, tmc, tuf, tuk-script_latin, tuk-script_arabic, bov, tue, kcg, tzh-dialect_bachaj√≥n, tzh-dialect_tenejapa, tzo-dialect_chenalh√≥, tzo-dialect_chamula, tzj-dialect_western, tzj-dialect_eastern, aoz, udm, udu, ukr, ppk, ubu, urk, ura, urt, urd-script_devanagari, urd-script_arabic, urd-script_latin, upv, usp, uig-script_arabic, uig-script_cyrillic, uzb-script_cyrillic, vag, bav, vid, vie, vif, vun, vut, prk, wwa, rro, bao, waw, lgl, wlx, cou, hub, gvc, mfi, wap, wba, war, way, guc, cym, kvw, tnp, hto, huu, wal-script_latin, wal-script_ethiopic, wlo, noa, wob, kao, xer, yad, yka, sah, yba, yli, nlk, yal, yam, yat, jmd, tao, yaa, ame, guu, yao, yre, yva, ybb, pib, byr, pil, ycn, ess, yuz, atb, zne, zaq, zpo, zad, zpc, zca, zpg, zai, zpl, zam, zaw, zpm, zac, zao, ztq, zar, zpt, zpi, zas, zaa, zpz, zab, zpu, zae, zty, zav, zza, zyb, ziw, zos, gnd. Default to English (eng).&#xA;  --device {cpu,gpu}    Type of processor unit for the audiobook conversion. If not specified: check first if gpu available, if not cpu is selected.&#xA;  --custom_model CUSTOM_MODEL&#xA;                        Path to the custom model (.zip file containing [&#39;config.json&#39;, &#39;vocab.json&#39;, &#39;model.pth&#39;, &#39;ref.wav&#39;]). Required if using a custom model.&#xA;  --temperature TEMPERATURE&#xA;                        Temperature for the model. Default to 0.65. Higher temperatures lead to more creative outputs.&#xA;  --length_penalty LENGTH_PENALTY&#xA;                        A length penalty applied to the autoregressive decoder. Default to 1.0. Not applied to custom models.&#xA;  --repetition_penalty REPETITION_PENALTY&#xA;                        A penalty that prevents the autoregressive decoder from repeating itself. Default to 2.5&#xA;  --top_k TOP_K         Top-k sampling. Lower values mean more likely outputs and increased audio generation speed. Default to 50&#xA;  --top_p TOP_P         Top-p sampling. Lower values mean more likely outputs and increased audio generation speed. Default to 0.8&#xA;  --speed SPEED         Speed factor for the speech generation. Default to 1.0&#xA;  --enable_text_splitting&#xA;                        Enable splitting text into sentences. Default to False.&#xA;  --fine_tuned FINE_TUNED&#xA;                        Name of the fine tuned model. Optional, uses the standard model according to the TTS engine and language.&#xA;  --version             Show the version of the script and exit&#xA;&#xA;Example usage:    &#xA;Windows:&#xA;    :&#xA;    ebook2audiobook.cmd -- --ebook &#39;path_to_ebook&#39;&#xA;    Graphic Interface:&#xA;    ebook2audiobook.cmd&#xA;Linux/Mac:&#xA;    :&#xA;    ./ebook2audiobook.sh -- --ebook &#39;path_to_ebook&#39;&#xA;    Graphic Interface:&#xA;    ./ebook2audiobook.sh&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can also use Docker to run the eBook to Audiobook converter. This method ensures consistency across different environments and simplifies setup.&lt;/p&gt; &#xA;&lt;h4&gt;Running the Docker Container&lt;/h4&gt; &#xA;&lt;p&gt;To run the Docker container and start the Gradio interface, use the following command:&lt;/p&gt; &#xA;&lt;p&gt;-Run with CPU only&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;docker run -it --rm -p 7860:7860 --platform=linux/amd64 athomasson2/ebook2audiobook python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;-Run with GPU Speedup (Nvida graphics cards only)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;docker run -it --rm --gpus all -p 7860:7860 --platform=linux/amd64 athomasson2/ebook2audiobook python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Building the Docker Container&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can build the docker image with the command: &#39;&#39;&#39;powershell docker build --platform linux/amd64 -t athomasson2/ebook2audiobook . &#39;&#39;&#39;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For more options like running the docker in mode or making the gradio link public add the &lt;code&gt;--help&lt;/code&gt; parameter after the &lt;code&gt;app.py&lt;/code&gt; in the docker launch command&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker container file locations&lt;/h2&gt; &#xA;&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/home/user/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/home/user/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/home/user/app/audiobooks&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Docker headless guide&lt;/h2&gt; &#xA;&lt;p&gt;first for a docker pull of the latest with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull athomasson2/ebook2audiobook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before you do run this you need to create a dir named &#34;input-folder&#34; in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm \&#xA;    -v $(pwd)/input-folder:/home/user/app/input_folder \&#xA;    -v $(pwd)/audiobooks:/home/user/app/audiobooks \&#xA;    --platform linux/amd64 \&#xA;    athomasson2/ebook2audiobook \&#xA;    python app.py --headless --ebook /input_folder/YOUR_INPUT_FILE.TXT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;And that should be it!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm \&#xA;    --platform linux/amd64 \&#xA;    athomasson2/ebook2audiobook \&#xA;    python app.py --help&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and that will output this&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output&#34;&gt;Help command output&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker Compose&lt;/h3&gt; &#xA;&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Steps to Run&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven&#39;t already):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git&#xA;cd ebook2audiobook&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the service:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href=&#34;http://localhost:7860&#34;&gt;http://localhost:7860&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;New v2.0 Docker Web GUI Interface!&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif&#34; alt=&#34;demo_web_gui&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 1&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 2&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 3&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Renting a GPU&lt;/h2&gt; &#xA;&lt;p&gt;Don&#39;t have the hardware to run it or you want to rent a GPU?&lt;/p&gt; &#xA;&lt;h4&gt;You can duplicate the hugginface space and rent a gpu for around $0.40 an hour&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#huggingface-space-demo&#34;&gt;Huggingface Space Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Or you can try using the google colab for free!&lt;/h4&gt; &#xA;&lt;p&gt;(Be aware it will time out after a bit of your not messing with the google colab) &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#free-google-colab&#34;&gt;Free Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Common Docker Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobook/issues/191&#34;&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Dockerfile&#34;&gt;docker run -it --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 -p 7860:7860 --platform=linux/amd64 athomasson2/ebook2audiobook python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; &#xA;&lt;p&gt;You can fine-tune your own xtts model easily with this repo &lt;a href=&#34;https://github.com/daswer123/xtts-finetune-webui&#34;&gt;xtts-finetune-webui&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to rent a GPU easily you can also duplicate this huggingface &lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu&#34;&gt;xtts-finetune-webui-space&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A space you can use to de-noise the training data easily also &lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit&#34;&gt;denoise-huggingface-space&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; &#xA;&lt;p&gt;To find our collection of already fine-tuned TTS models, visit &lt;a href=&#34;https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main&#34;&gt;this Hugging Face link&lt;/a&gt; For an XTTS custom model a ref audio clip of the voice will also be needed:&lt;/p&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;Rainy day voice&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/8486603c-38b1-43ce-9639-73757dfb1031&#34;&gt;https://github.com/user-attachments/assets/8486603c-38b1-43ce-9639-73757dfb1031&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;David Attenborough voice&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/47c846a7-9e51-4eb9-844a-7460402a20a8&#34;&gt;https://github.com/user-attachments/assets/47c846a7-9e51-4eb9-844a-7460402a20a8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Output&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creates an &lt;code&gt;.m4b&lt;/code&gt; file with metadata and chapters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Example Output&lt;/strong&gt;: &lt;img src=&#34;https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg&#34; alt=&#34;Example&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Common Issues:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;It&#39;s slow!&#34; - On CPU only this is very slow, and you can only get speedups though a NVIDIA GPU. &lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846&#34;&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobookpiper-tts&#34;&gt;project that uses piper-tts&lt;/a&gt; instead(It doesn&#39;t have zero-shot voice cloning though, and is siri quality voices, but it is much faster on cpu.)&lt;/li&gt; &#xA; &lt;li&gt;&#34;I&#39;m having dependency issues&#34; - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;-h&lt;/code&gt; parameter after the &lt;code&gt;app.py&lt;/code&gt; in the docker run command for more information.&lt;/li&gt; &#xA; &lt;li&gt;&#34;Im getting a truncated audio issue!&#34; - PLEASE MAKE AN ISSUE OF THIS, I don&#39;t speak every language and I need advise from each person to fine tune my sentense splitting function on any other languages.üòä&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What I need help with! üôå&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobook/issues/32&#34;&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Any help from people speaking any of the supported languages to help with proper sentence splitting methods&lt;/li&gt; &#xA; &lt;li&gt;Potentially creating readme Guides for Multiple languages(Becuase the only language I know is English üòî)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Special Thanks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href=&#34;https://github.com/idiap/coqui-ai-TTS&#34;&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href=&#34;https://calibre-ebook.com&#34;&gt;Calibre Website&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href=&#34;https://ffmpeg.org&#34;&gt;FFmpeg Website&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobook/issues/8&#34;&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/legacy/v1.0&#34;&gt;Legacy V1.0&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can view the code &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/legacy/v1.0&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Join Our Discord Server!&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/bg5Kx43c6w&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/https://discord.gg/bg5Kx43c6w&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/PromptWizard</title>
    <updated>2025-01-05T01:40:54Z</updated>
    <id>tag:github.com,2025-01-05:/microsoft/PromptWizard</id>
    <link href="https://github.com/microsoft/PromptWizard" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Task-Aware Agent-driven Prompt Optimization Framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PromptWizard üßô&lt;/h1&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2405.18369&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/arXiv-2409.10566-b31b1b.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/promptwizard-the-future-of-prompt-optimization-through-feedback-driven-self-evolving-prompts/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/msr_blog.png&#34; width=&#34;16&#34;&gt; Blog Post &lt;/a&gt; &lt;a href=&#34;https://microsoft.github.io/PromptWizard/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/github.png&#34; width=&#34;16&#34;&gt; Project Website &lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;PromptWizard: Task-Aware Prompt Optimization Framework&lt;/strong&gt;&lt;br&gt; Eshaan Agarwal, Joykirat Singh, Vivek Dani, Raghav Magazine, Tanuja Ganu, Akshay Nambi &lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Overview üåü&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Overview of the PromptWizard framework&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/overview.png&#34;&gt; &#xA;&lt;p&gt;PromptWizard is a discrete prompt optimization framework that employs a self-evolving mechanism where the LLM generates, critiques, and refines its own prompts and examples, continuously improving through iterative feedback and synthesis. This self-adaptive approach ensures holistic optimization by evolving both the instructions and in-context learning examples for better task performance.&lt;/p&gt; &#xA;&lt;p&gt;Three key components of PromptWizard are te following :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Feedback-driven Refinement: LLM generates, critiques, and refines its own prompts and examples, continuously improving through iterative feedback and synthesis‚Äã&lt;/li&gt; &#xA; &lt;li&gt;Critique and Synthesize diverse examples: Generates synthetic examples that are robust, diverse and task-aware. Also it optimizes both prompt and examples in tandem‚Äã&lt;/li&gt; &#xA; &lt;li&gt;Self generated Chain of Thought (CoT) steps with combination of positive, negative and synthetic examples&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Stage 1: Iterative optimization of instructions&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/iterative_flowchart-1.png&#34; width=&#34;49.5%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Stage 2: Sequential optimization of instruction and examples&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/sequential_flowchart-1.png&#34; width=&#34;49.5%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation ‚¨áÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Follow these steps to set up the development environment and install the package:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/microsoft/PromptWizard&#xA;cd PromptWizard&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create and activate a virtual environment&lt;/p&gt; &lt;p&gt;On Windows&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python -m venv venv&#xA;venv\Scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On macOS/Linux:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python -m venv venv&#xA;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the package in development mode:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Quickstart üèÉ&lt;/h2&gt; &#xA;&lt;p&gt;There are three main ways to use PromptWizard:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scenario 1 : Optimizing prompts without examples&lt;/li&gt; &#xA; &lt;li&gt;Scenario 2 : Generating synthetic examples and using them to optimize prompts&lt;/li&gt; &#xA; &lt;li&gt;Scenario 3 : Optimizing prompts with training data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; : Refer this &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/scenarios/dataset_scenarios_demo.ipynb&#34;&gt;notebook&lt;/a&gt; to get a detailed understanding of the usage for each of the scenarios. &lt;strong&gt;This serves as a starting point to understand the usage of PromptWizard&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;High level overview of using PromptWizard&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Decide your scenario&lt;/li&gt; &#xA; &lt;li&gt;Fix the configuration and environmental varibles for API calling &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use &lt;code&gt;promptopt_config.yaml&lt;/code&gt; to set configurations. For example for GSM8k this &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/configs/promptopt_config.yaml&#34;&gt;file&lt;/a&gt; can be used&lt;/li&gt; &#xA;   &lt;li&gt;Use &lt;code&gt;.env&lt;/code&gt; to set environmental varibles. For GSM8k this &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/.env&#34;&gt;file&lt;/a&gt; can be used&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;USE_OPENAI_API_KEY=&#34;XXXX&#34;&#xA;# Replace with True/False based on whether or not to use OPENAI API key&#xA;&#xA;# If the first variable is set to True then fill the following two&#xA;OPENAI_API_KEY=&#34;XXXX&#34;&#xA;OPENAI_MODEL_NAME =&#34;XXXX&#34;&#xA;&#xA;# If the first variable is set to False then fill the following three&#xA;AZURE_OPENAI_ENDPOINT=&#34;XXXXX&#34; &#xA;# Replace with your Azure OpenAI Endpoint&#xA;&#xA;OPENAI_API_VERSION=&#34;XXXX&#34;&#xA;# Replace with the version of your API&#xA;&#xA;AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=&#34;XXXXX&#34;&#xA;# Create a deployment for the model and place the deployment name here. &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run the code &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;To run PromptWizard on your custom dataset please jump &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/#run-on-custom-dataset&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Running PromptWizard with training data (Scenario 3)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We support &lt;a href=&#34;https://huggingface.co/datasets/openai/gsm8k&#34;&gt;GSM8k&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/ChilleD/SVAMP&#34;&gt;SVAMP&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/deepmind/aqua_rat&#34;&gt;AQUARAT&lt;/a&gt; and &lt;a href=&#34;https://github.com/xqlin98/INSTINCT/tree/main/Induction/experiments/data/instruction_induction/raw&#34;&gt;Instruction_Induction(BBII)&lt;/a&gt; datasets&lt;/li&gt; &#xA; &lt;li&gt;Please note that time taken for prompt optimzation is dependent on the dataset. In our experiments for the above mentioned datasets, it took around 20 - 30 minutes on average.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Running on GSM8k (AQUARAT/SVAMP)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please note that this code requires access to LLMs via API calling for which we support AZURE endpoints or OPENAI keys&lt;/li&gt; &#xA; &lt;li&gt;Set the AZURE endpoint configurations in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/.env&#34;&gt;.env&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow the steps in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/demo.ipynb&#34;&gt;demo.ipynb&lt;/a&gt; to download the data, run the prompt optimization and carry out inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Running on BBII&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BBII has many datasets in it, based on the dataset set the configs &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/bbh/configs/promptopt_config.yaml&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;In configs &lt;code&gt;task_description&lt;/code&gt;,&lt;code&gt;base_instruction&lt;/code&gt; and &lt;code&gt;answer_format&lt;/code&gt; need to be changed for different datasets in BBII, the rest of the configs remain the same&lt;/li&gt; &#xA; &lt;li&gt;A demo is presented in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/bbh/demo.ipynb&#34;&gt;demo.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run on Custom Datasets üóÉÔ∏è&lt;/h2&gt; &#xA;&lt;h3&gt;Create Custom Dataset&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our code expects the dataset to be in &lt;code&gt;.jsonl&lt;/code&gt; file format&lt;/li&gt; &#xA; &lt;li&gt;Both the train and test set follow the same format&lt;/li&gt; &#xA; &lt;li&gt;Every sample in the &lt;code&gt;.jsonl&lt;/code&gt; should have 2 fields : &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;question&lt;/code&gt; : It should contain the complete question that is to asked to the LLM&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;answer&lt;/code&gt; : It should contain the ground truth answer which can be verbose or consize&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run on Custom Dataset&lt;/h3&gt; &#xA;&lt;p&gt;NOTE : Refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos&#34;&gt;demos&lt;/a&gt; folder for examples of folders for four datasets. The &lt;code&gt;.ipynb&lt;/code&gt; in each of the folders shows how to run PromptWizard on that particular dataset. A similar procedure can be followed for a new dataset. Below is the explanation of each of the components of the &lt;code&gt;.ipynb&lt;/code&gt; and the dataset specifc folder structure in detail&lt;/p&gt; &#xA;&lt;h4&gt;Steps to be followed for custom datasets&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Every new dataset needs to have the following&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;configs&lt;/code&gt; folder to store files for defining optimization hyperparameters and setup configs&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;data&lt;/code&gt; folder to store &lt;code&gt;train.jsonl&lt;/code&gt; and &lt;code&gt;test.jsonl&lt;/code&gt; as curated &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/#create-custom-dataset&#34;&gt;here&lt;/a&gt; (this is done in the notebooks)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;.env&lt;/code&gt; file for environment varibles to be used for API calling&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;.py/.ipynb&lt;/code&gt; script to run the code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the hyperparameters like number of mutations, refine steps, in-context examples etc.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set the following in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/configs/promptopt_config.yaml&#34;&gt;promptopt_config.yaml&lt;/a&gt; : &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;task_description&lt;/code&gt; : Desciption of the task at hand which will be fed into the prompt&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;For GSM8k a description like the following can be used &lt;pre&gt;&lt;code&gt;You are a mathematics expert. You will be given a mathematics problem which you need to solve&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;base_instruction&lt;/code&gt; : Base instruction in line with the dataset&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;A commonly used base instruction could be &lt;pre&gt;&lt;code&gt;Lets think step by step.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;answer_format&lt;/code&gt; : Instruction for specifying the answer format&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;It is crucial to set the &lt;code&gt;answer_format&lt;/code&gt; properly to ensure correct extraction by &lt;code&gt;def extract_final_answer()&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;Answer format could be : &lt;pre&gt;&lt;code&gt;At the end, wrap only your final option between &amp;lt;ANS_START&amp;gt; and &amp;lt;ANS_END&amp;gt; tags&#xA;&lt;/code&gt;&lt;/pre&gt; Then in &lt;code&gt;def extract_final_answer()&lt;/code&gt; we can simply write code to extract string between the tags&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;seen_set_size&lt;/code&gt; : The number of train samples to be used for prompt optimization&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;In our experiments we set this to be 25. In general any number between 20-50 would work&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;few_shot_count&lt;/code&gt; : The number of in-context examples needed in the prompt&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;The value can be set to any positive integer based on the requirement&lt;/li&gt; &#xA;       &lt;li&gt;For generating zero-shot prompts, set the values to a small number (i.e between 2-5) and after the final prompt is generated the in-context examples can be removed. We suggest using some in-context examples as during the optimization process the instructions in the prompt are refined using in-context examples hence setting it to a small number will give better zero-shot instructions in the prompt&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;generate_reasoning&lt;/code&gt; : Whether or not to generate reasoning for the in-context examples&lt;/p&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;In our experiments we found it to improve the prompt overall as it provides a step-by-step approach to reach the final answer. However if there is a constraint on the prompt length or number of prompt tokens, it can be turned off to get smaller sized prompts&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;code&gt;generate_expert_identity&lt;/code&gt; and &lt;code&gt;generate_intent_keywords&lt;/code&gt; : Having these helped improve the prompt as they help making the prompt relevant to the task&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Refer &lt;code&gt;promptopt_config.yaml&lt;/code&gt; files in folders present &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos&#34;&gt;here&lt;/a&gt; for the descriptions used for AQUARAT, SVAMP and GSM8k. For BBII refer &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/bbh/description.py&#34;&gt;description.py&lt;/a&gt; which has the meta instructions for each of the datasets&lt;/li&gt; &#xA;   &lt;li&gt;Following are the global parameters which can be set based on the availability of the training data &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;run_without_train_examples&lt;/code&gt; is a global hyperparameter which can be used when there are no training samples and in-context examples are not required in the final prompt&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;generate_synthetic_examples&lt;/code&gt; is a global hyperparameter which can be used when there are no training samples and we want to generate synthetic data for training&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;use_examples&lt;/code&gt; is a global hyperparameter which can be used to optimize prompts using training data&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a dataset specific class which inherits &lt;code&gt;class DatasetSpecificProcessing&lt;/code&gt; similar to &lt;code&gt;GSM8k(DatasetSpecificProcessing)&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/demo.ipynb&#34;&gt;demo.ipynb&lt;/a&gt; and define the following functions in it&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;In &lt;code&gt;def extract_answer_from_output()&lt;/code&gt; : This is a dataset specific function, given the &lt;code&gt;answer&lt;/code&gt; from the dataset it should extract and return a consize form of the answer. Note that based on the dataset it can also simply return the &lt;code&gt;answer&lt;/code&gt; as it is like in case of SVAMP and AQUARAT datasets&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;def extract_final_answer()&lt;/code&gt; : This is a LLM output specific function, given the verbose answer from the LLM it should extract and return the consize final answer&lt;/li&gt; &#xA;   &lt;li&gt;Define &lt;code&gt;def access_answer()&lt;/code&gt; : This function takes an input the LLM output, then does the following: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Extracts the consize answer using &lt;code&gt;def extract_final_answer()&lt;/code&gt; from the LLM output as defined above&lt;/li&gt; &#xA;     &lt;li&gt;Evaluates the extracted answer with the ground truth and retuns &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Extracted answer from LLM output&lt;/li&gt; &#xA;       &lt;li&gt;Boolean value indicating if answer is correct or not&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;The evaluation done here is dataset specific, for datasets like GSM8k, SVAMP and AQUARAT which have final answer as an number, we can do a direct match between the numbers generated and the ground truth, while for datasets where the answer is a sentence or paragraph it would be better to do evaluation with llm-as-a-judge, to compare the generated and ground truth paragraph/sentence. An example is available in &lt;code&gt;def access_answer()&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/bbh/demo.ipynb&#34;&gt;this&lt;/a&gt; notebook&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How PromptWizard Works üîç&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using the problem description and initial prompt instruction, PW generates variations of the instruction by prompting LLMs to mutate it. Based on performance, the best prompt is selected. PW incorporates a critique component that provides feedback, thus guiding and refining the prompt over multiple iterations.&lt;/li&gt; &#xA; &lt;li&gt;PW also optimizes in-context examples. PW selects a diverse set of examples from the training data, identifying positive and negative examples based on their performance with the modified prompt. Negative examples help inform further prompt refinements.&lt;/li&gt; &#xA; &lt;li&gt;Examples and instructions are sequentially optimized, using the critique to generate synthetic examples that address the current prompt‚Äôs weaknesses. These examples are integrated to further refine the prompt.&lt;/li&gt; &#xA; &lt;li&gt;PW generates detailed reasoning chains via Chain-of-Thought (CoT), enriching the prompt‚Äôs capacity for problem-solving.&lt;/li&gt; &#xA; &lt;li&gt;PW aligns prompts with human reasoning by integrating task intent and expert personas, enhancing both model performance and interpretability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Configurations ‚öôÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Here we define the various hyperparameters used in prompt optimization process found in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/configs/promptopt_config.yaml&#34;&gt;promptopt_config.yaml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;mutate_refine_iterations&lt;/code&gt;: Number of iterations for conducting mutation of task description followed by refinement of instructions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mutation_rounds&lt;/code&gt;: Number of rounds of mutation to be performed when generating different styles&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;refine_task_eg_iterations&lt;/code&gt;: Number of iterations for refining task description and in context examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;style_variation&lt;/code&gt;: Number of thinking style variations to be used in prompt mutation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;questions_batch_size&lt;/code&gt;: Number of questions to be asked to LLM in a single batch, during training step&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;min_correct_count&lt;/code&gt;: Minimum number of batches of questions to correctly answered, for a prompt to be considered as performing good&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_eval_batches&lt;/code&gt;: Maximum number of mini-batches on which we should evaluate the prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;top_n&lt;/code&gt;: Number of top best prompts to be considered from scoring stage for the next stage&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;seen_set_size&lt;/code&gt;: Number of samples from trainset to be used for training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;few_shot_count&lt;/code&gt;: Number of in-context examples required in final prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Best Practices üí°&lt;/h2&gt; &#xA;&lt;p&gt;Following are some of best pracitices we followed during are experiments&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Regarding the parameters in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/demos/gsm8k/configs/promptopt_config.yaml&#34;&gt;promptopt_config.yaml&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We found the best performing values for &lt;code&gt;mutate_refine_iterations&lt;/code&gt;,&lt;code&gt;mutation_rounds&lt;/code&gt;,&lt;code&gt;refine_task_eg_iterations&lt;/code&gt; to be 3 or 5&lt;/li&gt; &#xA;   &lt;li&gt;Other parameters have been set to their ideal values. &lt;code&gt;seen_set_size&lt;/code&gt; can be increased to 50 and &lt;code&gt;few_shot_count&lt;/code&gt; can be set based on the use case&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The prompts generated at the end of the training process are usually very detailed, however user supervision can help tune it further for the task at hand&lt;/li&gt; &#xA; &lt;li&gt;Trying both configurations of having synthetic in-context examples or in-context examples from the train set can be tried to find the best prompt based on use case.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results üìà&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/images/curve.png&#34; width=&#34;45%&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;PromptWizard consistently outperforms other methods across various thresholds, maintaining the highest p(œÑ) values, indicating that it consistently performs near the best possible accuracy across all tasks&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The fiqure shows the performance profile curve for the instruction induction tasks. The performance profile curve visualizes how frequently different approaches‚Äô performance is within a given distance of the best performance. In this curve, the x-axis (œÑ) represents the performance ratio relative to the best-performing method, and the y-axis (p(œÑ )) reflects the fraction of tasks where a method‚Äôs performance is within this ratio. So for a given method, the curve tells what percentage of the tasks are within œÑ distance to the best performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to contribute: ‚úã&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;. When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA. This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Citation üìù&lt;/h2&gt; &#xA;&lt;p&gt;If you make use of our work, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{agarwal2024promptwizardtaskawarepromptoptimization,&#xA;      title={PromptWizard: Task-Aware Prompt Optimization Framework}, &#xA;      author={Eshaan Agarwal and Joykirat Singh and Vivek Dani and Raghav Magazine and Tanuja Ganu and Akshay Nambi},&#xA;      year={2024},&#xA;      eprint={2405.18369},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2405.18369}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Responsible AI Considerations&lt;/h2&gt; &#xA;&lt;p&gt;For guidelines and best practices related to Responsible AI, please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptWizard/main/RESPONSIBLE_AI.md&#34;&gt;Responsible AI Guidelines&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>