<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-02T02:02:23Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>BlinkDL/ChatRWKV</title>
    <updated>2023-04-02T02:02:23Z</updated>
    <id>tag:github.com,2023-04-02:/BlinkDL/ChatRWKV</id>
    <link href="https://github.com/BlinkDL/ChatRWKV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatRWKV is like ChatGPT but powered by RWKV (100% RNN) language model, and open source.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatRWKV (pronounced as &#34;RwaKuv&#34;, from 4 major params: R W K V)&lt;/h1&gt; &#xA;&lt;p&gt;ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI :) &lt;strong&gt;中文使用教程，请往下看，在本页面底部。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HuggingFace Gradio Demo (14B ctx8192)&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio&#34;&gt;https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Raven&lt;/strong&gt; (7B finetuned on Alpaca and more) Demo: &lt;a href=&#34;https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B&#34;&gt;https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV pip package&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/rwkv/&#34;&gt;https://pypi.org/project/rwkv/&lt;/a&gt; &lt;strong&gt;(please always check for latest version and upgrade)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update ChatRWKV v2 &amp;amp; pip rwkv package (0.7.0):&lt;/p&gt; &#xA;&lt;p&gt;Use v2/convert_model.py to convert a model for a strategy, for faster loading &amp;amp; saves CPU RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;### Note RWKV_CUDA_ON will build a CUDA kernel (&#34;pip install ninja&#34; first).&#xA;### How to build in Linux: set these and run v2/chat.py&#xA;export PATH=/usr/local/cuda/bin:$PATH&#xA;export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH&#xA;### How to build in win:&#xA;Install VS2022 build tools (https://aka.ms/vs/17/release/vs_BuildTools.exe select Desktop C++). Reinstall CUDA 11.7 (install VC++ extensions). Run v2/chat.py in &#34;x64 native tools command prompt&#34;. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Download RWKV-4 weights:&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/BlinkDL&#34;&gt;https://huggingface.co/BlinkDL&lt;/a&gt; (&lt;strong&gt;Use RWKV-4 models&lt;/strong&gt;. DO NOT use RWKV-4a and RWKV-4b models.)&lt;/p&gt; &#xA;&lt;h2&gt;RWKV Discord: &lt;a href=&#34;https://discord.gg/bDSBUMeFpc&#34;&gt;https://discord.gg/bDSBUMeFpc&lt;/a&gt; (let&#39;s build together)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Twitter:&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/BlinkDL_AI&#34;&gt;https://twitter.com/BlinkDL_AI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV LM:&lt;/strong&gt; &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM&#34;&gt;https://github.com/BlinkDL/RWKV-LM&lt;/a&gt; (explanation, fine-tuning, training, etc.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV in 150 lines&lt;/strong&gt; (model, inference, text generation): &lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV/raw/main/RWKV_in_150_lines.py&#34;&gt;https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ChatRWKV v2: with &#34;stream&#34; and &#34;split&#34; strategies, and INT8. 3G VRAM is enough to run RWKV 14B :) &lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV/tree/main/v2&#34;&gt;https://github.com/BlinkDL/ChatRWKV/tree/main/v2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;os.environ[&#34;RWKV_JIT_ON&#34;] = &#39;1&#39;&#xA;os.environ[&#34;RWKV_CUDA_ON&#34;] = &#39;0&#39; # if &#39;1&#39; then use CUDA kernel for seq mode (much faster)&#xA;from rwkv.model import RWKV                         # pip install rwkv&#xA;model = RWKV(model=&#39;/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040&#39;, strategy=&#39;cuda fp16&#39;)&#xA;&#xA;out, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json&#xA;print(out.detach().cpu().numpy())                   # get logits&#xA;out, state = model.forward([187, 510], None)&#xA;out, state = model.forward([1563], state)           # RNN has state (use deepcopy if you want to clone it)&#xA;out, state = model.forward([310, 247], state)&#xA;print(out.detach().cpu().numpy())                   # same result as above&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/RWKV-eval.png&#34; alt=&#34;RWKV-eval&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/ChatRWKV.png&#34; alt=&#34;ChatRWKV&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cool Community RWKV Projects:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/rwkvstic/&#34;&gt;https://pypi.org/project/rwkvstic/&lt;/a&gt; pip package (with 8bit &amp;amp; offload for low VRAM GPUs)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gururise/rwkv_gradio&#34;&gt;https://github.com/gururise/rwkv_gradio&lt;/a&gt; RWKV Gradio&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hizkifw/WebChatRWKVstic&#34;&gt;https://github.com/hizkifw/WebChatRWKVstic&lt;/a&gt; WebUI (WIP)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cryscan/eloise&#34;&gt;https://github.com/cryscan/eloise&lt;/a&gt; RWKV QQ bot&lt;/p&gt; &#xA;&lt;p&gt;It is not instruct-tuned, so don&#39;t directly ask it to do stuffs (unless it&#39;s a simple question).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen \nQ: prompt\n\nA:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For all RWKV-4 models, some great Q&amp;amp;A prompts:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen \nExpert Questions &amp;amp; Helpful Answers\nAsk Research Experts\nQuestion:\nCan penguins fly?\n\nFull Answer:\n&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen \nAsk Expert\n\nQuestion:\nWhat are some good plans to kill all mosquitoes?\n\nExpert Full Answer:\n&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen \nQ &amp;amp; A\n\nQuestion:\nHow&#39;s the weather of Mars?\n\nDetailed Expert Answer:\n&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Other examples:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen Here&#39;s a short cyberpunk sci-fi adventure story. The story&#39;s main character is an artificial human created by a company called OpenBot.\n\nThe Story:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen Here is a Python function that generates string of words that would confuse LLMs:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen List of penguin facts:\n1.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+qa Can penguins fly?&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen $ curl -i https://google.com/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen The following is the contents of https://en.wikipedia.org/wiki/Internet:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen Bob&#39;s Blog - Which is better, iOS or Android?&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;+gen Here is a shell script which will find all .hpp files in /home/workspace and delete the 3th row string of these files:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-1.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-2.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-3.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-4.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-5.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-6.png&#34; alt=&#34;ChatRWKV&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/misc/sample-7.png&#34; alt=&#34;ChatRWKV&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;中文模型&lt;/h2&gt; &#xA;&lt;p&gt;QQ群 553456870（加入时请简单自我介绍）。有研发能力的朋友加群 325154699。&lt;/p&gt; &#xA;&lt;p&gt;中文使用教程：&lt;a href=&#34;https://zhuanlan.zhihu.com/p/609154637&#34;&gt;https://zhuanlan.zhihu.com/p/609154637&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;咒语非常重要。试试这些咒语（注意这些咒语都会忽略聊天内容！都应该用于问独立的问题！）：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;【网文模型 testNovel，试试下列指令】&#xA;+gen 这是一颗&#xA;+gen 以下是不朽的科幻史诗长篇巨著，描写细腻，刻画了数百位个性鲜明的英雄和宏大的星际文明战争，情节曲折离奇，充满悬疑氛围，草蛇灰线，当谜底揭开，时而令人惊为天人，时而令人扼腕叹息。\n第一章&#xA;+gen 这是一个修真世界，详细世界设定如下：\n1.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;【问答模型 test4，试试下列指令】&#xA;+gen \n活动出席发言稿：\n大家好，&#xA;+gen \n怎样创立一家快速盈利的AI公司：\n1.&#xA;+gen 二向箔是一种超级武器，它的原理是&#xA;+gen 我抬头一看，竟然是&#xA;+gen import torch&#xA;【这些多试几次】&#xA;+qq 请以《我的驴》为题写一篇作文&#xA;+qq 请以《企鹅》为题写一首诗歌&#xA;+qq 请设定一个奇幻世界，告诉我详细的世界设定。&#xA;【问答咒语】&#xA;+gen \nExpert Questions &amp;amp; Helpful Answers\nAsk Research Experts\nQuestion:\n猫会编程吗？\n\nFull Answer:\n&#xA;+gen \nAsk Expert\n\nQuestion:\n猫会编程吗？\n\nExpert Full Answer:\n&#xA;【使用+qa需要在chat.py设置QA_PROMPT=True然后才能看到内容丰富的长回答】&#xA;+qa 奶茶好喝吗？&#xA;+qa 猫喜欢做什么？&#xA;+qa How can I learn Python?&#xA;+qa 猫会编程吗？&#xA;+qa 知乎大V有哪些特点？&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#BlinkDL/ChatRWKV&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=BlinkDL/ChatRWKV&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nsarrazin/serge</title>
    <updated>2023-04-02T02:02:23Z</updated>
    <id>tag:github.com,2023-04-02:/nsarrazin/serge</id>
    <link href="https://github.com/nsarrazin/serge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A web interface for chatting with Alpaca through llama.cpp. Fully dockerized, with an easy to use API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Serge - LLaMa made easy 🦙&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/nsarrazin/serge&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://discord.gg/62Hc6FEYQH&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1088427963801948201?label=Discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A chat interface based on &lt;code&gt;llama.cpp&lt;/code&gt; for running Alpaca models. Entirely self-hosted, no API keys needed. Fits on 4GB of RAM and runs on the CPU.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;SvelteKit&lt;/strong&gt; frontend&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MongoDB&lt;/strong&gt; for storing chat history &amp;amp; parameters&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FastAPI + beanie&lt;/strong&gt; for the API, wrapping calls to &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/25119303/226897188-914a6662-8c26-472c-96bd-f51fc020abf6.webm&#34;&gt;demo.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Setting up Serge is very easy. Starting it up can be done in a single command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -d -v weights:/usr/src/app/weights -v datadb:/data/db/ -p 8008:8008 ghcr.io/nsarrazin/serge:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then just go to &lt;a href=&#34;http://localhost:8008/&#34;&gt;http://localhost:8008/&lt;/a&gt; !&lt;/p&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;p&gt;Make sure you have docker desktop installed, WSL2 configured and enough free RAM to run models. (see below)&lt;/p&gt; &#xA;&lt;h4&gt;Kubernetes &amp;amp; docker compose&lt;/h4&gt; &#xA;&lt;p&gt;Setting up Serge on Kubernetes or docker compose can be found in the wiki: &lt;a href=&#34;https://github.com/nsarrazin/serge/wiki/Integrating-Serge-in-your-orchestration#kubernetes-example&#34;&gt;https://github.com/nsarrazin/serge/wiki/Integrating-Serge-in-your-orchestration#kubernetes-example&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;Currently only the 7B, 7B-native, 13B and 30B alpaca models are supported. If you have existing weights from another project you can add them to the &lt;code&gt;serge_weights&lt;/code&gt; volume using &lt;code&gt;docker cp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;⚠&lt;/span&gt; A note on &lt;em&gt;memory usage&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;llama will just crash if you don&#39;t have enough available memory for your model.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B requires about 4.5GB of free RAM&lt;/li&gt; &#xA; &lt;li&gt;13B requires about 12GB free&lt;/li&gt; &#xA; &lt;li&gt;30B requires about 20GB free&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compatible CPUS&lt;/h3&gt; &#xA;&lt;p&gt;Currently Serge requires a CPU compatible with AVX2 instructions. Try &lt;code&gt;lscpu | grep avx2&lt;/code&gt; in a shell, and if this returns nothing then your CPU is incompatible for now.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to join the discord if you need help with the setup: &lt;a href=&#34;https://discord.gg/62Hc6FEYQH&#34;&gt;https://discord.gg/62Hc6FEYQH&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Serge is always open for contributions! If you catch a bug or have a feature idea, feel free to open an issue or a PR.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run Serge in development mode (with hot-module reloading for svelte &amp;amp; autoreload for FastAPI) you can do so like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/nsarrazin/serge.git&#xA;DOCKER_BUILDKIT=1 docker compose -f docker-compose.dev.yml up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can test the production image with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DOCKER_BUILDKIT=1 docker compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What&#39;s next&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Front-end to interface with the API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pass model parameters when creating a chat&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Manager for model files&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support for other models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; LangChain integration&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; User profiles &amp;amp; authentication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And a lot more!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jesselau76/ebook-GPT-translator</title>
    <updated>2023-04-02T02:02:23Z</updated>
    <id>tag:github.com,2023-04-02:/jesselau76/ebook-GPT-translator</id>
    <link href="https://github.com/jesselau76/ebook-GPT-translator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enjoy reading with your favorite style.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ebook-GPT-translator: Enjoy reading with your favorite style.&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jesselau76/ebook-GPT-translator/raw/main/README.md&#34;&gt;En&lt;/a&gt; | &lt;a href=&#34;https://github.com/jesselau76/ebook-GPT-translator/raw/main/README-zh.md&#34;&gt;中文说明&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This tool is designed to help users convert text from one format to another, as well as translate it into a different language using the OpenAI API (model=&#34;gpt-3.5-turbo&#34;). It currently supports converting and translating PDF, DOCX, EPUB, and MOBI file formats into EPUB and text files and can translate text into multiple languages.&lt;/p&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For PDF, DOCX, and MOBI files, only the text portions will be processed, and graphical elements will not appear in the resulting files.&lt;/li&gt; &#xA; &lt;li&gt;For EPUB files, all graphical elements will be placed at the beginning of each chapter, as EPUB files use HTML language format. To maintain translation quality, the text will be translated in multiple segments without preserving the original formatting, so graphical elements will not be kept in their original positions but will be placed at the beginning of each chapter.&lt;/li&gt; &#xA; &lt;li&gt;The startpage and endpage settings are only supported for PDF files. This is because the font size and page size may vary in EPUB, DOCX, MOBI,and TXT files, making it difficult to process.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To use this tool, you will need to have Python 3 installed on your system, as well as the following packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pdfminer&lt;/li&gt; &#xA; &lt;li&gt;openai&lt;/li&gt; &#xA; &lt;li&gt;tqdm&lt;/li&gt; &#xA; &lt;li&gt;ebooklib&lt;/li&gt; &#xA; &lt;li&gt;bs4&lt;/li&gt; &#xA; &lt;li&gt;docx&lt;/li&gt; &#xA; &lt;li&gt;mobi&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can install these packages by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;git clone&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/jesselau76/ebook-GPT-translator.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update to new version&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ebook-GPT-translator&#xA;git pull&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To use this tool, you need rename settings.cfg.example to settings.cfg at first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ebook-GPT-translator&#xA;mv settings.cfg.example settings.cfg&#xA;nano settings.cfg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;openai-apikey = sk-xxxxxxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;replace sk-xxxxxxx to your OpenAI api key. Change others options then press CTRL-X to save.&lt;/p&gt; &#xA;&lt;p&gt;run the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py [-h] [--test] filename&#xA;&#xA;positional arguments:&#xA;  filename    Name of the input file&#xA;&#xA;options:&#xA;  -h, --help  show this help message and exit&#xA;  --test      Only translate the first 3 short texts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Simply run the &lt;code&gt;text_translation.py&lt;/code&gt; script with the file you want to translate or convert as an argument. For example, to translate a PDF file named &lt;code&gt;example.pdf&lt;/code&gt;, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to translate a epub file named &lt;code&gt;example.epub&lt;/code&gt;, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.epub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to translate a docx file named &lt;code&gt;example.docx&lt;/code&gt;, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.docx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to translate a text file named &lt;code&gt;example.txt&lt;/code&gt;, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to translate a MOBI file named example.mobi, you would run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 text_translation.py example.mobi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the script will attempt to translate the text into the language specified in the &lt;code&gt;settings.cfg&lt;/code&gt; file under the &lt;code&gt;target-language&lt;/code&gt; option. You can also choose to output a bilingual version of the text by setting the &lt;code&gt;bilingual-output&lt;/code&gt; option to &lt;code&gt;True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Feature&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code reads the OpenAI API key, target language, and other options from a settings.cfg file.&lt;/li&gt; &#xA; &lt;li&gt;The code converts PDF, DOCX and EPUB files to text using the pdfminer and ebooklib libraries, respectively.&lt;/li&gt; &#xA; &lt;li&gt;The code provides an option to output bilingual text.&lt;/li&gt; &#xA; &lt;li&gt;The code provides a progress bar to show the progress of PDF/EPUB to text conversion and translation&lt;/li&gt; &#xA; &lt;li&gt;Test function available. Only translate 3 short texts to save your API usage with --test.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;settings.cfg&lt;/code&gt; file contains several options that can be used to configure the behavior of the script:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;openai-apikey&lt;/code&gt;: Your API key for the OpenAI API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;target-language&lt;/code&gt;: The language you want to translate the text into (e.g. &lt;code&gt;ja&lt;/code&gt; for Japanese, &lt;code&gt;zh&lt;/code&gt; for Chinese, &lt;code&gt;文言文&lt;/code&gt; or &lt;code&gt;红楼梦风格的半文言文&lt;/code&gt; etc.). &lt;img src=&#34;https://user-images.githubusercontent.com/40444824/223943798-4faf91a0-05ec-4a4e-9731-ba80bc9845c2.png&#34; alt=&#34;文言文&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bilingual-output&lt;/code&gt;: Whether or not to output a bilingual version of the text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;langcode&lt;/code&gt;: The language code for the output epub file (e.g. &lt;code&gt;ja&lt;/code&gt; for Japanese, &lt;code&gt;zh&lt;/code&gt; for Chinese, etc.).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;startpage&lt;/code&gt;: Translation begins from the specified start page number and is exclusively available for PDF files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;endpage&lt;/code&gt;: Translation will continue until the specified page number in a PDF file. This feature supports PDF files exclusively. If the input is equal to -1, the translation will proceed until the end of the file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Output&lt;/h2&gt; &#xA;&lt;p&gt;The output of the script will be an EPUB file with the same name as the input file, but with &lt;code&gt;_translated&lt;/code&gt; appended to the end. For example, if the input file is &lt;code&gt;example.pdf&lt;/code&gt;, the output file will be &lt;code&gt;example_translated.epub&lt;/code&gt; and &lt;code&gt;example_translated.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This tool is released under the MIT License.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer:&lt;/h2&gt; &#xA;&lt;p&gt;This project is intended for use with public domain books and materials only. It is not designed for use with copyrighted content. Users are strongly advised to carefully review copyright information before utilizing this project and to adhere to relevant laws and regulations in order to protect their own rights and the rights of others.&lt;/p&gt; &#xA;&lt;p&gt;The authors and developers of this project shall not be held responsible for any loss or damage resulting from the use of this project. Users assume all risks associated with its use. It is the responsibility of users to ensure they have obtained permission from the original copyright holder or used open-source PDF, EPUB, or MOBI files before employing this project to avoid potential copyright risks.&lt;/p&gt; &#xA;&lt;p&gt;If you have any concerns or suggestions about the use of this project, please contact us through the issues section.&lt;/p&gt;</summary>
  </entry>
</feed>