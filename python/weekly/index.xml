<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-12T01:42:58Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>VikParuchuri/surya</title>
    <updated>2024-05-12T01:42:58Z</updated>
    <id>tag:github.com,2024-05-12:/VikParuchuri/surya</id>
    <link href="https://github.com/VikParuchuri/surya" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OCR, layout analysis, reading order, line detection in 90+ languages&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Surya&lt;/h1&gt; &#xA;&lt;p&gt;Surya is a document OCR toolkit that does:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OCR in 90+ languages that benchmarks favorably vs cloud services&lt;/li&gt; &#xA; &lt;li&gt;Line-level text detection in any language&lt;/li&gt; &#xA; &lt;li&gt;Layout analysis (table, image, header, etc detection)&lt;/li&gt; &#xA; &lt;li&gt;Reading order detection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It works on a range of documents (see &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/#usage&#34;&gt;usage&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/#benchmarks&#34;&gt;benchmarks&lt;/a&gt; for more details).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Detection&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;OCR&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt.png&#34; alt=&#34;New York Times Article Detection&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt_text.png&#34; alt=&#34;New York Times Article Recognition&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Layout&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reading Order&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt_layout.png&#34; alt=&#34;New York Times Article Layout&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt_reading.jpg&#34; alt=&#34;New York Times Article Reading Order&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Surya is named for the &lt;a href=&#34;https://en.wikipedia.org/wiki/Surya&#34;&gt;Hindu sun god&lt;/a&gt;, who has universal vision.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg//KuZwXNGnfH&#34;&gt;Discord&lt;/a&gt; is where we discuss future development.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Detection&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;OCR&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Layout&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Order&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Japanese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hindi&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arabic&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/arabic.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/arabic_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/arabic_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/arabic_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese + Hindi&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chi_hind.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chi_hind_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chi_hind_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chi_hind_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Presentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scientific Paper&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scanned Document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;New York Times&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt_order.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scanned Form&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd_reading.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Textbook&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/textbook.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/textbook_text.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/textbook_layout.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/textbook_order.jpg&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Commercial usage&lt;/h1&gt; &#xA;&lt;p&gt;I want surya to be as widely accessible as possible, while still funding my development/training costs. Research and personal usage is always okay, but there are some restrictions on commercial usage.&lt;/p&gt; &#xA;&lt;p&gt;The weights for the models are licensed &lt;code&gt;cc-by-nc-sa-4.0&lt;/code&gt;, but I will waive that for any organization under $5M USD in gross revenue in the most recent 12-month period AND under $5M in lifetime VC/angel funding raised. If you want to remove the GPL license requirements (dual-license) and/or use the weights commercially over the revenue limit, check out the options &lt;a href=&#34;https://www.datalab.to&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;You&#39;ll need python 3.9+ and PyTorch. You may need to install the CPU version of torch first if you&#39;re not using a Mac or a GPU machine. See &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install surya-ocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weights will automatically download the first time you run surya. Note that this does not work with the latest version of transformers &lt;code&gt;4.37+&lt;/code&gt; &lt;a href=&#34;https://github.com/huggingface/transformers/issues/28846#issuecomment-1926109135&#34;&gt;yet&lt;/a&gt;, so you will need to keep &lt;code&gt;4.36.2&lt;/code&gt;, which is installed with surya.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspect the settings in &lt;code&gt;surya/settings.py&lt;/code&gt;. You can override any settings with environment variables.&lt;/li&gt; &#xA; &lt;li&gt;Your torch device will be automatically detected, but you can override this. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt;. For text detection, the &lt;code&gt;mps&lt;/code&gt; device has a bug (on the &lt;a href=&#34;https://github.com/pytorch/pytorch/issues/84936&#34;&gt;Apple side&lt;/a&gt;) that may prevent it from working properly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Interactive App&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;ve included a streamlit app that lets you interactively try Surya on images or PDF files. Run it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install streamlit&#xA;surya_gui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pass the &lt;code&gt;--math&lt;/code&gt; command line argument to use the math text detection model instead of the default model. This will detect math better, but will be worse at everything else.&lt;/p&gt; &#xA;&lt;h2&gt;OCR (text recognition)&lt;/h2&gt; &#xA;&lt;p&gt;This command will write out a json file with the detected text and bboxes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;surya_ocr DATA_PATH --images --langs hi,en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--langs&lt;/code&gt; specifies the language(s) to use for OCR. You can comma separate multiple languages (I don&#39;t recommend using more than &lt;code&gt;4&lt;/code&gt;). Use the language name or two-letter ISO code from &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes&#34;&gt;here&lt;/a&gt;. Surya supports the 90+ languages found in &lt;code&gt;surya/languages.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--lang_file&lt;/code&gt; if you want to use a different language for different PDFs/images, you can specify languages here. The format is a JSON dict with the keys being filenames and the values as a list, like &lt;code&gt;{&#34;file1.pdf&#34;: [&#34;en&#34;, &#34;hi&#34;], &#34;file2.pdf&#34;: [&#34;en&#34;]}&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--start_page&lt;/code&gt; specifies the page number to start processing from&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;text_lines&lt;/code&gt; - the detected text and bounding boxes for each line &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;text&lt;/code&gt; - the text in the line&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;confidence&lt;/code&gt; - the confidence of the model in the detected text (0-1)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;polygon&lt;/code&gt; - the polygon for the text line in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;languages&lt;/code&gt; - the languages specified for the page&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page&lt;/code&gt; - the page number in the file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_bbox&lt;/code&gt; - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;RECOGNITION_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;50MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;256&lt;/code&gt;, which will use about 12.8GB of VRAM. Depending on your CPU core count, it may help, too - the default CPU batch size is &lt;code&gt;32&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from surya.ocr import run_ocr&#xA;from surya.model.detection import segformer&#xA;from surya.model.recognition.model import load_model&#xA;from surya.model.recognition.processor import load_processor&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;langs = [&#34;en&#34;] # Replace with your languages&#xA;det_processor, det_model = segformer.load_processor(), segformer.load_model()&#xA;rec_model, rec_processor = load_model(), load_processor()&#xA;&#xA;predictions = run_ocr([image], [langs], det_model, det_processor, rec_model, rec_processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text line detection&lt;/h2&gt; &#xA;&lt;p&gt;This command will write out a json file with the detected bboxes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;surya_detect DATA_PATH --images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--math&lt;/code&gt; uses a specialized math detection model instead of the default model. This will be better at math, but worse at everything else.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bboxes&lt;/code&gt; - detected bounding boxes for text &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;polygon&lt;/code&gt; - the polygon for the text line in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;confidence&lt;/code&gt; - the confidence of the model in the detected text (0-1)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vertical_lines&lt;/code&gt; - vertical lines detected in the document &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned line coordinates.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;horizontal_lines&lt;/code&gt; - horizontal lines detected in the document &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned line coordinates.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page&lt;/code&gt; - the page number in the file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_bbox&lt;/code&gt; - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;DETECTOR_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;280MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;32&lt;/code&gt;, which will use about 9GB of VRAM. Depending on your CPU core count, it might help, too - the default CPU batch size is &lt;code&gt;2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from surya.detection import batch_text_detection&#xA;from surya.model.detection.segformer import load_model, load_processor&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;model, processor = load_model(), load_processor()&#xA;&#xA;# predictions is a list of dicts, one per image&#xA;predictions = batch_text_detection([image], model, processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Layout analysis&lt;/h2&gt; &#xA;&lt;p&gt;This command will write out a json file with the detected layout.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;surya_layout DATA_PATH --images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bboxes&lt;/code&gt; - detected bounding boxes for text &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;polygon&lt;/code&gt; - the polygon for the text line in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;confidence&lt;/code&gt; - the confidence of the model in the detected text (0-1). This is currently not very reliable.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;label&lt;/code&gt; - the label for the bbox. One of &lt;code&gt;Caption&lt;/code&gt;, &lt;code&gt;Footnote&lt;/code&gt;, &lt;code&gt;Formula&lt;/code&gt;, &lt;code&gt;List-item&lt;/code&gt;, &lt;code&gt;Page-footer&lt;/code&gt;, &lt;code&gt;Page-header&lt;/code&gt;, &lt;code&gt;Picture&lt;/code&gt;, &lt;code&gt;Figure&lt;/code&gt;, &lt;code&gt;Section-header&lt;/code&gt;, &lt;code&gt;Table&lt;/code&gt;, &lt;code&gt;Text&lt;/code&gt;, &lt;code&gt;Title&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page&lt;/code&gt; - the page number in the file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_bbox&lt;/code&gt; - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;DETECTOR_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;280MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;32&lt;/code&gt;, which will use about 9GB of VRAM. Depending on your CPU core count, it might help, too - the default CPU batch size is &lt;code&gt;2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from surya.detection import batch_text_detection&#xA;from surya.layout import batch_layout_detection&#xA;from surya.model.detection.segformer import load_model, load_processor&#xA;from surya.settings import settings&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;model = load_model(checkpoint=settings.LAYOUT_MODEL_CHECKPOINT)&#xA;processor = load_processor(checkpoint=settings.LAYOUT_MODEL_CHECKPOINT)&#xA;det_model = load_model()&#xA;det_processor = load_processor()&#xA;&#xA;# layout_predictions is a list of dicts, one per image&#xA;line_predictions = batch_text_detection([image], det_model, det_processor)&#xA;layout_predictions = batch_layout_detection([image], model, processor, line_predictions)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reading order&lt;/h2&gt; &#xA;&lt;p&gt;This command will write out a json file with the detected reading order and layout.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;surya_order DATA_PATH --images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bboxes&lt;/code&gt; - detected bounding boxes for text &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;bbox&lt;/code&gt; - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;position&lt;/code&gt; - the position in the reading order of the bbox, starting from 0.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;label&lt;/code&gt; - the label for the bbox. See the layout section of the documentation for a list of potential labels.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page&lt;/code&gt; - the page number in the file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_bbox&lt;/code&gt; - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;ORDER_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;360MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;32&lt;/code&gt;, which will use about 11GB of VRAM. Depending on your CPU core count, it might help, too - the default CPU batch size is &lt;code&gt;4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from surya.ordering import batch_ordering&#xA;from surya.model.ordering.processor import load_processor&#xA;from surya.model.ordering.model import load_model&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;# bboxes should be a list of lists with layout bboxes for the image in [x1,y1,x2,y2] format&#xA;# You can get this from the layout model, see above for usage&#xA;bboxes = [bbox1, bbox2, ...]&#xA;&#xA;model = load_model()&#xA;processor = load_processor()&#xA;&#xA;# order_predictions will be a list of dicts, one per image&#xA;order_predictions = batch_ordering([image], [bboxes], model, processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is specialized for document OCR. It will likely not work on photos or other images.&lt;/li&gt; &#xA; &lt;li&gt;It is for printed text, not handwriting (though it may work on some handwriting).&lt;/li&gt; &#xA; &lt;li&gt;The text detection model has trained itself to ignore advertisements.&lt;/li&gt; &#xA; &lt;li&gt;You can find language support for OCR in &lt;code&gt;surya/languages.py&lt;/code&gt;. Text detection, layout analysis, and reading order will work with any language.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;If OCR isn&#39;t working properly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try increasing resolution of the image so the text is bigger. If the resolution is already very high, try decreasing it to no more than a &lt;code&gt;2048px&lt;/code&gt; width.&lt;/li&gt; &#xA; &lt;li&gt;Preprocessing the image (binarizing, deskewing, etc) can help with very old/blurry images.&lt;/li&gt; &#xA; &lt;li&gt;You can adjust &lt;code&gt;DETECTOR_BLANK_THRESHOLD&lt;/code&gt; and &lt;code&gt;DETECTOR_TEXT_THRESHOLD&lt;/code&gt; if you don&#39;t get good results. &lt;code&gt;DETECTOR_BLANK_THRESHOLD&lt;/code&gt; controls the space between lines - any prediction below this number will be considered blank space. &lt;code&gt;DETECTOR_TEXT_THRESHOLD&lt;/code&gt; controls how text is joined - any number above this is considered text. &lt;code&gt;DETECTOR_TEXT_THRESHOLD&lt;/code&gt; should always be higher than &lt;code&gt;DETECTOR_BLANK_THRESHOLD&lt;/code&gt;, and both should be in the 0-1 range. Looking at the heatmap from the debug output of the detector can tell you how to adjust these (if you see faint things that look like boxes, lower the thresholds, and if you see bboxes being joined together, raise the thresholds).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Manual install&lt;/h1&gt; &#xA;&lt;p&gt;If you want to develop surya, you can install it manually:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/VikParuchuri/surya.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd surya&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry install&lt;/code&gt; - installs main and dev dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry shell&lt;/code&gt; - activates the virtual environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;h2&gt;OCR&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_rec_chart.png&#34; alt=&#34;Benchmark chart tesseract&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Time per page (s)&lt;/th&gt; &#xA;   &lt;th&gt;Avg similarity (â¬†)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;surya&lt;/td&gt; &#xA;   &lt;td&gt;.62&lt;/td&gt; &#xA;   &lt;td&gt;0.97&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tesseract&lt;/td&gt; &#xA;   &lt;td&gt;.45&lt;/td&gt; &#xA;   &lt;td&gt;0.88&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/rec_acc_table.png&#34;&gt;Full language results&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tesseract is CPU-based, and surya is CPU or GPU. I tried to cost-match the resources used, so I used a 1xA6000 (48GB VRAM) for surya, and 28 CPU cores for Tesseract (same price on Lambda Labs/DigitalOcean).&lt;/p&gt; &#xA;&lt;h3&gt;Google Cloud Vision&lt;/h3&gt; &#xA;&lt;p&gt;I benchmarked OCR against Google Cloud vision since it has similar language coverage to Surya.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/gcloud_rec_bench.png&#34; alt=&#34;Benchmark chart google cloud&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/gcloud_full_langs.png&#34;&gt;Full language results&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I measured normalized sentence similarity (0-1, higher is better) based on a set of real-world and synthetic pdfs. I sampled PDFs from common crawl, then filtered out the ones with bad OCR. I couldn&#39;t find PDFs for some languages, so I also generated simple synthetic PDFs for those.&lt;/p&gt; &#xA;&lt;p&gt;I used the reference line bboxes from the PDFs with both tesseract and surya, to just evaluate the OCR quality.&lt;/p&gt; &#xA;&lt;p&gt;For Google Cloud, I aligned the output from Google Cloud with the ground truth. I had to skip RTL languages since they didn&#39;t align well.&lt;/p&gt; &#xA;&lt;h2&gt;Text line detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_chart_small.png&#34; alt=&#34;Benchmark chart&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Time (s)&lt;/th&gt; &#xA;   &lt;th&gt;Time per page (s)&lt;/th&gt; &#xA;   &lt;th&gt;precision&lt;/th&gt; &#xA;   &lt;th&gt;recall&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;surya&lt;/td&gt; &#xA;   &lt;td&gt;52.6892&lt;/td&gt; &#xA;   &lt;td&gt;0.205817&lt;/td&gt; &#xA;   &lt;td&gt;0.844426&lt;/td&gt; &#xA;   &lt;td&gt;0.937818&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tesseract&lt;/td&gt; &#xA;   &lt;td&gt;74.4546&lt;/td&gt; &#xA;   &lt;td&gt;0.290838&lt;/td&gt; &#xA;   &lt;td&gt;0.631498&lt;/td&gt; &#xA;   &lt;td&gt;0.997694&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Tesseract is CPU-based, and surya is CPU or GPU. I ran the benchmarks on a system with an A6000 GPU, and a 32 core CPU. This was the resource usage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;tesseract - 32 CPU cores, or 8 workers using 4 cores each&lt;/li&gt; &#xA; &lt;li&gt;surya - 32 batch size, for 9GB VRAM usage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Surya predicts line-level bboxes, while tesseract and others predict word-level or character-level. It&#39;s hard to find 100% correct datasets with line-level annotations. Merging bboxes can be noisy, so I chose not to use IoU as the metric for evaluation.&lt;/p&gt; &#xA;&lt;p&gt;I instead used coverage, which calculates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Precision - how well the predicted bboxes cover ground truth bboxes&lt;/li&gt; &#xA; &lt;li&gt;Recall - how well ground truth bboxes cover predicted bboxes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;First calculate coverage for each bbox, then add a small penalty for double coverage, since we want the detection to have non-overlapping bboxes. Anything with a coverage of 0.5 or higher is considered a match.&lt;/p&gt; &#xA;&lt;p&gt;Then we calculate precision and recall for the whole dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Layout analysis&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_layout_chart.png&#34; alt=&#34;Benchmark chart&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Layout Type&lt;/th&gt; &#xA;   &lt;th&gt;precision&lt;/th&gt; &#xA;   &lt;th&gt;recall&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image&lt;/td&gt; &#xA;   &lt;td&gt;0.95&lt;/td&gt; &#xA;   &lt;td&gt;0.99&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Table&lt;/td&gt; &#xA;   &lt;td&gt;0.95&lt;/td&gt; &#xA;   &lt;td&gt;0.96&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;0.89&lt;/td&gt; &#xA;   &lt;td&gt;0.95&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Title&lt;/td&gt; &#xA;   &lt;td&gt;0.92&lt;/td&gt; &#xA;   &lt;td&gt;0.89&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Time per image - .79 seconds on GPU (A6000).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I benchmarked the layout analysis on &lt;a href=&#34;https://github.com/ibm-aur-nlp/PubLayNet&#34;&gt;Publaynet&lt;/a&gt;, which was not in the training data. I had to align publaynet labels with the surya layout labels. I was then able to find coverage for each layout type:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Precision - how well the predicted bboxes cover ground truth bboxes&lt;/li&gt; &#xA; &lt;li&gt;Recall - how well ground truth bboxes cover predicted bboxes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reading Order&lt;/h2&gt; &#xA;&lt;p&gt;75% mean accuracy, and .14 seconds per image on an A6000 GPU. See methodology for notes - this benchmark is not perfect measure of accuracy, and is more useful as a sanity check.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I benchmarked the layout analysis on the layout dataset from &lt;a href=&#34;https://www.icst.pku.edu.cn/cpdp/sjzy/&#34;&gt;here&lt;/a&gt;, which was not in the training data. Unfortunately, this dataset is fairly noisy, and not all the labels are correct. It was very hard to find a dataset annotated with reading order and also layout information. I wanted to avoid using a cloud service for the ground truth.&lt;/p&gt; &#xA;&lt;p&gt;The accuracy is computed by finding if each pair of layout boxes is in the correct order, then taking the % that are correct.&lt;/p&gt; &#xA;&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;You can benchmark the performance of surya on your machine.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the manual install instructions above.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry install --group dev&lt;/code&gt; - installs dev dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text line detection&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will evaluate tesseract and surya for text line detection across a randomly sampled set of images from &lt;a href=&#34;https://huggingface.co/datasets/vikp/doclaynet_bench&#34;&gt;doclaynet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/detection.py --max 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt; will render images and detected bboxes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--pdf_path&lt;/code&gt; will let you specify a pdf to benchmark instead of the default data&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text recognition&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will evaluate surya and optionally tesseract on multilingual pdfs from common crawl (with synthetic data for missing languages).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/recognition.py --tesseract&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug 2&lt;/code&gt; will render images with detected text&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--tesseract&lt;/code&gt; will run the benchmark with tesseract. You have to run &lt;code&gt;sudo apt-get install tesseract-ocr-all&lt;/code&gt; to install all tesseract data, and set &lt;code&gt;TESSDATA_PREFIX&lt;/code&gt; to the path to the tesseract data folder.&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;RECOGNITION_BATCH_SIZE=864&lt;/code&gt; to use the same batch size as the benchmark.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Layout analysis&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will evaluate surya on the publaynet dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/layout.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt; will render images with detected text&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reading Order&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/ordering.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt; will render images with detected text&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;p&gt;Text detection was trained on 4x A6000s for 3 days. It used a diverse set of images as training data. It was trained from scratch using a modified segformer architecture that reduces inference RAM requirements.&lt;/p&gt; &#xA;&lt;p&gt;Text recognition was trained on 4x A6000s for 2 weeks. It was trained using a modified donut model (GQA, MoE layer, UTF-16 decoding, layer config changes).&lt;/p&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;p&gt;This work would not have been possible without amazing open source AI work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.15203.pdf&#34;&gt;Segformer&lt;/a&gt; from NVIDIA&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/clovaai/donut&#34;&gt;Donut&lt;/a&gt; from Naver&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; from huggingface&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/clovaai/CRAFT-pytorch&#34;&gt;CRAFT&lt;/a&gt;, a great scene text detection model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thank you to everyone who makes open source AI possible.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>JoeanAmier/XHS-Downloader</title>
    <updated>2024-05-12T01:42:58Z</updated>
    <id>tag:github.com,2024-05-12:/JoeanAmier/XHS-Downloader</id>
    <link href="https://github.com/JoeanAmier/XHS-Downloader" rel="alternate"></link>
    <summary type="html">&lt;p&gt;å°çº¢ä¹¦é“¾æ¥æå–/ä½œå“é‡‡é›†å·¥å…·ï¼šæå–è´¦å·å‘å¸ƒã€æ”¶è—ã€ç‚¹èµä½œå“é“¾æ¥ï¼›æå–æœç´¢ç»“æœä½œå“ã€ç”¨æˆ·é“¾æ¥ï¼›é‡‡é›†å°çº¢ä¹¦ä½œå“ä¿¡æ¯ï¼›æå–å°çº¢ä¹¦ä½œå“ä¸‹è½½åœ°å€ï¼›ä¸‹è½½å°çº¢ä¹¦æ— æ°´å°ä½œå“æ–‡ä»¶ï¼&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/XHS-Downloader.png&#34; alt=&#34;&#34; height=&#34;256&#34; width=&#34;256&#34;&gt;&#xA; &lt;br&gt; &#xA; &lt;h1&gt;XHS-Downloader&lt;/h1&gt; &#xA; &lt;p&gt;ç®€ä½“ä¸­æ–‡ | &lt;a href=&#34;https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/README_EN.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=ff7a45&#34;&gt; &#xA; &lt;img alt=&#34;GitHub forks&#34; src=&#34;https://img.shields.io/github/forks/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=9254de&#34;&gt; &#xA; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=ff7875&#34;&gt; &#xA; &lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/UserScript-ffec3d?style=for-the-badge&amp;amp;logo=tampermonkey&amp;amp;logoColor=%2300485B&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img alt=&#34;GitHub code size in bytes&#34; src=&#34;https://img.shields.io/github/languages/code-size/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=73d13d&#34;&gt; &#xA; &lt;img alt=&#34;GitHub release (with filter)&#34; src=&#34;https://img.shields.io/github/v/release/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=40a9ff&#34;&gt; &#xA; &lt;img alt=&#34;GitHub all releases&#34; src=&#34;https://img.shields.io/github/downloads/JoeanAmier/XHS-Downloader/total?style=for-the-badge&amp;amp;color=f759ab&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;ğŸ”¥ &lt;b&gt;å°çº¢ä¹¦é“¾æ¥æå–/ä½œå“é‡‡é›†å·¥å…·&lt;/b&gt;ï¼šæå–è´¦å·å‘å¸ƒã€æ”¶è—ã€ç‚¹èµä½œå“é“¾æ¥ï¼›æå–æœç´¢ç»“æœä½œå“é“¾æ¥ã€ç”¨æˆ·é“¾æ¥ï¼›é‡‡é›†å°çº¢ä¹¦ä½œå“ä¿¡æ¯ï¼›æå–å°çº¢ä¹¦ä½œå“ä¸‹è½½åœ°å€ï¼›ä¸‹è½½å°çº¢ä¹¦æ— æ°´å°ä½œå“æ–‡ä»¶ï¼&lt;/p&gt; &#xA;&lt;h1&gt;ğŸ“‘ é¡¹ç›®åŠŸèƒ½&lt;/h1&gt; &#xA;&lt;ul&gt;&#xA; &lt;b&gt;ç¨‹åºåŠŸèƒ½&lt;/b&gt; &#xA; &lt;li&gt;âœ… é‡‡é›†å°çº¢ä¹¦ä½œå“ä¿¡æ¯&lt;/li&gt; &#xA; &lt;li&gt;âœ… æå–å°çº¢ä¹¦ä½œå“ä¸‹è½½åœ°å€&lt;/li&gt; &#xA; &lt;li&gt;âœ… ä¸‹è½½å°çº¢ä¹¦æ— æ°´å°ä½œå“æ–‡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;âœ… è‡ªåŠ¨è·³è¿‡å·²ä¸‹è½½çš„ä½œå“æ–‡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;âœ… ä½œå“æ–‡ä»¶å®Œæ•´æ€§å¤„ç†æœºåˆ¶&lt;/li&gt; &#xA; &lt;li&gt;âœ… è‡ªå®šä¹‰å›¾æ–‡ä½œå“æ–‡ä»¶ä¸‹è½½æ ¼å¼&lt;/li&gt; &#xA; &lt;li&gt;âœ… æŒä¹…åŒ–å‚¨å­˜ä½œå“ä¿¡æ¯è‡³æ–‡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;âœ… ä½œå“æ–‡ä»¶å‚¨å­˜è‡³å•ç‹¬æ–‡ä»¶å¤¹&lt;/li&gt; &#xA; &lt;li&gt;âœ… åå°ç›‘å¬å‰ªè´´æ¿ä¸‹è½½ä½œå“&lt;/li&gt; &#xA; &lt;li&gt;âœ… è®°å½•å·²ä¸‹è½½ä½œå“ ID&lt;/li&gt; &#xA; &lt;li&gt;âœ… æ”¯æŒå‘½ä»¤è¡Œä¸‹è½½ä½œå“æ–‡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;âœ… ä»æµè§ˆå™¨è¯»å– Cookie&lt;/li&gt; &#xA; &lt;li&gt;âœ… è‡ªå®šä¹‰æ–‡ä»¶åç§°æ ¼å¼&lt;/li&gt; &#xA; &lt;li&gt;â˜‘ï¸ æ”¯æŒ API è°ƒç”¨åŠŸèƒ½&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt;&#xA; &lt;b&gt;è„šæœ¬åŠŸèƒ½&lt;/b&gt; &#xA; &lt;li&gt;âœ… ä¸‹è½½å°çº¢ä¹¦æ— æ°´å°ä½œå“æ–‡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;âœ… æå–å‘ç°é¡µé¢ä½œå“é“¾æ¥&lt;/li&gt; &#xA; &lt;li&gt;âœ… æå–è´¦å·å‘å¸ƒä½œå“é“¾æ¥&lt;/li&gt; &#xA; &lt;li&gt;âœ… æå–è´¦å·æ”¶è—ä½œå“é“¾æ¥&lt;/li&gt; &#xA; &lt;li&gt;âœ… æå–è´¦å·ç‚¹èµä½œå“é“¾æ¥&lt;/li&gt; &#xA; &lt;li&gt;âœ… æå–æœç´¢ç»“æœä½œå“é“¾æ¥&lt;/li&gt; &#xA; &lt;li&gt;âœ… æå–æœç´¢ç»“æœç”¨æˆ·é“¾æ¥&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;ğŸ“¸ ç¨‹åºæˆªå›¾&lt;/h1&gt; &#xA;&lt;p&gt;&lt;b&gt;ğŸ¥ ç‚¹å‡»å›¾ç‰‡è§‚çœ‹æ¼”ç¤ºè§†é¢‘&lt;/b&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1PJ4m1Y7Jt/&#34;&gt;&lt;img src=&#34;static/screenshot/ç¨‹åºè¿è¡Œæˆªå›¾CN1.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1PJ4m1Y7Jt/&#34;&gt;&lt;img src=&#34;static/screenshot/ç¨‹åºè¿è¡Œæˆªå›¾CN2.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1PJ4m1Y7Jt/&#34;&gt;&lt;img src=&#34;static/screenshot/ç¨‹åºè¿è¡Œæˆªå›¾CN3.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;h1&gt;ğŸ”— æ”¯æŒé“¾æ¥&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;https://www.xiaohongshu.com/explore/ä½œå“ID&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;https://www.xiaohongshu.com/discovery/item/ä½œå“ID&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;https://xhslink.com/åˆ†äº«ç &lt;/code&gt;&lt;/li&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;b&gt;æ”¯æŒå•æ¬¡è¾“å…¥å¤šä¸ªä½œå“é“¾æ¥ï¼Œé“¾æ¥ä¹‹é—´ä½¿ç”¨ç©ºæ ¼åˆ†éš”ã€‚&lt;/b&gt;&lt;/p&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;ğŸªŸ å…³äºç»ˆç«¯&lt;/h1&gt; &#xA;&lt;p&gt;â­ æ¨èä½¿ç”¨ &lt;a href=&#34;https://learn.microsoft.com/zh-cn/windows/terminal/install&#34;&gt;Windows ç»ˆç«¯&lt;/a&gt; ï¼ˆWindows 11 é»˜è®¤ç»ˆç«¯ï¼‰è¿è¡Œç¨‹åºä»¥ä¾¿è·å¾—æœ€ä½³æ˜¾ç¤ºæ•ˆæœï¼&lt;/p&gt; &#xA;&lt;h1&gt;ğŸ¥£ ä½¿ç”¨æ–¹æ³•&lt;/h1&gt; &#xA;&lt;p&gt;å¦‚æœä»…éœ€ä¸‹è½½æ— æ°´å°ä½œå“æ–‡ä»¶ï¼Œå»ºè®®é€‰æ‹© &lt;b&gt;ç¨‹åºè¿è¡Œ&lt;/b&gt;ï¼›å¦‚æœæœ‰å…¶ä»–éœ€æ±‚ï¼Œå»ºè®®é€‰æ‹© &lt;b&gt;æºç è¿è¡Œ&lt;/b&gt;ï¼&lt;/p&gt; &#xA;&lt;p&gt;å»ºè®®è‡ªè¡Œè®¾ç½® &lt;code&gt;cookie&lt;/code&gt; å‚æ•°ï¼Œè‹¥ä¸è®¾ç½®è¯¥å‚æ•°ï¼Œç¨‹åºåŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸ä½¿ç”¨ï¼&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ–± ç¨‹åºè¿è¡Œ&lt;/h2&gt; &#xA;&lt;p&gt;Windows 10 åŠä»¥ä¸Šç”¨æˆ·å¯å‰å¾€ &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/releases/latest&#34;&gt;Releases&lt;/a&gt; ä¸‹è½½ç¨‹åºå‹ç¼©åŒ…ï¼Œè§£å‹åæ‰“å¼€ç¨‹åºæ–‡ä»¶å¤¹ï¼ŒåŒå‡»è¿è¡Œ &lt;code&gt;main.exe&lt;/code&gt; å³å¯ä½¿ç”¨ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è‹¥é€šè¿‡æ­¤æ–¹å¼ä½¿ç”¨ç¨‹åºï¼Œæ–‡ä»¶é»˜è®¤ä¸‹è½½è·¯å¾„ä¸ºï¼š&lt;code&gt;.\_internal\Download&lt;/code&gt;ï¼›é…ç½®æ–‡ä»¶è·¯å¾„ä¸ºï¼š&lt;code&gt;.\_internal\settings.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;âŒ¨ï¸ æºç è¿è¡Œ&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å®‰è£…ç‰ˆæœ¬å·ä¸ä½äº &lt;code&gt;3.12&lt;/code&gt; çš„ Python è§£é‡Šå™¨&lt;/li&gt; &#xA; &lt;li&gt;è¿è¡Œ &lt;code&gt;pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt&lt;/code&gt; å‘½ä»¤å®‰è£…ç¨‹åºæ‰€éœ€æ¨¡å—&lt;/li&gt; &#xA; &lt;li&gt;ä¸‹è½½æœ¬é¡¹ç›®æœ€æ–°çš„æºç æˆ– &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/releases/latest&#34;&gt;Releases&lt;/a&gt; å‘å¸ƒçš„æºç è‡³æœ¬åœ°&lt;/li&gt; &#xA; &lt;li&gt;è¿è¡Œ &lt;code&gt;main.py&lt;/code&gt; å³å¯ä½¿ç”¨&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;ğŸ›  å‘½ä»¤è¡Œæ¨¡å¼&lt;/h1&gt; &#xA;&lt;p&gt;é¡¹ç›®æ”¯æŒå‘½ä»¤è¡Œè¿è¡Œæ¨¡å¼ï¼Œè‹¥æƒ³è¦ä¸‹è½½å›¾æ–‡ä½œå“çš„éƒ¨åˆ†å›¾ç‰‡ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ¨¡å¼ä¼ å…¥éœ€è¦ä¸‹è½½çš„å›¾ç‰‡åºå·ï¼&lt;/p&gt; &#xA;&lt;p&gt;å¯ä»¥ä½¿ç”¨å‘½ä»¤è¡Œä»æµè§ˆå™¨è¯»å– Cookie å¹¶å†™å…¥é…ç½®æ–‡ä»¶ï¼æ³¨æ„éœ€è¦å…³é—­å¯¹åº”æµè§ˆå™¨æ‰èƒ½è¯»å–æ•°æ®ï¼&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;bool&lt;/code&gt; ç±»å‹å‚æ•°æ”¯æŒä½¿ç”¨ &lt;code&gt;true&lt;/code&gt;ã€&lt;code&gt;false&lt;/code&gt;ã€&lt;code&gt;1&lt;/code&gt;ã€&lt;code&gt;0&lt;/code&gt;ã€&lt;code&gt;yes&lt;/code&gt;ã€&lt;code&gt;no&lt;/code&gt;ã€&lt;code&gt;on&lt;/code&gt; æˆ– &lt;code&gt;off&lt;/code&gt;ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰æ¥è®¾ç½®ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å‘½ä»¤ç¤ºä¾‹ï¼š&lt;code&gt;python .\main.py --browser_cookie Chrome --update_settings&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;img src=&#34;static/screenshot/å‘½ä»¤è¡Œæ¨¡å¼æˆªå›¾1.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;img src=&#34;static/screenshot/å‘½ä»¤è¡Œæ¨¡å¼æˆªå›¾2.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;h1&gt;ğŸ•¹ ç”¨æˆ·è„šæœ¬&lt;/h1&gt; &#xA;&lt;img src=&#34;static/screenshot/ç”¨æˆ·è„šæœ¬æˆªå›¾1.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;img src=&#34;static/screenshot/ç”¨æˆ·è„šæœ¬æˆªå›¾2.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨çš„æµè§ˆå™¨å®‰è£…äº† &lt;a href=&#34;https://www.tampermonkey.net/&#34;&gt;Tampermonkey&lt;/a&gt; æµè§ˆå™¨æ‰©å±•ç¨‹åºï¼Œå¯ä»¥æ·»åŠ  &lt;a href=&#34;https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/XHS-Downloader.js&#34;&gt;ç”¨æˆ·è„šæœ¬&lt;/a&gt;ï¼Œæ— éœ€ä¸‹è½½å®‰è£…å³å¯ä½“éªŒé¡¹ç›®åŠŸèƒ½ï¼&lt;/p&gt; &#xA;&lt;p&gt;æç¤ºï¼šä½¿ç”¨ XHS-Downloader ç”¨æˆ·è„šæœ¬æ‰¹é‡æå–ä½œå“é“¾æ¥ï¼Œæ­é… XHS-Downloader ç¨‹åºå¯ä»¥å®ç°æ‰¹é‡ä¸‹è½½æ— æ°´å°ä½œå“æ–‡ä»¶ï¼&lt;/p&gt; &#xA;&lt;h1&gt;ğŸ’» äºŒæ¬¡å¼€å‘&lt;/h1&gt; &#xA;&lt;p&gt;å¦‚æœæœ‰å…¶ä»–éœ€æ±‚ï¼Œå¯ä»¥æ ¹æ® &lt;code&gt;main.py&lt;/code&gt; çš„æ³¨é‡Šæç¤ºè¿›è¡Œä»£ç è°ƒç”¨æˆ–ä¿®æ”¹ï¼&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;async def example():&#xA;    &#34;&#34;&#34;é€šè¿‡ä»£ç è®¾ç½®å‚æ•°ï¼Œé€‚åˆäºŒæ¬¡å¼€å‘&#34;&#34;&#34;&#xA;    # ç¤ºä¾‹é“¾æ¥&#xA;    error_link = &#34;https://github.com/JoeanAmier/XHS_Downloader&#34;&#xA;    demo_link = &#34;https://www.xiaohongshu.com/explore/xxxxxxxxxx&#34;&#xA;    multiple_links = f&#34;{demo_link} {demo_link} {demo_link}&#34;&#xA;    # å®ä¾‹å¯¹è±¡&#xA;    work_path = &#34;D:\\&#34;  # ä½œå“æ•°æ®/æ–‡ä»¶ä¿å­˜æ ¹è·¯å¾„ï¼Œé»˜è®¤å€¼ï¼šé¡¹ç›®æ ¹è·¯å¾„&#xA;    folder_name = &#34;Download&#34;  # ä½œå“æ–‡ä»¶å‚¨å­˜æ–‡ä»¶å¤¹åç§°ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰ï¼Œé»˜è®¤å€¼ï¼šDownload&#xA;    user_agent = &#34;&#34;  # è¯·æ±‚å¤´ User-Agentï¼Œå¯é€‰å‚æ•°&#xA;    cookie = &#34;&#34;  # å°çº¢ä¹¦ç½‘é¡µç‰ˆ Cookieï¼Œæ— éœ€ç™»å½•ï¼Œå¿…éœ€å‚æ•°&#xA;    proxy = None  # ç½‘ç»œä»£ç†&#xA;    timeout = 5  # è¯·æ±‚æ•°æ®è¶…æ—¶é™åˆ¶ï¼Œå•ä½ï¼šç§’ï¼Œé»˜è®¤å€¼ï¼š10&#xA;    chunk = 1024 * 1024 * 10  # ä¸‹è½½æ–‡ä»¶æ—¶ï¼Œæ¯æ¬¡ä»æœåŠ¡å™¨è·å–çš„æ•°æ®å—å¤§å°ï¼Œå•ä½ï¼šå­—èŠ‚&#xA;    max_retry = 2  # è¯·æ±‚æ•°æ®å¤±è´¥æ—¶ï¼Œé‡è¯•çš„æœ€å¤§æ¬¡æ•°ï¼Œå•ä½ï¼šç§’ï¼Œé»˜è®¤å€¼ï¼š5&#xA;    record_data = False  # æ˜¯å¦ä¿å­˜ä½œå“æ•°æ®è‡³æ–‡ä»¶&#xA;    image_format = &#34;WEBP&#34;  # å›¾æ–‡ä½œå“æ–‡ä»¶ä¸‹è½½æ ¼å¼ï¼Œæ”¯æŒï¼šPNGã€WEBP&#xA;    folder_mode = False  # æ˜¯å¦å°†æ¯ä¸ªä½œå“çš„æ–‡ä»¶å‚¨å­˜è‡³å•ç‹¬çš„æ–‡ä»¶å¤¹&#xA;    async with XHS() as xhs:&#xA;        pass  # ä½¿ç”¨é»˜è®¤å‚æ•°&#xA;    async with XHS(work_path=work_path,&#xA;                   folder_name=folder_name,&#xA;                   user_agent=user_agent,&#xA;                   cookie=cookie,&#xA;                   proxy=proxy,&#xA;                   timeout=timeout,&#xA;                   chunk=chunk,&#xA;                   max_retry=max_retry,&#xA;                   record_data=record_data,&#xA;                   image_format=image_format,&#xA;                   folder_mode=folder_mode,&#xA;                   ) as xhs:  # ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°&#xA;        download = True  # æ˜¯å¦ä¸‹è½½ä½œå“æ–‡ä»¶ï¼Œé»˜è®¤å€¼ï¼šFalse&#xA;        # è¿”å›ä½œå“è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸‹è½½åœ°å€&#xA;        # è·å–æ•°æ®å¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸&#xA;        print(await xhs.extract(error_link, download, ))&#xA;        print(await xhs.extract(demo_link, download, ))&#xA;        # æ”¯æŒä¼ å…¥å¤šä¸ªä½œå“é“¾æ¥&#xA;        print(await xhs.extract(multiple_links, download, ))&#xA;&lt;/pre&gt; &#xA;&lt;h1&gt;âš™ï¸ é…ç½®æ–‡ä»¶&lt;/h1&gt; &#xA;&lt;p&gt;é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ &lt;code&gt;settings.json&lt;/code&gt; æ–‡ä»¶ï¼Œé¦–æ¬¡è¿è¡Œè‡ªåŠ¨ç”Ÿæˆï¼Œå¯ä»¥è‡ªå®šä¹‰éƒ¨åˆ†è¿è¡Œå‚æ•°ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœè®¾ç½®äº†æ— æ•ˆçš„å‚æ•°å€¼ï¼Œç¨‹åºå°†ä¼šä½¿ç”¨å‚æ•°é»˜è®¤å€¼ï¼&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;å‚æ•°&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ç±»å‹&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;å«ä¹‰&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;é»˜è®¤å€¼&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;work_path&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ä½œå“æ•°æ® / æ–‡ä»¶ä¿å­˜æ ¹è·¯å¾„&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;é¡¹ç›®æ ¹è·¯å¾„&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;folder_name&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ä½œå“æ–‡ä»¶å‚¨å­˜æ–‡ä»¶å¤¹åç§°&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Download&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;name_format&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ä½œå“æ–‡ä»¶åç§°æ ¼å¼ï¼Œå­—æ®µä¹‹é—´ä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼Œæ”¯æŒå­—æ®µï¼š&lt;code&gt;æ”¶è—æ•°é‡&lt;/code&gt;ã€&lt;code&gt;è¯„è®ºæ•°é‡&lt;/code&gt;ã€&lt;code&gt;åˆ†äº«æ•°é‡&lt;/code&gt;ã€&lt;code&gt;ç‚¹èµæ•°é‡&lt;/code&gt;ã€&lt;code&gt;ä½œå“æ ‡ç­¾&lt;/code&gt;ã€&lt;code&gt;ä½œå“ID&lt;/code&gt;ã€&lt;code&gt;ä½œå“æ ‡é¢˜&lt;/code&gt;ã€&lt;code&gt;ä½œå“æè¿°&lt;/code&gt;ã€&lt;code&gt;ä½œå“ç±»å‹&lt;/code&gt;ã€&lt;code&gt;å‘å¸ƒæ—¶é—´&lt;/code&gt;ã€&lt;code&gt;æœ€åæ›´æ–°æ—¶é—´&lt;/code&gt;ã€&lt;code&gt;ä½œè€…æ˜µç§°&lt;/code&gt;ã€&lt;code&gt;ä½œè€…ID&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;å‘å¸ƒæ—¶é—´ ä½œè€…æ˜µç§° ä½œå“æ ‡é¢˜&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;user_agent&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è¯·æ±‚å¤´ User-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;é»˜è®¤ UA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;cookie&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;å°çº¢ä¹¦ç½‘é¡µç‰ˆ Cookieï¼Œ&lt;b&gt;æ— éœ€ç™»å½•&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;æ— &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;proxy&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è®¾ç½®ç¨‹åºä»£ç†&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;null&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;timeout&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;int&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è¯·æ±‚æ•°æ®è¶…æ—¶é™åˆ¶ï¼Œå•ä½ï¼šç§’&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;chunk&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;int&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ä¸‹è½½æ–‡ä»¶æ—¶ï¼Œæ¯æ¬¡ä»æœåŠ¡å™¨è·å–çš„æ•°æ®å—å¤§å°ï¼Œå•ä½ï¼šå­—èŠ‚&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1048576(1 MB)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;max_retry&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;int&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è¯·æ±‚æ•°æ®å¤±è´¥æ—¶ï¼Œé‡è¯•çš„æœ€å¤§æ¬¡æ•°ï¼Œå•ä½ï¼šç§’&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;record_data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;æ˜¯å¦ä¿å­˜ä½œå“æ•°æ®è‡³æ–‡ä»¶ï¼Œä¿å­˜æ ¼å¼ï¼š&lt;code&gt;SQLite&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;false&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;image_format&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;å›¾æ–‡ä½œå“æ–‡ä»¶ä¸‹è½½æ ¼å¼ï¼Œæ”¯æŒï¼š&lt;code&gt;PNG&lt;/code&gt;ã€&lt;code&gt;WEBP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PNG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;image_download&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;å›¾æ–‡ä½œå“æ–‡ä»¶ä¸‹è½½å¼€å…³&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;true&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;video_download&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è§†é¢‘ä½œå“æ–‡ä»¶ä¸‹è½½å¼€å…³&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;true&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;folder_mode&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;æ˜¯å¦å°†æ¯ä¸ªä½œå“çš„æ–‡ä»¶å‚¨å­˜è‡³å•ç‹¬çš„æ–‡ä»¶å¤¹ï¼›æ–‡ä»¶å¤¹åç§°ä¸æ–‡ä»¶åç§°ä¿æŒä¸€è‡´&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;false&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;language&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;è®¾ç½®ç¨‹åºè¯­è¨€ï¼Œç›®å‰æ”¯æŒï¼š&lt;code&gt;zh_CN&lt;/code&gt;ã€&lt;code&gt;en_GB&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zh_CN&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;ğŸŒ Cookie&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æ‰“å¼€æµè§ˆå™¨ï¼ˆå¯é€‰æ— ç—•æ¨¡å¼å¯åŠ¨ï¼‰ï¼Œè®¿é—® &lt;code&gt;https://www.xiaohongshu.com/explore&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;æŒ‰ä¸‹ &lt;code&gt;F12&lt;/code&gt; æ‰“å¼€å¼€å‘äººå‘˜å·¥å…·&lt;/li&gt; &#xA; &lt;li&gt;é€‰æ‹© &lt;code&gt;ç½‘ç»œ&lt;/code&gt; é€‰é¡¹å¡&lt;/li&gt; &#xA; &lt;li&gt;å‹¾é€‰ &lt;code&gt;ä¿ç•™æ—¥å¿—&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;åœ¨ &lt;code&gt;è¿‡æ»¤&lt;/code&gt; è¾“å…¥æ¡†è¾“å…¥ &lt;code&gt;cookie-name:web_session&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;é€‰æ‹© &lt;code&gt;Fetch/XHR&lt;/code&gt; ç­›é€‰å™¨&lt;/li&gt; &#xA; &lt;li&gt;ç‚¹å‡»å°çº¢ä¹¦é¡µé¢ä»»æ„ä½œå“&lt;/li&gt; &#xA; &lt;li&gt;åœ¨ &lt;code&gt;ç½‘ç»œ&lt;/code&gt; é€‰é¡¹å¡é€‰æ‹©ä»»æ„æ•°æ®åŒ…ï¼ˆå¦‚æœæ— æ•°æ®åŒ…ï¼Œé‡å¤æ­¥éª¤7ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;å…¨é€‰å¤åˆ¶ Cookie å†™å…¥ç¨‹åºæˆ–é…ç½®æ–‡ä»¶&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;static/screenshot/è·å–Cookieç¤ºæ„å›¾.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;h1&gt;ğŸ—³ ä¸‹è½½è®°å½•&lt;/h1&gt; &#xA;&lt;p&gt;XHS-Downloader ä¼šå°†ä¸‹è½½è¿‡çš„ä½œå“ ID å‚¨å­˜è‡³æ•°æ®åº“ï¼Œå½“é‡å¤ä¸‹è½½ç›¸åŒçš„ä½œå“æ—¶ï¼ŒXHS-Downloader ä¼šè‡ªåŠ¨è·³è¿‡è¯¥ä½œå“çš„æ–‡ä»¶ä¸‹è½½ï¼ˆå³ä½¿ä½œå“æ–‡ä»¶ä¸å­˜åœ¨ï¼‰ï¼Œå¦‚æœæƒ³è¦é‡æ–°ä¸‹è½½ä½œå“æ–‡ä»¶ï¼Œè¯·å…ˆåˆ é™¤æ•°æ®åº“ä¸­å¯¹åº”çš„ä½œå“ IDï¼Œå†ä½¿ç”¨ XHS-Downloader ä¸‹è½½ä½œå“æ–‡ä»¶ï¼&lt;/p&gt; &#xA;&lt;h1&gt;â™¥ï¸ æ”¯æŒé¡¹ç›®&lt;/h1&gt; &#xA;&lt;p&gt;å¦‚æœ &lt;b&gt;XHS-Downloader&lt;/b&gt; å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä¸ºå®ƒç‚¹ä¸ª &lt;b&gt;Star&lt;/b&gt; â­ï¼Œæ„Ÿè°¢æ‚¨çš„æ”¯æŒï¼&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;å¾®ä¿¡(WeChat)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;æ”¯ä»˜å®(Alipay)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;./static/å¾®ä¿¡èµåŠ©äºŒç»´ç .png&#34; alt=&#34;å¾®ä¿¡èµåŠ©äºŒç»´ç &#34; height=&#34;200&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;./static/æ”¯ä»˜å®èµåŠ©äºŒç»´ç .png&#34; alt=&#34;æ”¯ä»˜å®èµåŠ©äºŒç»´ç &#34; height=&#34;200&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨æ„¿æ„ï¼Œå¯ä»¥è€ƒè™‘æä¾›èµ„åŠ©ä¸º &lt;b&gt;XHS-Downloader&lt;/b&gt; æä¾›é¢å¤–çš„æ”¯æŒï¼&lt;/p&gt; &#xA;&lt;h1&gt;âœ‰ï¸ è”ç³»ä½œè€…&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å¾®ä¿¡(å…¶ä»–äº‹åŠ¡): Downloader_Tools&lt;/li&gt; &#xA; &lt;li&gt;å¾®ä¿¡å…¬ä¼—å·(é—®é¢˜è§£ç­”): Downloader Tools&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;Discord ç¤¾åŒº&lt;/b&gt;: &lt;a href=&#34;https://discord.com/invite/ZYtmgKud9Y&#34;&gt;ç‚¹å‡»åŠ å…¥ç¤¾åŒº&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;QQ ç¾¤èŠ(ä½¿ç”¨äº¤æµ): &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/raw/master/static/QQ%E7%BE%A4%E8%81%8A%E4%BA%8C%E7%BB%B4%E7%A0%81.png&#34;&gt;æ‰«ç åŠ å…¥ç¾¤èŠ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;b&gt;è¯´æ˜ï¼š&lt;/b&gt;QQ ç¾¤èŠä»…é™äºè®¨è®ºé¡¹ç›®ä½¿ç”¨é—®é¢˜ï¼Œä¸¥ç¦å‘å¸ƒä»»ä½•å¹¿å‘Šï¼Œä¸¥ç¦è®¨è®ºä»»ä½•è´¦å·äº¤æ˜“ã€è´¦å·æµé‡ã€æµé‡å˜ç°ã€ç°è‰²äº§ä¸šç­‰ç›¸å…³çš„å†…å®¹ï¼&lt;/p&gt; &#xA;&lt;p&gt;âœ¨ &lt;b&gt;ä½œè€…çš„å…¶ä»–å¼€æºé¡¹ç›®ï¼š&lt;/b&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;TikTokDownloaderï¼ˆæŠ–éŸ³ / TikTokï¼‰&lt;/b&gt;ï¼š&lt;a href=&#34;https://github.com/JoeanAmier/TikTokDownloader&#34;&gt;https://github.com/JoeanAmier/TikTokDownloader&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;KS-Downloaderï¼ˆå¿«æ‰‹ï¼‰&lt;/b&gt;ï¼š&lt;a href=&#34;https://github.com/JoeanAmier/KS-Downloader&#34;&gt;https://github.com/JoeanAmier/KS-Downloader&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;âš ï¸ å…è´£å£°æ˜&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä½¿ç”¨è€…å¯¹æœ¬é¡¹ç›®çš„ä½¿ç”¨ç”±ä½¿ç”¨è€…è‡ªè¡Œå†³å®šï¼Œå¹¶è‡ªè¡Œæ‰¿æ‹…é£é™©ã€‚ä½œè€…å¯¹ä½¿ç”¨è€…ä½¿ç”¨æœ¬é¡¹ç›®æ‰€äº§ç”Ÿçš„ä»»ä½•æŸå¤±ã€è´£ä»»ã€æˆ–é£é™©æ¦‚ä¸è´Ÿè´£ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®çš„ä½œè€…æä¾›çš„ä»£ç å’ŒåŠŸèƒ½æ˜¯åŸºäºç°æœ‰çŸ¥è¯†å’ŒæŠ€æœ¯çš„å¼€å‘æˆæœã€‚ä½œè€…å°½åŠ›ç¡®ä¿ä»£ç çš„æ­£ç¡®æ€§å’Œå®‰å…¨æ€§ï¼Œä½†ä¸ä¿è¯ä»£ç å®Œå…¨æ²¡æœ‰é”™è¯¯æˆ–ç¼ºé™·ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ä½¿ç”¨è€…åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶å¿…é¡»ä¸¥æ ¼éµå®ˆ &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/raw/master/LICENSE&#34;&gt;GNU General Public License v3.0&lt;/a&gt; çš„è¦æ±‚ï¼Œå¹¶åœ¨é€‚å½“çš„åœ°æ–¹æ³¨æ˜ä½¿ç”¨äº† &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/raw/master/LICENSE&#34;&gt;GNU General Public License v3.0&lt;/a&gt; çš„ä»£ç ã€‚ &lt;/li&gt; &#xA; &lt;li&gt;ä½¿ç”¨è€…åœ¨ä»»ä½•æƒ…å†µä¸‹å‡ä¸å¾—å°†æœ¬é¡¹ç›®çš„ä½œè€…ã€è´¡çŒ®è€…æˆ–å…¶ä»–ç›¸å…³æ–¹ä¸ä½¿ç”¨è€…çš„ä½¿ç”¨è¡Œä¸ºè”ç³»èµ·æ¥ï¼Œæˆ–è¦æ±‚å…¶å¯¹ä½¿ç”¨è€…ä½¿ç”¨æœ¬é¡¹ç›®æ‰€äº§ç”Ÿçš„ä»»ä½•æŸå¤±æˆ–æŸå®³è´Ÿè´£ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ä½¿ç”¨è€…åœ¨ä½¿ç”¨æœ¬é¡¹ç›®çš„ä»£ç å’ŒåŠŸèƒ½æ—¶ï¼Œå¿…é¡»è‡ªè¡Œç ”ç©¶ç›¸å…³æ³•å¾‹æ³•è§„ï¼Œå¹¶ç¡®ä¿å…¶ä½¿ç”¨è¡Œä¸ºåˆæ³•åˆè§„ã€‚ä»»ä½•å› è¿åæ³•å¾‹æ³•è§„è€Œå¯¼è‡´çš„æ³•å¾‹è´£ä»»å’Œé£é™©ï¼Œå‡ç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®çš„ä½œè€…ä¸ä¼šæä¾› XHS-Downloader é¡¹ç›®çš„ä»˜è´¹ç‰ˆæœ¬ï¼Œä¹Ÿä¸ä¼šæä¾›ä¸ XHS-Downloader é¡¹ç›®ç›¸å…³çš„ä»»ä½•å•†ä¸šæœåŠ¡ã€‚&lt;/li&gt; &#xA; &lt;li&gt;åŸºäºæœ¬é¡¹ç›®è¿›è¡Œçš„ä»»ä½•äºŒæ¬¡å¼€å‘ã€ä¿®æ”¹æˆ–ç¼–è¯‘çš„ç¨‹åºä¸åŸåˆ›ä½œè€…æ— å…³ï¼ŒåŸåˆ›ä½œè€…ä¸æ‰¿æ‹…ä¸äºŒæ¬¡å¼€å‘è¡Œä¸ºæˆ–å…¶ç»“æœç›¸å…³çš„ä»»ä½•è´£ä»»ï¼Œä½¿ç”¨è€…åº”è‡ªè¡Œå¯¹å› äºŒæ¬¡å¼€å‘å¯èƒ½å¸¦æ¥çš„å„ç§æƒ…å†µè´Ÿå…¨éƒ¨è´£ä»»ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;b&gt;åœ¨ä½¿ç”¨æœ¬é¡¹ç›®çš„ä»£ç å’ŒåŠŸèƒ½ä¹‹å‰ï¼Œè¯·æ‚¨è®¤çœŸè€ƒè™‘å¹¶æ¥å—ä»¥ä¸Šå…è´£å£°æ˜ã€‚å¦‚æœæ‚¨å¯¹ä¸Šè¿°å£°æ˜æœ‰ä»»ä½•ç–‘é—®æˆ–ä¸åŒæ„ï¼Œè¯·ä¸è¦ä½¿ç”¨æœ¬é¡¹ç›®çš„ä»£ç å’ŒåŠŸèƒ½ã€‚å¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„ä»£ç å’ŒåŠŸèƒ½ï¼Œåˆ™è§†ä¸ºæ‚¨å·²å®Œå…¨ç†è§£å¹¶æ¥å—ä¸Šè¿°å…è´£å£°æ˜ï¼Œå¹¶è‡ªæ„¿æ‰¿æ‹…ä½¿ç”¨æœ¬é¡¹ç›®çš„ä¸€åˆ‡é£é™©å’Œåæœã€‚&lt;/b&gt; &#xA;&lt;h1&gt;ğŸ’¡ ä»£ç å‚è€ƒ&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.aiohttp.org/en/stable/&#34;&gt;https://docs.aiohttp.org/en/stable/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://textual.textualize.io/&#34;&gt;https://textual.textualize.io/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiosqlite.omnilib.dev/en/stable/&#34;&gt;https://aiosqlite.omnilib.dev/en/stable/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://click.palletsprojects.com/en/8.1.x/&#34;&gt;https://click.palletsprojects.com/en/8.1.x/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/borisbabic/browser_cookie3&#34;&gt;https://github.com/borisbabic/browser_cookie3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>roboflow/supervision</title>
    <updated>2024-05-12T01:42:58Z</updated>
    <id>tag:github.com,2024-05-12:/roboflow/supervision</id>
    <link href="https://github.com/roboflow/supervision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;We write your reusable computer vision tools. ğŸ’œ&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a align=&#34;center&#34; href=&#34;&#34; target=&#34;https://supervision.roboflow.com&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/notebooks&#34;&gt;notebooks&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/inference&#34;&gt;inference&lt;/a&gt; | &lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;autodistill&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/multimodal-maestro&#34;&gt;maestro&lt;/a&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/supervision.svg?sanitize=true&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/supervision&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/supervision&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/supervision&#34; alt=&#34;python-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/Annotators&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;gradio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/GbfgXGJ8Bk&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1159501506232451173&#34; alt=&#34;discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://squidfunk.github.io/mkdocs-material/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&amp;amp;logoColor=white&#34; alt=&#34;built-with-material-for-mkdocs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ğŸ‘‹ hello&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We write your reusable computer vision tools.&lt;/strong&gt; Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! ğŸ¤&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/orgs/roboflow/projects/10&#34;&gt;&lt;img src=&#34;https://github.com/roboflow/supervision/assets/26109316/c05cc954-b9a6-4ed5-9a52-d0b4b619ff65&#34; alt=&#34;supervision-hackfest&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ’» install&lt;/h2&gt; &#xA;&lt;p&gt;Pip install the supervision package in a &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install supervision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Read more about conda, mamba, and installing from source in our &lt;a href=&#34;https://roboflow.github.io/supervision/&#34;&gt;guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ”¥ quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;models&lt;/h3&gt; &#xA;&lt;p&gt;Supervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created &lt;a href=&#34;https://supervision.roboflow.com/latest/detection/core/#detections&#34;&gt;connectors&lt;/a&gt; for the most popular libraries like Ultralytics, Transformers, or MMDetection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;from ultralytics import YOLO&#xA;&#xA;image = cv2.imread(...)&#xA;model = YOLO(&#39;yolov8s.pt&#39;)&#xA;result = model(image)[0]&#xA;detections = sv.Detections.from_ultralytics(result)&#xA;&#xA;len(detections)&#xA;# 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;ğŸ‘‰ more model connectors&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;inference&lt;/p&gt; &lt;p&gt;Running with &lt;a href=&#34;https://github.com/roboflow/inference&#34;&gt;Inference&lt;/a&gt; requires a &lt;a href=&#34;https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key&#34;&gt;Roboflow API KEY&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;from inference import get_model&#xA;&#xA;image = cv2.imread(...)&#xA;model = get_model(model_id=&#34;yolov8s-640&#34;, api_key=&amp;lt;ROBOFLOW API KEY&amp;gt;)&#xA;result = model.infer(image)[0]&#xA;detections = sv.Detections.from_inference(result)&#xA;&#xA;len(detections)&#xA;# 5&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;annotators&lt;/h3&gt; &#xA;&lt;p&gt;Supervision offers a wide range of highly customizable &lt;a href=&#34;https://supervision.roboflow.com/latest/annotators/&#34;&gt;annotators&lt;/a&gt;, allowing you to compose the perfect visualization for your use case.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;&#xA;image = cv2.imread(...)&#xA;detections = sv.Detections(...)&#xA;&#xA;bounding_box_annotator = sv.BoundingBoxAnnotator()&#xA;annotated_frame = bounding_box_annotator.annotate(&#xA;    scene=image.copy(),&#xA;    detections=detections&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;datasets&lt;/h3&gt; &#xA;&lt;p&gt;Supervision provides a set of &lt;a href=&#34;https://supervision.roboflow.com/latest/datasets/&#34;&gt;utils&lt;/a&gt; that allow you to load, split, merge, and save datasets in one of the supported formats.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import supervision as sv&#xA;&#xA;dataset = sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset.classes&#xA;[&#39;dog&#39;, &#39;person&#39;]&#xA;&#xA;len(dataset)&#xA;#&amp;nbsp;1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;ğŸ‘‰ more dataset utils&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;load&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset = sv.DetectionDataset.from_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&#xA;dataset = sv.DetectionDataset.from_coco(&#xA;    images_directory_path=...,&#xA;    annotations_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;split&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_dataset, test_dataset = dataset.split(split_ratio=0.7)&#xA;test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)&#xA;&#xA;len(train_dataset), len(test_dataset), len(valid_dataset)&#xA;#&amp;nbsp;(700, 150, 150)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;merge&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds_1 = sv.DetectionDataset(...)&#xA;len(ds_1)&#xA;#&amp;nbsp;100&#xA;ds_1.classes&#xA;#&amp;nbsp;[&#39;dog&#39;, &#39;person&#39;]&#xA;&#xA;ds_2 = sv.DetectionDataset(...)&#xA;len(ds_2)&#xA;# 200&#xA;ds_2.classes&#xA;#&amp;nbsp;[&#39;cat&#39;]&#xA;&#xA;ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])&#xA;len(ds_merged)&#xA;#&amp;nbsp;300&#xA;ds_merged.classes&#xA;#&amp;nbsp;[&#39;cat&#39;, &#39;dog&#39;, &#39;person&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;save&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset.as_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset.as_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&#xA;dataset.as_coco(&#xA;    images_directory_path=...,&#xA;    annotations_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;convert&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;).as_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;ğŸ¬ tutorials&lt;/h2&gt; &#xA;&lt;p&gt;Want to learn how to use Supervision? Explore our &lt;a href=&#34;https://supervision.roboflow.com/develop/how_to/detect_and_annotate/&#34;&gt;how-to guides&lt;/a&gt;, &lt;a href=&#34;https://github.com/roboflow/supervision/tree/develop/examples&#34;&gt;end-to-end examples&lt;/a&gt;, and &lt;a href=&#34;https://supervision.roboflow.com/develop/cookbooks/&#34;&gt;cookbooks&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/hAWpsIuem10&#34; title=&#34;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/a742823d-c158-407d-b30f-063a5d11b4e1&#34; alt=&#34;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/hAWpsIuem10&#34; title=&#34;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&#34;&gt;&lt;strong&gt;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 5 Apr 2024&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt;Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios.&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/uWP6UjDeZvY&#34; title=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91&#34; alt=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/uWP6UjDeZvY&#34; title=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34;&gt;&lt;strong&gt;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 11 Jan 2024&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt;Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ’œ built with supervision&lt;/h2&gt; &#xA;&lt;p&gt;Did you build something cool using supervision? &lt;a href=&#34;https://github.com/roboflow/supervision/discussions/categories/built-with-supervision&#34;&gt;Let us know!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&#34;&gt;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“š documentation&lt;/h2&gt; &#xA;&lt;p&gt;Visit our &lt;a href=&#34;https://roboflow.github.io/supervision&#34;&gt;documentation&lt;/a&gt; page to learn how supervision can help you build computer vision applications faster and more reliably.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ† contribution&lt;/h2&gt; &#xA;&lt;p&gt;We love your input! Please see our &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started. Thank you ğŸ™ to all our contributors!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/roboflow/supervision/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=roboflow/supervision&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://youtube.com/roboflow&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.linkedin.com/company/roboflow-ai/&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://disuss.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584&#34; width=&#34;3%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;&#xA;  &lt;a href=&#34;https://blog.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;  &#xA; &lt;/div&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>