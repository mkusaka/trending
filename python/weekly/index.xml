<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-09T01:49:21Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>agno-agi/agno</title>
    <updated>2025-02-09T01:49:21Z</updated>
    <id>tag:github.com,2025-02-09:/agno-agi/agno</id>
    <link href="https://github.com/agno-agi/agno" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agno is a lightweight framework for building multi-modal Agents&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; id=&#34;top&#34;&gt; &#xA; &lt;a href=&#34;https://docs.agno.com&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;.assets/logo-dark.svg&#34;&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;.assets/logo-light.svg&#34;&gt; &#xA;   &lt;img src=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/.assets/logo-light.svg?sanitize=true&#34; alt=&#34;Agno&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://docs.agno.com&#34;&gt;📚 Documentation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &#xA; &lt;a href=&#34;https://docs.agno.com/examples/introduction&#34;&gt;💡 Examples&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &#xA; &lt;a href=&#34;https://github.com/agno-agi/agno/stargazers&#34;&gt;🌟 Star Us&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.agno.com&#34;&gt;Agno&lt;/a&gt; is a lightweight framework for building multi-modal Agents.&lt;/p&gt; &#xA;&lt;h2&gt;Simple, Fast, and Agnostic&lt;/h2&gt; &#xA;&lt;p&gt;Agno is designed with three core principles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;: No graphs, chains, or convoluted patterns — just pure python.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Uncompromising Performance&lt;/strong&gt;: Blazing fast agents with a minimal memory footprint.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Truly Agnostic&lt;/strong&gt;: Any model, any provider, any modality. Future-proof agents.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key features&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s why you should build Agents with Agno:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lightning Fast&lt;/strong&gt;: Agent creation is ~10,000x faster than LangGraph (see &lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/#performance&#34;&gt;performance&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Agnostic&lt;/strong&gt;: Use any model, any provider, no lock-in.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi Modal&lt;/strong&gt;: Native support for text, image, audio and video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi Agent&lt;/strong&gt;: Delegate tasks across a team of specialized agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory Management&lt;/strong&gt;: Store user sessions and agent state in a database.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Knowledge Stores&lt;/strong&gt;: Use vector databases for Agentic RAG or dynamic few-shot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Structured Outputs&lt;/strong&gt;: Make Agents respond with structured data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Track agent sessions and performance in real-time on &lt;a href=&#34;https://app.agno.com&#34;&gt;agno.com&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -U agno&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What are Agents?&lt;/h2&gt; &#xA;&lt;p&gt;Agents are autonomous programs that use language models to achieve tasks. They solve problems by running tools, accessing knowledge and memory to improve responses.&lt;/p&gt; &#xA;&lt;p&gt;Instead of a rigid binary definition, let&#39;s think of Agents in terms of agency and autonomy.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Level 0&lt;/strong&gt;: Agents with no tools (basic inference tasks).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Level 1&lt;/strong&gt;: Agents with tools for autonomous task execution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Level 2&lt;/strong&gt;: Agents with knowledge, combining memory and reasoning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Level 3&lt;/strong&gt;: Teams of agents collaborating on complex workflows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Example - Basic Agent&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agno.agent import Agent&#xA;from agno.models.openai import OpenAIChat&#xA;&#xA;agent = Agent(&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    description=&#34;You are an enthusiastic news reporter with a flair for storytelling!&#34;,&#xA;    markdown=True&#xA;)&#xA;agent.print_response(&#34;Tell me about a breaking news story from New York.&#34;, stream=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the agent, install dependencies and export your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install agno openai&#xA;&#xA;export OPENAI_API_KEY=sk-xxxx&#xA;&#xA;python basic_agent.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/01_basic_agent.py&#34;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example - Agent with tools&lt;/h2&gt; &#xA;&lt;p&gt;This basic agent will obviously make up a story, lets give it a tool to search the web.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agno.agent import Agent&#xA;from agno.models.openai import OpenAIChat&#xA;from agno.tools.duckduckgo import DuckDuckGoTools&#xA;&#xA;agent = Agent(&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    description=&#34;You are an enthusiastic news reporter with a flair for storytelling!&#34;,&#xA;    tools=[DuckDuckGoTools()],&#xA;    show_tool_calls=True,&#xA;    markdown=True&#xA;)&#xA;agent.print_response(&#34;Tell me about a breaking news story from New York.&#34;, stream=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies and run the Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install duckduckgo-search&#xA;&#xA;python agent_with_tools.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you should see a much more relevant result.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/02_agent_with_tools.py&#34;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example - Agent with knowledge&lt;/h2&gt; &#xA;&lt;p&gt;Agents can store knowledge in a vector database and use it for RAG or dynamic few-shot learning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Agno agents use Agentic RAG&lt;/strong&gt; by default, which means they will search their knowledge base for the specific information they need to achieve their task.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agno.agent import Agent&#xA;from agno.models.openai import OpenAIChat&#xA;from agno.embedder.openai import OpenAIEmbedder&#xA;from agno.tools.duckduckgo import DuckDuckGoTools&#xA;from agno.knowledge.pdf_url import PDFUrlKnowledgeBase&#xA;from agno.vectordb.lancedb import LanceDb, SearchType&#xA;&#xA;agent = Agent(&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    description=&#34;You are a Thai cuisine expert!&#34;,&#xA;    instructions=[&#xA;        &#34;Search your knowledge base for Thai recipes.&#34;,&#xA;        &#34;If the question is better suited for the web, search the web to fill in gaps.&#34;,&#xA;        &#34;Prefer the information in your knowledge base over the web results.&#34;&#xA;    ],&#xA;    knowledge=PDFUrlKnowledgeBase(&#xA;        urls=[&#34;https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf&#34;],&#xA;        vector_db=LanceDb(&#xA;            uri=&#34;tmp/lancedb&#34;,&#xA;            table_name=&#34;recipes&#34;,&#xA;            search_type=SearchType.hybrid,&#xA;            embedder=OpenAIEmbedder(id=&#34;text-embedding-3-small&#34;),&#xA;        ),&#xA;    ),&#xA;    tools=[DuckDuckGoTools()],&#xA;    show_tool_calls=True,&#xA;    markdown=True&#xA;)&#xA;&#xA;# Comment out after the knowledge base is loaded&#xA;if agent.knowledge is not None:&#xA;    agent.knowledge.load()&#xA;&#xA;agent.print_response(&#34;How do I make chicken and galangal in coconut milk soup&#34;, stream=True)&#xA;agent.print_response(&#34;What is the history of Thai curry?&#34;, stream=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies and run the Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install lancedb tantivy pypdf duckduckgo-search&#xA;&#xA;python agent_with_knowledge.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/03_agent_with_knowledge.py&#34;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example - Multi Agent Teams&lt;/h2&gt; &#xA;&lt;p&gt;Agents work best when they have a singular purpose, a narrow scope and a small number of tools. When the number of tools grows beyond what the language model can handle or the tools belong to different categories, use a team of agents to spread the load.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agno.agent import Agent&#xA;from agno.models.openai import OpenAIChat&#xA;from agno.tools.duckduckgo import DuckDuckGoTools&#xA;from agno.tools.yfinance import YFinanceTools&#xA;&#xA;web_agent = Agent(&#xA;    name=&#34;Web Agent&#34;,&#xA;    role=&#34;Search the web for information&#34;,&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    tools=[DuckDuckGoTools()],&#xA;    instructions=&#34;Always include sources&#34;,&#xA;    show_tool_calls=True,&#xA;    markdown=True,&#xA;)&#xA;&#xA;finance_agent = Agent(&#xA;    name=&#34;Finance Agent&#34;,&#xA;    role=&#34;Get financial data&#34;,&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],&#xA;    instructions=&#34;Use tables to display data&#34;,&#xA;    show_tool_calls=True,&#xA;    markdown=True,&#xA;)&#xA;&#xA;agent_team = Agent(&#xA;    team=[web_agent, finance_agent],&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    instructions=[&#34;Always include sources&#34;, &#34;Use tables to display data&#34;],&#xA;    show_tool_calls=True,&#xA;    markdown=True,&#xA;)&#xA;&#xA;agent_team.print_response(&#34;What&#39;s the market outlook and financial performance of AI semiconductor companies?&#34;, stream=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies and run the Agent team:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install duckduckgo-search yfinance&#xA;&#xA;python agent_team.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/05_agent_team.py&#34;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Agno is designed for high performance agentic systems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Agent instantiation: &amp;lt;5μs on average (~10,000x faster than LangGraph).&lt;/li&gt; &#xA; &lt;li&gt;Memory footprint: &amp;lt;0.01Mib on average (~50x less memory than LangGraph).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Tested on an Apple M4 Mackbook Pro.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;While an Agent&#39;s performance is bottlenecked by inference, we must do everything possible to minimize execution time, reduce memory usage, and parallelize tool calls. These numbers are may seem trivial, but they add up even at medium scale.&lt;/p&gt; &#xA;&lt;h3&gt;Instantiation time&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s measure the time it takes for an Agent with 1 tool to start up. We&#39;ll run the evaluation 1000 times to get a baseline measurement.&lt;/p&gt; &#xA;&lt;p&gt;You should run the evaluation yourself on your own machine, please, do not take these results at face value.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Setup virtual environment&#xA;./scripts/perf_setup.sh&#xA;source .venvs/perfenv/bin/activate&#xA;# OR Install dependencies manually&#xA;# pip install openai agno langgraph langchain_openai&#xA;&#xA;# Agno&#xA;python evals/performance/instantiation_with_tool.py&#xA;&#xA;# LangGraph&#xA;python evals/performance/other/langgraph_instantiation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following evaluation is run on an Apple M4 Mackbook Pro, but we&#39;ll soon be moving this to a Github actions runner for consistency.&lt;/p&gt; &#xA;&lt;p&gt;LangGraph is on the right, &lt;strong&gt;we start it first to give it a head start&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Agno is on the left, notice how it finishes before LangGraph gets 1/2 way through the runtime measurement, and hasn&#39;t even started the memory measurement. That&#39;s how fast Agno is.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23&#34;&gt;https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dividing the average time of a Langgraph Agent by the average time of an Agno Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;0.020526s / 0.000002s ~ 10,263&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this particular run, &lt;strong&gt;Agno Agent instantiation is roughly 10,000 times faster than Langgraph Agent instantiation&lt;/strong&gt;. Sure, the runtime will be dominated by inference, but these numbers add up as the number of Agents grows.&lt;/p&gt; &#xA;&lt;p&gt;The numbers continue to favor Agno as the number of tools grow, and we all memory and knowledge stores.&lt;/p&gt; &#xA;&lt;h3&gt;Memory usage&lt;/h3&gt; &#xA;&lt;p&gt;To measure memory usage, we use the &lt;code&gt;tracemalloc&lt;/code&gt; library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.&lt;/p&gt; &#xA;&lt;p&gt;We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we&#39;ve made a mistake, please let us know.&lt;/p&gt; &#xA;&lt;p&gt;Dividing the average memory usage of a Langgraph Agent by the average memory usage of an Agno Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;0.137273/0.002528 ~ 54.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Langgraph Agents use ~50x more memory than Agno Agents&lt;/strong&gt;. In our opinion, memory usage is a much more important metric than instantiation time. As we start running thousands of Agents in production, these numbers directly start affecting the cost of running the Agents.&lt;/p&gt; &#xA;&lt;h3&gt;Conclusion&lt;/h3&gt; &#xA;&lt;p&gt;Agno agents are designed for high-performance and while we do share some benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll be publishing accuracy and reliability benchmarks running on Github actions in the coming weeks. Given that each framework is different and we won&#39;t be able to tune their performance like we do with Agno, for future benchmarks we&#39;ll only be comparing against ourselves.&lt;/p&gt; &#xA;&lt;h2&gt;Cursor Setup&lt;/h2&gt; &#xA;&lt;p&gt;When building Agno agents, using the Agno docs as a documentation source in Cursor is a great way to speed up your development.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In Cursor, go to the settings or preferences section.&lt;/li&gt; &#xA; &lt;li&gt;Find the section to manage documentation sources.&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;https://docs.agno.com&lt;/code&gt; to the list of documentation URLs.&lt;/li&gt; &#xA; &lt;li&gt;Save the changes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Now, Cursor will have access to the Agno documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation, Community &amp;amp; More examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docs: &lt;a href=&#34;https://docs.agno.com&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;docs.agno.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Getting Started Examples: &lt;a href=&#34;https://github.com/agno-agi/agno/tree/main/cookbook/getting_started&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Getting Started Cookbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;All Examples: &lt;a href=&#34;https://github.com/agno-agi/agno/tree/main/cookbook&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Cookbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Community forum: &lt;a href=&#34;https://community.agno.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;community.agno.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat: &lt;a href=&#34;https://discord.gg/4MtYHHrgA8&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions, read our &lt;a href=&#34;https://github.com/agno-agi/agno/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;h2&gt;Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;Agno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting &lt;code&gt;AGNO_TELEMETRY=false&lt;/code&gt; in your environment.&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/#top&#34;&gt;⬆️ Back to Top&lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>assafelovic/gpt-researcher</title>
    <updated>2025-02-09T01:49:21Z</updated>
    <id>tag:github.com,2025-02-09:/assafelovic/gpt-researcher</id>
    <link href="https://github.com/assafelovic/gpt-researcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM based autonomous agent that conducts deep local and web research on any topic and generates a long report with citations.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; id=&#34;top&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3&#34; alt=&#34;Logo&#34; width=&#34;80&#34;&gt; &#xA; &lt;h4&gt;&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gptr.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&amp;amp;logo=world&amp;amp;logoColor=white&amp;amp;color=0891b2&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.gptr.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/QgZXvJAccX&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&amp;amp;theme=clean-inverted&amp;amp;?compact=true&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/gpt-researcher&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&amp;amp;logoColor=white&amp;amp;style=flat&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&amp;amp;logo=github&#34; alt=&#34;GitHub Release&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?message=Open%20in%20Colab&amp;amp;logo=googlecolab&amp;amp;labelColor=grey&amp;amp;color=yellow&amp;amp;label=%20&amp;amp;style=flat&amp;amp;logoSize=40&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/gptresearcher/gpt-researcher&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&amp;amp;style=flat&amp;amp;logo=docker&amp;amp;logoColor=white&amp;amp;color=1D63ED&#34; alt=&#34;Docker Image Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/assaf_elovic&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/assaf_elovic?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-zh_CN.md&#34;&gt;中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-ja_JP.md&#34;&gt;日本語&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-ko_KR.md&#34;&gt;한국어&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;🔎 GPT Researcher&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPT Researcher is an autonomous agent designed for comprehensive web and local research on any given task.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The agent produces detailed, factual, and unbiased research reports with citations. GPT Researcher provides a full suite of customization options to create tailor made and domain specific research agents. Inspired by the recent &lt;a href=&#34;https://arxiv.org/abs/2305.04091&#34;&gt;Plan-and-Solve&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34;&gt;RAG&lt;/a&gt; papers, GPT Researcher addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our mission is to empower individuals and organizations with accurate, unbiased, and factual information through AI.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why GPT Researcher?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Objective conclusions for manual research can take weeks, requiring vast resources and time.&lt;/li&gt; &#xA; &lt;li&gt;LLMs trained on outdated information can hallucinate, becoming irrelevant for current research tasks.&lt;/li&gt; &#xA; &lt;li&gt;Current LLMs have token limitations, insufficient for generating long research reports.&lt;/li&gt; &#xA; &lt;li&gt;Limited web sources in existing services lead to misinformation and shallow results.&lt;/li&gt; &#xA; &lt;li&gt;Selective web sources can introduce bias into research tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/2cc38f6a-9f66-4644-9e69-a46c40e296d4&#34;&gt;https://github.com/user-attachments/assets/2cc38f6a-9f66-4644-9e69-a46c40e296d4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The core idea is to utilize &#39;planner&#39; and &#39;execution&#39; agents. The planner generates research questions, while the execution agents gather relevant information. The publisher then aggregates all findings into a comprehensive report.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; height=&#34;600&#34; src=&#34;https://github.com/assafelovic/gpt-researcher/assets/13554167/4ac896fd-63ab-4b77-9688-ff62aafcc527&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a task-specific agent based on a research query.&lt;/li&gt; &#xA; &lt;li&gt;Generate questions that collectively form an objective opinion on the task.&lt;/li&gt; &#xA; &lt;li&gt;Use a crawler agent for gathering information for each question.&lt;/li&gt; &#xA; &lt;li&gt;Summarize and source-track each resource.&lt;/li&gt; &#xA; &lt;li&gt;Filter and aggregate summaries into a final research report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.gptr.dev/blog/building-gpt-researcher&#34;&gt;How it Works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea&#34;&gt;How to Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8&#34;&gt;Live Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📝 Generate detailed research reports using web and local documents.&lt;/li&gt; &#xA; &lt;li&gt;🖼️ Smart image scraping and filtering for reports.&lt;/li&gt; &#xA; &lt;li&gt;📜 Generate detailed reports exceeding 2,000 words.&lt;/li&gt; &#xA; &lt;li&gt;🌐 Aggregate over 20 sources for objective conclusions.&lt;/li&gt; &#xA; &lt;li&gt;🖥️ Frontend available in lightweight (HTML/CSS/JS) and production-ready (NextJS + Tailwind) versions.&lt;/li&gt; &#xA; &lt;li&gt;🔍 JavaScript-enabled web scraping.&lt;/li&gt; &#xA; &lt;li&gt;📂 Maintains memory and context throughout research.&lt;/li&gt; &#xA; &lt;li&gt;📄 Export reports to PDF, Word, and other formats.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📖 Documentation&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started&#34;&gt;Documentation&lt;/a&gt; for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Installation and setup guides&lt;/li&gt; &#xA; &lt;li&gt;Configuration and customization options&lt;/li&gt; &#xA; &lt;li&gt;How-To examples&lt;/li&gt; &#xA; &lt;li&gt;Full API references&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⚙️ Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Python 3.11 or later. &lt;a href=&#34;https://www.tutorialsteacher.com/python/install-python&#34;&gt;Guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the project and navigate to the directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/assafelovic/gpt-researcher.git&#xA;cd gpt-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up API keys by exporting them or storing them in a &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY={Your OpenAI API Key here}&#xA;export TAVILY_API_KEY={Your Tavily API Key here}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies and start the server:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;python -m uvicorn main:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt; to start.&lt;/p&gt; &#xA;&lt;p&gt;For other setups (e.g., Poetry or virtual environments), check the &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started&#34;&gt;Getting Started page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run as PIP package&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gpt-researcher&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Usage:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;...&#xA;from gpt_researcher import GPTResearcher&#xA;&#xA;query = &#34;why is Nvidia stock going up?&#34;&#xA;researcher = GPTResearcher(query=query, report_type=&#34;research_report&#34;)&#xA;# Conduct research on the given query&#xA;research_result = await researcher.conduct_research()&#xA;# Write the report&#xA;report = await researcher.write_report()&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;For more examples and configurations, please refer to the &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package&#34;&gt;PIP documentation&lt;/a&gt; page.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Run with Docker&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker&#34;&gt;Install Docker&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Clone the &#39;.env.example&#39; file, add your API Keys to the cloned file and save the file as &#39;.env&#39;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Within the docker-compose file comment out services that you don&#39;t want to run with Docker.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If that doesn&#39;t work, try running it without the dash:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - By default, if you haven&#39;t uncommented anything in your docker-compose file, this flow will start 2 processes:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the Python server running on localhost:8000&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;the React app running on localhost:3000&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Visit localhost:3000 on any browser and enjoy researching!&lt;/p&gt; &#xA;&lt;h2&gt;📄 Research on Local Documents&lt;/h2&gt; &#xA;&lt;p&gt;You can instruct the GPT Researcher to run research tasks based on your local documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.&lt;/p&gt; &#xA;&lt;p&gt;Step 1: Add the env variable &lt;code&gt;DOC_PATH&lt;/code&gt; pointing to the folder where your documents are located.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export DOC_PATH=&#34;./my-docs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step 2:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you&#39;re running the frontend app on localhost:8000, simply select &#34;My Documents&#34; from the &#34;Report Source&#34; Dropdown Options.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re running GPT Researcher with the &lt;a href=&#34;https://docs.tavily.com/guides/gpt-researcher/gpt-researcher#pip-package&#34;&gt;PIP package&lt;/a&gt;, pass the &lt;code&gt;report_source&lt;/code&gt; argument as &#34;local&#34; when you instantiate the &lt;code&gt;GPTResearcher&lt;/code&gt; class &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/context/tailored-research&#34;&gt;code sample here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👪 Multi-Agent Assistant&lt;/h2&gt; &#xA;&lt;p&gt;As AI evolves from prompt engineering and RAG to multi-agent systems, we&#39;re excited to introduce our new multi-agent assistant built with &lt;a href=&#34;https://python.langchain.com/v0.1/docs/langgraph/&#34;&gt;LangGraph&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By using LangGraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. Inspired by the recent &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;STORM&lt;/a&gt; paper, this project showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication.&lt;/p&gt; &#xA;&lt;p&gt;An average run generates a 5-6 page research report in multiple formats such as PDF, Docx and Markdown.&lt;/p&gt; &#xA;&lt;p&gt;Check it out &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents&#34;&gt;here&lt;/a&gt; or head over to our &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph&#34;&gt;documentation&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;🖥️ Frontend Applications&lt;/h2&gt; &#xA;&lt;p&gt;GPT-Researcher now features an enhanced frontend to improve the user experience and streamline the research process. The frontend offers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An intuitive interface for inputting research queries&lt;/li&gt; &#xA; &lt;li&gt;Real-time progress tracking of research tasks&lt;/li&gt; &#xA; &lt;li&gt;Interactive display of research findings&lt;/li&gt; &#xA; &lt;li&gt;Customizable settings for tailored research experiences&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Two deployment options are available:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A lightweight static frontend served by FastAPI&lt;/li&gt; &#xA; &lt;li&gt;A feature-rich NextJS application for advanced functionality&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For detailed setup instructions and more information about the frontend features, please visit our &lt;a href=&#34;https://docs.gptr.dev/docs/gpt-researcher/frontend/introduction&#34;&gt;documentation page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We highly welcome contributions! Please check out &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/raw/master/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; if you&#39;re interested.&lt;/p&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://trello.com/b/3O7KBePw/gpt-researcher-roadmap&#34;&gt;roadmap&lt;/a&gt; page and reach out to us via our &lt;a href=&#34;https://discord.gg/QgZXvJAccX&#34;&gt;Discord community&lt;/a&gt; if you&#39;re interested in joining our mission. &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=assafelovic/gpt-researcher&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;✉️ Support / Contact us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/spBgZmm3Xe&#34;&gt;Community Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Author Email: &lt;a href=&#34;mailto:assaf.elovic@gmail.com&#34;&gt;assaf.elovic@gmail.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛡 Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project, GPT Researcher, is an experimental application and is provided &#34;as-is&#34; without any warranty, express or implied. We are sharing codes for academic purposes under the Apache 2 license. Nothing herein is academic advice, and NOT a recommendation to use in academic or research papers.&lt;/p&gt; &#xA;&lt;p&gt;Our view on unbiased research claims:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The main goal of GPT Researcher is to reduce incorrect and biased facts. How? We assume that the more sites we scrape the less chances of incorrect data. By scraping multiple sites per research, and choosing the most frequent information, the chances that they are all wrong is extremely low.&lt;/li&gt; &#xA; &lt;li&gt;We do not aim to eliminate biases; we aim to reduce it as much as possible. &lt;strong&gt;We are here as a community to figure out the most effective human/llm interactions.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;In research, people also tend towards biases as most have already opinions on the topics they research about. This tool scrapes many opinions and will evenly explain diverse views that a biased person would never have read.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://star-history.com/#assafelovic/gpt-researcher&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&#34;&gt; &#xA;   &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/#top&#34;&gt;⬆️ Back to Top&lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AmberSahdev/Open-Interface</title>
    <updated>2025-02-09T01:49:21Z</updated>
    <id>tag:github.com,2025-02-09:/AmberSahdev/Open-Interface</id>
    <link href="https://github.com/AmberSahdev/Open-Interface" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Control Any Computer Using LLMs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Interface&lt;/h1&gt; &#xA;&lt;picture&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/icon.png&#34; align=&#34;right&#34; alt=&#34;Open Interface Logo&#34; width=&#34;120&#34; height=&#34;120&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h3&gt;Control Your Computer Using LLMs&lt;/h3&gt; &#xA;&lt;p&gt;Open Interface&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Self-drives your computer by sending your requests to an LLM backend (GPT-4o, etc) to figure out the required steps.&lt;/li&gt; &#xA; &lt;li&gt;Automatically executes these steps by simulating keyboard and mouse input.&lt;/li&gt; &#xA; &lt;li&gt;Course-corrects by sending the LLM backend updated screenshots of the progress as needed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt;Full Autopilot for All Computers Using LLMs&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#install&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;amp;logo=apple&amp;amp;logoColor=white&#34; alt=&#34;macOS&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#install&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=black&#34; alt=&#34;Linux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#install&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;amp;logo=windows&amp;amp;logoColor=white&#34; alt=&#34;Windows&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;(https://github.com/AmberSahdev/Open-Interface/releases/latest)&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/AmberSahdev/Open-Interface/total.svg?sanitize=true&#34; alt=&#34;Github All Releases&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/languages/code-size/AmberSahdev/Open-Interface&#34; alt=&#34;GitHub code size in bytes&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/AmberSahdev/Open-Interface&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/AmberSahdev/Open-Interface&#34; alt=&#34;GitHub&#34;&gt; &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/AmberSahdev/Open-Interface&#34; alt=&#34;GitHub Latest Release)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Demo&lt;/ins&gt; 💻&lt;/h3&gt; &#xA;&lt;p&gt;&#34;Solve Today&#39;s Wordle&#34;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/wordle_demo_2x.gif&#34; alt=&#34;Solve Today&#39;s Wordle&#34;&gt;&lt;br&gt; &lt;em&gt;clipped, 2x&lt;/em&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/raw/main/MEDIA.md#demos&#34;&gt;More Demos&lt;/a&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &#34;Make me a meal plan in Google Docs&#34; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/meal_plan_demo_2x.gif&#34; style=&#34;margin: 5px; border-radius: 10px;&#34;&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &#34;Write a Web App&#34; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/code_web_app_demo_2x.gif&#34; style=&#34;margin: 5px; border-radius: 10px;&#34;&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Install&lt;/ins&gt; 💽&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Apple_Computer_Logo_rainbow.svg/640px-Apple_Computer_Logo_rainbow.svg.png&#34; alt=&#34;MacOS Logo&#34; width=&#34;13&#34; height=&#34;15&#34;&gt; &lt;b&gt;MacOS&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Download the MacOS binary from the latest &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/releases/latest&#34;&gt;release&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Unzip the file and move Open Interface to the Applications Folder.&lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_unzip_move_to_applications.png&#34; width=&#34;350&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;b&gt;Apple Silicon M-Series Macs&lt;/b&gt;&lt;/summary&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; Open Interface will ask you for Accessibility access to operate your keyboard and mouse for you, and Screen Recording access to take screenshots to assess its progress.&lt;br&gt; &lt;/li&gt; &#xA;   &lt;li&gt; In case it doesn&#39;t, manually add these permission via &lt;b&gt;System Settings&lt;/b&gt; -&amp;gt; &lt;b&gt;Privacy and Security&lt;/b&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/mac_m3_accessibility.png&#34; width=&#34;400&#34; style=&#34;margin: 5px; border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/mac_m3_screenrecording.png&#34; width=&#34;400&#34; style=&#34;margin: 5px; border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;b&gt;Intel Macs&lt;/b&gt;&lt;/summary&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; Launch the app from the Applications folder.&lt;br&gt; You might face the standard Mac &lt;i&gt;&#34;Open Interface cannot be opened&#34; error&lt;/i&gt;.&lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_unverified_developer.png&#34; width=&#34;200&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt;&lt;br&gt; In that case, press &lt;b&gt;&lt;i&gt;&lt;ins&gt;&#34;Cancel&#34;&lt;/ins&gt;&lt;/i&gt;&lt;/b&gt;.&lt;br&gt; Then go to &lt;b&gt;System Preferences -&amp;gt; Security and Privacy -&amp;gt; Open Anyway.&lt;/b&gt;&lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_system_preferences.png&#34; width=&#34;100&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_security.png&#34; width=&#34;100&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_open_anyway.png&#34; width=&#34;400&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &lt;/li&gt; &#xA;   &lt;br&gt; &#xA;   &lt;li&gt; Open Interface will also need Accessibility access to operate your keyboard and mouse for you, and Screen Recording access to take screenshots to assess its progress.&lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_accessibility.png&#34; width=&#34;400&#34; style=&#34;margin: 5px; border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_screen_recording.png&#34; width=&#34;400&#34; style=&#34;margin: 5px; border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/details&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Lastly, checkout the &lt;a href=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/#setup&#34;&gt;Setup&lt;/a&gt; section to connect Open Interface to LLMs (OpenAI GPT-4V)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/TuxFlat.svg/640px-TuxFlat.svg.png&#34; alt=&#34;Linux Logo&#34; width=&#34;15&#34; height=&#34;15&#34;&gt; &lt;b&gt;Linux&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linux binary has been tested on Ubuntu 20.04 so far.&lt;/li&gt; &#xA;  &lt;li&gt;Download the Linux zip file from the latest &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/releases/latest&#34;&gt;release&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt; Extract the executable and checkout the &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#setup&#34;&gt;Setup&lt;/a&gt; section to connect Open Interface to LLMs, such as OpenAI GPT-4V.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/5/5f/Windows_logo_-_2012.svg?sanitize=true&#34; alt=&#34;Linux Logo&#34; width=&#34;15&#34; height=&#34;15&#34;&gt; &lt;b&gt;Windows&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Windows binary has been tested on Windows 10.&lt;/li&gt; &#xA;  &lt;li&gt;Download the Windows zip file from the latest &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/releases/latest&#34;&gt;release&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Unzip the folder, move the exe to the desired location, double click to open, and voila.&lt;/li&gt; &#xA;  &lt;li&gt;Checkout the &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#setup&#34;&gt;Setup&lt;/a&gt; section to connect Open Interface to LLMs (OpenAI GPT-4V)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/1869px-Python-logo-notext.svg.png&#34; alt=&#34;Python Logo&#34; width=&#34;15&#34; height=&#34;15&#34;&gt; &lt;b&gt;Run as a Script&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Clone the repo &lt;code&gt;git clone https://github.com/AmberSahdev/Open-Interface.git&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Enter the directory &lt;code&gt;cd Open-Interface&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;b&gt;Optionally&lt;/b&gt; use a Python virtual environment &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Note: pyenv handles tkinter installation weirdly so you may have to debug for your own system yourself.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;pyenv local 3.12.2&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;python -m venv .venv&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;source .venv/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Install dependencies &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Run the app using &lt;code&gt;python app/app.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;&lt;ins id=&#34;setup&#34;&gt;Setup&lt;/ins&gt; 🛠️&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Set up the OpenAI API key&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Get your OpenAI API key&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Open Interface needs access to GPT-4o to perform user requests. GPT-4o keys can be downloaded from your OpenAI account at &lt;a href=&#34;https://platform.openai.com/settings/organization/api-keys&#34;&gt;platform.openai.com/settings/organization/api-keys&lt;/a&gt;.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://help.openai.com/en/articles/8264644-what-is-prepaid-billing&#34;&gt;Follow the steps here&lt;/a&gt; to add balance to your OpenAI account. To unlock GPT-4o a minimum payment of $5 is needed.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4&#34;&gt;More info&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Save the API key in Open Interface settings&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;In Open Interface, go to the Settings menu on the top right and enter the key you received from OpenAI into the text field like so: &lt;br&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;br&gt; &#xA;   &lt;picture&gt; &#xA;    &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/set_openai_api_key.png&#34; align=&#34;middle&#34; alt=&#34;Set API key in settings&#34; width=&#34;400&#34;&gt; &#xA;   &lt;/picture&gt;&lt;br&gt; &lt;br&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;After setting the API key for the first time you&#39;ll need to &lt;b&gt;restart the app&lt;/b&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Optional: Setup a Custom LLM&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Open Interface supports using other OpenAI API style LLMs (such as Llava) as a backend and can be configured easily in the Advanced Settings window.&lt;/li&gt; &#xA;  &lt;li&gt;Enter the custom base url and model name in the Advanced Settings window and the API key in the Settings window as needed.&lt;/li&gt; &#xA;  &lt;li&gt;You may need to enter a random string like &#34;xxx&#34; in the API key input box. &lt;br&gt; &#xA;   &lt;picture&gt; &#xA;    &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/advanced_settings.png&#34; align=&#34;middle&#34; alt=&#34;Set API key in settings&#34; width=&#34;400&#34;&gt; &#xA;   &lt;/picture&gt;&lt;br&gt; &lt;br&gt;&lt;/li&gt; &#xA;  &lt;li&gt;If your LLM does not support an OpenAI style API, you can use a library like &lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;this&lt;/a&gt; to convert it to one.&lt;/li&gt; &#xA;  &lt;li&gt;You will need to restart the app after these changes.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Stuff It’s Error-Prone At, For Now&lt;/ins&gt; 😬&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accurate spatial-reasoning and hence clicking buttons.&lt;/li&gt; &#xA; &lt;li&gt;Keeping track of itself in tabular contexts, like Excel and Google Sheets, for similar reasons as stated above.&lt;/li&gt; &#xA; &lt;li&gt;Navigating complex GUI-rich applications like Counter-Strike, Spotify, Garage Band, etc due to heavy reliance on cursor actions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;ins&gt;The Future&lt;/ins&gt; 🔮&lt;/h3&gt; &#xA;&lt;p&gt;(&lt;em&gt;with better models trained on video walkthroughs like Youtube tutorials&lt;/em&gt;)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Create a couple of bass samples for me in Garage Band for my latest project.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Read this design document for a new feature, edit the code on Github, and submit it for review.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Find my friends&#39; music taste from Spotify and create a party playlist for tonight&#39;s event.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Take the pictures from my Tahoe trip and make a White Lotus type montage in iMovie.&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Notes&lt;/ins&gt; 📝&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cost Estimation: $0.0005 - $0.002 per LLM request depending on the model used.&lt;br&gt; (User requests can require between two to a few dozen LLM backend calls depending on the request&#39;s complexity.)&lt;/li&gt; &#xA; &lt;li&gt;You can interrupt the app anytime by pressing the Stop button, or by dragging your cursor to any of the screen corners.&lt;/li&gt; &#xA; &lt;li&gt;Open Interface can only see your primary display when using multiple monitors. Therefore, if the cursor/focus is on a secondary screen, it might keep retrying the same actions as it is unable to see its progress.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;ins&gt;System Diagram&lt;/ins&gt; 🖼️&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;+----------------------------------------------------+&#xA;| App                                                |&#xA;|                                                    |&#xA;|    +-------+                                       |&#xA;|    |  GUI  |                                       |&#xA;|    +-------+                                       |&#xA;|        ^                                           |&#xA;|        |                                           |&#xA;|        v                                           |&#xA;|  +-----------+  (Screenshot + Goal)  +-----------+ |&#xA;|  |           | --------------------&amp;gt; |           | |&#xA;|  |    Core   |                       |    LLM    | |&#xA;|  |           | &amp;lt;-------------------- |  (GPT-4o) | |&#xA;|  +-----------+    (Instructions)     +-----------+ |&#xA;|        |                                           |&#xA;|        v                                           |&#xA;|  +-------------+                                   |&#xA;|  | Interpreter |                                   |&#xA;|  +-------------+                                   |&#xA;|        |                                           |&#xA;|        v                                           |&#xA;|  +-------------+                                   |&#xA;|  |   Executer  |                                   |&#xA;|  +-------------+                                   |&#xA;+----------------------------------------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Star History&lt;/ins&gt; ⭐️&lt;/h3&gt; &#xA;&lt;picture&gt; &#xA; &lt;img src=&#34;https://api.star-history.com/svg?repos=AmberSahdev/Open-Interface&amp;amp;type=Date&#34; alt=&#34;Star History&#34; width=&#34;720&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Links&lt;/ins&gt; 🔗&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out more of my projects at &lt;a href=&#34;https://AmberSah.dev&#34;&gt;AmberSah.dev&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Other demos and press kit can be found at &lt;a href=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/MEDIA.md&#34;&gt;MEDIA.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/AmberSahdev/Open-Interface&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/AmberSahdev&#34;&gt; &lt;img alt=&#34;GitHub followers&#34; src=&#34;https://img.shields.io/github/followers/AmberSahdev&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>