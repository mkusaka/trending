<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T02:00:37Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tatsu-lab/stanford_alpaca</title>
    <updated>2023-03-26T02:00:37Z</updated>
    <id>tag:github.com,2023-03-26:/tatsu-lab/stanford_alpaca</id>
    <link href="https://github.com/tatsu-lab/stanford_alpaca" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code and documentation to train Stanford&#39;s Alpaca models, and generate the data.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a href=&#34;https://crfm.stanford.edu/alpaca/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/assets/logo.png&#34; alt=&#34;Stanford-Alpaca&#34; style=&#34;width: 50%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Stanford Alpaca: An Instruction-following LLaMA Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/#data-release&#34;&gt;52K data&lt;/a&gt; used for fine-tuning the model.&lt;/li&gt; &#xA; &lt;li&gt;The code for &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/#data-generation-process&#34;&gt;generating the data&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The code for &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/#fine-tuning&#34;&gt;fine-tuning the model&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: We thank the community for feedback on Stanford-Alpaca and supporting our research. Our live demo is suspended until further notice.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage and License Notices&lt;/strong&gt;: Alpaca is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The current Alpaca model is fine-tuned from a 7B LLaMA model [1] on 52K instruction-following data generated by the techniques in the Self-Instruct [2] paper, with some modifications that we discuss in the next section. In a preliminary human evaluation, we found that the Alpaca 7B model behaves similarly to the &lt;code&gt;text-davinci-003&lt;/code&gt; model on the Self-Instruct instruction-following evaluation suite [2].&lt;/p&gt; &#xA;&lt;p&gt;Alpaca is still under development, and there are many limitations that have to be addressed. Importantly, we have not yet fine-tuned the Alpaca model to be safe and harmless. We thus encourage users to be cautious when interacting with Alpaca, and to report any concerning behavior to help improve the safety and ethical considerations of the model.&lt;/p&gt; &#xA;&lt;p&gt;Our initial release contains the data generation procedure, dataset, and training recipe. We intend to release the model weights if we are given permission to do so by the creators of LLaMA. For now, we have chosen to host a live demo to help readers better understand the capabilities and limits of Alpaca, as well as a way to help us better evaluate Alpaca&#39;s performance on a broader audience.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please read our release &lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34;&gt;blog post&lt;/a&gt; for more details about the model, our discussion of the potential harm and limitations of Alpaca models, and our thought process for releasing a reproducible model.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[1]: LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. &lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;https://arxiv.org/abs/2302.13971v1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2]: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;https://arxiv.org/abs/2212.10560&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Data Release&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json&#34;&gt;&lt;code&gt;alpaca_data.json&lt;/code&gt;&lt;/a&gt; contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;instruction&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, describes the task the model should perform. Each of the 52K instructions is unique.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;input&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, optional context or input for the task. For example, when the instruction is &#34;Summarize the following article&#34;, the input is the article. Around 40% of the examples have an input.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, the answer to the instruction as generated by &lt;code&gt;text-davinci-003&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We used the following prompts for fine-tuning the Alpaca model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with a non-empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Input:&#xA;{input}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with an empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During inference (eg for the web demo), we use the user instruction with an empty input field (second option).&lt;/p&gt; &#xA;&lt;h2&gt;Data Generation Process&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;strong&gt; Running the code &lt;/strong&gt; &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set environment variables &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to your OpenAI API key.&lt;/li&gt; &#xA;  &lt;li&gt;Install the dependencies with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;python -m generate_instruction generate_instruction_following_data&lt;/code&gt; to generate the data.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;We built on the data generation pipeline from &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct&lt;/a&gt; and made the following modifications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We used &lt;code&gt;text-davinci-003&lt;/code&gt; to generate the instruction data instead of &lt;code&gt;davinci&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We wrote a new prompt (&lt;code&gt;prompt.txt&lt;/code&gt;) that explicitly gave the requirement of instruction generation to &lt;code&gt;text-davinci-003&lt;/code&gt;. Note: there is a slight error in the prompt we used, and future users should incorporate the edit in &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/pull/24&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca/pull/24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;We adopted much more aggressive batch decoding, i.e., generating 20 instructions at once, which significantly reduced the cost of data generation.&lt;/li&gt; &#xA; &lt;li&gt;We simplified the data generation pipeline by discarding the difference between classification and non-classification instructions.&lt;/li&gt; &#xA; &lt;li&gt;We only generated a single instance for each instruction, instead of 2 to 3 instances as in [1].&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This produced an instruction-following dataset with 52K examples obtained at a much lower cost (less than $500). In a preliminary study, we also find our 52K generated data to be much more diverse than the data released by &lt;a href=&#34;https://github.com/yizhongw/self-instruct/raw/main/data/seed_tasks.jsonl&#34;&gt;self-instruct&lt;/a&gt;. We plot the below figure (in the style of Figure 2 in the &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;self-instruct paper&lt;/a&gt; to demonstrate the diversity of our data. The inner circle of the plot represents the root verb of the instructions, and the outer circle represents the direct objects.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/assets/parse_analysis.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/assets/parse_analysis.png&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;We fine-tune our models using standard Hugging Face training code with the following hyperparameters:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Batch size&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Learning rate&lt;/td&gt; &#xA;   &lt;td&gt;2e-5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Epochs&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Max length&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Weight decay&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Given Hugging Face hasn&#39;t officially supported the LLaMA models, we fine-tuned LLaMA with Hugging Face&#39;s transformers library by installing it from a particular fork (i.e. this &lt;a href=&#34;https://github.com/huggingface/transformers/pull/21955&#34;&gt;PR&lt;/a&gt; to be merged). The hash of the specific commit we installed was &lt;code&gt;68d640f7c368bcaaaecfc678f11908ebbd3d6176&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To reproduce our fine-tuning runs for LLaMA, first install the requirements&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install the particular fork of Hugging Face&#39;s transformers library.&lt;/p&gt; &#xA;&lt;p&gt;Below is a command that fine-tunes LLaMA-7B with our dataset on a machine with 4 A100 80G GPUs in FSDP &lt;code&gt;full_shard&lt;/code&gt; mode. We were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using &lt;strong&gt;Python 3.10&lt;/strong&gt;. Replace &lt;code&gt;&amp;lt;your_random_port&amp;gt;&lt;/code&gt; with a port of your own, &lt;code&gt;&amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt;&lt;/code&gt; with the path to your converted checkpoint and tokenizer (following instructions in the PR), and &lt;code&gt;&amp;lt;your_output_dir&amp;gt;&lt;/code&gt; with where you want to store your outputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=4 --master_port=&amp;lt;your_random_port&amp;gt; train.py \&#xA;    --model_name_or_path &amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt; \&#xA;    --data_path ./alpaca_data.json \&#xA;    --bf16 True \&#xA;    --output_dir &amp;lt;your_output_dir&amp;gt; \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2000 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LLaMADecoderLayer&#39; \&#xA;    --tf32 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Warning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt; must be set to the name of the specific decoder layer. The LLaMA Hugging Face PR is not stable. Earlier commits used the name &lt;code&gt;LLaMADecoderLayer&lt;/code&gt; for their decoder layer (the commit hash our code is based on this). More recent commits use &lt;code&gt;LlamaDecoderLayer&lt;/code&gt; (notice the small case difference). Not setting &lt;code&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt; to the correct name will lead to drastic slowdowns in training.&lt;/p&gt; &#xA;&lt;h3&gt;Side notes&lt;/h3&gt; &#xA;&lt;p&gt;The same script also works for OPT fine-tuning. Here&#39;s an example for fine-tuning OPT-6.7B&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=4 --master_port=&amp;lt;your_random_port&amp;gt; train.py \&#xA;    --model_name_or_path &#34;facebook/opt-6.7b&#34; \&#xA;    --data_path ./alpaca_data.json \&#xA;    --bf16 True \&#xA;    --output_dir &amp;lt;your_output_dir&amp;gt; \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2000 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;OPTDecoderLayer&#39; \&#xA;    --tf32 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the given training script is meant to be simple and easy to use, and is not particularly optimized. To run on more gpus, you may prefer to turn down &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; to keep a global batch size of 128. Global batch size has not been tested for optimality.&lt;/p&gt; &#xA;&lt;h3&gt;Authors&lt;/h3&gt; &#xA;&lt;p&gt;All grad students below contributed equally and the order is determined by random draw.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rohantaori.com/&#34;&gt;Rohan Taori&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ishaan.io/&#34;&gt;Ishaan Gulrajani&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tiiiger.github.io/&#34;&gt;Tianyi Zhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yanndubs.github.io/&#34;&gt;Yann Dubois&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lxuechen.com/&#34;&gt;Xuechen Li&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All advised by &lt;a href=&#34;https://thashim.github.io/&#34;&gt;Tatsunori B. Hashimoto&lt;/a&gt;. Yann is also advised by &lt;a href=&#34;https://cs.stanford.edu/~pliang/&#34;&gt;Percy Liang&lt;/a&gt; and Xuechen is also advised by &lt;a href=&#34;https://guestrin.su.domains/&#34;&gt;Carlos Guestrin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;Please cite the repo if you use the data or code in this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{alpaca,&#xA;  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },&#xA;  title = {Stanford Alpaca: An Instruction-following LLaMA model},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Naturally, you should also cite the original LLaMA paper [1] and the Self-Instruct paper [2].&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;p&gt;We thank Yizhong Wang for his help in explaining the data generation pipeline in Self-Instruct and providing the code for the parse analysis plot. We thank Yifan Mai for helpful support, and members of the Stanford NLP Group as well as the Center for Research on Foundation Models (CRFM) for their helpful feedback.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yizhongw/self-instruct</title>
    <updated>2023-03-26T02:00:37Z</updated>
    <id>tag:github.com,2023-03-26:/yizhongw/self-instruct</id>
    <link href="https://github.com/yizhongw/self-instruct" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Aligning pretrained language models with instruction data generated by themselves.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Self-Instruct: Aligning LM with Self Generated Instructions&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code and data for the &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;Self-Instruct paper&lt;/a&gt;, a method for aligning pretrained language models with instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Self-Instruct is a framework that helps language models improve their ability to follow natural language instructions. It does this by using the model&#39;s own generations to create a large collection of instructional data. With Self-Instruct, it is possible to improve the instruction-following capabilities of language models without relying on extensive manual annotation.&lt;/p&gt; &#xA;&lt;h3&gt;Background&lt;/h3&gt; &#xA;&lt;p&gt;In recent years, there has been a growing interest in building models that can follow natural language instructions to perform a wide range of tasks. These models, known as &#34;instruction-tuned&#34; language models, have demonstrated the ability to generalize to new tasks. However, their performance is heavily dependent on the quality and quantity of the human-written instruction data used to train them, which can be limited in diversity and creativity. To overcome these limitations, it is important to develop alternative approaches for supervising instruction-tuned models and improving their instruction-following capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;How Self-Instruct works?&lt;/h3&gt; &#xA;&lt;p&gt;The Self-Instruct process is an iterative bootstrapping algorithm that starts with a seed set of manually-written instructions and uses them to prompt the language model to generate new instructions and corresponding input-output instances. These generations are then filtered to remove low-quality or similar ones, and the resulting data is added back to the task pool. This process can be repeated multiple times, resulting in a large collection of instructional data that can be used to fine-tune the language model to follow instructions more effectively.&lt;/p&gt; &#xA;&lt;p&gt;Here is an overview of Self-Instruct:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yizhongw/self-instruct/main/docs/pipeline.JPG&#34; alt=&#34;The pipeline for generating instruction data from a language model itself.&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;* &lt;strong&gt;This work is still in progress. We may update the code and data as we make progress. Please be cautious about the version control.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Instruction-tuning using our Self-Instruct data&lt;/h3&gt; &#xA;&lt;p&gt;We release a dataset that contains 52k instructions, paired with 82K instance inputs and outputs. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better. The entire model-generated data can be accessed in &lt;code&gt;data/gpt3-generations/batch_221203/all_instances_82K.jsonl&lt;/code&gt;. This data (+ the 175 seed tasks) reformatted in clean GPT3-finetuning format (prompt + completion) is put in &lt;code&gt;data/finetuning/self_instruct_221203&lt;/code&gt;. You can use the script in &lt;a href=&#34;https://raw.githubusercontent.com/yizhongw/self-instruct/main/scripts/finetune_gpt3.sh&#34;&gt;&lt;code&gt;./scripts/finetune_gpt3.sh&lt;/code&gt;&lt;/a&gt; to finetune GPT3 on this data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This data is generated by a language model (GPT-3) and inevitably contains some errors or biases. We analyzed the data quality on 200 random instructions in our paper, and found that 46% of the data points may have problems. We encourage users to use this data with caution and propose new methods to filter or improve the imperfections.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluating instruction-following capabilities&lt;/h3&gt; &#xA;&lt;p&gt;We also release a new set of 252 expert-written tasks and their instructions motivated by user-oriented applications (rather than well-studied NLP tasks). This data is used in the human evaluation section of &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;the self-instruct paper&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/yizhongw/self-instruct/main/human_eval/README.md&#34;&gt;the human evaluation README&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Generating Self-Instruct data from scratch&lt;/h3&gt; &#xA;&lt;p&gt;To generate Self-Instruct data using your own seed tasks or other models, we open-source our scripts for the entire pipeline here. Our current code is only tested on the GPT3 model accessible via the &lt;a href=&#34;https://beta.openai.com/docs/models/gpt-3&#34;&gt;OpenAI API&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here are the scripts for generating the data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1. Generate instructions from the seed tasks&#xA;./scripts/generate_instructions.sh&#xA;&#xA;# 2. Identify whether the instruction represents a classification task or not&#xA;./scripts/is_clf_or_not.sh&#xA;&#xA;# 3. Generate instances for each instruction&#xA;./scripts/generate_instances.sh&#xA;&#xA;# 4. Filtering, processing, and reformatting&#xA;./scripts/prepare_for_finetuning.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use the Self-Instruct framework or data, feel free to cite us.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{selfinstruct,&#xA;  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},&#xA;  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},&#xA;  journal={arXiv preprint arXiv:2212.10560},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mpoon/gpt-repository-loader</title>
    <updated>2023-03-26T02:00:37Z</updated>
    <id>tag:github.com,2023-03-26:/mpoon/gpt-repository-loader</id>
    <link href="https://github.com/mpoon/gpt-repository-loader" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convert code repos into an LLM prompt-friendly format. Mostly built by GPT-4.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-repository-loader&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;gpt-repository-loader&lt;/code&gt; is a command-line tool that converts the contents of a Git repository into a text format, preserving the structure of the files and file contents. The generated output can be interpreted by AI language models, allowing them to process the repository&#39;s contents for various tasks, such as code review or documentation generation.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Some context around building this is &lt;a href=&#34;https://github.com/mpoon/gpt-repository-loader/discussions/18&#34;&gt;located here&lt;/a&gt;. Appreciate any issues and pull requests in the spirit of having mostly GPT build out this tool. Using &lt;a href=&#34;https://chat.openai.com/&#34;&gt;ChatGPT Plus&lt;/a&gt; is recommended for quick access to GPT-4.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with &lt;code&gt;gpt-repository-loader&lt;/code&gt;, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you have Python 3 installed on your system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone or download the &lt;code&gt;gpt-repository-loader&lt;/code&gt; repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the repository&#39;s root directory in your terminal.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;gpt-repository-loader&lt;/code&gt; with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gpt_repository_loader.py /path/to/git/repository [-p /path/to/preamble.txt] [-o /path/to/output_file.txt]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace &lt;code&gt;/path/to/git/repository&lt;/code&gt; with the path to the Git repository you want to process. Optionally, you can specify a preamble file with -p or an output file with -o. If not specified, the default output file will be named output.txt in the current directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The tool will generate an output.txt file containing the text representation of the repository. You can now use this file as input for AI language models or other text-based processing tasks.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Running Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run the tests for &lt;code&gt;gpt-repository-loader&lt;/code&gt;, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you have Python 3 installed on your system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the repository&#39;s root directory in your terminal.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the tests with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m unittest test_gpt_repository_loader.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Now, the test harness is added to the &lt;code&gt;gpt-repository-loader&lt;/code&gt; project. You can run the tests by executing the command &lt;code&gt;python -m unittest test_gpt_repository_loader.py&lt;/code&gt; in your terminal.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
</feed>