<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-11T01:42:08Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>awslabs/agent-squad</title>
    <updated>2025-05-11T01:42:08Z</updated>
    <id>tag:github.com,2025-05-11:/awslabs/agent-squad</id>
    <link href="https://github.com/awslabs/agent-squad" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Flexible and powerful framework for managing multiple AI agents and handling complex conversations&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;center&#34;&gt;Agent Squad&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Flexible, lightweight open-source framework for orchestrating multiple AI agents to handle complex conversations.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;üì¢ New Name Alert:&lt;/strong&gt; Multi-Agent Orchestrator is now &lt;strong&gt;Agent Squad!&lt;/strong&gt; üéâ&lt;br&gt; Same powerful functionalities, new catchy name. Embrace the squad! &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/awslabs/agent-squad&#34;&gt;&lt;img alt=&#34;GitHub Repo&#34; src=&#34;https://img.shields.io/badge/GitHub-Repo-green.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/agent-squad&#34;&gt;&lt;img alt=&#34;npm&#34; src=&#34;https://img.shields.io/npm/v/agent-squad.svg?style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/agent-squad/&#34;&gt;&lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/agent-squad.svg?style=flat-square&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- GitHub Stats --&gt; &lt;img src=&#34;https://img.shields.io/github/stars/awslabs/agent-squad?style=social&#34; alt=&#34;GitHub stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/awslabs/agent-squad?style=social&#34; alt=&#34;GitHub forks&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/watchers/awslabs/agent-squad?style=social&#34; alt=&#34;GitHub watchers&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- Repository Info --&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/awslabs/agent-squad&#34; alt=&#34;Last Commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/awslabs/agent-squad&#34; alt=&#34;Issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/awslabs/agent-squad&#34; alt=&#34;Pull Requests&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://awslabs.github.io/agent-squad/&#34; style=&#34;display: inline-block; background-color: #0066cc; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; font-size: 15px; transition: background-color 0.3s;&#34;&gt; üìö Explore Full Documentation &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üîñ Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Intelligent intent classification&lt;/strong&gt; ‚Äî Dynamically route queries to the most suitable agent based on context and content.&lt;/li&gt; &#xA; &lt;li&gt;üî§ &lt;strong&gt;Dual language support&lt;/strong&gt; ‚Äî Fully implemented in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üåä &lt;strong&gt;Flexible agent responses&lt;/strong&gt; ‚Äî Support for both streaming and non-streaming responses from different agents.&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;strong&gt;Context management&lt;/strong&gt; ‚Äî Maintain and utilize conversation context across multiple agents for coherent interactions.&lt;/li&gt; &#xA; &lt;li&gt;üîß &lt;strong&gt;Extensible architecture&lt;/strong&gt; ‚Äî Easily integrate new agents or customize existing ones to fit your specific needs.&lt;/li&gt; &#xA; &lt;li&gt;üåê &lt;strong&gt;Universal deployment&lt;/strong&gt; ‚Äî Run anywhere - from AWS Lambda to your local environment or any cloud platform.&lt;/li&gt; &#xA; &lt;li&gt;üì¶ &lt;strong&gt;Pre-built agents and classifiers&lt;/strong&gt; ‚Äî A variety of ready-to-use agents and multiple classifier implementations available.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s the Agent Squad ‚ùì&lt;/h2&gt; &#xA;&lt;p&gt;The Agent Squad is a flexible framework for managing multiple AI agents and handling complex conversations. It intelligently routes queries and maintains context across interactions.&lt;/p&gt; &#xA;&lt;p&gt;The system offers pre-built components for quick deployment, while also allowing easy integration of custom agents and conversation messages storage solutions.&lt;/p&gt; &#xA;&lt;p&gt;This adaptability makes it suitable for a wide range of applications, from simple chatbots to sophisticated AI systems, accommodating diverse requirements and scaling efficiently.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üèóÔ∏è High-level architecture flow diagram&lt;/h2&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow.jpg&#34; alt=&#34;High-level architecture flow diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The process begins with user input, which is analyzed by a Classifier.&lt;/li&gt; &#xA; &lt;li&gt;The Classifier leverages both Agents&#39; Characteristics and Agents&#39; Conversation history to select the most appropriate agent for the task.&lt;/li&gt; &#xA; &lt;li&gt;Once an agent is selected, it processes the user input.&lt;/li&gt; &#xA; &lt;li&gt;The orchestrator then saves the conversation, updating the Agents&#39; Conversation history, before delivering the response back to the user.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/img/new.png&#34; alt=&#34;&#34;&gt; Introducing SupervisorAgent: Agents Coordination&lt;/h2&gt; &#xA;&lt;p&gt;The Agent Squad now includes a powerful new SupervisorAgent that enables sophisticated team coordination between multiple specialized agents. This new component implements a &#34;agent-as-tools&#34; architecture, allowing a lead agent to coordinate a team of specialized agents in parallel, maintaining context and delivering coherent responses.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow-supervisor.jpg&#34; alt=&#34;SupervisorAgent flow diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Key capabilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü§ù &lt;strong&gt;Team Coordination&lt;/strong&gt; - Coordonate multiple specialized agents working together on complex tasks&lt;/li&gt; &#xA; &lt;li&gt;‚ö° &lt;strong&gt;Parallel Processing&lt;/strong&gt; - Execute multiple agent queries simultaneously&lt;/li&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Smart Context Management&lt;/strong&gt; - Maintain conversation history across all team members&lt;/li&gt; &#xA; &lt;li&gt;üîÑ &lt;strong&gt;Dynamic Delegation&lt;/strong&gt; - Intelligently distribute subtasks to appropriate team members&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ &lt;strong&gt;Agent Compatibility&lt;/strong&gt; - Works with all agent types (Bedrock, Anthropic, Lex, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The SupervisorAgent can be used in two powerful ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Direct Usage&lt;/strong&gt; - Call it directly when you need dedicated team coordination for specific tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Classifier Integration&lt;/strong&gt; - Add it as an agent within the classifier to build complex hierarchical systems with multiple specialized teams&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here are just a few examples where this agent can be used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Customer Support Teams with specialized sub-teams&lt;/li&gt; &#xA; &lt;li&gt;AI Movie Production Studios&lt;/li&gt; &#xA; &lt;li&gt;Travel Planning Services&lt;/li&gt; &#xA; &lt;li&gt;Product Development Teams&lt;/li&gt; &#xA; &lt;li&gt;Healthcare Coordination Systems&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://awslabs.github.io/agent-squad/agents/built-in/supervisor-agent&#34;&gt;Learn more about SupervisorAgent ‚Üí&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üí¨ Demo App&lt;/h2&gt; &#xA;&lt;p&gt;In the screen recording below, we demonstrate an extended version of the demo app that uses 6 specialized agents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Travel Agent&lt;/strong&gt;: Powered by an Amazon Lex Bot&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Weather Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with a tool to query the open-meteo API&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Restaurant Agent&lt;/strong&gt;: Implemented as an Amazon Bedrock Agent&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Math Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with two tools for executing mathematical operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tech Agent&lt;/strong&gt;: A Bedrock LLM Agent designed to answer questions on technical topics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Health Agent&lt;/strong&gt;: A Bedrock LLM Agent focused on addressing health-related queries&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Watch as the system seamlessly switches context between diverse topics, from booking flights to checking weather, solving math problems, and providing health information. Notice how the appropriate agent is selected for each query, maintaining coherence even with brief follow-up inputs.&lt;/p&gt; &#xA;&lt;p&gt;The demo highlights the system&#39;s ability to handle complex, multi-turn conversations while preserving context and leveraging specialized agents across various domains.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/img/demo-app.gif?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üéØ Examples &amp;amp; Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Get hands-on experience with the Agent Squad through our diverse set of examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demo Applications&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/python&#34;&gt;Streamlit Global Demo&lt;/a&gt;: A single Streamlit application showcasing multiple demos, including: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;AI Movie Production Studio&lt;/li&gt; &#xA;     &lt;li&gt;AI Travel Planner&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://awslabs.github.io/agent-squad/cookbook/examples/chat-demo-app/&#34;&gt;Chat Demo App&lt;/a&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Explore multiple specialized agents handling various domains like travel, weather, math, and health&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://awslabs.github.io/agent-squad/cookbook/examples/ecommerce-support-simulator/&#34;&gt;E-commerce Support Simulator&lt;/a&gt;: Experience AI-powered customer support with: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Automated response generation for common queries&lt;/li&gt; &#xA;     &lt;li&gt;Intelligent routing of complex issues to human support&lt;/li&gt; &#xA;     &lt;li&gt;Real-time chat and email-style communication&lt;/li&gt; &#xA;     &lt;li&gt;Human-in-the-loop interactions for complex cases&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sample Projects&lt;/strong&gt;: Explore our example implementations in the &lt;code&gt;examples&lt;/code&gt; folder: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/chat-demo-app&#34;&gt;&lt;code&gt;chat-demo-app&lt;/code&gt;&lt;/a&gt;: Web-based chat interface with multiple specialized agents&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/ecommerce-support-simulator&#34;&gt;&lt;code&gt;ecommerce-support-simulator&lt;/code&gt;&lt;/a&gt;: AI-powered customer support system&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/chat-chainlit-app&#34;&gt;&lt;code&gt;chat-chainlit-app&lt;/code&gt;&lt;/a&gt;: Chat application built with Chainlit&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/fast-api-streaming&#34;&gt;&lt;code&gt;fast-api-streaming&lt;/code&gt;&lt;/a&gt;: FastAPI implementation with streaming support&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/text-2-structured-output&#34;&gt;&lt;code&gt;text-2-structured-output&lt;/code&gt;&lt;/a&gt;: Natural Language to Structured Data&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-inline-agents&#34;&gt;&lt;code&gt;bedrock-inline-agents&lt;/code&gt;&lt;/a&gt;: Bedrock Inline Agents sample&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-prompt-routing&#34;&gt;&lt;code&gt;bedrock-prompt-routing&lt;/code&gt;&lt;/a&gt;: Bedrock Prompt Routing sample code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples are available in both Python and TypeScript. Check out our &lt;a href=&#34;https://awslabs.github.io/agent-squad/&#34;&gt;documentation&lt;/a&gt; for comprehensive guides on setting up and using the Agent Squad framework!&lt;/p&gt; &#xA;&lt;h2&gt;üìö Deep Dives: Stories, Blogs &amp;amp; Podcasts&lt;/h2&gt; &#xA;&lt;p&gt;Discover creative implementations and diverse applications of the Agent Squad:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2lCi8jEKydhDm8eE8QFIQ5K23pF/from-bonjour-to-boarding-pass-multilingual-ai-chatbot-for-flight-reservations&#34;&gt;From &#39;Bonjour&#39; to &#39;Boarding Pass&#39;: Multilingual AI Chatbot for Flight Reservations&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build a multilingual chatbot using the Agent Squad framework. The article explains how to use an &lt;strong&gt;Amazon Lex&lt;/strong&gt; bot as an agent, along with 2 other new agents to make it work in many languages with just a few lines of code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2lq6cYYwTYGc7S3Zmz28xZoQNQj/beyond-auto-replies-building-an-ai-powered-e-commerce-support-system&#34;&gt;Beyond Auto-Replies: Building an AI-Powered E-commerce Support system&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI-driven multi-agent system for automated e-commerce customer email support. It covers the architecture and setup of specialized AI agents using the Agent Squad framework, integrating automated processing with human-in-the-loop oversight. The guide explores email ingestion, intelligent routing, automated response generation, and human verification, providing a comprehensive approach to balancing AI efficiency with human expertise in customer support.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2mt7CFG7xg4yw6GRHwH9akhg0oD/speak-up-ai-voicing-your-agents-with-amazon-connect-lex-and-bedrock&#34;&gt;Speak Up, AI: Voicing Your Agents with Amazon Connect, Lex, and Bedrock&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI customer call center. It covers the architecture and setup of specialized AI agents using the Agent Squad framework interacting with voice via &lt;strong&gt;Amazon Connect&lt;/strong&gt; and &lt;strong&gt;Amazon Lex&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2pTsHrYPqvAbJBl9ht1XxPOSPjR/unlock-bedrock-invokeinlineagent-api-s-hidden-potential-with-agent-squad&#34;&gt;Unlock Bedrock InvokeInlineAgent API&#39;s Hidden Potential&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to scale &lt;strong&gt;Amazon Bedrock Agents&lt;/strong&gt; beyond knowledge base limitations using the Agent Squad framework and &lt;strong&gt;InvokeInlineAgent API&lt;/strong&gt;. This article demonstrates dynamic agent creation and knowledge base selection for enterprise-scale AI applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2phMjQ0bqWMg4PBwejBs1uf4YQE/supercharging-amazon-bedrock-flows-with-aws-agent-squad&#34;&gt;Supercharging Amazon Bedrock Flows&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to enhance &lt;strong&gt;Amazon Bedrock Flows&lt;/strong&gt; with conversation memory and multi-flow orchestration using the Agent Squad framework. This guide shows how to overcome Bedrock Flows&#39; limitations to build more sophisticated AI workflows with persistent memory and intelligent routing between flows.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üéôÔ∏è Podcast Discussions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üá´üá∑ Podcast (French)&lt;/strong&gt;: L&#39;orchestrateur multi-agents : Un orchestrateur open source pour vos agents IA&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://podcasts.apple.com/be/podcast/lorchestrateur-multi-agents/id1452118442?i=1000684332612&#34;&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/4RdMazSRhZUyW2pniG91Vf&#34;&gt;Spotify&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üá¨üáß Podcast (English)&lt;/strong&gt;: An Orchestrator for Your AI Agents&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://podcasts.apple.com/us/podcast/an-orchestrator-for-your-ai-agents/id1574162669?i=1000677039579&#34;&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/2a9DBGZn2lVqVMBLWGipHU&#34;&gt;Spotify&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TypeScript Version&lt;/h3&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üîÑ &lt;code&gt;multi-agent-orchestrator&lt;/code&gt; becomes &lt;code&gt;agent-squad&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install agent-squad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;p&gt;The following example demonstrates how to use the Agent Squad with two different types of agents: a Bedrock LLM Agent with Converse API support and a Lex Bot Agent. This showcases the flexibility of the system in integrating various AI services.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { AgentSquad, BedrockLLMAgent, LexBotAgent } from &#34;agent-squad&#34;;&#xA;&#xA;const orchestrator = new AgentSquad();&#xA;&#xA;// Add a Bedrock LLM Agent with Converse API support&#xA;orchestrator.addAgent(&#xA;  new BedrockLLMAgent({&#xA;      name: &#34;Tech Agent&#34;,&#xA;      description:&#xA;        &#34;Specializes in technology areas including software development, hardware, AI, cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs related to technology products and services.&#34;,&#xA;      streaming: true&#xA;  })&#xA;);&#xA;&#xA;// Add a Lex Bot Agent for handling travel-related queries&#xA;orchestrator.addAgent(&#xA;  new LexBotAgent({&#xA;    name: &#34;Travel Agent&#34;,&#xA;    description: &#34;Helps users book and manage their flight reservations&#34;,&#xA;    botId: process.env.LEX_BOT_ID,&#xA;    botAliasId: process.env.LEX_BOT_ALIAS_ID,&#xA;    localeId: &#34;en_US&#34;,&#xA;  })&#xA;);&#xA;&#xA;// Example usage&#xA;const response = await orchestrator.routeRequest(&#xA;  &#34;I want to book a flight&#34;,&#xA;  &#39;user123&#39;,&#xA;  &#39;session456&#39;&#xA;);&#xA;&#xA;// Handle the response (streaming or non-streaming)&#xA;if (response.streaming == true) {&#xA;    console.log(&#34;\n** RESPONSE STREAMING ** \n&#34;);&#xA;    // Send metadata immediately&#xA;    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);&#xA;    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);&#xA;    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);&#xA;    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);&#xA;    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);&#xA;    console.log(&#xA;      `&amp;gt; Additional Parameters:`,&#xA;      response.metadata.additionalParams&#xA;    );&#xA;    console.log(`\n&amp;gt; Response: `);&#xA;&#xA;    // Stream the content&#xA;    for await (const chunk of response.output) {&#xA;      if (typeof chunk === &#34;string&#34;) {&#xA;        process.stdout.write(chunk);&#xA;      } else {&#xA;        console.error(&#34;Received unexpected chunk type:&#34;, typeof chunk);&#xA;      }&#xA;    }&#xA;&#xA;} else {&#xA;    // Handle non-streaming response (AgentProcessingResult)&#xA;    console.log(&#34;\n** RESPONSE ** \n&#34;);&#xA;    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);&#xA;    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);&#xA;    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);&#xA;    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);&#xA;    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);&#xA;    console.log(&#xA;      `&amp;gt; Additional Parameters:`,&#xA;      response.metadata.additionalParams&#xA;    );&#xA;    console.log(`\n&amp;gt; Response: ${response.output}`);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python Version&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üîÑ &lt;code&gt;multi-agent-orchestrator&lt;/code&gt; becomes &lt;code&gt;agent-squad&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Optional: Set up a virtual environment&#xA;python -m venv venv&#xA;source venv/bin/activate  # On Windows use `venv\Scripts\activate`&#xA;pip install agent-squad[aws]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Default Usage&lt;/h4&gt; &#xA;&lt;p&gt;Here&#39;s an equivalent Python example demonstrating the use of the Agent Squad with a Bedrock LLM Agent and a Lex Bot Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys&#xA;import asyncio&#xA;from agent_squad.orchestrator import AgentSquad&#xA;from agent_squad.agents import BedrockLLMAgent, BedrockLLMAgentOptions, AgentStreamResponse&#xA;&#xA;orchestrator = AgentSquad()&#xA;&#xA;tech_agent = BedrockLLMAgent(BedrockLLMAgentOptions(&#xA;  name=&#34;Tech Agent&#34;,&#xA;  streaming=True,&#xA;  description=&#34;Specializes in technology areas including software development, hardware, AI, \&#xA;  cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs \&#xA;  related to technology products and services.&#34;,&#xA;  model_id=&#34;anthropic.claude-3-sonnet-20240229-v1:0&#34;,&#xA;))&#xA;orchestrator.add_agent(tech_agent)&#xA;&#xA;&#xA;health_agent = BedrockLLMAgent(BedrockLLMAgentOptions(&#xA;  name=&#34;Health Agent&#34;,&#xA;  streaming=True,&#xA;  description=&#34;Specializes in health and well being&#34;,&#xA;))&#xA;orchestrator.add_agent(health_agent)&#xA;&#xA;async def main():&#xA;    # Example usage&#xA;    response = await orchestrator.route_request(&#xA;        &#34;What is AWS Lambda?&#34;,&#xA;        &#39;user123&#39;,&#xA;        &#39;session456&#39;,&#xA;        {},&#xA;        True&#xA;    )&#xA;&#xA;    # Handle the response (streaming or non-streaming)&#xA;    if response.streaming:&#xA;        print(&#34;\n** RESPONSE STREAMING ** \n&#34;)&#xA;        # Send metadata immediately&#xA;        print(f&#34;&amp;gt; Agent ID: {response.metadata.agent_id}&#34;)&#xA;        print(f&#34;&amp;gt; Agent Name: {response.metadata.agent_name}&#34;)&#xA;        print(f&#34;&amp;gt; User Input: {response.metadata.user_input}&#34;)&#xA;        print(f&#34;&amp;gt; User ID: {response.metadata.user_id}&#34;)&#xA;        print(f&#34;&amp;gt; Session ID: {response.metadata.session_id}&#34;)&#xA;        print(f&#34;&amp;gt; Additional Parameters: {response.metadata.additional_params}&#34;)&#xA;        print(&#34;\n&amp;gt; Response: &#34;)&#xA;&#xA;        # Stream the content&#xA;        async for chunk in response.output:&#xA;            async for chunk in response.output:&#xA;              if isinstance(chunk, AgentStreamResponse):&#xA;                  print(chunk.text, end=&#39;&#39;, flush=True)&#xA;              else:&#xA;                  print(f&#34;Received unexpected chunk type: {type(chunk)}&#34;, file=sys.stderr)&#xA;&#xA;    else:&#xA;        # Handle non-streaming response (AgentProcessingResult)&#xA;        print(&#34;\n** RESPONSE ** \n&#34;)&#xA;        print(f&#34;&amp;gt; Agent ID: {response.metadata.agent_id}&#34;)&#xA;        print(f&#34;&amp;gt; Agent Name: {response.metadata.agent_name}&#34;)&#xA;        print(f&#34;&amp;gt; User Input: {response.metadata.user_input}&#34;)&#xA;        print(f&#34;&amp;gt; User ID: {response.metadata.user_id}&#34;)&#xA;        print(f&#34;&amp;gt; Session ID: {response.metadata.session_id}&#34;)&#xA;        print(f&#34;&amp;gt; Additional Parameters: {response.metadata.additional_params}&#34;)&#xA;        print(f&#34;\n&amp;gt; Response: {response.output.content}&#34;)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;  asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These examples showcase:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The use of a Bedrock LLM Agent with Converse API support, allowing for multi-turn conversations.&lt;/li&gt; &#xA; &lt;li&gt;Integration of a Lex Bot Agent for specialized tasks (in this case, travel-related queries).&lt;/li&gt; &#xA; &lt;li&gt;The orchestrator&#39;s ability to route requests to the most appropriate agent based on the input.&lt;/li&gt; &#xA; &lt;li&gt;Handling of both streaming and non-streaming responses from different types of agents.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Modular Installation Options&lt;/h3&gt; &#xA;&lt;p&gt;The Agent Squad is designed with a modular architecture, allowing you to install only the components you need while ensuring you always get the core functionality.&lt;/p&gt; &#xA;&lt;h4&gt;Installation Options&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. AWS Integration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; pip install &#34;agent-squad[aws]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Includes core orchestration functionality with comprehensive AWS service integrations (&lt;code&gt;BedrockLLMAgent&lt;/code&gt;, &lt;code&gt;AmazonBedrockAgent&lt;/code&gt;, &lt;code&gt;LambdaAgent&lt;/code&gt;, etc.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Anthropic Integration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;agent-squad[anthropic]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. OpenAI Integration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;agent-squad[openai]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adds OpenAI&#39;s GPT models for agents and classification, along with core packages.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. Full Installation&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;agent-squad[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Includes all optional dependencies for maximum flexibility.&lt;/p&gt; &#xA;&lt;h3&gt;üôå &lt;strong&gt;We Want to Hear From You!&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Have something to share, discuss, or brainstorm? We‚Äôd love to connect with you and hear about your journey with the &lt;strong&gt;Agent Squad framework&lt;/strong&gt;. Here‚Äôs how you can get involved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üôå Show &amp;amp; Tell&lt;/strong&gt;: Got a success story, cool project, or creative implementation? Share it with us in the &lt;a href=&#34;https://github.com/awslabs/agent-squad/discussions/categories/show-and-tell&#34;&gt;&lt;strong&gt;Show and Tell&lt;/strong&gt;&lt;/a&gt; section. Your work might inspire the entire community! üéâ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üí¨ General Discussion&lt;/strong&gt;: Have questions, feedback, or suggestions? Join the conversation in our &lt;a href=&#34;https://github.com/awslabs/agent-squad/discussions/categories/general&#34;&gt;&lt;strong&gt;General Discussions&lt;/strong&gt;&lt;/a&gt; section. It‚Äôs the perfect place to connect with other users and contributors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üí° Ideas&lt;/strong&gt;: Thinking of a new feature or improvement? Share your thoughts in the &lt;a href=&#34;https://github.com/awslabs/agent-squad/discussions/categories/ideas&#34;&gt;&lt;strong&gt;Ideas&lt;/strong&gt;&lt;/a&gt; section. We‚Äôre always open to exploring innovative ways to make the orchestrator even better!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let‚Äôs collaborate, learn from each other, and build something incredible together! üöÄ&lt;/p&gt; &#xA;&lt;h2&gt;üìù Pull Request Guidelines&lt;/h2&gt; &#xA;&lt;h3&gt;Issue-First Policy&lt;/h3&gt; &#xA;&lt;p&gt;This repository follows an &lt;strong&gt;Issue-First&lt;/strong&gt; policy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Every pull request must be linked to an existing issue&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;If there isn&#39;t an issue for the changes you want to make, please create one first&lt;/li&gt; &#xA; &lt;li&gt;Use the issue to discuss proposed changes before investing time in implementation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to Link Pull Requests to Issues&lt;/h3&gt; &#xA;&lt;p&gt;When creating a pull request, you must link it to an issue using one of these methods:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Include a reference in the PR description using keywords:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Fixes #123&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Resolves #123&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Closes #123&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Manually link the PR to an issue through GitHub&#39;s UI:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;On the right sidebar of your PR, click &#34;Development&#34; and then &#34;Link an issue&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Automated Enforcement&lt;/h3&gt; &#xA;&lt;p&gt;We use GitHub Actions to automatically verify that each PR is linked to an issue. PRs without linked issues will not pass required checks and cannot be merged.&lt;/p&gt; &#xA;&lt;p&gt;This policy helps us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Maintain clear documentation of changes and their purposes&lt;/li&gt; &#xA; &lt;li&gt;Ensure community discussion before implementation&lt;/li&gt; &#xA; &lt;li&gt;Keep a structured development process&lt;/li&gt; &#xA; &lt;li&gt;Make project history more traceable and understandable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;‚ö†Ô∏è Note: Our project has been renamed from &lt;strong&gt;Multi-Agent Orchestrator&lt;/strong&gt; to &lt;strong&gt;Agent Squad&lt;/strong&gt;. Please use the new name in your contributions and discussions.&lt;/p&gt; &#xA;&lt;p&gt;‚ö†Ô∏è We value your contributions! Before submitting changes, please start a discussion by opening an issue to share your proposal.&lt;/p&gt; &#xA;&lt;p&gt;Once your proposal is approved, here are the next steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üìö Review our &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üí° Create a &lt;a href=&#34;https://github.com/awslabs/agent-squad/issues&#34;&gt;GitHub Issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üî® Submit a pull request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;‚úÖ Follow existing project structure and include documentation for new features.&lt;/p&gt; &#xA;&lt;p&gt;üåü &lt;strong&gt;Stay Updated&lt;/strong&gt;: Star the repository to be notified about new features, improvements, and exciting developments in the Agent Squad framework!&lt;/p&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/corneliucroitoru/&#34;&gt;Corneliu Croitoru&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/anthonybernabeu/&#34;&gt;Anthony Bernabeu&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üë• Contributors&lt;/h1&gt; &#xA;&lt;p&gt;Big shout out to our awesome contributors! Thank you for making this project better! üåü ‚≠ê üöÄ&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/agent-squad/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=awslabs/agent-squad&amp;amp;max=2000&#34; alt=&#34;contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; for guidelines on how to propose bugfixes and improvements.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 licence - see the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/agent-squad/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ Font License&lt;/h2&gt; &#xA;&lt;p&gt;This project uses the JetBrainsMono NF font, licensed under the SIL Open Font License 1.1. For full license details, see &lt;a href=&#34;https://github.com/JetBrains/JetBrainsMono/raw/master/OFL.txt&#34;&gt;FONT-LICENSE.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Peterande/D-FINE</title>
    <updated>2025-05-11T01:42:08Z</updated>
    <id>tag:github.com,2025-05-11:/Peterande/D-FINE</id>
    <link href="https://github.com/Peterande/D-FINE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement [ICLR 2025 Spotlight]&lt;/p&gt;&lt;hr&gt;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/README_cn.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/README_ja.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/src/zoo/dfine/blog.md&#34;&gt;English Blog&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/src/zoo/dfine/blog_cn.md&#34;&gt;‰∏≠ÊñáÂçöÂÆ¢&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; D-FINE: Redefine Regression Task of DETRs as Fine‚Äëgrained&amp;nbsp;Distribution&amp;nbsp;Refinement &lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/spaces/developer0hye/D-FINE&#34;&gt; &lt;img alt=&#34;hf&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Peterande/D-FINE/raw/master/LICENSE&#34;&gt; &lt;img alt=&#34;license&#34; src=&#34;https://img.shields.io/badge/LICENSE-Apache%202.0-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Peterande/D-FINE/pulls&#34;&gt; &lt;img alt=&#34;prs&#34; src=&#34;https://img.shields.io/github/issues-pr/Peterande/D-FINE&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Peterande/D-FINE/issues&#34;&gt; &lt;img alt=&#34;issues&#34; src=&#34;https://img.shields.io/github/issues/Peterande/D-FINE?color=olive&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2410.13842&#34;&gt; &lt;img alt=&#34;arXiv&#34; src=&#34;https://img.shields.io/badge/arXiv-2410.13842-red&#34;&gt; &lt;/a&gt; &#xA; &lt;!--     &lt;a href=&#34;mailto: pengyansong@mail.ustc.edu.cn&#34;&gt;&#xA;        &lt;img alt=&#34;email&#34; src=&#34;https://img.shields.io/badge/contact_me-email-yellow&#34;&gt;&#xA;    &lt;/a&gt; --&gt; &lt;a href=&#34;https://results.pre-commit.ci/latest/github/Peterande/D-FINE/master&#34;&gt; &lt;img alt=&#34;pre-commit.ci status&#34; src=&#34;https://results.pre-commit.ci/badge/github/Peterande/D-FINE/master.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Peterande/D-FINE&#34;&gt; &lt;img alt=&#34;stars&#34; src=&#34;https://img.shields.io/github/stars/Peterande/D-FINE&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üìÑ This is the official implementation of the paper: &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2410.13842&#34;&gt;D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, and Feng Wu &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; University of Science and Technology of China &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=d-fine-redefine-regression-task-in-detrs-as&#34;&gt; &lt;img alt=&#34;sota&#34; src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/d-fine-redefine-regression-task-in-detrs-as/real-time-object-detection-on-coco&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;!-- &lt;table&gt;&lt;tr&gt;&#xA;&lt;td&gt;&lt;img src=https://github.com/Peterande/storage/blob/master/latency.png border=0 width=333&gt;&lt;/td&gt;&#xA;&lt;td&gt;&lt;img src=https://github.com/Peterande/storage/blob/master/params.png border=0 width=333&gt;&lt;/td&gt;&#xA;&lt;td&gt;&lt;img src=https://github.com/Peterande/storage/blob/master/flops.png border=0 width=333&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&lt;/table&gt; --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;If you like D-FINE, please give us a ‚≠ê! Your support motivates us to keep improving!&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/stats_padded.png&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;D-FINE is a powerful real-time object detector that redefines the bounding box regression task in DETRs as Fine-grained Distribution Refinement (FDR) and introduces Global Optimal Localization Self-Distillation (GO-LSD), achieving outstanding performance without introducing additional inference and training costs.&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; Video &lt;/summary&gt; &#xA; &lt;p&gt;We conduct object detection using D-FINE and YOLO11 on a complex street scene video from &lt;a href=&#34;https://www.youtube.com/watch?v=CfhEWj9sd9A&#34;&gt;YouTube&lt;/a&gt;. Despite challenging conditions such as backlighting, motion blur, and dense crowds, D-FINE-X successfully detects nearly all targets, including subtle small objects like backpacks, bicycles, and traffic lights. Its confidence scores and the localization precision for blurred edges are significantly higher than those of YOLO11.&lt;/p&gt; &#xA; &lt;!-- We use D-FINE and YOLO11 on a street scene video from [YouTube](https://www.youtube.com/watch?v=CfhEWj9sd9A). Despite challenges like backlighting, motion blur, and dense crowds, D-FINE-X outperforms YOLO11x, detecting more objects with higher confidence and better precision. --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/e5933d8e-3c8a-400e-870b-4e452f5321d9&#34;&gt;https://github.com/user-attachments/assets/e5933d8e-3c8a-400e-870b-4e452f5321d9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üöÄ Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.10.18]&lt;/strong&gt; Release D-FINE series.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.10.25]&lt;/strong&gt; Add custom dataset finetuning configs (&lt;a href=&#34;https://github.com/Peterande/D-FINE/issues/7&#34;&gt;#7&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.10.30]&lt;/strong&gt; Update D-FINE-L (E25) pretrained model, with performance improved by 2.0%.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.11.07]&lt;/strong&gt; Release &lt;strong&gt;D-FINE-N&lt;/strong&gt;, achiving 42.8% AP&lt;sup&gt;val&lt;/sup&gt; on COCO @ 472 FPS&lt;sup&gt;T4&lt;/sup&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;h3&gt;COCO&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GFLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;logs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëN&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;42.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.12ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_n_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_n_coco.pth&#34;&gt;42.8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_n_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;48.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.49ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_s_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_s_coco.pth&#34;&gt;48.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_s_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëM&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;52.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.62ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_m_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_m_coco.pth&#34;&gt;52.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_m_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëL&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.07ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_l_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_l_coco.pth&#34;&gt;54.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_l_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëX&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.89ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;202&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_x_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_x_coco.pth&#34;&gt;55.8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_x_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Objects365+COCO&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GFLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;logs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Objects365+COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;50.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.49ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_s_obj2coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_s_obj2coco.pth&#34;&gt;50.7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj2coco/dfine_s_obj2coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëM&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Objects365+COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.62ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_m_obj2coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_m_obj2coco.pth&#34;&gt;55.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj2coco/dfine_m_obj2coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëL&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Objects365+COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.07ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_l_obj2coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_l_obj2coco_e25.pth&#34;&gt;57.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj2coco/dfine_l_obj2coco_log_e25.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëX&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Objects365+COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;59.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.89ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;202&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_x_obj2coco.pth&#34;&gt;59.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj2coco/dfine_x_obj2coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;We highly recommend that you use the Objects365 pre-trained model for fine-tuning:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Important&lt;/strong&gt;: Please note that this is generally beneficial for complex scene understanding. If your categories are very simple, it might lead to overfitting and suboptimal performance.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt; üî• Pretrained Models on Objects365 (Best generalization) &lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;5000&lt;/sup&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;GFLOPs&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;checkpoint&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;logs&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëS&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;31.0&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;30.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;10M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3.49ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_s_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_s_obj365.pth&#34;&gt;30.5&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_s_obj365_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëM&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;38.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;37.4&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;19M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5.62ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_m_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_m_obj365.pth&#34;&gt;37.4&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_m_obj365_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëL&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;40.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;8.07ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_l_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_l_obj365.pth&#34;&gt;40.6&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_l_obj365_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëL (E25)&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;44.7&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;42.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;8.07ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_l_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_l_obj365_e25.pth&#34;&gt;42.6&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_l_obj365_log_e25.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‚ÄëFINE‚ÄëX&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;49.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;46.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;62M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;12.89ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;202&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_x_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_x_obj365.pth&#34;&gt;46.5&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_x_obj365_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;E25&lt;/strong&gt;: Re-trained and extended the pretraining to 25 epochs.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; is evaluated on &lt;em&gt;Objects365&lt;/em&gt; full validation set.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;AP&lt;sup&gt;5000&lt;/sup&gt;&lt;/strong&gt; is evaluated on the first 5000 samples of the &lt;em&gt;Objects365&lt;/em&gt; validation set.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; is evaluated on &lt;em&gt;MSCOCO val2017&lt;/em&gt; dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt; is evaluated on a single T4 GPU with $batch\_size = 1$, $fp16$, and $TensorRT==10.4.0$.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Objects365+COCO&lt;/strong&gt; means finetuned model on &lt;em&gt;COCO&lt;/em&gt; using pretrained weights trained on &lt;em&gt;Objects365&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n dfine python=3.11.9&#xA;conda activate dfine&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; COCO2017 Dataset &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Download COCO2017 from &lt;a href=&#34;https://opendatalab.com/OpenDataLab/COCO_2017&#34;&gt;OpenDataLab&lt;/a&gt; or &lt;a href=&#34;https://cocodataset.org/#download&#34;&gt;COCO&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Modify paths in &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dataset/coco_detection.yml&#34;&gt;coco_detection.yml&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;train_dataloader:&#xA;    img_folder: /data/COCO2017/train2017/&#xA;    ann_file: /data/COCO2017/annotations/instances_train2017.json&#xA;val_dataloader:&#xA;    img_folder: /data/COCO2017/val2017/&#xA;    ann_file: /data/COCO2017/annotations/instances_val2017.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Objects365 Dataset &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Download Objects365 from &lt;a href=&#34;https://opendatalab.com/OpenDataLab/Objects365&#34;&gt;OpenDataLab&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Set the Base Directory:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export BASE_DIR=/data/Objects365/data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Extract and organize the downloaded files, resulting directory structure:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;${BASE_DIR}/train&#xA;‚îú‚îÄ‚îÄ images&#xA;‚îÇ   ‚îú‚îÄ‚îÄ v1&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patch0&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000000.jpg&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000001.jpg&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (more images)&#xA;‚îÇ   ‚îú‚îÄ‚îÄ v2&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patchx&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000000.jpg&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000001.jpg&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (more images)&#xA;‚îú‚îÄ‚îÄ zhiyuan_objv2_train.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;${BASE_DIR}/val&#xA;‚îú‚îÄ‚îÄ images&#xA;‚îÇ   ‚îú‚îÄ‚îÄ v1&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patch0&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000000.jpg&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (more images)&#xA;‚îÇ   ‚îú‚îÄ‚îÄ v2&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patchx&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000000000.jpg&#xA;‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (more images)&#xA;‚îú‚îÄ‚îÄ zhiyuan_objv2_val.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Create a New Directory to Store Images from the Validation Set:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir -p ${BASE_DIR}/train/images_from_val&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;5&#34;&gt; &#xA;  &lt;li&gt;Copy the v1 and v2 folders from the val directory into the train/images_from_val directory&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/&#xA;cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;6&#34;&gt; &#xA;  &lt;li&gt;Run remap_obj365.py to merge a subset of the validation set into the training set. Specifically, this script moves samples with indices between 5000 and 800000 from the validation set to the training set.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/remap_obj365.py --base_dir ${BASE_DIR}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;7&#34;&gt; &#xA;  &lt;li&gt;Run the resize_obj365.py script to resize any images in the dataset where the maximum edge length exceeds 640 pixels. Use the updated JSON file generated in Step 5 to process the sample data. Ensure that you resize images in both the train and val datasets to maintain consistency.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/resize_obj365.py --base_dir ${BASE_DIR}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;8&#34;&gt; &#xA;  &lt;li&gt; &lt;p&gt;Modify paths in &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dataset/obj365_detection.yml&#34;&gt;obj365_detection.yml&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;train_dataloader:&#xA;    img_folder: /data/Objects365/data/train&#xA;    ann_file: /data/Objects365/data/train/new_zhiyuan_objv2_train_resized.json&#xA;val_dataloader:&#xA;    img_folder: /data/Objects365/data/val/&#xA;    ann_file: /data/Objects365/data/val/new_zhiyuan_objv2_val_resized.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;CrowdHuman&lt;/summary&gt; &#xA; &lt;p&gt;Download COCO format dataset here: &lt;a href=&#34;https://aistudio.baidu.com/datasetdetail/231455&#34;&gt;url&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Custom Dataset&lt;/summary&gt; &#xA; &lt;p&gt;To train on your custom dataset, you need to organize it in the COCO format. Follow the steps below to prepare your dataset:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set &lt;code&gt;remap_mscoco_category&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This prevents the automatic remapping of category IDs to match the MSCOCO categories.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;remap_mscoco_category: False&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Organize Images:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Structure your dataset directories as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dataset/&#xA;‚îú‚îÄ‚îÄ images/&#xA;‚îÇ   ‚îú‚îÄ‚îÄ train/&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image2.jpg&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...&#xA;‚îÇ   ‚îú‚îÄ‚îÄ val/&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image2.jpg&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...&#xA;‚îî‚îÄ‚îÄ annotations/&#xA;    ‚îú‚îÄ‚îÄ instances_train.json&#xA;    ‚îú‚îÄ‚îÄ instances_val.json&#xA;    ‚îî‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&lt;code&gt;images/train/&lt;/code&gt;&lt;/strong&gt;: Contains all training images.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&lt;code&gt;images/val/&lt;/code&gt;&lt;/strong&gt;: Contains all validation images.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&lt;code&gt;annotations/&lt;/code&gt;&lt;/strong&gt;: Contains COCO-formatted annotation files.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Convert Annotations to COCO Format:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If your annotations are not already in COCO format, you&#39;ll need to convert them. You can use the following Python script as a reference or utilize existing tools:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;&#xA;def convert_to_coco(input_annotations, output_annotations):&#xA;    # Implement conversion logic here&#xA;    pass&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    convert_to_coco(&#39;path/to/your_annotations.json&#39;, &#39;dataset/annotations/instances_train.json&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Update Configuration Files:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dataset/custom_detection.yml&#34;&gt;custom_detection.yml&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;task: detection&#xA;&#xA;evaluator:&#xA;  type: CocoEvaluator&#xA;  iou_types: [&#39;bbox&#39;, ]&#xA;&#xA;num_classes: 777 # your dataset classes&#xA;remap_mscoco_category: False&#xA;&#xA;train_dataloader:&#xA;  type: DataLoader&#xA;  dataset:&#xA;    type: CocoDetection&#xA;    img_folder: /data/yourdataset/train&#xA;    ann_file: /data/yourdataset/train/train.json&#xA;    return_masks: False&#xA;    transforms:&#xA;      type: Compose&#xA;      ops: ~&#xA;  shuffle: True&#xA;  num_workers: 4&#xA;  drop_last: True&#xA;  collate_fn:&#xA;    type: BatchImageCollateFunction&#xA;&#xA;val_dataloader:&#xA;  type: DataLoader&#xA;  dataset:&#xA;    type: CocoDetection&#xA;    img_folder: /data/yourdataset/val&#xA;    ann_file: /data/yourdataset/val/ann.json&#xA;    return_masks: False&#xA;    transforms:&#xA;      type: Compose&#xA;      ops: ~&#xA;  shuffle: False&#xA;  num_workers: 4&#xA;  drop_last: False&#xA;  collate_fn:&#xA;    type: BatchImageCollateFunction&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; COCO2017 &lt;/summary&gt; &#xA; &lt;!-- &lt;summary&gt;1. Training &lt;/summary&gt; --&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set Model&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Training&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;2. Testing &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Testing&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;3. Tuning &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Tuning&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0 -t model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Objects365 to COCO2017 &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set Model&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Training on Objects365&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Tuning on COCO2017&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;2. Testing &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Testing&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Custom Dataset &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set Model&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Training on Custom Dataset&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/custom/dfine_hgnetv2_${model}_custom.yml --use-amp --seed=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;2. Testing &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Testing&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/custom/dfine_hgnetv2_${model}_custom.yml --test-only -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Tuning on Custom Dataset&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/custom/objects365/dfine_hgnetv2_${model}_obj2custom.yml --use-amp --seed=0 -t model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;5&#34;&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[Optional]&lt;/strong&gt; Modify Class Mappings:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;When using the Objects365 pre-trained weights to train on your custom dataset, the example assumes that your dataset only contains the classes &lt;code&gt;&#39;Person&#39;&lt;/code&gt; and &lt;code&gt;&#39;Car&#39;&lt;/code&gt;. For faster convergence, you can modify &lt;code&gt;self.obj365_ids&lt;/code&gt; in &lt;code&gt;src/solver/_solver.py&lt;/code&gt; as follows:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.obj365_ids = [0, 5]  # Person, Cars&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can replace these with any corresponding classes from your dataset. The list of Objects365 classes with their corresponding IDs: &lt;a href=&#34;https://github.com/Peterande/D-FINE/raw/352a94ece291e26e1957df81277bef00fe88a8e3/src/solver/_solver.py#L330&#34;&gt;https://github.com/Peterande/D-FINE/blob/352a94ece291e26e1957df81277bef00fe88a8e3/src/solver/_solver.py#L330&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;New training command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/custom/dfine_hgnetv2_${model}_custom.yml --use-amp --seed=0 -t model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;However, if you don&#39;t wish to modify the class mappings, the pre-trained Objects365 weights will still work without any changes. Modifying the class mappings is optional and can potentially accelerate convergence for specific tasks.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Customizing Batch Size &lt;/summary&gt; &#xA; &lt;p&gt;For example, if you want to double the total batch size when training D-FINE-L on COCO2017, here are the steps you should follow:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/include/dataloader.yml&#34;&gt;dataloader.yml&lt;/a&gt;&lt;/strong&gt; to increase the &lt;code&gt;total_batch_size&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;train_dataloader:&#xA;    total_batch_size: 64  # Previously it was 32, now doubled&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_l_coco.yml&#34;&gt;dfine_hgnetv2_l_coco.yml&lt;/a&gt;&lt;/strong&gt;. Here‚Äôs how the key parameters should be adjusted:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;optimizer:&#xA;type: AdamW&#xA;params:&#xA;    -&#xA;    params: &#39;^(?=.*backbone)(?!.*norm|bn).*$&#39;&#xA;    lr: 0.000025  # doubled, linear scaling law&#xA;    -&#xA;    params: &#39;^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$&#39;&#xA;    weight_decay: 0.&#xA;&#xA;lr: 0.0005  # doubled, linear scaling law&#xA;betas: [0.9, 0.999]&#xA;weight_decay: 0.0001  # need a grid search&#xA;&#xA;ema:  # added EMA settings&#xA;    decay: 0.9998  # adjusted by 1 - (1 - decay) * 2&#xA;    warmups: 500  # halved&#xA;&#xA;lr_warmup_scheduler:&#xA;    warmup_duration: 250  # halved&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Customizing Input Size &lt;/summary&gt; &#xA; &lt;p&gt;If you&#39;d like to train &lt;strong&gt;D-FINE-L&lt;/strong&gt; on COCO2017 with an input size of 320x320, follow these steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/include/dataloader.yml&#34;&gt;dataloader.yml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;&#xA;train_dataloader:&#xA;dataset:&#xA;    transforms:&#xA;        ops:&#xA;            - {type: Resize, size: [320, 320], }&#xA;collate_fn:&#xA;    base_size: 320&#xA;dataset:&#xA;    transforms:&#xA;        ops:&#xA;            - {type: Resize, size: [320, 320], }&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/include/dfine_hgnetv2.yml&#34;&gt;dfine_hgnetv2.yml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;eval_spatial_size: [320, 320]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Deployment &lt;/summary&gt; &#xA; &lt;!-- &lt;summary&gt;4. Export onnx &lt;/summary&gt; --&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install onnx onnxsim&#xA;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Export onnx&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Export &lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html&#34;&gt;tensorrt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;trtexec --onnx=&#34;model.onnx&#34; --saveEngine=&#34;model.engine&#34; --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Inference (Visualization) &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r tools/inference/requirements.txt&#xA;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;5. Inference &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Inference (onnxruntime / tensorrt / torch)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Inference on images and videos is now supported.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg  # video.mp4&#xA;python tools/inference/trt_inf.py --trt model.engine --input image.jpg&#xA;python tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Benchmark &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r tools/benchmark/requirements.txt&#xA;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;6. Benchmark &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Model FLOPs, MACs, and Params&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;TensorRT Latency&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Fiftyone Visualization &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install fiftyone&#xA;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Voxel51 Fiftyone Visualization (&lt;a href=&#34;https://github.com/voxel51/fiftyone&#34;&gt;fiftyone&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/visualization/fiftyone_vis.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Others &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Auto Resume Training&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash reference/safe_training.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Converting Model Weights&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python reference/convert_weight.py model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Figures and Visualizations&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; FDR and GO-LSD &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Overview of D-FINE with FDR. The probability distributions that act as a more fine- grained intermediate representation are iteratively refined by the decoder layers in a residual manner. Non-uniform weighting functions are applied to allow for finer localization.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/fdr-1.jpg&#34; alt=&#34;Fine-grained Distribution Refinement Process&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Overview of GO-LSD process. Localization knowledge from the final layer‚Äôs refined distributions is distilled into earlier layers through DDF loss with decoupled weighting strategies.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/go_lsd-1.jpg&#34; alt=&#34;GO-LSD Process&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; Distributions &lt;/summary&gt; &#xA; &lt;p&gt;Visualizations of FDR across detection scenarios with initial and refined bounding boxes, along with unweighted and weighted distributions.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/merged_image.jpg&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Hard Cases &lt;/summary&gt; &#xA; &lt;p&gt;The following visualization demonstrates D-FINE&#39;s predictions in various complex detection scenarios. These include cases with occlusion, low-light conditions, motion blur, depth of field effects, and densely populated scenes. Despite these challenges, D-FINE consistently produces accurate localization results.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/hard_case-1.jpg&#34; alt=&#34;D-FINE Predictions in Challenging Scenarios&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- &lt;div style=&#34;display: flex; flex-wrap: wrap; justify-content: center; margin: 0; padding: 0;&#34;&gt;&#xA;    &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/merged_image.jpg&#34; style=&#34;width:99.96%; margin: 0; padding: 0;&#34; /&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;table&gt;&lt;tr&gt;&#xA;&lt;td&gt;&lt;img src=https://raw.githubusercontent.com/Peterande/storage/master/figs/merged_image.jpg border=0 width=1000&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&lt;/table&gt; --&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;D-FINE&lt;/code&gt; or its methods in your work, please cite the following BibTeX entries:&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; bibtex &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{peng2024dfine,&#xA;      title={D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement},&#xA;      author={Yansong Peng and Hebei Li and Peixi Wu and Yueyi Zhang and Xiaoyan Sun and Feng Wu},&#xA;      year={2024},&#xA;      eprint={2410.13842},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our work is built upon &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR&#34;&gt;RT-DETR&lt;/a&gt;. Thanks to the inspirations from &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR&#34;&gt;RT-DETR&lt;/a&gt;, &lt;a href=&#34;https://github.com/implus/GFocal&#34;&gt;GFocal&lt;/a&gt;, &lt;a href=&#34;https://github.com/HikariTJU/LD&#34;&gt;LD&lt;/a&gt;, and &lt;a href=&#34;https://github.com/WongKinYiu/yolov9&#34;&gt;YOLOv9&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚ú® Feel free to contribute and reach out if you have any questions! ‚ú®&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Blaizzy/mlx-audio</title>
    <updated>2025-05-11T01:42:08Z</updated>
    <id>tag:github.com,2025-05-11:/Blaizzy/mlx-audio</id>
    <link href="https://github.com/Blaizzy/mlx-audio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple&#39;s MLX framework, providing efficient speech analysis on Apple Silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX-Audio&lt;/h1&gt; &#xA;&lt;p&gt;A text-to-speech (TTS) and Speech-to-Speech (STS) library built on Apple&#39;s MLX framework, providing efficient speech synthesis on Apple Silicon.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast inference on Apple Silicon (M series chips)&lt;/li&gt; &#xA; &lt;li&gt;Multiple language support&lt;/li&gt; &#xA; &lt;li&gt;Voice customization options&lt;/li&gt; &#xA; &lt;li&gt;Adjustable speech speed control (0.5x to 2.0x)&lt;/li&gt; &#xA; &lt;li&gt;Interactive web interface with 3D audio visualization&lt;/li&gt; &#xA; &lt;li&gt;REST API for TTS generation&lt;/li&gt; &#xA; &lt;li&gt;Quantization support for optimized performance&lt;/li&gt; &#xA; &lt;li&gt;Direct access to output files via Finder/Explorer integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install the package&#xA;pip install mlx-audio&#xA;&#xA;# For web interface and API dependencies&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;To generate audio with an LLM use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Basic usage&#xA;mlx_audio.tts.generate --text &#34;Hello, world&#34;&#xA;&#xA;# Specify prefix for output file&#xA;mlx_audio.tts.generate --text &#34;Hello, world&#34; --file_prefix hello&#xA;&#xA;# Adjust speaking speed (0.5-2.0)&#xA;mlx_audio.tts.generate --text &#34;Hello, world&#34; --speed 1.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to call from python&lt;/h3&gt; &#xA;&lt;p&gt;To generate audio with an LLM use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_audio.tts.generate import generate_audio&#xA;&#xA;# Example: Generate an audiobook chapter as mp3 audio&#xA;generate_audio(&#xA;    text=(&#34;In the beginning, the universe was created...\n&#34;&#xA;        &#34;...or the simulation was booted up.&#34;),&#xA;    model_path=&#34;prince-canuma/Kokoro-82M&#34;,&#xA;    voice=&#34;af_heart&#34;,&#xA;    speed=1.2,&#xA;    lang_code=&#34;a&#34;, # Kokoro: (a)f_heart, or comment out for auto&#xA;    file_prefix=&#34;audiobook_chapter1&#34;,&#xA;    audio_format=&#34;wav&#34;,&#xA;    sample_rate=24000,&#xA;    join_audio=True,&#xA;    verbose=True  # Set to False to disable print messages&#xA;)&#xA;&#xA;print(&#34;Audiobook chapter successfully generated!&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Web Interface &amp;amp; API Server&lt;/h3&gt; &#xA;&lt;p&gt;MLX-Audio includes a web interface with a 3D visualization that reacts to audio frequencies. The interface allows you to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate TTS with different voices and speed settings&lt;/li&gt; &#xA; &lt;li&gt;Upload and play your own audio files&lt;/li&gt; &#xA; &lt;li&gt;Visualize audio with an interactive 3D orb&lt;/li&gt; &#xA; &lt;li&gt;Automatically saves generated audio files to the outputs directory in the current working folder&lt;/li&gt; &#xA; &lt;li&gt;Open the output folder directly from the interface (when running locally)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Voice Options&lt;/strong&gt;: Choose from different voice styles (AF Heart, AF Nova, AF Bella, BF Emma)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adjustable Speech Speed&lt;/strong&gt;: Control the speed of speech generation with an interactive slider (0.5x to 2.0x)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-time 3D Visualization&lt;/strong&gt;: A responsive 3D orb that reacts to audio frequencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Audio Upload&lt;/strong&gt;: Play and visualize your own audio files&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto-play Option&lt;/strong&gt;: Automatically play generated audio&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Output Folder Access&lt;/strong&gt;: Convenient button to open the output folder in your system&#39;s file explorer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start the web interface and API server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Using the command-line interface&#xA;mlx_audio.server&#xA;&#xA;# With custom host and port&#xA;mlx_audio.server --host 0.0.0.0 --port 9000&#xA;&#xA;# With verbose logging&#xA;mlx_audio.server --verbose&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available command line arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;: Host address to bind the server to (default: 127.0.0.1)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;: Port to bind the server to (default: 8000)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then open your browser and navigate to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;API Endpoints&lt;/h4&gt; &#xA;&lt;p&gt;The server provides the following REST API endpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /tts&lt;/code&gt;: Generate TTS audio&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parameters (form data): &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;text&lt;/code&gt;: The text to convert to speech (required)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;voice&lt;/code&gt;: Voice to use (default: &#34;af_heart&#34;)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;speed&lt;/code&gt;: Speech speed from 0.5 to 2.0 (default: 1.0)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Returns: JSON with filename of generated audio&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;GET /audio/{filename}&lt;/code&gt;: Retrieve generated audio file&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /play&lt;/code&gt;: Play audio directly from the server&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parameters (form data): &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;filename&lt;/code&gt;: The filename of the audio to play (required)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Returns: JSON with status and filename&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /stop&lt;/code&gt;: Stop any currently playing audio&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Returns: JSON with status&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /open_output_folder&lt;/code&gt;: Open the output folder in the system&#39;s file explorer&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Returns: JSON with status and path&lt;/li&gt; &#xA;   &lt;li&gt;Note: This feature only works when running the server locally&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Generated audio files are stored in &lt;code&gt;~/.mlx_audio/outputs&lt;/code&gt; by default, or in a fallback directory if that location is not writable.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h3&gt;Kokoro&lt;/h3&gt; &#xA;&lt;p&gt;Kokoro is a multilingual TTS model that supports various languages and voice styles.&lt;/p&gt; &#xA;&lt;h4&gt;Example Usage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_audio.tts.models.kokoro import KokoroPipeline&#xA;from mlx_audio.tts.utils import load_model&#xA;from IPython.display import Audio&#xA;import soundfile as sf&#xA;&#xA;# Initialize the model&#xA;model_id = &#39;prince-canuma/Kokoro-82M&#39;&#xA;model = load_model(model_id)&#xA;&#xA;# Create a pipeline with American English&#xA;pipeline = KokoroPipeline(lang_code=&#39;a&#39;, model=model, repo_id=model_id)&#xA;&#xA;# Generate audio&#xA;text = &#34;The MLX King lives. Let him cook!&#34;&#xA;for _, _, audio in pipeline(text, voice=&#39;af_heart&#39;, speed=1, split_pattern=r&#39;\n+&#39;):&#xA;    # Display audio in notebook (if applicable)&#xA;    display(Audio(data=audio, rate=24000, autoplay=0))&#xA;&#xA;    # Save audio to file&#xA;    sf.write(&#39;audio.wav&#39;, audio[0], 24000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Language Options&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üá∫üá∏ &lt;code&gt;&#39;a&#39;&lt;/code&gt; - American English&lt;/li&gt; &#xA; &lt;li&gt;üá¨üáß &lt;code&gt;&#39;b&#39;&lt;/code&gt; - British English&lt;/li&gt; &#xA; &lt;li&gt;üáØüáµ &lt;code&gt;&#39;j&#39;&lt;/code&gt; - Japanese (requires &lt;code&gt;pip install misaki[ja]&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;üá®üá≥ &lt;code&gt;&#39;z&#39;&lt;/code&gt; - Mandarin Chinese (requires &lt;code&gt;pip install misaki[zh]&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CSM (Conversational Speech Model)&lt;/h3&gt; &#xA;&lt;p&gt;CSM is a model from Sesame that allows you text-to-speech and to customize voices using reference audio samples.&lt;/p&gt; &#xA;&lt;h4&gt;Example Usage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Generate speech using CSM-1B model with reference audio&#xA;python -m mlx_audio.tts.generate --model mlx-community/csm-1b --text &#34;Hello from Sesame.&#34; --play --ref_audio ./conversational_a.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can pass any audio to clone the voice from or download sample audio file from &lt;a href=&#34;https://huggingface.co/mlx-community/csm-1b/tree/main/prompts&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Advanced Features&lt;/h2&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;You can quantize models for improved performance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_audio.tts.utils import quantize_model, load_model&#xA;import json&#xA;import mlx.core as mx&#xA;&#xA;model = load_model(repo_id=&#39;prince-canuma/Kokoro-82M&#39;)&#xA;config = model.config&#xA;&#xA;# Quantize to 8-bit&#xA;group_size = 64&#xA;bits = 8&#xA;weights, config = quantize_model(model, config, group_size, bits)&#xA;&#xA;# Save quantized model&#xA;with open(&#39;./8bit/config.json&#39;, &#39;w&#39;) as f:&#xA;    json.dump(config, f)&#xA;&#xA;mx.save_safetensors(&#34;./8bit/kokoro-v1_0.safetensors&#34;, weights, metadata={&#34;format&#34;: &#34;mlx&#34;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MLX&lt;/li&gt; &#xA; &lt;li&gt;Python 3.8+&lt;/li&gt; &#xA; &lt;li&gt;Apple Silicon Mac (for optimal performance)&lt;/li&gt; &#xA; &lt;li&gt;For the web interface and API: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;FastAPI&lt;/li&gt; &#xA;   &lt;li&gt;Uvicorn&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to the Apple MLX team for providing a great framework for building TTS and STS models.&lt;/li&gt; &#xA; &lt;li&gt;This project uses the Kokoro model architecture for text-to-speech synthesis.&lt;/li&gt; &#xA; &lt;li&gt;The 3D visualization uses Three.js for rendering.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>