<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-02T02:03:18Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>THUDM/WebGLM</title>
    <updated>2023-07-02T02:03:18Z</updated>
    <id>tag:github.com,2023-07-02:/THUDM/WebGLM</id>
    <link href="https://github.com/THUDM/WebGLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WebGLM: An Efficient Web-enhanced Question Answering System (KDD 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WebGLM: Towards An Efficient Web-enhanced Question Answering System with Human Preferences&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;üìÉ &lt;a href=&#34;https://arxiv.org/pdf/2306.07906.pdf&#34; target=&#34;_blank&#34;&gt;Paper (KDD 2023)&lt;/a&gt; ‚Ä¢ üåê &lt;a href=&#34;https://github.com/THUDM/WebGLM/raw/main/README_zh.md&#34; target=&#34;_blank&#34;&gt;‰∏≠Êñá README&lt;/a&gt; ‚Ä¢ ü§ó HF Repo &lt;a href=&#34;https://huggingface.co/THUDM/WebGLM&#34; target=&#34;_blank&#34;&gt;[WebGLM-10B]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/THUDM/WebGLM-2B&#34; target=&#34;_blank&#34;&gt;[WebGLM-2B]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation of WebGLM. If you find our open-sourced efforts useful, please üåü the repo to encourage our following developement!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/THUDM/WebGLM/assets/129033897/d2e1dd35-6340-4175-ac2d-fd585daa17cf&#34;&gt;https://github.com/THUDM/WebGLM/assets/129033897/d2e1dd35-6340-4175-ac2d-fd585daa17cf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Read this in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/README_zh.md&#34;&gt;‰∏≠Êñá&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/06/25]&lt;/strong&gt; Release &lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;&gt;ChatGLM2-6B&lt;/a&gt;, an updated version of &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt; which introduces several new features:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stronger Performance&lt;/strong&gt;: we have fully upgraded the ChatGLM2-6B. It uses the hybrid objective function of &lt;a href=&#34;https://github.com/THUDM/GLM&#34;&gt;GLM&lt;/a&gt;, and has undergone pre-training with 1.4T bilingual tokens and human preference alignment training. The &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/README.md#evaluation-results&#34;&gt;evaluation results&lt;/a&gt; show that, compared to the first-generation model, ChatGLM2-6B has achieved substantial improvements in performance on datasets like MMLU (+23%), CEval (+33%), GSM8K (+571%), BBH (+60%), showing strong competitiveness among models of the same size.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Longer Context&lt;/strong&gt;: Based on &lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;FlashAttention&lt;/a&gt; technique, we have extended the context length of the base model from 2K in ChatGLM-6B to 32K, and trained with a context length of 8K during the dialogue alignment, allowing for more rounds of dialogue. However, the current version of ChatGLM2-6B has limited understanding of single-round ultra-long documents, which we will focus on optimizing in future iterations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;More Efficient Inference&lt;/strong&gt;: Based on &lt;a href=&#34;http://arxiv.org/abs/1911.02150&#34;&gt;Multi-Query Attention&lt;/a&gt; technique, ChatGLM2-6B has more efficient inference speed and lower GPU memory usage: under the official implementation, the inference speed has increased by 42% compared to the first generation; under INT4 quantization, the dialogue length supported by 6G GPU memory has increased from 1K to 8K.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;More details please refer to &lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;&gt;ChatGLM2-6B&lt;/a&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#overview&#34;&gt;Overview&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#news&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#preparation&#34;&gt;Preparation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-code-and-environments&#34;&gt;Prepare Code and Environments&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-serpapi-key&#34;&gt;Prepare SerpAPI Key&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-retriever-checkpoint&#34;&gt;Prepare Retriever Checkpoint&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#try-webglm&#34;&gt;Try WebGLM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#export-environment-variables&#34;&gt;Export Environment Variables&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#run-as-command-line-interface&#34;&gt;Run as Command Line Interface&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#run-as-web-service&#34;&gt;Run as Web Service&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#train-webglm&#34;&gt;Train WebGLM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#train-generator&#34;&gt;Train Generator&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-data&#34;&gt;Prepare Data&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#train-retriever&#34;&gt;Train Retriever&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#prepare-data-1&#34;&gt;Prepare Data&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#training-1&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#real-application-cases&#34;&gt;Real Application Cases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/main_process.png&#34; alt=&#34;paper&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;WebGLM aspires to provide an efficient and cost-effective web-enhanced question-answering system using the 10-billion-parameter General Language Model (GLM). It aims to improve real-world application deployment by integrating web search and retrieval capabilities into the pre-trained language model.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM-augmented Retriever&lt;/strong&gt;: Enhances the retrieval of relevant web content to better aid in answering questions accurately.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bootstrapped Generator&lt;/strong&gt;: Generates human-like responses to questions, leveraging the power of the GLM to provide refined answers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human Preference-aware Scorer&lt;/strong&gt;: Estimates the quality of generated responses by prioritizing human preferences, ensuring the system produces useful and engaging content.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023-06-24]&lt;/strong&gt; We support searching via &lt;a href=&#34;https://www.bing.com/&#34;&gt;Bing&lt;/a&gt; now!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023-06-14]&lt;/strong&gt; We release our code and the &lt;a href=&#34;https://arxiv.org/pdf/2306.07906.pdf&#34;&gt;paper&lt;/a&gt; of WebGLM!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Preparation&lt;/h1&gt; &#xA;&lt;h2&gt;Prepare Code and Environments&lt;/h2&gt; &#xA;&lt;p&gt;Clone this repo, and install python requirements.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install Nodejs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt install nodejs # If you use Ubuntu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install playwright dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If browsing environments are not installed in your host, you need to install them. Do not worry, playwright will give you instructions when you first execute it if so.&lt;/p&gt; &#xA;&lt;h2&gt;Prepare SerpAPI Key&lt;/h2&gt; &#xA;&lt;p&gt;In search process, we use SerpAPI to get search results. You need to get a SerpAPI key from &lt;a href=&#34;https://serpapi.com/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then, set the environment variable &lt;code&gt;SERPAPI_KEY&lt;/code&gt; to your key.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export SERPAPI_KEY=&#34;YOUR KEY&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can use Bing search with local browser environment (playwright). You can add &lt;code&gt;--searcher bing&lt;/code&gt; to start command lines to use Bing search. (See &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#run-as-command-line-interface&#34;&gt;Run as Command Line Interface&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/#run-as-web-service&#34;&gt;Run as Web Service&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Prepare Retriever Checkpoint&lt;/h2&gt; &#xA;&lt;p&gt;Download the checkpoint on &lt;a href=&#34;https://cloud.tsinghua.edu.cn/d/bc96946dd9a14c84b8d4/&#34;&gt;Tsinghua Cloud&lt;/a&gt; by running the command line below.&lt;/p&gt; &#xA;&lt;p&gt;You can manually specify the path to save the checkpoint by &lt;code&gt;--save SAVE_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download.py retriever-pretrained-checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Try WebGLM&lt;/h1&gt; &#xA;&lt;p&gt;Before you run the code, make sure that the space of your device is enough.&lt;/p&gt; &#xA;&lt;h2&gt;Export Environment Variables&lt;/h2&gt; &#xA;&lt;p&gt;Export the environment variable &lt;code&gt;WEBGLM_RETRIEVER_CKPT&lt;/code&gt; to the path of the retriever checkpoint. If you have downloaded the retriever checkpoint in the default path, you can simply run the command line below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export WEBGLM_RETRIEVER_CKPT=./download/retriever-pretrained-checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run as Command Line Interface&lt;/h2&gt; &#xA;&lt;p&gt;You can try WebGLM-2B model by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py -w THUDM/WebGLM-2B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or directly for WebGLM-10B model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use Bing search instead of SerpAPI, you can add &lt;code&gt;--searcher bing&lt;/code&gt; to the command line, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py -w THUDM/WebGLM-2B --searcher bing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run as Web Service&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;web_demo.py&lt;/code&gt; with the same arguments as &lt;code&gt;cli_demo.py&lt;/code&gt; to start a web service. For example, you can try WebGLM-2B model with Bing search by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo.py -w THUDM/WebGLM-2B --searcher bing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Train WebGLM&lt;/h1&gt; &#xA;&lt;h2&gt;Train Generator&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare Data&lt;/h3&gt; &#xA;&lt;p&gt;Download the training data on &lt;a href=&#34;https://cloud.tsinghua.edu.cn/d/d290dcfc92e342f9a017/&#34;&gt;Tsinghua Cloud&lt;/a&gt; by running the command line below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download.py generator-training-data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will automatically download all the data and preprocess them into the seq2seq form that can be used immediately in &lt;code&gt;./download&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/THUDM/GLM#train-with-your-own-data&#34;&gt;GLM repo&lt;/a&gt; for seq2seq training.&lt;/p&gt; &#xA;&lt;h2&gt;Train Retriever&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare Data&lt;/h3&gt; &#xA;&lt;p&gt;Download the training data on &lt;a href=&#34;https://cloud.tsinghua.edu.cn/d/3927b67a834c475288e2/&#34;&gt;Tsinghua Cloud&lt;/a&gt; by running the command line below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python download.py retriever-training-data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Run the following command line to train the retriever. If you have downloaded the retriever training data in the default path, you can simply run the command line below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_retriever.py --train_data_dir ./download/retriever-training-data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Evaluation&lt;/h1&gt; &#xA;&lt;p&gt;You can reproduce our results on TriviaQA, WebQuestions and NQ Open. Take TriviaQA for example, you can simply run the command line below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/triviaqa.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and start running the experiment.&lt;/p&gt; &#xA;&lt;h1&gt;Real Application Cases&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases&#34;&gt;Here&lt;/a&gt; you can see some examples of WebGLM real application scenarios.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;When will the COVID-19 disappear?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases/0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;How to balance career and hobbies?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases/1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;FL Studio and Cubase, which is better?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases/2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;Is attention better than CNN?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases/3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;How to survive in the first-tier cities without a high-salary work?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases/4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;What do you think of version 3.5 of Genshin Impact?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases/5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;transformers are originated in NLP, but why they can be applied in CV?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases/6.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;Who proposed Music Transformer? How does it work?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases/7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;What is the backbone of Toolformer?&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/WebGLM/main/assets/cases/8.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{liu2023webglm,&#xA;      title={WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences},&#xA;      author={Xiao Liu and Hanyu Lai and Hao Yu and Yifan Xu and Aohan Zeng and Zhengxiao Du and Peng Zhang and Yuxiao Dong and Jie Tang},&#xA;      year={2023},&#xA;      eprint={2306.07906},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repo is simplified for easier deployment.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>turboderp/exllama</title>
    <updated>2023-07-02T02:03:18Z</updated>
    <id>tag:github.com,2023-07-02:/turboderp/exllama</id>
    <link href="https://github.com/turboderp/exllama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ExLlama&lt;/h1&gt; &#xA;&lt;p&gt;A standalone Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weights, designed to be fast and memory-efficient on modern GPUs.&lt;/p&gt; &#xA;&lt;p&gt;Disclaimer: The project is coming along, but it&#39;s still a work in progress!&lt;/p&gt; &#xA;&lt;h2&gt;Hardware requirements&lt;/h2&gt; &#xA;&lt;p&gt;I am developing on an RTX 4090 and an RTX 3090-Ti. Both cards support the CUDA kernels, but there might be incompatibilities with older cards.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.9 or newer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;torch&lt;/code&gt; tested on 2.0.1 and 2.1.0 (nightly) with cu118&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;safetensors&lt;/code&gt; 0.3.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sentencepiece&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ninja&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, only for the web UI:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;flask&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;waitress&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Linux/WSL prerequisites&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Windows prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;To run on Windows (without WSL):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;MSVC 2022&lt;/a&gt;. You can choose to install the whole &lt;code&gt;Visual Studio 2022&lt;/code&gt; IDE, or alternatively just the &lt;code&gt;Build Tools for Visual Studio 2022&lt;/code&gt; package (make sure &lt;code&gt;Desktop development with C++&lt;/code&gt; is ticked in the installer), it doesn&#39;t really matter which.&lt;/li&gt; &#xA; &lt;li&gt;Install the appropriate version of &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;, choosing one of the CUDA versions. I am developing on the nightly build, but the stable version (2.0.1) should also work.&lt;/li&gt; &#xA; &lt;li&gt;Install CUDA Toolkit, (&lt;a href=&#34;https://developer.nvidia.com/cuda-11-7-0-download-archive&#34;&gt;11.7&lt;/a&gt; and &lt;a href=&#34;https://developer.nvidia.com/cuda-11-8-0-download-archive&#34;&gt;11.8&lt;/a&gt; both seem to work, just make sure to match PyTorch&#39;s Compute Platform version).&lt;/li&gt; &#xA; &lt;li&gt;For best performance, enable Hardware Accelerated GPU Scheduling.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to&lt;/h2&gt; &#xA;&lt;p&gt;Install dependencies, clone repo and run benchmark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&#xA;git clone https://github.com/turboderp/exllama&#xA;cd exllama&#xA;&#xA;python test_benchmark_inference.py -d &amp;lt;path_to_model_files&amp;gt; -p -ppl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The CUDA extension is loaded at runtime so there&#39;s no need to install it separately. It will be compiled on the first run and cached to &lt;code&gt;~/.cache/torch_extensions/&lt;/code&gt; which could take a little while. If nothing happens at first, give it a minute to compile.&lt;/p&gt; &#xA;&lt;p&gt;Chatbot example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python example_chatbot.py -d &amp;lt;path_to_model_files&amp;gt; -un &#34;Jeff&#34; -p prompt_chatbort.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Web UI&lt;/h2&gt; &#xA;&lt;p&gt;I made a simple web UI for it. Like the rest of the project, it&#39;s a work in progress. Don&#39;t look at the JavaScript, it was mostly written by ChatGPT and it will haunt your dreams. But it sort of works, and it&#39;s kinda fun, especially multibot mode:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/turboderp/exllama/master/doc/_screenshot.jpg&#34; alt=&#34;_screenshot.jpg&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements-web.txt&#xA;&#xA;python webui/app.py -d &amp;lt;path_to_model_files&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that sessions are stored in &lt;code&gt;~/exllama_sessions/&lt;/code&gt;. You can change the location of the sessions storage with &lt;code&gt;-sd&lt;/code&gt; if you want.&lt;/p&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;For security benefits and easier deployment, it is also possible to run the web UI in an isolated docker container. Note: the docker image currently only supports NVIDIA GPUs.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It is recommended to run docker in &lt;a href=&#34;https://docs.docker.com/engine/security/rootless/&#34;&gt;rootless mode&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to build the docker image is using docker compose. First, set the &lt;code&gt;MODEL_PATH&lt;/code&gt; and &lt;code&gt;SESSIONS_PATH&lt;/code&gt; variables in the &lt;code&gt;.env&lt;/code&gt; file to the actual directories on the host. Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is also possible to manually build the image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t exllama-web .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: by default, the service inside the docker container is run by a non-root user. Hence, the ownership of bind-mounted directories (&lt;code&gt;/data/model&lt;/code&gt; and &lt;code&gt;/data/exllama_sessions&lt;/code&gt; in the default &lt;code&gt;docker-compose.yml&lt;/code&gt; file) is changed to this non-root user in the container entrypoint (&lt;code&gt;entrypoint.sh&lt;/code&gt;). To disable this, set &lt;code&gt;RUN_UID=0&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file if using &lt;code&gt;docker compose&lt;/code&gt;, or the following command if you manually build the image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t exllama-web --build-arg RUN_UID=0 .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;p&gt;Using docker compose:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The web UI can now be accessed on the host at &lt;a href=&#34;http://localhost:5000&#34;&gt;http://localhost:5000&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The configuration can be viewed in &lt;code&gt;docker-compose.yml&lt;/code&gt; and changed by creating a &lt;code&gt;docker-compose.override.yml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Run manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --gpus all -p 5000:5000 -v &amp;lt;path_to_model_dir&amp;gt;:/data/model/ -v &amp;lt;path_to_session_dir&amp;gt;:/data/exllama_sessions --rm -it exllama-web --host 0.0.0.0:5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results so far&lt;/h2&gt; &#xA;&lt;h3&gt;New implementation&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;grpsz&lt;/th&gt; &#xA;   &lt;th&gt;act&lt;/th&gt; &#xA;   &lt;th&gt;Seq. len.&lt;/th&gt; &#xA;   &lt;th&gt;VRAM&lt;/th&gt; &#xA;   &lt;th&gt;Prompt&lt;/th&gt; &#xA;   &lt;th&gt;Best&lt;/th&gt; &#xA;   &lt;th&gt;Worst&lt;/th&gt; &#xA;   &lt;th&gt;Ppl&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;5,194 MB&lt;/td&gt; &#xA;   &lt;td&gt;13,918 t/s&lt;/td&gt; &#xA;   &lt;td&gt;173 t/s&lt;/td&gt; &#xA;   &lt;td&gt;140 t/s&lt;/td&gt; &#xA;   &lt;td&gt;6.45&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;9,127 MB&lt;/td&gt; &#xA;   &lt;td&gt;7,507 t/s&lt;/td&gt; &#xA;   &lt;td&gt;102 t/s&lt;/td&gt; &#xA;   &lt;td&gt;86 t/s&lt;/td&gt; &#xA;   &lt;td&gt;5.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;20,795 MB&lt;/td&gt; &#xA;   &lt;td&gt;2,959 t/s&lt;/td&gt; &#xA;   &lt;td&gt;47 t/s&lt;/td&gt; &#xA;   &lt;td&gt;40 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;20,795 MB&lt;/td&gt; &#xA;   &lt;td&gt;2,784 t/s&lt;/td&gt; &#xA;   &lt;td&gt;45 t/s&lt;/td&gt; &#xA;   &lt;td&gt;37 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.55&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;1,550 t &lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;21,486 MB&lt;/td&gt; &#xA;   &lt;td&gt;2,636 t/s&lt;/td&gt; &#xA;   &lt;td&gt;41 t/s&lt;/td&gt; &#xA;   &lt;td&gt;37 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.52&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Koala&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;9,127 MB&lt;/td&gt; &#xA;   &lt;td&gt;5,529 t/s&lt;/td&gt; &#xA;   &lt;td&gt;93 t/s&lt;/td&gt; &#xA;   &lt;td&gt;79 t/s&lt;/td&gt; &#xA;   &lt;td&gt;6.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardLM&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;no &lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;20,199 MB&lt;/td&gt; &#xA;   &lt;td&gt;2,313 t/s&lt;/td&gt; &#xA;   &lt;td&gt;47 t/s&lt;/td&gt; &#xA;   &lt;td&gt;40 t/s&lt;/td&gt; &#xA;   &lt;td&gt;5.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Can not achieve full sequence length without OoM (yet)&lt;br&gt; &lt;sup&gt;2&lt;/sup&gt; Not quite sure if this is act-order or not. Weights have no group index, at least&lt;/p&gt; &#xA;&lt;p&gt;All tests done on stock RTX 4090 / 12900K, running with a desktop environment, with a few other apps also using VRAM.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&#34;Prompt&#34;&lt;/strong&gt; speed is inference over the sequence length listed minus 128 tokens. &lt;strong&gt;&#34;Worst&#34;&lt;/strong&gt; is the average speed for the last 128 tokens of the full context (worst case) and &lt;strong&gt;&#34;Best&#34;&lt;/strong&gt; lists the speed for the first 128 tokens in an empty sequence (best case.)&lt;/p&gt; &#xA;&lt;p&gt;VRAM usage is as reported by PyTorch and does not include PyTorch&#39;s own overhead (CUDA kernels, internal buffers etc.) This is somewhat unpredictable anyway. Best bet is to just optimize VRAM usage by the model, probably aiming for 20 GB on a 24 GB GPU to ensure there is room for a desktop environment and all of Torch&#39;s internals.&lt;/p&gt; &#xA;&lt;p&gt;Perplexity is measured only to verify that the models are working. The dataset used is a particular, small sample from WikiText, so scores are not necessarily comparable to other Llama benchmarks.&lt;/p&gt; &#xA;&lt;h3&gt;Dual GPU results&lt;/h3&gt; &#xA;&lt;p&gt;Since many seem to be interested in running 65B models, I can confirm that this works with two 24 GB GPUs. The following benchmarks are from a 4090 + 3090-Ti with &lt;code&gt;-gs 17.2,24&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;groupsize&lt;/th&gt; &#xA;   &lt;th&gt;act&lt;/th&gt; &#xA;   &lt;th&gt;Seq. len.&lt;/th&gt; &#xA;   &lt;th&gt;VRAM&lt;/th&gt; &#xA;   &lt;th&gt;Prompt&lt;/th&gt; &#xA;   &lt;th&gt;Best&lt;/th&gt; &#xA;   &lt;th&gt;Worst&lt;/th&gt; &#xA;   &lt;th&gt;Ppl&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;39,804 MB&lt;/td&gt; &#xA;   &lt;td&gt;1,109 t/s&lt;/td&gt; &#xA;   &lt;td&gt;20 t/s&lt;/td&gt; &#xA;   &lt;td&gt;18 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;43,424 MB&lt;/td&gt; &#xA;   &lt;td&gt;1,037 t/s&lt;/td&gt; &#xA;   &lt;td&gt;17 t/s&lt;/td&gt; &#xA;   &lt;td&gt;16 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Testing long sequences&lt;/h3&gt; &#xA;&lt;p&gt;The following tests were all done on &lt;strong&gt;33B/65B, 4bit 128g&lt;/strong&gt; with various settings, just to test the max sequence length and get a sense of what can be achieved with different or multiple GPUs right now. Llama goes incoherent generating past 2048 tokens anyway, but with some fine-tuning, who knows? Note that these tests were run a while ago and the speeds are no longer current.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Seq. len.&lt;/th&gt; &#xA;   &lt;th&gt;VRAM&lt;/th&gt; &#xA;   &lt;th&gt;Long seq.&lt;/th&gt; &#xA;   &lt;th&gt;Ind.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4090/24GB&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;2,516 t&lt;/td&gt; &#xA;   &lt;td&gt;22,145 MB&lt;/td&gt; &#xA;   &lt;td&gt;1140 t/s&lt;/td&gt; &#xA;   &lt;td&gt;28 t/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4090/24GB + 3070Ti/8GB&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;3,932 t&lt;/td&gt; &#xA;   &lt;td&gt;22,055 MB + 7,377 MB&lt;/td&gt; &#xA;   &lt;td&gt;840 t/s&lt;/td&gt; &#xA;   &lt;td&gt;22 t/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A6000/48GB (headless)&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;9,032 t&lt;/td&gt; &#xA;   &lt;td&gt;46,863 MB&lt;/td&gt; &#xA;   &lt;td&gt;645 t/s&lt;/td&gt; &#xA;   &lt;td&gt;12 t/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A100/80GB (headless)&lt;/td&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;9,520 t&lt;/td&gt; &#xA;   &lt;td&gt;79,009 MB&lt;/td&gt; &#xA;   &lt;td&gt;650 t/s&lt;/td&gt; &#xA;   &lt;td&gt;9 t/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;p&gt;Moved the todo list &lt;a href=&#34;https://raw.githubusercontent.com/turboderp/exllama/master/doc/TODO.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;I downloaded a whole bunch of GPTQ models to test compatibility. &lt;a href=&#34;https://raw.githubusercontent.com/turboderp/exllama/master/doc/model_compatibility.md&#34;&gt;Here&lt;/a&gt; is the list of models confirmed to be working right now.&lt;/p&gt; &#xA;&lt;h2&gt;Recent updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-02&lt;/strong&gt;: Web UI is now in a fairly working state. Expect it to be a little scuffed in places. There will be a rewrite at some point to make the client-side code less seizure-inducing. It has multibot mode, chat rewind and editing features, sessions, and more. I&#39;m going to build it out with support for instruct prompting and such, in time.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-04&lt;/strong&gt;: Refactored a whole bunch to move more of the work into the extension, setting up for more tuning options to come soon and eventually auto tuning. Also optimized a little, for about a 5% speedup.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-06&lt;/strong&gt;: Some minor optimizations. Also it should now compile the extension more easily and run more seamlessly on Windows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-09&lt;/strong&gt;: Fused most of the self-attention step. More to come. Slight speedup already, but more importantly went from 69% actual CPU utilization to 37%. This should do a lot to address the bottleneck on CPUs with lower single-threaded performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-10&lt;/strong&gt;: Docker support now! And some minor optimizations. Cleaned up the project a bit.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-11&lt;/strong&gt;: Added some concurrency a couple of places. It&#39;s only beneficial on the 4090, on small models where the cores are somewhat underutilized and the L2 cache can keep up. For the 3090 it&#39;s detrimental to performance, so it&#39;s disabled by default. YMMV. Use &lt;code&gt;-cs&lt;/code&gt; to try it out.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-17&lt;/strong&gt;: Fixed a nasty bug in the fused attention that was causing slightly incorrect cache states on 13B and 33B models. You definitely want to update.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-18&lt;/strong&gt;: LoRA support now. Still needs a lot of testing and some optimization, and currently you can&#39;t stack multiple LoRAs during the same inference. There&#39;s also no support in the web UI yet.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Stability-AI/generative-models</title>
    <updated>2023-07-02T02:03:18Z</updated>
    <id>tag:github.com,2023-07-02:/Stability-AI/generative-models</id>
    <link href="https://github.com/Stability-AI/generative-models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative Models by Stability AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Generative Models by Stability AI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/generative-models/main/assets/000.jpg&#34; alt=&#34;sample1&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;June 22, 2023&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We are releasing two new diffusion models for research purposes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;SD-XL 0.9-base&lt;/code&gt;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The base model uses &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;OpenCLIP-ViT/G&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/CLIP/tree/main&#34;&gt;CLIP-ViT/L&lt;/a&gt; for text encoding whereas the refiner model only uses the OpenCLIP model.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SD-XL 0.9-refiner&lt;/code&gt;: The refiner has been trained to denoise small noise levels of high quality data and as such is not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you would like to access these models for your research, please apply using one of the following links: &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9&#34;&gt;SDXL-0.9-Base model&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9&#34;&gt;SDXL-0.9-Refiner&lt;/a&gt;. This means that you can apply for any of the two links - and if you are granted - you can access both. Please log in to your Hugging Face Account with your organization email to request access. &lt;strong&gt;We plan to do a full release soon (July).&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;The codebase&lt;/h2&gt; &#xA;&lt;h3&gt;General Philosophy&lt;/h3&gt; &#xA;&lt;p&gt;Modularity is king. This repo implements a config-driven approach where we build and combine submodules by calling &lt;code&gt;instantiate_from_config()&lt;/code&gt; on objects defined in yaml configs. See &lt;code&gt;configs/&lt;/code&gt; for many examples.&lt;/p&gt; &#xA;&lt;h3&gt;Changelog from the old &lt;code&gt;ldm&lt;/code&gt; codebase&lt;/h3&gt; &#xA;&lt;p&gt;For training, we use &lt;a href=&#34;https://www.pytorchlightning.ai/index.html&#34;&gt;pytorch-lightning&lt;/a&gt;, but it should be easy to use other training wrappers around the base modules. The core diffusion model class (formerly &lt;code&gt;LatentDiffusion&lt;/code&gt;, now &lt;code&gt;DiffusionEngine&lt;/code&gt;) has been cleaned up:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial conditionings, and all combinations thereof) in a single class: &lt;code&gt;GeneralConditioner&lt;/code&gt;, see &lt;code&gt;sgm/modules/encoders/modules.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We separate guiders (such as classifier-free guidance, see &lt;code&gt;sgm/modules/diffusionmodules/guiders.py&lt;/code&gt;) from the samplers (&lt;code&gt;sgm/modules/diffusionmodules/sampling.py&lt;/code&gt;), and the samplers are independent of the model.&lt;/li&gt; &#xA; &lt;li&gt;We adopt the &lt;a href=&#34;https://arxiv.org/abs/2206.00364&#34;&gt;&#34;denoiser framework&#34;&lt;/a&gt; for both training and inference (most notable change is probably now the option to train continuous time models): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Discrete times models (denoisers) are simply a special case of continuous time models (denoisers); see &lt;code&gt;sgm/modules/diffusionmodules/denoiser.py&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The following features are now independent: weighting of the diffusion loss function (&lt;code&gt;sgm/modules/diffusionmodules/denoiser_weighting.py&lt;/code&gt;), preconditioning of the network (&lt;code&gt;sgm/modules/diffusionmodules/denoiser_scaling.py&lt;/code&gt;), and sampling of noise levels during training (&lt;code&gt;sgm/modules/diffusionmodules/sigma_sampling.py&lt;/code&gt;).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Autoencoding models have also been cleaned up.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a name=&#34;installation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1. Clone the repo&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:Stability-AI/generative-models.git&#xA;cd generative-models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Setting up the virtualenv&lt;/h4&gt; &#xA;&lt;p&gt;This is assuming you have navigated to the &lt;code&gt;generative-models&lt;/code&gt; root after cloning it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This is tested under &lt;code&gt;python3.8&lt;/code&gt; and &lt;code&gt;python3.10&lt;/code&gt;. For other python versions, you might encounter version conflicts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyTorch 1.13&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# install required packages from pypi&#xA;python3 -m venv .pt1&#xA;source .pt1/bin/activate&#xA;pip3 install wheel&#xA;pip3 install -r requirements_pt13.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyTorch 2.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# install required packages from pypi&#xA;python3 -m venv .pt2&#xA;source .pt2/bin/activate&#xA;pip3 install wheel&#xA;pip3 install -r requirements_pt2.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference:&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://streamlit.io/&#34;&gt;streamlit&lt;/a&gt; demo for text-to-image and image-to-image sampling in &lt;code&gt;scripts/demo/sampling.py&lt;/code&gt;. The following models are currently supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9&#34;&gt;SD-XL 0.9-base&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9&#34;&gt;SD-XL 0.9-refiner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors&#34;&gt;SD 2.1-512&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors&#34;&gt;SD 2.1-768&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Weights for SDXL&lt;/strong&gt;: If you would like to access these models for your research, please apply using one of the following links: &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9&#34;&gt;SDXL-0.9-Base model&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9&#34;&gt;SDXL-0.9-Refiner&lt;/a&gt;. This means that you can apply for any of the two links - and if you are granted - you can access both. Please log in to your Hugging Face Account with your organization email to request access.&lt;/p&gt; &#xA;&lt;p&gt;After obtaining the weights, place them into &lt;code&gt;checkpoints/&lt;/code&gt;. Next, start the demo using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamlit run scripts/demo/sampling.py --server.port &amp;lt;your_port&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Invisible Watermark Detection&lt;/h3&gt; &#xA;&lt;p&gt;Images generated with our code use the &lt;a href=&#34;https://github.com/ShieldMnt/invisible-watermark/&#34;&gt;invisible-watermark&lt;/a&gt; library to embed an invisible watermark into the model output. We also provide a script to easily detect that watermark. Please note that this watermark is not the same as in previous Stable Diffusion 1.x/2.x versions.&lt;/p&gt; &#xA;&lt;p&gt;To run the script you need to either have a working installation as above or try an &lt;em&gt;experimental&lt;/em&gt; import using only a minimal amount of packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .detect&#xA;source .detect/bin/activate&#xA;&#xA;pip install &#34;numpy&amp;gt;=1.17&#34; &#34;PyWavelets&amp;gt;=1.1.1&#34; &#34;opencv-python&amp;gt;=4.1.0.25&#34;&#xA;pip install --no-deps invisible-watermark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the script you need to have a working installation as above. The script is then useable in the following ways (don&#39;t forget to activate your virtual environment beforehand, e.g. &lt;code&gt;source .pt1/bin/activate&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# test a single file&#xA;python scripts/demo/detect.py &amp;lt;your filename here&amp;gt;&#xA;# test multiple files at once&#xA;python scripts/demo/detect.py &amp;lt;filename 1&amp;gt; &amp;lt;filename 2&amp;gt; ... &amp;lt;filename n&amp;gt;&#xA;# test all files in a specific folder&#xA;python scripts/demo/detect.py &amp;lt;your folder name here&amp;gt;/*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training:&lt;/h2&gt; &#xA;&lt;p&gt;We are providing example training configs in &lt;code&gt;configs/example_training&lt;/code&gt;. To launch a training, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --base configs/&amp;lt;config1.yaml&amp;gt; configs/&amp;lt;config2.yaml&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where configs are merged from left to right (later configs overwrite the same values). This can be used to combine model, training and data configs. However, all of them can also be defined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py --base configs/example_training/toy/mnist_cond.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE 1:&lt;/strong&gt; Using the non-toy-dataset configs &lt;code&gt;configs/example_training/imagenet-f8_cond.yaml&lt;/code&gt;, &lt;code&gt;configs/example_training/txt2img-clipl.yaml&lt;/code&gt; and &lt;code&gt;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&lt;/code&gt; for training will require edits depending on the used dataset (which is expected to stored in tar-file in the &lt;a href=&#34;https://github.com/webdataset/webdataset&#34;&gt;webdataset-format&lt;/a&gt;). To find the parts which have to be adapted, search for comments containing &lt;code&gt;USER:&lt;/code&gt; in the respective config.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE 2:&lt;/strong&gt; This repository supports both &lt;code&gt;pytorch1.13&lt;/code&gt; and &lt;code&gt;pytorch2&lt;/code&gt;for training generative models. However for autoencoder training as e.g. in &lt;code&gt;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&lt;/code&gt;, only &lt;code&gt;pytorch1.13&lt;/code&gt; is supported.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE 3:&lt;/strong&gt; Training latent generative models (as e.g. in &lt;code&gt;configs/example_training/imagenet-f8_cond.yaml&lt;/code&gt;) requires retrieving the checkpoint from &lt;a href=&#34;https://huggingface.co/stabilityai/sdxl-vae/tree/main&#34;&gt;Hugging Face&lt;/a&gt; and replacing the &lt;code&gt;CKPT_PATH&lt;/code&gt; placeholder in &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/generative-models/main/configs/example_training/imagenet-f8_cond.yaml#81&#34;&gt;this line&lt;/a&gt;. The same is to be done for the provided text-to-image configs.&lt;/p&gt; &#xA;&lt;h3&gt;Building New Diffusion Models&lt;/h3&gt; &#xA;&lt;h4&gt;Conditioner&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;GeneralConditioner&lt;/code&gt; is configured through the &lt;code&gt;conditioner_config&lt;/code&gt;. Its only attribute is &lt;code&gt;emb_models&lt;/code&gt;, a list of different embedders (all inherited from &lt;code&gt;AbstractEmbModel&lt;/code&gt;) that are used to condition the generative model. All embedders should define whether or not they are trainable (&lt;code&gt;is_trainable&lt;/code&gt;, default &lt;code&gt;False&lt;/code&gt;), a classifier-free guidance dropout rate is used (&lt;code&gt;ucg_rate&lt;/code&gt;, default &lt;code&gt;0&lt;/code&gt;), and an input key (&lt;code&gt;input_key&lt;/code&gt;), for example, &lt;code&gt;txt&lt;/code&gt; for text-conditioning or &lt;code&gt;cls&lt;/code&gt; for class-conditioning. When computing conditionings, the embedder will get &lt;code&gt;batch[input_key]&lt;/code&gt; as input. We currently support two to four dimensional conditionings and conditionings of different embedders are concatenated appropriately. Note that the order of the embedders in the &lt;code&gt;conditioner_config&lt;/code&gt; is important.&lt;/p&gt; &#xA;&lt;h4&gt;Network&lt;/h4&gt; &#xA;&lt;p&gt;The neural network is set through the &lt;code&gt;network_config&lt;/code&gt;. This used to be called &lt;code&gt;unet_config&lt;/code&gt;, which is not general enough as we plan to experiment with transformer-based diffusion backbones.&lt;/p&gt; &#xA;&lt;h4&gt;Loss&lt;/h4&gt; &#xA;&lt;p&gt;The loss is configured through &lt;code&gt;loss_config&lt;/code&gt;. For standard diffusion model training, you will have to set &lt;code&gt;sigma_sampler_config&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Sampler config&lt;/h4&gt; &#xA;&lt;p&gt;As discussed above, the sampler is independent of the model. In the &lt;code&gt;sampler_config&lt;/code&gt;, we set the type of numerical solver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free guidance.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Handling&lt;/h3&gt; &#xA;&lt;p&gt;For large scale training we recommend using the data pipelines from our &lt;a href=&#34;https://github.com/Stability-AI/datapipelines&#34;&gt;data pipelines&lt;/a&gt; project. The project is contained in the requirement and automatically included when following the steps from the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/generative-models/main/#installation&#34;&gt;Installation section&lt;/a&gt;. Small map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of data keys/values, e.g.,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;example = {&#34;jpg&#34;: x,  # this is a tensor -1...1 chw&#xA;           &#34;txt&#34;: &#34;a beautiful image&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where we expect images in -1...1, channel-first format.&lt;/p&gt;</summary>
  </entry>
</feed>