<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-08T01:44:48Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FoundationVision/VAR</title>
    <updated>2024-12-08T01:44:48Z</updated>
    <id>tag:github.com,2024-12-08:/FoundationVision/VAR</id>
    <link href="https://github.com/FoundationVision/VAR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[NeurIPS 2024 Oral][GPT beats diffusionüî•] [scaling laws in visual generationüìà] Official impl. of &#34;Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction&#34;. An *ultra-simple, user-friendly yet state-of-the-art* codebase for autoregressive image generation!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VAR: a new visual generation method elevates GPT-style models beyond diffusionüöÄ &amp;amp; Scaling laws observedüìà&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://var.vision/demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Play%20with%20VAR%21-VAR%20demo%20platform-lightblue&#34; alt=&#34;demo platform&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv%20paper-2404.02905-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://huggingface.co/FoundationVision/var&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Weights-FoundationVision/var-yellow&#34; alt=&#34;huggingface weights&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://paperswithcode.com/sota/image-generation-on-imagenet-256x256?tag_filter=485&amp;amp;p=visual-autoregressive-modeling-scalable-image&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/State%20of%20the%20Art-Image%20Generation%20on%20ImageNet%20%28AR%29-32B1B4?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iNjA2IiBoZWlnaHQ9IjYwNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgb3ZlcmZsb3c9ImhpZGRlbiI%2BPGRlZnM%2BPGNsaXBQYXRoIGlkPSJjbGlwMCI%2BPHJlY3QgeD0iLTEiIHk9Ii0xIiB3aWR0aD0iNjA2IiBoZWlnaHQ9IjYwNiIvPjwvY2xpcFBhdGg%2BPC9kZWZzPjxnIGNsaXAtcGF0aD0idXJsKCNjbGlwMCkiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDEgMSkiPjxyZWN0IHg9IjUyOSIgeT0iNjYiIHdpZHRoPSI1NiIgaGVpZ2h0PSI0NzMiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIxOSIgeT0iNjYiIHdpZHRoPSI1NyIgaGVpZ2h0PSI0NzMiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIyNzQiIHk9IjE1MSIgd2lkdGg9IjU3IiBoZWlnaHQ9IjMwMiIgZmlsbD0iIzQ0RjJGNiIvPjxyZWN0IHg9IjEwNCIgeT0iMTUxIiB3aWR0aD0iNTciIGhlaWdodD0iMzAyIiBmaWxsPSIjNDRGMkY2Ii8%2BPHJlY3QgeD0iNDQ0IiB5PSIxNTEiIHdpZHRoPSI1NyIgaGVpZ2h0PSIzMDIiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIzNTkiIHk9IjE3MCIgd2lkdGg9IjU2IiBoZWlnaHQ9IjI2NCIgZmlsbD0iIzQ0RjJGNiIvPjxyZWN0IHg9IjE4OCIgeT0iMTcwIiB3aWR0aD0iNTciIGhlaWdodD0iMjY0IiBmaWxsPSIjNDRGMkY2Ii8%2BPHJlY3QgeD0iNzYiIHk9IjY2IiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI0ODIiIHk9IjY2IiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI3NiIgeT0iNDgyIiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI0ODIiIHk9IjQ4MiIgd2lkdGg9IjQ3IiBoZWlnaHQ9IjU3IiBmaWxsPSIjNDRGMkY2Ii8%2BPC9nPjwvc3ZnPg%3D%3D&#34; alt=&#34;SOTA&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;font-size: larger;&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34; style=&#34;font-size: larger;&#34;&gt; &lt;strong&gt;NeurIPS 2024 Oral&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/9850df90-20b1-4f29-8592-e3526d16d755&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-12:&lt;/strong&gt; We Release our Text-to-Image research based on VAR, please check &lt;a href=&#34;https://arxiv.org/abs/2412.04431&#34;&gt;Infinity&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-09:&lt;/strong&gt; VAR is accepted as &lt;strong&gt;NeurIPS 2024 Oral&lt;/strong&gt; Presentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-04:&lt;/strong&gt; &lt;a href=&#34;https://github.com/FoundationVision/VAR&#34;&gt;Visual AutoRegressive modeling&lt;/a&gt; is released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üïπÔ∏è Try and Play with VAR!&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://var.vision/demo&#34;&gt;demo website&lt;/a&gt; for you to play with VAR models and generate images interactively. Enjoy the fun of visual autoregressive modeling!&lt;/p&gt; &#xA;&lt;p&gt;We also provide &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/demo_sample.ipynb&#34;&gt;demo_sample.ipynb&lt;/a&gt; for you to see more technical details about VAR.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New?&lt;/h2&gt; &#xA;&lt;h3&gt;üî• Introducing VAR: a new paradigm in autoregressive visual generation‚ú®:&lt;/h3&gt; &#xA;&lt;p&gt;Visual Autoregressive Modeling (VAR) redefines the autoregressive learning on images as coarse-to-fine &#34;next-scale prediction&#34; or &#34;next-resolution prediction&#34;, diverging from the standard raster-scan &#34;next-token prediction&#34;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/3e12655c-37dc-4528-b923-ec6c4cfef178&#34; width=&#34;93%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• For the first time, GPT-style autoregressive models surpass diffusion modelsüöÄ:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/cc30b043-fa4e-4d01-a9b1-e50650d5675d&#34; width=&#34;55%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• Discovering power-law Scaling Laws in VAR transformersüìà:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/c35fb56e-896e-4e4b-9fb9-7a1c38513804&#34; width=&#34;85%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/91d7b92c-8fc3-44d9-8fb4-73d6cdb8ec1e&#34; width=&#34;85%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• Zero-shot generalizabilityüõ†Ô∏è:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/a54a4e52-6793-4130-bae2-9e459a08e96a&#34; width=&#34;70%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h4&gt;For a deep dive into our analyses, discussions, and evaluations, check out our &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;paper&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;h2&gt;VAR zoo&lt;/h2&gt; &#xA;&lt;p&gt;We provide VAR models for you to play with, which are on &lt;a href=&#34;https://huggingface.co/FoundationVision/var&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Huggingface-FoundationVision/var-yellow&#34;&gt;&lt;/a&gt; or can be downloaded from the following links:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;reso.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FID&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;rel. cost&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;HF weightsü§ó&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.55&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;310M&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d16.pth&#34;&gt;var_d16.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;600M&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d20.pth&#34;&gt;var_d20.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d24.pth&#34;&gt;var_d24.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d30.pth&#34;&gt;var_d30.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d30-re&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.80&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d30.pth&#34;&gt;var_d30.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can load these models to generate images via the codes in &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/demo_sample.ipynb&#34;&gt;demo_sample.ipynb&lt;/a&gt;. Note: you need to download &lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/vae_ch160v4096z32.pth&#34;&gt;vae_ch160v4096z32.pth&lt;/a&gt; first.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;torch&amp;gt;=2.0.0&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install other pip packages via &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Prepare the &lt;a href=&#34;http://image-net.org/&#34;&gt;ImageNet&lt;/a&gt; dataset&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt; assume the ImageNet is in `/path/to/imagenet`. It should be like this:&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code&gt;/path/to/imagenet/:&#xA;    train/:&#xA;        n01440764: &#xA;            many_images.JPEG ...&#xA;        n01443537:&#xA;            many_images.JPEG ...&#xA;    val/:&#xA;        n01440764:&#xA;            ILSVRC2012_val_00000293.JPEG ...&#xA;        n01443537:&#xA;            ILSVRC2012_val_00000236.JPEG ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;&lt;strong&gt;NOTE: The arg &lt;code&gt;--data_path=/path/to/imagenet&lt;/code&gt; should be passed to the training script.&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) install and compile &lt;code&gt;flash-attn&lt;/code&gt; and &lt;code&gt;xformers&lt;/code&gt; for faster attention computation. Our code will automatically use them if installed. See &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/models/basic_var.py#L15-L30&#34;&gt;models/basic_var.py#L15-L30&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Training Scripts&lt;/h2&gt; &#xA;&lt;p&gt;To train VAR-{d16, d20, d24, d30, d36-s} on ImageNet 256x256 or 512x512, you can run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# d16, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1&#xA;# d20, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=20 --bs=768 --ep=250 --fp16=1 --alng=1e-3 --wpe=0.1&#xA;# d24, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=24 --bs=768 --ep=350 --tblr=8e-5 --fp16=1 --alng=1e-4 --wpe=0.01&#xA;# d30, 256x256&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=30 --bs=1024 --ep=350 --tblr=8e-5 --fp16=1 --alng=1e-5 --wpe=0.01 --twde=0.08&#xA;# d36-s, 512x512 (-s means saln=1, shared AdaLN)&#xA;torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py \&#xA;  --depth=36 --saln=1 --pn=512 --bs=768 --ep=350 --tblr=8e-5 --fp16=1 --alng=5e-6 --wpe=0.01 --twde=0.08&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A folder named &lt;code&gt;local_output&lt;/code&gt; will be created to save the checkpoints and logs. You can monitor the training process by checking the logs in &lt;code&gt;local_output/log.txt&lt;/code&gt; and &lt;code&gt;local_output/stdout.txt&lt;/code&gt;, or using &lt;code&gt;tensorboard --logdir=local_output/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If your experiment is interrupted, just rerun the command, and the training will &lt;strong&gt;automatically resume&lt;/strong&gt; from the last checkpoint in &lt;code&gt;local_output/ckpt*.pth&lt;/code&gt; (see &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/utils/misc.py#L344-L357&#34;&gt;utils/misc.py#L344-L357&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Sampling &amp;amp; Zero-shot Inference&lt;/h2&gt; &#xA;&lt;p&gt;For FID evaluation, use &lt;code&gt;var.autoregressive_infer_cfg(..., cfg=1.5, top_p=0.96, top_k=900, more_smooth=False)&lt;/code&gt; to sample 50,000 images (50 per class) and save them as PNG (not JPEG) files in a folder. Pack them into a &lt;code&gt;.npz&lt;/code&gt; file via &lt;code&gt;create_npz_from_sample_folder(sample_folder)&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/utils/misc.py#L360&#34;&gt;utils/misc.py#L344&lt;/a&gt;. Then use the &lt;a href=&#34;https://github.com/openai/guided-diffusion/tree/main/evaluations&#34;&gt;OpenAI&#39;s FID evaluation toolkit&lt;/a&gt; and reference ground truth npz file of &lt;a href=&#34;https://openaipublic.blob.core.windows.net/diffusion/jul-2021/ref_batches/imagenet/256/VIRTUAL_imagenet256_labeled.npz&#34;&gt;256x256&lt;/a&gt; or &lt;a href=&#34;https://openaipublic.blob.core.windows.net/diffusion/jul-2021/ref_batches/imagenet/512/VIRTUAL_imagenet512.npz&#34;&gt;512x512&lt;/a&gt; to evaluate FID, IS, precision, and recall.&lt;/p&gt; &#xA;&lt;p&gt;Note a relatively small &lt;code&gt;cfg=1.5&lt;/code&gt; is used for trade-off between image quality and diversity. You can adjust it to &lt;code&gt;cfg=5.0&lt;/code&gt;, or sample with &lt;code&gt;autoregressive_infer_cfg(..., more_smooth=True)&lt;/code&gt; for &lt;strong&gt;better visual quality&lt;/strong&gt;. We&#39;ll provide the sampling script later.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If our work assists your research, feel free to give us a star ‚≠ê or cite us using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Article{VAR,&#xA;      title={Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction}, &#xA;      author={Keyu Tian and Yi Jiang and Zehuan Yuan and Bingyue Peng and Liwei Wang},&#xA;      year={2024},&#xA;      eprint={2404.02905},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>stanfordnlp/dspy</title>
    <updated>2024-12-08T01:44:48Z</updated>
    <id>tag:github.com,2024-12-08:/stanfordnlp/dspy</id>
    <link href="https://github.com/stanfordnlp/dspy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DSPy: The framework for programming‚Äînot prompting‚Äîlanguage models&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/static/img/dspy_logo.png&#34; width=&#34;460px&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;DSPy: &lt;em&gt;Programming&lt;/em&gt;‚Äînot prompting‚ÄîFoundation Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href=&#34;https://dspy.ai/&#34;&gt;DSPy Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pepy.tech/project/dspy-ai&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/dspy-ai&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/dspy-ai&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/dspy-ai/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;DSPy is the open-source framework for &lt;em&gt;programming‚Äîrather than prompting‚Äîlanguage models&lt;/em&gt;. It allows you to iterate fast on &lt;strong&gt;building modular AI systems&lt;/strong&gt; and provides algorithms for &lt;strong&gt;optimizing their prompts and weights&lt;/strong&gt;, whether you&#39;re building simple classifiers, sophisticated RAG pipelines, or Agent loops.&lt;/p&gt; &#xA;&lt;p&gt;DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional &lt;em&gt;Python code&lt;/em&gt; and use DSPy&#39;s tools to &lt;strong&gt;teach your LM to deliver high-quality outputs&lt;/strong&gt;. This &lt;a href=&#34;https://www.youtube.com/watch?v=JEMYuzrKLUw&#34;&gt;lecture&lt;/a&gt; is a good conceptual introduction. Meet the community, seek help, or start contributing via our GitHub repo here and our &lt;a href=&#34;https://discord.gg/XCGy2WDCQB&#34;&gt;Discord server&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation: &lt;a href=&#34;https://dspy.ai&#34;&gt;dspy.ai&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please go to the &lt;a href=&#34;https://dspy.ai&#34;&gt;DSPy Docs at dspy.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dspy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install the very latest from &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/stanfordnlp/dspy.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìú Citation &amp;amp; Reading More&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Jun&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2406.11695&#34;&gt;Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;[Oct&#39;23] &lt;a href=&#34;https://arxiv.org/abs/2310.03714&#34;&gt;DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; [Jul&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2407.10930&#34;&gt;Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together&lt;/a&gt;&lt;br&gt; [Jun&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2406.11706&#34;&gt;Prompts as Auto-Optimized Training Hyperparameters&lt;/a&gt;&lt;br&gt; [Feb&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models&lt;/a&gt;&lt;br&gt; [Jan&#39;24] &lt;a href=&#34;https://arxiv.org/abs/2401.12178&#34;&gt;In-Context Learning for Extreme Multi-Label Classification&lt;/a&gt;&lt;br&gt; [Dec&#39;23] &lt;a href=&#34;https://arxiv.org/abs/2312.13382&#34;&gt;DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines&lt;/a&gt;&lt;br&gt; [Dec&#39;22] &lt;a href=&#34;https://arxiv.org/abs/2212.14024.pdf&#34;&gt;Demonstrate-Search-Predict: Composing Retrieval &amp;amp; Language Models for Knowledge-Intensive NLP&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To stay up to date or learn more, follow &lt;a href=&#34;https://twitter.com/lateinteraction&#34;&gt;@lateinteraction&lt;/a&gt; on Twitter.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;DSPy&lt;/strong&gt; logo is designed by &lt;strong&gt;Chuyi Zhang&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you use DSPy or DSP in a research paper, please cite our work as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{khattab2024dspy,&#xA;  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},&#xA;  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},&#xA;  journal={The Twelfth International Conference on Learning Representations},&#xA;  year={2024}&#xA;}&#xA;@article{khattab2022demonstrate,&#xA;  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},&#xA;  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},&#xA;  journal={arXiv preprint arXiv:2212.14024},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:&#xA;&#xA;* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) &#xA;* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) &#xA;* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)&#xA;* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)&#xA;* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)&#xA;* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) --&gt;</summary>
  </entry>
  <entry>
    <title>Guovin/iptv-api</title>
    <updated>2024-12-08T01:44:48Z</updated>
    <id>tag:github.com,2024-12-08:/Guovin/iptv-api</id>
    <link href="https://github.com/Guovin/iptv-api" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üì∫IPTVÁîµËßÜÁõ¥Êí≠Ê∫êÊõ¥Êñ∞Â∑•ÂÖ∑üöÄÔºö‚ú®Â§ÆËßÜ„ÄÅüì°Âç´ËßÜ„ÄÅ‚òòÔ∏èÂπø‰∏úÂèäÂêÑÁúÅ‰ªΩÂú∞ÊñπÂè∞„ÄÅüåäÊ∏Ø¬∑Êæ≥¬∑Âè∞„ÄÅüé¨ÁîµÂΩ±„ÄÅüé•Âí™Âíï„ÄÅüèÄ‰ΩìËÇ≤„ÄÅü™ÅÂä®Áîª„ÄÅüéÆÊ∏∏Êàè„ÄÅüéµÈü≥‰πê„ÄÅüèõÁªèÂÖ∏ÂâßÂú∫ÔºõÊîØÊåÅIPv4/IPv6ÔºõÊîØÊåÅËá™ÂÆö‰πâÂ¢ûÂä†È¢ëÈÅìÔºõÊîØÊåÅÁªÑÊí≠Ê∫ê„ÄÅÈÖíÂ∫óÊ∫ê„ÄÅËÆ¢ÈòÖÊ∫ê„ÄÅÂÖ≥ÈîÆÂ≠óÊêúÁ¥¢ÔºõÊØèÂ§©Ëá™Âä®Êõ¥Êñ∞‰∏§Ê¨°ÔºåÁªìÊûúÂèØÁî®‰∫éTVBoxÁ≠âÊí≠ÊîæËΩØ‰ª∂ÔºõÊîØÊåÅÂ∑•‰ΩúÊµÅ„ÄÅDocker(amd64/arm64/arm v7)„ÄÅÂëΩ‰ª§Ë°å„ÄÅGUIËøêË°åÊñπÂºè | IPTV live TV source update tool&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/static/images/logo.png&#34; alt=&#34;logo&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;IPTV-API&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; Ëá™ÂÆö‰πâÈ¢ëÈÅìÔºåËá™Âä®Ëé∑ÂèñÁõ¥Êí≠Ê∫êÊé•Âè£ÔºåÊµãÈÄüÈ™åÊïàÂêéÁîüÊàêÂèØÁî®ÁöÑÁªìÊûú&#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; ÈªòËÆ§ÁªìÊûúÂåÖÂê´Ôºöüì∫Â§ÆËßÜÈ¢ëÈÅì„ÄÅüí∞Â§ÆËßÜ‰ªòË¥πÈ¢ëÈÅì„ÄÅüì°Âç´ËßÜÈ¢ëÈÅì„ÄÅüè†Âπø‰∏úÈ¢ëÈÅì„ÄÅüåäÊ∏Ø¬∑Êæ≥¬∑Âè∞È¢ëÈÅì„ÄÅüé¨ÁîµÂΩ±È¢ëÈÅì„ÄÅüé•Âí™ÂíïÁõ¥Êí≠„ÄÅüèÄ‰ΩìËÇ≤È¢ëÈÅì„ÄÅü™ÅÂä®ÁîªÈ¢ëÈÅì„ÄÅüéÆÊ∏∏ÊàèÈ¢ëÈÅì„ÄÅüéµÈü≥‰πêÈ¢ëÈÅì„ÄÅüèõÁªèÂÖ∏ÂâßÂú∫&#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;ÂÖ∑‰ΩìÈ¢ëÈÅì&lt;/summary&gt; &#xA; &lt;div&gt;&#xA;   üì∫Â§ÆËßÜÈ¢ëÈÅì: CCTV-1, CCTV-2, CCTV-3, CCTV-4, CCTV-5, CCTV-5+, CCTV-6, CCTV-7, CCTV-8, CCTV-9, CCTV-10, CCTV-11, CCTV-12, CCTV-13, CCTV-14, CCTV-15, CCTV-16, CCTV-17, CETV1, CETV2, CETV4, CETV5 &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   üí∞Â§ÆËßÜ‰ªòË¥πÈ¢ëÈÅì: ÊñáÂåñÁ≤æÂìÅ, Â§ÆËßÜÂè∞ÁêÉ, È£é‰∫ëÈü≥‰πê, Á¨¨‰∏ÄÂâßÂú∫, È£é‰∫ëÂâßÂú∫, ÊÄÄÊóßÂâßÂú∫, Â•≥ÊÄßÊó∂Â∞ö, È´òÂ∞îÂ§´ÁΩëÁêÉ, È£é‰∫ëË∂≥ÁêÉ, ÁîµËßÜÊåáÂçó, ‰∏ñÁïåÂú∞ÁêÜ, ÂÖµÂô®ÁßëÊäÄ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   üì°Âç´ËßÜÈ¢ëÈÅì: Âπø‰∏úÂç´ËßÜ, È¶ôÊ∏ØÂç´ËßÜ, ÊµôÊ±üÂç´ËßÜ, ÊπñÂçóÂç´ËßÜ, Âåó‰∫¨Âç´ËßÜ, ÊπñÂåóÂç´ËßÜ, ÈªëÈæôÊ±üÂç´ËßÜ, ÂÆâÂæΩÂç´ËßÜ, ÈáçÂ∫ÜÂç´ËßÜ, ‰∏úÊñπÂç´ËßÜ, ‰∏úÂçóÂç´ËßÜ, ÁîòËÇÉÂç´ËßÜ, ÂπøË•øÂç´ËßÜ, Ë¥µÂ∑ûÂç´ËßÜ, Êµ∑ÂçóÂç´ËßÜ, Ê≤≥ÂåóÂç´ËßÜ, Ê≤≥ÂçóÂç´ËßÜ, ÂêâÊûóÂç´ËßÜ, Ê±üËãèÂç´ËßÜ, Ê±üË•øÂç´ËßÜ, ËæΩÂÆÅÂç´ËßÜ, ÂÜÖËíôÂè§Âç´ËßÜ, ÂÆÅÂ§èÂç´ËßÜ, ÈùíÊµ∑Âç´ËßÜ, Â±±‰∏úÂç´ËßÜ, Â±±Ë•øÂç´ËßÜ, ÈôïË•øÂç´ËßÜ, ÂõõÂ∑ùÂç´ËßÜ, Ê∑±Âú≥Âç´ËßÜ, ‰∏âÊ≤ôÂç´ËßÜ, Â§©Ê¥•Âç´ËßÜ, Ë•øËóèÂç´ËßÜ, Êñ∞ÁñÜÂç´ËßÜ, ‰∫ëÂçóÂç´ËßÜ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ‚òòÔ∏èÂπø‰∏úÈ¢ëÈÅì: Âπø‰∏úÁè†Ê±ü, Âπø‰∏ú‰ΩìËÇ≤, Âπø‰∏úÊñ∞Èóª, Âπø‰∏úÊ∞ëÁîü, Âπø‰∏úÂç´ËßÜ, Â§ßÊπæÂå∫Âç´ËßÜ, ÂπøÂ∑ûÁªºÂêà, ÂπøÂ∑ûÂΩ±ËßÜ, ÂπøÂ∑ûÁ´ûËµõ, Ê±üÈó®ÁªºÂêà, Ê±üÈó®‰æ®‰π°ÁîüÊ¥ª, ‰ΩõÂ±±ÁªºÂêà, Ê∑±Âú≥Âç´ËßÜ, Ê±ïÂ§¥ÁªºÂêà, Ê±ïÂ§¥ÁªèÊµé, Ê±ïÂ§¥ÊñáÊóÖ, ËåÇÂêçÁªºÂêà, ËåÇÂêçÂÖ¨ÂÖ± &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ‚òòÔ∏èÂêÑÁúÅ‰ªΩÂú∞ÊñπÂè∞ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   üåäÊ∏Ø¬∑Êæ≥¬∑Âè∞: Áø°Áø†Âè∞, ÊòéÁè†Âè∞, Âá§Âá∞‰∏≠Êñá, Âá§Âá∞ËµÑËÆØ, Âá§Âá∞È¶ôÊ∏Ø, Âá§Âá∞Âç´ËßÜ, TVBS‰∫öÊ¥≤, È¶ôÊ∏ØÂç´ËßÜ, Á∫¨Êù•‰ΩìËÇ≤, Á∫¨Êù•ËÇ≤‰πê, J2, Viutv, ‰∏âÁ´ãÂè∞Êπæ, Êó†Á∫øÊñ∞Èóª, ‰∏âÁ´ãÊñ∞Èóª, ‰∏úÊ£ÆÁªºÂêà, ‰∏úÊ£ÆË∂ÖËßÜ, ‰∏úÊ£ÆÁîµÂΩ±, NowÂâßÈõÜ, NowÂçéÂâß, ÈùñÂ§©ËµÑËÆØ, ÊòüÂç´Â®±‰πê, Âç´ËßÜÂç°Âºè &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   üé¨ÁîµÂΩ±È¢ëÈÅì: CHCÂÆ∂Â∫≠ÂΩ±Èô¢, CHCÂä®‰ΩúÁîµÂΩ±, CHCÈ´òÊ∏ÖÁîµÂΩ±, Ê∑òÂâßÂú∫, Ê∑òÂ®±‰πê, Ê∑òÁîµÂΩ±, NewTVÊÉäÊÇöÊÇ¨Áñë, NewTVÂä®‰ΩúÁîµÂΩ±, ÈªëËéìÁîµÂΩ±, Á∫¨Êù•ÁîµÂΩ±, ÈùñÂ§©Êò†Áîª, ÈùñÂ§©ÊàèÂâß, ÊòüÂç´Â®±‰πê, ËâæÂ∞îËææÂ®±‰πê, ÁªèÂÖ∏ÁîµÂΩ±, IPTVÁªèÂÖ∏ÁîµÂΩ±, Â§©Êò†ÁªèÂÖ∏, Êó†Á∫øÊòüÊ≤≥, ÊòüÁ©∫Âç´ËßÜ, ÁßÅ‰∫∫ÂΩ±Èô¢, ‰∏úÊ£ÆÁîµÂΩ±, ÈæôÁ••ÁîµÂΩ±, ‰∏úÊ£ÆÊ¥ãÁâá, ‰∏úÊ£ÆË∂ÖËßÜ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   üé•Âí™ÂíïÁõ¥Êí≠: Âí™ÂíïÁõ¥Êí≠1-45 &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   üèÄ‰ΩìËÇ≤È¢ëÈÅì: CCTV-5, CCTV-5+, Âπø‰∏ú‰ΩìËÇ≤, Á∫¨Êù•‰ΩìËÇ≤, ‰∫îÊòü‰ΩìËÇ≤, ‰ΩìËÇ≤Ëµõ‰∫ã, Âä≤ÁàÜ‰ΩìËÇ≤, Áà±‰ΩìËÇ≤, Ë∂ÖÁ∫ß‰ΩìËÇ≤, Á≤æÂìÅ‰ΩìËÇ≤, ÂπøÂ∑ûÁ´ûËµõ, Ê∑±Âú≥‰ΩìËÇ≤, Á¶èÂª∫‰ΩìËÇ≤, ËæΩÂÆÅ‰ΩìËÇ≤, Â±±‰∏ú‰ΩìËÇ≤, ÊàêÈÉΩ‰ΩìËÇ≤, Â§©Ê¥•‰ΩìËÇ≤, Ê±üËãè‰ΩìËÇ≤, ÂÆâÂæΩÁªºËâ∫‰ΩìËÇ≤, ÂêâÊûóÁØÆÁêÉ, ÁùõÂΩ©ÁØÆÁêÉ, ÁùõÂΩ©ÁæΩÊØõÁêÉ, ÁùõÂΩ©ÂπøÂú∫Ëàû, È£é‰∫ëË∂≥ÁêÉ, Ë∂≥ÁêÉÈ¢ëÈÅì, È≠ÖÂäõË∂≥ÁêÉ, Â§©ÂÖÉÂõ¥Ê£ã, Âø´‰πêÂûÇÈíì, JJÊñóÂú∞‰∏ª &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   ü™ÅÂä®ÁîªÈ¢ëÈÅì: Â∞ëÂÑøÂä®Áîª, Âç°ÈÖ∑Âä®Áîª, Âä®Êº´ÁßÄÂú∫, Êñ∞Âä®Êº´, ÈùíÊò•Âä®Êº´, Áà±Âä®Êº´, ‰∏≠ÂΩïÂä®Êº´, ÂÆùÂÆùÂä®Áîª, CNÂç°ÈÄö, ‰ºòÊº´Âç°ÈÄö, ÈáëÈπ∞Âç°ÈÄö, ÁùõÂΩ©Â∞ëÂÑø, ÈªëËéìÂä®Áîª, ÁÇ´Âä®Âç°ÈÄö, 24HÂõΩÊº´ÁÉ≠Êí≠, ÊµôÊ±üÂ∞ëÂÑø, Ê≤≥ÂåóÂ∞ëÂÑøÁßëÊïô, ‰∏ÉÈæôÁè†, ÁÅ´ÂΩ±ÂøçËÄÖ, Êµ∑ÁªµÂÆùÂÆù, ‰∏≠ÂçéÂ∞èÂΩìÂÆ∂, ÊñóÁ†¥ËãçÁ©πÁéÑÂπªÂâß, Áå´ÂíåËÄÅÈº†, ÁªèÂÖ∏Âä®Êº´, Ëú°Á¨îÂ∞èÊñ∞, Êº´ÁîªËß£ËØ¥ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   üéÆÊ∏∏ÊàèÈ¢ëÈÅì: Ê∏∏ÊàèÈ£é‰∫ë, Ê∏∏ÊàèÁ´ûÊäÄ, ÁîµÁ´ûÊ∏∏Êàè, Êµ∑ÁúãÁîµÁ´û, ÁîµÁ´ûÂ§©Â†Ç, Áà±ÁîµÁ´û &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   üéµÈü≥‰πêÈ¢ëÈÅì: CCTV-15, È£é‰∫ëÈü≥‰πê, Èü≥‰πêÁé∞Âú∫, Èü≥‰πê‰πãÂ£∞, ÊΩÆÊµÅÈü≥‰πê, Â§©Ê¥•Èü≥‰πê, Èü≥‰πêÂπøÊí≠, Èü≥‰πêË∞ÉÈ¢ëÂπøÊí≠ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt;&#xA;   üèõÁªèÂÖ∏ÂâßÂú∫: Á¨ëÂÇ≤Ê±üÊπñ, Â§©ÈæôÂÖ´ÈÉ®, ÈπøÈºéËÆ∞, ‰ªôÂâëÂ•á‰æ†‰º†, Ë•øÊ∏∏ËÆ∞, ‰∏âÂõΩÊºî‰πâ, Ê∞¥Êµí‰º†, Êñ∞ÁôΩÂ®òÂ≠ê‰º†Â•á, Â§©ÈæôÂÖ´ÈÉ®, ÊµéÂÖ¨Ê∏∏ËÆ∞, Â∞ÅÁ•ûÊ¶ú, ÈóØÂÖ≥‰∏ú, ‰∏äÊµ∑Êª©, Â∞ÑÈõïËã±ÈõÑ‰º† &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Guovin/iptv-api/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/guovin/iptv-api&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.python.org/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-%20%3D%203.13-47c219&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Guovin/iptv-api/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/downloads/guovin/iptv-api/total&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/repository/docker/guovern/iptv-api&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/pulls/guovern/iptv-api?label=docker:iptv-api&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/repository/docker/guovern/tv-requests&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/pulls/guovern/tv-requests?label=docker:requests&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/repository/docker/guovern/tv-driver&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/pulls/guovern/tv-driver?label=docker:driver&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Guovin/iptv-api/fork&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/guovin/iptv-api&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/README_en.md&#34;&gt;English&lt;/a&gt; | ‰∏≠Êñá&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E7%89%B9%E7%82%B9&#34;&gt;‚úÖ ÁâπÁÇπ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E6%9C%80%E6%96%B0%E7%BB%93%E6%9E%9C&#34;&gt;üîó ÊúÄÊñ∞ÁªìÊûú&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/config.md&#34;&gt;‚öôÔ∏è ÈÖçÁΩÆÂèÇÊï∞&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B&#34;&gt;üöÄ Âø´ÈÄü‰∏äÊâã&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/tutorial.md&#34;&gt;üìñ ËØ¶ÁªÜÊïôÁ®ã&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/CHANGELOG.md&#34;&gt;üóìÔ∏è Êõ¥Êñ∞Êó•Âøó&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E8%B5%9E%E8%B5%8F&#34;&gt;‚ù§Ô∏è ËµûËµè&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E5%85%B3%E6%B3%A8&#34;&gt;üëÄ ÂÖ≥Ê≥®&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E5%85%8D%E8%B4%A3%E5%A3%B0%E6%98%8E&#34;&gt;üì£ ÂÖçË¥£Â£∞Êòé&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/#%E8%AE%B8%E5%8F%AF%E8%AF%81&#34;&gt;‚öñÔ∏è ËÆ∏ÂèØËØÅ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÁâπÁÇπ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ Ëá™ÂÆö‰πâÊ®°ÊùøÔºåÁîüÊàêÊÇ®ÊÉ≥Ë¶ÅÁöÑÈ¢ëÈÅì&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ ÊîØÊåÅÂ§öÁßçËé∑ÂèñÊ∫êÊñπÂºèÔºöÁªÑÊí≠Ê∫ê„ÄÅÈÖíÂ∫óÊ∫ê„ÄÅËÆ¢ÈòÖÊ∫ê„ÄÅÂÖ≥ÈîÆÂ≠óÊêúÁ¥¢&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Êé•Âè£ÊµãÈÄüÈ™åÊïàÔºåÂìçÂ∫îÊó∂Èó¥„ÄÅÂàÜËæ®Áéá‰ºòÂÖàÁ∫ßÔºåËøáÊª§Êó†ÊïàÊé•Âè£&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ ÂÅèÂ•ΩËÆæÁΩÆÔºöIPv6„ÄÅÊé•Âè£Êù•Ê∫êÊéíÂ∫è‰ºòÂÖàÁ∫ß‰∏éÊï∞ÈáèÈÖçÁΩÆ„ÄÅÊé•Âè£ÁôΩÂêçÂçï&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ ÂÆöÊó∂ÊâßË°åÔºåÂåó‰∫¨Êó∂Èó¥ÊØèÊó• 6:00 ‰∏é 18:00 ÊâßË°åÊõ¥Êñ∞&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ ÊîØÊåÅÂ§öÁßçËøêË°åÊñπÂºèÔºöÂ∑•‰ΩúÊµÅ„ÄÅÂëΩ‰ª§Ë°å„ÄÅGUI ËΩØ‰ª∂„ÄÅDocker(amd64/arm64/arm v7)&lt;/li&gt; &#xA; &lt;li&gt;‚ú® Êõ¥Â§öÂäüËÉΩËØ∑ËßÅ&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/config.md&#34;&gt;ÈÖçÁΩÆÂèÇÊï∞&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÊúÄÊñ∞ÁªìÊûú&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Êé•Âè£Ê∫êÔºö&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://ghp.ci/raw.githubusercontent.com/Guovin/iptv-api/gd/output/result.m3u&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://ghp.ci/raw.githubusercontent.com/Guovin/iptv-api/gd/output/result.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Êï∞ÊçÆÊ∫êÔºö&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://ghp.ci/raw.githubusercontent.com/Guovin/iptv-api/gd/source.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ÈÖçÁΩÆ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/config.md&#34;&gt;ÈÖçÁΩÆÂèÇÊï∞&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Âø´ÈÄü‰∏äÊâã&lt;/h2&gt; &#xA;&lt;h3&gt;ÊñπÂºè‰∏ÄÔºöÂ∑•‰ΩúÊµÅ&lt;/h3&gt; &#xA;&lt;p&gt;Fork Êú¨È°πÁõÆÂπ∂ÂºÄÂêØÂ∑•‰ΩúÊµÅÊõ¥Êñ∞ÔºåÂÖ∑‰ΩìÊ≠•È™§ËØ∑ËßÅ&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/tutorial.md&#34;&gt;ËØ¶ÁªÜÊïôÁ®ã&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ÊñπÂºè‰∫åÔºöÂëΩ‰ª§Ë°å&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install pipenv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipenv install --dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ÂêØÂä®Êõ¥Êñ∞Ôºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipenv run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ÂêØÂä®ÊúçÂä°Ôºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipenv run service&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ÊñπÂºè‰∏âÔºöGUI ËΩØ‰ª∂&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;‰∏ãËΩΩ&lt;a href=&#34;https://github.com/Guovin/iptv-api/releases&#34;&gt;IPTV-API Êõ¥Êñ∞ËΩØ‰ª∂&lt;/a&gt;ÔºåÊâìÂºÄËΩØ‰ª∂ÔºåÁÇπÂáªÊõ¥Êñ∞ÔºåÂç≥ÂèØÂÆåÊàêÊõ¥Êñ∞&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ÊàñËÄÖÂú®È°πÁõÆÁõÆÂΩï‰∏ãËøêË°å‰ª•‰∏ãÂëΩ‰ª§ÔºåÂç≥ÂèØÊâìÂºÄ GUI ËΩØ‰ª∂Ôºö&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipenv run ui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/docs/images/ui.png&#34; alt=&#34;IPTV-APIÊõ¥Êñ∞ËΩØ‰ª∂&#34; title=&#34;IPTV-APIÊõ¥Êñ∞ËΩØ‰ª∂&#34; style=&#34;height:600px&#34;&gt; &#xA;&lt;h3&gt;ÊñπÂºèÂõõÔºöDocker&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-apiÔºàÂÆåÊï¥ÁâàÊú¨ÔºâÔºöÊÄßËÉΩË¶ÅÊ±ÇËæÉÈ´òÔºåÊõ¥Êñ∞ÈÄüÂ∫¶ËæÉÊÖ¢ÔºåÁ®≥ÂÆöÊÄß„ÄÅÊàêÂäüÁéáÈ´òÔºõ‰øÆÊîπÈÖçÁΩÆ open_driver = False ÂèØÂàáÊç¢Âà∞ Lite ÁâàÊú¨ËøêË°åÊ®°ÂºèÔºàÊé®ËçêÈÖíÂ∫óÊ∫ê„ÄÅÁªÑÊí≠Ê∫ê„ÄÅÂÖ≥ÈîÆÂ≠óÊêúÁ¥¢‰ΩøÁî®Ê≠§ÁâàÊú¨Ôºâ&lt;/li&gt; &#xA; &lt;li&gt;iptv-api:liteÔºàÁ≤æÁÆÄÁâàÊú¨ÔºâÔºöËΩªÈáèÁ∫ßÔºåÊÄßËÉΩË¶ÅÊ±Ç‰ΩéÔºåÊõ¥Êñ∞ÈÄüÂ∫¶Âø´ÔºåÁ®≥ÂÆöÊÄß‰∏çÁ°ÆÂÆöÔºàÊé®ËçêËÆ¢ÈòÖÊ∫ê‰ΩøÁî®Ê≠§ÁâàÊú¨Ôºâ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ÊãâÂèñÈïúÂÉèÔºö&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-apiÔºö&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull guovern/iptv-api:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-api:liteÔºö&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull guovern/iptv-api:lite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;ËøêË°åÂÆπÂô®Ôºö&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-apiÔºö&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 8000:8000 guovern/iptv-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-api:liteÔºö&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 8000:8000 guovern/iptv-api:lite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Âç∑ÊåÇËΩΩÂèÇÊï∞ÔºàÂèØÈÄâÔºâÔºö ÂÆûÁé∞ÂÆø‰∏ªÊú∫Êñá‰ª∂‰∏éÂÆπÂô®Êñá‰ª∂ÂêåÊ≠•Ôºå‰øÆÊîπÊ®°Êùø„ÄÅÈÖçÁΩÆ„ÄÅËé∑ÂèñÊõ¥Êñ∞ÁªìÊûúÊñá‰ª∂ÂèØÁõ¥Êé•Âú®ÂÆø‰∏ªÊú∫Êñá‰ª∂Â§π‰∏ãÊìç‰Ωú&lt;/p&gt; &#xA;&lt;p&gt;‰ª•ÂÆø‰∏ªÊú∫Ë∑ØÂæÑ/etc/docker ‰∏∫‰æãÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-apiÔºö&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v /etc/docker/config:/iptv-api/config -v /etc/docker/output:/iptv-api/output -d -p 8000:8000 guovern/iptv-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iptv-api:liteÔºö&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v /etc/docker/config:/iptv-api-lite/config -v /etc/docker/output:/iptv-api-lite/output -d -p 8000:8000 guovern/iptv-api:lite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Êõ¥Êñ∞ÁªìÊûúÔºö&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Êé•Âè£Âú∞ÂùÄÔºöip:8000&lt;/li&gt; &#xA; &lt;li&gt;M3u Êé•Âè£Ôºöip:8000/m3u&lt;/li&gt; &#xA; &lt;li&gt;Txt Êé•Âè£Ôºöip:8000/txt&lt;/li&gt; &#xA; &lt;li&gt;Êé•Âè£ÂÜÖÂÆπÔºöip:8000/content&lt;/li&gt; &#xA; &lt;li&gt;ÊµãÈÄüÊó•ÂøóÔºöip:8000/log&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Êõ¥Êñ∞Êó•Âøó&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/CHANGELOG.md&#34;&gt;Êõ¥Êñ∞Êó•Âøó&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ËµûËµè&lt;/h2&gt; &#xA;&lt;div&gt;&#xA; ÂºÄÂèëÁª¥Êä§‰∏çÊòìÔºåËØ∑ÊàëÂñùÊùØÂíñÂï°‚òïÔ∏èÂêß~&#xA;&lt;/div&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ÊîØ‰ªòÂÆù&lt;/th&gt; &#xA;   &lt;th&gt;ÂæÆ‰ø°&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/static/images/alipay.jpg&#34; alt=&#34;ÊîØ‰ªòÂÆùÊâ´Á†Å&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/static/images/appreciate.jpg&#34; alt=&#34;ÂæÆ‰ø°Êâ´Á†Å&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ÂÖ≥Ê≥®&lt;/h2&gt; &#xA;&lt;p&gt;ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÊêúÁ¥¢ GovinÔºåÊàñÊâ´Á†ÅÔºåÊé•Êî∂Êõ¥Êñ∞Êé®ÈÄÅ„ÄÅÂ≠¶‰π†Êõ¥Â§ö‰ΩøÁî®ÊäÄÂ∑ßÔºö&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/static/images/qrcode.jpg&#34; alt=&#34;ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ÂÖçË¥£Â£∞Êòé&lt;/h2&gt; &#xA;&lt;p&gt;Êú¨È°πÁõÆ‰ªÖ‰æõÂ≠¶‰π†‰∫§ÊµÅÁî®ÈÄîÔºåÊé•Âè£Êï∞ÊçÆÂùáÊù•Ê∫ê‰∫éÁΩëÁªúÔºåÂ¶ÇÊúâ‰æµÊùÉÔºåËØ∑ËÅîÁ≥ªÂà†Èô§&lt;/p&gt; &#xA;&lt;h2&gt;ËÆ∏ÂèØËØÅ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Guovin/iptv-api/master/LICENSE&#34;&gt;MIT&lt;/a&gt; License ¬© 2024-PRESENT &lt;a href=&#34;https://github.com/guovin&#34;&gt;Govin&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>