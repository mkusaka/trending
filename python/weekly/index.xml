<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-16T01:46:21Z</updated>
  <subtitle>Weekly Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PaddlePaddle/PaddleNLP</title>
    <updated>2022-10-16T01:46:21Z</updated>
    <id>tag:github.com,2022-10-16:/PaddlePaddle/PaddleNLP</id>
    <link href="https://github.com/PaddlePaddle/PaddleNLP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easy-to-use and powerful NLP library with Awesome model zoo, supporting wide-range of NLP tasks from research to industrial applications, including Neural Search, Question Answering, Information Extraction and Sentiment Analysis end-to-end system.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;简体中文&lt;/strong&gt;🀄 | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/README_en.md&#34;&gt;English🌎&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/1371212/175816733-8ec25eb0-9af3-4380-9218-27c154518258.png&#34; align=&#34;middle&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-dfd.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/PaddleNLP?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.6.2+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/PaddlePaddle/PaddleNLP?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/commits&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleNLP?color=3af&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/paddlenlp/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/paddlenlp?color=9cf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PaddlePaddle/PaddleNLP?color=9cc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleNLP?color=ccf&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;#特性&#34;&gt; 特性 &lt;/a&gt; | &lt;a href=&#34;#安装&#34;&gt; 安装 &lt;/a&gt; | &lt;a href=&#34;#快速开始&#34;&gt; 快速开始 &lt;/a&gt; | &lt;a href=&#34;#api文档&#34;&gt; API文档 &lt;/a&gt; | &lt;a href=&#34;#社区交流&#34;&gt; 社区交流 &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaddleNLP&lt;/strong&gt;是一款&lt;strong&gt;简单易用&lt;/strong&gt;且&lt;strong&gt;功能强大&lt;/strong&gt;的自然语言处理开发库。聚合业界&lt;strong&gt;优质预训练模型&lt;/strong&gt;并提供&lt;strong&gt;开箱即用&lt;/strong&gt;的开发体验，覆盖NLP多场景的模型库搭配&lt;strong&gt;产业实践范例&lt;/strong&gt;可满足开发者&lt;strong&gt;灵活定制&lt;/strong&gt;的需求。&lt;/p&gt; &#xA;&lt;h2&gt;News 📢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;2022.9.6 发布 &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/releases/tag/v2.4.0&#34;&gt;PaddleNLP v2.4&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;💎 NLP工具：&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/pipelines&#34;&gt;NLP 流水线系统 Pipelines&lt;/a&gt;&lt;/strong&gt; 发布，支持快速搭建搜索引擎、问答系统，可扩展支持各类NLP系统，让解决 NLP 任务像搭积木一样便捷、灵活、高效！&lt;/li&gt; &#xA;   &lt;li&gt;💢 产业应用：新增 &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/text_classification&#34;&gt;文本分类全流程应用方案&lt;/a&gt;&lt;/strong&gt; ，覆盖多分类、多标签、层次分类各类场景，支持 &lt;strong&gt;小样本学习&lt;/strong&gt; 和 &lt;strong&gt;TrustAI&lt;/strong&gt; 可信计算模型训练与调优；&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie&#34;&gt;&lt;strong&gt;通用信息抽取 UIE 能力升级&lt;/strong&gt;&lt;/a&gt;，发布 &lt;strong&gt;UIE-M&lt;/strong&gt;，支持中英文混合抽取，新增&lt;strong&gt;UIE 数据蒸馏&lt;/strong&gt;方案，打破 UIE 推理瓶颈，推理速度提升 100 倍以上；&lt;/li&gt; &#xA;   &lt;li&gt;🍭 AIGC 内容生成：新增代码生成 SOTA 模型&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples/code_generation/codegen&#34;&gt;&lt;strong&gt;CodeGen&lt;/strong&gt;&lt;/a&gt;，支持多种编程语言代码生成；集成&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/raw/develop/docs/model_zoo/taskflow.md#%E6%96%87%E5%9B%BE%E7%94%9F%E6%88%90&#34;&gt;&lt;strong&gt;文图生成潮流模型&lt;/strong&gt;&lt;/a&gt; DALL·E Mini、Disco Diffusion、Stable Diffusion，更多趣玩模型等你来玩；新增&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/text_summarization&#34;&gt;&lt;strong&gt;中文文本摘要应用&lt;/strong&gt;&lt;/a&gt;，基于大规模语料的中文摘要模型首次发布，可支持 Taskflow 一键调用和定制训练；&lt;/li&gt; &#xA;   &lt;li&gt;💪 框架升级：&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/compression.md&#34;&gt;&lt;strong&gt;模型自动压缩 API&lt;/strong&gt;&lt;/a&gt; 发布，自动对模型进行裁减和量化，大幅降低模型压缩技术使用门槛；&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/text_classification/multi_class/few-shot&#34;&gt;&lt;strong&gt;小样本 Prompt&lt;/strong&gt;&lt;/a&gt;能力发布，集成 PET、P-Tuning、RGL 等经典算法。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;2022.5.16 发布 &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/releases/tag/v2.3.0&#34;&gt;PaddleNLP v2.3&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;💎 发布通用信息抽取技术 &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie&#34;&gt;&lt;strong&gt;UIE&lt;/strong&gt;&lt;/a&gt;，单模型支持实体识别、关系和事件抽取、情感分析等多种开放域信息抽取任务，不限领域和抽取目标，支持&lt;strong&gt;零样本抽取&lt;/strong&gt;与全流程&lt;strong&gt;小样本&lt;/strong&gt;高效定制开发；&lt;/li&gt; &#xA;   &lt;li&gt;😊 发布文心大模型 &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/ernie-3.0&#34;&gt;&lt;strong&gt;ERNIE 3.0&lt;/strong&gt;&lt;/a&gt; 轻量级模型，在 &lt;a href=&#34;https://www.cluebenchmarks.com/&#34;&gt;CLUE &lt;/a&gt;上实现同规模结构效果最佳，并提供&lt;strong&gt;🗜️无损压缩&lt;/strong&gt;和&lt;strong&gt;⚙️全场景部署&lt;/strong&gt;方案；&lt;/li&gt; &#xA;   &lt;li&gt;🏥 发布中文医疗领域预训练模型 &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/ernie-health&#34;&gt;&lt;strong&gt;ERNIE-Health&lt;/strong&gt;&lt;/a&gt;，&lt;a href=&#34;https://github.com/CBLUEbenchmark/CBLUE&#34;&gt;CBLUE&lt;/a&gt; 中文医疗信息处理评测冠军模型；&lt;/li&gt; &#xA;   &lt;li&gt;💬 发布大规模百亿开放域对话预训练模型 &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/plato-xl&#34;&gt;&lt;strong&gt;PLATO-XL&lt;/strong&gt;&lt;/a&gt; ，配合⚡&lt;strong&gt;FasterGeneration&lt;/strong&gt;⚡快速实现高性能GPU并行推理加速。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;社区交流&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;微信扫描二维码并填写问卷，回复小助手关键词（NLP）之后，即可加入交流群领取福利&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;与众多社区开发者以及官方团队深度交流。&lt;/li&gt; &#xA;   &lt;li&gt;10G重磅NLP学习大礼包！&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/184784832-bb97930f-a738-4480-99be-517aeb65afac.png&#34; width=&#34;150&#34; height=&#34;150&#34;&gt; &#xA;  &lt;/div&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;特性&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#开箱即用的nlp工具集&#34;&gt; 📦 开箱即用的NLP工具集 &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#丰富完备的中文模型库&#34;&gt; 🤗 丰富完备的中文模型库 &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#产业级端到端系统范例&#34;&gt; 🎛️ 产业级端到端系统范例 &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#高性能分布式训练与推理&#34;&gt; 🚀 高性能分布式训练与推理 &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h3&gt;开箱即用的NLP工具集&lt;/h3&gt; &#xA;&lt;p&gt;Taskflow提供丰富的&lt;strong&gt;📦开箱即用&lt;/strong&gt;的产业级NLP预置模型，覆盖自然语言理解与生成两大场景，提供&lt;strong&gt;💪产业级的效果&lt;/strong&gt;与&lt;strong&gt;⚡️极致的推理性能&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11793384/159693816-fda35221-9751-43bb-b05c-7fc77571dd76.gif&#34; alt=&#34;taskflow1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Taskflow最新集成了文生图的趣玩应用，三行代码体验 &lt;strong&gt;Stable Diffusion&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paddlenlp import Taskflow&#xA;text_to_image = Taskflow(&#34;text_to_image&#34;, model=&#34;CompVis/stable-diffusion-v1-4&#34;)&#xA;image_list = text_to_image(&#39;&#34;In the morning light,Chinese ancient buildings in the mountains,Magnificent and fantastic John Howe landscape,lake,clouds,farm,Fairy tale,light effect,Dream,Greg Rutkowski,James Gurney,artstation&#34;&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;300&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/16698950/194882669-f7cc7c98-d63a-45f4-99c1-0514c6712368.png&#34;&gt; &#xA;&lt;p&gt;更多使用方法可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/taskflow.md&#34;&gt;Taskflow文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;丰富完备的中文模型库&lt;/h3&gt; &#xA;&lt;h4&gt;🀄 业界最全的中文预训练模型&lt;/h4&gt; &#xA;&lt;p&gt;精选 45+ 个网络结构和 500+ 个预训练模型参数，涵盖业界最全的中文预训练模型：既包括文心NLP大模型的ERNIE、PLATO等，也覆盖BERT、GPT、RoBERTa、T5等主流结构。通过&lt;code&gt;AutoModel&lt;/code&gt; API一键⚡&lt;strong&gt;高速下载&lt;/strong&gt;⚡。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paddlenlp.transformers import *&#xA;&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;bert = AutoModel.from_pretrained(&#39;bert-wwm-chinese&#39;)&#xA;albert = AutoModel.from_pretrained(&#39;albert-chinese-tiny&#39;)&#xA;roberta = AutoModel.from_pretrained(&#39;roberta-wwm-ext&#39;)&#xA;electra = AutoModel.from_pretrained(&#39;chinese-electra-small&#39;)&#xA;gpt = AutoModelForPretraining.from_pretrained(&#39;gpt-cpm-large-cn&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;针对预训练模型计算瓶颈，可以使用API一键使用文心ERNIE-Tiny全系列轻量化模型，降低预训练模型部署难度。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 6L768H&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;# 6L384H&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-mini-zh&#39;)&#xA;# 4L384H&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-micro-zh&#39;)&#xA;# 4L312H&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-nano-zh&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;对预训练模型应用范式如语义表示、文本分类、句对匹配、序列标注、问答等，提供统一的API体验。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import paddle&#xA;from paddlenlp.transformers import *&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;text = tokenizer(&#39;自然语言处理&#39;)&#xA;&#xA;# 语义表示&#xA;model = AutoModel.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;sequence_output, pooled_output = model(input_ids=paddle.to_tensor([text[&#39;input_ids&#39;]]))&#xA;# 文本分类 &amp;amp; 句对匹配&#xA;model = AutoModelForSequenceClassification.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;# 序列标注&#xA;model = AutoModelForTokenClassification.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;# 问答&#xA;model = AutoModelForQuestionAnswering.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;💯 全场景覆盖的应用示例&lt;/h4&gt; &#xA;&lt;p&gt;覆盖从学术到产业的NLP应用示例，涵盖NLP基础技术、NLP系统应用以及拓展应用。全面基于飞桨核心框架2.0全新API体系开发，为开发者提供飞桨文本领域的最佳实践。&lt;/p&gt; &#xA;&lt;p&gt;精选预训练模型示例可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo&#34;&gt;Model Zoo&lt;/a&gt;，更多场景示例文档可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples&#34;&gt;examples目录&lt;/a&gt;。更有免费算力支持的&lt;a href=&#34;https://aistudio.baidu.com&#34;&gt;AI Studio&lt;/a&gt;平台的&lt;a href=&#34;https://aistudio.baidu.com/aistudio/personalcenter/thirdview/574995&#34;&gt;Notbook交互式教程&lt;/a&gt;提供实践。&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; PaddleNLP预训练模型适用任务汇总（&lt;b&gt;点击展开详情&lt;/b&gt;）&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;Sequence Classification&lt;/th&gt; &#xA;     &lt;th&gt;Token Classification&lt;/th&gt; &#xA;     &lt;th&gt;Question Answering&lt;/th&gt; &#xA;     &lt;th&gt;Text Generation&lt;/th&gt; &#xA;     &lt;th&gt;Multiple Choice&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ALBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BART&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BigBird&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BlenderBot&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ChineseBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ConvBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;CTRL&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;DistilBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ELECTRA&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-CTM&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-Doc&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-GEN&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-Gram&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-M&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;FNet&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Funnel-Transformer&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;GPT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LayoutLM&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LayoutLMv2&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LayoutXLM&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LUKE&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;mBART&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MegatronBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MobileBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MPNet&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;NEZHA&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;PP-MiniLM&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ProphetNet&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Reformer&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;RemBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;RoBERTa&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;RoFormer&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;SKEP&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;SqueezeBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;T5&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;TinyBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;UnifiedTransformer&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;XLNet&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;p&gt;可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/index.rst&#34;&gt;Transformer 文档&lt;/a&gt; 查看目前支持的预训练模型结构、参数和详细用法。&lt;/p&gt; &#xA;&lt;h3&gt;产业级端到端系统范例&lt;/h3&gt; &#xA;&lt;p&gt;PaddleNLP针对信息抽取、语义检索、智能问答、情感分析等高频NLP场景，提供了端到端系统范例，打通&lt;em&gt;数据标注&lt;/em&gt;-&lt;em&gt;模型训练&lt;/em&gt;-&lt;em&gt;模型调优&lt;/em&gt;-&lt;em&gt;预测部署&lt;/em&gt;全流程，持续降低NLP技术产业落地门槛。更多详细的系统级产业范例使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications&#34;&gt;Applications&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;🔍 语义检索系统&lt;/h4&gt; &#xA;&lt;p&gt;针对无监督数据、有监督数据等多种数据情况，结合SimCSE、In-batch Negatives、ERNIE-Gram单塔模型等，推出前沿的语义检索方案，包含召回、排序环节，打通训练、调优、高效向量检索引擎建库和查询全流程。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168514909-8817d79a-72c4-4be1-8080-93d1f682bb46.gif&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/neural_search&#34;&gt;语义检索系统&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;❓ 智能问答系统&lt;/h4&gt; &#xA;&lt;p&gt;基于&lt;a href=&#34;https://github.com/PaddlePaddle/RocketQA&#34;&gt;🚀RocketQA&lt;/a&gt;技术的检索式问答系统，支持FAQ问答、说明书问答等多种业务场景。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168514868-1babe981-c675-4f89-9168-dd0a3eede315.gif&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/question_answering&#34;&gt;智能问答系统&lt;/a&gt;与&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/doc_vqa&#34;&gt;文档智能问答&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;💌 评论观点抽取与情感分析&lt;/h4&gt; &#xA;&lt;p&gt;基于情感知识增强预训练模型SKEP，针对产品评论进行评价维度和观点抽取，以及细粒度的情感分析。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168407260-b7f92800-861c-4207-98f3-2291e0102bbe.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/sentiment_analysis&#34;&gt;情感分析&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;🎙️ 智能语音指令解析&lt;/h4&gt; &#xA;&lt;p&gt;集成了&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech&#34;&gt;PaddleSpeech&lt;/a&gt;和&lt;a href=&#34;https://ai.baidu.com/&#34;&gt;百度开放平台&lt;/a&gt;的的语音识别和&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie&#34;&gt;UIE&lt;/a&gt;通用信息抽取等技术，打造智能一体化的语音指令解析系统范例，该方案可应用于智能语音填单、智能语音交互、智能语音检索等场景，提高人机交互效率。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/16698950/168589100-a6c6f346-97bb-47b2-ac26-8d50e71fddc5.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/speech_cmd_analysis&#34;&gt;智能语音指令解析&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;高性能分布式训练与推理&lt;/h3&gt; &#xA;&lt;h4&gt;⚡ FasterTokenizer：高性能文本处理库&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168407921-b4395b1d-44bd-41a0-8c58-923ba2b703ef.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;AutoTokenizer.from_pretrained(&#34;ernie-3.0-medium-zh&#34;, use_faster=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;为了实现更极致的模型部署性能，安装FastTokenizers后只需在&lt;code&gt;AutoTokenizer&lt;/code&gt; API上打开 &lt;code&gt;use_faster=True&lt;/code&gt;选项，即可调用C++实现的高性能分词算子，轻松获得超Python百余倍的文本处理加速，更多使用说明可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/faster_tokenizer&#34;&gt;FasterTokenizer文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;⚡️ FasterGeneration：高性能生成加速库&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168407831-914dced0-3a5a-40b8-8a65-ec82bf13e53c.gif&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = GPTLMHeadModel.from_pretrained(&#39;gpt-cpm-large-cn&#39;)&#xA;...&#xA;outputs, _ = model.generate(&#xA;    input_ids=inputs_ids, max_length=10, decode_strategy=&#39;greedy_search&#39;,&#xA;    use_faster=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;简单地在&lt;code&gt;generate()&lt;/code&gt;API上打开&lt;code&gt;use_faster=True&lt;/code&gt;选项，轻松在Transformer、GPT、BART、PLATO、UniLM等生成式预训练模型上获得5倍以上GPU加速，更多使用说明可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/faster_generation&#34;&gt;FasterGeneration文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;🚀 Fleet：飞桨4D混合并行分布式训练技术&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168515134-513f13e0-9902-40ef-98fa-528271dcccda.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多关于千亿级AI模型的分布式训练使用说明可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples/language_model/gpt-3&#34;&gt;GPT-3&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;h3&gt;环境依赖&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python &amp;gt;= 3.6&lt;/li&gt; &#xA; &lt;li&gt;paddlepaddle &amp;gt;= 2.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;pip安装&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install --upgrade paddlenlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;更多关于PaddlePaddle和PaddleNLP安装的详细教程请查看&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/get_started/installation.rst&#34;&gt;Installation&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;快速开始&lt;/h2&gt; &#xA;&lt;p&gt;这里以信息抽取-命名实体识别任务，UIE模型为例，来说明如何快速使用PaddleNLP:&lt;/p&gt; &#xA;&lt;h3&gt;一键预测&lt;/h3&gt; &#xA;&lt;p&gt;PaddleNLP提供&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/taskflow.md&#34;&gt;一键预测功能&lt;/a&gt;，无需训练，直接输入数据即可开放域抽取结果：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from pprint import pprint&#xA;&amp;gt;&amp;gt;&amp;gt; from paddlenlp import Taskflow&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; schema = [&#39;时间&#39;, &#39;选手&#39;, &#39;赛事名称&#39;] # Define the schema for entity extraction&#xA;&amp;gt;&amp;gt;&amp;gt; ie = Taskflow(&#39;information_extraction&#39;, schema=schema)&#xA;&amp;gt;&amp;gt;&amp;gt; pprint(ie(&#34;2月8日上午北京冬奥会自由式滑雪女子大跳台决赛中中国选手谷爱凌以188.25分获得金牌！&#34;))&#xA;[{&#39;时间&#39;: [{&#39;end&#39;: 6,&#xA;          &#39;probability&#39;: 0.9857378532924486,&#xA;          &#39;start&#39;: 0,&#xA;          &#39;text&#39;: &#39;2月8日上午&#39;}],&#xA;  &#39;赛事名称&#39;: [{&#39;end&#39;: 23,&#xA;            &#39;probability&#39;: 0.8503089953268272,&#xA;            &#39;start&#39;: 6,&#xA;            &#39;text&#39;: &#39;北京冬奥会自由式滑雪女子大跳台决赛&#39;}],&#xA;  &#39;选手&#39;: [{&#39;end&#39;: 31,&#xA;          &#39;probability&#39;: 0.8981548639781138,&#xA;          &#39;start&#39;: 28,&#xA;          &#39;text&#39;: &#39;谷爱凌&#39;}]}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;小样本学习&lt;/h3&gt; &#xA;&lt;p&gt;如果对一键预测效果不满意，也可以使用少量数据进行模型精调，进一步提升特定场景的效果，详见&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie/&#34;&gt;UIE小样本定制训练&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;更多PaddleNLP内容可参考：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo&#34;&gt;精选模型库&lt;/a&gt;，包含优质预训练模型的端到端全流程使用。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples&#34;&gt;多场景示例&lt;/a&gt;，了解如何使用PaddleNLP解决NLP多种技术问题，包含基础技术、系统应用与拓展应用。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/personalcenter/thirdview/574995&#34;&gt;交互式教程&lt;/a&gt;，在🆓免费算力平台AI Studio上快速学习PaddleNLP。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;API文档&lt;/h2&gt; &#xA;&lt;p&gt;PaddleNLP提供全流程的文本领域API，可大幅提升NLP任务建模的效率：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持&lt;a href=&#34;https://www.luge.ai&#34;&gt;千言&lt;/a&gt;等丰富中文数据集加载的&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_list.html&#34;&gt;Dataset API&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;提供🤗Hugging Face Style的API，支持 &lt;strong&gt;500+&lt;/strong&gt; 优质预训练模型加载的&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html&#34;&gt;Transformers API&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;提供30+多语言词向量的&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/model_zoo/embeddings.html&#34;&gt;Embedding API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;更多使用方法请参考&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/&#34;&gt;API文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;如果PaddleNLP对您的研究有帮助，欢迎引用&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{=paddlenlp,&#xA;    title={PaddleNLP: An Easy-to-use and High Performance NLP Library},&#xA;    author={PaddleNLP Contributors},&#xA;    howpublished = {\url{https://github.com/PaddlePaddle/PaddleNLP}},&#xA;    year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledge&lt;/h2&gt; &#xA;&lt;p&gt;我们借鉴了Hugging Face的&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;🤗关于预训练模型使用的优秀设计，在此对Hugging Face作者及其开源社区表示感谢。&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PaddleNLP遵循&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/LICENSE&#34;&gt;Apache-2.0开源协议&lt;/a&gt;。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>danielgatis/rembg</title>
    <updated>2022-10-16T01:46:21Z</updated>
    <id>tag:github.com,2022-10-16:/danielgatis/rembg</id>
    <link href="https://github.com/danielgatis/rembg" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Rembg is a tool to remove images background.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rembg&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pepy.tech/project/rembg&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/rembg&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/rembg/month&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/rembg/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/rembg/week&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/rembg/week&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/badge/License-MIT-blue.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/KenjieDec/RemBG&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Rembg is a tool to remove images background. That is it.&lt;/p&gt; &#xA;&lt;p style=&#34;display: flex;align-items: center;justify-content: center;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-1.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-1.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-2.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-2.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-3.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-3.out.png&#34; width=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;p style=&#34;display: flex;align-items: center;justify-content: center;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-1.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-1.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-2.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-2.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-3.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/animal-3.out.png&#34; width=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;p style=&#34;display: flex;align-items: center;justify-content: center;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-1.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-1.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-2.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-2.out.png&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-3.jpg&#34; width=&#34;100&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/girl-3.out.png&#34; width=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If this project has helped you, please consider making a &lt;a href=&#34;https://www.buymeacoffee.com/danielgatis&#34;&gt;donation&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;CPU support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install rembg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;GPU support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install rembg[gpu]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a cli&lt;/h3&gt; &#xA;&lt;p&gt;Remove the background from a remote image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -s http://input.png | rembg i &amp;gt; output.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remove the background from a local file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rembg i path/to/input.png path/to/output.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remove the background from all images in a folder&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rembg p path/to/input path/to/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a server&lt;/h3&gt; &#xA;&lt;p&gt;Start the server&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rembg s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And go to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://localhost:5000/docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image with background:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Gull_portrait_ca_usa.jpg/1280px-Gull_portrait_ca_usa.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image without background:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://localhost:5000/?url=https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Gull_portrait_ca_usa.jpg/1280px-Gull_portrait_ca_usa.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also you can send the file as a FormData (multipart/form-data):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;form&#xA;    action=&#34;http://localhost:5000&#34;&#xA;    method=&#34;post&#34;&#xA;    enctype=&#34;multipart/form-data&#34;&#xA;&amp;gt;&#xA;    &amp;lt;input type=&#34;file&#34; name=&#34;file&#34; /&amp;gt;&#xA;    &amp;lt;input type=&#34;submit&#34; value=&#34;upload&#34; /&amp;gt;&#xA;&amp;lt;/form&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a library&lt;/h3&gt; &#xA;&lt;p&gt;Input and output as bytes&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rembg import remove&#xA;&#xA;input_path = &#39;input.png&#39;&#xA;output_path = &#39;output.png&#39;&#xA;&#xA;with open(input_path, &#39;rb&#39;) as i:&#xA;    with open(output_path, &#39;wb&#39;) as o:&#xA;        input = i.read()&#xA;        output = remove(input)&#xA;        o.write(output)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Input and output as a PIL image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rembg import remove&#xA;from PIL import Image&#xA;&#xA;input_path = &#39;input.png&#39;&#xA;output_path = &#39;output.png&#39;&#xA;&#xA;input = Image.open(input_path)&#xA;output = remove(input)&#xA;output.save(output_path)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Input and output as a numpy array&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rembg import remove&#xA;import cv2&#xA;&#xA;input_path = &#39;input.png&#39;&#xA;output_path = &#39;output.png&#39;&#xA;&#xA;input = cv2.imread(input_path)&#xA;output = remove(input)&#xA;cv2.imwrite(output_path, output)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a docker&lt;/h3&gt; &#xA;&lt;p&gt;Try this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p 5000:5000 danielgatis/rembg s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image with background:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Gull_portrait_ca_usa.jpg/1280px-Gull_portrait_ca_usa.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Image without background:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://localhost:5000/?url=https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Gull_portrait_ca_usa.jpg/1280px-Gull_portrait_ca_usa.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;All models are downloaded and saved in the user home folder in the &lt;code&gt;.u2net&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;The available models are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;u2net (&lt;a href=&#34;https://drive.google.com/uc?id=1tCU5MM1LhRgGou5OpmpjBQbSrYIUoYab&#34;&gt;download&lt;/a&gt; - &lt;a href=&#34;http://depositfiles.com/files/ltxbqa06w&#34;&gt;alternative&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;source&lt;/a&gt;): A pre-trained model for general use cases.&lt;/li&gt; &#xA; &lt;li&gt;u2netp (&lt;a href=&#34;https://drive.google.com/uc?id=1tNuFmLv0TSNDjYIkjEdeH1IWKQdUA4HR&#34;&gt;download&lt;/a&gt; - &lt;a href=&#34;http://depositfiles.com/files/0y9i0r2fy&#34;&gt;alternative&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;source&lt;/a&gt;): A lightweight version of u2net model.&lt;/li&gt; &#xA; &lt;li&gt;u2net_human_seg (&lt;a href=&#34;https://drive.google.com/uc?id=1ZfqwVxu-1XWC1xU1GHIP-FM_Knd_AX5j&#34;&gt;download&lt;/a&gt; - &lt;a href=&#34;http://depositfiles.com/files/6spp8qpey&#34;&gt;alternative&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;source&lt;/a&gt;): A pre-trained model for human segmentation.&lt;/li&gt; &#xA; &lt;li&gt;u2net_cloth_seg (&lt;a href=&#34;https://drive.google.com/uc?id=15rKbQSXQzrKCQurUjZFg8HqzZad8bcyz&#34;&gt;download&lt;/a&gt; - &lt;a href=&#34;http://depositfiles.com/files/l3z3cxetq&#34;&gt;alternative&lt;/a&gt;, &lt;a href=&#34;https://github.com/levindabhi/cloth-segmentation&#34;&gt;source&lt;/a&gt;): A pre-trained model for Cloths Parsing from human portrait. Here clothes are parsed into 3 category: Upper body, Lower body and Full body.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;How to train your own model&lt;/h4&gt; &#xA;&lt;p&gt;If You need more fine tunned models try this: &lt;a href=&#34;https://github.com/danielgatis/rembg/issues/193#issuecomment-1055534289&#34;&gt;https://github.com/danielgatis/rembg/issues/193#issuecomment-1055534289&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Advance usage&lt;/h3&gt; &#xA;&lt;p&gt;Sometimes it is possible to achieve better results by turning on alpha matting. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -s http://input.png | rembg i -a -ae 15 &amp;gt; output.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Original&lt;/td&gt; &#xA;   &lt;td&gt;Without alpha matting&lt;/td&gt; &#xA;   &lt;td&gt;With alpha matting (-a -ae 15)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/food-1.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/food-1.out.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/danielgatis/rembg/master/examples/food-1.out.alpha.jpg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;In the cloud&lt;/h3&gt; &#xA;&lt;p&gt;Please contact me at &lt;a href=&#34;mailto:danielgatis@gmail.com&#34;&gt;danielgatis@gmail.com&lt;/a&gt; if you need help to put it on the cloud.&lt;/p&gt; &#xA;&lt;h3&gt;References&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.09007.pdf&#34;&gt;https://arxiv.org/pdf/2005.09007.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NathanUA/U-2-Net&#34;&gt;https://github.com/NathanUA/U-2-Net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pymatting/pymatting&#34;&gt;https://github.com/pymatting/pymatting&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Buy me a coffee&lt;/h3&gt; &#xA;&lt;p&gt;Liked some of my work? Buy me a coffee (or more likely a beer)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/danielgatis&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://bmc-cdn.nyc3.digitaloceanspaces.com/BMC-button-images/custom_images/orange_img.png&#34; alt=&#34;Buy Me A Coffee&#34; style=&#34;height: auto !important;width: auto !important;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;Copyright (c) 2020-present &lt;a href=&#34;https://github.com/danielgatis&#34;&gt;Daniel Gatis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Licensed under &lt;a href=&#34;https://raw.githubusercontent.com/danielgatis/rembg/main/LICENSE.txt&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>GuyTevet/motion-diffusion-model</title>
    <updated>2022-10-16T01:46:21Z</updated>
    <id>tag:github.com,2022-10-16:/GuyTevet/motion-diffusion-model</id>
    <link href="https://github.com/GuyTevet/motion-diffusion-model" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official PyTorch implementation of the paper &#34;Human Motion Diffusion Model&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MDM: Human Motion Diffusion Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/motion-synthesis-on-humanact12?p=human-motion-diffusion-model&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/human-motion-diffusion-model/motion-synthesis-on-humanact12&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/motion-synthesis-on-humanml3d?p=human-motion-diffusion-model&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/human-motion-diffusion-model/motion-synthesis-on-humanml3d&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.14916&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-%3C2209.14916%3E-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The official PyTorch implementation of the paper &lt;a href=&#34;https://arxiv.org/abs/2209.14916&#34;&gt;&lt;strong&gt;&#34;Human Motion Diffusion Model&#34;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please visit our &lt;a href=&#34;https://guytevet.github.io/mdm-page/&#34;&gt;&lt;strong&gt;webpage&lt;/strong&gt;&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/GuyTevet/mdm-page/raw/main/static/figures/github.gif&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Bibtex&lt;/h4&gt; &#xA;&lt;p&gt;If you find this code useful in your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{tevet2022human,&#xA;  title={Human Motion Diffusion Model},&#xA;  author={Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Bermano, Amit H and Cohen-Or, Daniel},&#xA;  journal={arXiv preprint arXiv:2209.14916},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;📢 &lt;strong&gt;9/Oct/22&lt;/strong&gt; - Added training and evaluation scripts. Note slight env changes adapting to the new code. If you already have an installed environment, run &lt;code&gt;bash prepare/download_glove.sh; pip install clearml&lt;/code&gt; to adapt.&lt;/p&gt; &#xA;&lt;p&gt;📢 &lt;strong&gt;6/Oct/22&lt;/strong&gt; - First release - sampling and rendering using pre-trained models.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;This code was tested on &lt;code&gt;Ubuntu 18.04.5 LTS&lt;/code&gt; and requires:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7&lt;/li&gt; &#xA; &lt;li&gt;conda3 or miniconda3&lt;/li&gt; &#xA; &lt;li&gt;CUDA capable GPU (one is enough)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. Setup environment&lt;/h3&gt; &#xA;&lt;p&gt;Install ffmpeg (if not already installed):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt update&#xA;sudo apt install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For windows use &lt;a href=&#34;https://www.geeksforgeeks.org/how-to-install-ffmpeg-on-windows/&#34;&gt;this&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;Setup conda env:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f environment.yml&#xA;conda activate mdm&#xA;python -m spacy download en_core_web_sm&#xA;pip install git+https://github.com/openai/CLIP.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash prepare/download_smpl_files.sh&#xA;bash prepare/download_glove.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Get data&lt;/h3&gt; &#xA;&lt;p&gt;There are two paths to get the data:&lt;/p&gt; &#xA;&lt;p&gt;(a) &lt;strong&gt;Go the easy way if&lt;/strong&gt; you just want to generate text-to-motion (excluding editing which does require motion capture data)&lt;/p&gt; &#xA;&lt;p&gt;(b) &lt;strong&gt;Get full data&lt;/strong&gt; to train and evaluate the model.&lt;/p&gt; &#xA;&lt;h4&gt;a. The easy way (text only)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt; - Clone HumanML3D, then copy the data dir to our repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ..&#xA;git clone https://github.com/EricGuo5513/HumanML3D.git&#xA;unzip ./HumanML3D/HumanML3D/texts.zip -d ./HumanML3D/HumanML3D/&#xA;cp -r HumanML3D/HumanML3D motion-diffusion-model/dataset/HumanML3D&#xA;cd motion-diffusion-model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;b. Full data (text + motion capture)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt; - Follow the instructions in &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt;, then copy the result dataset to our repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cp -r ../HumanML3D/HumanML3D ./dataset/HumanML3D&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt; - Download from &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt; (no processing needed this time) and the place result in &lt;code&gt;./dataset/KIT-ML&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3. Download the pretrained models&lt;/h3&gt; &#xA;&lt;p&gt;Download the model(s) you wish to use, then unzip and place it in &lt;code&gt;./save/&lt;/code&gt;. &lt;strong&gt;For text-to-motion, you need only the first one.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1PE0PK8e5a5j-7-Xhs5YET5U5pGh0c821/view?usp=sharing&#34;&gt;humanml-encoder-512&lt;/a&gt; (best model)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1q3soLadvVh7kJuJPd2cegMNY2xVuVudj/view?usp=sharing&#34;&gt;humanml-decoder-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1GnsW0K3UjuOkNkAWmjrGIUmeDDZrmPE5/view?usp=sharing&#34;&gt;humanml-decoder-with-emb-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1SHCRcE0es31vkJMLGf9dyLe7YsWj7pNL/view?usp=sharing&#34;&gt;kit-encoder-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Generate text-to-motion&lt;/h2&gt; &#xA;&lt;h3&gt;Generate from test set prompts&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --num_samples 10 --num_repetitions 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate from your text file&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --input_text ./assets/example_text_prompts.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate a single prompt&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --text_prompt &#34;the person walked forward and is picking up his toolbox.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can also define:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--device&lt;/code&gt; id.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--seed&lt;/code&gt; to sample different prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--motion_length&lt;/code&gt; in seconds (maximum is 9.8[sec]).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running those will get you:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;results.npy&lt;/code&gt; file with text prompts and xyz positions of the generated animation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample##_rep##.mp4&lt;/code&gt; - a stick figure animation for each generated motion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It will look something like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GuyTevet/motion-diffusion-model/main/assets/example_stick_fig.gif&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can stop here, or render the SMPL mesh using the following script.&lt;/p&gt; &#xA;&lt;h3&gt;Render SMPL mesh&lt;/h3&gt; &#xA;&lt;p&gt;To create SMPL mesh per frame run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m visualize.render_mesh --input_path /path/to/mp4/stick/figure/file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;This script outputs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sample##_rep##_smpl_params.npy&lt;/code&gt; - SMPL parameters (thetas, root translations, vertices and faces)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample##_rep##_obj&lt;/code&gt; - Mesh per frame in &lt;code&gt;.obj&lt;/code&gt; format.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;.obj&lt;/code&gt; can be integrated into Blender/Maya/3DS-MAX and rendered using them.&lt;/li&gt; &#xA; &lt;li&gt;This script is running &lt;a href=&#34;https://smplify.is.tue.mpg.de/&#34;&gt;SMPLify&lt;/a&gt; and needs GPU as well (can be specified with the &lt;code&gt;--device&lt;/code&gt; flag).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Important&lt;/strong&gt; - Do not change the original &lt;code&gt;.mp4&lt;/code&gt; path before running the script.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes for 3d makers:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You have two ways to animate the sequence: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Use the &lt;a href=&#34;https://smpl.is.tue.mpg.de/index.html&#34;&gt;SMPL add-on&lt;/a&gt; and the theta parameters saved to &lt;code&gt;sample##_rep##_smpl_params.npy&lt;/code&gt; (we always use beta=0 and the gender-neutral model).&lt;/li&gt; &#xA;   &lt;li&gt;A more straightforward way is using the mesh data itself. All meshes have the same topology (SMPL), so you just need to keyframe vertex locations. Since the OBJs are not preserving vertices order, we also save this data to the &lt;code&gt;sample##_rep##_smpl_params.npy&lt;/code&gt; file for your convenience.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Editing&lt;/h3&gt; &#xA;&lt;p&gt;ETA - Nov 22&lt;/p&gt; &#xA;&lt;h2&gt;Train your own MDM&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m train.train_mdm --save_dir save/my_humanml_trans_enc_512 --dataset humanml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m train.train_mdm --save_dir save/my_kit_trans_enc_512 --dataset kit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--device&lt;/code&gt; to define GPU id.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--arch&lt;/code&gt; to choose one of the architectures reported in the paper &lt;code&gt;{trans_enc, trans_dec, gru}&lt;/code&gt; (&lt;code&gt;trans_enc&lt;/code&gt; is default).&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--train_platform_type {ClearmlPlatform, TensorboardPlatform}&lt;/code&gt; to track results with either &lt;a href=&#34;https://clear.ml/&#34;&gt;ClearML&lt;/a&gt; or &lt;a href=&#34;https://www.tensorflow.org/tensorboard&#34;&gt;Tensorboard&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--eval_during_training&lt;/code&gt; to run a short (90 minutes) evaluation for each saved checkpoint. This will slow down training but will give you better monitoring.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluate&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Takes about 20 hours (on a single GPU)&lt;/li&gt; &#xA; &lt;li&gt;The output of this script for the pre-trained models (as was reported in the paper) is provided in the checkpoints zip file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m eval.eval_humanml --model_path ./save/humanml_trans_enc_512/model000475000.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m eval.eval_humanml --model_path ./save/kit_trans_enc_512/model000400000.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This code is standing on the shoulders of giants. We want to thank the following contributors that our code is based on:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;guided-diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/GuyTevet/MotionCLIP&#34;&gt;MotionCLIP&lt;/a&gt;, &lt;a href=&#34;https://github.com/EricGuo5513/text-to-motion&#34;&gt;text-to-motion&lt;/a&gt;, &lt;a href=&#34;https://github.com/Mathux/ACTOR&#34;&gt;actor&lt;/a&gt;, &lt;a href=&#34;https://github.com/wangsen1312/joints2smpl&#34;&gt;joints2smpl&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code is distributed under an &lt;a href=&#34;https://raw.githubusercontent.com/GuyTevet/motion-diffusion-model/main/LICENSE&#34;&gt;MIT LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that our code depends on other libraries, including CLIP, SMPL, SMPL-X, PyTorch3D, and uses datasets that each have their own respective licenses that must also be followed.&lt;/p&gt;</summary>
  </entry>
</feed>