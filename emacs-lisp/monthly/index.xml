<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Emacs Lisp Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-01T02:18:13Z</updated>
  <subtitle>Monthly Trending of Emacs Lisp in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rspeele/cubescript-mode</title>
    <updated>2023-12-01T02:18:13Z</updated>
    <id>tag:github.com,2023-12-01:/rspeele/cubescript-mode</id>
    <link href="https://github.com/rspeele/cubescript-mode" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Emacs major mode for editing CubeScript (see http://sauerbraten.org/docs/config.html#cubescript)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;cubescript-mode&lt;/h1&gt; &#xA;&lt;p&gt;Emacs major mode for editing CubeScript (see &lt;a href=&#34;http://sauerbraten.org/docs/config.html#cubescript&#34;&gt;http://sauerbraten.org/docs/config.html#cubescript&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;To use, place both cubescript-mode.el and cubescript-mode-load.el in your load-path, and add (require &#39;cubescript-mode-load) to your .emacs.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>karthink/gptel</title>
    <updated>2023-12-01T02:18:13Z</updated>
    <id>tag:github.com,2023-12-01:/karthink/gptel</id>
    <link href="https://github.com/karthink/gptel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple LLM client for Emacs&lt;/p&gt;&lt;hr&gt;&lt;p&gt;#+title: GPTel: A simple LLM client for Emacs&lt;/p&gt; &#xA;&lt;p&gt;[[https://melpa.org/#/gptel][file:https://melpa.org/packages/gptel-badge.svg]]&lt;/p&gt; &#xA;&lt;p&gt;GPTel is a simple Large Language Model chat client for Emacs, with support for multiple models/backends.&lt;/p&gt; &#xA;&lt;p&gt;| LLM Backend | Supports | Requires | |-------------+----------+-------------------------| | ChatGPT | ✓ | [[https://platform.openai.com/account/api-keys][API key]] | | Azure | ✓ | Deployment and API key | | Ollama | ✓ | [[https://ollama.ai/][Ollama running locally]] | | GPT4All | ✓ | [[https://gpt4all.io/index.html][GPT4All running locally]] | | PrivateGPT | Planned | - | | Llama.cpp | Planned | - |&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;General usage&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/8607532/230516812-86510a09-a2fb-4cbd-b53f-cc2522d05a13.mp4&#34;&gt;https://user-images.githubusercontent.com/8607532/230516812-86510a09-a2fb-4cbd-b53f-cc2522d05a13.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/8607532/230516816-ae4a613a-4d01-4073-ad3f-b66fa73c6e45.mp4&#34;&gt;https://user-images.githubusercontent.com/8607532/230516816-ae4a613a-4d01-4073-ad3f-b66fa73c6e45.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Multi-LLM support demo&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github-production-user-asset-6210df.s3.amazonaws.com/8607532/278854024-ae1336c4-5b87-41f2-83e9-e415349d6a43.mp4&#34;&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/8607532/278854024-ae1336c4-5b87-41f2-83e9-e415349d6a43.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It&#39;s async and fast, streams responses.&lt;/li&gt; &#xA; &lt;li&gt;Interact with LLMs from anywhere in Emacs (any buffer, shell, minibuffer, wherever)&lt;/li&gt; &#xA; &lt;li&gt;LLM responses are in Markdown or Org markup.&lt;/li&gt; &#xA; &lt;li&gt;Supports conversations and multiple independent sessions.&lt;/li&gt; &#xA; &lt;li&gt;Save chats as regular Markdown/Org/Text files and resume them later.&lt;/li&gt; &#xA; &lt;li&gt;You can go back and edit your previous prompts or LLM responses when continuing a conversation. These will be fed back to the model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;GPTel uses Curl if available, but falls back to url-retrieve to work without external dependencies.&lt;/p&gt; &#xA;&lt;p&gt;** Contents :toc:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[[#breaking-changes][Breaking Changes]]&lt;/li&gt; &#xA; &lt;li&gt;[[#installation][Installation]] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[[#straight][Straight]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#manual][Manual]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#doom-emacs][Doom Emacs]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#spacemacs][Spacemacs]]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[[#setup][Setup]] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[[#chatgpt][ChatGPT]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#other-llm-backends][Other LLM backends]] &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;[[#azure][Azure]]&lt;/li&gt; &#xA;     &lt;li&gt;[[#gpt4all][GPT4All]]&lt;/li&gt; &#xA;     &lt;li&gt;[[#ollama][Ollama]]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[[#usage][Usage]] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[[#in-any-buffer][In any buffer:]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#in-a-dedicated-chat-buffer][In a dedicated chat buffer:]] &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;[[#save-and-restore-your-chat-sessions][Save and restore your chat sessions]]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[[#using-it-your-way][Using it your way]] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[[#extensions-using-gptel][Extensions using GPTel]]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[[#additional-configuration][Additional Configuration]]&lt;/li&gt; &#xA; &lt;li&gt;[[#why-another-llm-client][Why another LLM client?]]&lt;/li&gt; &#xA; &lt;li&gt;[[#will-you-add-feature-x][Will you add feature X?]]&lt;/li&gt; &#xA; &lt;li&gt;[[#alternatives][Alternatives]]&lt;/li&gt; &#xA; &lt;li&gt;[[#acknowledgments][Acknowledgments]]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;** Breaking Changes&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Possible breakage, see #120: If streaming responses stop working for you after upgrading to v0.5, try reinstalling gptel and deleting its native comp eln cache in =native-comp-eln-load-path=.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The user option =gptel-host= is deprecated. If the defaults don&#39;t work for you, use =gptel-make-openai= (which see) to customize server settings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;=gptel-api-key-from-auth-source= now searches for the API key using the host address for the active LLM backend, /i.e./ &#34;api.openai.com&#34; when using ChatGPT. You may need to update your =~/.authinfo=.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;** Installation&lt;/p&gt; &#xA;&lt;p&gt;GPTel is on MELPA. Ensure that MELPA is in your list of sources, then install gptel with =M-x package-install⏎= =gptel=.&lt;/p&gt; &#xA;&lt;p&gt;(Optional: Install =markdown-mode=.)&lt;/p&gt; &#xA;&lt;p&gt;#+html: &lt;/p&gt;&#xA;&lt;details&gt;&#xA; &lt;summary&gt; **** Straight #+html: &lt;/summary&gt; #+begin_src emacs-lisp (straight-use-package &#39;gptel) #+end_src&#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;Installing the =markdown-mode= package is optional. #+html: &lt;/p&gt;&#xA;&lt;/details&gt; #+html: &#xA;&lt;details&gt;&#xA; &lt;summary&gt; **** Manual #+html: &lt;/summary&gt; Clone or download this repository and run =M-x package-install-file⏎= on the repository directory.&#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;Installing the =markdown-mode= package is optional. #+html: &lt;/p&gt;&#xA;&lt;/details&gt; #+html: &#xA;&lt;details&gt;&#xA; &lt;summary&gt; **** Doom Emacs #+html: &lt;/summary&gt; In =packages.el= #+begin_src emacs-lisp (package! gptel) #+end_src&#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;In =config.el= #+begin_src emacs-lisp (use-package! gptel :config (setq! gptel-api-key &#34;your key&#34;)) #+end_src #+html: &lt;/p&gt;&#xA;&lt;/details&gt; #+html: &#xA;&lt;details&gt;&#xA; &lt;summary&gt; **** Spacemacs #+html: &lt;/summary&gt; After installation with =M-x package-install⏎= =gptel=&#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Add =gptel= to =dotspacemacs-additional-packages=&lt;/li&gt; &#xA;  &lt;li&gt;Add =(require &#39;gptel)= to =dotspacemacs/user-config= #+html: &lt;/li&gt;&#xA; &lt;/ul&gt;&#xA;&lt;/details&gt; ** Setup *** ChatGPT Procure an [[https://platform.openai.com/account/api-keys][OpenAI API key]].  &#xA;&lt;p&gt;Optional: Set =gptel-api-key= to the key. Alternatively, you may choose a more secure method such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Storing in =~/.authinfo=. By default, &#34;api.openai.com&#34; is used as HOST and &#34;apikey&#34; as USER. #+begin_src authinfo machine api.openai.com login apikey password TOKEN #+end_src&lt;/li&gt; &#xA; &lt;li&gt;Setting it to a function that returns the key.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;*** Other LLM backends #+html: &lt;/p&gt;&#xA;&lt;details&gt;&#xA; &lt;summary&gt; **** Azure #+html: &lt;/summary&gt;&#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;Register a backend with #+begin_src emacs-lisp (gptel-make-azure &#34;Azure-1&#34; ;Name, whatever you&#39;d like :protocol &#34;https&#34; ;optional -- https is the default :host &#34;YOUR_RESOURCE_NAME.openai.azure.com&#34; :endpoint &#34;/openai/deployments/YOUR_DEPLOYMENT_NAME/completions?api-version=2023-05-15&#34; ;or equivalent :stream t ;Enable streaming responses :models &#39;(&#34;gpt-3.5-turbo&#34; &#34;gpt-4&#34;)) #+end_src Refer to the documentation of =gptel-make-azure= to set more parameters.&lt;/p&gt; &#xA; &lt;p&gt;You can pick this backend from the transient menu when using gptel. (See usage)&lt;/p&gt; &#xA; &lt;p&gt;If you want it to be the default, set it as the default value of =gptel-backend=: #+begin_src emacs-lisp (setq-default gptel-backend (gptel-make-azure &#34;Azure-1&#34; ...)) #+end_src #+html: &lt;/p&gt;&#xA;&lt;/details&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;#+html: &lt;/p&gt;&#xA;&lt;details&gt;&#xA; &lt;summary&gt; **** GPT4All #+html: &lt;/summary&gt;&#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;Register a backend with #+begin_src emacs-lisp (gptel-make-gpt4all &#34;GPT4All&#34; ;Name of your choosing :protocol &#34;http&#34;&lt;br&gt; :host &#34;localhost:4891&#34; ;Where it&#39;s running :models &#39;(&#34;mistral-7b-openorca.Q4_0.gguf&#34;)) ;Available models #+end_src These are the required parameters, refer to the documentation of =gptel-make-gpt4all= for more.&lt;/p&gt; &#xA; &lt;p&gt;You can pick this backend from the transient menu when using gptel (see usage), or set this as the default value of =gptel-backend=. Additionally you may want to increase the response token size since GPT4All uses very short (often truncated) responses by default:&lt;/p&gt; &#xA; &lt;p&gt;#+begin_src emacs-lisp ;; OPTIONAL configuration (setq-default gptel-model &#34;mistral-7b-openorca.Q4_0.gguf&#34; ;Pick your default model gptel-backend (gptel-make-gpt4all &#34;GPT4All&#34; :protocol ...)) (setq-default gptel-max-tokens 500) #+end_src&lt;/p&gt; &#xA; &lt;p&gt;#+html: &lt;/p&gt;&#xA;&lt;/details&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;#+html: &lt;/p&gt;&#xA;&lt;details&gt;&#xA; &lt;summary&gt; **** Ollama #+html: &lt;/summary&gt;&#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;Register a backend with #+begin_src emacs-lisp (gptel-make-ollama &#34;Ollama&#34; ;Any name of your choosing :host &#34;localhost:11434&#34; ;Where it&#39;s running :models &#39;(&#34;mistral:latest&#34;) ;Installed models :stream t) ;Stream responses #+end_src These are the required parameters, refer to the documentation of =gptel-make-ollama= for more.&lt;/p&gt; &#xA; &lt;p&gt;You can pick this backend from the transient menu when using gptel (see Usage), or set this as the default value of =gptel-backend=:&lt;/p&gt; &#xA; &lt;p&gt;#+begin_src emacs-lisp ;; OPTIONAL configuration (setq-default gptel-model &#34;mistral:latest&#34; ;Pick your default model gptel-backend (gptel-make-ollama &#34;Ollama&#34; :host ...)) #+end_src&lt;/p&gt; &#xA; &lt;p&gt;#+html: &lt;/p&gt;&#xA;&lt;/details&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;** Usage&lt;/p&gt; &#xA;&lt;p&gt;|-------------------+-------------------------------------------------------------------------| | &lt;em&gt;Command&lt;/em&gt; | Description | |-------------------+-------------------------------------------------------------------------| | =gptel= | Create a new dedicated chat buffer. (Not required, gptel works anywhere.) | | =gptel-send= | Send selection, or conversation up to =(point)=. (Works anywhere in Emacs.) | | =C-u= =gptel-send= | Transient menu for preferenes, input/output redirection etc. | | =gptel-menu= | /(Same)/ | |-------------------+-------------------------------------------------------------------------| | =gptel-set-topic= | /(Org-mode only)/ Limit conversation context to an Org heading | |-------------------+-------------------------------------------------------------------------|&lt;/p&gt; &#xA;&lt;p&gt;*** In any buffer:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Select a region of text and call =M-x gptel-send=. The response will be inserted below your region.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can select both the original prompt and the response and call =M-x gptel-send= again to continue the conversation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Call =M-x gptel-send= with a prefix argument to&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set chat parameters (GPT model, directives etc) for this buffer,&lt;/li&gt; &#xA; &lt;li&gt;to read the prompt from elsewhere or redirect the response elsewhere,&lt;/li&gt; &#xA; &lt;li&gt;or to replace the prompt with the response.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[[https://user-images.githubusercontent.com/8607532/230770018-9ce87644-6c17-44af-bd39-8c899303dce1.png]]&lt;/p&gt; &#xA;&lt;p&gt;With a region selected, you can also rewrite prose or refactor code from here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Code&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;p&gt;[[https://user-images.githubusercontent.com/8607532/230770162-1a5a496c-ee57-4a67-9c95-d45f238544ae.png]]&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Prose&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;p&gt;[[https://user-images.githubusercontent.com/8607532/230770352-ee6f45a3-a083-4cf0-b13c-619f7710e9ba.png]]&lt;/p&gt; &#xA;&lt;p&gt;*** In a dedicated chat buffer:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Run =M-x gptel= to start or switch to the chat buffer. It will ask you for the key if you skipped the previous step. Run it with a prefix-arg (=C-u M-x gptel=) to start a new session.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the gptel buffer, send your prompt with =M-x gptel-send=, bound to =C-c RET=.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set chat parameters (LLM provider, model, directives etc) for the session by calling =gptel-send= with a prefix argument (=C-u C-c RET=):&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;[[https://user-images.githubusercontent.com/8607532/224946059-9b918810-ab8b-46a6-b917-549d50c908f2.png]]&lt;/p&gt; &#xA;&lt;p&gt;That&#39;s it. You can go back and edit previous prompts and responses if you want.&lt;/p&gt; &#xA;&lt;p&gt;The default mode is =markdown-mode= if available, else =text-mode=. You can set =gptel-default-mode= to =org-mode= if desired.&lt;/p&gt; &#xA;&lt;p&gt;**** Save and restore your chat sessions&lt;/p&gt; &#xA;&lt;p&gt;Saving the file will save the state of the conversation as well. To resume the chat, open the file and turn on =gptel-mode= before editing the buffer.&lt;/p&gt; &#xA;&lt;p&gt;** Using it your way&lt;/p&gt; &#xA;&lt;p&gt;GPTel&#39;s default usage pattern is simple, and will stay this way: Read input in any buffer and insert the response below it.&lt;/p&gt; &#xA;&lt;p&gt;If you want custom behavior, such as&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;reading input from or output to the echo area,&lt;/li&gt; &#xA; &lt;li&gt;or in pop-up windows,&lt;/li&gt; &#xA; &lt;li&gt;sending the current line only, etc,&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;GPTel provides a general =gptel-request= function that accepts a custom prompt and a callback to act on the response. You can use this to build custom workflows not supported by =gptel-send=. See the documentation of =gptel-request=, and the [[https://github.com/karthink/gptel/wiki][wiki]] for examples.&lt;/p&gt; &#xA;&lt;p&gt;*** Extensions using GPTel&lt;/p&gt; &#xA;&lt;p&gt;These are packages that depend on GPTel to provide additional functionality&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[[https://github.com/kamushadenes/gptel-extensions.el][gptel-extensions]]: Extra utility functions for GPTel.&lt;/li&gt; &#xA; &lt;li&gt;[[https://github.com/kamushadenes/ai-blog.el][ai-blog.el]]: Streamline generation of blog posts in Hugo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;** Additional Configuration :PROPERTIES: :ID: f885adac-58a3-4eba-a6b7-91e9e7a17829 :END:&lt;/p&gt; &#xA;&lt;p&gt;#+begin_src emacs-lisp :exports none :results list (let ((all)) (mapatoms (lambda (sym) (when (and (string-match-p &#34;^gptel-[^-]&#34; (symbol-name sym)) (get sym &#39;variable-documentation)) (push sym all)))) all) #+end_src&lt;/p&gt; &#xA;&lt;p&gt;|---------------------------+---------------------------------------------------------------------| | &lt;em&gt;Connection options&lt;/em&gt; | | |---------------------------+---------------------------------------------------------------------| | =gptel-use-curl= | Use Curl (default), fallback to Emacs&#39; built-in =url=. | | =gptel-proxy= | Proxy server for requests, passed to curl via =--proxy=. | | =gptel-api-key= | Variable/function that returns the API key for the active backend. | |---------------------------+---------------------------------------------------------------------|&lt;/p&gt; &#xA;&lt;p&gt;|---------------------------+---------------------------------------------------------------------| | &lt;em&gt;LLM options&lt;/em&gt; | /(Note: not supported uniformly across LLMs)/ | |---------------------------+---------------------------------------------------------------------| | =gptel-backend= | Default LLM Backend. | | =gptel-model= | Default model to use (depends on the backend). | | =gptel-stream= | Enable streaming responses (overrides backend-specific preference). | | =gptel-directives= | Alist of system directives, can switch on the fly. | | =gptel-max-tokens= | Maximum token count (in query + response). | | =gptel-temperature= | Randomness in response text, 0 to 2. | |---------------------------+---------------------------------------------------------------------|&lt;/p&gt; &#xA;&lt;p&gt;|---------------------------+---------------------------------------------------------------------| | &lt;em&gt;Chat UI options&lt;/em&gt; | | |---------------------------+---------------------------------------------------------------------| | =gptel-default-mode= | Major mode for dedicated chat buffers. | | =gptel-prompt-prefix-alist= | Text demarcating queries and replies. | |---------------------------+---------------------------------------------------------------------|&lt;/p&gt; &#xA;&lt;p&gt;** Why another LLM client?&lt;/p&gt; &#xA;&lt;p&gt;Other Emacs clients for LLMs prescribe the format of the interaction (a comint shell, org-babel blocks, etc). I wanted:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Something that is as free-form as possible: query the model using any text in any buffer, and redirect the response as required. Using a dedicated =gptel= buffer just adds some visual flair to the interaction.&lt;/li&gt; &#xA; &lt;li&gt;Integration with org-mode, not using a walled-off org-babel block, but as regular text. This way the model can generate code blocks that I can run.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;** Will you add feature X?&lt;/p&gt; &#xA;&lt;p&gt;Maybe, I&#39;d like to experiment a bit more first. Features added since the inception of this package include&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Curl support (=gptel-use-curl=)&lt;/li&gt; &#xA; &lt;li&gt;Streaming responses (=gptel-stream=)&lt;/li&gt; &#xA; &lt;li&gt;Cancelling requests in progress (=gptel-abort=)&lt;/li&gt; &#xA; &lt;li&gt;General API for writing your own commands (=gptel-request=, [[https://github.com/karthink/gptel/wiki][wiki]])&lt;/li&gt; &#xA; &lt;li&gt;Dispatch menus using Transient (=gptel-send= with a prefix arg)&lt;/li&gt; &#xA; &lt;li&gt;Specifying the conversation context size&lt;/li&gt; &#xA; &lt;li&gt;GPT-4 support&lt;/li&gt; &#xA; &lt;li&gt;Response redirection (to the echo area, another buffer, etc)&lt;/li&gt; &#xA; &lt;li&gt;A built-in refactor/rewrite prompt&lt;/li&gt; &#xA; &lt;li&gt;Limiting conversation context to Org headings using properties (#58)&lt;/li&gt; &#xA; &lt;li&gt;Saving and restoring chats (#17)&lt;/li&gt; &#xA; &lt;li&gt;Support for local LLMs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Features being considered or in the pipeline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fully stateless design (#17)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;** Alternatives&lt;/p&gt; &#xA;&lt;p&gt;Other Emacs clients for LLMs include&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[[https://github.com/xenodium/chatgpt-shell][chatgpt-shell]]: comint-shell based interaction with ChatGPT. Also supports DALL-E, executable code blocks in the responses, and more.&lt;/li&gt; &#xA; &lt;li&gt;[[https://github.com/rksm/org-ai][org-ai]]: Interaction through special =#+begin_ai ... #+end_ai= Org-mode blocks. Also supports DALL-E, querying ChatGPT with the contents of project files, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are several more: [[https://github.com/CarlQLange/chatgpt-arcana.el][chatgpt-arcana]], [[https://github.com/MichaelBurge/leafy-mode][leafy-mode]], [[https://github.com/iwahbe/chat.el][chat.el]]&lt;/p&gt; &#xA;&lt;p&gt;** Acknowledgments&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[[https://github.com/algal][Alexis Gallagher]] and [[https://github.com/d1egoaz][Diego Alvarez]] for fixing a nasty multi-byte bug with =url-retrieve=.&lt;/li&gt; &#xA; &lt;li&gt;[[https://github.com/tarsius][Jonas Bernoulli]] for the Transient library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Local Variables:&lt;/h1&gt; &#xA;&lt;h1&gt;toc-org-max-depth: 4&lt;/h1&gt; &#xA;&lt;h1&gt;End:&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>emacs-lsp/dap-mode</title>
    <updated>2023-12-01T02:18:13Z</updated>
    <id>tag:github.com,2023-12-01:/emacs-lsp/dap-mode</id>
    <link href="https://github.com/emacs-lsp/dap-mode" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Emacs ❤️ Debug Adapter Protocol&lt;/p&gt;&lt;hr&gt;&lt;p&gt;[[https://melpa.org/#/dap-mode][file:https://melpa.org/packages/dap-mode-badge.svg]] [[https://stable.melpa.org/#/dap-mode][file:https://stable.melpa.org/packages/dap-mode-badge.svg]] [[http://spacemacs.org][file:https://cdn.rawgit.com/syl20bnr/spacemacs/442d025779da2f62fc86c2082703697714db6514/assets/spacemacs-badge.svg]] [[https://github.com/emacs-lsp/dap-mode/actions][file:https://github.com/emacs-lsp/dap-mode/workflows/CI/badge.svg]] [[https://discord.gg/swuxy5AAgT][file:https://discordapp.com/api/guilds/789885435026604033/widget.png?style=shield]]&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;dap-mode ** Table of Contents :TOC_4_gh:noexport:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[[#dap-mode][dap-mode]] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[[#summary][Summary]] &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;[[#project-status][Project status]]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;[[#usage][Usage]] &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;[[#docker-usage][Docker usage]]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;[[#features][Features]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#configuration][Configuration]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#gallery][Gallery]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#extending-dap-with-new-debug-servers][Extending DAP with new Debug servers]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#links][Links]]&lt;/li&gt; &#xA;   &lt;li&gt;[[#acknowledgments][Acknowledgments]]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;** Summary Emacs client/library for [[https://microsoft.github.io/debug-adapter-protocol/][Debug Adapter Protocol]] is a wire protocol for communication between client and Debug Server. It&#39;s similar to the [[https://github.com/Microsoft/language-server-protocol][LSP]] but provides integration with debug server. *** Project status The API considered unstable until 1.0 release is out. It is tested against Java, Python, Ruby, Elixir and LLDB (C/C++/Objective-C/Swift). ** Usage The main entry points are &lt;del&gt;dap-debug&lt;/del&gt; and &lt;del&gt;dap-debug-edit-template&lt;/del&gt;. The first one asks for a registered debug template and starts the configuration using the default values for that particular configuration. The latter creates a debug template which could be customized before running. &lt;del&gt;dap-debug-edit-template&lt;/del&gt; will prepare a template declaration inside a temporary buffer. You should execute this code using &lt;del&gt;C-M-x&lt;/del&gt; for the changes to apply. You should also copy this code into your Emacs configuration if you wish to make it persistent.&lt;/p&gt; &#xA;&lt;p&gt;dap-mode also provides a [[https://github.com/abo-abo/hydra][hydra]] with &lt;del&gt;dap-hydra&lt;/del&gt;. You can automatically trigger the hydra when the program hits a breakpoint by using the following code.&lt;/p&gt; &#xA;&lt;p&gt;#+BEGIN_SRC elisp (add-hook &#39;dap-stopped-hook (lambda (arg) (call-interactively #&#39;dap-hydra))) #+END_SRC&lt;/p&gt; &#xA;&lt;p&gt;*** Docker usage You can also use this tool with dockerized debug servers: configure it either with a &lt;del&gt;.dir-locals&lt;/del&gt; file or drop an &lt;del&gt;.lsp-docker.yml&lt;/del&gt; configuration file (use [[https://github.com/emacs-lsp/lsp-docker][lsp-docker]] for general reference). Basically you have one function &lt;del&gt;dap-docker-register&lt;/del&gt; that performs all the heavy lifting (finding the original debug template, patching it, registering a debug provider e.t.c). This function examines a configuration file or falls back to the default configuration (which can be patched using the &lt;del&gt;.dir-locals&lt;/del&gt; approach, take a note that the default configuration doesn&#39;t provide any sane defaults for debugging) and then operates on the combination of the two. This mechanism is the same as in &lt;del&gt;lsp-docker&lt;/del&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Note: currently you cannot use this mode when using a network connection to connect to debuggers (this part is yet to be implemented).&#xA;Still want to talk to debuggers over network? In order to do so you have to look at the ~launch-args~ patching&#xA;done by ~dap-docker--dockerize-start-file-args~, you have to somehow assign ~nil~ to ~dap-server-path~ before it is passed further into session creation.&#xA;&#xA;If you want to stick to a configuration file, take a look at the example below:&#xA;&#xA;#+begin_src yaml&#xA;lsp:&#xA;  server:&#xA;    # &#39;lsp-docker&#39; fields&#xA;  mappings:&#xA;    - source: &#34;/your/host/source/path&#34; # used both by &#39;lsp-docker&#39; and &#39;dap-docker&#39;&#xA;      destination: &#34;/your/local/path/inside/a/container&#34; # used both by &#39;lsp-docker&#39; and &#39;dap-docker&#39;&#xA;  debug:&#xA;    type: docker # only docker is supported&#xA;    subtype: image # or &#39;container&#39;&#xA;    name: &amp;lt;docker image or container that has the debugger in&amp;gt; # you can omit this field&#xA;    # in this case the &#39;lsp-docker&#39; (&#39;server&#39; section) image name is used&#xA;    enabled: true # you can explicitly disable &#39;dap-docker&#39; by using &#39;false&#39;&#xA;    provider: &amp;lt;your default language debug provider, double quoted string&amp;gt;&#xA;    template: &amp;lt;your default language debug template, double quoted string&amp;gt;&#xA;    launch_command: &amp;lt;an explicit command if you want to override a default one provided by the debug provider&amp;gt;&#xA;    # e.g. if you have installed a debug server in a different directory, not used with &#39;container&#39; subtype debuggers&#xA;#+end_src&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;** [[https://emacs-lsp.github.io/dap-mode/page/features/][Features]] ** [[https://emacs-lsp.github.io/dap-mode/page/configuration/][Configuration]] ** [[https://emacs-lsp.github.io/dap-mode/page/gallery][Gallery]] ** [[https://emacs-lsp.github.io/dap-mode/page/adding-debug-server][Extending DAP with new Debug servers]] ** Links&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[[https://code.visualstudio.com/docs/extensionAPI/api-debugging][Debug Adapter Protocol]]&lt;/li&gt; &#xA; &lt;li&gt;[[https://github.com/emacs-lsp/lsp-java][LSP Java]]&lt;/li&gt; &#xA; &lt;li&gt;[[https://microsoft.github.io/debug-adapter-protocol/implementors/adapters/][Debug Adapter Protocol Server Implementations]] ** Acknowledgments&lt;/li&gt; &#xA; &lt;li&gt;[[https://github.com/danielmartin][Daniel Martin]] - LLDB integration.&lt;/li&gt; &#xA; &lt;li&gt;[[https://github.com/kiennq][Kien Nguyen]] - NodeJS debugger, Edge debuggers, automatic extension installation.&lt;/li&gt; &#xA; &lt;li&gt;[[https://github.com/Ladicle][Aya Igarashi]] - Go debugger integration.&lt;/li&gt; &#xA; &lt;li&gt;[[https://github.com/nbfalcon][Nikita Bloshchanevich]] - launch.json support (+ variable expansion), debugpy support, (with some groundwork by yyoncho) runInTerminal support, various bug fixes.&lt;/li&gt; &#xA; &lt;li&gt;[[https://github.com/factyy][Andrei Mochalov]] - Docker (debugging in containers) integration.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>