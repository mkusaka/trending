<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Emacs Lisp Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-14T01:30:47Z</updated>
  <subtitle>Daily Trending of Emacs Lisp in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>milanglacier/minuet-ai.el</title>
    <updated>2025-05-14T01:30:47Z</updated>
    <id>tag:github.com,2025-05-14:/milanglacier/minuet-ai.el</id>
    <link href="https://github.com/milanglacier/minuet-ai.el" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ðŸ’ƒ Dance with LLM in Your Code. Minuet offers code completion as-you-type from popular LLMs including OpenAI, Gemini, Claude, Ollama, Llama.cpp, Codestral, and more.&lt;/p&gt;&lt;hr&gt;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet&#34;&gt;Minuet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#quick-start-llm-provider-examples&#34;&gt;Quick Start: LLM Provider Examples&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#ollama-qwen-25-coder3b&#34;&gt;Ollama Qwen-2.5-coder:3b&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#openrouter-deepseek-v3-0324&#34;&gt;OpenRouter Deepseek-V3-0324&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#llamacpp-qwen-25-coder15b&#34;&gt;Llama.cpp Qwen-2.5-coder:1.5b&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#api-keys&#34;&gt;API Keys&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#selecting-a-provider-or-model&#34;&gt;Selecting a Provider or Model&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#understanding-model-speed&#34;&gt;Understanding Model Speed&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#prompt&#34;&gt;Prompt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#configuration&#34;&gt;Configuration&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet-provider&#34;&gt;minuet-provider&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet-context-window&#34;&gt;minuet-context-window&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet-context-ratio&#34;&gt;minuet-context-ratio&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet-request-timeout&#34;&gt;minuet-request-timeout&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet-show-error-message-on-minibuffer&#34;&gt;minuet-show-error-message-on-minibuffer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet-add-single-line-entry&#34;&gt;minuet-add-single-line-entry&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet-n-completions&#34;&gt;minuet-n-completions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet-auto-suggestion-debounce-delay&#34;&gt;minuet-auto-suggestion-debounce-delay&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#minuet-auto-suggestion-throttle-delay&#34;&gt;minuet-auto-suggestion-throttle-delay&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#provider-options&#34;&gt;Provider Options&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#openai&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#claude&#34;&gt;Claude&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#codestral&#34;&gt;Codestral&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#gemini&#34;&gt;Gemini&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#openai-compatible&#34;&gt;OpenAI-compatible&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#openai-fim-compatible&#34;&gt;OpenAI-FIM-Compatible&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#non-openai-fim-compatible-apis&#34;&gt;Non-OpenAI-FIM-Compatible APIs&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#contributions&#34;&gt;Contributions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Minuet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://elpa.gnu.org/packages/minuet.html&#34;&gt;&lt;img src=&#34;https://elpa.gnu.org/packages/minuet.svg?sanitize=true&#34; alt=&#34;GNU ELPA badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://melpa.org/#/minuet&#34;&gt;&lt;img src=&#34;https://melpa.org/packages/minuet-badge.svg?sanitize=true&#34; alt=&#34;MELPA badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Minuet: Dance with LLM in Your Code ðŸ’ƒ.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Minuet&lt;/code&gt; brings the grace and harmony of a minuet to your coding process. Just as dancers move during a minuet.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLM-powered code completion with dual modes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Specialized prompts and various enhancements for chat-based LLMs on code completion tasks.&lt;/li&gt; &#xA;   &lt;li&gt;Fill-in-the-middle (FIM) completion for compatible models (DeepSeek, Codestral, and some Ollama models).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support for multiple LLM providers (OpenAI, Claude, Gemini, Codestral, Ollama, Llama.cpp and OpenAI-compatible providers)&lt;/li&gt; &#xA; &lt;li&gt;Customizable configuration options&lt;/li&gt; &#xA; &lt;li&gt;Streaming support to enable completion delivery even with slower LLMs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;With minibuffer frontend&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/assets/minuet-completion-in-region.jpg&#34; alt=&#34;example-completion-in-region&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;With overlay ghost text frontend&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/assets/minuet-overlay.jpg&#34; alt=&#34;example-overlay&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;emacs 29+ compiled with native JSON support (verify with &lt;code&gt;json-available-p&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;plz 0.9+&lt;/li&gt; &#xA; &lt;li&gt;dash&lt;/li&gt; &#xA; &lt;li&gt;An API key for at least one of the supported LLM providers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;minuet&lt;/code&gt; is available on ELPA and MELPA and can be installed using your preferred package managers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-elisp&#34;&gt;&#xA;;; install with package.el&#xA;(package-install &#39;minuet)&#xA;;; install with straight&#xA;(straight-use-package &#39;minuet)&#xA;&#xA;(use-package minuet&#xA;    :bind&#xA;    ((&#34;M-y&#34; . #&#39;minuet-complete-with-minibuffer) ;; use minibuffer for completion&#xA;     (&#34;M-i&#34; . #&#39;minuet-show-suggestion) ;; use overlay for completion&#xA;     (&#34;C-c m&#34; . #&#39;minuet-configure-provider)&#xA;     :map minuet-active-mode-map&#xA;     ;; These keymaps activate only when a minuet suggestion is displayed in the current buffer&#xA;     (&#34;M-p&#34; . #&#39;minuet-previous-suggestion) ;; invoke completion or cycle to next completion&#xA;     (&#34;M-n&#34; . #&#39;minuet-next-suggestion) ;; invoke completion or cycle to previous completion&#xA;     (&#34;M-A&#34; . #&#39;minuet-accept-suggestion) ;; accept whole completion&#xA;     ;; Accept the first line of completion, or N lines with a numeric-prefix:&#xA;     ;; e.g. C-u 2 M-a will accepts 2 lines of completion.&#xA;     (&#34;M-a&#34; . #&#39;minuet-accept-suggestion-line)&#xA;     (&#34;M-e&#34; . #&#39;minuet-dismiss-suggestion))&#xA;&#xA;    :init&#xA;    ;; if you want to enable auto suggestion.&#xA;    ;; Note that you can manually invoke completions without enable minuet-auto-suggestion-mode&#xA;    (add-hook &#39;prog-mode-hook #&#39;minuet-auto-suggestion-mode)&#xA;&#xA;    :config&#xA;    ;; You can use M-x minuet-configure-provider to interactively configure provider and model&#xA;    (setq minuet-provider &#39;openai-fim-compatible)&#xA;&#xA;    (minuet-set-optional-options minuet-openai-fim-compatible-options :max_tokens 64))&#xA;&#xA;    ;; For Evil users: When defining `minuet-ative-mode-map` in insert&#xA;    ;; or normal states, the following one-liner is required.&#xA;&#xA;    ;; (add-hook &#39;minuet-active-mode-hook #&#39;evil-normalize-keymaps)&#xA;&#xA;    ;; This is *not* necessary when defining `minuet-active-mode-map`.&#xA;&#xA;    ;; To minimize frequent overhead, it is recommended to avoid adding&#xA;    ;; `evil-normalize-keymaps` to `minuet-active-mode-hook`. Instead,&#xA;    ;; bind keybindings directly within `minuet-active-mode-map` using&#xA;    ;; standard Emacs key sequences, such as `M-xxx`. This approach should&#xA;    ;; not conflict with Evil&#39;s keybindings, as Evil primarily avoids&#xA;    ;; using `M-xxx` bindings.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Quick Start: LLM Provider Examples&lt;/h1&gt; &#xA;&lt;h2&gt;Ollama Qwen-2.5-coder:3b&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-elisp&#34;&gt;(use-package minuet&#xA;    :config&#xA;    (setq minuet-provider &#39;openai-fim-compatible)&#xA;    (setq minuet-n-completions 1) ; recommended for Local LLM for resource saving&#xA;    ;; I recommend beginning with a small context window size and incrementally&#xA;    ;; expanding it, depending on your local computing power. A context window&#xA;    ;; of 512, serves as an good starting point to estimate your computing&#xA;    ;; power. Once you have a reliable estimate of your local computing power,&#xA;    ;; you should adjust the context window to a larger value.&#xA;    (setq minuet-context-window 512)&#xA;    (plist-put minuet-openai-fim-compatible-options :end-point &#34;http://localhost:11434/v1/completions&#34;)&#xA;    ;; an arbitrary non-null environment variable as placeholder.&#xA;    ;; For Windows users, TERM may not be present in environment variables.&#xA;    ;; Consider using APPDATA instead.&#xA;    (plist-put minuet-openai-fim-compatible-options :name &#34;Ollama&#34;)&#xA;    (plist-put minuet-openai-fim-compatible-options :api-key &#34;TERM&#34;)&#xA;    (plist-put minuet-openai-fim-compatible-options :model &#34;qwen2.5-coder:3b&#34;)&#xA;&#xA;    (minuet-set-optional-options minuet-openai-fim-compatible-options :max_tokens 56))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;OpenRouter Deepseek-V3-0324&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-elisp&#34;&gt;(use-package minuet&#xA;    :config&#xA;    (setq minuet-provider &#39;openai-compatible)&#xA;    (setq minuet-request-timeout 2.5)&#xA;    (setq minuet-auto-suggestion-throttle-delay 1.5) ;; Increase to reduce costs and avoid rate limits&#xA;    (setq minuet-auto-suggestion-debounce-delay 0.6) ;; Increase to reduce costs and avoid rate limits&#xA;&#xA;    (plist-put minuet-openai-compatible-options :end-point &#34;https://openrouter.ai/api/v1/chat/completions&#34;)&#xA;    (plist-put minuet-openai-compatible-options :api-key &#34;OPENROUTER_API_KEY&#34;)&#xA;    (plist-put minuet-openai-compatible-options :model &#34;deepseek/deepseek-chat-v3-0324&#34;)&#xA;&#xA;&#xA;    ;; Prioritize throughput for faster completion&#xA;    (minuet-set-optional-options minuet-openai-compatible-options :provider &#39;(:sort &#34;throughput&#34;))&#xA;    (minuet-set-optional-options minuet-openai-compatible-options :max_tokens 56)&#xA;    (minuet-set-optional-options minuet-openai-compatible-options :top_p 0.9))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Llama.cpp Qwen-2.5-coder:1.5b&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;First, launch the &lt;code&gt;llama-server&lt;/code&gt; with your chosen model.&lt;/p&gt; &#xA; &lt;p&gt;Here&#39;s an example of a bash script to start the server if your system has less than 8GB of VRAM:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-server \&#xA;    -hf ggml-org/Qwen2.5-Coder-1.5B-Q8_0-GGUF \&#xA;    --port 8012 -ngl 99 -fa -ub 1024 -b 1024 \&#xA;    --ctx-size 0 --cache-reuse 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-elisp&#34;&gt;(use-package minuet&#xA;    :config&#xA;    (setq minuet-provider &#39;openai-fim-compatible)&#xA;    (setq minuet-n-completions 1) ; recommended for Local LLM for resource saving&#xA;    ;; I recommend beginning with a small context window size and incrementally&#xA;    ;; expanding it, depending on your local computing power. A context window&#xA;    ;; of 512, serves as an good starting point to estimate your computing&#xA;    ;; power. Once you have a reliable estimate of your local computing power,&#xA;    ;; you should adjust the context window to a larger value.&#xA;    (setq minuet-context-window 512)&#xA;    (plist-put minuet-openai-fim-compatible-options :end-point &#34;http://localhost:8012/v1/completions&#34;)&#xA;    ;; an arbitrary non-null environment variable as placeholder&#xA;    ;; For Windows users, TERM may not be present in environment variables.&#xA;    ;; Consider using APPDATA instead.&#xA;    (plist-put minuet-openai-fim-compatible-options :name &#34;Llama.cpp&#34;)&#xA;    (plist-put minuet-openai-fim-compatible-options :api-key &#34;TERM&#34;)&#xA;    ;; The model is set by the llama-cpp server and cannot be altered&#xA;    ;; post-launch.&#xA;    (plist-put minuet-openai-fim-compatible-options :model &#34;PLACEHOLDER&#34;)&#xA;&#xA;    ;; Llama.cpp does not support the `suffix` option in FIM completion.&#xA;    ;; Therefore, we must disable it and manually populate the special&#xA;    ;; tokens required for FIM completion.&#xA;    (minuet-set-nested-plist minuet-openai-fim-compatible-options nil :template :suffix)&#xA;    (minuet-set-optional-options&#xA;     minuet-openai-fim-compatible-options&#xA;     :prompt&#xA;     (defun minuet-llama-cpp-fim-qwen-prompt-function (ctx)&#xA;         (format &#34;&amp;lt;|fim_prefix|&amp;gt;%s\n%s&amp;lt;|fim_suffix|&amp;gt;%s&amp;lt;|fim_middle|&amp;gt;&#34;&#xA;                 (plist-get ctx :language-and-tab)&#xA;                 (plist-get ctx :before-cursor)&#xA;                 (plist-get ctx :after-cursor)))&#xA;     :template)&#xA;&#xA;    (minuet-set-optional-options minuet-openai-fim-compatible-options :max_tokens 56))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For additional example bash scripts to run llama.cpp based on your local computing power, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/recipes.md&#34;&gt;recipes.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;API Keys&lt;/h1&gt; &#xA;&lt;p&gt;Minuet requires API keys to function. Set the following environment variables:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt; for OpenAI&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt; for Gemini&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; for Claude&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CODESTRAL_API_KEY&lt;/code&gt; for Codestral&lt;/li&gt; &#xA; &lt;li&gt;Custom environment variable for OpenAI-compatible services (as specified in your configuration)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Provide the name of the environment variable to Minuet inside the provider options, not the actual value. For instance, pass &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to Minuet, not the value itself (e.g., &lt;code&gt;sk-xxxx&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;If using Ollama, you need to assign an arbitrary, non-null environment variable as a placeholder for it to function.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can provide a function that returns the API key. This function should return the result instantly as it will be called with each completion request.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;;; Good&#xA;(plist-put minuet-openai-compatible-options :api-key &#34;FIREWORKS_API_KEY&#34;)&#xA;(plist-put minuet-openai-compatible-options :api-key (defun my-fireworks-api-key () &#34;sk-xxxx&#34;))&#xA;;; Bad&#xA;(plist-put minuet-openai-compatible-options :api-key &#34;sk-xxxxx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Selecting a Provider or Model&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;code&gt;gemini-flash&lt;/code&gt; and &lt;code&gt;codestral&lt;/code&gt; models offer high-quality output with free and fast processing. For optimal quality (albeit slower generation speed), consider using the &lt;code&gt;deepseek-chat&lt;/code&gt; model, which is compatible with both &lt;code&gt;openai-fim-compatible&lt;/code&gt; and &lt;code&gt;openai-compatible&lt;/code&gt; providers. For local LLM inference, you can deploy either &lt;code&gt;qwen-2.5-coder&lt;/code&gt; or &lt;code&gt;deepseek-coder-v2&lt;/code&gt; through Ollama using the &lt;code&gt;openai-fim-compatible&lt;/code&gt; provider.&lt;/p&gt; &#xA;&lt;p&gt;Note: as of January 27, 2025, the high server demand from deepseek may significantly slow down the default provider used by Minuet (&lt;code&gt;openai-fim-compatible&lt;/code&gt; with deepseek). We recommend trying alternative providers instead.&lt;/p&gt; &#xA;&lt;h2&gt;Understanding Model Speed&lt;/h2&gt; &#xA;&lt;p&gt;For cloud-based providers, &lt;a href=&#34;https://openrouter.ai/google/gemini-2.0-flash-001/providers&#34;&gt;Openrouter&lt;/a&gt; offers a valuable resource for comparing the speed of both closed-source and open-source models hosted by various cloud inference providers.&lt;/p&gt; &#xA;&lt;p&gt;When assessing model speed, two key metrics are latency (time to first token) and throughput (tokens per second). Latency is often a more critical factor than throughput.&lt;/p&gt; &#xA;&lt;p&gt;Ideally, one would aim for a latency of less than 1 second and a throughput exceeding 100 tokens per second.&lt;/p&gt; &#xA;&lt;p&gt;For local LLM, &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/4167&#34;&gt;llama.cpp#4167&lt;/a&gt; provides valuable data on model speed for 7B models running on Apple M-series chips. The two crucial metrics are &lt;code&gt;Q4_0 PP [t/s]&lt;/code&gt;, which measures latency (tokens per second to process the KV cache, equivalent to the time to generate the first token), and &lt;code&gt;Q4_0 TG [t/s]&lt;/code&gt;, which indicates the tokens per second generation speed.&lt;/p&gt; &#xA;&lt;h1&gt;Prompt&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/prompt.md&#34;&gt;prompt&lt;/a&gt; for the default prompt used by &lt;code&gt;minuet&lt;/code&gt; and instructions on customization.&lt;/p&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;minuet&lt;/code&gt; employs two distinct prompt systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A system designed for chat-based LLMs (OpenAI, OpenAI-Compatible, Claude, and Gemini)&lt;/li&gt; &#xA; &lt;li&gt;A separate system designed for Codestral and OpenAI-FIM-compatible models&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Configuration&lt;/h1&gt; &#xA;&lt;p&gt;Below are commonly used configuration options. To view the complete list of available settings, search for &lt;code&gt;minuet&lt;/code&gt; through the &lt;code&gt;customize&lt;/code&gt; interface.&lt;/p&gt; &#xA;&lt;h2&gt;minuet-provider&lt;/h2&gt; &#xA;&lt;p&gt;Set the provider you want to use for completion with minuet, available options: &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;openai-compatible&lt;/code&gt;, &lt;code&gt;claude&lt;/code&gt;, &lt;code&gt;gemini&lt;/code&gt;, &lt;code&gt;openai-fim-compatible&lt;/code&gt;, and &lt;code&gt;codestral&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The default is &lt;code&gt;openai-fim-compatible&lt;/code&gt; using the deepseek endpoint.&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;ollama&lt;/code&gt; with either &lt;code&gt;openai-compatible&lt;/code&gt; or &lt;code&gt;openai-fim-compatible&lt;/code&gt; provider, depending on your model is a chat model or code completion (FIM) model.&lt;/p&gt; &#xA;&lt;h2&gt;minuet-context-window&lt;/h2&gt; &#xA;&lt;p&gt;The maximum total characters of the context before and after cursor. This limits how much surrounding code is sent to the LLM for context.&lt;/p&gt; &#xA;&lt;p&gt;The default is 16000, which roughly equates to 4000 tokens after tokenization.&lt;/p&gt; &#xA;&lt;h2&gt;minuet-context-ratio&lt;/h2&gt; &#xA;&lt;p&gt;Ratio of context before cursor vs after cursor. When the total characters exceed the context window, this ratio determines how much context to keep before vs after the cursor. A larger ratio means more context before the cursor will be used. The ratio should between 0 and &lt;code&gt;1&lt;/code&gt;, and default is &lt;code&gt;0.75&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;minuet-request-timeout&lt;/h2&gt; &#xA;&lt;p&gt;Maximum timeout in seconds for sending completion requests. In case of the timeout, the incomplete completion items will be delivered. The default is &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;minuet-show-error-message-on-minibuffer&lt;/h2&gt; &#xA;&lt;p&gt;Whether to show the error messages in minibuffer. The default value is &lt;code&gt;nil&lt;/code&gt;. When non-nil, if a request fails or times out without generating even a single token, the error message will be shown in the minibuffer. Note that you can always inspect &lt;code&gt;minuet-buffer-name&lt;/code&gt; to view the complete error log.&lt;/p&gt; &#xA;&lt;h2&gt;minuet-add-single-line-entry&lt;/h2&gt; &#xA;&lt;p&gt;For &lt;code&gt;minuet-complete-with-minibuffer&lt;/code&gt; function, Whether to create additional single-line completion items. When non-nil and a completion item has multiple lines, create another completion item containing only its first line. This option has no impact for overlay-based suggesion.&lt;/p&gt; &#xA;&lt;h2&gt;minuet-n-completions&lt;/h2&gt; &#xA;&lt;p&gt;For FIM model, this is the number of requests to send. For chat LLM , this is the number of completions encoded as part of the prompt. Note that when &lt;code&gt;minuet-add-single-line-entry&lt;/code&gt; is true, the actual number of returned items may exceed this value. Additionally, the LLM cannot guarantee the exact number of completion items specified, as this parameter serves only as a prompt guideline. The default is &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If resource efficiency is imporant, it is recommended to set this value to &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;minuet-auto-suggestion-debounce-delay&lt;/h2&gt; &#xA;&lt;p&gt;The delay in seconds before sending a completion request after typing stops. The default is &lt;code&gt;0.4&lt;/code&gt; seconds.&lt;/p&gt; &#xA;&lt;h2&gt;minuet-auto-suggestion-throttle-delay&lt;/h2&gt; &#xA;&lt;p&gt;The minimum time in seconds between 2 completion requests. The default is &lt;code&gt;1.0&lt;/code&gt; seconds.&lt;/p&gt; &#xA;&lt;h1&gt;Provider Options&lt;/h1&gt; &#xA;&lt;p&gt;You can customize the provider options using &lt;code&gt;plist-put&lt;/code&gt;, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(with-eval-after-load &#39;minuet&#xA;    ;; change openai model to gpt-4.1&#xA;    (plist-put minuet-openai-options :model &#34;gpt-4.1&#34;)&#xA;&#xA;    ;; change openai-compatible provider to use fireworks&#xA;    (setq minuet-provider &#39;openai-compatible)&#xA;    (plist-put minuet-openai-compatible-options :end-point &#34;https://api.fireworks.ai/inference/v1/chat/completions&#34;)&#xA;    (plist-put minuet-openai-compatible-options :api-key &#34;FIREWORKS_API_KEY&#34;)&#xA;    (plist-put minuet-openai-compatible-options :model &#34;accounts/fireworks/models/llama-v3p3-70b-instruct&#34;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To pass optional parameters (like &lt;code&gt;max_tokens&lt;/code&gt; and &lt;code&gt;top_p&lt;/code&gt;) to send to the curl request, you can use function &lt;code&gt;minuet-set-optional-options&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(minuet-set-optional-options minuet-openai-options :max_tokens 256)&#xA;(minuet-set-optional-options minuet-openai-options :top_p 0.9)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;OpenAI&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;Below is the default value:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defvar minuet-openai-options&#xA;    `(:model &#34;gpt-4.1-mini&#34;&#xA;      :api-key &#34;OPENAI_API_KEY&#34;&#xA;      :system&#xA;      (:template minuet-default-system-template&#xA;       :prompt minuet-default-prompt&#xA;       :guidelines minuet-default-guidelines&#xA;       :n-completions-template minuet-default-n-completion-template)&#xA;      :fewshots minuet-default-fewshots&#xA;      :chat-input&#xA;      (:template minuet-default-chat-input-template&#xA;       :language-and-tab minuet--default-chat-input-language-and-tab-function&#xA;       :context-before-cursor minuet--default-chat-input-before-cursor-function&#xA;       :context-after-cursor minuet--default-chat-input-after-cursor-function)&#xA;      :optional nil)&#xA;    &#34;config options for Minuet OpenAI provider&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Claude&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;Below is the default value:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defvar minuet-claude-options&#xA;    `(:model &#34;claude-3-5-haiku-20241022&#34;&#xA;      :max_tokens 512&#xA;      :api-key &#34;ANTHROPIC_API_KEY&#34;&#xA;      :system&#xA;      (:template minuet-default-system-template&#xA;       :prompt minuet-default-prompt&#xA;       :guidelines minuet-default-guidelines&#xA;       :n-completions-template minuet-default-n-completion-template)&#xA;      :fewshots minuet-default-fewshots&#xA;      :chat-input&#xA;      (:template minuet-default-chat-input-template&#xA;       :language-and-tab minuet--default-chat-input-language-and-tab-function&#xA;       :context-before-cursor minuet--default-chat-input-before-cursor-function&#xA;       :context-after-cursor minuet--default-chat-input-after-cursor-function)&#xA;      :optional nil)&#xA;    &#34;config options for Minuet Claude provider&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Codestral&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;Codestral is a text completion model, not a chat model, so the system prompt and few shot examples does not apply. Note that you should use the &lt;code&gt;CODESTRAL_API_KEY&lt;/code&gt;, not the &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt;, as they are using different endpoint. To use the Mistral endpoint, simply modify the &lt;code&gt;end_point&lt;/code&gt; and &lt;code&gt;api_key&lt;/code&gt; parameters in the configuration.&lt;/p&gt; &#xA; &lt;p&gt;Below is the default value:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defvar minuet-codestral-options&#xA;    &#39;(:model &#34;codestral-latest&#34;&#xA;      :end-point &#34;https://codestral.mistral.ai/v1/fim/completions&#34;&#xA;      :api-key &#34;CODESTRAL_API_KEY&#34;&#xA;      :template (:prompt minuet--default-fim-prompt-function&#xA;                 :suffix minuet--default-fim-suffix-function)&#xA;      :optional nil)&#xA;    &#34;config options for Minuet Codestral provider&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The following configuration is not the default, but recommended to prevent request timeout from outputing too many tokens.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(minuet-set-optional-options minuet-codestral-options :stop [&#34;\n\n&#34;])&#xA;(minuet-set-optional-options minuet-codestral-options :max_tokens 256)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Gemini&lt;/h2&gt; &#xA;&lt;p&gt;You should register the account and use the service from Google AI Studio instead of Google Cloud. You can get an API key via their &lt;a href=&#34;https://makersuite.google.com/app/apikey&#34;&gt;Google API page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;The following config is the default.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defvar minuet-gemini-options&#xA;    `(:model &#34;gemini-2.0-flash&#34;&#xA;      :api-key &#34;GEMINI_API_KEY&#34;&#xA;      :system&#xA;      (:template minuet-default-system-template&#xA;       :prompt minuet-default-prompt-prefix-first&#xA;       :guidelines minuet-default-guidelines&#xA;       :n-completions-template minuet-default-n-completion-template)&#xA;      :fewshots minuet-default-fewshots-prefix-first&#xA;      :chat-input&#xA;      (:template minuet-default-chat-input-template-prefix-first&#xA;       :language-and-tab minuet--default-chat-input-language-and-tab-function&#xA;       :context-before-cursor minuet--default-chat-input-before-cursor-function&#xA;       :context-after-cursor minuet--default-chat-input-after-cursor-function)&#xA;      :optional nil)&#xA;    &#34;config options for Minuet Gemini provider&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The following configuration is not the default, but recommended to prevent request timeout from outputing too many tokens. You can also adjust the safety settings following the example:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(minuet-set-optional-options&#xA; minuet-gemini-options :generationConfig&#xA; &#39;(:maxOutputTokens 256&#xA;   :topP 0.9&#xA;   ;; When using `gemini-2.5-flash`, it is recommended to entirely&#xA;   ;; disable thinking for faster completion retrieval.&#xA;   :thinkingConfig (:thinkingBudget 0)))&#xA;&#xA;(minuet-set-optional-options&#xA; minuet-gemini-options :safetySettings&#xA; [(:category &#34;HARM_CATEGORY_DANGEROUS_CONTENT&#34;&#xA;   :threshold &#34;BLOCK_NONE&#34;)&#xA;  (:category &#34;HARM_CATEGORY_HATE_SPEECH&#34;&#xA;   :threshold &#34;BLOCK_NONE&#34;)&#xA;  (:category &#34;HARM_CATEGORY_HARASSMENT&#34;&#xA;   :threshold &#34;BLOCK_NONE&#34;)&#xA;  (:category &#34;HARM_CATEGORY_SEXUALLY_EXPLICIT&#34;&#xA;   :threshold &#34;BLOCK_NONE&#34;)])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;OpenAI-compatible&lt;/h2&gt; &#xA;&lt;p&gt;Use any providers compatible with OpenAI&#39;s chat completion API.&lt;/p&gt; &#xA;&lt;p&gt;For example, you can set the &lt;code&gt;end_point&lt;/code&gt; to &lt;code&gt;http://localhost:11434/v1/chat/completions&lt;/code&gt; to use &lt;code&gt;ollama&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;The following config is the default.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defvar minuet-openai-compatible-options&#xA;    `(:end-point &#34;https://openrouter.ai/api/v1/chat/completions&#34;&#xA;      :api-key &#34;OPENROUTER_API_KEY&#34;&#xA;      :model &#34;qwen/qwen2.5-32b-instruct&#34;&#xA;      :system&#xA;      (:template minuet-default-system-template&#xA;       :prompt minuet-default-prompt&#xA;       :guidelines minuet-default-guidelines&#xA;       :n-completions-template minuet-default-n-completion-template)&#xA;      :fewshots minuet-default-fewshots&#xA;      :chat-input&#xA;      (:template minuet-default-chat-input-template&#xA;       :language-and-tab minuet--default-chat-input-language-and-tab-function&#xA;       :context-before-cursor minuet--default-chat-input-before-cursor-function&#xA;       :context-after-cursor minuet--default-chat-input-after-cursor-function)&#xA;      :optional nil)&#xA;    &#34;Config options for Minuet OpenAI compatible provider.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The following configuration is not the default, but recommended to prevent request timeout from outputing too many tokens.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(minuet-set-optional-options minuet-openai-compatible-options :max_tokens 256)&#xA;(minuet-set-optional-options minuet-openai-compatible-options :top_p 0.9)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;OpenAI-FIM-Compatible&lt;/h2&gt; &#xA;&lt;p&gt;Use any provider compatible with OpenAI&#39;s completion API. This request uses the text &lt;code&gt;/completions&lt;/code&gt; endpoint, &lt;strong&gt;not&lt;/strong&gt; &lt;code&gt;/chat/completions&lt;/code&gt; endpoint, so system prompts and few-shot examples are not applicable.&lt;/p&gt; &#xA;&lt;p&gt;For example, you can set the &lt;code&gt;end_point&lt;/code&gt; to &lt;code&gt;http://localhost:11434/v1/completions&lt;/code&gt; to use &lt;code&gt;ollama&lt;/code&gt;, or set it to &lt;code&gt;http://localhost:8012/v1/completions&lt;/code&gt; to use &lt;code&gt;llama.cpp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;Refer to the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/completions&#34;&gt;Completions Legacy&lt;/a&gt; section of the OpenAI documentation for details.&lt;/p&gt; &#xA; &lt;p&gt;Additionally, for Ollama users, it is essential to verify whether the model&#39;s template supports FIM completion. For example, qwen2.5-coder offers FIM support, as suggested in its &lt;a href=&#34;https://ollama.com/library/qwen2.5-coder/blobs/e94a8ecb9327&#34;&gt;template&lt;/a&gt;. However it may come as a surprise to some users that, &lt;code&gt;deepseek-coder&lt;/code&gt; does not support the FIM template, and you should use &lt;code&gt;deepseek-coder-v2&lt;/code&gt; instead.&lt;/p&gt; &#xA; &lt;p&gt;The following config is the default.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defvar minuet-openai-fim-compatible-options&#xA;    &#39;(:model &#34;deepseek-chat&#34;&#xA;      :end-point &#34;https://api.deepseek.com/beta/completions&#34;&#xA;      :api-key &#34;DEEPSEEK_API_KEY&#34;&#xA;      :name &#34;Deepseek&#34;&#xA;      :template (:prompt minuet--default-fim-prompt-function&#xA;                 :suffix minuet--default-fim-suffix-function)&#xA;      :optional nil)&#xA;    &#34;config options for Minuet OpenAI FIM compatible provider&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The following configuration is not the default, but recommended to prevent request timeout from outputing too many tokens.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(minuet-set-optional-options minuet-openai-fim-compatible-options :max_tokens 256)&#xA;(minuet-set-optional-options minuet-openai-fim-compatible-options :top_p 0.9)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For example bash scripts to run llama.cpp based on your local computing power, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/recipes.md&#34;&gt;recipes.md&lt;/a&gt;. Note that the model for &lt;code&gt;llama.cpp&lt;/code&gt; must be determined when you launch the &lt;code&gt;llama.cpp&lt;/code&gt; server and cannot be changed thereafter.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Non-OpenAI-FIM-Compatible APIs&lt;/h3&gt; &#xA;&lt;p&gt;For providers like &lt;strong&gt;DeepInfra FIM&lt;/strong&gt; (&lt;code&gt;https://api.deepinfra.com/v1/inference/&lt;/code&gt;), refer to &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/recipes.md&#34;&gt;recipes.md&lt;/a&gt; for advanced configuration instructions.&lt;/p&gt; &#xA;&lt;h1&gt;Troubleshooting&lt;/h1&gt; &#xA;&lt;p&gt;If your setup failed, there are two most likely reasons:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You may set the API key incorrectly. Checkout the &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.el/main/#api-keys&#34;&gt;API Key&lt;/a&gt; section to see how to correctly specify the API key.&lt;/li&gt; &#xA; &lt;li&gt;You are using a model or a context window that is too large, causing completion items to timeout before returning any tokens. This is particularly common with local LLM. It is recommended to start with the following settings to have a better understanding of your provider&#39;s inference speed. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Begin by testing with manual completions.&lt;/li&gt; &#xA;   &lt;li&gt;Use a smaller context window (e.g., &lt;code&gt;context-window = 768&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Use a smaller model&lt;/li&gt; &#xA;   &lt;li&gt;Set a longer request timeout (e.g., &lt;code&gt;request-timeout = 5&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To diagnose issues, examine the buffer content from &lt;code&gt;*minuet*&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contributions&lt;/h1&gt; &#xA;&lt;p&gt;Since this package is part of GNU ELPA, substantial contributions require a copyright assignment to the Free Software Foundation (FSF).&lt;/p&gt; &#xA;&lt;p&gt;However, minor contributionsâ€”such as small bug fixes or documentation improvementsâ€”are welcome even without copyright assignment. If you&#39;re unsure where to begin, feel free to open an issue for guidance.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.continue.dev&#34;&gt;continue.dev&lt;/a&gt;: not a emacs plugin, but I find a lot LLM models from here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.vim&#34;&gt;llama.vim&lt;/a&gt;: Reference for CLI parameters used to launch the llama-cpp server.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>