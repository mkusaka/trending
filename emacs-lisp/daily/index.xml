<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Emacs Lisp Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T01:35:06Z</updated>
  <subtitle>Daily Trending of Emacs Lisp in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>MichaelBurge/leafy-mode</title>
    <updated>2023-03-26T01:35:06Z</updated>
    <id>tag:github.com,2023-03-26:/MichaelBurge/leafy-mode</id>
    <link href="https://github.com/MichaelBurge/leafy-mode" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Emacs minor-mode built on top of org-mode for working with ChatGPT&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Leafy Mode&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Clone this repo to some folder $FOLDER, get an $OPENAI_API_KEY, then add the following to your .emacs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-elisp&#34;&gt;(defvar leafy-api-key &#34;$OPENAI_API_KEY&#34;)&#xA;(add-to-list &#39;load-path &#34;$FOLDER&#34;)&#xA;(require &#39;leafy)&#xA;&#xA;(add-hook &#39;leafy-mode-hook #&#39;leafy-enable-mode-line)&#xA;(add-hook &#39;org-mode-hook #&#39;leafy-mode)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Default keybindings are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-elisp&#34;&gt;  :keymap (let ((map (make-sparse-keymap)))&#xA;            (define-key map (kbd &#34;C-c C-c&#34;) &#39;request-completion-at-point)&#xA;&#x9;    (define-key map (kbd &#34;C-c c&#34;) &#39;leafy-log-context)&#xA;            map)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Write an org-mode document as usual, but you can invoke &lt;code&gt;request-completion-at-point&lt;/code&gt; at any time to send your document to ChatGPT to request completion. The first time you request a completion, a property drawer will be created that holds numbers like total token counts per model.&lt;/p&gt; &#xA;&lt;p&gt;If your context fills up, sibling and ancestor nodes are prioritized first and anything else dropped until it gets below the limit.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;leafy-log-context&lt;/code&gt; is used to make a temporary buffer with the context that would be sent to the API, for debugging or for easy pasting to the web UI.&lt;/p&gt; &#xA;&lt;p&gt;There is a status bar that looks like &#34;Leafy: GPT3.5 | Cost: $0.79&#34;. It shows what model you&#39;re making API requests to, and how much you&#39;ve spent so far. You can click the &#34;GPT3.5&#34; part to change the model. See &lt;code&gt;leafy-model-info-alist&lt;/code&gt; if you want to add new models.&lt;/p&gt; &#xA;&lt;p&gt;You can add tags to your headlines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;* You are ProjectGPT - a project management assistant that can also code :system:&#xA;** Some headline :ignore:&#xA;* GPT chat&#xA;** Introduction :user:&#xA;Hello ChatGPT! We are working on an [...]&#xA;** ChatGPT Response :assistant:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;:ignore: means the branch is always removed from context&lt;/li&gt; &#xA; &lt;li&gt;:system: means the Chat Completions API receives that headline as a &#34;system&#34; role.&lt;/li&gt; &#xA; &lt;li&gt;:assistant: specifies &#34;assistant&#34; role to the API&lt;/li&gt; &#xA; &lt;li&gt;:user: is assumed by default, and specifies &#34;user&#34; role to the API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO items&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute, consider working on these:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accurate token counting: &lt;code&gt;leafy-estimate-tokens-regex&lt;/code&gt; is an approximation of OpenAI&#39;s tokenization used to prioritize which sections are removed first. Right now, it overcounts and could be made exact. An older version of leafy-mode kept a Python process active and passed strings to it, but it was too error-prone so I asked ChatGPT for a regex replacement that&#39;s close enough. You might use OpenAI&#39;s Rust tiktoken or a reimplementation and load it via FFI, fix the Python interpreter, or more.&lt;/li&gt; &#xA; &lt;li&gt;Code block responses: Instead of creating a sibling section, maybe it could create an org-mode code block to hold the response. The :assistant: tags are used to determine the role sent to the API, but as long as you find some reliable way to find and tag the ChatGPT responses, code blocks might be a preferred way.&lt;/li&gt; &#xA; &lt;li&gt;Minifying code before sending: ChatGPT can read minified code, so if there&#39;s a general way to minify code(especially in a code block that&#39;s been tagged with a specific language) that could help save on tokens. Some early experiments showed up to a 60% token savings, though the more expensive ChatGPT-4 was needed to read the minified code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Remember to run the tests with &lt;code&gt;M-x ert-run-tests-interactively&lt;/code&gt; before committing any changes.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;GNU GPL v3&lt;/p&gt;</summary>
  </entry>
</feed>