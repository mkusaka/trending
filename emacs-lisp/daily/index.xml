<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Emacs Lisp Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-03T01:29:05Z</updated>
  <subtitle>Daily Trending of Emacs Lisp in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jart/emacs-copilot</title>
    <updated>2024-01-03T01:29:05Z</updated>
    <id>tag:github.com,2024-01-03:/jart/emacs-copilot</id>
    <link href="https://github.com/jart/emacs-copilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large language model code completion for Emacs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Emacs Copilot&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jart/emacs-copilot/assets/49262/1a79d4e4-9622-452e-9944-950c6f21d67f&#34;&gt;https://github.com/jart/emacs-copilot/assets/49262/1a79d4e4-9622-452e-9944-950c6f21d67f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;copilot-complete&lt;/code&gt; function demonstrates that ~100 lines of LISP is all it takes for Emacs to do that thing Github Copilot and VSCode are famous for doing except superior w.r.t. both quality and freedom&lt;/p&gt; &#xA;&lt;p&gt;Emacs Copilot helps you do pair programming with a local-running LLM that generates code completions within Emacs buffers. The LLM is run as a sub-command that remembers your local editing history on a file by file basis. Tokens stream into your buffer without delay as gen&#39;d and you can hit &lt;code&gt;C-g&lt;/code&gt; to interrupt your LLM at any time. History and memory can also be deleted from the LLM&#39;s context when deleting code from your Emacs buffer that matches up verbatim. Copilot is language agnostic and your programming language is determed by file extension&lt;/p&gt; &#xA;&lt;p&gt;One really good LLM right now is WizardCoder 34b since it scores the same as GPT-4 on HumanEval. You need a computer like a Mac Studio M2 Ultra in order to use it. If you have a mere Macbook Pro, then try the Q3 version. If you have a modest PC then you could consider downloading the WizardCoder-Python-13b llamafile since it&#39;s almost as good, and will even go acceptably fast on CPU-only systems having at least AVX2 and 2200 MT/s RAM. If you&#39;re even more strapped for compute and use things like Raspberry Pi, then give Phi-2 a spin&lt;/p&gt; &#xA;&lt;p&gt;To get started, try writing yourself the first line of a function. For example, you might open up a file in your editor named &lt;code&gt;hello.c&lt;/code&gt; and then type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;bool is_prime(int x) {&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then place your caret at the end of the line, and press &lt;code&gt;C-c C-k&lt;/code&gt; to hand over control to your LLM, which should generate the rest of the function implementation for you. Things are also tuned so the LLM is likely to stop as soon as a function is made. Explanations and other kind of ELI5 commentary is avoided too.&lt;/p&gt; &#xA;&lt;p&gt;Later on, if you were to write something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int main() {&#xA;  for (int i = 0; i &amp;lt; 100;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And ask your LLM to complete that, then your LLM will likely recall that you two wrote an is_prime() function earlier, even though it&#39;s only considering those two lines in the current instruction. You&#39;ll most likely then see it decide to generate code to print the primes&lt;/p&gt; &#xA;&lt;h2&gt;Reference Implementation&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;ve downloaded your LLM (see below) then all you really need is to copy and paste this code into an Emacs buffer and run &lt;code&gt;M-x eval-buffer&lt;/code&gt;. You&#39;ll want to tune the code to your own personal taste. That&#39;s why it&#39;s being presented in full as a succinct code example here.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-elisp&#34;&gt;(defun copilot-complete ()&#xA;  (interactive)&#xA;  (let* ((spot (point))&#xA;         (inhibit-quit t)&#xA;         (curfile (buffer-file-name))&#xA;         (cash (concat curfile &#34;.cache&#34;))&#xA;         (hist (concat curfile &#34;.prompt&#34;))&#xA;         (lang (file-name-extension curfile))&#xA;&#xA;         ;; extract current line, to left of caret&#xA;         ;; and the previous line, to give the llm&#xA;         (code (save-excursion&#xA;                 (dotimes (i 2)&#xA;                   (when (&amp;gt; (line-number-at-pos) 1)&#xA;                     (previous-line)))&#xA;                 (beginning-of-line)&#xA;                 (buffer-substring-no-properties (point) spot)))&#xA;&#xA;         ;; create new prompt for this interaction&#xA;         (system &#34;\&#xA;You are an Emacs code generator. \&#xA;Writing comments is forbidden. \&#xA;Writing test code is forbidden. \&#xA;Writing English explanations is forbidden. &#34;)&#xA;         (prompt (format&#xA;                  &#34;[INST]%sGenerate %s code to complete:[/INST]\n```%s\n%s&#34;&#xA;                  (if (file-exists-p cash) &#34;&#34; system) lang lang code)))&#xA;&#xA;    ;; iterate text deleted within editor then purge it from prompt&#xA;    (when kill-ring&#xA;      (save-current-buffer&#xA;        (find-file hist)&#xA;        (dotimes (i 10)&#xA;          (let ((substring (current-kill i t)))&#xA;            (when (and substring (string-match-p &#34;\n.*\n&#34; substring))&#xA;              (goto-char (point-min))&#xA;              (while (search-forward substring nil t)&#xA;                (delete-region (- (point) (length substring)) (point))))))&#xA;        (save-buffer 0)&#xA;        (kill-buffer (current-buffer))))&#xA;&#xA;    ;; append prompt for current interaction to the big old prompt&#xA;    (write-region prompt nil hist &#39;append &#39;silent)&#xA;&#xA;    ;; run llamafile streaming stdout into buffer catching ctrl-g&#xA;    (with-local-quit&#xA;      (call-process &#34;wizardcoder-python-34b-v1.0.Q5_K_M.llamafile&#34;&#xA;                    nil (list (current-buffer) nil) t&#xA;                    &#34;--prompt-cache&#34; cash&#xA;                    &#34;--prompt-cache-all&#34;&#xA;                    &#34;--silent-prompt&#34;&#xA;                    &#34;--temp&#34; &#34;0&#34;&#xA;                    &#34;-c&#34; &#34;1024&#34;&#xA;                    &#34;-ngl&#34; &#34;35&#34;&#xA;                    &#34;-r&#34; &#34;```&#34;&#xA;                    &#34;-r&#34; &#34;\n}&#34;&#xA;                    &#34;-f&#34; hist))&#xA;&#xA;    ;; get rid of most markdown syntax&#xA;    (let ((end (point)))&#xA;      (save-excursion&#xA;        (goto-char spot)&#xA;        (while (search-forward &#34;\\_&#34; end t)&#xA;          (backward-char)&#xA;          (delete-backward-char 1 nil)&#xA;          (setq end (- end 1)))&#xA;        (goto-char spot)&#xA;        (while (search-forward &#34;```&#34; end t)&#xA;          (delete-backward-char 3 nil)&#xA;          (setq end (- end 3))))&#xA;&#xA;      ;; append generated code to prompt&#xA;      (write-region spot end hist &#39;append &#39;silent))))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Emacs Download Link&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t have Emacs installed, or you use a platform like Windows where it&#39;s normally difficult to obtain, then here&#39;s a single-file build of Emacs that (like llamafile) is directly runnable and needn&#39;t be installed.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cosmo.zip/pub/cosmos/bin/emacs&#34;&gt;https://cosmo.zip/pub/cosmos/bin/emacs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/jart/emacs-copilot/main/#gotchas&#34;&gt;Gotchas&lt;/a&gt; below if you have trouble running it. See also the &lt;a href=&#34;https://raw.githubusercontent.com/jart/emacs-copilot/main/#supported-oses-and-cpus&#34;&gt;Supported OSes and CPUs&lt;/a&gt; list too.&lt;/p&gt; &#xA;&lt;h2&gt;LLM Download Links&lt;/h2&gt; &#xA;&lt;p&gt;Here are some LLMs that are known to work reasonably well with Emacs Copilot, that are freely available to download online. They&#39;re all good, but the biggest one is the best one. Choose the size that&#39;s appropriate for your hardware.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/mozilla-Ocho/llamafile&#34;&gt;llamafile&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardCoder-Python-34b (Q5)&lt;/td&gt; &#xA;   &lt;td&gt;23.9 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;LLaMA 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/WizardCoder-Python-34B-V1.0-llamafile/resolve/main/wizardcoder-python-34b-v1.0.Q5_K_M.llamafile?download=true&#34;&gt;wizardcoder-python-34b-v1.0.Q5_K_M.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardCoder-Python-34b (Q3)&lt;/td&gt; &#xA;   &lt;td&gt;16.3 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;LLaMA 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/WizardCoder-Python-34B-V1.0-llamafile/resolve/main/wizardcoder-python-34b-v1.0.Q3_K_M.llamafile?download=true&#34;&gt;wizardcoder-python-34b-v1.0.Q3_K_M.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardCoder-Python-13b&lt;/td&gt; &#xA;   &lt;td&gt;7.33 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;LLaMA 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/wizardcoder-13b-python/resolve/main/wizardcoder-python-13b-main.llamafile?download=true&#34;&gt;wizardcoder-python-13b-main.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi-2&lt;/td&gt; &#xA;   &lt;td&gt;2.09 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE&#34;&gt;microsoft-research-license&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/phi-2-llamafile/resolve/main/phi-2.Q5_K_M.llamafile?download=true&#34;&gt;phi-2.Q5_K_M.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Be sure to &lt;code&gt;chmod +x&lt;/code&gt; your llamafile executable after you download it. Then consider placing it on the system path. If you have any trouble running the llamafile, then see the &lt;a href=&#34;https://raw.githubusercontent.com/jart/emacs-copilot/main/#gotchas&#34;&gt;Gotchas&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Cache files&lt;/h2&gt; &#xA;&lt;p&gt;If you decide to switch models, then be sure to delete all the &lt;code&gt;FILE.cache&lt;/code&gt; files that got generated on your local filesystem.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;find . -name \*.cache | xargs rm -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also tune the Emacs LISP code above to just not use prompt caching at all, by removing those flags. That might have a negative impact on code completion latency though. On Apple Metal GPU, which has extremely fast prompt loading, the slowdown might be ~1 second, but for systems that need CPU inference it could be significantly higher.&lt;/p&gt; &#xA;&lt;h2&gt;Gotchas&lt;/h2&gt; &#xA;&lt;p&gt;On macOS with Apple Silicon you need to have Xcode installed for llamafile to be able to bootstrap itself.&lt;/p&gt; &#xA;&lt;p&gt;If you use zsh and have trouble running llamafile, try saying &lt;code&gt;sh -c ./llamafile&lt;/code&gt;. This is due to a bug that was fixed in zsh 5.9+. The same is the case for Python &lt;code&gt;subprocess&lt;/code&gt;, old versions of Fish, etc.&lt;/p&gt; &#xA;&lt;p&gt;On some Linux systems, you might get errors relating to &lt;code&gt;run-detectors&lt;/code&gt; or WINE. This is due to &lt;code&gt;binfmt_misc&lt;/code&gt; registrations. You can fix that by adding an additional registration for the APE file format llamafile uses:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo wget -O /usr/bin/ape https://cosmo.zip/pub/cosmos/bin/ape-$(uname -m).elf&#xA;sudo chmod +x /usr/bin/ape&#xA;sudo sh -c &#34;echo &#39;:APE:M::MZqFpD::/usr/bin/ape:&#39; &amp;gt;/proc/sys/fs/binfmt_misc/register&#34;&#xA;sudo sh -c &#34;echo &#39;:APE-jart:M::jartsr::/usr/bin/ape:&#39; &amp;gt;/proc/sys/fs/binfmt_misc/register&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As mentioned above, on Windows you may need to rename your llamafile by adding &lt;code&gt;.exe&lt;/code&gt; to the filename.&lt;/p&gt; &#xA;&lt;p&gt;Also as mentioned above, Windows also has a maximum file size limit of 4GB for executables. The LLaVA server executable above is just 30MB shy of that limit, so it&#39;ll work on Windows, but with larger models like WizardCoder 13B, you need to store the weights in a separate file. An example is provided above; see &#34;Using llamafile with external weights.&#34;&lt;/p&gt; &#xA;&lt;p&gt;On WSL, it&#39;s recommended that the WIN32 interop feature be disabled:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo sh -c &#34;echo -1 &amp;gt; /proc/sys/fs/binfmt_misc/WSLInterop&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On any platform, if your llamafile process is immediately killed, check if you have CrowdStrike and then ask to be whitelisted.&lt;/p&gt; &#xA;&lt;h2&gt;Supported OSes and CPUs&lt;/h2&gt; &#xA;&lt;p&gt;llamafile supports the following operating systems, which require a minimum stock install:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux 2.6.18+ (ARM64 or AMD64) i.e. any distro RHEL5 or newer&lt;/li&gt; &#xA; &lt;li&gt;Darwin (macOS) 23.1.0+ [1] (ARM64 or AMD64, with GPU only supported on ARM64)&lt;/li&gt; &#xA; &lt;li&gt;Windows 8+ (AMD64)&lt;/li&gt; &#xA; &lt;li&gt;FreeBSD 13+ (AMD64, GPU should work in theory)&lt;/li&gt; &#xA; &lt;li&gt;NetBSD 9.2+ (AMD64, GPU should work in theory)&lt;/li&gt; &#xA; &lt;li&gt;OpenBSD 7+ (AMD64, no GPU support)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;llamafile supports the following CPUs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AMD64 microprocessors must have SSSE3. Otherwise llamafile will print an error and refuse to run. This means that if you have an Intel CPU, it needs to be Intel Core or newer (circa 2006+), and if you have an AMD CPU, then it needs to be Bulldozer or newer (circa 2011+). If you have a newer CPU with AVX, or better yet AVX2, then llamafile will utilize your chipset features to go faster. There is no support for AVX512+ runtime dispatching yet.&lt;/li&gt; &#xA; &lt;li&gt;ARM64 microprocessors must have ARMv8a+. This means everything from Apple Silicon to 64-bit Raspberry Pis will work, provided your weights fit into memory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[1] Darwin kernel versions 15.6+ &lt;em&gt;should&lt;/em&gt; be supported, but we currently have no way of testing that.&lt;/p&gt; &#xA;&lt;h2&gt;A note about models&lt;/h2&gt; &#xA;&lt;p&gt;The example llamafiles provided above should not be interpreted as endorsements or recommendations of specific models, licenses, or data sets on the part of Mozilla.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rougier/nano-agenda</title>
    <updated>2024-01-03T01:29:05Z</updated>
    <id>tag:github.com,2024-01-03:/rougier/nano-agenda</id>
    <link href="https://github.com/rougier/nano-agenda" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A minimal org agenda for Emacs&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;GNU Emacs / N Λ N O Agenda&lt;/h2&gt; &#xA;&lt;p&gt;N Λ N O agenda is a minimal view of your org agenda files. It displays a calendar view of current month (or the month corresponding to the current selected date) alongside a view of your agenda displaying timestamped entries.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rougier/nano-agenda/master/nano-agenda.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Kyure-A/jobcan.el</title>
    <updated>2024-01-03T01:29:05Z</updated>
    <id>tag:github.com,2024-01-03:/Kyure-A/jobcan.el</id>
    <link href="https://github.com/Kyure-A/jobcan.el" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Managing jobcan in Emacs&lt;/p&gt;&lt;hr&gt;&lt;p&gt;#+STARTUP: content #+STARTUP: fold [[https://github.com/Kyure-A/jobcan][https://img.shields.io/github/tag/Kyure-A/jobcan.svg?style=flat-square]] [[file:LICENSE][https://img.shields.io/github/license/Kyure-A/jobcan.svg?style=flat-square]] [[https://codecov.io/gh/Kyure-A/jobcan?branch=master][https://img.shields.io/codecov/c/github/Kyure-A/jobcan.svg?style=flat-square]] [[https://github.com/Kyure-A/jobcan/actions][https://img.shields.io/github/actions/workflow/status/Kyure-A/jobcan/test.yml.svg?branch=master&amp;amp;style=flat-square]]&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;jobcan.el&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Managing jobcan in Emacs.&lt;/p&gt; &#xA;&lt;p&gt;** Usage *** Punch in #+begin_src console M-x jobcan-touch #+end_src *** Get information about linked users #+begin_src console M-x jobcan-linked #+end_src *** Status #+begin_src console M-x jobcan-current-status #+end_src ** License This package is licensed by The GNU General Public License verson 3 or later. See [[file:LICENSE][LICENSE]].&lt;/p&gt;</summary>
  </entry>
</feed>