<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Emacs Lisp Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-08T01:33:13Z</updated>
  <subtitle>Daily Trending of Emacs Lisp in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>natrys/whisper.el</title>
    <updated>2023-04-08T01:33:13Z</updated>
    <id>tag:github.com,2023-04-08:/natrys/whisper.el</id>
    <link href="https://github.com/natrys/whisper.el" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Speech-to-Text interface for Emacs using OpenAI&#39;s whisper model and whisper.cpp as inference engine.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;#+STARTUP: showeverything&lt;/p&gt; &#xA;&lt;p&gt;** whisper.el&lt;/p&gt; &#xA;&lt;p&gt;Speech-to-Text interface for Emacs using OpenAI&#39;s [[https://github.com/openai/whisper][whisper speech recognition model]]. For the inference engine it uses the awesome C/C++ port [[https://github.com/ggerganov/whisper.cpp][whisper.cpp]] that can run on consumer grade CPU (without requiring a high end GPU).&lt;/p&gt; &#xA;&lt;p&gt;You can capture audio with your local input device (microphone) or choose a media file on disk in your local language, and have the transcribed text pasted to your Emacs buffer (optionally after translating to English). This runs offline without having to use non-free cloud service for decent result (though result quality of whisper varies widely depending on language, see below).&lt;/p&gt; &#xA;&lt;p&gt;*** Install and Usage&lt;/p&gt; &#xA;&lt;p&gt;Aside from a C++ compiler (to compile whisper.cpp), the system needs to have =FFmpeg= for recording audio.&lt;/p&gt; &#xA;&lt;p&gt;You can install =whisper.el= by cloning this repo somewhere, and then use it like:&lt;/p&gt; &#xA;&lt;p&gt;#+begin_src elisp (use-package whisper :load-path &#34;path/to/whisper.el&#34; :bind (&#34;C-H-r&#34; . whisper-run) :config (setq whisper-install-directory &#34;/tmp/&#34; whisper-model &#34;base&#34; whisper-language &#34;en&#34; whisper-translate nil)) #+end_src&lt;/p&gt; &#xA;&lt;p&gt;You will use these functions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;=whisper-run=: Toggle between recording from your microphone and transcribing&lt;/li&gt; &#xA; &lt;li&gt;=whisper-file=: Same as before but transcribes a local file on disk&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Invoking =whisper-run= with a prefix argument (C-u) has the same effect as =whisper-file=.&lt;/p&gt; &#xA;&lt;p&gt;Both of these functions will automatically compile whisper.cpp dependency and download language model the first time they are run. When recording is in progress, invoking them stops it and starts transcribing. Otherwise if compilation, download (of model file) or transcription job is in progress, calling them again cancels that.&lt;/p&gt; &#xA;&lt;p&gt;*** Variables&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;=whisper-install-directory=: Location where whisper.cpp will be installed. Default is =~/.emacs.d/.cache/=.&lt;/li&gt; &#xA; &lt;li&gt;=whisper-language=: Specify your spoken language. Default is =en=. For all possible short-codes: [[https://github.com/ggerganov/whisper.cpp/blob/aa6adda26e1ee9843dddb013890e3312bee52cfe/whisper.cpp#L31][see here]]. You can also set it to =auto= to allow whisper.cpp to infer the language from first 30 seconds of audio. How well whisper works will vary depending on the language. Some scores could be found in the original paper, or [[https://github.com/openai/whisper#available-models-and-languages][here]].&lt;/li&gt; &#xA; &lt;li&gt;=whisper-model=: Which language model to use. Default is =base=. Values are: tiny, base, small, medium, large-v1, large. Bigger models are more accurate, but takes more time and more RAM to run (aside from more disk space and download size), see: [[https://github.com/ggerganov/whisper.cpp#memory-usage][resource requirements]]. Note that these come with .en variants that might be faster, but are for English only.&lt;/li&gt; &#xA; &lt;li&gt;=whisper-translate=: Default =nil= means transcription output language is same as spoken language. Setting it to =t= translates it to English first.&lt;/li&gt; &#xA; &lt;li&gt;=whisper-use-threads=: Default =nil= means let whisper.cpp choose appropriate value (which it sets with formula min(4, num_of_cores)). If you want to use more than 4 threads (as you have more than 4 cpu cores), set this number manually.&lt;/li&gt; &#xA; &lt;li&gt;=whisper-recording-timeout=: Default is =300= seconds. We do not want to start recording and then forget. The intermediate temporary file is stored in uncompressed =wav= format (roughly 4.5mb per minute but can vary), they can grow and fill disk even if &lt;del&gt;/tmp/&lt;/del&gt; is used for it by default.&lt;/li&gt; &#xA; &lt;li&gt;=whisper-enable-speed-up=: Default is =nil=. This can supposedly speed up transcribing up to 2x, at the expense of some accuracy loss. You should experiment if it works for you, specially when using larger models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, depending on your input device and system you will need to modify these variables to get recording to work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;=whisper--ffmpeg-input-format=: This is what you would pass to the =-f= flag of FFmpeg to input, to record audio. Default is =pulse= on Linux, =avfoundation= on OSX and =dshow= on Windows.&lt;/li&gt; &#xA; &lt;li&gt;=whisper--ffmpeg-input-device=: This is what you would pass to the =-i= flag of FFmpeg to record audio, like &lt;del&gt;hw:0,2&lt;/del&gt; or something. There is no default (unless you are using pulseaudio in that case it&#39;s =default=) so this will likely need to be set.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;*** Caveats&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Whisper is open-source in the sense that weights and the engine source is available. But training data or methodology is not.&lt;/li&gt; &#xA; &lt;li&gt;Real time transcribing is probably not feasible with it yet. The accuracy is better when it has a bigger window of surrounding context. Plus it would need beefy hardware to keep up, possibly using a smaller model. There is some interesting activity going on at whisper.cpp upstream, but in the end I don&#39;t see the appeal of that in my workflow (yet).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>